Reynold Xin <rxin@databricks.com>,"Fri, 1 Apr 2016 00:15:06 -0700",[discuss] using deep learning to improve Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Hope you all enjoyed the Tesla 3 unveiling earlier tonight.

I'd like to bring your attention to a project called DeepSpark that we have
been working on for the past three years. We realized that scaling software
development was challenging. A large fraction of software engineering has
been manual and mundane: writing test cases, fixing bugs, implementing
features according to specs, and reviewing pull requests. So we started
this project to see how much we could automate.

After three years of development and one year of testing, we now have
enough confidence that this could work well in practice. For example, Matei
confessed to me today: ""It looks like DeepSpark has a better understanding
of Spark internals than I ever will. It updated several pieces of code I
wrote long ago that even I no longer understood.‚Äù


I think it's time to discuss as a community about how we want to continue
this project to ensure Spark is stable, secure, and easy to use yet able to
progress as fast as possible. I'm still working on a more formal design
doc, and it might take a little bit more time since I haven't been able to
fully grasp DeepSpark's capabilities yet. Based on my understanding right
now, I've written a blog post about DeepSpark here:
https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html


Please take a look and share your thoughts. Obviously, this is an ambitious
is cost. The current Spark Jenkins infrastructure provided by the AMPLab
has only 8 machines, but DeepSpark uses 12000 machines. I'm not sure
whether AMPLab or Databricks can fund DeepSpark's operation for a long
period of time. Perhaps AWS can help out here. Let me know if you have
other ideas.
"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Fri, 1 Apr 2016 07:33:07 +0000 (UTC)",Re: [discuss] using deep learning to improve Spark,"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","I see you've been burning the midnight oil.

      From: Reynold Xin <rxin@databricks.com>
 To: ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Friday, April 1, 2016 1:15 AM
 Subject: [discuss] using deep learning to improve Spark
   
Hi all,
Hope you all enjoyed the Tesla 3 unveiling earlier tonight.
I'd like to bring your attention to a project called DeepSpark that we have been working on for the past three years. We realized that scaling software development was challenging. A large fraction of software engineering has been manual and mundane: writing test cases, fixing bugs, implementing features according to specs, and reviewing pull requests. So we started this project to see how much we could automate.
After three years of development and one year of testing, we now have enough confidence that this could work well in practice. For example, Matei confessed to me today: ""It looks like DeepSpark has a better understanding of Spark internals than I ever will. It updated several pieces of code I wrote long ago that even I no longer understood.‚Äù

I think it's time to discuss as a community about how we want to continue this project to¬†ensure Spark is stable, secure, and easy to use yet able to progress as fast as possible. I'm still working on a more formal design doc, and it might take a little bit more time since I haven't been able to fully grasp DeepSpark's capabilities yet. Based on my understanding right now, I've written a blog post about DeepSpark here: https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html

Please take a look and share your thoughts. Obviously, this is an ambitiousis cost. The current Spark Jenkins infrastructure provided by the AMPLab has only 8 machines, but DeepSpark uses 12000 machines. I'm not sure whether AMPLab or Databricks can fund DeepSpark's operation for a long period of time. Perhaps AWS can help out here. Let me know if you have other ideas.







  "
Xiao Li <gatorsmile@gmail.com>,"Fri, 1 Apr 2016 00:37:27 -0700",Re: [discuss] using deep learning to improve Spark,Michael Malak <michaelmalak@yahoo.com>,"April 1st... : )

2016-04-01 0:33 GMT-07:00 Michael Malak <michaelmalak@yahoo.com.invalid>:

ei
g
to
o
-learning-on-spark.html
he
re
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 1 Apr 2016 18:40:21 +0900",Re: [discuss] using deep learning to improve Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Oh, the annual event...


,
o
tei
ng
e
 to
to
t
p-learning-on-spark.html
r
the
ure


-- 
---
Takeshi Yamamuro
"
Michael Segel <msegel_hadoop@hotmail.com>,"Fri, 1 Apr 2016 07:23:11 -0500",Re: Any documentation on Spark's security model beyond YARN?,Steve Loughran <stevel@hortonworks.com>,"Guys, 

Getting a bit off topic.  

Saying Security and HBase in the same sentence is a bit of a joke until HBase rejiggers its co-processers. Although‚Äôs Andrew‚Äôs fix could be enough to keep CSOs and their minions happy.

The larger picture is that Security has to stop being a ‚Äòsecond restricted data, you will have issues and anything you can do to stop leakage or the potential of leakage would be great. 

Getting back to spark specifically, you have components like the Thrift Service which can persist RDDs and I don‚Äôt see any restrictions on access. 

Does this mean integration w Ranger or Sentry? Does it mean rolling a separate solution? 

And if you‚Äôre going to look at Thrift, do you want to look at other potential areas as well? 

Please note: This may all be for nothing. It may be just having the discussion and coming to a conclusion as to the potential risks and how to mitigate is enough. 

Thx

-Mike

inherit its security from the underlying YARN job.
think about some use cases.
thinking about how to enhance spark‚Äôs security.
long-lived services, alongside hdfs
assumes its only for HDFS (indeed, that the filesystem is HDFS and therefore the #of tokens > 0); there' s no fundamental reason why the code in YarnSparkHadoopUtils can't run in the AM too.
to
for it to be a fairly generic API. Even if Spark has to use reflection to get at it, at least it would be consistent across services. See https://issues.apache.org/jira/browse/HADOOP-12563 <https://issues.apache.org/jira/browse/HADOOP-12563>
elsewhere; needed for long-haul job submission when  you don' t have a keytab to hand. This could be useful as it'd avoid actually needing hbase-*.jar on the classpath at submit time.
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Fri, 1 Apr 2016 17:05:43 +0200",Re: [discuss] using deep learning to improve Spark,Takeshi Yamamuro <linguin.m.s@gmail.com>,"Amazing! I'll fund $1/2 million for such a interesting initiative.
Oh, wait... I have only $4 on my pocket

Cheers :)


:
s,
So
atei
ing
I
e
en't
ep-learning-on-spark.html
or
 the
sure
"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Fri, 01 Apr 2016 17:03:24 +0000",Re: how about a custom coalesce() policy?,Reynold Xin <rxin@databricks.com>,"Hey Reynold,
Created an issue (and a PR) for this change to get discussions started.

Thanks,
Nezih


:
ve
g
h
bute
"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 1 Apr 2016 10:10:22 -0700","Declare rest of @Experimental items non-experimental if they've
 existed since 1.2.0","sowen@cloudera.com, Tathagata Das <tdas@databricks.com>","Hi Sean,

We're upgrading Mobius (C# binding for Spark) in Microsoft to align with
Spark 1.6.2 and noticed some changes in API you did in

https://github.com/apache/spark/commit/6f81eae24f83df51a99d4bb2629dd7daadc01519


mostly on APIs with Approx postfix. (still marked as experimental in
pyspark though)


Can you help us understand how important these APIs are?


(in C# we don't implement experimental APIs yet. We're not so sure whether
we should or not)


Thanks a lot,

Renyi.
"
Michael Slavitch <slavitch@gmail.com>,"Fri, 1 Apr 2016 14:32:05 -0400",Eliminating shuffle write and spill disk IO reads/writes in Spark,"user@spark.apache.org,
 dev@spark.apache.org","Hello;

I‚Äôm working on spark with very large memory systems (2TB+) and notice that Spark spills to disk in shuffle.  Is there a way to force spark to stay in memory when doing shuffle operations?   The goal is to keep the shuffle data either in the heap or in off-heap memory (in 1.6.x) and never touch the IO subsystem.  I am willing to have the job fail if it runs out of RAM.

spark.shuffle.spill true  is deprecated in 1.6 and does not work in Tungsten sort in 1.5.x

""WARN UnsafeShuffleManager: spark.shuffle.spill was set to false, but this is ignored by the tungsten-sort shuffle manager; its optimized shuffles will continue to spill to disk when necessary.‚Äù

If this is impossible via configuration changes what code changes would be needed to accomplish this?





---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 1 Apr 2016 12:54:49 -0700","Re: Declare rest of @Experimental items non-experimental if they've
 existed since 1.2.0",Renyi Xiong <renyixiong0@gmail.com>,"The change there was just to mark the methods non-experimental. The
logic was that they'd been around for many releases without change,
and are unlikely to be changed now that they've been in the wild so
long, so already acted as if they're part of the normal stable API.

Are they important? I personally consider the approximate count
methods useful. There was some recent talk of deprecating the
approximate sum, mean methods. But they're no longer experimental and
not going away soon so I suppose they're worth supporting.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 1 Apr 2016 13:26:27 -0700",Re: Spark SQL UDF Returning Rows,Hamel Kothari <hamelkothari@gmail.com>,"
There is an experimental preview of Datasets in Spark 1.6



Even if you give us a Row there's still a conversion into the binary format
of InternalRow



This was before we decided to unify the APIs for Scala and Java, so its
mostly historical.
"
Michael Armbrust <michael@databricks.com>,"Fri, 1 Apr 2016 13:29:11 -0700",Re: What influences the space complexity of Spark operations?,Steve Johnston <sjohnston@algebraixdata.com>,"Blocking operators like Sort, Join or Aggregate will put all of the data
for a whole partition into a hash table or array.  However, if you are
running Spark 1.5+ we should be spilling to disk.  In Spark 1.6 if you are
seeing OOMs for SQL operations you should report it as a bug.

m

d
he
nd
t
ingle
d
s-the-space-complexity-of-Spark-operations-tp16944.html>
"
Reynold Xin <rxin@databricks.com>,"Fri, 1 Apr 2016 13:57:12 -0700",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Michael Slavitch <slavitch@gmail.com>,"spark.shuffle.spill actually has nothing to do with whether we write
shuffle files to disk. Currently it is not possible to not write shuffle
files to disk, and typically it is not a problem because the network fetch
throughput is lower than what disks can sustain. In most cases, especially
with SSDs, there is little difference between putting all of those in
memory and on disk.

However, it is becoming more common to run Spark on a few number of beefy
nodes (e.g. 2 nodes each with 1TB of RAM). We do want to look into
improving performance for those. Meantime, you can setup local ramdisks on
each node for shuffle writes.




tice that
n
M.
s
e
"
Michael Slavitch <slavitch@gmail.com>,"Fri, 1 Apr 2016 17:10:25 -0400",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Reynold Xin <rxin@databricks.com>,"I totally disagree that it‚Äôs not a problem.

- Network fetch throughput on 40G Ethernet exceeds the throughput of NVME drives.
- What Spark is depending on is Linux‚Äôs IO cache as an effective buffer pool  This is fine for small jobs but not for jobs with datasets in the TB/node range.
multiple executors writing out to disk increases greatly. 

I thought the whole point of Spark was in-memory computing?  It‚Äôs in fact in-memory for some things but  use spark.local.dir as a buffer pool of others.  

Hence, the performance of  Spark is gated by the performance of spark.local.dir, even on large memory systems.

""Currently it is not possible to not write shuffle files to disk.‚Äù

What changes >would< make it possible?

The only one that seems possible is to clone the shuffle service and make it in-memory.





shuffle files to disk. Currently it is not possible to not write shuffle files to disk, and typically it is not a problem because the network fetch throughput is lower than what disks can sustain. In most cases, especially with SSDs, there is little difference between putting all of those in memory and on disk.
beefy nodes (e.g. 2 nodes each with 1TB of RAM). We do want to look into improving performance for those. Meantime, you can setup local ramdisks on each node for shuffle writes.
notice that Spark spills to disk in shuffle.  Is there a way to force spark to stay in memory when doing shuffle operations?   The goal is to keep the shuffle data either in the heap or in off-heap memory (in 1.6.x) and never touch the IO subsystem.  I am willing to have the job fail if it runs out of RAM.
Tungsten sort in 1.5.x
this is ignored by the tungsten-sort shuffle manager; its optimized shuffles will continue to spill to disk when necessary.‚Äù
would be needed to accomplish this?
<mailto:user-unsubscribe@spark.apache.org>
<mailto:user-help@spark.apache.org>

"
Reynold Xin <rxin@databricks.com>,"Fri, 1 Apr 2016 14:22:34 -0700",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Michael Slavitch <slavitch@gmail.com>,"Sure - feel free to totally disagree.


:

buffer
 in fact
ù
h
y
n
otice
er
t
"
Reynold Xin <rxin@databricks.com>,"Fri, 1 Apr 2016 14:57:08 -0700",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Michael Slavitch <slavitch@gmail.com>,"If you work for a certain hardware vendor that builds expensive, high
performance nodes, and want to use Spark to demonstrate the performance
gains of your new great systems, you will of course totally disagree.

Anyway - I offered you a simple solution to work around the low hanging
fruits. Feel free to totally disagree and reject that. Yes you might see
problems with kernel being unable to manage the buffer pool as well as
Spark itself can, but you also might not because most of the software stack
(not just Spark, but software in general) are in general inefficient and
far away from what the hardware can do at its limit, so having minor, or
sometimes even major imperfections somewhere in the stack isn't necessary a
problem.

For example, you will find that the network software stack in Spark (or
majority of the open source projects you will find) actually won't be able
to saturate a 10G network in practical jobs, let alone 40G. Decryption,
deserialization, or data processing themselves can be expensive, to the
point that it doesn't really matter how high your disk throughput or
network throughput is.

While I think 40G network is coming, they are far away from ubiquity. Does
that mean we shouldn't care? No. But it takes time and resources to address
them, and in most cases they are not actually the bottleneck. It is not as
simple as putting the data in memory, because we'd need to build a bunch of
machinery to share that limited memory with the execution part, which have
been by far the largest bottleneck.


So what does it take to improve this?

First and foremost, we would need to substantially speed up the execution
engine itself. We are making great progress in Spark 2.0. For a lot of the
common SQL-like operations, Spark 2.0 can be pretty fast (e.g. filtering 1
billion records a second, or joining 100 million records a second; using a
single core).

However, I still don't think it matters much disk vs memory for temporary
shuffle files in a moderately sized cluster with SSDs, until we rewrite the
network stack to be able to sustainably saturate 10G links. Spark was able
to do that two years ago when I first implemented the current network
module, but I'm sure after two years of feature development, bug fixes, and
security improvement, the network module can no longer do that. Why haven't
we fixed it yet? Because most workloads don't shuffle enormous amount of
data and when they do, they are not bounded by slower network stack (either
software or hardware).


number of nodes. In the most extreme case with only a single node, the
current way we do shuffle in Spark is one to two orders of magnitude slower
than some simple in-memory data partitioning algorithm (e.g. radix sort).
Addressing that can speed up certain Spark workloads (large joins, large
aggregations) quite a bit.





E
 buffer
e
s in fact
ù
e
ch
ly
y
on
notice
ver
ut
"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 1 Apr 2016 15:05:18 -0700",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Michael Slavitch <slavitch@gmail.com>,"I think Reynold's suggestion of using ram disk would be a good way to
test if these are the bottlenecks or something else is.
For most practical purposes, pointing local dir to ramdisk should
effectively give you 'similar' performance as shuffling from memory.

Are there concerns with taking that approach to test ? (I dont see
any, but I am not sure if I missed something).


Regards,
Mridul




:
buffer pool
 in fact
ù
 it
fle
put
,
ing
e
otice that
in
data
IO
is
will
be

---------------------------------------------------------------------


"
Michael Slavitch <slavitch@gmail.com>,"Fri, 01 Apr 2016 22:28:16 +0000",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Mridul Muralidharan <mridul@gmail.com>,"RAMdisk is a fine interim step but there is a lot of layers eliminated by
keeping things in memory unless there is need for spillover.   At one time
there was support for turning off spilling.  That was eliminated.  Why?


ME
e buffer
de
le
ôs in fact
ù
to
n
fy
 notice
y
e
e
s
d
-- 
Michael Slavitch
62 Renfrew Ave.
K1S 1Z5
"
Reynold Xin <rxin@databricks.com>,"Fri, 1 Apr 2016 15:32:23 -0700",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Michael Slavitch <slavitch@gmail.com>,"Michael - I'm not sure if you actually read my email, but spill has nothing
to do with the shuffle files on disk. It was for the partitioning (i.e.
sorting) process. If that flag is off, Spark will just run out of memory
when data doesn't fit in memory.


:

e
:
ve buffer
ôs in
Äù
on
d notice
"
Yong Zhang <java8964@hotmail.com>,"Fri, 1 Apr 2016 18:37:23 -0400","RE: Eliminating shuffle write and spill disk IO reads/writes in
 Spark","Reynold Xin <rxin@databricks.com>, Michael Slavitch <slavitch@gmail.com>","Is there a configuration in the Spark of location of ""shuffle spilling""? I didn't recall ever see that one. Can you share it out?
It will be good for a test writing to RAM Disk if that configuration is available.
Thanks
Yong

From: rxin@databricks.com
Date: Fri, 1 Apr 2016 15:32:23 -0700
Subject: Re: Eliminating shuffle write and spill disk IO reads/writes in Spark
To: slavitch@gmail.com
CC: mridul@gmail.com; dev@spark.apache.org; user@spark.apache.org

Michael - I'm not sure if you actually read my email, but spill has nothing to do with the shuffle files on disk. It was for the partitioning (i.e. sorting) process. If that flag is off, Spark will just run out of memory when data doesn't fit in memory. 

RAMdisk is a fine interim step but there is a lot of layers eliminated by keeping things in memory unless there is need for spillover.   At one time there was support for turning off spilling.  That was eliminated.  Why? 

rote:
I think Reynold's suggestion of using ram disk would be a good way to

test if these are the bottlenecks or something else is.

For most practical purposes, pointing local dir to ramdisk should

effectively give you 'similar' performance as shuffling from memory.



Are there concerns with taking that approach to test ? (I dont see

any, but I am not sure if I missed something).





Regards,

Mridul














 pool







ct











 it







:


fle


ghput

Ds,




fy

ing

ode





om>





that

in

data

IO






this

s will



be













-- 
Michael Slavitch62 Renfrew Ave.
K1S 1Z5

 		 	   		  "
Reynold Xin <rxin@databricks.com>,"Fri, 1 Apr 2016 15:38:30 -0700",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Yong Zhang <java8964@hotmail.com>,"It's spark.local.dir.



I
e
:
ME
e buffer
de
le
ôs in fact
ù
to
n
fy
 notice
y
e
e
s
d
"
Michael Slavitch <slavitch@gmail.com>,"Fri, 01 Apr 2016 22:39:30 +0000",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Reynold Xin <rxin@databricks.com>,"Shuffling a 1tb set of keys and values (aka sort by key)  results in about
500gb of io to disk if compression is enabled. Is there any way to
eliminate shuffling causing io?


y
me
ive
ôs in
f
Äù
s
nd notice
t
-
Michael Slavitch
62 Renfrew Ave.
K1S 1Z5
"
Michael Slavitch <slavitch@gmail.com>,"Fri, 01 Apr 2016 22:48:24 +0000",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Reynold Xin <rxin@databricks.com>,"As I mentioned earlier this flag is now ignored.


t
g
e
tive
Äôs in
of
Äù
d
h
m
and
.
--
-- 
Michael Slavitch
62 Renfrew Ave.
K1S 1Z5
"
Koert Kuipers <koert@tresata.com>,"Fri, 1 Apr 2016 19:10:25 -0400",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Michael Armbrust <michael@databricks.com>,"as long as we don't lock ourselves into supporting scala 2.10 for the
entire spark 2 lifespan it sounds reasonable to me


"
Saisai Shao <sai.sai.shao@gmail.com>,"Sat, 2 Apr 2016 07:25:53 +0800",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Michael Slavitch <slavitch@gmail.com>,"Hi Michael, shuffle data (mapper output) have to be materialized into disk
finally, no matter how large memory you have, it is the design purpose of
Spark. In you scenario, since you have a big memory, shuffle spill should
not happen frequently, most of the disk IO you see might be final shuffle
file write.

So if you want to avoid this disk IO, you could use ramdisk as Reynold
suggested. If you want to avoid FS overhead of ramdisk, you could try to
hack a new shuffle implementation, since shuffle framework is pluggable.


:

ng
ne
f
ctive
Äôs in
‚Äù
d
:
e
h
 and
h
M.
n
"
Michael Slavitch <slavitch@gmail.com>,"Fri, 01 Apr 2016 23:27:29 +0000",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Saisai Shao <sai.sai.shao@gmail.com>,"Yes we see it on final write.  Our preference is to eliminate this.


k
:
o
ing
f
d
one
.
o
ective
Äôs
l
‚Äù
f
) and
o
in
-
Michael Slavitch
62 Renfrew Ave.
K1S 1Z5
"
Saisai Shao <sai.sai.shao@gmail.com>,"Sat, 2 Apr 2016 07:44:09 +0800",Re: Eliminating shuffle write and spill disk IO reads/writes in Spark,Michael Slavitch <slavitch@gmail.com>,"So I think ramdisk is simple way to try.

Besides I think Reynold's suggestion is quite valid, with such high-end
machine, putting everything in memory might not improve the performance a
lot as assumed. Since bottleneck will be shifted, like memory bandwidth,
NUMA, CPU efficiency (serialization-deserialization, data processing...).
Also code design should well consider such usage scenario, to use resource
more efficiently.

Thanks
Saisai

:

ose
to
ning
of
lover.
to
.
m>
fective
‚Äôs
‚Äù
o
+) and
,
s
--
"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 1 Apr 2016 18:59:03 -0700","RE: Declare rest of @Experimental items non-experimental if
 they'veexisted since 1.2.0",Sean Owen <sowen@cloudera.com>,"Thanks a lot, Sean, really appreciate your comments.

Sent from my Windows 10 phone

From: Sean Owen
Sent: Friday, April 1, 2016 12:55 PM
To: Renyi Xiong
Cc: Tathagata Das; dev
Subject: Re: Declare rest of @Experimental items non-experimental if they'veexisted since 1.2.0

The change there was just to mark the methods non-experimental. The
logic was that they'd been around for many releases without change,
and are unlikely to be changed now that they've been in the wild so
long, so already acted as if they're part of the normal stable API.

Are they important? I personally consider the approximate count
methods useful. There was some recent talk of deprecating the
approximate sum, mean methods. But they're no longer experimental and
not going away soon so I suppose they're worth supporting.

c01519
ark
r

"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Sat, 2 Apr 2016 05:00:34 +0000",RE: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,"Koert Kuipers <koert@tresata.com>, Michael Armbrust
	<michael@databricks.com>","What about a seperate branch for scala 2.10?



Sent from my Samsung Galaxy smartphone.


-------- Original message --------
From: Koert Kuipers <koert@tresata.com>
Date: 4/2/2016 02:10 (GMT+02:00)
To: Michael Armbrust <michael@databricks.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Mark Hamstra <mark@clearstorydata.com>, Cody Koeninger <cody@koeninger.org>, Sean Owen <sowen@cloudera.com>, dev@spark.apache.org
Subject: Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle

as long as we don't lock ourselves into supporting scala 2.10 for the entire spark 2 lifespan it sounds reasonable to me

+1 to Matei's reasoning.

I agree that putting it in 2.0 doesn't mean keeping Scala 2.10 for the entire 2.x line. My vote is to keep Scala 2.10 in Spark 2.0, because it's the default version we built with in 1.x. We want to make the transition from 1.x to 2.0 as easy as possible. In 2.0, we'll have the default downloads be for Scala 2.11, so people will more easily move, but we shouldn't create obstacles that lead to fragmenting the community and slowing down Spark 2.0's adoption. I've seen companies that stayed on an old Scala version for multiple years because switching it, or mixing versions, would affect the company's entire codebase.

Matei


oh wow, had no idea it got ripped out

No, with 2.0 Spark really doesn't use Akka: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L744


Spark still runs on akka. So if you want the benefits of the latest akka (not saying we do, was just an example) then you need to drop scala 2.10

I agree with Mark in that I don't see how supporting scala 2.10 for
spark 2.0 implies supporting it for all of spark 2.x

Regarding Koert's comment on akka, I thought all akka dependencies
have been removed from spark after SPARK-7997 and the recent removal
of external/akka

 be
s,
bscribe@spark.apache.org>
lp@spark.apache.org>

---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>






"
Hemant Bhanawat <hemant9379@gmail.com>,"Sun, 3 Apr 2016 11:09:00 +0530",Re: how about a custom coalesce() policy?,Nezih Yigitbasi <nyigitbasi@netflix.com.invalid>,"Hi Nezih,

Can you share JIRA and PR numbers?

This partial de-coupling of data partitioning strategy and spark
parallelism would be a useful feature for any data store.

Hemant

Hemant Bhanawat <https://www.linkedin.com/in/hemant-bhanawat-92a3811>
www.snappydata.io


o
ive
ng
sh
ibute
"
Hemant Bhanawat <hemant9379@gmail.com>,"Sun, 3 Apr 2016 11:10:40 +0530",Re: how about a custom coalesce() policy?,Nezih Yigitbasi <nyigitbasi@netflix.com>,"correcting email id for Nezih

Hemant Bhanawat <https://www.linkedin.com/in/hemant-bhanawat-92a3811>
www.snappydata.io


:
e
do
Hive
.
ing
ish
"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Sun, 03 Apr 2016 06:27:13 +0000",Re: how about a custom coalesce() policy?,Hemant Bhanawat <hemant9379@gmail.com>,"Sure, here <https://issues.apache.org/jira/browse/SPARK-14042> is the jira
and this <https://github.com/apache/spark/pull/11865> is the PR.

Nezih


I.
the
 Hive
y.
mented a
r
lish
"
Jacek Laskowski <jacek@japila.pl>,"Sun, 3 Apr 2016 15:23:05 -0400",[SQL] Dataset.map gives error: missing parameter type for expanded function?,dev <dev@spark.apache.org>,"Hi,

(since 2.0.0-SNAPSHOT it's more for dev not user)

With today's master I'm getting the following:

scala> ds
res14: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]

// WHY?!
scala> ds.groupBy(_._1)
<console>:26: error: missing parameter type for expanded function
((x$1) => x$1._1)
       ds.groupBy(_._1)
                  ^

scala> ds.filter(_._1.size > 10)
res23: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]

It's even on the slide of Michael in
https://youtu.be/i7l3JQRx7Qw?t=7m38s from Spark Summit East?! Am I
doing something wrong? Please guide.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 3 Apr 2016 16:00:38 -0700",explain codegen,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
Based on master branch refreshed today, I issued 'git clean -fdx' first.

Then this command:
build/mvn clean -Phive -Phive-thriftserver -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.0 package -DskipTests

I got the following error:

scala>  sql(""explain codegen select 'a' as a group by 1"").head
org.apache.spark.sql.catalyst.parser.ParseException:
extraneous input 'codegen' expecting {'(', 'SELECT', 'FROM', 'ADD', 'DESC',
'WITH', 'VALUES', 'CREATE', 'TABLE', 'INSERT', 'DELETE', 'DESCRIBE',
'EXPLAIN', 'LOGICAL', 'SHOW', 'USE', 'DROP', 'ALTER', 'MAP', 'SET',
'START', 'COMMIT', 'ROLLBACK', 'REDUCE', 'EXTENDED', 'REFRESH', 'CLEAR',
'CACHE', 'UNCACHE', 'FORMATTED', 'DFS', 'TRUNCATE', 'ANALYZE', 'REVOKE',
'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'EXPORT', 'IMPORT', 'LOAD'}(line 1, pos
8)

== SQL ==
explain codegen select 'a' as a group by 1
--------^^^

Can someone shed light ?

Thanks
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 4 Apr 2016 00:38:09 -0400",Re: explain codegen,Ted Yu <yuzhihong@gmail.com>,"Hi,

Looks related to the recent commit...

Repository: spark
Updated Branches:
  refs/heads/master 2262a9335 -> 1f0c5dceb

[SPARK-14350][SQL] EXPLAIN output should be in a single cell

Jacek
03.04.2016 7:00 PM ""Ted Yu"" <yuzhihong@gmail.com> napisa≈Ç(a):

"
Reynold Xin <rxin@databricks.com>,"Sun, 3 Apr 2016 21:42:16 -0700",Re: explain codegen,Jacek Laskowski <jacek@japila.pl>,"Works for me on latest master.



scala> sql(""explain codegen select 'a' as a group by 1"").head
res3: org.apache.spark.sql.Row =
[Found 2 WholeStageCodegen subtrees.
== Subtree 1 / 2 ==
WholeStageCodegen
:  +- TungstenAggregate(key=[], functions=[], output=[a#10])
:     +- INPUT
+- Exchange SinglePartition, None
   +- WholeStageCodegen
      :  +- TungstenAggregate(key=[], functions=[], output=[])
      :     +- INPUT

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ /** Codegened pipeline for:
/* 006 */ * TungstenAggregate(key=[], functions=[], output=[a#10])
/* 007 */ +- INPUT
/* 008 */ */
/* 009 */ final class GeneratedIterator extends
org.apache.spark.sql.execution.BufferedRowIterator {
/* 010 */   private Object[] references;
/* 011 */   ...



"
Reynold Xin <rxin@databricks.com>,"Sun, 3 Apr 2016 22:28:41 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Liwei Lin <lwlin7@gmail.com>,"Since my original email, I've talked to a lot more users and looked at what
various environments support. It is true that a lot of enterprises, and
up until this date, users still can't install openjdk 8 on Ubuntu by
default. I see that as an indication that it is too early to drop Java 7.

Looking at the timeline, JDK release a major new version roughly every 3
years. We dropped Java 6 support one year ago, so from a timeline point of
view we would be very aggressive here if we were to drop Java 7 support in
Spark 2.0.

Note that not dropping Java 7 support now doesn't mean we have to support
Java 7 throughout Spark 2.x. We dropped Java 6 support in Spark 1.5, even
though Spark 1.0 started with Java 6.

In terms of testing, Josh has actually improved our test infra so now we
would run the Java 8 tests: https://github.com/apache/spark/pull/12073





"
Ted Yu <yuzhihong@gmail.com>,"Mon, 4 Apr 2016 03:15:54 -0700",Re: explain codegen,Reynold Xin <rxin@databricks.com>,"Could the error I encountered be due to missing import(s) of implicit ?

Thanks


.
,
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Mon, 4 Apr 2016 13:06:41 +0200",Re: explain codegen,Ted Yu <yuzhihong@gmail.com>,"No, it can''t. You only need implicits when you are using the catalyst DSL.

The error you get is due to the fact that the parser does not recognize the
CODEGEN keyword (which was the case before we introduced this in
https://github.com/apache/spark/commit/fa1af0aff7bde9bbf7bfa6a3ac74699734c2fd8a).
That suggests to me that you are not on the latest master.

Kind regards,

Herman van H√∂vell

2016-04-04 12:15 GMT+02:00 Ted Yu <yuzhihong@gmail.com>:

',
,
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Mon, 04 Apr 2016 21:49:33 +0900",Spark 1.6.1 binary pre-built for Hadoop 2.6 may be broken,dev <dev@spark.apache.org>,"Hi all,

I noticed the binary pre-build for Hadoop 2.6 which we can download from
spark.apache.org/downloads.html (Direct Download) may be broken.
I couldn't decompress at least following 4 tgzs with ""tar xfzv"" command
and md5-checksum did't match.

* spark-1.6.1-bin-hadoop2.6.tgz
* spark-1.6.1-bin-hadoop2.4.tgz
* spark-1.6.1-bin-hadoop2.3.tgz
* spark-1.6.1-bin-cdh4.tgz

Following 3 tgzs were decompressed successfully.

* spark-1.6.1-bin-hadoop1.tgz
* spark-1.6.1-bin-without-hadoop.tgz
* spark-1.6.1.tgz was decompressed successfully.

Regards,
Kousuke


---------------------------------------------------------------------


"
Mike Hynes <91mbbh@gmail.com>,"Mon, 4 Apr 2016 09:12:13 -0400",RDD Partitions not distributed evenly to executors,user@spark.apache.org,"[ CC'ing dev list since nearly identical questions have occurred in
user list recently w/o resolution;
c.f.:
http://apache-spark-user-list.1001560.n3.nabble.com/Spark-work-distribution-among-execs-tt26502.html
http://apache-spark-user-list.1001560.n3.nabble.com/Partitions-are-get-placed-on-the-single-node-tt26597.html
]

Hello,

In short, I'm reporting a problem concerning load imbalance of RDD
partitions across a standalone cluster. Though there are 16 cores
available per node, certain nodes will have >16 partitions, and some
will correspondingly have <16 (and even 0).

In more detail: I am running some scalability/performance tests for
vector-type operations. The RDDs I'm considering are simple block
vectors of type RDD[(Int,Vector)] for a Breeze vector type. The RDDs
are generated with a fixed number of elements given by some multiple
of the available cores, and subsequently hash-partitioned by their
integer block index.

I have verified that the hash partitioning key distribution, as well
as the keys themselves, are both correct; the problem is truly that
the partitions are *not* evenly distributed across the nodes.

For instance, here is a representative output for some stages and
tasks in an iterative program. This is a very simple test with 2
nodes, 64 partitions, 32 cores (16 per node), and 2 executors. Two
examples stages from the stderr log are stages 7 and 9:
7,mapPartitions at DummyVector.scala:113,64,1459771364404,1459771365272
9,mapPartitions at DummyVector.scala:113,64,1459771364431,1459771365639

When counting the location of the partitions on the compute nodes from
the stderr logs, however, you can clearly see the imbalance. Examples
lines are:
13627&INFO&TaskSetManager&Starting task 0.0 in stage 7.0 (TID 196,
himrod-2, partition 0,PROCESS_LOCAL, 3987 bytes)&
13628&INFO&TaskSetManager&Starting task 1.0 in stage 7.0 (TID 197,
himrod-2, partition 1,PROCESS_LOCAL, 3987 bytes)&
13629&INFO&TaskSetManager&Starting task 2.0 in stage 7.0 (TID 198,
himrod-2, partition 2,PROCESS_LOCAL, 3987 bytes)&

Grep'ing the full set of above lines for each hostname, himrod-?,
shows the problem occurs in each stage. Below is the output, where the
number of partitions stored on each node is given alongside its
hostname as in (himrod-?,num_partitions):
Stage 7: (himrod-1,0) (himrod-2,64)
Stage 9: (himrod-1,16) (himrod-2,48)
Stage 12: (himrod-1,0) (himrod-2,64)
Stage 14: (himrod-1,16) (himrod-2,48)
The imbalance is also visible when the executor ID is used to count
the partitions operated on by executors.

I am working off a fairly recent modification of 2.0.0-SNAPSHOT branch
(but the modifications do not touch the scheduler, and are irrelevant
for these particular tests). Has something changed radically in 1.6+
that would make a previously (<=1.5) correct configuration go haywire?
Have new configuration settings been added of which I'm unaware that
could lead to this problem?

Please let me know if others in the community have observed this, and
thank you for your time,
Mike

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 04 Apr 2016 13:58:23 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Michael Armbrust <michael@databricks.com>,"This is still an issue. The Spark 1.6.1 packages on S3 are corrupt.

Is anyone looking into this issue? Is there anything contributors can do to
help solve this problem?

Nick

m>

ase-build.sh>
nd
p2.6.tgz
p2.6.tgz
m getting
p2.4.tgz
p2.6.tgz
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 4 Apr 2016 07:07:37 -0700",Re: explain codegen,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"The commit you mentioned was made Friday.
I refreshed workspace Sunday - so it was included.

Maybe this was related:

$ bin/spark-shell
Failed to find Spark jars directory
(/home/hbase/spark/assembly/target/scala-2.10).
You need to build Spark before running this program.

Then I did:

$ ln -s /home/hbase/spark/assembly/target/scala-2.11
assembly/target/scala-2.10

Cheers


L.
c2fd8a).
)
:
P',
,
',
"
Michael Slavitch <slavitch@gmail.com>,"Mon, 4 Apr 2016 10:25:41 -0400",Re: RDD Partitions not distributed evenly to executors,Mike Hynes <91mbbh@gmail.com>,"Just to be sure:  Has spark-env.sh and spark-defaults.conf been correctly propagated to all nodes?  Are they identical?


http://apache-spark-user-list.1001560.n3.nabble.com/Spark-work-distribution-among-execs-tt26502.html
http://apache-spark-user-list.1001560.n3.nabble.com/Partitions-are-get-placed-on-the-single-node-tt26597.html
DummyVector.scala:113,64,1459771364404,1459771365272
DummyVector.scala:113,64,1459771364431,1459771365639
haywire?


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 4 Apr 2016 07:44:15 -0700",Re: RDD Partitions not distributed evenly to executors,Mike Hynes <91mbbh@gmail.com>,"bq. the modifications do not touch the scheduler

If the changes can be ported over to 1.6.1, do you mind reproducing the
issue there ?

I ask because master branch changes very fast. It would be good to narrow
the scope where the behavior you observed started showing.


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Tue, 5 Apr 2016 00:02:17 +0900",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,"Nicholas Chammas <nicholas.chammas@gmail.com>,
 Michael Armbrust <michael@databricks.com>","Oh, I overlooked that.  Thanks.

Kousuke


"
Jitendra Shelar <jitendra.shelar410@gmail.com>,"Mon, 4 Apr 2016 21:09:44 +0530",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,sarutak@oss.nttdata.co.jp,"We can think of using checksum for this kind of issues.


/
ease-build.sh>
.
e
n
oop2.6.tgz>
op2.6.tgz
oop2.6.tgz>
op2.6.tgz
ôm
oop2.4.tgz>
op2.4.tgz
oop2.6.tgz>
op2.6.tgz
e
"
Xuefeng Wu <benewu@gmail.com>,"Mon, 4 Apr 2016 15:41:16 +0000 (UTC)",Re: [discuss] ending support for Java 7 in Spark 2.0,"rxin@databricks.com, lwlin7@gmail.com","

Many open source projects are aggressive, such as Oracle JDK and Ubuntu, But they provide stable commercial supporting.


In other words, the enterprises doesn't drop JDK7, might aslo do not drop Spark 1.x to adopt Spark 2.x early version.















Since my original email, I've talked to a lot more users and looked at what various environments support. It is true that a lot of enterprises, and evt up until this date, users still can't install openjdk 8 on Ubuntu by default. I see that as an indication that it is too early to drop Java 7.

Looking at the timeline, JDK release a major new version roughly every 3 years. We dropped Java 6 support one year ago, so from a timeline point of view we would be very aggressive here if we were to drop Java 7 support in Spark 2.0.
Note that not dropping Java 7 support now doesn't mean we have to support Java 7 throughout Spark 2.x. We dropped Java 6 support in Spark 1.5, even though Spark 1.0 started with Java 6.
In terms of testing, Josh has actually improved our test infra so now we would run the Java 8 tests:¬†https://github.com/apache/spark/pull/12073





Arguments are really convincing; new Dataset API as well as performance


improvements is exiting, so I'm personally +1 on moving onto¬†Java8.

¬†

However, I'm afraid Tencent is one of ""the organizations stuck with¬†Java7""

-- our IT Infra division wouldn't upgrade to Java7 until Java8 is out, and

wouldn't upgrade to Java8 until Java9 is out.




So:¬†

(non-binding) +1 on dropping scala 2.10 support

(non-binding) ¬†-1 on dropping Java 7 support

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† * as long as we figure out a practical way to run Spark with

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† JDK8 on JDK7 clusters, this -1 would then definitely be +1




Thanks !
i think that logic is reasonable, but then the same should also apply to scala 2.10, which is also unmaintained/unsupported at this point (basically has been since march 2015 except for one hotfix due to a license incompatibility)

who wants to support scala 2.10 three years after they did the last maintenance release?


te:
Removing compatibility (with jdk, etc) can be done with a major release- given that 7 has been EOLed a while back and is now unsupported, we have to decide if we drop support for it in 2.0 or 3.0 (2+ years from now).
Given the functionality &¬†performance benefits of going to jdk8, future enhancements relevant¬†in 2.x timeframe ( scala, dependencies) which requires it, and¬†simplicity wrt code, test &¬†support it looks like a good checkpoint to drop jdk7 support.
As already mentioned in the thread, existing yarn clusters are unaffected if they want to continue running jdk7 and yet use spark2¬†(install jdk8 on all nodes and use it via JAVA_HOME, or worst case distribute jdk8¬†as archive - suboptimal).I am unsure about mesos (standalone might be easier upgrade I guess ?).

Proposal is for¬†1.6x line to¬†continue to be supported with critical fixes;¬†newer features will require 2.x and so jdk8
Regards¬†Mridul¬†


ad





I understand both sides. But if you look at what I've been asking

since the beginning, it's all about the cost and benefits of dropping

support for java 1.7.



The biggest argument in your original e-mail is about testing. And the

testing cost is much bigger for supporting scala 2.10 than it is for

supporting java 1.7. If you read one of my earlier replies, it should

be even possible to just do everything in a single job - compile for

java 7 and still be able to test things in 1.8, including lambdas,

which seems to be the main thing you were worried about.





te:


:























--

Marcelo



---------------------------------------------------------------------

















"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Tue, 5 Apr 2016 00:42:08 +0900",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Jitendra Shelar <jitendra.shelar410@gmail.com>,"Thanks.
Of course, I verified checksum and it didn't matched.

Kousuke


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 4 Apr 2016 08:45:38 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Maybe temporarily take out the artifacts on S3 before the root cause is
found.


e-build.sh>
t
?
.6.tgz
.6.tgz
getting
.4.tgz
.6.tgz
"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 4 Apr 2016 08:48:38 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"Reynold,

Considering the performance improvements you mentioned in your original
e-mail and also considering that few other big data projects have already
or are in progress of abandoning JDK 7, I think it would benefit Spark if
we go with JDK 8.0 only.

Are there users that will be less aggressive ? Yes, but those would most
likely be in more stable releases like 1.6.x.




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Koert Kuipers <koert@tresata.com>,"Mon, 4 Apr 2016 12:24:51 -0400",Re: RDD Partitions not distributed evenly to executors,Mike Hynes <91mbbh@gmail.com>,"we ran into similar issues and it seems related to the new memory
management. can you try:
spark.memory.useLegacyMode = true


"
Reynold Xin <rxin@databricks.com>,"Mon, 4 Apr 2016 09:37:19 -0700",Re: explain codegen,Ted Yu <yuzhihong@gmail.com>,"Why don't you wipe everything out and try again?


4c2fd8a).
])
):
AP',
',
E',
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 4 Apr 2016 09:59:53 -0700",Re: explain codegen,Reynold Xin <rxin@databricks.com>,"Thanks to all who have responded.

It turned out that the following command line for maven caused the error (I
forgot to include this in first email):
eclipse:eclipse



34c2fd8a).
?
0])
MAP',
H',
ZE',
"
Michael Armbrust <michael@databricks.com>,"Mon, 4 Apr 2016 11:05:47 -0700","Re: [SQL] Dataset.map gives error: missing parameter type for
 expanded function?",Jacek Laskowski <jacek@japila.pl>,"It is called groupByKey now.  Similar to joinWith, the schema produced by
relational joins and aggregations is different than what you would expect
when working with objects.  So, when combining DataFrame+Dataset we renamed
these functions to make this distinction clearer.


"
Joseph Bradley <joseph@databricks.com>,"Mon, 4 Apr 2016 11:33:12 -0700",Re: running lda in spark throws exception,Li Li <fancyerii@gmail.com>,"It's possible this was caused by incorrect Graph creation, fixed in
[SPARK-13355].

Could you retry your dataset using the current master to see if the problem
is fixed?  Thanks!


"
Ofir Manor <ofir.manor@equalum.io>,"Mon, 4 Apr 2016 22:58:52 +0300",Re: [discuss] ending support for Java 7 in Spark 2.0,"Luciano Resende <luckbr1975@gmail.com>, Reynold Xin <rxin@databricks.com>","I think that a backup plan could be to announce that JDK7 is deprecated in
Spark 2.0 and support for it will be fully removed in Spark 2.1. This gives
admins enough warning to install JDK8 along side their ""main"" JDK (or fully
migrate to it), while allowing the project to merge JDK8-specific changes
to trunk right after the 2.0 release.

However, I personally think it is better to drop JDK7 now. I'm sure that
both the community and the distributors (Databricks, Cloudera, Hortonworks,
MapR, IBM etc) will all rush to help their customers migrate their
environment to support Spark 2.0, so I think any backlash won't be dramatic
or lasting.

Just my two cents,

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 04 Apr 2016 21:39:53 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Ted Yu <yuzhihong@gmail.com>,"An additional note: The Spark packages being served off of CloudFront (i.e.
the ‚Äúdirect download‚Äù option on spark.apache.org) are also corrupt.

Btw what‚Äôs the correct way to verify the SHA of a Spark package? I‚Äôve tried
a few commands on working packages downloaded from Apache mirrors, but I
can‚Äôt seem to reproduce the published SHA for spark-1.6.1-bin-hadoop2.6.tgz
<http://www.apache.org/dist/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz.sha>
.
‚Äã


se-build.sh>
:
e
d
s?
2.6.tgz
2.6.tgz
 getting
2.4.tgz
2.6.tgz
"
Karlis Zigurs <homolupus@gmail.com>,"Tue, 5 Apr 2016 01:01:20 +0300",Re: [discuss] ending support for Java 7 in Spark 2.0,Ofir Manor <ofir.manor@equalum.io>,"Curveball: Is there a need to use lambdas quite yet?


---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Mon, 4 Apr 2016 15:19:48 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"The published hash is a SHA512.

You can verify the integrity of the packages by running `sha512sum` on
the archive and comparing the computed hash with the published one.
Unfortunately however, I don't know what tool is used to generate the
hash and I can't reproduce the format, so I ended up manually
comparing the hashes.

e.
o corrupt.
I‚Äôve tried
oop2.6.tgz.
e:
me
nd
-hadoop2.6.tgz
-hadoop2.6.tgz
m getting
in-hadoop2.4.tgz
-bin-hadoop2.6.tgz

---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Mon, 4 Apr 2016 15:28:09 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I just found out how the hash is calculated:

gpg --print-md sha512 <spark-archive>.tgz

you can use that to check if the resulting output matches the contents
of <spark-archive>.tgz.sha

.e.
so corrupt.
 I‚Äôve tried
doop2.6.tgz.
/
.
te:
e
ame
ond
n
n-hadoop2.6.tgz
n-hadoop2.6.tgz
ôm getting
bin-hadoop2.4.tgz
1-bin-hadoop2.6.tgz
e

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 04 Apr 2016 22:37:24 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Jakob Odersky <jakob@odersky.com>,"Thanks, that was the command. :thumbsup:


also corrupt.
e? I‚Äôve
 I
is
.tgz
.tgz
.
ôm
.tgz
m
.tgz
is
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 4 Apr 2016 20:16:39 -0400","error: reference to sql is ambiguous after import org.apache.spark._
 in shell?",dev <dev@spark.apache.org>,"Hi Spark devs,

I'm unsure if what I'm seeing is correct. I'd appreciate any input
to...rest my nerves :-) I did `import org.apache.spark._` by mistake,
but since it's valid, I'm wondering why does Spark shell imports sql
at all since it's available after the import?!

(it's today's build)

scala> sql(""SELECT * FROM dafa"").show(false)
<console>:30: error: reference to sql is ambiguous;
it is imported twice in the same scope by
import org.apache.spark._
and import sqlContext.sql
       sql(""SELECT * FROM dafa"").show(false)
       ^

scala> :imports
 1) import sqlContext.implicits._  (52 terms, 31 are implicit)
 2) import sqlContext.sql          (1 terms)

scala> sc.version
res19: String = 2.0.0-SNAPSHOT

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Mike Hynes <91mbbh@gmail.com>,"Mon, 4 Apr 2016 20:17:17 -0400",Re: RDD Partitions not distributed evenly to executors,"Koert Kuipers <koert@tresata.com>, Michael Slavitch <slavitch@gmail.com>, Ted Yu <yuzhihong@gmail.com>","Dear all,

Thank you for your responses.

Michael Slavitch:
Yes; these files are stored on a shared memory directory accessible to
all nodes.

Koert Kuipers:
I reran the exact same code with a restarted cluster using this
modification, and did not observe any difference. The partitioning is
still imbalanced.

Ted Yu:
Since the spark.memory.useLegacyMode setting did not impact my code
execution, I will have to change the Spark dependency back to earlier
versions to see if the issue persists and get back to you.

Meanwhile, if anyone else has any other ideas or experience, please let me know.

Mike



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 4 Apr 2016 17:20:04 -0700","Re: error: reference to sql is ambiguous after import
 org.apache.spark._ in shell?",Jacek Laskowski <jacek@japila.pl>,"Looks like the import comes from
repl/scala-2.11/src/main/scala/org/apache/spark/repl/SparkILoop.scala :

      processLine(""import sqlContext.sql"")


"
Reynold Xin <rxin@databricks.com>,"Mon, 4 Apr 2016 18:16:05 -0700",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,james <yiazhou@gmail.com>,"Nezih,

Have you had a chance to figure out why this is happening?



"
Reynold Xin <rxin@databricks.com>,"Mon, 4 Apr 2016 18:16:27 -0700",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,james <yiazhou@gmail.com>,"BTW do you still see this when dynamic allocation is off?


"
Koert Kuipers <koert@tresata.com>,"Mon, 4 Apr 2016 22:57:01 -0400",Re: RDD Partitions not distributed evenly to executors,Mike Hynes <91mbbh@gmail.com>,"can you try:
spark.shuffle.reduceLocality.enabled=false


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 4 Apr 2016 20:00:42 -0700",Build changes after SPARK-13579,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

We merged  SPARK-13579 today, and if you're like me and have your
hands automatically type ""sbt assembly"" anytime you're building Spark,
that won't work anymore.

You should now use ""sbt package""; you'll still need ""sbt assembly"" if
you require one of the remaining assemblies (streaming connectors,
yarn shuffle service).


-- 
Marcelo

---------------------------------------------------------------------


"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Tue, 05 Apr 2016 03:52:03 +0000",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,"Reynold Xin <rxin@databricks.com>, james <yiazhou@gmail.com>","Nope, I didn't have a chance to track the root cause, and IIRC we didn't
observe it when dyn. alloc. is off.


"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Apr 2016 00:58:19 -0400",Re: Build changes after SPARK-13579,Marcelo Vanzin <vanzin@cloudera.com>,"do i need to run sbt package before doing tests?


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 4 Apr 2016 21:59:58 -0700",Re: Build changes after SPARK-13579,Koert Kuipers <koert@tresata.com>,"No, tests (except pyspark) should work without having to package anything first.




-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 4 Apr 2016 22:01:01 -0700",Re: Build changes after SPARK-13579,Marcelo Vanzin <vanzin@cloudera.com>,"pyspark and R


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 5 Apr 2016 10:35:28 +0000",Re: [discuss] ending support for Java 7 in Spark 2.0,Xuefeng Wu <benewu@gmail.com>,"


Many open source projects are aggressive, such as Oracle JDK and Ubuntu, But they provide stable commercial supporting.


supporting old versions of jdk is one of the key revenue streams for oracle's sun group: there's a lot of webapps out there which need a secure/stable JDK version, and whose owners don't want to spend time & money doing the upgrade.


In other words, the enterprises doesn't drop JDK7, might aslo do not drop Spark 1.x to adopt Spark 2.x early version.



probably true, except for the complication that in a large multitenant cluster, you need to get everyone who runs code in the cluster and the ops team happy with the plan
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 5 Apr 2016 10:41:02 +0000",Re: [discuss] ending support for Java 7 in Spark 2.0,Ofir Manor <ofir.manor@equalum.io>,"
> On 4 Apr 2016, at 20:58, Ofir Manor <ofir.manor@equalum.io> wrote:
> 
> I think that a backup plan could be to announce that JDK7 is deprecated in Spark 2.0 and support for it will be fully removed in Spark 2.1. This gives admins enough warning to install JDK8 along side their ""main"" JDK (or fully migrate to it), while allowing the project to merge JDK8-specific changes to trunk right after the 2.0 release.
> 

Announcing a plan is good; anything which can be done to help mixed JVM deployment (documentation, testing) would be useful too

> However, I personally think it is better to drop JDK7 now. I'm sure that both the community and the distributors (Databricks, Cloudera, Hortonworks, MapR, IBM etc) will all rush to help their customers migrate their environment to support Spark 2.0, so I think any backlash won't be dramatic or lasting. 
> 

People using Spark tend to be pretty aggressive about wanting the latest version, at least on the 1.x line; so far there've been no major problems allowing mixed spark version deployments, provided shared bits of infrastructure (spark history server) were recent. Hive metadata repository access is the other big issue: moving spark up to hive  1.2.1 addresses that for the moment.

I don't know about organisations adoption of JDK8 vs 7; or how anyone would react to having to move to java 8 for spark 2. Maybe it'll be a barrier to adoption ‚Äîmaybe it'll be an incentive to upgrade.

Oh, I do know that Java 9 is going to be trouble. Different topic."
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Tue, 5 Apr 2016 10:44:28 +0000",Build with Thrift Server & Scala 2.11,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is anyone looking into this one, Build with Thrift Server & Scala 2.11?
I9f so when can we expect it

Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>
"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Apr 2016 05:57:14 -0700",Re: Build with Thrift Server & Scala 2.11,Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"What do you mean? The Jenkins build for Spark uses 2.11 and also builds the
thrift server.


"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Tue, 5 Apr 2016 13:22:45 +0000",RE: Build with Thrift Server & Scala 2.11,Reynold Xin <rxin@databricks.com>,"I can see that the build is successful
(-Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver ‚ÄìDscala-2.11 -DskipTests clean package)

the documents page it still says that
‚Äú
Building With Hive and JDBC Support
To enable Hive integration for Spark SQL along with its JDBC server and CLI, add the -Phive and Phive-thriftserver profiles to your existing build options. By default Spark will build with Hive 0.13.1 bindings.

# Apache Hadoop 2.4.X with Hive 13 support
mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package
Building for Scala 2.11
To produce a Spark package compiled with Scala 2.11, use the -Dscala-2.11 property:

./dev/change-scala-version.sh 2.11
mvn -Pyarn -Phadoop-2.4 -Dscala-2.11 -DskipTests clean package
Spark does not yet support its JDBC component for Scala 2.11.
‚Äù
Source : http://spark.apache.org/docs/latest/building-spark.html

When I try to start the thrift server I get the following error:
‚Äú
16/04/05 16:09:11 INFO BlockManagerMaster: Registered BlockManager
16/04/05 16:09:12 ERROR SparkContext: Error initializing SparkContext.
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
                at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374)
                at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:312)
                at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:178)
                at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:665)
                at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:601)
                at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)
                at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)
                at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
                at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
                at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
                at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
                at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1667)
                at org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:67)
                at org.apache.spark.SparkContext.<init>(SparkContext.scala:517)
                at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:57)
                at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:77)
                at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:498)
                at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:726)
                at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:183)
                at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
                at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
                at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.UnknownHostException: namenode
                ... 26 more
16/04/05 16:09:12 INFO SparkUI: Stopped Spark web UI at http://10.10.182.195:4040
16/04/05 16:09:12 INFO SparkDeploySchedulerBackend: Shutting down all executors
‚Äù



Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Tuesday, April 05, 2016 3:57 PM
To: Raymond Honderdors <Raymond.Honderdors@sizmek.com>
Cc: dev@spark.apache.org
Subject: Re: Build with Thrift Server & Scala 2.11

What do you mean? The Jenkins build for Spark uses 2.11 and also builds the thrift server.

On Tuesday, April 5, 2016, Raymond Honderdors <Raymond.Honderdors@sizmek.com<mailto:Raymond.Honderdors@sizmek.com>> wrote:
Is anyone looking into this one, Build with Thrift Server & Scala 2.11?
I9f so when can we expect it

Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<javascript:_e(%7B%7D,'cvml','raymond.honderdors@sizmek.com');>
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>

---------------------------------------------------------------------"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Tue, 5 Apr 2016 13:27:51 +0000",RE: Build with Thrift Server & Scala 2.11,"Raymond Honderdors <Raymond.Honderdors@sizmek.com>, Reynold Xin
	<rxin@databricks.com>","Here is the error after build with scala 2.10
‚Äú
Spark Command: /usr/lib/jvm/java-1.8.0/bin/java -cp /home/raymond.honderdors/Documents/IdeaProjects/spark/conf/:/home/raymond.honderdors/Documents/IdeaProjects/spark/assembly/target/scala-2.10/jars/* -Xms5g -Xmx5g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal
========================================
Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Implementing class
                at java.lang.ClassLoader.defineClass1(Native Method)
                at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
                at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
                at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
                at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
                at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
                at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
                at java.security.AccessController.doPrivileged(Native Method)
                at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
                at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
                at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
                at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
                at java.lang.Class.getDeclaredMethods0(Native Method)
                at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
                at java.lang.Class.privateGetMethodRecursive(Class.java:3048)
                at java.lang.Class.getMethod0(Class.java:3018)
                at java.lang.Class.getMethod(Class.java:1784)
                at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:710)
                at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:183)
                at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
                at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
                at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
‚Äù



Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<maizliya

From: Raymond Honderdors [mailto:Raymond.Honderdors@sizmek.com]
Sent: Tuesday, April 05, 2016 4:23 PM
To: Reynold Xin <rxin@databricks.com>
Cc: dev@spark.apache.org
Subject: RE: Build with Thrift Server & Scala 2.11

I can see that the build is successful
(-Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver ‚ÄìDscala-2.11 -DskipTests clean package)

the documents page it still says that
‚Äú
Building With Hive and JDBC Support
To enable Hive integration for Spark SQL along with its JDBC server and CLI, add the -Phive and Phive-thriftserver profiles to your existing build options. By default Spark will build with Hive 0.13.1 bindings.

# Apache Hadoop 2.4.X with Hive 13 support
mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package
Building for Scala 2.11
To produce a Spark package compiled with Scala 2.11, use the -Dscala-2.11 property:

./dev/change-scala-version.sh 2.11
mvn -Pyarn -Phadoop-2.4 -Dscala-2.11 -DskipTests clean package
Spark does not yet support its JDBC component for Scala 2.11.
‚Äù
Source : http://spark.apache.org/docs/latest/building-spark.html

When I try to start the thrift server I get the following error:
‚Äú
16/04/05 16:09:11 INFO BlockManagerMaster: Registered BlockManager
16/04/05 16:09:12 ERROR SparkContext: Error initializing SparkContext.
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
                at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374)
                at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:312)
                at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:178)
                at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:665)
                at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:601)
                at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)
                at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)
                at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
                at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
                at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
                at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
                at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1667)
                at org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:67)
                at org.apache.spark.SparkContext.<init>(SparkContext.scala:517)
                at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:57)
                at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:77)
                at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:498)
                at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:726)
                at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:183)
                at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
                at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
                at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.UnknownHostException: namenode
                ... 26 more
16/04/05 16:09:12 INFO SparkUI: Stopped Spark web UI at http://10.10.182.195:4040
16/04/05 16:09:12 INFO SparkDeploySchedulerBackend: Shutting down all executors
‚Äù



Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
Tin@databricks.com]
Sent: Tuesday, April 05, 2016 3:57 PM
To: Raymond Honderdors <Raymond.Honderdors@sizmek.com<mailto:Raymond.Honderdors@sizmek.com>>
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Build with Thrift Server & Scala 2.11

What do you mean? The Jenkins build for Spark uses 2.11 and also builds the thrift server.

On Tuesday, April 5, 2016, Raymond Honderdors <Raymond.Honderdors@sizmek.com<mailto:Raymond.Honderdors@sizmek.com>> wrote:
Is anyone looking into this one, Build with Thrift Server & Scala 2.11?
I9f so when can we expect it

Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<javascript:_e(%7B%7D,'cvml','raymond.honderdors@sizmek.com');>
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>
"
Sean Owen <sowen@cloudera.com>,"Tue, 5 Apr 2016 14:31:29 +0100",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"Following https://github.com/apache/spark/pull/12165#issuecomment-205791222
I'd like to make a point about process and then answer points below.

We have this funny system where anyone can propose a change, and any
of a few people can veto a change unilaterally. The latter rarely
comes up. 9 changes out of 10 nobody disagrees on; sometimes a
committer will say 'no' to a change and nobody else with that bit
disagrees.

Sometimes it matters and here I see, what, 4 out of 5 people including
committers supporting a particular change. A veto to oppose that is
pretty drastic. It's not something to use because you or customers
prefer a certain outcome. This reads like you're informing people
you've changed your mind and that's the decision, when it can't work
that way. I saw this happen to a lesser extent in the thread about
Scala 2.10.

It doesn't mean majority rules here either, but can I suggest you
instead counter-propose an outcome that the people here voting in
favor of what you're vetoing would probably also buy into? I bet
everyone's willing to give wide accommodation to your concerns. It's
probably not hard, like: let's plan to not support Java 7 in Spark
2.1.0. (Then we can debate the logic of that.)


I have Java 8 on my Ubuntu instance, and installed it directly via apt-get.
http://openjdk.java.net/install/



The metric is really (IMHO) when the JDK goes EOL. Java 6 was EOL in
Feb 2013, so supporting it into Spark 1.x was probably too long. Java
7 was EOL in April 2015. It's not really somehow every ~3 years.



Whatever arguments one has about preventing people from updating to
the latest and greatest then apply to a *minor* release, which is
worse. Java 6 support was probably overdue for removal at 1.0;
better-late-than-never, not necessarily the right time to do it.



Excellent, but, orthogonal.

Even if I personally don't see the merit in these arguments compared
to the counter-arguments, retaining Java 7 support now wouldn't be a
terrible outcome. I'd like to see better process and a more reasonable
compromise result though.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Apr 2016 07:06:21 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Sean Owen <sowen@cloudera.com>,"Hi Sean,

See http://www.oracle.com/technetwork/java/eol-135779.html

Java 7 hasn't EOLed yet. If you look at support you can get from Oracle,
it's actually goes to 2019. And you can even get more support after that.

Spark has always maintained great backward compatibility with other
systems, way beyond what vendors typically support. For example, we
supported Hadoop 1.x all the way until Spark 1.6 (basically the last
release), while all the vendors have dropped support for them already.

Putting my Databricks hat on we actually only support Java 8, but I think
it would be great to still support Java 7 in the upstream release for some
larger deployments. I like the idea of deprecating or at least strongly
encouraging people to update.


"
Steve Johnston <sjohnston@algebraixdata.com>,"Tue, 5 Apr 2016 07:15:11 -0700 (MST)",Re: What influences the space complexity of Spark operations?,dev@spark.apache.org,"Submitted: SPARK-14389 - OOM during BroadcastNestedLoopJoin.



--

---------------------------------------------------------------------


"
Khaled Ammar <khaled.ammar@gmail.com>,"Tue, 5 Apr 2016 12:06:40 -0400",Re: RDD Partitions not distributed evenly to executors,Koert Kuipers <koert@tresata.com>,"I have a similar experience.

Using 32 machines, I can see than number of tasks (partitions) assigned to
executors (machines) is not even. Moreover, the distribution change every
stage (iteration).

I wonder why Spark needs to move partitions around any way, should not the
scheduler reduce network (and other IO) overhead by reducing such
relocation.

Thanks,
-Khaled







-- 
Thanks,
-Khaled
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 5 Apr 2016 09:22:17 -0700",Re: Build with Thrift Server & Scala 2.11,Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Raymond:

Did ""namenode"" appear in any of the Spark config files ?

BTW Scala 2.11 is used by the default build.


d
er
de
va:374)
ava:312)
78)
stem.java:148)
0)
er.scala:67)
a:57)
Server2.scala:77)
erver2.scala)
:62)
mpl.java:43)
$runMain(SparkSubmit.scala:726)
"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Tue, 5 Apr 2016 16:25:01 +0000",Re: Build with Thrift Server & Scala 2.11,"""yuzhihong@gmail.com"" <yuzhihong@gmail.com>","I did a check of that i could not find that in any of the config files

I also used config files that work with 1.6.1

Sent from Outlook Mobile<https://aka.ms/blhgte>




Raymond:

Did ""namenode"" appear in any of the Spark config files ?

BTW Scala 2.11 is used by the default build.

I can see that the build is successful
(-Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -Dscala-2.11 -DskipTests clean package)

the documents page it still says that
""
Building With Hive and JDBC Support
To enable Hive integration for Spark SQL along with its JDBC server and CLI, add the -Phive and Phive-thriftserver profiles to your existing build options. By default Spark will build with Hive 0.13.1 bindings.

# Apache Hadoop 2.4.X with Hive 13 support
mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package
Building for Scala 2.11
To produce a Spark package compiled with Scala 2.11, use the -Dscala-2.11 property:

./dev/change-scala-version.sh 2.11
mvn -Pyarn -Phadoop-2.4 -Dscala-2.11 -DskipTests clean package
Spark does not yet support its JDBC component for Scala 2.11.
""
Source : http://spark.apache.org/docs/latest/building-spark.html

When I try to start the thrift server I get the following error:
""
16/04/05 16:09:11 INFO BlockManagerMaster: Registered BlockManager
16/04/05 16:09:12 ERROR SparkContext: Error initializing SparkContext.
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
                at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374)
                at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:312)
                at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:178)
                at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:665)
                at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:601)
                at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)
                at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)
                at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
                at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
                at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
                at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
                at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1667)
                at org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:67)
                at org.apache.spark.SparkContext.<init>(SparkContext.scala:517)
                at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:57)
                at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:77)
                at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:498)
                at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:726)
                at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:183)
                at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
                at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
                at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.UnknownHostException: namenode
                ... 26 more
16/04/05 16:09:12 INFO SparkUI: Stopped Spark web UI at http://10.10.182.195:4040
16/04/05 16:09:12 INFO SparkDeploySchedulerBackend: Shutting down all executors
""



Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya

From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
Sent: Tuesday, April 05, 2016 3:57 PM
To: Raymond Honderdors <Raymond.Honderdors@sizmek.com<mailto:Raymond.Honderdors@sizmek.com>>
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Build with Thrift Server & Scala 2.11

What do you mean? The Jenkins build for Spark uses 2.11 and also builds the thrift server.

Is anyone looking into this one, Build with Thrift Server & Scala 2.11?
I9f so when can we expect it

Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>


---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>

"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 05 Apr 2016 18:01:16 +0000",Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,"dev <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi all,

More than a year ago, in Spark 1.2 we introduced the ML pipeline API built
on top of Spark SQL‚Äôs DataFrames. Since then the new DataFrame-based API
has been developed under the spark.ml package, while the old RDD-based API
has been developed in parallel under the spark.mllib package. While it was
easier to implement and experiment with new APIs under a new package, it
became harder and harder to maintain as both packages grew bigger and
bigger. And new users are often confused by having two sets of APIs with
overlapped functions.

We started to recommend the DataFrame-based API over the RDD-based API in
Spark 1.5 for its versatility and flexibility, and we saw the development
and the usage gradually shifting to the DataFrame-based API. Just counting
the lines of Scala code, from 1.5 to the current master we added ~10000
lines to the DataFrame-based API while ~700 to the RDD-based API. So, to
gather more resources on the development of the DataFrame-based API and to
help users migrate over sooner, I want to propose switching RDD-based MLlib
APIs to maintenance mode in Spark 2.0. What does it mean exactly?

* We do not accept new features in the RDD-based spark.mllib package,
unless they block implementing new features in the DataFrame-based spark.ml
package.
* We still accept bug fixes in the RDD-based API.
* We will add more features to the DataFrame-based API in the 2.x series to
reach feature parity with the RDD-based API.
the RDD-based API.
* We will remove the RDD-based API from the main Spark repo in Spark 3.0.

Though the RDD-based API is already in de facto maintenance mode, this
announcement will make it clear and hence important to both MLlib
developers and users. So we‚Äôd greatly appreciate your feedback!

(As a side note, people sometimes use ‚ÄúSpark ML‚Äù to refer to the
DataFrame-based API or even the entire MLlib component. This also causes
confusion. To be clear, ‚ÄúSpark ML‚Äù is not an official name and there are no
plans to rename MLlib to ‚ÄúSpark ML‚Äù at this time.)

Best,
Xiangrui
"
Renyi Xiong <renyixiong0@gmail.com>,"Tue, 5 Apr 2016 11:02:05 -0700",Spark Streaming UI reporting a different task duration,Tathagata Das <tdas@databricks.com>,"Hi TD,

We noticed that Spark Streaming UI is reporting a different task duration
from time to time.

e.g. here's the standard output of the application which reports the
duration of the longest task is about 3.3 minutes:

16/04/01 16:07:19 INFO TaskSetManager: Finished task 1077.0 in stage 0.0
(TID 1077) in 125425 ms on CH1SCH080051460 (1562/1563)

16/04/01 16:08:30 INFO TaskSetManager: Finished task 926.0 in stage 0.0
(TID 926) in 196776 ms on CH1SCH080100841 (1563/1563)


but on spark streaming UI it's about 2.3 minutes




*Summary Metrics for 1563 Completed Tasks*


*Metric*

*Min*

*25th percentile*

*Median*

*75th percentile*

*Max*

Duration

12 s

21 s

24 s

29 s

2.3 min

GC Time

0 ms

0 ms

0 ms

0 ms

62 ms

Shuffle Write Size / Records

0.0 B / 0

532.9 KB / 131

1198.9 KB / 299

3.8 MB / 1161

9.0 MB / 2865

I wonder if you have any quick idea about where the missing 1 minute could
be?


thanks a lot,

Renyi.
"
Sean Owen <sowen@cloudera.com>,"Tue, 5 Apr 2016 19:48:22 +0100",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Xiangrui Meng <mengxr@gmail.com>,"FWIW, all of that sounds like a good plan to me. Developing one API is
certainly better than two.

t
sed API has
s
g
o
ib
ess
to
ers
 to the
e and there are no

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 5 Apr 2016 12:07:46 -0700",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Sean Owen <sowen@cloudera.com>,"addition to the ML algorithms we have a number of linear algebra
(various distributed matrices) and statistical methods in the
spark.mllib package. Is the plan to port or move these to the spark.ml
namespace in the 2.x series ?

Thanks
Shivaram

lt
ased API has
as
n
t
ng
to
lib
less
 to
e
.
pers
r to the
me and there are no

---------------------------------------------------------------------


"
Xiangrui Meng <meng@databricks.com>,"Tue, 05 Apr 2016 20:44:35 +0000",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,"shivaram@eecs.berkeley.edu, Sean Owen <sowen@cloudera.com>","Yes, DB (cc'ed) is working on porting the local linear algebra library over
(SPARK-13944). There are also frequent pattern mining algorithms we need to
port over in order to reach feature parity. -Xiangrui


-based
it
th
0
to
d
fer to the
es
name and there
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 05 Apr 2016 21:14:00 +0000",Updating Spark PR builder and 2.x test jobs to use Java 8 JDK,Dev <dev@spark.apache.org>,"In order to be able to run Java 8 API compatibility tests, I'm going to
push a new set of Jenkins configurations for Spark's test and PR builders
so that those jobs use a Java 8 JDK. I tried this once in the past and it
seemed to introduce some rare, transient flakiness in certain tests, so if
anyone observes new test failures please email me and I'll investigate
right away.

Note that this change has no impact on Spark's supported JDK versions and
our build will still target Java 7 and emit Java 7 bytecode; the purpose of
this change is simply to allow the Java 8 lambda tests to be run as part of
PR builder runs.

- Josh
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Apr 2016 18:40:22 -0400","[STREAMING] DStreamClosureSuite.scala with { return;
 ssc.sparkContext.emptyRDD[Int] } Why?!",dev <dev@spark.apache.org>,"Hi,

In https://github.com/apache/spark/blob/master/streaming/src/test/scala/org/apache/spark/streaming/DStreamClosureSuite.scala#L190:

{ return; ssc.sparkContext.emptyRDD[Int] }

What is this return inside for? I don't understand the line and am
about to propose a change to remove it.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 5 Apr 2016 15:47:15 -0700","Re: [STREAMING] DStreamClosureSuite.scala with { return;
 ssc.sparkContext.emptyRDD[Int] } Why?!",Jacek Laskowski <jacek@japila.pl>,"The next line should give some clue:
    expectCorrectException { ssc.transform(Seq(ds), transformF) }

Closure shouldn't include return.


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 5 Apr 2016 16:14:29 -0700",Re: Updating Spark PR builder and 2.x test jobs to use Java 8 JDK,Josh Rosen <joshrosen@databricks.com>,"Josh:
You may have noticed the following error (
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/566/console
):

[error] javac: invalid source release: 1.8
[error] Usage: javac <options> <source files>
[error] use -help for a list of possible options



"
Holden Karau <holden@pigscanfly.ca>,"Tue, 5 Apr 2016 16:39:59 -0700",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"deprecation warnings in our builds that we can't fix without introducing a
wrapper/ scala version specific code. This isn't a big deal, and if we drop
2.10 in the 3-6 month time frame talked about we can cleanup those warnings
once we get there.




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Apr 2016 19:48:13 -0400","Re: [STREAMING] DStreamClosureSuite.scala with { return;
 ssc.sparkContext.emptyRDD[Int] } Why?!",Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

Yeah, I saw the line, but forgot it's a test that may have been
testing that closures should not have return. More clear now. Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Apr 2016 19:51:02 -0400",BROKEN BUILD? Is this only me or not?,dev <dev@spark.apache.org>,"Hi,

Just checked out the latest sources and got this...

/Users/jacek/dev/oss/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala:626:
error: annotation argument needs to be a constant; found: ""_FUNC_(str)
- "".+(""Returns str, with the first letter of each word in uppercase,
all other letters in "").+(""lowercase. Words are delimited by white
space."")
    ""Returns str, with the first letter of each word in uppercase, all
other letters in "" +

                   ^

It's in https://github.com/apache/spark/commit/c59abad052b7beec4ef550049413e95578e545be.

Is this a real issue with the build now or is this just me? I may have
seen a similar case before, but can't remember what the fix was.
Looking into it.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 5 Apr 2016 19:58:37 -0400",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Xiangrui Meng <meng@databricks.com>,"This sounds good to me as well. The one thing we should pay attention to is how we update the docs so that people know to start with the spark.ml classes. Right now the docs list spark.mllib first and also seem more comprehensive in that area than in spark.ml, so maybe people naturally move towards that.

Matei

over (SPARK-13944). There are also frequent pattern mining algorithms we need to port over in order to reach feature parity. -Xiangrui
<http://spark.ml/>
is
API built
DataFrame-based API has
the old RDD-based API has
was
package, it
and
with
API in
development
counting
~10000
So, to
and to
RDD-based MLlib
package, unless
spark.ml <http://spark.ml/>
series to
deprecate
Spark 3.0.
this
developers
refer to the
causes
official name and there are no
---------------------------------------------------------------------
<mailto:user-unsubscribe@spark.apache.org>
<mailto:user-help@spark.apache.org>

"
Josh Rosen <joshrosen@databricks.com>,"Wed, 06 Apr 2016 00:09:37 +0000",Re: Updating Spark PR builder and 2.x test jobs to use Java 8 JDK,Ted Yu <yuzhihong@gmail.com>,"I've reverted the bulk of the conf changes while I investigate. I think
that Zinc might be handling JAVA_HOME in a weird way and am SSH'ing to
Jenkins to try to reproduce the problem in isolation.


"
Joseph Bradley <joseph@databricks.com>,"Tue, 5 Apr 2016 17:32:44 -0700",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Matei Zaharia <matei.zaharia@gmail.com>,"+1  By the way, the JIRA for tracking (Scala) API parity is:
https://issues.apache.org/jira/browse/SPARK-4591


:
e-based
s
I
00
s
efer to the
 name and there
"
Holden Karau <holden@pigscanfly.ca>,"Tue, 5 Apr 2016 17:38:41 -0700",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Joseph Bradley <joseph@databricks.com>,"I'm very much in favor of this, the less porting work there is the better :)


s
me-based
as
,
d
,
d
,
is
refer to the
l name and there


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 5 Apr 2016 17:41:40 -0700",Re: BROKEN BUILD? Is this only me or not?,Jacek Laskowski <jacek@japila.pl>,"Looking at recent
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7
builds, there was no such error.
I don't see anything wrong with the code:

  usage = ""_FUNC_(str) - "" +
    ""Returns str, with the first letter of each word in uppercase, all
other letters in "" +

Mind refresh and build again ?

If it still fails, please share the build command.


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 06 Apr 2016 01:23:07 +0000",Re: Updating Spark PR builder and 2.x test jobs to use Java 8 JDK,Ted Yu <yuzhihong@gmail.com>,"I finally figured out the problem: it seems that my *export
JAVA_HOME=/path/to/java8/home* was somehow not affecting the javac
executable that Zinc's SBT incremental compiler uses when it forks out to
javac to handle Java source files. As a result, we were passing a -source
1.8 flag to the platform's default javac, which happens to be Java 7.

To fix this, I'm going to modify the build to just prepend $JAVA_HOME/bin
to $PATH while setting up the test environment


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Apr 2016 21:32:03 -0400",Re: BROKEN BUILD? Is this only me or not?,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

This is a similar issue
https://issues.apache.org/jira/browse/SPARK-12530. I've fixed today's
one and am sending a pull req.

My build command is as follows:

./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.2 -Phive
-Phive-thriftserver -DskipTests clean install

I'm on Java 8 / Mac OS X

‚ûú  spark git:(master) ‚úó java -version
java version ""1.8.0_77""
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


-2.7
her
sql/catalyst/expressions/stringExpressions.scala:626:
8e545be.

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 5 Apr 2016 18:40:45 -0700",Re: BROKEN BUILD? Is this only me or not?,Jacek Laskowski <jacek@japila.pl>,"Probably related to Java 8.

I used :

$ java -version
java version ""1.7.0_67""
Java(TM) SE Runtime Environment (build 1.7.0_67-b01)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)


-2.7
:
ql/catalyst/expressions/stringExpressions.scala:626:
e545be
"
Kostas Sakellis <kostas@cloudera.com>,"Tue, 5 Apr 2016 18:54:53 -0700",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Holden Karau <holden@pigscanfly.ca>,"people have different notions of compatibility guarantees between major and
minor versions.
A simple question I have is: What compatibility can we break between minor
vs. major releases?

It might be worth getting on the same page wrt compatibility guarantees.

Just a thought,
Kostas


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Apr 2016 21:00:43 -0500",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Joseph Bradley <joseph@databricks.com>,"+1

This is a no brainer IMO.



s
me-based
as
,
d
,
d
,
is
refer to the
l name and there
"
Chris Fregly <chris@fregly.com>,"Tue, 5 Apr 2016 20:02:50 -0700",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Reynold Xin <rxin@databricks.com>,"perhaps renaming to Spark ML would actually clear up code and documentation confusion?

+1 for rename 

ote:
es.apache.org/jira/browse/SPARK-4591
rote:
 is how we update the docs so that people know to start with the spark.ml classes. Right now the docs list spark.mllib first and also seem more comprehensive in that area than in spark.ml, so maybe people naturally move towards that.
ver (SPARK-13944). There are also frequent pattern mining algorithms we need to port over in order to reach feature parity. -Xiangrui


s
te:
I built
me-based API has
PI has
as
e, it
nd
 with
PI in
opment
ounting
0000
o, to
 and to
ed MLlib
e, unless
l
eries to
recate
k 3.0.
his
evelopers
 refer to the
auses
al name and there are no
-
"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 06 Apr 2016 04:12:58 +0000",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,"Chris Fregly <chris@fregly.com>, Reynold Xin <rxin@databricks.com>","+1 for this proposal - as you mention I think it's the defacto current
situation anyway.

Note that from a developer view it's just the user-facing API that will be
only ""ml"" - the majority of the actual algorithms still operate on RDDs
under the good cur"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 06 Apr 2016 11:35:43 +0000",ClassCastException when extracting and collecting DF array column type,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi there,

In writing some tests for a PR I'm working on, with a more complex array
type in a DF, I ran into this issue (running off latest master).

Any thoughts?

*// create DF with a column of Array[(Int, Double)]*
val df = sc.parallelize(Seq(
(0, Array((1, 6.0), (1, 4.0))),
(1, Array((1, 3.0), (2, 1.0))),
(2, Array((3, 3.0), (4, 6.0))))
).toDF(""id"", ""predictions"")

*// extract the field from the Row, and use map to extract first element of
tuple*
*// the type of RDD appears correct*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1).map(_._1) }
res14: org.apache.spark.rdd.RDD[Seq[Int]] = MapPartitionsRDD[32] at map at
<console>:27

*// however, calling collect on the same expression throws
ClassCastException*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1).map(_._1) }.collect
16/04/06 13:02:49 ERROR Executor: Exception in task 5.0 in stage 10.0 (TID
74)
java.lang.ClassCastException:
org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be
cast to scala.Tuple2
at
$line54.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1$$anonfun$apply$1.apply(<console>:27)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at
$line54.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:27)
at
$line54.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:27)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:370)
at scala.collection.Iterator$class.foreach(Iterator.scala:742)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
at scala.collection.AbstractIterator.to(Iterator.scala:1194)
at
at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1194)
at scala.collection.AbstractIterator.toArray(Iterator.scala:1194)
at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:880)
at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:880)

*// can collect the extracted field*
*// again, return type appears correct*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1) }.collect
res23: Array[Seq[(Int, Double)]] = Array(WrappedArray([1,6.0], [1,4.0]),
WrappedArray([1,3.0], [2,1.0]), WrappedArray([3,3.0], [4,6.0]))

*// trying to apply map to extract first element of tuple fails*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1)
}.collect.map(_.map(_._1))
java.lang.ClassCastException:
org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be
cast to scala.Tuple2
  at $anonfun$2$$anonfun$apply$1.apply(<console>:27)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at $anonfun$2.apply(<console>:27)
  at $anonfun$2.apply(<console>:27)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
"
Chaturvedi Chola <chaturvedichola@gmail.com>,"Wed, 6 Apr 2016 18:52:21 +0530",Big Data Interview FAQ,"user-info@spark.apache.org, dev@spark.apache.org, user@spark.apache.org","Hello Team

The below is a very good book on Big Data for interview preparation.
https://notionpress.com/read/big-data-interview-faqs

Thanks,
Chaturvedi.
"
Mike Hynes <91mbbh@gmail.com>,"Wed, 6 Apr 2016 09:36:00 -0400",Re: RDD Partitions not distributed evenly to executors,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hello All (and Devs in particular),

Thank you again for your further responses. Please find a detailed
email below which identifies the cause (I believe) of the partition
imbalance problem, which occurs in spark 1.5, 1.6, and a 2.0-SNAPSHOT.
This is followed by follow-up questions for the dev community with
more intimate knowledge of the scheduler so that they may confirm my
guess at the cause, and please provide insight at how best to avoid
the problem.

Attached to this email are Gantt-chart plots which show the task
execution over elapsed time in a Spark program. This program was meant
to investigate the simplest possible vector operation for block-vector
data stored in RDDs of type RDD[(Int,Vector)]. In the Gantt plots,
you'll see the tasks shown as horizontal lines along the x axis, which
shows elapsed time. The shaded regions represent a single executor
such that all tasks managed by a single executor lie in a contiguous
shaded region. The executors all managed 16 cores on 4 different
compute nodes, and the tasks have been sorted and fit into 16 slots
for each executor according their chronological order, as determined
by the task information in the event log for the program, such that
the y-axis corresponds to essentially the unique core id, ranging from
1 to 64. The numbers running horizontally at the top of these plots is
the stage number, as determined by the DAG scheduler.

In the program itself, two block vectors, v_1 and v_2, were created
and copartitioned, cached, and then added together elementwise through
a join operation on their block index keys. Stages 0 and 1 correspond
to the map and count operations to create v_1; stages 2 and 3
correspond to the same operations on v_2; and stages 6 through 15
consist of identical count operations to materialize the vector v =
v_1 + v_2, formed through a join on v_1 and v_2. The vectors v_1 and
v_2 were initialized by first creating the keys using a
sc.parallelize{0 to num_blocks - 1} operation, after which the keys
were partitioned with a HashPartitioner (note that first a dummy map
{k => (k,k)} on the keys was done so that the HashPartitioner could be
used; the motivation for this was that, for large block vector RDDs,
it was be better to hash partition the keys before generating the
data). The size of the vectors is determined as a multiple of a fixed
vector block size (size of each sub-block) times the number of
partitions, which is itself an integer multiple of the number of
cores. Furthermore, each partition has \gamma blocks. So each
partition has \gamma blocks; there are \alpha partitions per core, and
each block has size 2^16.

The first plot, 02_4node_imbalance_spark1.6_run2.pdf, shows a
representative run of the block vector addition program for \alpha =
4, \gamma = 4. A well-balanced partitioning would correspond to 4
partitions for core, such that each executor is managing 64 tasks.
However, as you can see in stage 0, this does not occur: there is a
large imbalance, where cores 46--64 have many more tasks to compute
than the others.

Observing the order of the task assignment, I believe that what is
happening here is that, due to the initial random delay of the
executors in responding/receiving master instructions, the driver is
assigning more tasks to the executor whose initial wave of tasks
finishes first. Since there is *no* data locality in stage 0 to factor
into determining on which nodes the computation should occur, my
understanding is that the driver will allocate the tasks
greedily---hence the initial delay is crucial for allocating
partitions evenly across the nodes. Furthermore, note that stage 2 (an
identical vector initialization operation to stage 0) is
well-balanced, since all of the executors completed tasks at
approximately the same time, and hence without data locality being a
factor, were assigned new tasks at the same rate. Also, note here that
the individual task durations are *decreasing markedly* through stages
6--15 (again, all of which are identical), but that the stages are
longer than need be due to the load imbalance of the tasks.

The second plot, 02_4node_balance_longer.pdf, shows a second version
of this same program. The code is identical, however the commandline
input parameters have been changed such that there were 64 partitions
(\alpha = 1 partition per core), an identical blocksize of 2^16, but
\gamma = 16 blocks per partitions---i.e. fewer yet larger partitions
such that the vector is the same size. Here, stage 0 and 2 are both
evenly partitioned; since the tasks in these stages are longer than
the initial executor delay, no imbalance is created. However, despite
the better balance in partitions across the nodes, this program takes
*longer* in total elapsed time, and the tasks do not seem to be
getting shorter by the same proportion as in the previous test with
more partitions.

Given the above, I would like to ask the following questions:

1. Is my inference correct that the partition imbalance arises due to
the greedy nature of the scheduler when data locality is not a factor?

2a. Why are the task times in plot 1 decreasing so dramatically, but
not in plot 2?
2b. Could the decrease in time be due to just-in-time compilation?
2c. If so, Why would the JIT occur only for the first case with many
partitions when the same amount of computational work is to be done in
both cases?

3. If an RDD is to be created in such a manner (i.e. initialized for,
say, an iterative algorithm, rather than by reading data from disk or
hdfs), what is the best practice to promote good load balancing? My
first idea would be to create the full RDD with 2x as many partitions
but then coalesce it down to half the number of partitions with the
shuffle flag set to true. Would that be reasonable?

Thank you very much for your time, and I very much hope that someone
from the dev community who is familiar with the scheduler may be able
to clarify the above observations and questions.

Thanks,
Mike

P.S. Koert Kuipers: neither spark-defaults.sh setting impacted the
observed behaviour, but thank you kindly for your suggestions.




-- 
Thanks,
Mike

---------------------------------------------------------------------"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 06 Apr 2016 13:56:58 +0000",Re: ClassCastException when extracting and collecting DF array column type,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Ah I got it - Seq[(Int, Float)] is actually represented as Seq[Row] (seq of
struct type) internally.

So a further extraction is required, e.g. row => row.getSeq[Row](1).map { r
=> r.getInt(0) }


"
Scott walent <scottwalent@gmail.com>,"Wed, 06 Apr 2016 16:44:29 +0000",Agenda Announced for Spark Summit 2016 in San Francisco,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Spark Summit 2016 (www.spark-summit.org/2016) will be held from June 6-8 at
the Union Square Hilton in San Francisco, and the recently released agenda
features a stellar lineup of community talks led by top engineers,
architects, data scientists, researchers, entrepreneurs and analysts from
UC Berkeley, Duke, Microsoft, Netflix, Oracle, Bloomberg, Viacom, Airbnb,
Uber, CareerBuilder and, of course, Databricks. There‚Äôs also a full day of
hands-on Spark training, with courses for both beginners and advanced
users.

As the excitement around Spark continues to grow, and the rapid adoption
rate shows no signs of slowing down, Spark Summit is growing, too. More
than 2,500 participants are expected at the San Francisco conference,
making it the largest event yet.

Join us in June to learn more about data engineering and data science at
scale, spend time with other members of the Spark community, attend
community meetups, revel in social activities associated with the Summit,
and enjoy the beautiful city by the bay.

Developer Day: (June 7)
Aimed at a highly technical audience, this day will focus on topics about
Spark dealing with memory management, performance, optimization, scale, and
integration with the ecosystem, including dedicated tracks and sessions
covering:
- Keynotes focusing on what‚Äôs new with Spark, where Spark is heading, and
technical trends within Big Data
- Five technical tracks, including Developer, Data Science, Spark
Ecosystem, Use Cases & Experiences, and Research
- Office hours from the Spark project leads at the Expo Hall Theater

Enterprise Day: (June 8)
For anyone interested in understanding how Spark is used in the enterprise,
this day will include:
- Keynotes from leading vendors contributing to Spark and enterprise use
cases
- Full day-long track of enterprise talks featuring use cases and a vendor
panel
- Four technical tracks for continued learning from Developer Day

With more than 90 sessions, you‚Äôll be able to pick and choose the topics
that best suit your interests and expertise.

Registration (www.spark-summit.org/2016/register/) is open now, and you can
save $200 when you buy tickets before April 8th.

We hope to see you at Spark Summit 2016 in San Francisco. Follow
@spark_summit and #SparkSummit for updates.

-Spark Summit Organizers
"
Dean Wampler <deanwampler@gmail.com>,"Wed, 6 Apr 2016 12:51:42 -0500",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Kostas Sakellis <kostas@cloudera.com>,"A few other reasons to drop 2.10 support sooner rather than later.

   - We at Lightbend are evaluating some fundamental changes to the REPL to
   make it work better for large heaps, especially for Spark. There are other
   recent and planned enhancements. This work will be benefit notebook users,
   too. However, we won't back port these improvements to 2.10.
   - Scala 2.12 is coming out midyear. It will require Java 8, which means
   it will produce dramatically smaller code (by exploiting lambdas instead of
   custom class generation for functions) and it will offer some performance
   improvements. Hopefully Spark will will support it as an optional Scala
   version relatively quickly after availability, which means it would be nice
   to avoid supporting 3 versions of Scala.

Using Scala 2.10 at this point is like using Java 1.6, seriously out of
date. If you're using libraries that still require 2.10, are you sure that
library is being properly maintained? Or is it a legacy dependency that
should be eliminated before it becomes a liability? Even if you can't
upgrade Scala versions in the next few months, you can certainly continue
using Spark 1.X until you're ready to upgrade.

So, I recommend that Spark 2.0 drop Scala 2.10 support from the beginning.

dean

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Lightbend <http://lightbend.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Sean Owen <sowen@cloudera.com>,"Wed, 6 Apr 2016 19:04:56 +0100",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,"""dev@spark.apache.org"" <dev@spark.apache.org>","Answering for myself: I assume everyone is following
http://semver.org/ semantic versioning. If not, would be good to hear
an alternative theory.

For semver, strictly speaking, minor releases should be
backwards-compatible for callers. Are things like stopping support for
Java 8 or Scala 2.10 backwards-incompatible? In the end, yes,
non-trivially, on both counts. This is why it seems like these changes
must go with 2.0 or wait until 3.0.

Rules are made to be broken and few software projects do this right. I
hear the legitimate concern that getting the latest features and fixes
into users hands is really important, and it is.

Really, that argues that it's important to keep maintaining the 1.x
branch with features and fixes. However, it seems obvious there will
never be a 1.7, and probably not 1.6.2, for lack of bandwidth. And
indeed it's way too much work given the scope of this project compared
to hands to do the work.

But this is what's forcing conflation of backwards-compatibility
concerns onto a version boundary that doesn't inherently have one.
It's a reason I personally root for breaking as many promises and
encumbrances that make sense to break all at once at this boundary.

Anyway, hope that explains my general logic.



---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 6 Apr 2016 11:51:44 -0700",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Sean Owen <sowen@cloudera.com>,"In general, I agree - it is preferable to break backward compatibility
(where unavoidable) only at major versions.
Unfortunately, this usually is planned better - with earlier versions
announcing intent of the change - deprecation across multiple
releases, defaults changed, etc.

has been mentioned as a large disruption to drop support in 2.0.
The same applies for jdk 7 too unfortunately - since backward
compatibility is stronger guarantee in jvm compared to scala, it might
work better there ?


Regards,
Mridul




---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 6 Apr 2016 11:57:30 -0700",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Sean Owen <sowen@cloudera.com>,"I agree with your general logic and understanding of semver.  That is why
if we are going to violate the strictures of semver, I'd only be happy
doing so if support for Java 7 and/or Scala 2.10 were clearly understood to
be deprecated already in the 2.0.0 release -- i.e. from the outset not to
be understood to be part of the semver guarantees implied in 2.x.


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 6 Apr 2016 11:57:57 -0700",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Dean Wampler <deanwampler@gmail.com>,"
Almost by definition the answer to that is ""No; a library that hasn't been
upgraded to Scala 2.11 is not being properly maintained.""  That means that
a user of such a library is already facing the choice of whether to take on
the maintenance burden to bring the library up to date or to replace the
unmaintained library with an alternative that is being properly
maintained.  My concern is that either of those options will take more
resources than some Spark users will have available in the ~3 months
remaining before Spark 2.0.0, which will cause fragmentation into Spark 1.x
and Spark 2.x user communities.

Which is the bigger risk and cost, maintaining Scala 2.10 support for a
while longer or fragmenting the Spark user community with the 2.0.0
release?



"
Josh Rosen <joshrosen@databricks.com>,"Wed, 06 Apr 2016 19:19:11 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,"Nicholas Chammas <nicholas.chammas@gmail.com>, Jakob Odersky <jakob@odersky.com>","I downloaded the Spark 1.6.1 artifacts from the Apache mirror network and
re-uploaded them to the spark-related-packages S3 bucket, so hopefully
these packages should be fixed now.


 also corrupt.
ge? I‚Äôve
t
.
z
e
m
e
6.tgz
6.tgz
w.
Äôm
4.tgz
6.tgz
"
Sung Hwan Chung <codedeft@gmail.com>,"Wed, 6 Apr 2016 19:24:18 +0000",Executor shutdown hooks?,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi,

I'm looking for ways to add shutdown hooks to executors : i.e., when a Job
is forcefully terminated before it finishes.

The scenario goes likes this : executors are running a long running job
within a 'map' function. The user decides to terminate the job, then the
mappers should perform some cleanups before going offline.

What would be the best way to do this?
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 06 Apr 2016 19:27:32 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,"Josh Rosen <joshrosen@databricks.com>, Jakob Odersky <jakob@odersky.com>","Thank you Josh! I confirmed that the Spark 1.6.1 / Hadoop 2.6 package on S3
is now working, and the SHA512 checks out.


:
n
t
e also corrupt.
age? I‚Äôve
e
r
m
.6.tgz
.6.tgz
Äôm
.4.tgz
.6.tgz
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 6 Apr 2016 12:36:01 -0700",Re: Executor shutdown hooks?,codedeft@cs.stanford.edu,"Why would the Executors shutdown when the Job is terminated?  Executors are
bound to Applications, not Jobs.  Furthermore,
Application and DAGScheduler level won't actually interrupt the Tasks
can catch the interrupt exception within the Task.


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 6 Apr 2016 12:54:10 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Josh Rosen <joshrosen@databricks.com>,"Josh:
Can you check spark-1.6.1-bin-hadoop2.4.tgz ?

$ tar zxf spark-1.6.1-bin-hadoop2.4.tgz

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now

$ ls -l !$
ls -l spark-1.6.1-bin-hadoop2.4.tgz
-rw-r--r--. 1 hbase hadoop 323614720 Apr  5 19:25
spark-1.6.1-bin-hadoop2.4.tgz

Thanks


:
n
t
e also corrupt.
age? I‚Äôve
e
r
m
.6.tgz
.6.tgz
Äôm
.4.tgz
.6.tgz
"
Josh Rosen <joshrosen@databricks.com>,"Wed, 06 Apr 2016 19:55:32 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Ted Yu <yuzhihong@gmail.com>,"Sure, I'll take a look. Planning to do full verification in a bit.


d
on
e
re also corrupt.
kage?
/
a
a
2.6.tgz
2.6.tgz
Äôm
2.4.tgz
2.6.tgz
f
d
"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 6 Apr 2016 15:58:00 -0700",Re: Switch RDD-based MLlib APIs to maintenance mode in Spark 2.0,Chris Fregly <chris@fregly.com>,"+1 for renaming the jar file.

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D


on
o
l
move
e
is
I
ame-based
e,
nd
o,
ed
e,
l
k
 refer to the
al name and there
-

---------"
Sung Hwan Chung <codedeft@cs.stanford.edu>,"Wed, 6 Apr 2016 23:39:00 +0000",Re: Executor shutdown hooks?,Mark Hamstra <mark@clearstorydata.com>,"What I meant is 'application'. I.e., when we manually terminate an
application that was submitted via spark-submit.
When we manually kill an application, it seems that individual tasks do not
receive the interruptException.

That interruptException seems to work iff we cancel the job through
sc.cancellJob or cancelAllJobs while the application is still alive.

My option so far seems to be using JVM's shutdown hook, but I was wondering
if Spark itself had an API for tasks.


"
Reynold Xin <rxin@databricks.com>,"Wed, 6 Apr 2016 17:38:52 -0700",Re: Executor shutdown hooks?,Sung Hwan Chung <codedeft@cs.stanford.edu>,"

Spark would be using that under the hood anyway, so you might as well just
use the jvm shutdown hook directly.
"
Hemant Bhanawat <hemant9379@gmail.com>,"Thu, 7 Apr 2016 10:21:36 +0530",Re: Executor shutdown hooks?,Reynold Xin <rxin@databricks.com>,"As part of PR https://github.com/apache/spark/pull/11723, I have added a
killAllTasks function that can be used to kill (rather interrupt)
individual tasks before an executor exits. If this PR is accepted, for
doing task level cleanups, we can add a call to this function before
executor exits. The exit thread will wait for a certain period of time
before the executor jvm exits to allow proper cleanups of the tasks.

Hemant Bhanawat <https://www.linkedin.com/in/hemant-bhanawat-92a3811>
www.snappydata.io


"
Mihir Monani <mmonani@salesforce.com>,"Thu, 7 Apr 2016 13:01:39 +0530",Possible bug related to [SPARK-5708],dev@spark.apache.org,"Hi Everyone,

I was not able to get Metrics in files separately  using Slf4jsink with
below configuration in *metrics.properties* :-

*# Enable Slf4jSink for all instances by class name*
**.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink*

*# Polling period for Slf4JSink*
**.sink.slf4j.period=1*

**.sink.slf4j.unit=minutes*

It only prints in root logger file.

After making below changes in Slf4jsink.scala , i was able to get metrics
using log4j.properties in separate file :-


Slf4jsink.scala (only changes):-

*import org.slf4j.Logger*
*import org.slf4j.LoggerFactory*

*val reporter: Slf4jReporter =
Slf4jReporter.forRegistry(registry).outputTo(LoggerFactory.getLogger(""org.apache.spark.metrics""))*
*    .convertDurationsTo(TimeUnit.MILLISECONDS)*
*    .convertRatesTo(TimeUnit.SECONDS)*
*    .build()*

log4j.properties (only changes) :-

log4j.logger.org.apache.spark.metrics=INFO, metricFileAppender
log4j.additivity.org.apache.spark.metrics=true

log4j.appender.metricFileAppender=org.apache.log4j.RollingFileAppender
log4j.appender.metricFileAppender.File=logs/metric.log
log4j.appender.metricFileAppender.MaxFileSize=10MB
log4j.appender.metricFileAppender.MaxBackupIndex=10
log4j.appender.metricFileAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.metricFileAppender.layout.ConversionPattern=%d{yyyy-MM-dd
HH:mm:ss} %-5p %c{1}:%L - %m%n

Am i using wrong configuration or there is something missing in
Slf4jsink.scala
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala#L51>

There is one more bug :-

Spark is printing metric as log twice in file or on console(if you root
logger is enable) with or without above mention changes.

Thanks,
Mihir Monani
"
Wojciech Indyk <wojciechindyk@gmail.com>,"Fri, 8 Apr 2016 11:01:30 +0200",Delegation of Kerberos tokens,dev@spark.apache.org,"Hello!
 TL;DR Could you explain how (and which) Kerberos tokens should be
delegated from driver to workers? Does it depend on spark mode?

I have a Hadoop cluster HDP 2.3 with Kerberos. I use spark-sql (1.6.1
compiled with hadoop 2.7.1 and hive 1.2.1) on yarn-cluster mode to
query my hive tables.
1. When I query hive table stored in HDFS everything is fine. (assume
there is no problem with my app, config and credentials setup)
2. When I try to query external table of HBase (defined in Hive using
HBaseHandler) I have a permissions problem on RPC call from
Spark-workers to HBase region server. (there is no problem to connect
HBaseMaster from driver, Zookeepers from both driver and workers)
3. When I query the HBase table by hive (beeswax) everything is ok.
(assume there is no problem with HBaseHandler)

After some time of debugging (and write some additional logging) I see
the driver has (and delegates) only:
16/03/31 15:03:52 DEBUG YarnSparkHadoopUtil: token for:
16/03/31 15:03:52 DEBUG YarnSparkHadoopUtil: token for: 172.xx.xx102:8188
16/03/31 15:03:52 DEBUG YarnSparkHadoopUtil: token for: ha-hdfs:dataocean
Which means there are only credentials for YARN and HDFS. I am curious
is it proper behavior? I see another user has similar doubt:
https://issues.apache.org/jira/browse/SPARK-12279?focusedCommentId=15067020&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15067020

Could you explain how (and which) Kerberos tokens should be delegated
from driver to workers? Does it depend on spark mode? As I saw in the
code the method obtainTokenForHBase is calling when yarn-client mode
is on, but not for yarn-cluster. Am I right? Is it ok?

--
Kind regards/ Pozdrawiam,
Wojciech Indyk
http://datacentric.pl

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 8 Apr 2016 11:18:41 -0700","[build system] taking amp-jenkins-worker-06 and -07 offline due to
 disk space issues","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","looks like something filled up /home (0% space left), and i'll need to
figure out what that is as well as clean up some space.

once we're good, i'll put them back online and let everyone know.

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 8 Apr 2016 11:29:09 -0700","Re: [build system] taking amp-jenkins-worker-06 and -07 offline due
 to disk space issues","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","we had a couple of hanging/stuck builds that were filling up /home.
some of the procs didn't like the SIGKILL, so i just rebooted them.
/home on both of these boxes is back down to ~33% usage.

anyways, these two nodes are back up and building.  if i find anymore
stuck builds, i'll open a spark ticket for further investigation.


---------------------------------------------------------------------


"
Sung Hwan Chung <codedeft@gmail.com>,"Sat, 9 Apr 2016 04:19:18 +0000",How Spark handles dead machines during a job.,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hello,

Say, that I'm doing a simple rdd.map followed by collect. Say, also, that
one of the executors finish all of its tasks, but there are still other
executors running.

If the machine that hosted the finished executor gets terminated, does the
master still have the results from the finished tasks (and thus doesn't
restart those finished tasks)?

Or does the master require that all the executors be alive during the
entire map-collect cycle?

Thanks!
"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Apr 2016 23:35:31 -0700",Re: How Spark handles dead machines during a job.,"""codedeft@cs.stanford.edu"" <codedeft@cs.stanford.edu>","The driver has the data and wouldn't need to rerun.


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 9 Apr 2016 09:45:31 +0000",Re: Delegation of Kerberos tokens,Wojciech Indyk <wojciechindyk@gmail.com>,"
> On 8 Apr 2016, at 10:01, Wojciech Indyk <wojciechindyk@gmail.com> wrote:
> 
> Hello!
> TL;DR Could you explain how (and which) Kerberos tokens should be
> delegated from driver to workers? Does it depend on spark mode?

Hadoop tokens, not kerberos tickets...though the original k tickets are used to acquire the tokens

the most up to date coverage of the topic in general is in fact

http://hortonworks.com/webinar/hadoop-and-kerberos-the-madness-beyond-the-gate/
https://www.gitbook.com/book/steveloughran/kerberos_and_hadoop/details


> 
> I have a Hadoop cluster HDP 2.3 with Kerberos. I use spark-sql (1.6.1
> compiled with hadoop 2.7.1 and hive 1.2.1) on yarn-cluster mode to
> query my hive tables.
> 1. When I query hive table stored in HDFS everything is fine. (assume
> there is no problem with my app, config and credentials setup)
> 2. When I try to query external table of HBase (defined in Hive using
> HBaseHandler) I have a permissions problem on RPC call from
> Spark-workers to HBase region server. (there is no problem to connect
> HBaseMaster from driver, Zookeepers from both driver and workers)
> 3. When I query the HBase table by hive (beeswax) everything is ok.
> (assume there is no problem with HBaseHandler)
> 
> After some time of debugging (and write some additional logging) I see
> the driver has (and delegates) only:
> 16/03/31 15:03:52 DEBUG YarnSparkHadoopUtil: token for:
> 16/03/31 15:03:52 DEBUG YarnSparkHadoopUtil: token for: 172.xx.xx102:8188
> 16/03/31 15:03:52 DEBUG YarnSparkHadoopUtil: token for: ha-hdfs:dataocean
> Which means there are only credentials for YARN and HDFS. I am curious
> is it proper behavior? I see another user has similar doubt:
> https://issues.apache.org/jira/browse/SPARK-12279?focusedCommentId=15067020&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15067020
> 
> Could you explain how (and which) Kerberos tokens should be delegated
> from driver to workers? Does it depend on spark mode? As I saw in the
> code the method obtainTokenForHBase is calling when yarn-client mode
> is on, but not for yarn-cluster. Am I right? Is it ok?
> 

the tokens are picked up in both cases: Spark introspects on hive and Hbase if they are in the classpath, looks at their configs, decides if tokens are needed ‚Äîand asks for them if it thinks they are

They're then attached to the AM launch context, and passed down to containers after

see also https://github.com/steveloughran/spark/blob/stevel/feature/SPARK-13148-oozie/docs/running-on-yarn.md"
Jacek Laskowski <jacek@japila.pl>,"Sat, 9 Apr 2016 15:01:43 -0400",[BUILD FAILURE] Spark Project ML Local Library - me or it's real?,dev <dev@spark.apache.org>,"Hi,

Is this me or the build is broken today? I'm looking for help as it looks scary.

$ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.2 -Phive
-Phive-thriftserver -DskipTests clean install
[INFO] --- scala-maven-plugin:3.2.2:testCompile
(scala-test-compile-first) @ spark-mllib-local_2.11 ---
[INFO] Using zinc server for incremental compilation
[warn] Pruning sources from previous analysis, due to incompatible CompileSetup.
[info] Compiling 1 Scala source to
/Users/jacek/dev/oss/spark/mllib-local/target/scala-2.11/test-classes...
[error] missing or invalid dependency detected while loading class
file 'SparkFunSuite.class'.
[error] Could not access type Logging in package org.apache.spark.internal,
[error] because it (or its dependencies) are missing. Check your build
definition for
[error] missing or conflicting dependencies. (Re-run with
`-Ylog-classpath` to see the problematic classpath.)
[error] A full rebuild may help if 'SparkFunSuite.class' was compiled
against an incompatible version of org.apache.spark.internal.
[error] one error found
[error] Compile failed at Apr 9, 2016 2:27:30 PM [0.475s]
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [  4.338 s]
[INFO] Spark Project Test Tags ............................ SUCCESS [  5.238 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  6.158 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 10.397 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  7.263 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 10.448 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 11.028 s]
[INFO] Spark Project Core ................................. SUCCESS [02:04 min]
[INFO] Spark Project GraphX ............................... SUCCESS [ 16.973 s]
[INFO] Spark Project Streaming ............................ SUCCESS [ 38.458 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [01:18 min]
[INFO] Spark Project SQL .................................. SUCCESS [01:24 min]
[INFO] Spark Project ML Local Library ..................... FAILURE [  1.083 s]
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project Docker Integration Tests ............. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] Spark Project YARN ................................. SKIPPED
[INFO] Spark Project Hive Thrift Server ................... SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Spark Project External Flume Sink .................. SKIPPED
[INFO] Spark Project External Flume ....................... SKIPPED
[INFO] Spark Project External Flume Assembly .............. SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project External Kafka Assembly .............. SKIPPED
[INFO] Spark Project Java 8 Tests ......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 06:39 min
[INFO] Finished at: 2016-04-09T14:27:30-04:00
[INFO] Final Memory: 79M/893M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal
net.alchim31.maven:scala-maven-plugin:3.2.2:testCompile
(scala-test-compile-first) on project spark-mllib-local_2.11:
Execution scala-test-compile-first of goal
net.alchim31.maven:scala-maven-plugin:3.2.2:testCompile failed.
CompileFailed -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with
the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions,
please read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-mllib-local_2.11

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 9 Apr 2016 12:40:49 -0700",Re: [BUILD FAILURE] Spark Project ML Local Library - me or it's real?,Jacek Laskowski <jacek@japila.pl>,"The broken build was caused by the following:

[SPARK-14462][ML][MLLIB] add the mllib-local build to maven pom

See
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/607/

FYI


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 9 Apr 2016 13:16:59 -0700",Re: [BUILD FAILURE] Spark Project ML Local Library - me or it's real?,Jacek Laskowski <jacek@japila.pl>,"Sent PR:
https://github.com/apache/spark/pull/12276

I was able to get build going past mllib-local module.

FYI


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 9 Apr 2016 19:26:58 -0400",Re: [BUILD FAILURE] Spark Project ML Local Library - me or it's real?,Ted Yu <yuzhihong@gmail.com>,"Hi Ted et al,

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10:41 min
[INFO] Finished at: 2016-04-09T19:21:02-04:00
[INFO] Final Memory: 106M/961M
[INFO] ------------------------------------------------------------------------

Thank you so much for the prompt solution! And that's while I was
driving from Toronto to Mississauga. Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
zhang juntao <juntao.zhang.cn@gmail.com>,"Mon, 11 Apr 2016 00:08:59 +0800",spark graphx storage RDD memory leak,dev@spark.apache.org,"hi experts,

I‚Äôm reporting a problem about spark graphx, I use zeppelin submit spark jobs, 
note that scala environment shares the same SparkContext, SQLContext instance,
and I call  Connected components algorithm to do some Business,  
found that every time when the job finished, some graph storage RDDs weren‚Äôt bean released, 
after several times there would be a lot of  storage RDDs existing even through all the jobs have finished . 



So I check the code of connectedComponents  and find that may be a problem in Pregel.scala .
when param graph has been cached, there isn‚Äôt any way to unpersist,  
so I add red font code to solve the problem
def apply[VD: ClassTag, ED: ClassTag, A: ClassTag]
   (graph: Graph[VD, ED],
    initialMsg: A,
    maxIterations: Int = Int.MaxValue,
    activeDirection: EdgeDirection = EdgeDirection.Either)
   (vprog: (VertexId, VD, A) => VD,
    sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],
    mergeMsg: (A, A) => A)
  : Graph[VD, ED] =
{
  ......
  var g = graph.mapVertices((vid, vdata) => vprog(vid, vdata, initialMsg)).cache()
  graph.unpersistVertices(blocking = false)
  graph.edges.unpersist(blocking = false)
  ......

} // end of apply

I'm not sure if this is a bug, 
and thank you for your time,
juntao


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 10 Apr 2016 10:15:23 -0700",Re: spark graphx storage RDD memory leak,zhang juntao <juntao.zhang.cn@gmail.com>,"I see the following code toward the end of the method:

      // Unpersist the RDDs hidden by newly-materialized RDDs
      oldMessages.unpersist(blocking = false)
      prevG.unpersistVertices(blocking = false)
      prevG.edges.unpersist(blocking = false)

Wouldn't the above achieve same effect ?


 spark
m
t,
D],    initialMsg: A,    maxIterations: Int = Int.MaxValue,    activeDirection: EdgeDirection = EdgeDirection.Either)   (vprog: (VertexId, VD, A) => VD,    sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],    mergeMsg: (A, A) => A)  : Graph[VD, ED] ={*
lMsg)).cache()  graph.unpersistVertices(blocking = false)  graph.edges.unpersist(blocking = false)*
"
Yash Sharma <yash360@gmail.com>,"Mon, 11 Apr 2016 12:46:11 +1000","Spark Sql on large number of files (~500Megs each) fails after couple
 of hours",dev@spark.apache.org,"Hi All,
I am trying Spark Sql on a dataset ~16Tb with large number of files (~50K).
Each file is roughly 400-500 Megs.

I am issuing a fairly simple hive query on the dataset with just filters
(No groupBy's and Joins) and the job is very very slow. It runs for 7-8 hrs
and processes about 80-100 Gigs on a 12 node cluster.

I have experimented with different values of spark.sql.shuffle.partitions
from 20 to 4000 but havn't seen lot of difference.

below spark configs [2] for the job.

Is there any other tuning I need to look into. Any tips would be
appreciated,

Thanks


2. Spark config -
spark-submit
--master yarn-client
--driver-memory 1G
--executor-memory 10G
--executor-cores 5
--conf spark.dynamicAllocation.enabled=true
--conf spark.shuffle.service.enabled=true
--conf spark.dynamicAllocation.initialExecutors=2
--conf spark.dynamicAllocation.minExecutors=2


1. Yarn Error:

"
cherry_zhang <chao.c.zhang@intel.com>,"Sun, 10 Apr 2016 19:54:24 -0700 (MST)",Spark Jenkins test configurations,dev@spark.apache.org,"We are running unit test on our own Jenkins Server, but we encounter some
problems about that, so could someone give me a detail list of the
configurations about Jenkins Server? thx.



--

---------------------------------------------------------------------


"
"""Yu, Yucai"" <yucai.yu@intel.com>","Mon, 11 Apr 2016 03:10:24 +0000","RE: Spark Sql on large number of files (~500Megs each) fails after
 couple of hours","Yash Sharma <yash360@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Yash,

How about checking the executor(yarn container) log? Most of time, it shows more details, we are using CDH, the log is at:

[yucai@sr483 container_1457699919227_0094_01_000014]$ pwd
/mnt/DP_disk1/yucai/yarn/logs/application_1457699919227_0094/container_1457699919227_0094_01_000014
[yucai@sr483 container_1457699919227_0094_01_000014]$ ls -tlr
total 408
-rw-r--r-- 1 yucai DP 382676 Mar 13 18:04 stderr
-rw-r--r-- 1 yucai DP  22302 Mar 13 18:04 stdout

Please pay attention, you had better check the first failure container .

Thanks,
Yucai

From: Yash Sharma [mailto:yash360@gmail.com]
Sent: Monday, April 11, 2016 10:46 AM
To: dev@spark.apache.org
Subject: Spark Sql on large number of files (~500Megs each) fails after couple of hours

Hi All,
I am trying Spark Sql on a dataset ~16Tb with large number of files (~50K). Each file is roughly 400-500 Megs.

I am issuing a fairly simple hive query on the dataset with just filters (No groupBy's and Joins) and the job is very very slow. It runs for 7-8 hrs and processes about 80-100 Gigs on a 12 node cluster.

I have experimented with different values of spark.sql.shuffle.partitions from 20 to 4000 but havn't seen lot of difference.

From the logs I have the yarn error attached at end [1]. I have got the below spark configs [2] for the job.

Is there any other tuning I need to look into. Any tips would be appreciated,

Thanks


2. Spark config -
spark-submit
--master yarn-client
--driver-memory 1G
--executor-memory 10G
--executor-cores 5
--conf spark.dynamicAllocation.enabled=true
--conf spark.shuffle.service.enabled=true
--conf spark.dynamicAllocation.initialExecutors=2
--conf spark.dynamicAllocation.minExecutors=2


1. Yarn Error:

16/04/07 13:05:37 INFO yarn.YarnAllocator: Container marked as failed: container_1459747472046_1618_02_000003. Exit status: 1. Diagnostics: Exception from container-launch.
Container id: container_1459747472046_1618_02_000003
Exit code: 1
Stack trace: ExitCodeException exitCode=1:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
        at org.apache.hadoop.util.Shell.run(Shell.java:455)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

Container exited with a non-zero exit code 1
"
Yash Sharma <yash360@gmail.com>,"Mon, 11 Apr 2016 13:51:21 +1000","Re: Spark Sql on large number of files (~500Megs each) fails after
 couple of hours","""Yu, Yucai"" <yucai.yu@intel.com>","Hi Yucai,
Thanks for the info. I have explored the container logs but did not get lot
of information from it.

I have seen this error log for few containers but not sure of the cause for
it.
1. java.lang.NullPointerException (DiskBlockManager.scala:167)
2. java.lang.ClassCastException: RegisterExecutorFailed

Attaching the log for reference.


16/04/07 13:05:43 INFO storage.MemoryStore: MemoryStore started with



"
"""Yu, Yucai"" <yucai.yu@intel.com>","Mon, 11 Apr 2016 04:52:06 +0000","RE: Spark Sql on large number of files (~500Megs each) fails after
 couple of hours",Yash Sharma <yash360@gmail.com>,"It is possible not the first failure, could you increase below and rerun?
spark.yarn.executor.memoryOverhead           4096

In my experience, sometimes, netty will use lots of off-heap memory, which may lead to exceed container memory limitation and be killed by yarn‚Äôs node mana60@gmail.com]
Sent: Monday, April 11, 2016 11:51 AM
To: Yu, Yucai <yucai.yu@intel.com>
Cc: dev@spark.apache.org
Subject: Re: Spark Sql on large number of files (~500Megs each) fails after couple of hours

Hi Yucai,
Thanks for the info. I have explored the container logs but did not get lot of information from it.

I have seen this error log for few containers but not sure of the cause for it.
1. java.lang.NullPointerException (DiskBlockManager.scala:167)
2. java.lang.ClassCastException: RegisterExecutorFailed

Attaching the log for reference.


16/04/07 13:05:43 INFO storage.MemoryStore: MemoryStore started with capacity 2.6 GB
16/04/07 13:05:43 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@10.65.224.199:44692/user/CoarseGrainedScheduler<http://sparkDriver@10.65.224.199:44692/user/CoarseGrainedScheduler>
16/04/07 13:05:43 ERROR executor.CoarseGrainedExecutorBackend: Cannot register with driver: akka.tcp://sparkDriver@10.65.224.199:44692/user/CoarseGrainedScheduler<http://sparkDriver@10.65.224.199:44692/user/CoarseGrainedScheduler>
java.lang.ClassCastException: Cannot cast org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RegisterExecutorFailed to org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RegisteredExecutor$
        at java.lang.Class.cast(Class.java:3186)
        at scala.concurrent.Future$$anonfun$mapTo$1.apply(Future.scala:405)
        at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
        at scala.util.Try$.apply(Try.scala:161)
        at scala.util.Success.map(Try.scala:206)
        at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
        at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
        at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
        at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
        at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
        at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
        at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
        at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
        at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
        at scala.concurrent.impl.Promise$KeptPromise.onComplete(Promise.scala:333)
        at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:254)
        at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
        at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
        at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
        at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:266)
        at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:89)
        at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:935)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:411)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
16/04/07 13:05:44 INFO storage.DiskBlockManager: Shutdown hook called
16/04/07 13:05:44 ERROR util.Utils: Uncaught exception in thread Thread-2
java.lang.NullPointerException
        at org.apache.spark.storage.DiskBlockManager.org<http://org.apache.spark.storage.DiskBlockManager.org>$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:167)
        at org.apache.spark.storage.DiskBlockManager$$anonfun$addShutdownHook$1.apply$mcV$sp(DiskBlockManager.scala:149)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:264)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:234)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:234)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:234)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:216)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
16/04/07 13:05:44 INFO util.ShutdownHookManager: Shutdown hook called

On Mon, Apr 11, 2016 at 1:10 PM, Yu, Yucai <yucai.yu@intel.com<mailto:yucai.yu@intel.com>> wrote:
Hi Yash,

How about checking the executor(yarn container) log? Most of time, it shows more details, we are using CDH, the log is at:

[yucai@sr483 container_1457699919227_0094_01_000014]$ pwd
/mnt/DP_disk1/yucai/yarn/logs/application_1457699919227_0094/container_1457699919227_0094_01_000014
[yucai@sr483 container_1457699919227_0094_01_000014]$ ls -tlr
total 408
-rw-r--r-- 1 yucai DP 382676 Mar 13 18:04 stderr
-rw-r--r-- 1 yucai DP  22302 Mar 13 18:04 stdout

Please pay attention, you had better check the first failure container .

Than<mailto:yash360@gmail.com>]
Sent: Monday, April 11, 2016 10:46 AM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Spark Sql on large number of files (~500Megs each) fails after couple of hours

Hi All,
I am trying Spark Sql on a dataset ~16Tb with large number of files (~50K). Each file is roughly 400-500 Megs.

I am issuing a fairly simple hive query on the dataset with just filters (No groupBy's and Joins) and the job is very very slow. It runs for 7-8 hrs and processes about 80-100 Gigs on a 12 node cluster.

I have experimented with different values of spark.sql.shuffle.partitions from 20 to 4000 but havn't seen lot of difference.

From the logs I have the yarn error attached at end [1]. I have got the below spark configs [2] for the job.

Is there any other tuning I need to look into. Any tips would be appreciated,

Thanks


2. Spark config -
spark-submit
--master yarn-client
--driver-memory 1G
--executor-memory 10G
--executor-cores 5
--conf spark.dynamicAllocation.enabled=true
--conf spark.shuffle.service.enabled=true
--conf spark.dynamicAllocation.initialExecutors=2
--conf spark.dynamicAllocation.minExecutors=2


1. Yarn Error:

16/04/07 13:05:37 INFO yarn.YarnAllocator: Container marked as failed: container_1459747472046_1618_02_000003. Exit status: 1. Diagnostics: Exception from container-launch.
Container id: container_1459747472046_1618_02_000003
Exit code: 1
Stack trace: ExitCodeException exitCode=1:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
        at org.apache.hadoop.util.Shell.run(Shell.java:455)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

Container exited with a non-zero exit code 1

"
zhang juntao <juntao.zhang.cn@gmail.com>,"Mon, 11 Apr 2016 14:23:08 +0800",Fwd: spark graphx storage RDD memory leak,Ted Yu <yuzhihong@gmail.com>,"thanks ted for replying ,
these three lines can‚Äôt release param graph cache, it only release g ( graph.mapVertices((vid, vdata) => vprog(vid, vdata, initialMsg)).cache() )
ConnectedComponents.scala param graph will cache in ccGraph and won‚Äôt be release in Pregel
  def run[VD: ClassTag, ED: ClassTag](graph: Graph[VD, ED]): Graph[VertexId, ED] = {
    val ccGraph = graph.mapVertices { case (vid, _) => vid }
    def sendMessage(edge: EdgeTriplet[VertexId, ED]): Iterator[(VertexId, VertexId)] = {
      if (edge.srcAttr < edge.dstAttr) {
        Iterator((edge.dstId, edge.srcAttr))
      } else if (edge.srcAttr > edge.dstAttr) {
        Iterator((edge.srcId, edge.dstAttr))
      } else {
        Iterator.empty
      }
    }
    val initialMessage = Long.MaxValue
    Pregel(ccGraph, initialMessage, activeDirection = EdgeDirection.Either)(
      vprog = (id, attr, msg) => math.min(attr, msg),
      sendMsg = sendMessage,
      mergeMsg = (a, b) => math.min(a, b))
  } // end of connectedComponents
}
thanks
juntao


submit spark jobs, 
instance,
weren‚Äôt bean released, 
even through all the jobs have finished . 
problem in Pregel.scala .
unpersist,  
initialMsg)).cache()

"
Rahul Tanwani <tanwanirahul@gmail.com>,"Mon, 11 Apr 2016 02:06:59 -0700 (MST)","Different maxBins value for categorical and continuous features in
 RandomForest implementation.",dev@spark.apache.org,"Hi,

Currently the RandomForest algo takes a single maxBins value to decide the
number of splits to take. This sometimes causes training time to go very
high when there is a single categorical column having sufficiently large
number of unique values. This single column impacts all the numeric
(continuous) columns even though such a high number of splits are not
required.

Encoding the  categorical column into features make the data very wide and
this requires us to increase the maxMemoryInMB and puts more pressure on the
GC as well.

Keeping the separate maxBins values for categorial and continuous features
should be useful in this regard.




--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 11 Apr 2016 07:29:46 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Josh Rosen <joshrosen@databricks.com>,"Gentle ping: spark-1.6.1-bin-hadoop2.4.tgz from S3 is still corrupt.


lly
:
s
he
are also corrupt.
ckage?
:
m>
 /
 a
p2.6.tgz
p2.6.tgz
‚Äôm
p2.4.tgz
p2.6.tgz
d
ed
"
Yogesh Mahajan <ymahajan@snappydata.io>,"Mon, 11 Apr 2016 23:40:56 +0530",Re: [Streaming] textFileStream has no events shown in web UI,"Hao Ren <invkrh@gmail.com>, dev@spark.apache.org","Yes, this has observed in my case also. The Input Rate is 0 even in case of
rawSocketStream.
Is there a way we can enable the Input Rate for these types of streams ?

Thanks,
http://www.snappydata.io/blog <http://snappydata.io>


"
Robin East <robin.east@xense.co.uk>,"Mon, 11 Apr 2016 19:13:10 +0100",Re: spark graphx storage RDD memory leak,zhang juntao <juntao.zhang.cn@gmail.com>,"this looks like https://issues.apache.org/jira/browse/SPARK-12655 <https://issues.apache.org/jira/browse/SPARK-12655> fixed in 2.0
-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action <http://www.manning.com/books/spark-graphx-in-action>





release g ( graph.mapVertices((vid, vdata) => vprog(vid, vdata, initialMsg)).cache() )
won‚Äôt be release in Pregel
Graph[VertexId, ED] = {
Iterator[(VertexId, VertexId)] = {
EdgeDirection.Either)(
<mailto:juntao.zhang.cn@gmail.com>>
<dev@spark.apache.org <mailto:dev@spark.apache.org>>
submit spark jobs, 
instance,
weren‚Äôt bean released, 
even through all the jobs have finished . 
problem in Pregel.scala .
unpersist,  
initialMsg)).cache()

"
Daniel Siegmann <daniel.siegmann@teamaol.com>,"Mon, 11 Apr 2016 15:10:40 -0400",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Mark Hamstra <mark@clearstorydata.com>,"
... My concern is that either of those options will take more resources

It's not as if everyone is going to switch over to Spark 2.0.0 on release
day anyway. It's not that unusual to see posts on the user list from people
who are a version or two behind. I think a few extra months lag time will
be OK for a major version.

Besides, in my experience if you give people more time to upgrade, they're
just going to kick the can down the road a ways and you'll eventually end
up with the same problem. I don't see a good reason to *not* drop Java 7
and Scala 2.10 support with Spark 2.0.0. Time to bite the bullet. If
companies stick with Spark 1.x and find themselves missing the new features
in the 2.x line, that will be a good motivation for them to upgrade.

~Daniel Siegmann
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 11 Apr 2016 12:23:36 -0700",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Daniel Siegmann <daniel.siegmann@teamaol.com>,"Yes, some organization do lag behind the current release by sometimes a
significant amount.  That is a bug, not a feature -- and one that increases
pressure toward fragmentation of the Spark community.  To date, that hasn't
been a significant problem, and I think that is mainly because the factors
motivating a decision not to upgrade in a timely fashion are almost
entirely internal to a lagging organization -- Spark itself has tried to
present minimal impediments to upgrading as soon as a new release is
available.

Changing the supported Java and Scala versions within the same quarter in
which the next version is scheduled for release would represent more than a
minimal impediment, and would increase fragmentation pressure to a degree
with which I am not entirely comfortable.


"
zhang juntao <juntao.zhang.cn@gmail.com>,"Tue, 12 Apr 2016 09:12:12 +0800",Fwd: spark graphx storage RDD memory leak,Ted Yu <yuzhihong@gmail.com>,"yes I use version 1.6 , and thanks Ted 

<https://issues.apache.org/jira/browse/SPARK-12655> fixed in 2.0
-------------------------------------------------------------------------------
<http://www.manning.com/books/spark-graphx-in-action>
release g ( graph.mapVertices((vid, vdata) => vprog(vid, vdata, initialMsg)).cache() )
won‚Äôt be release in Pregel
Graph[VertexId, ED] = {
Iterator[(VertexId, VertexId)] = {
EdgeDirection.Either)(
<mailto:juntao.zhang.cn@gmail.com>>
<dev@spark.apache.org <mailto:dev@spark.apache.org>>
submit spark jobs, 
instance,
weren‚Äôt bean released, 
even through all the jobs have finished . 
problem in Pregel.scala .
unpersist,  
initialMsg)).cache()

"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 12 Apr 2016 12:46:19 +0530",Possible deadlock in registering applications in the recovery mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I have encountered a small issue in the standalone recovery mode.

Let's say there was an application A running in the cluster. Due to some
issue, the entire cluster, together with the application A goes down.

Then later on, cluster comes back online, and the master then goes into the
'recovering' mode, because it sees some apps, workers and drivers have
already been in the cluster from Persistence Engine. While in the recovery
process, the application comes back online, but now it would have a
different ID, let's say B.

But then, as per the master, application registration logic, this
application B will NOT be added to the 'waitingApps' with the message
""""Attempted to re-register application at same address"". [1]

  private def registerApplication(app: ApplicationInfo): Unit = {
    val appAddress = app.driver.address
    if (addressToApp.contains(appAddress)) {
      logInfo(""Attempted to re-register application at same address: "" +
appAddress)
      return
    }


The problem here is, master is trying to recover application A, which is
not in there anymore. Therefore after the recovery process, app A will be
dropped. However app A's successor, app B was also omitted from the
'waitingApps' list because it had the same address as App A previously.

This creates a deadlock in the cluster, app A nor app B is available in the
cluster.

When the master is in the RECOVERING mode, shouldn't it add all the
registering apps to a list first, and then after the recovery is completed
(once the unsuccessful recoveries are removed), deploy the apps which are
new?

This would sort this deadlock IMO?

look forward to hearing from you.

best

[1]
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L834

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Rajesh Balamohan <rajesh.balamohan@gmail.com>,"Tue, 12 Apr 2016 18:02:02 +0530",SparkSQL - Limit pushdown on BroadcastHashJoin,dev@spark.apache.org,"Hi,

I ran the following query in spark (latest master codebase) and it took a
lot of time to complete even though it was a broadcast hash join.

It appears that limit computation is done only after computing complete
join condition.  Shouldn't the limit condition be pushed to
BroadcastHashJoin (wherein it would have to stop processing after
generating 10 rows?).  Please let me know if my understanding on this is
wrong.


select l_partkey from lineitem, partsupp where ps_partkey=l_partkey limit
10;

| == Physical Plan ==
CollectLimit 10
+- WholeStageCodegen
   :  +- Project [l_partkey#893]
   :     +- BroadcastHashJoin [l_partkey#893], [ps_partkey#908], Inner,
BuildRight, None
   :        :- Project [l_partkey#893]
   :        :  +- Filter isnotnull(l_partkey#893)
   :        :     +- Scan HadoopFiles[l_partkey#893] Format: ORC,
PushedFilters: [IsNotNull(l_partkey)], ReadSchema: struct<l_partkey:int>
   :        +- INPUT
   +- BroadcastExchange
HashedRelationBroadcastMode(true,List(cast(ps_partkey#908 as
bigint)),List(ps_partkey#908))
      +- WholeStageCodegen
         :  +- Project [ps_partkey#908]
         :     +- Filter isnotnull(ps_partkey#908)
         :        +- Scan HadoopFiles[ps_partkey#908] Format: ORC,
PushedFilters: [IsNotNull(ps_partkey)], ReadSchema: struct<ps_partkey:int>
 |




-- 
~Rajesh.B
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Tue, 12 Apr 2016 14:46:12 +0200",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Rajesh Balamohan <rajesh.balamohan@gmail.com>,"I am not sure if you can push a limit through a join. This becomes
problematic if not all keys are present on both sides; in such a case a
limit can produce fewer rows than the set limit.

This might be a rare case in which whole stage codegen is slower, due to
the fact that we need to buffer the result of such a stage. You could try
to disable it by setting ""spark.sql.codegen.wholeStage"" to false.

2016-04-12 14:32 GMT+02:00 Rajesh Balamohan <rajesh.balamohan@gmail.com>:

"
Yang Lei <genially@gmail.com>,"Tue, 12 Apr 2016 17:05:29 -0400",Spark on Mesos 0.28 issue,dev <dev@spark.apache.org>,"I have been able to run spark submission in docker container (HOST network) through Marathon on mesos and target to Mesos cluster (zk address) for at least Spark 1.6, 1.5.2 over Mesos 0.26, 0.27. 

I do need to define SPARK_PUBLIC_DNS and SPARK_LOCAL_IP so that the spark driver can announce the right IP address.

However, on Mesos 0.28, the spark framework will fail with ""Failed to shutdown socket with fd 54: Transport endpoint is not connected <https://www.google.com/search?client=safari&rls=en&q=Failed+to+shutdown+socket+with+fd+54:+Transport+endpoint+is+not+connected&ie=UTF-8&oe=UTF-8>‚Äù. Eventually, I got the problem bypassed by defining additional  LIBPROCESS_IP

Please let me know if the behavior is as expected. If it is,  it will be good to document the requirement on the Spark Mesos cluster website.

Thank you.

Yang."
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 13 Apr 2016 01:05:09 +0000",Re: Spark 1.6.1 packages on S3 corrupt?,"Augustus Hong <augustus@branchmetrics.io>, user <user@spark.apache.org>","Yes, this is a known issue. The core devs are already aware of it. [CC dev]

FWIW, I believe the Spark 1.6.1 / Hadoop 2.6 package on S3 is not corrupt.
It may be the only 1.6.1 package that is not corrupt, though. :/

Nick



t?
z
============================================================================================>]
Äô saved
"
Timothy Chen <tnachen@gmail.com>,"Tue, 12 Apr 2016 18:34:50 -0700",Re: Spark on Mesos 0.28 issue,Yang Lei <genially@gmail.com>,"Hi Yang,

Can you share the master log/slave log?

Tim


) through Marathon on mesos and target to Mesos cluster (zk address) for at least Spark 1.6, 1.5.2 over Mesos 0.26, 0.27. 
river can announce the right IP address.
down socket with fd 54: Transport endpoint is not connected°±. Eventually, I got the problem bypassed by defining additional  LIBPROCESS_IP
ood to document the requirement on the Spark Mesos cluster website.
"
Joseph Bradley <joseph@databricks.com>,"Tue, 12 Apr 2016 20:05:09 -0700","Re: Different maxBins value for categorical and continuous features
 in RandomForest implementation.",Rahul Tanwani <tanwanirahul@gmail.com>,"That sounds useful.  Would you mind creating a JIRA for it?  Thanks!
Joseph


"
Tony Kinsley <tkinsley.9@gmail.com>,"Wed, 13 Apr 2016 04:57:33 +0000",Accessing Secure Hadoop from Mesos cluster,dev@spark.apache.org,"I have been working towards getting some spark streaming jobs to run in
Mesos cluster mode (using docker containers) and write data periodically to
a secure HDFS cluster. Unfortunately this does not seem to be well
supported currently in spark (
https://issues.apache.org/jira/browse/SPARK-12909). The problem seems to be
that A) passing in a principal and keytab only get processed if the backend
is yarn, B) all the code for renewing tickets is implemented by the yarn
backend.


My first attempt to get around this problem was to create docker containers
that would use a custom entrypoint to run a process manager. Then have cron
running in each container which would periodically run kinit. I was hoping
this would work since the spark can correctly log in if the TGT exists (at
least from my tests manually kinit‚Äôing and running spark in local mode).
However this hack will not work (currently anyways) as the Mesos scheduler
does not specify whether a shell should be used for the command. Mesos will
default to using the shell and then override the entrypoint of the docker
image with /bin/sh (https://issues.apache.org/jira/browse/MESOS-1770).


Since I have not been able to come up with an acceptable work around I am
looking into the possibility of adding the functionality into Spark, but I
wanted to check in to make sure I was not duplicating others work and also
to get some general advice on a good approach to solving this problem. I
have found this old email chain that talks about some different challenges
associated with authenticating correctly to the NameNodes (
http://comments.gmane.org/gmane.comp.lang.scala.spark.user/14257).


I've noticed that the Yarn security settings are namespaced to be specific
to Yarn and that there is some code that seems to be fairly generic
(AMDelegationTokenRenewer.scala and ExecutorDelegationTokenUpdater for
instance although I'm not sure about the use of the YarnSparkHadoopUtils).
It would seem to me that some of this code could be reused across the
various cluster backends. That said, I am fairly new to working with Hadoop
and Spark, and do not claim to understand the inner workings of Yarn or
Mesos, although I feel much more comfortable with Mesos.


I would definitely appreciate some guidance especially since whatever work
that I or ViaSat (my employer) gets working we would definitely be
interested in contributing it back and would very much want to avoid
maintaining a fork of Spark.

Tony
"
"""Justin.Pihony"" <justin.pihony@gmail.com>","Tue, 12 Apr 2016 22:51:38 -0700 (MST)",jdbc/save DataFrameWriter implementation change,dev@spark.apache.org,"Hi,

I have a ticket open on how save should delegate to the jdbc method, however
I went to implement this and it just didn't seem clean. Please take a look
at my comment on https://issues.apache.org/jira/browse/SPARK-14525 and let
me know if you agree with the second approach or not.

Thanks,
Justin



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 13 Apr 2016 07:45:21 +0000",Code freeze?,dev <dev@spark.apache.org>,"I've heard several people refer to a code freeze for 2.0. Unless I missed
it, nobody has discussed a particular date for this:
https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage

I'd like to start with a review of JIRAs before anyone decides a freeze is
appropriate. There are hundreds of issues, some blockers, still targeted
for 2.0. Probably best for everyone to review and retarget non essentials
and then see where we are at?
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Apr 2016 00:50:46 -0700",Re: Code freeze?,Sean Owen <sowen@cloudera.com>,"I think the main things are API things that we need to get right.

- Implement essential DDLs https://issues.apache.org/jira/browse/SPARK-14118
 this blocks the next one

- Merge HiveContext and SQLContext and create SparkSession
https://issues.apache.org/jira/browse/SPARK-13485

- Separate out local linear algebra as a standalone module without Spark
dependency https://issues.apache.org/jira/browse/SPARK-13944

- Run Spark without assembly jars (mostly done?)


Probably realistic to have it in ~ 2 weeks.




"
Adrian Bridgett <adrian@opensignal.com>,"Wed, 13 Apr 2016 12:03:57 +0100",Re: Spark on Mesos 0.28 issue,"Timothy Chen <tnachen@gmail.com>, Yang Lei <genially@gmail.com>","I think you maybe hitting 
https://issues.apache.org/jira/browse/MESOS-4878 which was fixed in 
Mesos 0.28.1


-- 
*Adrian Bridgett* |  Sysadmin Engineer, OpenSignal 
<http://www.opensignal.com>
_____________________________________________________
Office: 3rd Floor, The Angel Office, 2 Angel Square, London, EC1V 1NY
Phone #: +44 777-377-8251
Skype: abridgett  |@adrianbridgett <http://twitter.com/adrianbridgett>| 
LinkedIn link <https://uk.linkedin.com/in/abridgett>
_____________________________________________________
"
Yang Lei <genially@gmail.com>,"Wed, 13 Apr 2016 07:59:07 -0400",Re: Spark on Mesos 0.28 issue,Adrian Bridgett <adrian@opensignal.com>,"I looked at the JIRA. I do not think it is related, as without using docker image for spark task, the framework still fail. After my work around, both scenarios, w/ docker and w/o worked. 

About logs, the only thing caught my eyes is the line I pasted. It is from the master mesos log. The slave log does not have error messages... 

Yang



Sent from my iPad

:
 which was fixed in Mesos 0.28.1
rk) through Marathon on mesos and target to Mesos cluster (zk address) for at least Spark 1.6, 1.5.2 over Mesos 0.26, 0.27. 
k driver can announce the right IP address.
utdown socket with fd 54: Transport endpoint is not connected°±. Eventually, I got the problem bypassed by defining additional  LIBPROCESS_IP
 good to document the requirement on the Spark Mesos cluster website.
"
Marcin Tustin <mtustin@handybook.com>,"Wed, 13 Apr 2016 09:15:31 -0400","Should localProperties be inheritable? Should we change that or
 document it?",dev@spark.apache.org,"*Tl;dr: *SparkContext.setLocalProperty is implemented with
InheritableThreadLocal.
This has unexpected consequences, not least because the method
documentation doesn't say anything about it:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L605

I'd like to propose that we do one of: (1) document explicitly that these
properties are inheritable; (2) stop them being inheritable; or (3)
introduce the option to set these in a non-inheritable way.

*Motivation: *This started with me investigating a last vestige of the
leaking spark.sql.execution.id issue in Spark 1.5.2 (it's not reproducible
under controlled conditions, and given the many and excellent fixes on this
issue it's completely mysterious that this hangs around; the bug itself is
largely beside the point).

The specific contribution that inheritable localProperties makes to this
problem is that if a localProperty like spark.sql.execution.id leaks (i.e.
remains set when it shouldn't) because those properties are inherited by
spawned threads, that pollution affects all subsequently spawned threads.

This doesn't sound like a big deal - why would worker threads be spawning
other threads? It turns out that Java's ThreadPoolExecutor has worker
threads spawn other worker threads (it has no master dispatcher thread; the
workers themselves run all the housekeeping). JavaDoc here:
https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html
and source code here:
http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/8u40-b25/java/util/concurrent/ThreadPoolExecutor.java#ThreadPoolExecutor

Accordingly, if using Scala Futures and any kind of thread pool that comes
built-in with Java, it's impossible to avoid localproperties propagating
haphazardly to different threads. For localProperties explicitly set by
user code this isn't nice, and requires work arounds like explicitly
clearing known properties at the start of every future, or in a
beforeExecute hook on the threadpool. For leaky properties the work around
is pretty much the same: defensively clear them in the threadpool.

*Options:*
(0) Do nothing at all. Unattractive, because documenting this would still
be better;
(1) Update the scaladoc to explicitly say that localProperties are
inherited by spawned threads and note that caution should be exercised with
thread pools.
(2) Switch to using ordinary, non-inheritable thread locals. I assume this
would break something for somebody, but if not, this would be my preferred
option. Also a very simple change to implement if no-one is relying on
property inheritance.
(3) Introduce a second localProperty facility which is not inherited. This
would not break any existing code, and should not be too hard to implement.
localProperties which need cleanup could be migrated to using this
non-inheritable facility, helping to limit the impact of failing to clean
up.
The way I envisage this working is that non-inheritable localProperties
would be checked first, then inheritable, then global properties.

*Actions:*
I'm happy to do the coding and open such Jira tickets as desirable or
necessary. Before I do any of that, I'd like to know if there's any support
for this, and ideally secure a committer who can help shepherd this change
through.

Marcin Tustin

-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Travis Crawford <traviscrawford@gmail.com>,"Wed, 13 Apr 2016 14:45:35 +0000",DynamoDB data source questions,dev@spark.apache.org,"Hi Spark gurus,

At Medium we're using Spark for an ETL job that scans DynamoDB tables and
loads into Redshift. Currently I use a parallel scanner implementation that
writes files to local disk, then have Spark read them as a DataFrame.

Ideally we could read the DynamoDB table directly as a DataFrame, so I
started putting together a data source at
https://github.com/traviscrawford/spark-dynamodb

A few questions:

* What's the best way to incrementally build the RDD[Row] returned by
""buildScan""? Currently I make an RDD[Row] from each page of results, and
union them together. Does this approach seem reasonable? Any suggestions
for a better way?

* Currently my stand-alone scanner creates separate threads for each scan
segment. I could use that same approach and create threads in the Spark
driver, though ideally each scan segment would run in an executor. Any tips
on how to get the segment scanners to run on Spark executors?

Thanks,
Travis
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Apr 2016 10:40:15 -0700",Re: DynamoDB data source questions,Travis Crawford <traviscrawford@gmail.com>,"Responses inline



If the number of pages can be high (e.g. > 100), it is best to avoid using
union. The simpler way is ...

val pages = ...
sc.parallelize(pages, pages.size).flatMap { page =>
  ...
}

The above creates a task per page.

Looking at your code, you are relying on Spark's JSON inference to read the
JSON data. You would need a different thing there in order to parallelize
this. Right now you are bringing all the data into the driver and then send
them out.




I'm not too familiar with dynamo. Is segment different from the page above?



"
Rahul Tanwani <tanwanirahul@gmail.com>,"Wed, 13 Apr 2016 12:18:20 -0700 (MST)","Re: Different maxBins value for categorical and continuous features
 in RandomForest implementation.",dev@spark.apache.org,"Added https://issues.apache.org/jira/browse/SPARK-14606



--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 13 Apr 2016 19:43:36 -0400","Dataset.explain, ExplainCommand and sqlContext.executePlan twice?",dev <dev@spark.apache.org>,"Hi,

While reviewing explain(extended: Boolean) -
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L408
- I've noticed that:

1. It first creates ExplainCommand that does
sqlContext.executePlan(logicalPlan) in run
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala#L247

2. And then calls sqlContext.executePlan(explain)
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L410

Why are there two sqlContext.executePlan's? It appears that we calls
the former to execute the latter (?) I'm confused. Please explain :)
I'd appreciate.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Thu, 14 Apr 2016 11:46:55 +0900","Coding style question (about extra anonymous closure within
 functional transformations)",dev <dev@spark.apache.org>,"Hi all,

I recently noticed that actually there are some usages of functional
transformations (eg. map, foreach and etc.) with extra anonymous closure.

For example,

...map(item => {
  ...
})

which can be just simply as below:

...map { item =>
  ...
}

I wrote a regex to find all of them and corrected them for a PR (I did not
submit yet).

However, I feel a bit hesitating because only reasons I can think for this
are,

    firstly, Spark coding guides in both
https://github.com/databricks/scala-style-guide and
https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide
are not using the examples as above

    secondly, I feel like extra anonymous closure can harm performance but
I am too sure,

which I think are not persuasive enough.



To cut it short, my questions are,

1. Would this be a proper change for a PR?

2. Would there be more explicit reasons to remove extra closure not only
for coding style?


Thanks!
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Apr 2016 21:11:48 -0700","Re: Coding style question (about extra anonymous closure within
 functional transformations)",Hyukjin Kwon <gurwls223@gmail.com>,"We prefer the latter. I don't think there are performance differences
though.

It depends on how big the change is -- massive style updates can make
backports harder.



"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 14 Apr 2016 00:35:12 -0700","Re: Coding style question (about extra anonymous closure within
 functional transformations)",Hyukjin Kwon <gurwls223@gmail.com>,"I don't believe the Scala compiler understands the difference between your
two examples the same way that you do.  Looking at a few similar cases,
I've only found the bytecode produced to be the same regardless of which
style is used.


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Thu, 14 Apr 2016 17:13:44 +0900","Re: Coding style question (about extra anonymous closure within
 functional transformations)",dev <dev@spark.apache.org>,"The latter is simpler and less-typing, I think.
How about adding this as an example in these style guides?

// maropu




-- 
---
Takeshi Yamamuro
"
Hyukjin Kwon <gurwls223@gmail.com>,"Thu, 14 Apr 2016 18:38:53 +0900","Re: Coding style question (about extra anonymous closure within
 functional transformations)",Takeshi Yamamuro <linguin.m.s@gmail.com>,"Yea I agree with you all.

Just let you know, this was anyway fixed in
https://github.com/apache/spark/commit/6fc3dc8839eaed673c64ec87af6dfe24f8cebe0c





your two examples the same way that you do.  Looking at a few similar
cases, I've only found the bytecode produced to be the same regardless of
which style is used.


transformations (eg. map, foreach and etc.) with extra anonymous closure.
not submit yet).
this are,
https://github.com/databricks/scala-style-guide and
https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide
are not using the examples as above
but I am too sure,
only for coding style?
"
=?UTF-8?Q?Sergio_Ram=c3=adrez?= <sramirezga@ugr.es>,"Thu, 14 Apr 2016 12:18:50 +0200",[Streaming] Infinite delay when stopping the context,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello:

I use the stop method in my streaming programs to finish the executions 
of my experiments. However, despite of getting these messages:
/
//16/04/14 12:03:39 INFO JobGenerator: Stopping JobGenerator immediately//
//16/04/14 12:03:39 INFO RecurringTimer: Stopped timer for JobGenerator 
after time 1460628219000//
//16/04/14 12:03:39 INFO JobGenerator: Stopped JobGenerator//
//16/04/14 12:03:39 INFO ReceivedBlockTracker: Deleting batches 
ArrayBuffer()

/The thread associated to this app never ends completely. If I look for 
this process in the OS, I can see that it is still running. I get the 
same problem with both version of the stop function (gracefully and 
abrupt). The spark version used is 1.6.1.

Any clue about this problem?

Regards,

Sergio R.
"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 14 Apr 2016 12:28:34 +0000",Organizing Spark ML example packages,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Spark devs

I noticed that we now have a large number of examples for ML & MLlib in the
examples project - 57 for ML and 67 for MLLIB to be precise. This is bound
to get larger as we add features (though I know there are some PRs to clean
up duplicated examples).

What do you think about organizing them into packages to match the use case
and the structure of the code base? e.g.

org.apache.spark.examples.ml.recommendation

org.apache.spark.examples.ml.feature

and so on...

Is it worth doing? The doc pages with include_example would need updating,
and the run_example script input would just need to change the package
slightly. Did I miss any potential issue?

N
"
Travis Crawford <traviscrawford@gmail.com>,"Thu, 14 Apr 2016 15:25:49 +0000",Re: DynamoDB data source questions,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Thanks for the tips. I made some changes based on your suggestion, and now
the table scan happens on executors.
https://github.com/traviscrawford/spark-dynamodb/blob/master/src/main/scala/com/github/traviscrawford/spark/dynamodb/DynamoDBRelation.scala

sqlContext.sparkContext
    .parallelize(scanConfigs, scanConfigs.length)
    .flatMap(DynamoDBRelation.scan)

When scanning a DynamoDB table, you create one or more scan requests, which
I'll call ""segments."" Each segment provides an iterator over pages, and
each page contains a collection of items. The actual network transfer
happens when getting the next page. At that point you can iterate over
items in memory.

Based on your feedback I now parallelize a collection of configs that
describe each segment to scan, then in flatMap create the scanner and fetch
all it's items.

I'm pointed enough in the right direction to finish this up.

Thanks,
Travis





"
Imran Rashid <irashid@cloudera.com>,"Thu, 14 Apr 2016 11:24:56 -0500",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,Nezih Yigitbasi <nyigitbasi@netflix.com.invalid>,"Hi Nezih,

I just reported a somewhat similar issue, and I have a potential fix --
SPARK-14560, looks like you are already watching it :).  You can try out
that patch, you have to explicitly enable the change in behavior with
""spark.shuffle.spillAfterRead=true"".  Honestly, I don't think these issues
are the same, as I've always seen that case lead to acquiring 0 bytes,
while in your case you are requesting GBs and getting something pretty
close, so my hunch is that it is different ... but might be worth a shot to
see if it is the issue.

Turning on debug logging for TaskMemoryManager might help track the root
cause -- you'll get information on which consumers are using memory and
when there are spill attempts.  (Note that even if the patch I have for
SPARK-14560 doesn't fix your issue, it might still make those debug logs a
bit more clear, since it'll report memory used by Spillables.)

Imran


"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Thu, 14 Apr 2016 16:31:58 +0000",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,"Imran Rashid <irashid@cloudera.com>, Nezih Yigitbasi <nyigitbasi@netflix.com.invalid>","Thanks Imran. I will give it a shot when I have some time.

Nezih


"
Michael Gummelt <mgummelt@mesosphere.io>,"Thu, 14 Apr 2016 15:00:12 -0700",Re: Accessing Secure Hadoop from Mesos cluster,Tony Kinsley <tkinsley.9@gmail.com>,"DCOS Spark 1.6.1 supports kerberos.  It'll be available in DCOS 1.7, to be
released in a couple weeks.


to
he
if
ning spark
e
I
o
s
c
.
op
k


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Fri, 15 Apr 2016 00:26:29 -0700","Re: Should localProperties be inheritable? Should we change that or
 document it?",Marcin Tustin <mtustin@handybook.com>,"I think this was added a long time ago by me in order to make certain
things work for Shark (good old times ...). You are probably right that by
now some apps depend on the fact that this is inheritable, and changing
that could break them in weird ways.

Do you mind documenting this, and also add a test case?



"
Marcin Tustin <mtustin@handybook.com>,"Fri, 15 Apr 2016 06:45:37 -0400","Re: Should localProperties be inheritable? Should we change that or
 document it?",Reynold Xin <rxin@databricks.com>,"It would be a pleasure. That said, what do you think about adding the
non-inheritable feature? I think that would be a big win for everything
that doesn't specifically need Inheritability.



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 15 Apr 2016 15:01:09 +0100",BytesToBytes and unaligned memory,dev@spark.apache.org,"Hi, I'm testing Spark 2.0.0 on various architectures and have a question, 
are we sure if 
core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java 
really is attempting to use unaligned memory access (for the 
BytesToBytesMapOffHeapSuite tests specifically)?

Our JDKs on zSystems for example return false for the 
java.nio.Bits.unaligned() method and yet if I skip this check and add 
s390x to the supported architectures (for zSystems), all thirteen tests 
here pass. 

The 13 tests here all fail as we do not pass the unaligned requirement 
(but perhaps incorrectly):
core/src/test/java/org/apache/spark/unsafe/map/BytesToBytesMapOffHeapSuite.java 
and I know the unaligned checking is at 
common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java

Either our JDK's method is returning false incorrectly or this test isn't 
using unaligned memory access (so the requirement is invalid), there's no 
mention of alignment in the test itself.

Any guidance would be very much appreciated, cheers


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Chadha Pooja <Chadha.Pooja@bcg.com>,"Fri, 15 Apr 2016 14:29:04 +0000","Unable to access Resource Manager /Name Node on port 9026 / 9101 on a
 Spark EMR Cluster","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi ,


We have setup a Spark Cluster (3 node) on Amazon EMR.

We aren't able to use port 9026 and 9101 on the existing Spark EMR Cluster which are part of the Web UIs offered with Amazon EMR. I was able to use other ports like Zeppelin port, 8890, HUE etc

We checked that the security settings currently are open to everyone, and it is not an issue with security.

URLs

Hadoop ResourceManager

http://master-node-IP:9026/

Hadoop HDFS NameNode

http://master-node-IP:9101/


Errors Observed on Fiddler:
Port 9026:
[Fiddler] The connection to 'masternodeIP' failed.
Error: TimedOut (0x274c).
System.Net.Sockets.SocketException A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond <<MasternodeIP>>:9026

Port 9101:
[Fiddler] The connection to <<MasternodeIP>>: failed.
Error: TimedOut (0x274c).
System.Net.Sockets.SocketException A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond <<MasternodeIP>>:9101

Does anyone have any experiences or pointers? Appreciate your help!


Thanks!

______________________________________________________________________________
The Boston Consulting Group, Inc.
 
This e-mail message may contain confidential and/or privileged information.
If you are not an addressee or otherwise authorized to receive this message,
you should not use, copy, disclose or take any action based on this e-mail or
any information contained in the message. If you have received this material
in error, please advise the sender immediately by reply e-mail and delete this
message. Thank you.
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 15 Apr 2016 07:32:34 -0700",Re: BytesToBytes and unaligned memory,Adam Roberts <AROBERTS@uk.ibm.com>,"I assume you tested 2.0 with SPARK-12181 .

Related code from Platform.java if java.nio.Bits#unaligned() throws
exception:

      // We at least know x86 and x64 support unaligned access.
      String arch = System.getProperty(""os.arch"", """");
      //noinspection DynamicRegexReplaceableByCompiledPattern
      _unaligned = arch.matches(""^(i[3-6]86|x86(_64)?|x64|amd64)$"");

Can you give us some detail on how the code runs for JDKs on zSystems ?

Thanks


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 15 Apr 2016 16:32:47 +0100",Re: BytesToBytes and unaligned memory,Ted Yu <yuzhihong@gmail.com>,"Ted, yep I'm working from the latest code which includes that unaligned 
check, for experimenting I've modified that code to ignore the unaligned 
check (just go ahead and say we support it anyway, even though our JDK 
returns false: the return value of java.nio.Bits.unaligned()).

My Platform.java for testing contains:

private static final boolean unaligned;

static {
  boolean _unaligned;
  // use reflection to access unaligned field
  try {
    System.out.println(""Checking unaligned support"");
    Class<?> bitsClass =
      Class.forName(""java.nio.Bits"", false, 
ClassLoader.getSystemClassLoader());
    Method unalignedMethod = bitsClass.getDeclaredMethod(""unaligned"");
    unalignedMethod.setAccessible(true);
    _unaligned = Boolean.TRUE.equals(unalignedMethod.invoke(null));
    System.out.println(""Used reflection and _unaligned is: "" + 
_unaligned);
    System.out.println(""Setting to true anyway for experimenting"");
    _unaligned = true;
    } catch (Throwable t) {
      // We at least know x86 and x64 support unaligned access.
      String arch = System.getProperty(""os.arch"", """");
      //noinspection DynamicRegexReplaceableByCompiledPattern
      // We don't actually get here since we find the unaligned method OK 
and it returns false (I override with true anyway)
      // but add s390x incase we somehow fail anyway.
      System.out.println(""Checking for s390x, os.arch is: "" + arch);
      _unaligned = arch.matches(""^(i[3-6]86|x86(_64)?|x64|s390x|amd64)$"");
    }
    unaligned = _unaligned;
    System.out.println(""returning: "" + unaligned);
  }
}

Output is, as you'd expect, ""used reflection and _unaligned is false, 
setting to true anyway for experimenting"", and the tests pass.

No other problems on the platform (pending a different pull request).

Cheers,







From:   Ted Yu <yuzhihong@gmail.com>
To:     Adam Roberts/UK/IBM@IBMGB
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   15/04/2016 15:32
Subject:        Re: BytesToBytes and unaligned memory



I assume you tested 2.0 with SPARK-12181 .

Related code from Platform.java if java.nio.Bits#unaligned() throws 
exception:

      // We at least know x86 and x64 support unaligned access.
      String arch = System.getProperty(""os.arch"", """");
      //noinspection DynamicRegexReplaceableByCompiledPattern
      _unaligned = arch.matches(""^(i[3-6]86|x86(_64)?|x64|amd64)$"");

Can you give us some detail on how the code runs for JDKs on zSystems ?

Thanks

Hi, I'm testing Spark 2.0.0 on various architectures and have a question, 
are we sure if 
core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java 
really is attempting to use unaligned memory access (for the 
BytesToBytesMapOffHeapSuite tests specifically)? 

Our JDKs on zSystems for example return false for the 
java.nio.Bits.unaligned() method and yet if I skip this check and add 
s390x to the supported architectures (for zSystems), all thirteen tests 
here pass. 

The 13 tests here all fail as we do not pass the unaligned requirement 
(but perhaps incorrectly): 
core/src/test/java/org/apache/spark/unsafe/map/BytesToBytesMapOffHeapSuite.java 
and I know the unaligned checking is at 
common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java 

Either our JDK's method is returning false incorrectly or this test isn't 
using unaligned memory access (so the requirement is invalid), there's no 
mention of alignment in the test itself. 

Any guidance would be very much appreciated, cheers 


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Jonathan Kelly <jonathakamzn@gmail.com>,"Fri, 15 Apr 2016 15:36:21 +0000","Re: Unable to access Resource Manager /Name Node on port 9026 / 9101
 on a Spark EMR Cluster","Chadha Pooja <Chadha.Pooja@bcg.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Ever since emr-4.x, the service ports have been synced as much as possible
with open source, so the YARN ResourceManager UI is on port 8088, and the
NameNode UI is on port 50070. See
http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/emr-release-differences.html#d0e23719
for
more information.

~ Jonathan


R Cluster
e
d
e
d
ve
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 15 Apr 2016 08:43:05 -0700",Re: BytesToBytes and unaligned memory,Adam Roberts <AROBERTS@uk.ibm.com>,"Can you clarify whether BytesToBytesMapOffHeapSuite passed or failed with
the forced true value for unaligned ?

If the test failed, please pastebin the failure(s).

Thanks


"
Hamel Kothari <hamelkothari@gmail.com>,"Fri, 15 Apr 2016 15:44:53 +0000",Skipping Type Conversion and using InternalRows for UDF,dev <dev@spark.apache.org>,"Hi all,

So we have these UDFs which take <1ms to operate and we're seeing pretty
poor performance around them in practice, the overhead being >10ms for the
projections (this data is deeply nested with ArrayTypes and MapTypes so
that could be the cause). Looking at the logs and code for ScalaUDF, I
noticed that there are a series of projections which take place before and
after in order to make the Rows safe and then unsafe again. Is there any
way to opt out of this and input/return InternalRows to skip the
performance hit of the type conversion? It doesn't immediately appear to be
possible but I'd like to make sure that I'm not missing anything.

I suspect we could make this possible by checking if typetags in the
register function are all internal types, if they are, passing a false
value for ""needs[Input|Output]Conversion"" to ScalaUDF and then in ScalaUDF
checking for that flag to figure out if the conversion process needs to
take place. We're still left with the issue of missing a schema in the case
of outputting InternalRows, but we could expose the DataType parameter
rather than inferring it in the register function. Is there anything else
in the code that would prevent this from working?

Regards,
Hamel
"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 15 Apr 2016 16:47:57 +0100",Re: BytesToBytes and unaligned memory,Ted Yu <yuzhihong@gmail.com>,"Ted, yeah with the forced true value the tests in that suite all pass and 
I know they're being executed thanks to prints I've added

Cheers,




From:   Ted Yu <yuzhihong@gmail.com>
To:     Adam Roberts/UK/IBM@IBMGB
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   15/04/2016 16:43
Subject:        Re: BytesToBytes and unaligned memory



Can you clarify whether BytesToBytesMapOffHeapSuite passed or failed with 
the forced true value for unaligned ?

If the test failed, please pastebin the failure(s).

Thanks

Ted, yep I'm working from the latest code which includes that unaligned 
check, for experimenting I've modified that code to ignore the unaligned 
check (just go ahead and say we support it anyway, even though our JDK 
returns false: the return value of java.nio.Bits.unaligned()). 

My Platform.java for testing contains: 

private static final boolean unaligned; 

static { 
  boolean _unaligned; 
  // use reflection to access unaligned field 
  try { 
    System.out.println(""Checking unaligned support""); 
    Class<?> bitsClass = 
      Class.forName(""java.nio.Bits"", false, 
ClassLoader.getSystemClassLoader()); 
    Method unalignedMethod = bitsClass.getDeclaredMethod(""unaligned""); 
    unalignedMethod.setAccessible(true); 
    _unaligned = Boolean.TRUE.equals(unalignedMethod.invoke(null)); 
    System.out.println(""Used reflection and _unaligned is: "" + 
_unaligned); 
    System.out.println(""Setting to true anyway for experimenting""); 
    _unaligned = true; 
    } catch (Throwable t) { 
      // We at least know x86 and x64 support unaligned access. 
      String arch = System.getProperty(""os.arch"", """"); 
      //noinspection DynamicRegexReplaceableByCompiledPattern 
      // We don't actually get here since we find the unaligned method OK 
and it returns false (I override with true anyway) 
      // but add s390x incase we somehow fail anyway. 
      System.out.println(""Checking for s390x, os.arch is: "" + arch); 
      _unaligned = arch.matches(""^(i[3-6]86|x86(_64)?|x64|s390x|amd64)$""); 

    } 
    unaligned = _unaligned; 
    System.out.println(""returning: "" + unaligned); 
  } 
} 

Output is, as you'd expect, ""used reflection and _unaligned is false, 
setting to true anyway for experimenting"", and the tests pass. 

No other problems on the platform (pending a different pull request). 

Cheers, 







From:        Ted Yu <yuzhihong@gmail.com> 
To:        Adam Roberts/UK/IBM@IBMGB 
Cc:        ""dev@spark.apache.org"" <dev@spark.apache.org> 
Date:        15/04/2016 15:32 
Subject:        Re: BytesToBytes and unaligned memory 




I assume you tested 2.0 with SPARK-12181 . 

Related code from Platform.java if java.nio.Bits#unaligned() throws 
exception: 

      // We at least know x86 and x64 support unaligned access. 
      String arch = System.getProperty(""os.arch"", """"); 
      //noinspection DynamicRegexReplaceableByCompiledPattern 
      _unaligned = arch.matches(""^(i[3-6]86|x86(_64)?|x64|amd64)$""); 

Can you give us some detail on how the code runs for JDKs on zSystems ? 

Thanks 


Hi, I'm testing Spark 2.0.0 on various architectures and have a question, 
are we sure if 
core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java 
really is attempting to use unaligned memory access (for the 
BytesToBytesMapOffHeapSuite tests specifically)? 

Our JDKs on zSystems for example return false for the 
java.nio.Bits.unaligned() method and yet if I skip this check and add 
s390x to the supported architectures (for zSystems), all thirteen tests 
here pass. 

The 13 tests here all fail as we do not pass the unaligned requirement 
(but perhaps incorrectly): 
core/src/test/java/org/apache/spark/unsafe/map/BytesToBytesMapOffHeapSuite.java 
and I know the unaligned checking is at 
common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java 

Either our JDK's method is returning false incorrectly or this test isn't 
using unaligned memory access (so the requirement is invalid), there's no 
mention of alignment in the test itself. 

Any guidance would be very much appreciated, cheers 


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU 



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 15 Apr 2016 09:01:55 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",dev <dev@spark.apache.org>,"After some collaboration with other community members, we have created a
initial draft for Spark Extras which is available for review at

https://docs.google.com/document/d/1zRFGG4414LhbKlGbYncZ13nyX34Rw4sfWhZRA5YBtIE/edit?usp=sharing

We would like to invite other community members to participate in the
project, particularly the Spark Committers and PMC (feel free to express
interest and I will update the proposal). Another option here is just to
give ALL Spark committers write access to ""Spark Extras"".


We also have couple asks from the Spark PMC :

- Permission to use ""Spark Extras"" as the project name. We already checked
this with Apache Brand Management, and the recommendation was to discuss
and reach consensus with the Spark PMC.

- We would also want to check with the Spark PMC that, in case of
successfully creation of  ""Spark Extras"", if the PMC would be willing to
continue the development of the remaining connectors that stayed in Spark
2.0 codebase in the ""Spark Extras"" project.


Thanks in advance, and we welcome any feedback around this proposal before
we present to the Apache Board for consideration.







-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Sean Owen <sowen@cloudera.com>,"Fri, 15 Apr 2016 17:18:08 +0100","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Luciano Resende <luckbr1975@gmail.com>,"Why would this need to be an ASF project of its own? I don't think
it's possible to have a yet another separate ""Spark Extras"" TLP (?)

There is already a project to manage these bits of code on Github. How
about all of the interested parties manage the code there, under the
same process, under the same license, etc?

I'm not against calling it Spark Extras myself but I wonder if that
needlessly confuses the situation. They aren't part of the Spark TLP
on purpose, so trying to give it some special middle-ground status
might just be confusing. The thing that comes to mind immediately is
""Connectors for Apache Spark"", spark-connectors, etc.



---------------------------------------------------------------------


"
Chris Fregly <chris@fregly.com>,"Fri, 15 Apr 2016 12:33:24 -0400","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Sean Owen <sowen@cloudera.com>,"and how does this all relate to the existing 1-and-a-half-class citizen
known as spark-packages.org?

support for this citizen is buried deep in the Spark source (which was
always a bit odd, in my opinion):

https://github.com/apache/spark/search?utf8=%E2%9C%93&q=spark-packages





-- 

*Chris Fregly*
Principal Data Solutions Engineer
IBM Spark Technology Center, San Francisco, CA
http://spark.tc | http://advancedspark.com
"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 15 Apr 2016 09:34:02 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Sean Owen <sowen@cloudera.com>,"

This whole discussion started when some of the connectors were moved from
Apache to Github, which makes a statement that The ""Spark Governance"" of
the bits is something very valuable by the community, consumers, and other
companies that are consuming open source code. Being an Apache project also
allows the project to use and share the Apache infrastructure to run the
project.


I know the name might be confusing, but I also think that the projects have
a very big synergy, more like sibling projects, where ""Spark Extras""
extends the Spark community and develop/maintain components for, and pretty
much only for, Apache Spark.  Based on your comment above, if making the
project ""Spark-Extras"" a more acceptable name, I believe this is ok as well.

I also understand that the Spark PMC might have concerns with branding, and
that's why we are inviting all members of the Spark PMC to join the project
and help oversee and manage the project.






-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Cody Koeninger <cody@koeninger.org>,"Fri, 15 Apr 2016 11:34:23 -0500","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Sean Owen <sowen@cloudera.com>,"Given that not all of the connectors were removed, I think this
creates a weird / confusing three tier system

1. connectors in the official project's spark/extras or spark/external
2. connectors in ""Spark Extras""
3. connectors in some random organization's github




---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 15 Apr 2016 09:35:31 -0700",Re: BytesToBytes and unaligned memory,Adam Roberts <AROBERTS@uk.ibm.com>,"I am curious if all Spark unit tests pass with the forced true value for
unaligned.
If that is the case, it seems we can add s390x to the known architectures.

It would also give us some more background if you can describe how
java.nio.Bits#unaligned()
is implemented on s390x.

Josh / Andrew / Davies / Ryan are more familiar with related code. It would
be good to hear what they think.

Thanks


"
Koert Kuipers <koert@tresata.com>,"Fri, 15 Apr 2016 12:38:18 -0400",ClassFormatError in latest spark 2 SNAPSHOT build,"""dev@spark.apache.org"" <dev@spark.apache.org>","not sure why, but i am getting this today using spark 2 snapshots...
i am on java 7 and scala 2.11

16/04/15 12:35:46 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 15,
localhost): java.lang.ClassFormatError: Duplicate field name&signature in
class file
org/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificMutableProjection
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at
org.codehaus.janino.ByteArrayClassLoader.findClass(ByteArrayClassLoader.java:66)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at
org.apache.spark.sql.catalyst.expressions.GeneratedClass.generate(Unknown
Source)
    at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$create$2.apply(GenerateMutableProjection.scala:140)
    at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$create$2.apply(GenerateMutableProjection.scala:139)
    at
org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:178)
    at
org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:197)
    at
org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:39)
    at
org.apache.spark.sql.execution.aggregate.SortBasedAggregate$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregate.scala:80)
    at
org.apache.spark.sql.execution.aggregate.SortBasedAggregate$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregate.scala:71)
    at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:768)
    at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:768)
    at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
    at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:72)
    at org.apache.spark.scheduler.Task.run(Task.scala:86)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:239)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$
"
Sean Owen <sowen@cloudera.com>,"Fri, 15 Apr 2016 17:38:08 +0100","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Chris Fregly <chris@fregly.com>,"I think this meant to be understood as a community site, and as a
directory listing pointers to third-party projects. It's not a project
of its own, and not part of Spark itself, with no special status. At
least, I think that's how it should be presented and pretty much seems
to come across that way.


---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 15 Apr 2016 09:39:01 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Cody Koeninger <cody@koeninger.org>,"
Agree Cody, and I think this is one of the goals of ""Spark Extras"",
centralize the development of these connectors under one central place at
Apache, and that's why one of our asks is to invite the Spark PMC to
continue developing the remaining connectors that stayed in Spark proper,
in ""Spark Extras"". We will also discuss some process policies on enabling
lowering the bar to allow proposal of these other github extensions to be
part of ""Spark Extras"" while also considering a way to move code to a
maintenance mode location.


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 15 Apr 2016 16:41:10 +0000","Re: Creating Spark Extras project, was Re: SPARK-13843 and future
 of streaming backends","Luciano Resende <luckbr1975@gmail.com>,
        Cody Koeninger
	<cody@koeninger.org>","Yeah in support of this statement I think that my primary interest in
this Spark Extras and the good work by Luciano here is that anytime we
take bits out of a code base and ‚Äúmove it to GitHub‚Äù I see a bad precedent
being set.

Creating this project at the ASF creates a synergy between *Apache Spark*
which is *at the ASF*.

We welcome comments and as Luciano said, this is meant to invite and be
open to those in the Apache Spark PMC to join and help.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 168-519, Mailstop: 168-527
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Director, Information Retrieval and Data Science Group (IRDS)
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
WWW: http://irds.usc.edu/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++










On 4/15/16, 9:39 AM, ""Luciano Resende"" <luckbr1975@gmail.com> wrote:

>
>
>On Fri, Apr 15, 2016 at 9:34 AM, Cody Koeninger 
><cody@koeninger.org> wrote:
>
>Given that not all of the connectors were removed, I think this
>creates a weird / confusing three tier system
>
>1. connectors in the official project's spark/extras or spark/external
>2. connectors in ""Spark Extras""
>3. connectors in some random organization's github
>
>
>
>
>
>
>
>Agree Cody, and I think this is one of the goals of ""Spark Extras"", centralize the development of these connectors under one central place at Apache, and that's why one of our asks is to invite the Spark PMC to continue developing the remaining connectors
> that stayed in Spark proper, in ""Spark Extras"". We will also discuss some process policies on enabling lowering the bar to allow proposal of these other github extensions to be part of ""Spark Extras"" while also considering a way to move code to a maintenance
> mode location.
>
> 
>
>
>-- 
>Luciano Resende
>http://twitter.com/lresende1975
>http://lresende.blogspot.com/
>
>
>
>
"
Reynold Xin <rxin@databricks.com>,"Fri, 15 Apr 2016 09:41:37 -0700",Re: ClassFormatError in latest spark 2 SNAPSHOT build,Koert Kuipers <koert@tresata.com>,"Can you post the generated code?

df.queryExecution.debug.codeGen()

(Or something similar to that)


"
,"Fri, 15 Apr 2016 18:44:06 +0200","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",dev@spark.apache.org,"+1

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 15 Apr 2016 09:47:30 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends","""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Anybody is free and welcomed to create another ASF project, but I don't
think ""Spark extras"" is a good name. It unnecessarily creates another tier
of code that ASF is ""endorsing"".


e a bad precedent
"
Sean Owen <sowen@cloudera.com>,"Fri, 15 Apr 2016 17:50:34 +0100","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Luciano Resende <luckbr1975@gmail.com>,"
This also grants special status to a third-party project. It's not
clear this should be *the* official unofficial third-party Spark
project over some other one. If something's to be blessed, it should
be in the Spark project.

And why isn't it in the Spark project? the argument was that these
bits were not used and pretty de minimis as code. It's not up to me or
anyone else to tell you code X isn't useful to you. But arguing X
should be a TLP asserts it is substantial and of broad interest, since
there's non-zero effort for volunteers to deal with it. I am not sure
I've heard anyone argue that -- or did I miss it? because removing
bits of unused code happens all the time and isn't a bad precedent or
even unusual.

It doesn't actually enable any more cooperation than is already
possible with any other project (like Kafka, Mesos, etc). You can run
the same governance model anywhere you like. I realize literally being
operated under the ASF banner is something different.

What I hear here is a proposal to make an unofficial official Spark
project as a TLP, that begins with these fairly inconsequential
extras. I question the value of that on its face. Example: what goes
into this project? deleted Spark code only? or is this a glorified
""contrib"" folder with a lower and somehow different bar determined by
different people?

And at that stage... is it really helping to give that special status?

---------------------------------------------------------------------


"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 15 Apr 2016 16:51:21 +0000","Re: Creating Spark Extras project, was Re: SPARK-13843 and future
 of streaming backends",Reynold Xin <rxin@databricks.com>,"Hey Reynold,

Thanks. Getting to the heart of this, I think that this project would
be successful if the Apache Spark PMC decided to participate and there
was some overlap. As much as I think it would be great to stand up another
project, the goal here from Luciano and crew (myself included) would be
to suggest it‚Äôs just as easy to start an Apache Incubator project to 
manage ‚Äúextra‚Äù pieces of Apache Spark code outside of the release cycle
and the other reasons stated that it made sense to move this code out of
the code base. This isn‚Äôt a competing effort to some code on GitHub that
was moved out of Apache source control from Apache Spark - it‚Äôs meant to 
be an enabler to suggest that code could be managed here just as easily
(see the difference?)

Let me know what you think thanks Reynold.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 168-519, Mailstop: 168-527
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Director, Information Retrieval and Data Science Group (IRDS)
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
WWW: http://irds.usc.edu/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++









On 4/15/16, 9:47 AM, ""Reynold Xin"" <rxin@databricks.com> wrote:

>
>
>
>Anybody is free and welcomed to create another ASF project, but I don't think ""Spark extras"" is a good name. It unnecessarily creates another tier of code that ASF is ""endorsing"".
>On Friday, April 15, 2016, Mattmann, Chris A (3980) <chris.a.mattmann@jpl.nasa.gov> wrote:
>
>Yeah in support of this statement I think that my primary interest in
>this Spark Extras and the good work by Luciano here is that anytime we
>take bits out of a code base and ‚Äúmove it to GitHub‚Äù I see a bad precedent
>being set.
>
>Creating this project at the ASF creates a synergy between *Apache Spark*
>which is *at the ASF*.
>
>We welcome comments and as Luciano said, this is meant to invite and be
>open to those in the Apache Spark PMC to join and help.
>
>Cheers,
>Chris
>
>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>Chris Mattmann, Ph.D.
>Chief Architect
>Instrument Software and Science Data Systems Section (398)
>NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
>Office: 168-519, Mailstop: 168-527
>Email: 
>chris.a.mattmann@nasa.gov <javascript:;>
>WWW:  http://sunset.usc.edu/~mattmann/
>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>Director, Information Retrieval and Data Science Group (IRDS)
>Adjunct Associate Professor, Computer Science Department
>University of Southern California, Los Angeles, CA 90089 USA
>WWW: http://irds.usc.edu/
>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>
>
>
>
>
>
>
>
>
>
>On 4/15/16, 9:39 AM, ""Luciano Resende"" <luckbr1975@gmail.com <javascript:;>> wrote:
>
>>
>>
>>On Fri, Apr 15, 2016 at 9:34 AM, Cody Koeninger
>><cody@koeninger.org <javascript:;>> wrote:
>>
>>Given that not all of the connectors were removed, I think this
>>creates a weird / confusing three tier system
>>
>>1. connectors in the official project's spark/extras or spark/external
>>2. connectors in ""Spark Extras""
>>3. connectors in some random organization's github
>>
>>
>>
>>
>>
>>
>>
>>Agree Cody, and I think this is one of the goals of ""Spark Extras"", centralize the development of these connectors under one central place at Apache, and that's why one of our asks is to invite the Spark PMC to continue developing the remaining connectors
>> that stayed in Spark proper, in ""Spark Extras"". We will also discuss some process policies on enabling lowering the bar to allow proposal of these other github extensions to be part of ""Spark Extras"" while also considering a way to move code to a maintenance
>> mode location.
>>
>>
>>
>>
>>--
>>Luciano Resende
>>http://twitter.com/lresende1975
>>http://lresende.blogspot.com/
>>
>>
>>
>>
>
>
>
"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 15 Apr 2016 16:52:43 +0000","Re: Creating Spark Extras project, was Re: SPARK-13843 and future
 of streaming backends","Sean Owen <sowen@cloudera.com>, Luciano Resende <luckbr1975@gmail.com>","Yeah, so it‚Äôs the *Apache Spark* project. Just to clarify.
Not once did you say Apache Spark below.






On 4/15/16, 9:50 AM, ""Sean Owen"" <sowen@cloudera.com> wrote:

>On Fri, Apr 15, 2016 at 5:34 PM, Luciano Resende <luckbr1975@gmail.com> wrote:
>> I know the name might be confusing, but I also think that the projects have
>> a very big synergy, more like sibling projects, where ""Spark Extras"" extends
>> the Spark community and develop/maintain components for, and pretty much
>> only for, Apache Spark.  Based on your comment above, if making the project
>> ""Spark-Extras"" a more acceptable name, I believe this is ok as well.
>
>This also grants special status to a third-party project. It's not
>clear this should be *the* official unofficial third-party Spark
>project over some other one. If something's to be blessed, it should
>be in the Spark project.
>
>And why isn't it in the Spark project? the argument was that these
>bits were not used and pretty de minimis as code. It's not up to me or
>anyone else to tell you code X isn't useful to you. But arguing X
>should be a TLP asserts it is substantial and of broad interest, since
>there's non-zero effort for volunteers to deal with it. I am not sure
>I've heard anyone argue that -- or did I miss it? because removing
>bits of unused code happens all the time and isn't a bad precedent or
>even unusual.
>
>It doesn't actually enable any more cooperation than is already
>possible with any other project (like Kafka, Mesos, etc). You can run
>the same governance model anywhere you like. I realize literally being
>operated under the ASF banner is something different.
>
>What I hear here is a proposal to make an unofficial official Spark
>project as a TLP, that begins with these fairly inconsequential
>extras. I question the value of that on its face. Example: what goes
>into this project? deleted Spark code only? or is this a glorified
>""contrib"" folder with a lower and somehow different bar determined by
>different people?
>
>And at that stage... is it really helping to give that special status?

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Cody Koeninger <cody@koeninger.org>,"Fri, 15 Apr 2016 12:16:21 -0500","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Sean Owen <sowen@cloudera.com>,"100% agree with Sean & Reynold's comments on this.

Adding this as a TLP would just cause more confusion as to ""official""
endorsement.




---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 15 Apr 2016 10:38:23 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends","""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","
e a bad precedent


Can't agree more !





In addition, this will give all the ""goodness "" of being an Apache project
from a user/consumer point of view compared to a general  github project.




This would definitely be something worthwhile to explore.
+1

Regards
Mridul



"
Michael Armbrust <michael@databricks.com>,"Fri, 15 Apr 2016 11:47:40 -0700",Re: Skipping Type Conversion and using InternalRows for UDF,Hamel Kothari <hamelkothari@gmail.com>,"This would also probably improve performance:
https://github.com/apache/spark/pull/9565


"
Alexander Pivovarov <apivovarov@gmail.com>,"Fri, 15 Apr 2016 12:40:01 -0700","Will not store rdd_16_4383 as it would require dropping another block
 from the same RDD",dev <dev@spark.apache.org>,"I run Spark 1.6.1 on YARN   (EMR-4.5.0)

I call RDD.count on MEMORY_ONLY_SER cached RDD (spark.serializer is
KryoSerializer)

after count task is done I noticed that Spark UI shows that RDD Fraction
Cached is 6% only
Size in Memory = 65.3 GB

I looked at Executors stderr on Spark UI and saw lots of messages like

16/04/15 19:08:03 INFO storage.MemoryStore: Will not store rdd_16_4383
as it would require dropping another block from the same RDD
16/04/15 19:08:03 WARN storage.MemoryStore: Not enough space to cache
rdd_16_4383 in memory! (computed 1462.4 MB so far)
16/04/15 19:08:03 INFO storage.MemoryStore: Memory use = 11.0 KB
(blocks) + 33.8 GB (scratch space shared across 17 tasks(s)) = 33.8
GB. Storage limit = 33.8 GB.
16/04/15 19:08:06 INFO storage.MemoryStore: Will not store rdd_16_4306
as it would require dropping another block from the same RDD
16/04/15 19:08:06 WARN storage.MemoryStore: Not enough space to cache
rdd_16_4306 in memory! (computed 1920.6 MB so far)
16/04/15 19:08:06 INFO storage.MemoryStore: Memory use = 11.0 KB
(blocks) + 33.8 GB (scratch space shared across 17 tasks(s)) = 33.8
GB. Storage limit = 33.8 GB.



But the cluster has memory to cache 3 RDDs like that

spark.executor.instances - 100

spark.executor.memory - 48524M

Storage Memory on each executor - 33.8 GB

Executors Memory: 67.2 GB Used (3.3 TB Total)


If my RDD takes 65.3 GB in memory storage when RDD Fraction Cached =
6%  then total size in memory should be about 1.1 TB


The cluster has 3.3 TB total storage memory and only 1 application is
running now (the RDD is the first RDD to cache in my programm)

Why Spark can not store entire RDD in memory?


BTW. Previous Spark 1.5.2 stores 100% of the RDD


Should I switch to legacy mode? spark.memory.useLegacyMode=true


   -
"
Wei-Shun Lo <raliclo@gmail.com>,"Fri, 15 Apr 2016 17:35:44 -0700","Re: Unable to access Resource Manager /Name Node on port 9026 / 9101
 on a Spark EMR Cluster",Chadha Pooja <Chadha.Pooja@bcg.com>,"Hi Chanda,

You may want to check by using nmap to check whether the port and service
is correctly started locally.
ex. nmap localhost

If the port is already successfully internally, it might be related to the
outbound/inbound traffic control in your security group setting.

Just fyi.



R Cluster
e
d
e
d
ve



-- 
Best Luck,
Ralic Lo
*------------------------------------------------------------------------*
*---*
Phone: 408-609-7628
Email: RalicLo@gmail.com
*------------------------------------------------------------------------*
*---*
*INSPIRATION FILLS THE GAP OF KNOWLEDGE  !*
"
Steve Loughran <stevel@hortonworks.com>,"Sat, 16 Apr 2016 11:46:54 +0000","Re: Creating Spark Extras project, was Re: SPARK-13843 and future
 of streaming backends","""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>, ""Luciano
 Resende"" <luckbr1975@gmail.com>, Cody Koeninger <cody@koeninger.org>","




On 15/04/2016, 17:41, ""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov> wrote:

>Yeah in support of this statement I think that my primary interest in
>this Spark Extras and the good work by Luciano here is that anytime we
>take bits out of a code base and ‚Äúmove it to GitHub‚Äù I see a bad precedent
>being set.
>
>Creating this project at the ASF creates a synergy between *Apache Spark*
>which is *at the ASF*.
>
>We welcome comments and as Luciano said, this is meant to invite and be
>open to those in the Apache Spark PMC to join and help.
>
>Cheers,
>Chris

As one of the people named, here's my rationale:

Throwing stuff into github creates that world of branches, and its no longer something that could be managed through the ASF, where managed is: governance, participation and a release process that includes auditing dependencies, code-signoff, etc,


As an example, there's a mutant hive JAR which spark uses, that's something which currently evolved between my repo and Patrick Wendell's; now that Josh Rosen has taken on the bold task of ""trying to move spark and twill to Kryo 3"", he's going to own that code, and now the reference branch will move somewhere else.

In contrast, if there was an ASF location for this, then it'd be something anyone with commit rights could maintain and publish

(actually, I've just realised life is hard here as the hive is a fork of ASF hive ‚Äîreally the spark branch should be a separate branch in Hive's own repo ... But the concept is the same: those bits of the codebase which are core parts of the spark project should really live in or near it)


If everyone on the spark commit list gets write access to this extras repo, moving things is straightforward. Release wise, things could/should be in sync.

If there's a risk, its the eternal problem of the contrib/ dir .... Stuff ends up there that never gets maintained. I don't see that being any worse than if things were thrown to the wind of a thousand github repos: at least now there'd be a central issue tracking location.
"
Evan Chan <velvia.github@gmail.com>,"Sat, 16 Apr 2016 17:38:09 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Steve Loughran <stevel@hortonworks.com>,"Hi folks,

Sorry to join the discussion late.  I had a look at the design doc
earlier in this thread, and it was not mentioned what types of
projects are the targets of this new ""spark extras"" ASF umbrella....

Is the desire to have a maintained set of spark-related projects that
keep pace with the main Spark development schedule?  Is it just for
streaming connectors?  what about data sources, and other important
projects in the Spark ecosystem?

I'm worried that this would relegate spark-packages to third tier
status, and the promotion of a select set of committers, and the
project itself, to top level ASF status (a la Arrow) would create a
further split in the community.

-Evan

ote:
e a bad precedent
ger something that could be managed through the ASF, where managed is: governance, participation and a release process that includes auditing dependencies, code-signoff, etc,
ng which currently evolved between my repo and Patrick Wendell's; now that Josh Rosen has taken on the bold task of ""trying to move spark and twill to Kryo 3"", he's going to own that code, and now the reference branch will move somewhere else.
g anyone with commit rights could maintain and publish
ASF hive ‚Äîreally the spark branch should be a separate branch in Hive's own repo ... But the concept is the same: those bits of the codebase which are core parts of the spark project should really live in or near it)
o, moving things is straightforward. Release wise, things could/should be in sync.
 ends up there that never gets maintained. I don't see that being any worse than if things were thrown to the wind of a thousand github repos: at least now there'd be a central issue tracking location.

---------------------------------------------------------------------


"
Evan Chan <velvia.github@gmail.com>,"Sat, 16 Apr 2016 17:47:17 -0700",Using local-cluster mode for testing Spark-related projects,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey folks,

I'd like to use local-cluster mode in my Spark-related projects to
test Spark functionality in an automated way in a simulated local
cluster.    The idea is to test multi-process things in a much easier
fashion than setting up a real cluster.   However, getting this up and
running in a separate project (I'm using Scala 2.10 and ScalaTest) is
nontrivial.   Does anyone have any suggestions to get up and running?

This is what I've observed so far (I'm testing against 1.5.1, but
suspect this would apply equally to 1.6.x):

- SPARK_SCALA_VERSION needs to be set
missing.  For example, build an assembly jar of all your deps.  Java
class directory hierarchies don't seem to work with the setJars(...).

How does Spark's internal scripts make it possible to run
local-cluster mode and set up all the class paths correctly?   And, is
it possible to mimic this setup for external Spark projects?

thanks,
Evan

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Sat, 16 Apr 2016 22:42:36 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Evan Chan <velvia.github@gmail.com>,"

The proposal draft below has some more details on what type of projects,
but in summary, ""Spark-Extras"" would be a good place for any of these
components you mentioned.

https://docs.google.com/document/d/1zRFGG4414LhbKlGbYncZ13nyX34Rw4sfWhZRA5YBtIE/edit?usp=sharing




Owen answered a similar question about spark-packages earlier on this
thread, but while ""Spark-Extras"" would a place in Apache for collaboration
on the development of these extensions, they might still be published to
spark-packages as they existing streaming connectors are today.


As for the select set of committers, we have invited all Spark committers
to be committers on the project, and I have updated the project proposal
with the existing set of active Spark committers ( that have committed in
the last one year)


see a bad
k*
nd
ch
f
Hive's own
e
y
at



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Hyukjin Kwon <gurwls223@gmail.com>,"Sun, 17 Apr 2016 15:06:09 +0900","Question about Scala style, explicit typing within transformation
 functions and anonymous val.",dev <dev@spark.apache.org>,"Hi all,

First of all, I am sorry that this is relatively trivial and too minor but
I just want to be clear on this and careful for the more PRs in the future.

Recently, I have submitted a PR (https://github.com/apache/spark/pull/12413)
about Scala style and this was merged. In this PR, I changed

1.

from

.map(item => {
  ...
})

to

.map { item =>
  ...
}



2.
from

words.foreachRDD { (rdd: RDD[String], time: Time) => ...

to

words.foreachRDD { (rdd, time) => ...



3.

from

.map { x =>
  function(x)
}

to

.map(function(_))


My question is, I think it looks 2. and 3. are arguable (please see the
discussion in the PR).
I agree that I might not have to change those in the future but I just
wonder if I should revert 2. and 3..

FYI,
- The usage of 2. is pretty rare.
- 3. is pretty a lot. but the PR corrects ones like above only when the val
within closure looks obviously meaningless (such as x or a) and with only
single line.

I would appreciate that if you add some comments and opinions on this.

Thanks!
"
Reynold Xin <rxin@apache.org>,"Sat, 16 Apr 2016 23:12:13 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Luciano Resende <luckbr1975@gmail.com>,"First, really thank you for leading the discussion.

I am concerned that it'd hurt Spark more than it helps. As many others have
pointed out, this unnecessarily creates a new tier of connectors or 3rd
party libraries appearing to be endorsed by the Spark PMC or the ASF. We
can alleviate this concern by not having ""Spark"" in the name, and the
project proposal and documentation should label clearly that this is not
affiliated with Spark.

Also Luciano - assuming you are interested in creating a project like this
and find a home for the connectors that were removed, I find it surprising
that few of the initially proposed PMC members have actually contributed
much to the connectors, and people that have contributed a lot were left
out. I am sure that is just an oversight.




5YBtIE/edit?usp=sharing
n
 see a bad
e
:
and
nch
 in Hive's
ch
d
ny
 at
"
Reynold Xin <rxin@databricks.com>,"Sat, 16 Apr 2016 23:48:28 -0700","Re: Question about Scala style, explicit typing within transformation
 functions and anonymous val.",Hyukjin Kwon <gurwls223@gmail.com>,"Hi Hyukjin,

Thanks for asking.

For 1 the change is almost always better.

For 2 it depends on the context. In general if the type is not obvious, it
helps readability to explicitly declare them.

For 3 again it depends on context.


So while it is a good idea to change 1 to reflect a more consistent code
base (and maybe we should codify it), it is almost always a bad idea to
change 2 and 3 just for the sake of changing them.




"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 16 Apr 2016 23:54:30 -0700","Re: Question about Scala style, explicit typing within transformation
 functions and anonymous val.",Reynold Xin <rxin@databricks.com>,"FWIW, 3 should work as just `.map(function)`.


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sun, 17 Apr 2016 15:56:14 +0900","Re: Question about Scala style, explicit typing within transformation
 functions and anonymous val.",Mark Hamstra <mark@clearstorydata.com>,"Hi Mark,

I know but that could harm readability. AFAIK, for this reason, that is not
(or rarely) used in Spark.

2016-04-17 15:54 GMT+09:00 Mark Hamstra <mark@clearstorydata.com>:

"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 17 Apr 2016 00:02:07 -0700","Re: Question about Scala style, explicit typing within transformation
 functions and anonymous val.",Hyukjin Kwon <gurwls223@gmail.com>,"I actually find my version of 3 more readable than the one with the `_`,
which looks too much like a partially applied function.  It's a minor
issue, though.


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Sun, 17 Apr 2016 21:37:23 +0900",Re: Using local-cluster mode for testing Spark-related projects,Evan Chan <velvia.github@gmail.com>,"Hi,
Is this a bad idea to create `SparkContext` with a `local-cluster` mode by
yourself like '
https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/ShuffleSuite.scala#L55
'?

// maropu




-- 
---
Takeshi Yamamuro
"
Evan Chan <velvia.github@gmail.com>,"Sun, 17 Apr 2016 08:28:34 -0700",Re: Using local-cluster mode for testing Spark-related projects,Takeshi Yamamuro <linguin.m.s@gmail.com>,"What I want to find out is how to run tests like Spark's with
local-cluster, just like that suite, but in your own projects.   Has
anyone done this?


---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Sun, 17 Apr 2016 09:16:33 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Reynold Xin <rxin@apache.org>,"

I really thought we could use the Spark name (e.g. similar to
spark-packages) as this project is really aligned and dedicated to curating
extensions to Apache Spark and that's why we were inviting Spark PMC
members to join the new project PMC so that Apache Spark has the necessary
oversight and influence on the project direction. I understand folks have
concerns with the name, and thus we will start looking into name
alternatives unless there is any way I could address the community concerns
around this.


s
g
Reynold, thanks for your concern, we are not leaving anyone out, we took
the following criteria to identify initial PMC/Committers list as described
on the first e-mail on this thread:

   - Spark Committers and Apache Members can request to participate as PMC
members
   - All active spark committers (committed on the last one year) will have
write access to the project (committer access)
   - Other committers can request to become committers.
   - Non committers would be added based on meritocracy after the start of
the project.

Based on this criteria, all people that have expressed interest in joining
the project PMC has been added to it, but I don't feel comfortable adding
names to it at my will. And I have updated the list of committers and
currently we have the following on the draft proposal:


Initial PMC


   -

   Luciano Resende (lresende AT apache DOT org) (Apache Member)
   -

   Chris Mattmann (mattmann  AT apache DOT org) (Apache Member, Apache
   board member)
   -

   Steve Loughran (stevel AT apache DOT org) (Apache Member)
   -

   -

   Marcelo Masiero Vanzin (vanzin AT apache DOT org) (Apache Spark
   committer)
   -

   Sean R. Owen (srowen AT apache DOT org) (Apache Member and Spark PMC)
   -

   Mridul Muralidharan (mridulm80 AT apache DOT org) (Apache Spark PMC)


Initial Committers (write access to active Spark committers that have
committed in the last one year)


   -

   Andy Konwinski (andrew AT apache DOT org) (Apache Spark)
   -

   Andrew Or (andrewor14 AT apache DOT org) (Apache Spark)
   -

   Ankur Dave (ankurdave AT apache DOT org) (Apache Spark)
   -

   Davies Liu (davies AT apache DOT org) (Apache Spark)
   -

   DB Tsai (dbtsai AT apache DOT org) (Apache Spark)
   -

   Haoyuan Li (haoyuan AT apache DOT org) (Apache Spark)
   -

   Ram Sriharsha (harsha AT apache DOT org) (Apache Spark)
   -

   Herman van H√∂vell (hvanhovell AT apache DOT org) (Apache Spark)
   -

   Imran Rashid (irashid AT apache DOT org) (Apache Spark)
   -

   Joseph Kurata Bradley (jkbradley AT apache DOT org) (Apache Spark)
   -

   Josh Rosen (joshrosen AT apache DOT org) (Apache Spark)
   -

   Kay Ousterhout (kayousterhout AT apache DOT org) (Apache Spark)
   -

   Cheng Lian (lian AT apache DOT org) (Apache Spark)
   -

   Mark Hamstra (markhamstra AT apache DOT org) (Apache Spark)
   -

   Michael Armbrust (marmbrus AT apache DOT org) (Apache Spark)
   -

   Matei Alexandru Zaharia (matei AT apache DOT org) (Apache Spark)
   -

   Xiangrui Meng (meng AT apache DOT org) (Apache Spark)
   -

   Prashant Sharma (prashant AT apache DOT org) (Apache Spark)
   -

   Patrick Wendell (pwendell AT apache DOT org) (Apache Spark)
   -

   Reynold Xin (rxin AT apache DOT org) (Apache Spark)
   -

   Sanford Ryza (sandy AT apache DOT org) (Apache Spark)
   -

   Kousuke Saruta (sarutak AT apache DOT org) (Apache Spark)
   -

   Shivaram Venkataraman (shivaram AT apache DOT org) (Apache Spark)
   -

   Tathagata Das (tdas AT apache DOT org) (Apache Spark)
   -

   Thomas Graves  (tgraves AT apache DOT org) (Apache Spark)
   -

   Wenchen Fan (wenchen AT apache DOT org) (Apache Spark)
   -

   Yin Huai (yhuai AT apache DOT org) (Apache Spark)
   - Shixiong Zhu (zsxwing AT apache DOT org) (Apache Spark)



BTW, It would be really good to have you on the PMC as well, and any others
that volunteer based on the criteria above. May I add you as PMC to the new
project proposal ?



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Jon Maurer <tritab@gmail.com>,"Sun, 17 Apr 2016 11:51:01 -0500",Re: Using local-cluster mode for testing Spark-related projects,Evan Chan <velvia.github@gmail.com>,"Take a look at spark testing base.
https://github.com/holdenk/spark-testing-base/blob/master/README.md

"
Rahul Tanwani <tanwanirahul@gmail.com>,"Sun, 17 Apr 2016 10:27:49 -0700 (MST)",Impact of STW GC events for the driver JVM on overall cluster,dev@spark.apache.org,"Hi Devs,

In case of stop the world GC events on the driver JVM, since all the
application threads will be stopped, there won't be any new task scheduled /
launched on the executors. In cases where the full collection is happening,
the applications threads may be stopped for a long time, and if the running
tasks are small, entire cluster will be idle till the GC is over and
application threads are resumed.

Is my understanding in this regard correct?



--

---------------------------------------------------------------------


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Mon, 18 Apr 2016 03:26:23 +0900",Recent Jenkins always fails in specific two tests,dev@spark.apache.org,"I realized that recent Jenkins among different pull requests always fails 
in the following two tests
""SPARK-8020: set sql conf in spark conf""
""SPARK-9757 Persist Parquet relation with decimal column""

Here are examples.
https://github.com/apache/spark/pull/11956 (consoleFull: 
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/56058/consoleFull
)
https://github.com/apache/spark/pull/12259 (consoleFull: 
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/56056/consoleFull
)
https://github.com/apache/spark/pull/12450 (consoleFull: 
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/56051/consoleFull
)
https://github.com/apache/spark/pull/12453 (consoleFull: 
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/56050/consoleFull
)
https://github.com/apache/spark/pull/12257 (consoleFull: 
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/56061/consoleFull
)
https://github.com/apache/spark/pull/12451 (consoleFull: 
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/56045/consoleFull
)

I have just realized that the latest master also causes the same two 
failures at amplab Jenkins. 
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.6/627/
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.6/625/

Since they seem to have some relationships with failures in recent pull 
requests, I created two JIRA entries.
https://issues.apache.org/jira/browse/SPARK-14689
https://issues.apache.org/jira/browse/SPARK-14690

Best regards,
Kazuaki Ishizaki

"
Reynold Xin <rxin@databricks.com>,"Sun, 17 Apr 2016 11:32:24 -0700",Re: Impact of STW GC events for the driver JVM on overall cluster,Rahul Tanwani <tanwanirahul@gmail.com>,"Your understanding is correct. If the driver is stuck in GC, then during
that period it cannot schedule any tasks.



"
Rahul Tanwani <tanwanirahul@gmail.com>,"Sun, 17 Apr 2016 11:47:01 -0700 (MST)",Re: Impact of STW GC events for the driver JVM on overall cluster,dev@spark.apache.org,"Does that not mean, GC settings with concurrent collectors should be
preferred over parallel collectors atleast on the driver side? If so, why
not have concurrent collectors specified by default when the driver JVM is
launched without any overriding on this part?



--

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Sun, 17 Apr 2016 17:51:49 -0400","Re: Question about Scala style, explicit typing within transformation
 functions and anonymous val.",Mark Hamstra <mark@clearstorydata.com>,"i find version 3 without the _ also more readable


"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 18 Apr 2016 10:22:41 +0900",Re: Recent Jenkins always fails in specific two tests,Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>,"+1

Yea, I am facing this problem as well,
https://github.com/apache/spark/pull/12452

I thought they are spurious because the tests are passed in my local.



2016-04-18 3:26 GMT+09:00 Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>:

"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 18 Apr 2016 07:10:07 +0530",Re: Possible deadlock in registering applications in the recovery mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi guys,

Any update on this?

Best





-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Marcin Tustin <mtustin@handybook.com>,"Sun, 17 Apr 2016 22:52:35 -0400",Re: Recent Jenkins always fails in specific two tests,Hyukjin Kwon <gurwls223@gmail.com>,"Also hitting this: https://github.com/apache/spark/pull/12455.





-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Reynold Xin <rxin@databricks.com>,"Sun, 17 Apr 2016 20:32:31 -0700",Re: Possible deadlock in registering applications in the recovery mode,Niranda Perera <niranda.perera@gmail.com>,"I haven't looked closely at this, but I think your proposal makes sense.



"
Evan Chan <velvia.github@gmail.com>,"Sun, 17 Apr 2016 23:56:32 -0700",Re: Using local-cluster mode for testing Spark-related projects,Jon Maurer <tritab@gmail.com>,"Jon,  Thanks.   I think I've figured it out, actually.   It's really
simple, one needs to simply set spark.executor.extraClassPath to the
current value of the java class path (java.class.path system
property).   Also, to not use HiveContext, which gives errors about
initializing a Derby database multiple times.


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Mon, 18 Apr 2016 07:23:38 +0000",Re: Code freeze?,"Reynold Xin <rxin@databricks.com>, Sean Owen <sowen@cloudera.com>","Is there a list of Jiras to be considered for 2.0? I would really like to
get https://issues.apache.org/jira/browse/SPARK-13745 in so that Big Endian
platforms are not broken.

Cheers,


"
Sean Owen <sowen@cloudera.com>,"Mon, 18 Apr 2016 09:43:11 +0100",Re: Code freeze?,Pete Robbins <robbinspg@gmail.com>,"FWIW, here's what I do to look at JIRA's answer to this:

1) Go download http://almworks.com/jiraclient/overview.html
2) Set up a query for ""target = 2.0.0 and status = Open, In Progress, Reopened""
3) Set up sub-queries for bugs vs non-bugs, and for critical, blocker and other

Right now there are 172 issues open for 2.0.0. 40 are bugs, 4 of which
are critical and 1 of which is a blocker. 9 non-bugs are blockers, 5
critical.

JIRA info is inevitably noisy, but now is a good time to make this
info meaningful so we have some shared reference about the short-term
plan.

What I suggest we do now is ...

a) un-target anything that wasn't targeted to 2.0.0 by a committer
b) committers un-target or re-target anything they know isn't that
important for 2.0.0 (thanks jkbradley)
c) focus on bugs > features, high priority > low priority this week
d) see where we are next week, repeat

I suggest we simply have ""no blockers"" as an exit criteria, with a
strong pref for ""no critical bugs either"".

It's a major release, so taking a little extra time to get it all done
comfortably is both possible and unusually important. A couple weeks
indeed might be realistic for an RC, but it really depends on burndown
more than anything.


---------------------------------------------------------------------


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Mon, 18 Apr 2016 14:49:50 +0100",Re: BytesToBytes and unaligned memory,Ted Yu <yuzhihong@gmail.com>,"Ted, yes with the forced true value all tests pass, we use the unaligned 
check in 15 other suites.

Our java.nio.Bits.unaligned() function checks that the detected os.arch 
value matches a list of known implementations (not including s390x).

We could add it to the known architectures in the catch block but this 
won't make a difference here as because we call unaligned() OK (no 
exception is thrown), we don't reach the architecture checking stage 
anyway.

I see in org.apache.spark.memory.MemoryManager that unaligned support is 
required for off-heap memory in Tungsten (perhaps incorrectly if no code 
ever exercises it in Spark?). Instead of having a requirement should we 
instead log a warning once that this is likely to lead to slow 
performance? What's the rationale for supporting unaligned memory access: 
it's my understanding that it's typically very slow, are there any design 
docs or perhaps a JIRA where I can learn more? 

Will run a simple test case exercising unaligned memory access for Linux 
on Z (without using Spark) and can also run the tests claiming to require 
unaligned memory access on a platform where unaligned memory access is 
definitely not supported for shorts/ints/longs. 

if these tests continue to pass then I think the Spark tests don't 
exercise unaligned memory access, cheers


 




From:   Ted Yu <yuzhihong@gmail.com>
To:     Adam Roberts/UK/IBM@IBMGB
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   15/04/2016 17:35
Subject:        Re: BytesToBytes and unaligned memory



I am curious if all Spark unit tests pass with the forced true value for 
unaligned.
If that is the case, it seems we can add s390x to the known architectures.

It would also give us some more background if you can describe 
how java.nio.Bits#unaligned() is implemented on s390x.

Josh / Andrew / Davies / Ryan are more familiar with related code. It 
would be good to hear what they think.

Thanks

Ted, yeah with the forced true value the tests in that suite all pass and 
I know they're being executed thanks to prints I've added 

Cheers, 




From:        Ted Yu <yuzhihong@gmail.com> 
To:        Adam Roberts/UK/IBM@IBMGB 
Cc:        ""dev@spark.apache.org"" <dev@spark.apache.org> 
Date:        15/04/2016 16:43 
Subject:        Re: BytesToBytes and unaligned memory 



Can you clarify whether BytesToBytesMapOffHeapSuite passed or failed with 
the forced true value for unaligned ? 

If the test failed, please pastebin the failure(s). 

Thanks 


Ted, yep I'm working from the latest code which includes that unaligned 
check, for experimenting I've modified that code to ignore the unaligned 
check (just go ahead and say we support it anyway, even though our JDK 
returns false: the return value of java.nio.Bits.unaligned()). 

My Platform.java for testing contains: 

private static final boolean unaligned; 

static { 
  boolean _unaligned; 
  // use reflection to access unaligned field 
  try { 
    System.out.println(""Checking unaligned support""); 
    Class<?> bitsClass = 
      Class.forName(""java.nio.Bits"", false, 
ClassLoader.getSystemClassLoader()); 
    Method unalignedMethod = bitsClass.getDeclaredMethod(""unaligned""); 
    unalignedMethod.setAccessible(true); 
    _unaligned = Boolean.TRUE.equals(unalignedMethod.invoke(null)); 
    System.out.println(""Used reflection and _unaligned is: "" + 
_unaligned); 
    System.out.println(""Setting to true anyway for experimenting""); 
    _unaligned = true; 
    } catch (Throwable t) { 
      // We at least know x86 and x64 support unaligned access. 
      String arch = System.getProperty(""os.arch"", """"); 
      //noinspection DynamicRegexReplaceableByCompiledPattern 
      // We don't actually get here since we find the unaligned method OK 
and it returns false (I override with true anyway) 
      // but add s390x incase we somehow fail anyway. 
      System.out.println(""Checking for s390x, os.arch is: "" + arch); 
      _unaligned = arch.matches(""^(i[3-6]86|x86(_64)?|x64|s390x|amd64)$""); 

    } 
    unaligned = _unaligned; 
    System.out.println(""returning: "" + unaligned); 
  } 
} 

Output is, as you'd expect, ""used reflection and _unaligned is false, 
setting to true anyway for experimenting"", and the tests pass. 

No other problems on the platform (pending a different pull request). 

Cheers, 







From:        Ted Yu <yuzhihong@gmail.com> 
To:        Adam Roberts/UK/IBM@IBMGB 
Cc:        ""dev@spark.apache.org"" <dev@spark.apache.org> 
Date:        15/04/2016 15:32 
Subject:        Re: BytesToBytes and unaligned memory 




I assume you tested 2.0 with SPARK-12181 . 

Related code from Platform.java if java.nio.Bits#unaligned() throws 
exception: 

      // We at least know x86 and x64 support unaligned access. 
      String arch = System.getProperty(""os.arch"", """"); 
      //noinspection DynamicRegexReplaceableByCompiledPattern 
      _unaligned = arch.matches(""^(i[3-6]86|x86(_64)?|x64|amd64)$""); 

Can you give us some detail on how the code runs for JDKs on zSystems ? 

Thanks 


Hi, I'm testing Spark 2.0.0 on various architectures and have a question, 
are we sure if 
core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java 
really is attempting to use unaligned memory access (for the 
BytesToBytesMapOffHeapSuite tests specifically)? 

Our JDKs on zSystems for example return false for the 
java.nio.Bits.unaligned() method and yet if I skip this check and add 
s390x to the supported architectures (for zSystems), all thirteen tests 
here pass. 

The 13 tests here all fail as we do not pass the unaligned requirement 
(but perhaps incorrectly): 
core/src/test/java/org/apache/spark/unsafe/map/BytesToBytesMapOffHeapSuite.java 
and I know the unaligned checking is at 
common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java 

Either our JDK's method is returning false incorrectly or this test isn't 
using unaligned memory access (so the requirement is invalid), there's no 
mention of alignment in the test itself. 

Any guidance would be very much appreciated, cheers 


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU 



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU 



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 18 Apr 2016 06:58:51 -0700",Re: BytesToBytes and unaligned memory,Adam Roberts <AROBERTS@uk.ibm.com>,"bq. run the tests claiming to require unaligned memory access on a platform
where unaligned memory access is definitely not supported for
shorts/ints/longs.

That would help us understand interactions on s390x platform better.


"
shane knapp <sknapp@berkeley.edu>,"Mon, 18 Apr 2016 10:02:28 -0700",[build system] issue w/jenkins,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","right now it looks like we're having problems connection to jenkins
through our firewall.  i'm currently looking in to this and will let
everyone know immediately when it's been resolved.

thanks in advance for your patience...

shane

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 18 Apr 2016 10:22:37 -0700",Re: [build system] issue w/jenkins,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","for now, you can log in to jenkins by ignoring the http reverse proxy:
https://hadrian.ist.berkeley.edu/jenkins/

this still doesn't allow for things like the pull request builder and
whatnot to run...  i'm still digging in to this.

thanks,

shane


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 18 Apr 2016 19:23:30 +0200",Implicit from ProcessingTime to scala.concurrent.duration.Duration?,dev <dev@spark.apache.org>,"Hi,

While working with structured streaming (aka SparkSQL Streams :)) I
thought about adding

implicit def toProcessingTime(duration: Duration) = ProcessingTime(duration)

What do you think?

I think it'd improve the API:

.trigger(ProcessingTime(10 seconds))

vs

.trigger(10 seconds)

(since it's not a release feature I didn't mean to file an issue in
JIRA - please guide if needed).

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 18 Apr 2016 10:38:50 -0700",Re: [build system] issue w/jenkins,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","somehow DNS, internal to berkeley, got borked and the redirect failed.
we've hard-coded in some entries in to /etc/hosts, and re-ordered our
nameservers, and are still trying to figure out what happened.

anyways, we're back:
https://amplab.cs.berkeley.edu/jenkins/


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 10:44:15 -0700",Re: Implicit from ProcessingTime to scala.concurrent.duration.Duration?,Jacek Laskowski <jacek@japila.pl>,"The problem with this is that we might introduce event time based trigger
in the future, and then it would be more confusing...


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 18 Apr 2016 19:46:11 +0200",Re: Implicit from ProcessingTime to scala.concurrent.duration.Duration?,Reynold Xin <rxin@databricks.com>,"When you say ""in the future"", do you have any specific timeframe in
mind? You got me curious :)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Andrew Ray <ray.andrew@gmail.com>,"Mon, 18 Apr 2016 12:46:28 -0500",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"While you can't automatically push the limit *through* the join, we could
push it *into* the join (stop processing after generating 10 records). I
believe that is what Rajesh is suggesting.


a
mit
t>
"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 18 Apr 2016 10:48:34 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Evan Chan <velvia@gmail.com>,"Evan,

As long as you meet the criteria we discussed on this thread, you are
welcome to join.

Having said that, I have already seen other contributors that are very
active on some of connectors but are not Apache Committers yet, and i
wanted to be fair, and also avoid using the project as an avenue to bring
new committers to Apache.



ep
hat if
:
.
ng
y
ns
ot
ed
C
f
g
er)
)
o


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 10:48:50 -0700",Re: Implicit from ProcessingTime to scala.concurrent.duration.Duration?,Jacek Laskowski <jacek@japila.pl>,"Nope. It is unclear whether they would be useful enough or not. But when
designing APIs we always need to anticipate future changes.


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 18 Apr 2016 20:05:22 +0200",More elaborate toString for StreamExecution?,dev <dev@spark.apache.org>,"Hi,

I'd love having a more elaborate toString to StreamExecution:

scala> sqlContext.streams.active.foreach(println)
Continuous Query - memStream [state = ACTIVE]
Continuous Query - hello2 [state = ACTIVE]
Continuous Query - hello [state = ACTIVE]

Any work in this area? trigger is something it could have.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 11:09:41 -0700",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Andrew Ray <ray.andrew@gmail.com>,"I could be wrong but I think we currently do that through whole stage
codegen. After processing every row on the stream side, the generated code
for broadcast join checks whether it has hit the limit or not (through this
thing called shouldStop).

It is not the most optimal solution, because a single stream side row might
output multiple hits, but it is usually not a problem.



y
:
s
nt>
"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 12:02:15 -0700",auto closing pull requests that have been inactive > 30 days?,"""dev@spark.apache.org"" <dev@spark.apache.org>","We have hit a new high in open pull requests: 469 today. While we can
certainly get more review bandwidth, many of these are old and still open
for other reasons. Some are stale because the original authors have become
busy and inactive, and some others are stale because the committers are not
sure whether the patch would be useful, but have not rejected the patch
explicitly. We can cut down the signal to noise ratio by closing pull
requests that have been inactive for greater than 30 days, with a nice
message. I just checked and this would close ~ half of the pull requests.

For example:

""Thank you for creating this pull request. Since this pull request has been
inactive for 30 days, we are automatically closing it. Closing the pull
request does not remove it from history and will retain all the diff and
review comments. If you have the bandwidth and would like to continue
pushing this forward, please reopen it. Thanks again!""
"
Soumitra Johri <soumitra.siddharth@gmail.com>,"Mon, 18 Apr 2016 15:04:54 -0400",inter spark application communication,"user <user@spark.apache.org>, dev@spark.apache.org","Hi,

I have two applications : App1 and App2.
App2.

What would be the best way to send data from the 5 App1 instances to the
single App2 instance ?

Right now I am using Kafka to send data from one spark application to the
spark application  but the setup doesn't seem right and I hope there is a
better way to do this.

Warm Regards
Soumitra
"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 12:16:45 -0700",more uniform exception handling?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Josh's pull request <https://github.com/apache/spark/pull/12433> on rpc
exception handling got me to think ...

In my experience, there have been a few things related exceptions that
created a lot of trouble for us in production debugging:

1. Some exception is thrown, but is caught by some try/catch that does not
do any logging nor rethrow.
2. Some exception is thrown, but is caught by some try/catch that does not
do any logging, but do rethrow. But the original exception is now masked.
2. Multiple exceptions are logged at different places close to each other,
but we don't know whether they are caused by the same problem or not.


To mitigate some of the above, here's an idea ...

(1) Create a common root class for all the exceptions (e.g. call it
SparkException) used in Spark. We should make sure every time we catch an
exception from a 3rd party library, we rethrow them as SparkException (a
lot of places already do that). In SparkException's constructor, log the
exception and the stacktrace.

(2) SparkException has a monotonically increasing ID, and this ID appears
in the exception error message (say at the end).


I think (1) will eliminate most of the cases that an exception gets
swallowed. The main downside I can think of is we might log an exception
multiple times. However, I'd argue exceptions should be rare, and it is not
that big of a deal to log them twice or three times. The unique ID (2) can
help us correlate exceptions if they appear multiple times.

Thoughts?
"
Cody Koeninger <cody@koeninger.org>,"Mon, 18 Apr 2016 14:17:15 -0500",Re: auto closing pull requests that have been inactive > 30 days?,Reynold Xin <rxin@databricks.com>,"For what it's worth, I have definitely had PRs that sat inactive for
more than 30 days due to committers not having time to look at them,
but did eventually end up successfully being merged.

I guess if this just ends up being a committer ping and reopening the
PR, it's fine, but I don't know if it really addresses the underlying
issue.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 18 Apr 2016 12:20:29 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Cody Koeninger <cody@koeninger.org>,"I had one PR which got merged after 3 months.

If the inactivity was due to contributor, I think it can be closed after 30
days.
But if the inactivity was due to lack of review, the PR should be kept open.


"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 12:20:49 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Cody Koeninger <cody@koeninger.org>,"Cody,

Thanks for commenting. ""inactive"" here means no code push nor comments. So
any ""ping"" would actually keep the pr in the open queue. Getting
auto-closed also by no means indicate the pull request can't be reopened.


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 18 Apr 2016 12:33:18 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Ted Yu <yuzhihong@gmail.com>,"Personally I'd rather err on the side of keeping PRs open, but I understand
wanting to keep the open PRs limited to ones which have a reasonable chance
of being merged.

What about if we filtered for non-mergeable PRs or instead left a comment
asking the author to respond if they are still available to move the PR
forward - and close the ones where they don't respond for a week?

Just a suggestion.


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Marcin Tustin <mtustin@handybook.com>,"Mon, 18 Apr 2016 15:36:29 -0400",Re: auto closing pull requests that have been inactive > 30 days?,,"+1 and at the same time maybe surface a report to this list of PRs which
need committer action and have only had submitters responding to pings in
the last 30 days?



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy"
Ted Yu <yuzhihong@gmail.com>,"Mon, 18 Apr 2016 12:41:29 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Holden Karau <holden@pigscanfly.ca>,"bq. close the ones where they don't respond for a week

Does this imply that the script understands response from human ?

Meaning, would the script use some regex which signifies that the
contributor is willing to close the PR ?

If the contributor is willing to close, why wouldn't he / she do it
him/herself ?


"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 12:43:33 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Ted Yu <yuzhihong@gmail.com>,"Part of it is how difficult it is to automate this. We can build a perfect
engine with a lot of rules that understand everything. But the more
complicated rules we need, the more unlikely for any of these to happen. So
I'd rather do this and create a nice enough message to tell contributors
sometimes mistake happen but the cost to reopen is approximately zero (i.e.
click a button on the pull request).



"
Ted Yu <yuzhihong@gmail.com>,"Mon, 18 Apr 2016 12:46:54 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Reynold Xin <rxin@databricks.com>,"
If not, the cost is not close to zero.
Meaning, some potentially useful PRs would never see the light of day.

My two cents.


"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 12:52:59 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Ted Yu <yuzhihong@gmail.com>,"The cost of ""reopen"" is close to zero, because it is just clicking a
button. I think you were referring to the cost of closing the pull request,
and you are assuming people look at the pull requests that have been
inactive for a long time. That seems equally likely (or unlikely) as
committers looking at the recently closed pull requests.

In either case, most pull requests are scanned through by us when they are
first open, and if they are important enough, usually they get merged
quickly or a target version is set in JIRA. We can definitely improve that
by making it more explicit.




"
Michael Segel <msegel_hadoop@hotmail.com>,"Mon, 18 Apr 2016 12:54:01 -0700",Re: inter spark application communication,Soumitra Johri <soumitra.siddharth@gmail.com>,"have you thought about Akka? 

What are you trying to send? Why do you want them to talk to one another? 

of App2.
the single App2 instance ?
the spark application  but the setup doesn't seem right and I hope there is a better way to do this.


---------------------------------------------------------------------


"
Evan Chan <velvia.github@gmail.com>,"Mon, 18 Apr 2016 13:18:52 -0700",Re: more uniform exception handling?,Reynold Xin <rxin@databricks.com>,"+1000.

Especially if the UI can help correlate exceptions, and we can reduce
some exceptions.

There are some exceptions which are in practice very common, such as
the nasty ClassNotFoundException, that most folks end up spending tons
of time debugging.
"
Mark Grover <mark@apache.org>,"Mon, 18 Apr 2016 13:51:01 -0700",YARN Shuffle service and its compatibility,dev <dev@spark.apache.org>,"Hi all,
If you don't use Spark on YARN, you probably don't need to read further.

Here's the *user scenario*:
There are going to be folks who may be interested in running two versions
of Spark (say Spark 1.6.x and Spark 2.x) on the same YARN cluster.

And, here's the *problem*:
That's all fine, should work well. However, there's one problem that
relates to the YARN shuffle service
<https://github.com/apache/spark/blob/master/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java>.
This service is run by the YARN Node Managers on all nodes of the cluster
that have YARN NMs as an auxillary service
<https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html>
.

The key question here is -
Option A:  Should the user be running 2 shuffle services - one for Spark
1.6.x and one for Spark 2.x?
OR
Option B: Should the user be running only 1 shuffle service that services
both the Spark 1.6.x and Spark 2.x installs? This will likely have to be
the Spark 1.6.x shuffle service (while ensuring it's forward compatible
with Spark 2.x).

*Discussion of above options:*
A few things to note about the shuffle service:
1. Looking at the commit history, there aren't a whole of lot of changes
that go into the shuffle service, rarely ones that are incompatible.
There's only one incompatible change
<https://issues.apache.org/jira/browse/SPARK-12130> that's been made to the
shuffle service, as far as I can tell, and that too, seems fairly cosmetic.
2. Shuffle services for 1.6.x and 2.x serve very similar purpose (to
provide shuffle blocks) and can easily be just one service that does it,
even on a YARN cluster that runs both Spark 1.x and Spark 2.x.
3. The shuffle service is not version-spaced. This means that, the way the
code is currently, if we were to drop the jars for Spark1 and Spark2's
shuffle service in YARN NM's classpath, YARN NM won't be able to start both
services. It would arbitrarily pick one service to start (based on what
appears on the classpath first). Also, the service name is hardcoded
<https://github.com/apache/spark/blob/master/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java#L96>
in Spark code and that name is also not version-spaced.

Option A is arguably cleaner but it's more operational overhead and some
code relocation/shading/version-spacing/name-spacing to make it work (due
to #3 above), potentially to not a whole lot of value (given #2 above).

Option B is simpler, lean and more operationally efficient. However, that
requires that we as a community, keep Spark 1's shuffle service forward
compatible with Spark 2 i.e. don't break compatibility between Spark1's and
Spark2's shuffle service. We could even add a test (mima?) to assert that
during the life time of Spark2. If we do go down that way, we should revert
SPARK-12130 <https://issues.apache.org/jira/browse/SPARK-12130> - the only
backwards incompatible change made to Spark2 shuffle service so far.

My personal vote goes towards Option B and I think reverting SPARK-12130 is
ok. What do others think?

Thanks!
Mark
"
Sean Busbey <busbey@cloudera.com>,"Mon, 18 Apr 2016 15:52:54 -0500",Re: auto closing pull requests that have been inactive > 30 days?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Having a PR closed, especially if due to committers not having hte
bandwidth to check on things, will be very discouraging to new folks.
Doubly so for those inexperienced with opensource. Even if the message
says ""feel free to reopen for so-and-so reason"", new folks who lack
confidence are going to see reopening as ""pestering"" and busy folks
are going to see it as a clear indication that their work is not even
valuable enough for a human to give a reason for closing. In either
case, the cost of reopening is substantially higher than that button
press.

How about we start by keeping a report of ""at-risk"" PRs that have been
stale for 30 days to make it easier for committers to look at the prs
that have been long inactive?




-- 
busbey

---------------------------------------------------------------------


"
Marcin Tustin <mtustin@handybook.com>,"Mon, 18 Apr 2016 16:53:50 -0400",Re: YARN Shuffle service and its compatibility,Mark Grover <mark@apache.org>,"I'm good with option B at least until it blocks something utterly wonderful
(like shuffles are 10x faster).



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 13:53:56 -0700",Re: YARN Shuffle service and its compatibility,Mark Grover <mark@apache.org>,"That's not the only one. For example, the hash shuffle manager has been off
by default since Spark 1.2, and we'd like to remove it in 2.0:
https://github.com/apache/spark/pull/12423

How difficult it is to just change the package name to say v2?




"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 18 Apr 2016 13:59:37 -0700",Re: YARN Shuffle service and its compatibility,Reynold Xin <rxin@databricks.com>,"
If I understand things correctly, Mark's option B (running a single
shuffle service, the one from the older Spark release) would still
work, wouldn't it?

You'd run into problems when Spark adds a new shuffle manager that is
not known to the old shuffle service, though. Perhaps at that time we
should investigate making the shuffle service more agnostic to the
app's shuffle manager.

-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 14:02:33 -0700",Re: YARN Shuffle service and its compatibility,Marcelo Vanzin <vanzin@cloudera.com>,"Yea I re-read the email again. It'd work in this case.

The bigger problem is that it is much easier to maintain backward
compatibility rather than dictating forward compatibility. For example, as
Marcin said, if we come up with a slightly different shuffle layout to
improve shuffle performance, we wouldn't be able to do that if we want to
allow Spark 1.6 shuffle service to read something generated by Spark 2.1.





"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 18 Apr 2016 15:05:04 -0700",Re: YARN Shuffle service and its compatibility,Reynold Xin <rxin@databricks.com>,"
And I think that's really what Mark is proposing. Basically, ""don't
intentionally break backwards compatibility unless it's really
required"" (e.g. SPARK-12130). That would allow option B to work.

If a new shuffle manager is created, then neither option A nor option
B would really work. Moving all the shuffle-related classes to a
different package, to support option A, would be really messy. At that
point, you're better off maintaining the new shuffle service outside
of YARN, which is rather messy too.

The best would be if the shuffle service didn't really need to
understand the shuffle manager, and could find files regardless; I'm
not sure how feasible that is, though.

-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 15:09:19 -0700",Re: YARN Shuffle service and its compatibility,Marcelo Vanzin <vanzin@cloudera.com>,"Got it. So Mark is pushing for ""best-effort"" support.

IIUC, the reason for that PR is that they found the string comparison to
increase the size in large shuffles. Maybe we should add the ability to
support the short name to Spark 1.6.2?



"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 18 Apr 2016 15:23:33 -0700",Re: YARN Shuffle service and its compatibility,Reynold Xin <rxin@databricks.com>,"
Is that something that really yields noticeable gains in performance?

If it is, it seems like it would be simple to allow executors register
with the full class name, and map the long names to short names in the
shuffle service itself.

You could even get fancy and have different ExecutorShuffleInfo
implementations for each shuffle service, with an abstract
""getBlockData"" method that gets called instead of the current if/else
in ExternalShuffleBlockResolver.java.

-- 
Marcelo

---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Mon, 18 Apr 2016 17:28:39 -0700",Re: YARN Shuffle service and its compatibility,"Marcelo Vanzin <vanzin@cloudera.com>, lianhuiwang09@gmail.com","Thanks for responding, Reynold, Marcelo and Marcin.


Yeah, that's exactly what Option B is proposing.

I also don't think it'd make a huge difference to go back to full class
name but I have explicitly added Lianhui to this thread, who worked on
SPARK-12130, so he can correct me if I am blantantly wrong.

And, even then, we could keep the Spark1 and Spark2 shuffle services
compatible by doing mapping of short-long names or Abstract getBlockData
implementation, if we decide it's necessary.

Mark


"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 19 Apr 2016 08:56:15 +0800",Re: auto closing pull requests that have been inactive > 30 days?,Sean Busbey <busbey@cloudera.com>,"It would be better to have a specific technical reason why this PR should
be closed, either the implementation is not good or the problem is not
valid, or something else. That will actually help the contributor to shape
their codes and reopen the PR again. Otherwise reasons like ""feel free to
reopen for so-and-so reason"" is actually discouraging and no difference
than directly close the PR.

Just my two cents.

Thanks
Jerry



"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 19 Apr 2016 11:25:33 +0900",Re: auto closing pull requests that have been inactive > 30 days?,Saisai Shao <sai.sai.shao@gmail.com>,"I also think this might not have to be closed only because it is inactive.


How about closing issues after 30 days when a committer's comment is added
at the last without responses from the author?


IMHO, If the committers are not sure whether the patch would be useful,
then I think they should leave some comments why they are not sure, not
just ignoring.

Or, simply they could ask the author to prove that the patch is useful or
safe with some references and tests.


I think it might be nicer than that users are supposed to keep pinging.
**Personally**, apparently, I am sometimes a bit worried if pinging
multiple times can be a bit annoying.



2016-04-19 9:56 GMT+09:00 Saisai Shao <sai.sai.shao@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 18 Apr 2016 19:30:38 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Hyukjin Kwon <gurwls223@gmail.com>,"During the months of November / December, the 30 day period should be
relaxed.

Some people(at least in US) may take extended vacation during that time.

For Chinese developers, Spring Festival would bear similar circumstance.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 19 Apr 2016 03:46:17 +0000",Re: auto closing pull requests that have been inactive > 30 days?,"Ted Yu <yuzhihong@gmail.com>, Hyukjin Kwon <gurwls223@gmail.com>","Relevant: https://github.com/databricks/spark-pr-dashboard/issues/1

A lot of this was discussed a while back when the PR Dashboard was first
introduced, and several times before and after that as well. (e.g. August
2014
<http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-stale-PRs-td8015.html>
)

If there is not enough momentum to build the tooling that people are
discussing here, then perhaps Reynold's suggestion is the most practical
one that is likely to see the light of day.

I think asking committers to be more active in commenting on PRs is
theoretically the correct thing to do, but impractical. I'm not a
committer, but I would guess that most of them are already way
overcommitted (ha!) and asking them to do more just won't yield results.

We've had several instances in the past where we all tried to rally
<https://mail-archives.apache.org/mod_mbox/spark-dev/201412.mbox/%3CCAOhmDzeR4cG_wXgKTOxsG8s34KrQEzYgjFZDOYMgu9VhYJBRDg@mail.gmail.com%3E>
and be more proactive about giving feedback, closing PRs, and nudging
contributors who have gone silent. My observation is that the level of
energy required to ""properly"" curate PR activity in that way is simply not
sustainable. People can do it for a few weeks and then things revert to the
way they are now.

Perhaps the missing link that would make this sustainable is better
tooling. If you think so and can sling some Javascript, you might want to
contribute to the PR Dashboard <https://spark-prs.appspot.com/>.

Perhaps the missing link is something else: A different PR review process;
more committers; a higher barrier to contributing; a combination thereof;
etc...

Also relevant: http://danluu.com/discourage-oss/

By the way, some people noted that closing PRs may discourage contributors.
I think our open PR count alone is very discouraging. Under what
circumstances would you feel encouraged to open a PR against a project that
has hundreds of open PRs, some from many, many months ago
<https://github.com/apache/spark/pulls?q=is%3Apr+is%3Aopen+sort%3Aupdated-asc>
?

Nick


2016ÎÖÑ 4Ïõî 18Ïùº (Ïõî) Ïò§ÌõÑ 10:30, Ted Yu <yuzhihong@gmail.com>ÎãòÏù¥ ÏûëÏÑ±:

:
is
o
e
d
e
:
it
a
nd
d
"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 19 Apr 2016 13:14:48 +0900",Re: auto closing pull requests that have been inactive > 30 days?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I don't think asking committers to be more active is impractical. I am not
too sure if other projects apply the same rules here but

I think if a project is being more popular, then I think it is appropriate
that there should be more committers or they are asked to be more active.


In addition, I believe there are a lot of PRs waiting for committer's
comments.


If committers are too busy to review a PR, then I think they better ask
authors to provide the evidence to decide, maybe with a message such as

""I am currently too busy to review or decide. Could you please add
some evidence/benchmark/performance
test or survey for demands?""


If the evidence is not enough or not easy to see, then they can ask to
simplify the evidence or make a proper conclusion, maybe with a message
such as

""I think the evidence is not enough/trustable because .... Could you
please simplify/provide
some more evidence?"".



Or, I think they can be manually closed with a explicit message such as

""This is closed for now because we are not sure for this patch because..""


I think they can't be closed only because it is ""expired"" with a copy and
pasted message.



2016-04-19 12:46 GMT+09:00 Nicholas Chammas <nicholas.chammas@gmail.com>:

-PRs-td8015.html>
DzeR4cG_wXgKTOxsG8s34KrQEzYgjFZDOYMgu9VhYJBRDg@mail.gmail.com%3E>
t
he
;
ct
ed-asc>
, Ted Yu <yuzhihong@gmail.com>ÎãòÏù¥ ÏûëÏÑ±:
 is
to
l
e
n
a
ed
a
t
?
e
I
 a
e
t
o
"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 19 Apr 2016 12:16:57 +0800",Re: auto closing pull requests that have been inactive > 30 days?,Nicholas Chammas <nicholas.chammas@gmail.com>,"contributors. I think our open PR count alone is very discouraging. Under
what circumstances would you feel encouraged to open a PR against a project
that has hundreds of open PRs, some from many, many months ago
<https://github.com/apache/spark/pulls?q=is%3Apr+is%3Aopen+sort%3Aupdated-asc>
?

I think the original meaning of ""discouraging contributors"" is  closing
without specific technical reasons, or just lack of bandwidth. These PRs
may not be so important for committers/maintainers, but for individual
contributor especially new open source guy a simple fix for a famous
project means a lot. We actually can have other solutions like setting a
high bar beforehand to reduce the PR number.

Thanks
Jerry




-PRs-td8015.html>
DzeR4cG_wXgKTOxsG8s34KrQEzYgjFZDOYMgu9VhYJBRDg@mail.gmail.com%3E>
t
he
;
ct
ed-asc>
, Ted Yu <yuzhihong@gmail.com>ÎãòÏù¥ ÏûëÏÑ±:
 is
to
l
e
n
a
ed
a
t
?
e
I
 a
e
t
o
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 18 Apr 2016 21:22:16 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Hyukjin Kwon <gurwls223@gmail.com>,"bq. there should be more committers or they are asked to be more active.

Bingo.

bq. they can't be closed only because it is ""expired"" with a copy and
pasted message.

+1


e
vidence/benchmark/performance
t
e-PRs-td8015.html>
mDzeR4cG_wXgKTOxsG8s34KrQEzYgjFZDOYMgu9VhYJBRDg@mail.gmail.com%3E>
ot
the
n
r
ect
ted-asc>
0, Ted Yu <yuzhihong@gmail.com>ÎãòÏù¥ ÏûëÏÑ±:
.
.
sure,
.
m is
 to
el
.
ge
n
en
s
,
m>
 a
 ?
he
o
 I
t
d
l
to
-
-
"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 19 Apr 2016 04:26:07 +0000",Re: more uniform exception handling?,Evan Chan <velvia.github@gmail.com>,"+1
Both of the would be very helpful in debugging

Thanks.

Zhan Zhang


:
ot
ot
.
r,
n
 lot
s in
not
an


---------------------------------------------------------------------


"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 19 Apr 2016 04:44:29 +0000",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Reynold Xin <rxin@databricks.com>,"n, Thus, I donít think shouldStop would work here. To move it work, the limit has to be part of the wholeStageCodeGen.

Correct me if I am wrong.

Thanks.

Zhan Zhang


I could be wrong but I think we currently do that through whole stage codegen. After processing every row on the stream side, the generated code for broadcast join checks whether it has hit the limit or not (through this thing called shouldStop).

It is not the most optimal solution, because a single stream side row might output multiple hits, but it is usually not a problem.


While you can't automatically push the limit *through* the join, we could push it *into* the join (stop processing after generating 10 records). I believe that is what Rajesh is suggesting.

I am not sure if you can push a limit through a join. This becomes problematic if not all keys are present on both sides; in such a case a limit can produce fewer rows than the set limit.

This might be a rare case in which whole stage codegen is slower, due to the fact that we need to buffer the result of such a stage. You could try to disable it by setting ""spark.sql.codegen.wholeStage"" to false.

2016-04-12 14:32 GMT+02:00 Rajesh Balamohan <rajesh.balamohan@gmail.com<mailto:rajesh.balamohan@gmail.com>>:
Hi,

I ran the following query in spark (latest master codebase) and it took a lot of time to complete even though it was a broadcast hash join.

It appears that limit computation is done only after computing complete join condition.  Shouldn't the limit condition be pushed to BroadcastHashJoin (wherein it would have to stop processing after generating 10 rows?).  Please let me know if my understanding on this is wrong.


select l_partkey from lineitem, partsupp where ps_partkey=l_partkey limit 10;

| == Physical Plan ==
CollectLimit 10
+- WholeStageCodegen
   :  +- Project [l_partkey#893]
   :     +- BroadcastHashJoin [l_partkey#893], [ps_partkey#908], Inner, BuildRight, None
   :        :- Project [l_partkey#893]
   :        :  +- Filter isnotnull(l_partkey#893)
   :        :     +- Scan HadoopFiles[l_partkey#893] Format: ORC, PushedFilters: [IsNotNull(l_partkey)], ReadSchema: struct<l_partkey:int>
   :        +- INPUT
   +- BroadcastExchange HashedRelationBroadcastMode(true,List(cast(ps_partkey#908 as bigint)),List(ps_partkey#908))
      +- WholeStageCodegen
         :  +- Project [ps_partkey#908]
         :     +- Filter isnotnull(ps_partkey#908)
         :        +- Scan HadoopFiles[ps_partkey#908] Format: ORC, PushedFilters: [IsNotNull(ps_partkey)], ReadSchema: struct<ps_partkey:int>  |




--
~Rajesh.B




"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 21:50:30 -0700",Re: auto closing pull requests that have been inactive > 30 days?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Thanks a lot for commenting. We are getting great feedback on this thread.
The take-aways are:

1. In general people prefer having explicit reasons why pull requests
should be closed. We should push committers to leave messages that are more
explicit about why certain PR should be closed or not. I can't agree more.
But this is not mutually exclusive.


2.  It is difficult to deal with the scale we are talking about. There is
not a single measure that could ""fix"" everything.

Spark is as far as I know one of the most active open source projects in
terms of contributions, in part because we have made it very easy to accept
contributions. There have been very few open source projects that needed to
deal with this scale. Actually if you look at all the historic PRs, we
closed 12k and have ~450 open. That's less than 4% of the prs outstanding
-- not a bad number. The actual ratio is likely even lower because many of
the 450 open will be merged in the future.

I also took a look at some of the most popular projects on github (e.g.
jquery, angular, react) -- they either have far fewer merged pull requests
or a higher ratio of open-to-close. So we are actually doing pretty well.
But of course there is always room for improvement.





-PRs-td8015.html>
DzeR4cG_wXgKTOxsG8s34KrQEzYgjFZDOYMgu9VhYJBRDg@mail.gmail.com%3E>
t
he
;
ct
ed-asc>
, Ted Yu <yuzhihong@gmail.com>ÎãòÏù¥ ÏûëÏÑ±:
 is
to
l
e
n
a
ed
a
t
?
e
I
 a
e
t
o
"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 22:08:00 -0700",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Zhan Zhang <zzhang@hortonworks.com>,"Unless I'm really missing something I don't think so. As I said, it goes
through an iterator and after processing each stream side we do a
shouldStop check. The generated code looks like

/* 094 */   protected void processNext() throws java.io.IOException {
/* 095 */     /*** PRODUCE: Project [id#79L] */
/* 096 */
/* 097 */     /*** PRODUCE: BroadcastHashJoin [id#79L], [id#82L], Inner,
BuildRight, None */
/* 098 */
/* 099 */     /*** PRODUCE: Range 0, 1, 8, 100, [id#79L] */
/* 100 */
/* 101 */     // initialize Range
/* 102 */     if (!range_initRange) {
/* 103 */       range_initRange = true;
/* 104 */       initRange(partitionIndex);
/* 105 */     }
/* 106 */
/* 107 */     while (!range_overflow && range_number < range_partitionEnd) {
/* 108 */       long range_value = range_number;
/* 109 */       range_number += 1L;
/* 110 */       if (range_number < range_value ^ 1L < 0) {
/* 111 */         range_overflow = true;
/* 112 */       }
/* 113 */
/* 114 */       /*** CONSUME: BroadcastHashJoin [id#79L], [id#82L], Inner,
BuildRight, None */
/* 115 */
/* 116 */       // generate join key for stream side
/* 117 */
/* 118 */       // find matches from HashedRelation
/* 119 */       UnsafeRow bhj_matched = false ? null:
(UnsafeRow)bhj_relation.getValue(range_value);
/* 120 */       if (bhj_matched == null) continue;
/* 121 */
/* 122 */       bhj_metricValue.add(1);
/* 123 */
/* 124 */       /*** CONSUME: Project [id#79L] */
/* 125 */
/* 126 */       System.out.println(""i got one row"");
/* 127 */
/* 128 */       /*** CONSUME: WholeStageCodegen */
/* 129 */
/* 130 */       project_rowWriter.write(0, range_value);
/* 131 */       append(project_result);
/* 132 */
*/* 133 */       if (shouldStop()) return;*
/* 134 */     }
/* 135 */   }
/* 136 */ }


shouldStop is false once we go pass the limit.




. To move
e
is
:
d
<
o
ry
k
e
is
,
t>
int>
"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 19 Apr 2016 05:32:18 +0000",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

I just check the code for CollectLimit, there is a shuffle happening to collect them in one partition.


protected override def doExecute(): RDD[InternalRow] = {
  val shuffled = new ShuffledRowRDD(
    ShuffleExchange.prepareShuffleDependency(
      child.execute(), child.output, SinglePartition, serializer))
  shuffled.mapPartitionsInternal(_.take(limit))
}

Thus, there is no way to avoid processing all data before the shuffle. I think that is the reason. Do I understand correctly?

Thanks.

Zhan Zhang

Unless I'm really missing something I don't think so. As I said, it goes through an iterator and after processing each stream side we do a shouldStop check. The generated code looks like

/* 094 */   protected void processNext() throws java.io.IOException {
/* 095 */     /*** PRODUCE: Project [id#79L] */
/* 096 */
/* 097 */     /*** PRODUCE: BroadcastHashJoin [id#79L], [id#82L], Inner, BuildRight, None */
/* 098 */
/* 099 */     /*** PRODUCE: Range 0, 1, 8, 100, [id#79L] */
/* 100 */
/* 101 */     // initialize Range
/* 102 */     if (!range_initRange) {
/* 103 */       range_initRange = true;
/* 104 */       initRange(partitionIndex);
/* 105 */     }
/* 106 */
/* 107 */     while (!range_overflow && range_number < range_partitionEnd) {
/* 108 */       long range_value = range_number;
/* 109 */       range_number += 1L;
/* 110 */       if (range_number < range_value ^ 1L < 0) {
/* 111 */         range_overflow = true;
/* 112 */       }
/* 113 */
/* 114 */       /*** CONSUME: BroadcastHashJoin [id#79L], [id#82L], Inner, BuildRight, None */
/* 115 */
/* 116 */       // generate join key for stream side
/* 117 */
/* 118 */       // find matches from HashedRelation
/* 119 */       UnsafeRow bhj_matched = false ? null: (UnsafeRow)bhj_relation.getValue(range_value);
/* 120 */       if (bhj_matched == null) continue;
/* 121 */
/* 122 */       bhj_metricValue.add(1);
/* 123 */
/* 124 */       /*** CONSUME: Project [id#79L] */
/* 125 */
/* 126 */       System.out.println(""i got one row"");
/* 127 */
/* 128 */       /*** CONSUME: WholeStageCodegen */
/* 129 */
/* 130 */       project_rowWriter.write(0, range_value);
/* 131 */       append(project_result);
/* 132 */
/* 133 */       if (shouldStop()) return;
/* 134 */     }
/* 135 */   }
/* 136 */ }


shouldStop is false once we go pass the limit.



n, Thus, I donít think shouldStop would work here. To move it work, the limit has to be part of the wholeStageCodeGen.

Correct me if I am wrong.

Thanks.

Zhan Zhang


I could be wrong but I think we currently do that through whole stage codegen. After processing every row on the stream side, the generated code for broadcast join checks whether it has hit the limit or not (through this thing called shouldStop).

It is not the most optimal solution, because a single stream side row might output multiple hits, but it is usually not a problem.


While you can't automatically push the limit *through* the join, we could push it *into* the join (stop processing after generating 10 records). I believe that is what Rajesh is suggesting.

I am not sure if you can push a limit through a join. This becomes problematic if not all keys are present on both sides; in such a case a limit can produce fewer rows than the set limit.

This might be a rare case in which whole stage codegen is slower, due to the fact that we need to buffer the result of such a stage. You could try to disable it by setting ""spark.sql.codegen.wholeStage"" to false.

2016-04-12 14:32 GMT+02:00 Rajesh Balamohan <rajesh.balamohan@gmail.com<mailto:rajesh.balamohan@gmail.com>>:
Hi,

I ran the following query in spark (latest master codebase) and it took a lot of time to complete even though it was a broadcast hash join.

It appears that limit computation is done only after computing complete join condition.  Shouldn't the limit condition be pushed to BroadcastHashJoin (wherein it would have to stop processing after generating 10 rows?).  Please let me know if my understanding on this is wrong.


select l_partkey from lineitem, partsupp where ps_partkey=l_partkey limit 10;

| == Physical Plan ==
CollectLimit 10
+- WholeStageCodegen
   :  +- Project [l_partkey#893]
   :     +- BroadcastHashJoin [l_partkey#893], [ps_partkey#908], Inner, BuildRight, None
   :        :- Project [l_partkey#893]
   :        :  +- Filter isnotnull(l_partkey#893)
   :        :     +- Scan HadoopFiles[l_partkey#893] Format: ORC, PushedFilters: [IsNotNull(l_partkey)], ReadSchema: struct<l_partkey:int>
   :        +- INPUT
   +- BroadcastExchange HashedRelationBroadcastMode(true,List(cast(ps_partkey#908 as bigint)),List(ps_partkey#908))
      +- WholeStageCodegen
         :  +- Project [ps_partkey#908]
         :     +- Filter isnotnull(ps_partkey#908)
         :        +- Scan HadoopFiles[ps_partkey#908] Format: ORC, PushedFilters: [IsNotNull(ps_partkey)], ReadSchema: struct<ps_partkey:int>  |




--
~Rajesh.B






"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 22:35:07 -0700",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Zhan Zhang <zzhang@hortonworks.com>,"But doExecute is not called?

:

)
,
e. To move
de
his
 <
a
ld
m
in.
 is
nt>
:int>
"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Apr 2016 22:36:05 -0700",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Zhan Zhang <zzhang@hortonworks.com>,"Anyway we can verify this easily. I just added a println to each row and
verified that only limit + 1 row was printed after the join and before the
limit.

It'd be great if you do some debugging yourself and see if it is going
through some other code path.



re. To move
ode
this
r <
 a
uld
oin.
s is
y
int>
y:int>
"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 19 Apr 2016 05:41:41 +0000",Re: SparkSQL - Limit pushdown on BroadcastHashJoin,Reynold Xin <rxin@databricks.com>,"Thanks Reynold.

Not sure why doExecute is not invoked, since CollectLimit does not support wholeStage

case class CollectLimit(limit: Int, child: SparkPlan) extends UnaryNode {

I will dig further into this.

Zhan Zhang


Anyway we can verify this easily. I just added a println to each row and verified that only limit + 1 row was printed after the join and before the limit.

It'd be great if you do some debugging yourself and see if it is going through some other code path.


But doExecute is not called?

Hi Reynold,

I just check the code for CollectLimit, there is a shuffle happening to collect them in one partition.


protected override def doExecute(): RDD[InternalRow] = {
  val shuffled = new ShuffledRowRDD(
    ShuffleExchange.prepareShuffleDependency(
      child.execute(), child.output, SinglePartition, serializer))
  shuffled.mapPartitionsInternal(_.take(limit))
}

Thus, there is no way to avoid processing all data before the shuffle. I think that is the reason. Do I understand correctly?

Thanks.

Zhan Zhang

Unless I'm really missing something I don't think so. As I said, it goes through an iterator and after processing each stream side we do a shouldStop check. The generated code looks like

/* 094 */   protected void processNext() throws java.io.IOException {
/* 095 */     /*** PRODUCE: Project [id#79L] */
/* 096 */
/* 097 */     /*** PRODUCE: BroadcastHashJoin [id#79L], [id#82L], Inner, BuildRight, None */
/* 098 */
/* 099 */     /*** PRODUCE: Range 0, 1, 8, 100, [id#79L] */
/* 100 */
/* 101 */     // initialize Range
/* 102 */     if (!range_initRange) {
/* 103 */       range_initRange = true;
/* 104 */       initRange(partitionIndex);
/* 105 */     }
/* 106 */
/* 107 */     while (!range_overflow && range_number < range_partitionEnd) {
/* 108 */       long range_value = range_number;
/* 109 */       range_number += 1L;
/* 110 */       if (range_number < range_value ^ 1L < 0) {
/* 111 */         range_overflow = true;
/* 112 */       }
/* 113 */
/* 114 */       /*** CONSUME: BroadcastHashJoin [id#79L], [id#82L], Inner, BuildRight, None */
/* 115 */
/* 116 */       // generate join key for stream side
/* 117 */
/* 118 */       // find matches from HashedRelation
/* 119 */       UnsafeRow bhj_matched = false ? null: (UnsafeRow)bhj_relation.getValue(range_value);
/* 120 */       if (bhj_matched == null) continue;
/* 121 */
/* 122 */       bhj_metricValue.add(1);
/* 123 */
/* 124 */       /*** CONSUME: Project [id#79L] */
/* 125 */
/* 126 */       System.out.println(""i got one row"");
/* 127 */
/* 128 */       /*** CONSUME: WholeStageCodegen */
/* 129 */
/* 130 */       project_rowWriter.write(0, range_value);
/* 131 */       append(project_result);
/* 132 */
/* 133 */       if (shouldStop()) return;
/* 134 */     }
/* 135 */   }
/* 136 */ }


shouldStop is false once we go pass the limit.



n, Thus, I donít think shouldStop would work here. To move it work, the limit has to be part of the wholeStageCodeGen.

Correct me if I am wrong.

Thanks.

Zhan Zhang


I could be wrong but I think we currently do that through whole stage codegen. After processing every row on the stream side, the generated code for broadcast join checks whether it has hit the limit or not (through this thing called shouldStop).

It is not the most optimal solution, because a single stream side row might output multiple hits, but it is usually not a problem.


While you can't automatically push the limit *through* the join, we could push it *into* the join (stop processing after generating 10 records). I believe that is what Rajesh is suggesting.

I am not sure if you can push a limit through a join. This becomes problematic if not all keys are present on both sides; in such a case a limit can produce fewer rows than the set limit.

This might be a rare case in which whole stage codegen is slower, due to the fact that we need to buffer the result of such a stage. You could try to disable it by setting ""spark.sql.codegen.wholeStage"" to false.

2016-04-12 14:32 GMT+02:00 Rajesh Balamohan <rajesh.balamohan@gmail.com<mailto:rajesh.balamohan@gmail.com>>:
Hi,

I ran the following query in spark (latest master codebase) and it took a lot of time to complete even though it was a broadcast hash join.

It appears that limit computation is done only after computing complete join condition.  Shouldn't the limit condition be pushed to BroadcastHashJoin (wherein it would have to stop processing after generating 10 rows?).  Please let me know if my understanding on this is wrong.


select l_partkey from lineitem, partsupp where ps_partkey=l_partkey limit 10;

| == Physical Plan ==
CollectLimit 10
+- WholeStageCodegen
   :  +- Project [l_partkey#893]
   :     +- BroadcastHashJoin [l_partkey#893], [ps_partkey#908], Inner, BuildRight, None
   :        :- Project [l_partkey#893]
   :        :  +- Filter isnotnull(l_partkey#893)
   :        :     +- Scan HadoopFiles[l_partkey#893] Format: ORC, PushedFilters: [IsNotNull(l_partkey)], ReadSchema: struct<l_partkey:int>
   :        +- INPUT
   +- BroadcastExchange HashedRelationBroadcastMode(true,List(cast(ps_partkey#908 as bigint)),List(ps_partkey#908))
      +- WholeStageCodegen
         :  +- Project [ps_partkey#908]
         :     +- Filter isnotnull(ps_partkey#908)
         :        +- Scan HadoopFiles[ps_partkey#908] Format: ORC, PushedFilters: [IsNotNull(ps_partkey)], ReadSchema: struct<ps_partkey:int>  |




--
~Rajesh.B









"
Yanbo Liang <ybliang8@gmail.com>,"Mon, 18 Apr 2016 23:52:23 -0700",Re: Organizing Spark ML example packages,Nick Pentreath <nick.pentreath@gmail.com>,"This sounds good to me, and it will make ML examples more neatly.

2016-04-14 5:28 GMT-07:00 Nick Pentreath <nick.pentreath@gmail.com>:

"
Yanbo Liang <ybliang8@gmail.com>,"Mon, 18 Apr 2016 23:55:57 -0700",Re: [spark.ml] Why is private class ColumnPruner?,Jacek Laskowski <jacek@japila.pl>,"Hi Jacek,

This is due to ColumnPruner is only used for RFormula currently, we did not
expose it as a feature transformer.
Please feel free to create JIRA and work on it.

Thanks
Yanbo

2016-03-25 8:50 GMT-07:00 Jacek Laskowski <jacek@japila.pl>:

"
Sean Owen <sowen@cloudera.com>,"Tue, 19 Apr 2016 09:15:22 +0100",Re: more uniform exception handling?,Reynold Xin <rxin@databricks.com>,"We already have SparkException, indeed. The ID is an interesting idea;
simple to implement and might help disambiguate.

Does it solve a lot of problems of this form? if something is
squelching Exception or SparkException the result will be the same. #2
is something we can sniff out with static analysis pretty easily, but
not as much #1. Ideally we'd just fix blocks like this but I bet there
are lots of them.

I like the idea but for a different reason, and that's that it's
probably best to control exceptions that propagate from the public
API, since in some cases they're a meaningful part of the API (see
https://issues.apache.org/jira/browse/SPARK-8393 which I'm hoping to
fix now)

And the catch there is -- throwing checked exceptions from Scala code
in a way that Java code can catch requires annotating lots of methods.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 19 Apr 2016 09:56:24 +0100",Re: auto closing pull requests that have been inactive > 30 days?,Reynold Xin <rxin@databricks.com>,"I support this. We used to do this, right? Anecdotally, from watching
the stream most days, most stale PRs are, in descending order of
frequency:

1. Probably not a good change, and not looked at (as a result)
2. Abandoned by submitter at some stage
3. Not an important change, not so bad, not really reviewed
4. A good change that needs review

Whether your PR is #1 or #4 is a matter of perspective. But, I
disagree with the tacit assumption that we're mostly talking about
good PRs being closed because nobody could be bothered; #4 is, I
think, well under 10%.

So generating reports and warnings etc don't seem to address that.
Closing merely means ""at the moment there's not a reason to expect
this to proceed, but that could change"". Unlike JIRA we don't have
more nuanced resolutions like ""WontFix"" vs ""Later"". Welcome the
submitter to reopen if they really think it should be kept alive in
good faith.

As for always stating a close reason: well, a lot of PRs are simply
not very good code, or features that just don't look that useful
relative to their cost. Is it more polite to soft-close or honestly
say ""I don't think this is worth adding""?

There is a carrying cost to not doing this. Right now being ""Open"" is
fairly meaningless, and I've long since stopped bothering reviewing
the backlog of open PRs since it's noise, instead sifting for good new
ones to fast-track.


I agree with comments here that suggest more can be pushed back on the
contributors. We've started with a campaign to get people to read
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
first, which would solve a lot of the ""mystery pull request"" problems
if followed: no real description, no test, no real problem statement.

Put another way, any contribution that is clearly explained, cleanly
implemented, and makes a good case why its pros outweigh its cons, is
pretty consistently reviewed and quickly. Pushing on contributors to
do these things won't harm good contributions, which already do these
things; it'll make it harder for bad contributions to distract from
them.

And I think the effect of a change like this is, in the main, to push
back mostly on less good contributions.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 19 Apr 2016 09:26:16 +0000",Re: YARN Shuffle service and its compatibility,Marcelo Vanzin <vanzin@cloudera.com>,"
> On 18 Apr 2016, at 23:05, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> 
> On Mon, Apr 18, 2016 at 2:02 PM, Reynold Xin <rxin@databricks.com> wrote:
>> The bigger problem is that it is much easier to maintain backward
>> compatibility rather than dictating forward compatibility. For example, as
>> Marcin said, if we come up with a slightly different shuffle layout to
>> improve shuffle performance, we wouldn't be able to do that if we want to
>> allow Spark 1.6 shuffle service to read something generated by Spark 2.1.
> 
> And I think that's really what Mark is proposing. Basically, ""don't
> intentionally break backwards compatibility unless it's really
> required"" (e.g. SPARK-12130). That would allow option B to work.
> 
> If a new shuffle manager is created, then neither option A nor option
> B would really work. Moving all the shuffle-related classes to a
> different package, to support option A, would be really messy. At that
> point, you're better off maintaining the new shuffle service outside
> of YARN, which is rather messy too.
> 


There's a WiP in YARN to move Aux NM services into their own CP, though that doesn't address shared native libs, such as the leveldb support that went into 1.6


There's already been some fun with Jackson versions and that of Hadoop ‚Äî SPARK-12807; something that per-service classpaths would fix.

would having separate CPs allow multiple spark shuffle JARs to be loaded, as long as everything bonded to the right one?"
Steve Loughran <stevel@hortonworks.com>,"Tue, 19 Apr 2016 09:45:03 +0000",Re: more uniform exception handling?,Reynold Xin <rxin@databricks.com>,"
On 18 Apr 2016, at 20:16, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:

Josh's pull request<https://github.com/apache/spark/pull/12433> on rpc exception handling got me to think ...

In my experience, there have been a few things related exceptions that created a lot of trouble for us in production debugging:

1. Some exception is thrown, but is caught by some try/catch that does not do any logging nor rethrow.
2. Some exception is thrown, but is caught by some try/catch that does not do any logging, but do rethrow. But the original exception is now masked.
2. Multiple exceptions are logged at different places close to each other, but we don't know whether they are caused by the same problem or not.


To mitigate some of the above, here's an idea ...

(1) Create a common root class for all the exceptions (e.g. call it SparkException) used in Spark. We should make sure every time we catch an exception from a 3rd party library, we rethrow them as SparkException (a lot of places already do that). In SparkException's constructor, log the exception and the stacktrace.

(2) SparkException has a monotonically increasing ID, and this ID appears in the exception error message (say at the end).


I think (1) will eliminate most of the cases that an exception gets swallowed. The main downside I can think of is we might log an exception multiple times. However, I'd argue exceptions should be rare, and it is not that big of a deal to log them twice or three times. The unique ID (2) can help us correlate exceptions if they appear multiple times.

Thoughts?






1. unique IDs is a nice touch
2. there are some exceptions where code really needs to match on them, usually in the network layer, also interruptedException. Its dangerous to swallow them.
3. I've done work on other projects (Slider, with YARN-679 to get them  into Hadoop) where exceptions can also declare an exit code. This means system exits can have different exit codes for different problems ‚Äîand the exception raising code gets to choose it. For extra fun, the set of exit codes attempt to lift numbers from HTTP errors, so ""41"" is Unauthed, from HTTP 401: https://slider.incubator.apache.org/docs/exitcodes.html
4. Once you have different exit codes, then you can start writing tests for the scripts designed to trigger failures ‚Äîasserting about the exit code as way to assess the outcome

Something else to consider is ""what can be added atop the classic runtime exceptions to make them useful. Hadoop's NetUtils.wrapException() does this: catches things coming up from the network stack and rethrows an exception of the same type (where possible), but now with source/dest hostnames and ports. That is incredibly useful. The exceptions also tack in wiki references to what the exceptions mean in a desparate attempt to reduce the #of JIRAs complaining about services refusing connections. Its hard to tell how often that works ‚Äîsome people do now just paste in the stack trace without reading the wiki link. At least now there's somewhere to point them at when the issue is closed as invalid. [ see: http://steveloughran.blogspot.co.uk/2011/09/note-on-distributed-computing.html

I'm now considering what could be done at the Kerberos layer too, though there the problem is that the JVM Exception is invariably a meaningless ""Failure Unspecified at GSS API Level"" + text which varies across JVM vendor and versions. Maybe the wiki URL should just point to page saying ""nobody understands kerberos ‚Äîsorry""
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 19 Apr 2016 13:48:12 +0200",Re: [spark.ml] Why is private class ColumnPruner?,Yanbo Liang <ybliang8@gmail.com>,"Hi Yanbo,

https://issues.apache.org/jira/browse/SPARK-14730

Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 19 Apr 2016 22:06:14 +0900",Re: auto closing pull requests that have been inactive > 30 days?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I agree with you, Sean, almost all.

If feedback can be given enough, it might be okay to apply the rule as
Reynold said it looks they are not mutually exclusive (although I still
think there should be more committers or should be more active because the
main reason for this looks there are PRs not reviewed or closed without
feedback enough. I think there should be some comments even though it
closes the issue and the comments are ""I don't think it is worth adding
because..."". This could help develop contributer's logical process which
would eventually decrease committer's help).

FWIW, I strongly feel at least those 10% of other remaining ""good"" PRs
should be given feedback and comments enough because **Personally** I am
pretty sure that the PRs closed by the rule would never be reopened
including those 10% and I believe the copy-and-pasted message would do
nothing but just notifying ""closing this as it is expired"". We don't really
care how nice the messages left by Jenkins are.

Lastly, I think being open and closed might mean something and that might
be the reason why there are some PRs not closed even though comitters think
they don't think it is worth adding.

e
vidence/benchmark/performance
t
e-PRs-td8015.html>
mDzeR4cG_wXgKTOxsG8s34KrQEzYgjFZDOYMgu9VhYJBRDg@mail.gmail.com%3E>
ot
the
n
r
ect
ted-asc>
0, Ted Yu <yuzhihong@gmail.com>ÎãòÏù¥ ÏûëÏÑ±:
.
.
sure,
.
m is
 to
el
.
ge
n
en
s
,
m>
 a
 ?
he
o
 I
t
d
l
to
-
-
"
"""Rich Bowen""<rbowen@apache.org>","Tue, 19 Apr 2016 13:16:09 -0000","Introduction to Spark workshop, May 9, New York",<dev@spark.apache.org>,"Hi, folks, I received the following request:

-----------
The guy who was going to teach the Introduction to Spark workshop at Data Summit on May 9th has changed jobs and can no longer do the workshop. Know anybody in the New York area who could fill in? It's scheduled from 9 to 12 at the New York Hilton in Midtown.

Any suggestions welcome!
---------

Thanks to anybody that can step up for this, or can suggest someone else. Feel free to contact me off-list - rbowen@apache.org - if you prefer.

Thanks!

--Rich
------
Sent via Pony Mail for dev@spark.apache.org. 
View this email online at:
https://pony-poc.apache.org/list.html?dev@spark.apache.org

---------------------------------------------------------------------


"
Patrick Woody <patrick.woody1@gmail.com>,"Tue, 19 Apr 2016 14:32:37 +0100",Question about storage memory in unified memory manager,dev@spark.apache.org,"Hey all,

I had a question about the MemoryStore for the BlockManager with the
unified memory manager v.s. the legacy mode.

In the unified format, I would expect the max size of the MemoryStore to be

<total max memory> * <spark.memory.fraction> *
<spark.memory.storageFraction>

 in the same way that when using the StaticMemoryManager it is

<total max memory> * <spark.storage.memoryFraction> *
<spark.storage.safetyFraction>.

Instead it appears to be

(<total max memory> * <spark.memory.fraction>) -
onHeapExecutionMemoryPool.memoryUsed

I would expect onHeapExecutionMemoryPool.memoryUsed to be around size 0 at
the time of initialization, so this may be overcommitting memory to the
MemoryStore. Does this line up with expectations?

For context, I have a very large SQL query that does a significant amount
of broadcast joins (~100) so I need my driver to be able to toss those into
the DiskStore without going into GC hell. Since my MemoryStore seems to not
be bounded by the storage pool, this fills up my heap and causes my
application to OOM. Simply reducing spark.memory.fraction alleviates the
problem, but I'd love to understand if that is actually the correct fix
here rather than simply lowering the storageFraction.

Thanks!
-Pat
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 19 Apr 2016 14:44:59 +0000 (UTC)",Re: YARN Shuffle service and its compatibility,"Marcelo Vanzin <vanzin@cloudera.com>, Reynold Xin <rxin@databricks.com>","It would be nice if we could keep this compatible between 1.6 and 2.0 so I'm more for Option B at this point since¬†the change made seems minor and we can change to have shuffle service do internally like Marcelo mention. Then lets try to keep compatible, but if there is a forcing function lets figure out a good way to run 2 at once.

Tom 

 


Is that something that really yields noticeable gains in performance?

If it is, it seems like it would be simple to allow executors register
with the full class name, and map the long names to short names in the
shuffle service itself.

You could even get fancy and have different ExecutorShuffleInfo
implementations for each shuffle service, with an abstract
""getBlockData"" method that gets called instead of the current if/else
in ExternalShuffleBlockResolver.java.

-- 
Marcelo

---------------------------------------------------------------------



  "
Mark Grover <mark@apache.org>,"Tue, 19 Apr 2016 09:47:33 -0700",Re: YARN Shuffle service and its compatibility,Steve Loughran <stevel@hortonworks.com>,"
,
‚Äî

I just checked out https://issues.apache.org/jira/browse/YARN-1593. It's
hard to say if it'd help or not, I wasn't able to find any design doc or
patch attached to that JIRA. If there were a way to specify different JAR
names/locations for starting the separate process, it would work but if the
start happened by pointing to a full class name, that comes back to Option
A, and we'd have to do a good chunk of name/version spacing in order to
isolate.
"
Mark Grover <mark@apache.org>,"Tue, 19 Apr 2016 09:52:05 -0700",Re: YARN Shuffle service and its compatibility,Tom Graves <tgraves_cs@yahoo.com>,"Thanks.

I'm more than happy to wait for more people to chime in here but I do feel
that most of us are leaning towards Option B anyways. So, I created a JIRA
(SPARK-14731) for reverting SPARK-12130 in Spark 2.0 and file a PR shortly.
Mark


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 10:15:31 -0700","RFC: Remote ""HBaseTest"" from examples?","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

Two reasons why I think we should remove that from the examples:

- HBase now has Spark integration in its own repo, so that really
should be the template for how to use HBase from Spark, making that
example less useful, even misleading.

- It brings up a lot of extra dependencies that make the size of the
Spark distribution grow.

Any reason why we shouldn't drop that example?

-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 10:20:09 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"Corrected typo in subject.

I want to note that the hbase-spark module in HBase is incomplete. Zhan has
several patches pending review.

hbase-spark module is currently only in master branch which would be
released as 2.0
However the release date for 2.0 is unclear - probably half a year from now.

If we remove the examples now, there would be no release from either
project which can show users how to access hbase.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 10:23:54 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"
I wouldn't call it ""incomplete"". Lots of functionality is there, which
doesn't mean new ones, or more efficient implementations of existing
ones, can't be added.


Just as a side note, it's part of CDH 5.7.0, not that it matters much
for upstream HBase.

-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 10:28:27 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"bq. I wouldn't call it ""incomplete"".

I would call it incomplete.

Please see HBASE-15333 'Enhance the filter to handle short, integer, long,
float and double' which is a bug fix.

Please exclude presence of related of module in vendor distro from this
discussion.

Thanks


"
Reynold Xin <rxin@databricks.com>,"Tue, 19 Apr 2016 10:28:39 -0700","Re: RFC: Remote ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"Yea in general I feel examples that bring in a large amount of dependencies
should be outside Spark.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 10:34:03 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Reynold Xin <rxin@databricks.com>,"
Another option to avoid the dependency problem is to not ship examples
in the distribution, and maybe create a separate tarball for them;
removing HBaseTest only solves one of the dependency problems. Since
we have examples for flume and kafka, for example, the Spark
distribution ends up shipping flume and kafka jars (and a bunch of
other things).




-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 10:35:58 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"bq. create a separate tarball for them

Probably another thread can be started for the above.
I am fine with it.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 10:38:46 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"Alright, if you prefer, I'll say ""it's actually in use right now in
spite of not being in any upstream HBase release"", and it's more
useful than a single example file in the Spark repo for those who
really want to integrate with HBase.

Spark's example is really very trivial (just uses one of HBase's input
formats), which makes it not very useful as a blueprint for developing
HBase apps with Spark.




-- 
Marcelo

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Tue, 19 Apr 2016 17:41:12 +0000","Re: RFC: Remote ""HBaseTest"" from examples?","Reynold Xin <rxin@databricks.com>, Marcelo Vanzin <vanzin@cloudera.com>","+1; I think that it's preferable for code examples, especially third-party
integration examples, to live outside of Spark.


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 10:41:43 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"bq. it's actually in use right now in spite of not being in any upstream
HBase release

If it is not in upstream, then it is not relevant for discussion on Apache
mailing list.


"
Reynold Xin <rxin@databricks.com>,"Tue, 19 Apr 2016 10:42:48 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"Ted - what's the ""bq"" thing? Are you using some 3rd party (e.g. Atlassian)
syntax? They are not being rendered in email.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 10:43:26 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"You're entitled to your own opinions.

While you're at it, here's some much better documentation, from the
HBase project themselves, than what the Spark example provides:
http://hbase.apache.org/book.html#spark




-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 10:45:34 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Reynold Xin <rxin@databricks.com>,"'bq.' is used in JIRA to quote what other people have said.


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 10:47:32 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"There is an Open JIRA for fixing the documentation: HBASE-15473

I would say the refguide link you provided should not be considered as
complete.

Note it is marked as Blocker by Sean B.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 10:50:14 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"You're completely missing my point. I'm saying that HBase's current
support, even if there are bugs or things that still need to be done,
is much better than the Spark example, which is basically a call to
""SparkContext.hadoopRDD"".

Spark's example is not helpful in learning how to build an HBase
application on Spark, and clashes head on with how the HBase
developers think it should be done. That, and because it brings too
many dependencies for something that is not really useful, is why I'm
suggesting removing it.





-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 10:59:04 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"bq. HBase's current support, even if there are bugs or things that still
need to be done, is much better than the Spark example

In my opinion, a simple example that works is better than a buggy package.

I hope before long the hbase-spark module in HBase can arrive at a state
which we can advertise as mature - but we're not there yet.


"
Marcin Tustin <mtustin@handybook.com>,"Tue, 19 Apr 2016 14:01:57 -0400","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"Let's posit that the spark example is much better than what is available in
HBase. Why is that a reason to keep it within Spark?



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 11:07:34 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcin Tustin <mtustin@handybook.com>,"The same question can be asked w.r.t. examples for other projects,
such as flume
and kafka.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 11:10:46 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"

The main difference being that flume and kafka integration are part of
Spark itself. HBase integration is not.





-- 
Marcelo
"
Bryan Cutler <cutlerb@gmail.com>,"Tue, 19 Apr 2016 11:12:08 -0700",Re: Organizing Spark ML example packages,Yanbo Liang <ybliang8@gmail.com>,"+1, adding some organization would make it easier for people to find a
specific example


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 19 Apr 2016 11:21:11 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Marcelo Vanzin <vanzin@cloudera.com>,"Clarification: in my previous email, I was not talking
about spark-streaming-flume artifact or spark-streaming-kafka artifact.

I was talking about examples for these projects, such
as examples//src/main/python/streaming/flume_wordcount.py


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Apr 2016 11:26:00 -0700","Re: RFC: Remove ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"

I understand. And those examples are showing how to use code that is part
of Spark. HBaseTest just shows how to use a generic Spark API that can both
be used to talk to HBase or to anything else that has an InputFormat, so
it's much less useful as an example.

I'd put CassandraTest in that same category, although that particular
example at least shows more functionality than the HBase one.





-- 
Marcelo
"
Niranda Perera <niranda.perera@gmail.com>,"Wed, 20 Apr 2016 03:00:01 +0530",Re: Possible deadlock in registering applications in the recovery mode,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

I have created a JIRA for this [1]. I have also created a PR for the same
issue [2].

Would be very grateful if you could look into this, because this is a
blocker in our spark deployment, which uses number of spark custom
extension.

thanks
best

[1] https://issues.apache.org/jira/browse/SPARK-14736
[2] https://github.com/apache/spark/pull/12506




-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Reynold Xin <rxin@databricks.com>,"Tue, 19 Apr 2016 16:20:51 -0700",Re: YARN Shuffle service and its compatibility,Mark Grover <mark@apache.org>,"I talked to Lianhui offline and he said it is not that big of a deal to
revert the patch.



"
Mark Grover <mark@apache.org>,"Tue, 19 Apr 2016 16:26:12 -0700",Re: YARN Shuffle service and its compatibility,Reynold Xin <rxin@databricks.com>,"Great, thanks for confirming, Reynold. Appreciate it!


"
Saisai Shao <sai.sai.shao@gmail.com>,"Wed, 20 Apr 2016 17:43:35 +0800","Re: RFC: Remote ""HBaseTest"" from examples?",Josh Rosen <joshrosen@databricks.com>,"+1, HBaseTest in Spark Example is quite old and obsolete, the HBase
connector in HBase repo has evolved a lot, it would be better to guide user
to refer to that not here in Spark example. So good to remove it.

Thanks
Saisai


"
"""FangFang Chen"" <lulynn_2015_spark@163.com>","Wed, 20 Apr 2016 18:06:06 +0800 (GMT+08:00)",Spark sql and hive into different result with same sql,"""user@spark.apache.org"" <user@spark.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
Please give some suggestions. Thanks


With following same sql, spark sql and hive give different result. The sql is sum(decimal(38,18)) columns.
Select sum(column) from table;
column is defined as decimal(38,18).


Spark version:1.5.3
Hive version:2.0.0


∑¢◊‘ Õ¯“◊” œ‰¥Û ¶"
"""FangFang Chen"" <lulynn_2015_spark@163.com>","Wed, 20 Apr 2016 18:45:28 +0800 (GMT+08:00)","=?GBK?Q?=BB=D8=B8=B4=A3=BASpark_sql_and_hive_into_d?=
 =?GBK?Q?ifferent_result_with_same_sql?=","""user@spark.apache.org"" <user@spark.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","The output is:
Spark SQ:6828127
Hive:6980574.1269


∑¢◊‘ Õ¯“◊” œ‰¥Û ¶
‘⁄2016ƒÍ04‘¬20»’ 18:06£¨FangFang Chen –¥µ¿:
Hi all,
Please give some suggestions. Thanks


With following same sql, spark sql and hive give different result. The sql is sum(decimal(38,18)) columns.
Select sum(column) from table;
column is defined as decimal(38,18).


Spark version:1.5.3
Hive version:2.0.0


∑¢◊‘ Õ¯“◊” œ‰¥Û ¶


"
"""FangFang Chen"" <lulynn_2015_spark@163.com>","Wed, 20 Apr 2016 20:25:59 +0800 (GMT+08:00)","=?GBK?Q?=BB=D8=B8=B4=A3=BA=BB=D8=B8=B4=A3=BASpark_sql_and_hive_into_?=
 =?GBK?Q?different_result_with_same_sql?=","""user@spark.apache.org"" <user@spark.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","I found spark sql lost precision, and handle data as int with some rule. Following is data got via hive shell and spark sql, with same sql to same hive table:
Hive:
0.4
0.5
1.8
0.4
0.49
1.5
Spark sql:
1
2
2
Seems the handle rule is: when decimal point data <0.5 then to 0, when decimal point data>=0.5 then to 1.


Is this a bug or some configuration thing? Please give some suggestions. Thanks


∑¢◊‘ Õ¯“◊” œ‰¥Û ¶
‘⁄2016ƒÍ04‘¬20»’ 18:45£¨FangFang Chen –¥µ¿:
The output is:
Spark SQ:6828127
Hive:6980574.1269


∑¢◊‘ Õ¯“◊” œ‰¥Û ¶
‘⁄2016ƒÍ04‘¬20»’ 18:06£¨FangFang Chen –¥µ¿:
Hi all,
Please give some suggestions. Thanks


With following same sql, spark sql and hive give different result. The sql is sum(decimal(38,18)) columns.
Select sum(column) from table;
column is defined as decimal(38,18).


Spark version:1.5.3
Hive version:2.0.0


∑¢◊‘ Õ¯“◊” œ‰¥Û ¶





"
atootoonchian <ali@levyx.com>,"Wed, 20 Apr 2016 10:45:48 -0700 (MST)",Improving system design logging in spark,dev@spark.apache.org,"Current spark logging mechanism can be improved by adding the following
parameters. It will help in understanding system bottlenecks and provide
useful guidelines for Spark application developer to design an optimized
application.

1. Shuffle Read Local Time: Time for a task to read shuffle data from local
storage.
2. Shuffle Read Remote Time: Time for a  task to read shuffle data from
remote node.
3. Distribution processing time between computation, I/O, network: Show
distribution of processing time of each task between computation, reading
data from, and reading data from network.
4. Average I/O bandwidth: Average time of I/O throughput for each task when
it fetches data from disk.
5. Average Network bandwidth: Average network throughput for each task when
it fetches data from remote nodes.




--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 20 Apr 2016 10:47:53 -0700",Re: Improving system design logging in spark,atootoonchian <ali@levyx.com>,"Interesting.

For #3:

bq. reading data from,

I guess you meant reading from disk.


"
Joseph Bradley <joseph@databricks.com>,"Wed, 20 Apr 2016 15:10:15 -0700",Re: Organizing Spark ML example packages,Bryan Cutler <cutlerb@gmail.com>,"Sounds good to me.  I'd request we be strict during this process about
requiring *no* changes to the example itself, which will make review easier.


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Thu, 21 Apr 2016 13:26:06 +0900",Re: Improving system design logging in spark,atootoonchian <ali@levyx.com>,"Hi,

As for #1 and #2, seems it is hard to catch remote/local fetching time
because they are overlapped with each other: See
`ShuffleBlockFetcherIterator`.
IMO the current metric there (catching block time to fetch data from a
queue) is kind of enough for most of users because remote fetching could be
a bottleneck in case the metric gets worse.
Any benefit to handle respective time, remote and local?

// maropu





-- 
---
Takeshi Yamamuro
"
"""FangFang Chen"" <lulynn_2015_spark@163.com>","Thu, 21 Apr 2016 14:34:44 +0800 (GMT+08:00)","=?UTF-8?Q?=E5=9B=9E=E5=A4=8D=EF=BC=9ARe:_=E5=9B=9E?=
 =?UTF-8?Q?=E5=A4=8D=EF=BC=9ASpark_?= =?UTF-8?Q?sql_and_hive_int?=
 =?UTF-8?Q?o_different_result_with_same_sql?=","""Ted Yu"" <yuzhihong@gmail.com>","maybe I found the root cause from spark doc:
""Unlimited precision decimal columns are no longer supported, instead Spark SQL enforces a maximum precision of 38. When inferring schema from BigDecimal objects, a precision of (38, 18) is now used. When no precision is specified in DDL then the default remainsDecimal(10, 0).""
I got decimal(38,18) when describe this table, while got decimal when show create this table. Seems spark is getting the scheme information based on create table side. Correct?
Is there any workaround to resolve this problem? Beside alter hive table column type from decimal to decimal with precision.


Thanks






ÂèëËá™ ÁΩëÊòìÈÇÆÁÆ±Â§ßÂ∏à
Âú®2016Âπ¥04Êúà20Êó• 20:47ÔºåTed Yu ÂÜôÈÅì:
Do you mind trying out build from master branch ?


1.5.3 is a bit old.



On Wed, Apr 20, 2016 at 5:25 AM, FangFang Chen <lulynn_2015_spark@163.com> wrote:

I found spark sql lost precision, and handle data as int with some rule. Following is data got via hive shell and spark sql, with same sql to same hive table:
Hive:
0.4
0.5
1.8
0.4
0.49
1.5
Spark sql:
1
2
2
Seems the handle rule is: when decimal point data <0.5 then to 0, when decimal point data>=0.5 then to 1.


Is this a bug or some configuration thing? Please give some suggestions. Thanks


ÂèëËá™ ÁΩëÊòìÈÇÆÁÆ±Â§ßÂ∏à
Âú®2016Âπ¥04Êúà20Êó• 18:45ÔºåFangFang Chen ÂÜôÈÅì:
The output is:
Spark SQ:6828127
Hive:6980574.1269


ÂèëËá™ ÁΩëÊòìÈÇÆÁÆ±Â§ßÂ∏à
Âú®2016Âπ¥04Êúà20Êó• 18:06ÔºåFangFang Chen ÂÜôÈÅì:
Hi all,
Please give some suggestions. Thanks


With following same sql, spark sql and hive give different result. The sql is sum(decimal(38,18)) columns.
Select sum(column) from table;
column is defined as decimal(38,18).


Spark version:1.5.3
Hive version:2.0.0


ÂèëËá™ ÁΩëÊòìÈÇÆÁÆ±Â§ßÂ∏à










"
atootoonchian <ali@levyx.com>,"Thu, 21 Apr 2016 10:29:53 -0700 (MST)",[Spark-SQL] Reduce Shuffle Data by pushing filter toward storage,dev@spark.apache.org,"SQL query planner can have intelligence to push down filter commands towards
the storage layer. If we optimize the query planner such that the IO to the
storage is reduced at the cost of running multiple filters (i.e., compute),
this should be desirable when the system is IO bound. An example to prove
the case in point is below from TPCH test bench:Let‚Äôs look at query q19 of
TPCH test bench.select    sum(l_extendedprice* (1 - l_discount)) as
revenuefrom lineitem, partwhere      ( p_partkey = l_partkey        and
p_brand = 'Brand#12'        and p_container in ('SM CASE', 'SM BOX', 'SM
PACK', 'SM PKG')        and l_quantity >= 1 and l_quantity <= 1 + 10       
and p_size between 1 and 5        and l_shipmode in ('AIR', 'AIR REG')       
and l_shipinstruct = 'DELIVER IN PERSON')      or      ( p_partkey =
l_partkey        and p_brand = 'Brand#23'        and p_container in ('MED
BAG', 'MED BOX', 'MED PKG', 'MED PACK')        and l_quantity >= 10 and
l_quantity <= 10 + 10        and p_size between 1 and 10        and
l_shipmode in ('AIR', 'AIR REG')        and l_shipinstruct = 'DELIVER IN
PERSON')      or      ( p_partkey = l_partkey        and p_brand =
'Brand#34'        and p_container in ('LG CASE', 'LG BOX', 'LG PACK', 'LG
PKG')        and l_quantity >= 20 and l_quantity <= 20 + 10        and
p_size between 1 and 15        and l_shipmode in ('AIR', 'AIR REG')       
and l_shipinstruct = 'DELIVER IN PERSON')Latest version of Spark creates a
following planner (not exactly, more readable planner) to execute
q19.Aggregate [(sum(cast((l_extendedprice * (1.0 - l_discount))  Project
[l_extendedprice,l_discount]    Join Inner, Some(((p_partkey = l_partkey) &&
((((((   (p_brand = Brand#12) &&     p_container IN (SM CASE,SM BOX,SM
PACK,SM PKG)) &&    (l_quantity >= 1.0)) && (l_quantity <= 11.0)) &&   
(p_size <= 5)) || (((((p_brand = Brand#23) &&      p_container IN (MED
BAG,MED BOX,MED PKG,MED PACK)) &&     (l_quantity >= 10.0)) && (l_quantity
<= 20.0)) &&     (p_size <= 10))) || (((((p_brand = Brand#34) &&     
p_container IN (LG CASE,LG BOX,LG PACK,LG PKG)) &&     (l_quantity >= 20.0))
&& (l_quantity <= 30.0)) &&     (p_size <= 15)))))      Project [l_partkey,
l_quantity, l_extendedprice, l_discount]        Filter
((isnotnull(l_partkey) &&                 (isnotnull(l_shipinstruct) &&                
(l_shipmode IN (AIR,AIR REG) &&                 (l_shipinstruct = DELIVER IN
PERSON))))          LogicalRDD [l_orderkey, l_partkey, l_suppkey,
l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag,
l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct,
l_shipmode, l_comment], MapPartitionsRDD[316]       Project [p_partkey,
p_brand, p_size, p_container]        Filter ((isnotnull(p_partkey) &&    
(isnotnull(p_size) &&     (cast(cast(p_size as decimal(20,0)) as int) >=
1)))          LogicalRDD [p_partkey, p_name, p_mfgr, p_brand, p_type,
p_size, p_container, p_retailprice, p_comment], MapPartitionsRDD[314]    As
you see only three filter commands are pushed before join process is
executed.  l_shipmode IN (AIR,AIR REG)  l_shipinstruct = DELIVER IN PERSON 
(cast(cast(p_size as decimal(20,0)) as int) >= 1)
And the following filters are applied during the join process  p_brand =
Brand#12  p_container IN (SM CASE,SM BOX,SM PACK,SM PKG)   l_quantity >= 1.0
&& l_quantity <= 11.0   p_size <= 5    p_brand = Brand#23   p_container IN
(MED BAG,MED BOX,MED PKG,MED PACK)   l_quantity >= 10.0 && l_quantity <=
20.0   p_size <= 10   p_brand = Brand#34   p_container IN (LG CASE,LG BOX,LG
PACK,LG PKG)   l_quantity >= 20.0 && l_quantity <= 30.0  p_size <= 15Let‚Äôs
look at the following sequence of SQL commands which produce same result.val
partDfFilter = sqlContext.sql(""""""        |select p_brand, p_partkey from
part         |where        | (p_brand = 'Brand#12'        |   and
p_container in ('SM CASE', 'SM BOX', 'SM PACK', 'SM PKG')        |   and
p_size between 1 and 5)        | or        | (p_brand = 'Brand#23'        |  
and p_container in ('MED BAG', 'MED BOX', 'MED PKG', 'MED PACK')        |  
and p_size between 1 and 10)        | or        | (p_brand = 'Brand#34'       
|   and p_container in ('LG CASE', 'LG BOX', 'LG PACK', 'LG PKG')        |  
and p_size between 1 and 15)       """""".stripMargin)val itemLineDfFilter =
sqlContext.sql(""""""        |select         | l_partkey, l_quantity,
l_extendedprice, l_discount from lineitem        |where        | (l_quantity
')       
|   and l_shipinstruct = 'DELIVER IN PERSON')     
"""""".stripMargin)partDfFilter.registerTempTable(""partFilter"")itemLineDfFilter.registerTempTable(""lineitemFilter"")var
q19Query = """"""                 |select        | sum(l_extendedprice* (1 -
l_discount)) as revenue        |from        | lineitemFilter,        |
partFilter        |where        | (p_partkey = l_partkey        |   and
p_brand = 'Brand#12'        |   and l_quantity >= 1 and l_quantity <= 1 +
10)        | or        | ( p_partkey = l_partkey        |   and p_brand =
'Brand#23'        |   and l_quantity >= 10 and l_quantity <= 10 + 10)       
| or        | ( p_partkey = l_partkey        |   and p_brand = 'Brand#34'       
|   and l_quantity >= 20 and l_quantity <= 20 + 10)      """""".stripMarginAnd
as following planner shows how spark will execute new q19 query.Aggregate
[(sum(cast((l_extendedprice * (1.0 - l_discount))  Project
[l_extendedprice,l_discount]    Join Inner, Some(((p_partkey = l_partkey) &&
(((((p_brand = Brand#12) &&     (l_quantity >= 1.0)) && (l_quantity <=
11.0)) ||   (((p_brand = Brand#23) &&     (l_quantity >= 10.0)) &&
(l_quantity <= 20.0))) ||   (((p_brand = Brand#34) &&     (l_quantity >=
20.0)) && (l_quantity <= 30.0)))))      Project [l_partkey, l_quantity,
l_extendedprice, l_discount]        Filter ((isnotnull(l_partkey) &&               
((isnotnull(l_shipinstruct) &&                  isnotnull(l_quantity)) &&              
(((cast(l_quantity as float) >= 1.0) &&                 (cast(l_quantity as
float) <= 30.0)) &&                 (l_shipmode IN (AIR,AIR REG) &&                
(l_shipinstruct = DELIVER IN PERSON)))))          LogicalRDD [l_orderkey,
l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount,
l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate,
l_shipinstruct, l_shipmode, l_comment], MapPartitionsRDD[316]           
Project [p_partkey, p_brand, p_size, p_container]              Filter
((isnotnull(p_partkey) && 		isnotnull(cast(cast(p_partkey as decimal(20,0))
as int))) && (isnotnull(p_size) &&             ((cast(cast(p_size as
decimal(20,0)) as int) >= 1) &&            (((((p_brand = Brand#12) &&                 
p_container IN (SM CASE,SM BOX,SM PACK,SM PKG)) && 		     (cast(cast(p_size
as decimal(20,0)) as int) <= 5)) ||   (((p_brand = Brand#23) &&     
p_container IN (MED BAG,MED BOX,MED PKG,MED PACK)) &&     (cast(cast(p_size
as decimal(20,0)) as int) <= 10))) ||   (((p_brand = Brand#34) &&     
p_container IN (LG CASE,LG BOX,LG PACK,LG PKG)) &&     (cast(cast(p_size as
decimal(20,0)) as int) <= 15))))))                  LogicalRDD [p_partkey,
p_name, p_mfgr, p_brand, p_type, p_size, p_container, p_retailprice,
p_comment], MapPartitionsRDD[314]With new approach all filter commands is
pushed down beyond join process   l_shipmode IN (AIR,AIR REG) 
l_shipinstruct = DELIVER IN PERSON   cast(cast(p_size as decimal(20,0)) as
int) >= 1)  p_brand = Brand#12  p_container IN (SM CASE,SM BOX,SM PACK,SM
PKG)  l_quantity >= 1.0 && l_quantity <= 11.0   p_size <= 5   p_brand =
Brand#23   p_container IN (MED BAG,MED BOX,MED PKG,MED PACK)   l_quantity >=
10.0 && l_quantity <= 20.0   p_size <= 10   p_brand = Brand#34   p_container
IN (LG CASE,LG BOX,LG PACK,LG PKG)   l_quantity >= 20.0 && l_quantity <=
30.0  p_size <= 15But still some filter commands needs to be executed during
join process to distinguish different sets of items. In other words some
filter commands are re-evaluated.  p_brand = Brand#12  l_quantity >= 1.0 &&
l_quantity <= 11.0   p_brand = Brand#23   l_quantity >= 10.0 && l_quantity
<= 20.0   p_brand = Brand#34   l_quantity#807 >= 20.0 && l_quantity#807 <=
30.0Our main goal to push down filter as much as possible is to minimize I/O
and maximize processor utilization. So let‚Äôs compare result of original q19
and modified q19 from I/O point of
view.+--------+--------+---------------------------------------------+--------------------------------------------+|
TPCH   | Stage  |              Q19                                       |                         
Q19 modified              || Scale   |          
+----------+---------------+----------------+----------+----------------+---------------+|
Factor  |           | Input     | Shuffle Read  | Shuffle Write  | Input    
| Shuffle Read   | Shuffle Write
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
1         | 1         | 724 MB  |                    | 4.2 MB           |
724 MB  |                      | 2.7 MB        
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
1         | 2         | 23.0 MB |                    | 4.0 MB           |
23.0 MB |                      | 22.9 KB       
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
1         | 3         |              | 8.2 MB         | 11.0 KB         |             
| 2.7 MB           | 11.0 KB       
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
1         | 4         |              | 11.0 KB        |                    
|              | 11.0 KB          |                   
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
10       | 1         | 7.2 GB   |                    | 43.5 MB         | 7.2
GB   |                      | 28.0 MB       
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
10       | 2         | 232 MB  |                    | 39.1 MB         | 232
MB  |                      | 146.2 KB      
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
10       | 3         |              | 82.5 MB       | 11.0 KB          |             
| 28.1 MB         | 11.0 KB       
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
10       | 4         |              | 11.0 KB        |                     
|              | 11.0 KB         |                   
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
100     | 1         | 74.1 GB |                     | 448 MB          | 74.1
GB |                      | 266 MB       
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
100     | 2         | 2.3 GB   |                     | 385 MB          | 2.3
GB   |                     | 1570 KB       
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
100     | 3         |              | 834 MB         | 11.0 KB         |             
| 288 MB          | 11.0 KB       
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+|
100     | 4         |              | 11.0 KB        |                      |             
| 11.0 KB         |                   
|+--------+--------+----------+---------------+----------------+----------+----------------+---------------+As
rate of read and write amplification reduction for each scale factor is
shown in the following
table.+--------------------+--------------------------+------------------------------+--------+|
TPCH Scale Facto  | Q19 Shuffle Data        | Q19 Modified Shuffle Data  |
Rate   
|+--------------------+--------------------------+------------------------------+--------+|
1                         |  8.211 MB                   | 2.733 MB                        
|  3.00   |    
+--------------------+--------------------------+------------------------------+--------+|
10                       |  82.611 MB                 | 28.157 MB                      
|  2.93   |    
+--------------------+--------------------------+------------------------------+--------+|
100                     |  834.311 MB               | 288.081 MB                     
|  2.89   |    
+--------------------+--------------------------+------------------------------+--------+So
as you see shuffle read and write can be reduced by factor of 3 if we can
push more intelligent toward of storage.



--
3.nabble.com/Spark-SQL-Reduce-Shuffle-Data-by-pushing-filter-toward-storage-tp17296.html
om."
atootoonchian <ali@levyx.com>,"Thu, 21 Apr 2016 11:07:38 -0700 (MST)",[Spark-SQL] Reduce Shuffle Data by pushing filter toward storage,dev@spark.apache.org,"SQL query planner can have intelligence to push down filter commands towards
the storage layer. If we optimize the query planner such that the IO to the
storage is reduced at the cost of running multiple filters (i.e., compute),
this should be desirable when the system is IO bound. An example to prove
the case in point is below from TPCH test bench:

Let‚Äôs look at query q19 of TPCH test bench.
select
    sum(l_extendedprice* (1 - l_discount)) as revenue
from lineitem, part
where
      ( p_partkey = l_partkey
        and p_brand = 'Brand#12'
        and p_container in ('SM CASE', 'SM BOX', 'SM PACK', 'SM PKG')
        and l_quantity >= 1 and l_quantity <= 1 + 10
        and p_size between 1 and 5
        and l_shipmode in ('AIR', 'AIR REG')
        and l_shipinstruct = 'DELIVER IN PERSON')
      or
      ( p_partkey = l_partkey
        and p_brand = 'Brand#23'
        and p_container in ('MED BAG', 'MED BOX', 'MED PKG', 'MED PACK')
        and l_quantity >= 10 and l_quantity <= 10 + 10
        and p_size between 1 and 10
        and l_shipmode in ('AIR', 'AIR REG')
        and l_shipinstruct = 'DELIVER IN PERSON')
      or
      ( p_partkey = l_partkey
        and p_brand = 'Brand#34'
        and p_container in ('LG CASE', 'LG BOX', 'LG PACK', 'LG PKG')
        and l_quantity >= 20 and l_quantity <= 20 + 10
        and p_size between 1 and 15
        and l_shipmode in ('AIR', 'AIR REG')
        and l_shipinstruct = 'DELIVER IN PERSON')

Latest version of Spark creates a following planner (not exactly, more
readable planner) to execute q19.
Aggregate [(sum(cast((l_extendedprice * (1.0 - l_discount))
  Project [l_extendedprice,l_discount]
    Join Inner, Some(((p_partkey = l_partkey) && 
((((((
   (p_brand = Brand#12) && 
    p_container IN (SM CASE,SM BOX,SM PACK,SM PKG)) && 
   (l_quantity >= 1.0)) && (l_quantity <= 11.0)) && 
   (p_size <= 5)) || 
(((((p_brand = Brand#23) && 
     p_container IN (MED BAG,MED BOX,MED PKG,MED PACK)) && 
    (l_quantity >= 10.0)) && (l_quantity <= 20.0)) && 
    (p_size <= 10))) || 
(((((p_brand = Brand#34) && 
     p_container IN (LG CASE,LG BOX,LG PACK,LG PKG)) && 
    (l_quantity >= 20.0)) && (l_quantity <= 30.0)) && 
    (p_size <= 15)))))
      Project [l_partkey, l_quantity, l_extendedprice, l_discount]
        Filter ((isnotnull(l_partkey) && 
                (isnotnull(l_shipinstruct) && 
                (l_shipmode IN (AIR,AIR REG) && 
                (l_shipinstruct = DELIVER IN PERSON))))
          LogicalRDD [l_orderkey, l_partkey, l_suppkey, l_linenumber,
l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus,
l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode,
l_comment], MapPartitionsRDD[316] 
      Project [p_partkey, p_brand, p_size, p_container]
        Filter ((isnotnull(p_partkey) && 
    (isnotnull(p_size) && 
    (cast(cast(p_size as decimal(20,0)) as int) >= 1)))
          LogicalRDD [p_partkey, p_name, p_mfgr, p_brand, p_type, p_size,
p_container, p_retailprice, p_comment], MapPartitionsRDD[314]    

As you see only three filter commands are pushed before join process is
executed.
  l_shipmode IN (AIR,AIR REG)
  l_shipinstruct = DELIVER IN PERSON
  (cast(cast(p_size as decimal(20,0)) as int) >= 1)

And the following filters are applied during the join process
  p_brand = Brand#12
  p_container IN (SM CASE,SM BOX,SM PACK,SM PKG) 
  l_quantity >= 1.0 && l_quantity <= 11.0 
  p_size <= 5  
  p_brand = Brand#23 
  p_container IN (MED BAG,MED BOX,MED PKG,MED PACK) 
  l_quantity >= 10.0 && l_quantity <= 20.0 
  p_size <= 10 
  p_brand = Brand#34 
  p_container IN (LG CASE,LG BOX,LG PACK,LG PKG) 
  l_quantity >= 20.0 && l_quantity <= 30.0
  p_size <= 15

Let‚Äôs look at the following sequence of SQL commands which produce same
result.
val partDfFilter = sqlContext.sql(""""""
        |select p_brand, p_partkey from part 
        |where
        | (p_brand = 'Brand#12'
        |   and p_container in ('SM CASE', 'SM BOX', 'SM PACK', 'SM PKG')
        |   and p_size between 1 and 5)
        | or
        | (p_brand = 'Brand#23'
        |   and p_container in ('MED BAG', 'MED BOX', 'MED PKG', 'MED PACK')
        |   and p_size between 1 and 10)
        | or
        | (p_brand = 'Brand#34'
        |   and p_container in ('LG CASE', 'LG BOX', 'LG PACK', 'LG PKG')
        |   and p_size between 1 and 15)
       """""".stripMargin)

val itemLineDfFilter = sqlContext.sql(""""""
        |select 
        | l_partkey, l_quantity, l_extendedprice, l_discount from lineitem
        |where
        | (l_quantity >= 1 and l_quantity <= 30
        |   and l_shipmode in ('AIR', 'AIR REG')
        |   and l_shipinstruct = 'DELIVER IN PERSON')
      """""".stripMargin)

partDfFilter.registerTempTable(""partFilter"")
itemLineDfFilter.registerTempTable(""lineitemFilter"")

var q19Query = """"""         
        |select
        | sum(l_extendedprice* (1 - l_discount)) as revenue
        |from
        | lineitemFilter,
        | partFilter
        |where
        | (p_partkey = l_partkey
        |   and p_brand = 'Brand#12'
        |   and l_quantity >= 1 and l_quantity <= 1 + 10)
        | or
        | ( p_partkey = l_partkey
        |   and p_brand = 'Brand#23'
        |   and l_quantity >= 10 and l_quantity <= 10 + 10)
        | or
        | ( p_partkey = l_partkey
        |   and p_brand = 'Brand#34'
        |   and l_quantity >= 20 and l_quantity <= 20 + 10)
      """""".stripMargin

And as following planner shows how spark will execute new q19 query.
Aggregate [(sum(cast((l_extendedprice * (1.0 - l_discount))
  Project [l_extendedprice,l_discount]
    Join Inner, Some(((p_partkey = l_partkey) && 
(((((p_brand = Brand#12) && 
    (l_quantity >= 1.0)) && (l_quantity <= 11.0)) || 
  (((p_brand = Brand#23) && 
    (l_quantity >= 10.0)) && (l_quantity <= 20.0))) || 
  (((p_brand = Brand#34) && 
    (l_quantity >= 20.0)) && (l_quantity <= 30.0)))))
      Project [l_partkey, l_quantity, l_extendedprice, l_discount]
        Filter ((isnotnull(l_partkey) && 
               ((isnotnull(l_shipinstruct) && 
                 isnotnull(l_quantity)) && 
              (((cast(l_quantity as float) >= 1.0) && 
                (cast(l_quantity as float) <= 30.0)) && 
                (l_shipmode IN (AIR,AIR REG) && 
                (l_shipinstruct = DELIVER IN PERSON)))))
          LogicalRDD [l_orderkey, l_partkey, l_suppkey, l_linenumber,
l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus,
l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode,
l_comment], MapPartitionsRDD[316]
            Project [p_partkey, p_brand, p_size, p_container]
              Filter ((isnotnull(p_partkey) &&
 		isnotnull(cast(cast(p_partkey as decimal(20,0)) as int))) && 
(isnotnull(p_size) && 
            ((cast(cast(p_size as decimal(20,0)) as int) >= 1) &&
            (((((p_brand = Brand#12) && 
                 p_container IN (SM CASE,SM BOX,SM PACK,SM PKG)) &&
 		     (cast(cast(p_size as decimal(20,0)) as int) <= 5)) || 
  (((p_brand = Brand#23) && 
     p_container IN (MED BAG,MED BOX,MED PKG,MED PACK)) &&
     (cast(cast(p_size as decimal(20,0)) as int) <= 10))) || 
  (((p_brand = Brand#34) && 
     p_container IN (LG CASE,LG BOX,LG PACK,LG PKG)) &&
     (cast(cast(p_size as decimal(20,0)) as int) <= 15))))))
                  LogicalRDD [p_partkey, p_name, p_mfgr, p_brand, p_type,
p_size, p_container, p_retailprice, p_comment], MapPartitionsRDD[314]

With new approach all filter commands is pushed down beyond join process 
  l_shipmode IN (AIR,AIR REG)
  l_shipinstruct = DELIVER IN PERSON
  cast(cast(p_size as decimal(20,0)) as int) >= 1)
  p_brand = Brand#12
  p_container IN (SM CASE,SM BOX,SM PACK,SM PKG)
  l_quantity >= 1.0 && l_quantity <= 11.0 
  p_size <= 5 
  p_brand = Brand#23 
  p_container IN (MED BAG,MED BOX,MED PKG,MED PACK) 
  l_quantity >= 10.0 && l_quantity <= 20.0 
  p_size <= 10 
  p_brand = Brand#34 
  p_container IN (LG CASE,LG BOX,LG PACK,LG PKG) 
  l_quantity >= 20.0 && l_quantity <= 30.0
  p_size <= 15

But still some filter commands needs to be executed during join process to
distinguish different sets of items. In other words some filter commands are
re-evaluated.
  p_brand = Brand#12
  l_quantity >= 1.0 && l_quantity <= 11.0 
  p_brand = Brand#23 
  l_quantity >= 10.0 && l_quantity <= 20.0 
  p_brand = Brand#34 
  l_quantity#807 >= 20.0 && l_quantity#807 <= 30.0

Our main goal to push down filter as much as possible is to minimize I/O and
maximize processor utilization. So let‚Äôs compare result of original q19 and
modified q19 from I/O point of view.

+--------+--------+---------------------------------------------+--------------------------------------------+
| TPCH   | Stage  | Q19                                                    |
Q19 modified                                      | 
| Scale   |          
+----------+---------------+----------------+----------+----------------+---------------+ 
| Factor  |           | Input      | Shuffle Read | Shuffle Write  | Input    
| Shuffle Read  | Shuffle Write | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 1         | 1         | 724 MB  |                    | 4.2 MB           |
724 MB  |                      | 2.7 MB         | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 1         | 2         | 23.0 MB |                    | 4.0 MB           |
23.0 MB |                      | 22.9 KB        | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 1         | 3         |              | 8.2 MB         | 11.0 KB         |             
| 2.7 MB           | 11.0 KB        | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 1         | 4         |              | 11.0 KB        |                    
|               | 11.0 KB        |                     | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 10       | 1         | 7.2 GB   |                     | 43.5 MB        |
7.2 GB    |                     | 28.0 MB        | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 10       | 2         | 232 MB  |                     | 39.1 MB        |
232 MB   |                     | 146.2 KB       | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 10       | 3         |              | 82.5 MB        | 11.0 KB         |             
| 28.1 MB         | 11.0 KB        | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 10       | 4         |              | 11.0 KB        |                     
|              | 11.0 KB         |                    | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 100     | 1         | 74.1 GB |                     | 448 MB          |
74.1 GB |                      | 266 MB        | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 100     | 2         | 2.3 GB   |                     | 385 MB          |
2.3 GB   |                      | 1570 KB       | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 100     | 3         |              | 834 MB         | 11.0 KB         |             
| 288 MB          | 11.0 KB        | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+ 
| 100     | 4         |              | 11.0 KB        |                     
|              | 11.0 KB         |                    | 
+--------+--------+----------+---------------+----------------+----------+----------------+---------------+

As rate of read and write amplification reduction for each scale factor is
shown in the following table.
+--------------------+--------------------------+------------------------------+--------+ 
| TPCH Scale Facto  | Q19 Shuffle Data         | Q19 Modified Shuffle Data |
Rate    | 
+--------------------+--------------------------+------------------------------+--------+ 
| 1                         | 8.211 MB                    | 2.733 MB                        
| 3.00    | 
+--------------------+--------------------------+------------------------------+--------+ 
| 10                       | 82.611 MB                  | 28.157 MB                      
| 2.93    | 
+--------------------+--------------------------+------------------------------+--------+ 
| 100                     | 834.311 MB                | 288.081 MB                     
| 2.89    | 
+--------------------+--------------------------+------------------------------+--------+
So as you see shuffle read and write amplification can be reduced by factor
of 3 if we can push more intelligent toward of storage.






--
3.nabble.com/Spark-SQL-Reduce-Shuffle-Data-by-pushing-filter-toward-storage-tp17297.html
om.

---------------------------------------------------------------------


"
Marcin Tustin <mtustin@handybook.com>,"Thu, 21 Apr 2016 14:10:29 -0400",Re: [Spark-SQL] Reduce Shuffle Data by pushing filter toward storage,"atootoonchian <ali@levyx.com>, dev <dev@spark.apache.org>","I think that's an important result. Could you format your email to split
out your parts a little more? It all runs together for me in gmail, so it's
hard to follow, and I very much would like to.


he
),
s,
e same
m
s,
 ||
o
al q19 and
------------------------------------+
---------------+
t
+----------------+---------------+
 |
+----------------+---------------+
 |
+----------------+---------------+
 |
+----------------+---------------+
+----------------+---------------+
|
+----------------+---------------+
+----------------+---------------+
|
+----------------+---------------+
+----------------+---------------+
+----------------+---------------+
|
+----------------+---------------+
+----------------+---------------+
+----------------+---------------+
s
------+--------+
a
------+--------+
------+--------+
------+--------+
------+--------+
or
e-Shuffle-Data-by-pushing-filter-toward-storage-tp17297.html

-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
atootoonchian <ali@levyx.com>,"Thu, 21 Apr 2016 11:24:51 -0700 (MST)","Re: [Spark-SQL] Reduce Shuffle Data by pushing filter toward
 storage",dev@spark.apache.org,"Hi Marcin

I attached a pdf format of issue.

Reduce_Shuffle_Data_by_pushing_filter_toward_storage.pdf
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n17299/Reduce_Shuffle_Data_by_pushing_filter_toward_storage.pdf>  



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 21 Apr 2016 11:38:38 -0700",Re: [Spark-SQL] Reduce Shuffle Data by pushing filter toward storage,atootoonchian <ali@levyx.com>,"Interesting analysis. 

Can you log a JIRA ?

ds
e
,
,
 same
)
,


re
nd
l q19 and
-----------------------------------+
 |
--------------+ 
    
----------------+---------------+ 

----------------+---------------+ 

----------------+---------------+ 
             
----------------+---------------+ 
   
----------------+---------------+ 

----------------+---------------+ 
----------------+---------------+ 
             
----------------+---------------+ 
   
----------------+---------------+ 
----------------+---------------+ 

----------------+---------------+ 
            
----------------+---------------+ 
  
----------------+---------------+

-----+--------+ 
 |
-----+--------+ 
                  
-----+--------+ 
               
-----+--------+ 
             
-----+--------+
r
n3.nabble.com/Spark-SQL-Reduce-Shuffle-Data-by-pushing-filter-toward-storage-tp17297.html
com.

---------------------------------------------------------------------


"
tgensol <thibaut.gensollen@gmail.com>,"Thu, 21 Apr 2016 11:47:18 -0700 (MST)",[GRAPHX] Graph Algorithms and Spark,dev@spark.apache.org,"Hi there,

I am working in a group of the University of Michigan, and we are trying to
make (and find first) some Distributed graph algorithms. 

I know spark, and I found GraphX. I read the docs, but I only found Latent
Dirichlet Allocation algorithms working with GraphX, so I was wondering why
?

Basically, the groupe wants to implement Minimal Spanning Tree, kNN,
shortest path at first.

So my askings are :
Is graphX enough stable for developing this kind of algorithms on it ?
Do you know some algorithms like these working on top of GraphX ? And if no,
why do you think, nobody tried to do it ? Is this too hard ? Or just because
nobody needs it ?

Maybe, it is only my knowledge about GraphX which is weak, and it is not
possible to make these algorithms with GraphX.

Thanking you in advance,
Best regards,

Thibaut 



--

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 21 Apr 2016 12:32:09 -0700",Re: [GRAPHX] Graph Algorithms and Spark,tgensol <thibaut.gensollen@gmail.com>,"Hi,

   1. Yep, GraphX is stable and would be a good choice for you to implement
   algorithms. For a quick intro you can refer to our Strata MLlib tutorial
   GraphX slides http://goo.gl/Ffq2Az
   2. GraphX has implemented algorithms like PageRank &
   ConnectedComponents[1]
   3. It also has primitives to develop the kind of algorithms that you are
   talking about
   4. For you to implement interesting algorithms, the main APIs of
   interest would be the pregel API and the aggregateMessages API[2]. Am sure
   you will also use the map*, subgraph and the join APIs.

Cheers
<k/>
[1]
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.GraphOps
[2]
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.Graph


"
Robin East <robin.east@xense.co.uk>,"Thu, 21 Apr 2016 20:53:27 +0100",Re: [GRAPHX] Graph Algorithms and Spark,tgensol <thibaut.gensollen@gmail.com>,"Hi

Aside from LDA, which is implemented in MLLib, GraphX has the following built-in algorithms:

PageRank/Personalised PageRank
Connected Components
Strongly Connected Components
Triangle Count
Shortest Paths
Label Propagation

It also implements a version of Pregel framework, a form of bulk-synchronous parallel processing that is the foundation of most of the above algorithms. We cover other algorithms in our book and if you search on google you will find a number of other examples.

-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action <http://www.manning.com/books/spark-graphx-in-action>





trying to
Latent
wondering why
if no,
because
not
http://apache-spark-developers-list.1001551.n3.nabble.com/GRAPHX-Graph-Algorithms-and-Spark-tp17301.html
Nabble.com.

"
Zhan Zhang <zzhang@hortonworks.com>,"Thu, 21 Apr 2016 20:25:58 +0000",Re: [GRAPHX] Graph Algorithms and Spark,Robin East <robin.east@xense.co.uk>,"
You can take a look at this blog from data bricks about GraphFrames

https://databricks.com/blog/2016/03/03/introducing-graphframes.html

Thanks.

Zhan Zhang


Hi

Aside from LDA, which is implemented in MLLib, GraphX has the following built-in algorithms:


  *   PageRank/Personalised PageRank
  *   Connected Components
  *   Strongly Connected Components
  *   Triangle Count
  *   Shortest Paths
  *   Label Propagation

It also implements a version of Pregel framework, a form of bulk-synchronous parallel processing that is the foundation of most of the above algorithms. We cover other algorithms in our book and if you search on google you will find a number of other examples.

-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action






Hi there,

I am working in a group of the University of Michigan, and we are trying to
make (and find first) some Distributed graph algorithms.

I know spark, and I found GraphX. I read the docs, but I only found Latent
Dirichlet Allocation algorithms working with GraphX, so I was wondering why
?

Basically, the groupe wants to implement Minimal Spanning Tree, kNN,
shortest path at first.

So my askings are :
Is graphX enough stable for developing this kind of algorithms on it ?
Do you know some algorithms like these working on top of GraphX ? And if no,
why do you think, nobody tried to do it ? Is this too hard ? Or just because
nobody needs it ?

Maybe, it is only my knowledge about GraphX which is weak, and it is not
possible to make these algorithms with GraphX.

Thanking you in advance,
Best regards,

Thibaut



--
3.nabble.com/GRAPHX-Graph-Algorithms-and-Spark-tp17301.html
om<http://nabble.com/>.

---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>



"
Zhan Zhang <zzhang@hortonworks.com>,"Thu, 21 Apr 2016 20:28:09 +0000","Re: RFC: Remote ""HBaseTest"" from examples?",Saisai Shao <sai.sai.shao@gmail.com>,"FYI: There are several pending patches for DataFrame support on top of HBase.

Thanks.

Zhan Zhang


+1, HBaseTest in Spark Example is quite old and obsolete, the HBase connector in HBase repo has evolved a lot, it would be better to guide user to refer to that not here in Spark example. So good to remove it.

Thanks
Saisai

+1; I think that it's preferable for code examples, especially third-party integration examples, to live outside of Spark.

Yea in general I feel examples that bring in a large amount of dependencies should be outside Spark.


Hey all,

Two reasons why I think we should remove that from the examples:

- HBase now has Spark integration in its own repo, so that really
should be the template for how to use HBase from Spark, making that
example less useful, even misleading.

- It brings up a lot of extra dependencies that make the size of the
Spark distribution grow.

Any reason why we shouldn't drop that example?

--
Marcelo

---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>




"
atootoonchian <ali@levyx.com>,"Thu, 21 Apr 2016 13:33:18 -0700 (MST)","Re: [Spark-SQL] Reduce Shuffle Data by pushing filter toward
 storage",dev@spark.apache.org,"I create an issue in Spark project: SPARK-14820



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 21 Apr 2016 13:47:53 -0700","Re: RFC: Remote ""HBaseTest"" from examples?",Zhan Zhang <zzhang@hortonworks.com>,"Zhan:
I have mentioned the JIRA numbers in the thread starting with (note the
typo in subject of this thread):

RFC: Remove ...


"
Ali Tootoonchian <ali@levyx.com>,"Thu, 21 Apr 2016 13:51:57 -0700 (MST)",Re: Improving system design logging in spark,dev@spark.apache.org,"Hi,

My point for #2 is distinguishing between how long does it take for each
task to read a data from disk and transfer it through network to targeted
node. As I know (correct me if I'm wrong) block time to fetch data includes
both reading a data by remote node and transferring it to requested node. If
the block time is bigger than our expectation, from system design, we cannot
identify which component is weakest link, storage or network. 



--

---------------------------------------------------------------------


"
Dimitris Kouzis - Loukas <lookfwd@gmail.com>,"Thu, 21 Apr 2016 22:30:40 +0100",Re: [GRAPHX] Graph Algorithms and Spark,Zhan Zhang <zzhang@hortonworks.com>,"This thread is good. Maybe it should make it to doc or the users group


"
Denny Lee <denny.g.lee@gmail.com>,"Thu, 21 Apr 2016 21:33:56 +0000",Re: [GRAPHX] Graph Algorithms and Spark,"Dimitris Kouzis - Loukas <lookfwd@gmail.com>, Zhan Zhang <zzhang@hortonworks.com>","BTW, we recently had a webinar on GraphFrames at
http://go.databricks.com/graphframes-dataframe-based-graphs-for-apache-spark


"
Niranda Perera <niranda.perera@gmail.com>,"Fri, 22 Apr 2016 11:42:14 +0530",Re: Possible deadlock in registering applications in the recovery mode,Reynold Xin <rxin@databricks.com>,"Hi guys,

any update on this?

Best





-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Sean Busbey <busbey@cloudera.com>,"Fri, 22 Apr 2016 10:48:16 -0500","Re: RFC: Remove ""HBaseTest"" from examples?","""dev@spark.apache.org"" <dev@spark.apache.org>","I'd suggest that the hbase-downstreamer project[1] is a better place
for folks to see these examples. There's already an example for spark
streaming that does not rely on any of the new goodness in the
hbase-spark module[2].

Granted, it uses the Spark Java APIs[3], but we'd be glad to have a
scala based example if someone wanted to translate.

[1]: https://github.com/saintstack/hbase-downstreamer
[2]: https://github.com/saintstack/hbase-downstreamer#spark-streaming-test-application
[3]: https://s.apache.org/apvQ






-- 
busbey

---------------------------------------------------------------------


"
Mike Hynes <91mbbh@gmail.com>,"Fri, 22 Apr 2016 19:40:30 -0400",Re: executor delay in Spark,Raghava Mutharaju <m.vijayaraghava@gmail.com>,"Glad to hear that the problem was solvable! I have not seen delays of this
type for later stages in jobs run by spark-submit, but I do not think it
impossible if your stage has no lineage dependence on other RDDs.

I'm CC'ing the dev list to report of other users observing load imbalance
caused by unusual initial task scheduling. I don't know of ways to avoid
this other than creating a dummy task to synchronize the executors, but
hopefully someone from there can suggest other possibilities.

Mike

"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 23 Apr 2016 12:56:54 +0900","Proposal of closing some PRs which at least one of committers
 suggested so",dev <dev@spark.apache.org>,"Hi all,

I realised that there are many open PRs and it is somehow problematic after
the past discussion (
http://apache-spark-developers-list.1001551.n3.nabble.com/auto-closing-pull-requests-that-have-been-inactive-gt-30-days-td17208.html
).
‚Äã

I looked through them PR by PR and could make a list for PRs which is
suggested to be closed after filtering them with the keyword ""close"" and
""closed""

https://github.com/apache/spark/pull/7647

https://github.com/apache/spark/pull/8195

https://github.com/apache/spark/pull/8741

https://github.com/apache/spark/pull/8972

https://github.com/apache/spark/pull/9490

https://github.com/apache/spark/pull/10419

https://github.com/apache/spark/pull/10761

https://github.com/apache/spark/pull/11003

https://github.com/apache/spark/pull/11201

https://github.com/apache/spark/pull/11803

https://github.com/apache/spark/pull/12111

https://github.com/apache/spark/pull/12442

https://github.com/apache/spark/pull/12524  <- I am 100% not sure for this
one.


I assume (from the past discussion above) it is possible to close PRs by
committers.

Could those PRs maybe be closed if it makes sense?


Thanks!
"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 22 Apr 2016 21:49:53 -0700",Spark streaming Kafka receiver WriteAheadLog question,dev <dev@spark.apache.org>,"Hi,

Is it possible for Kafka receiver generated WriteAheadLogBackedBlockRDD to
hold corresponded Kafka offset range so that during recovery the RDD can
refer back to Kafka queue instead of paying the cost of write ahead log?

I guess there must be a reason here. Could anyone please help me understand?

Thanks,
Renyi.
"
"""Anthony D. Joseph"" <adj@eecs.berkeley.edu>","Fri, 22 Apr 2016 22:18:53 -0700",Help us teach Spark and grow the Spark community,dev@spark.apache.org,"Dear Spark Devs,

Ameet Talwalkar, Brian Clapper, and I are teaching several large free MOOCs
<http://en.wikipedia.org/wiki/Massive_open_online_course> this year on
Apache Spark and we are looking for participants from the community who
would like to help by being Teaching Assistants for the courses. I am a
Professor in Electrical Engineering and Computer Science at UC Berkeley in
the AMPLab <https://amplab.cs.berkeley.edu/>, Ameet is an Assistant
Professor in Computer Science at UCLA , and Brian is the Databricks Senior
Instructor and Application Engineer with more than 32 years of development
experience.

Being a TA is a great opportunity to interact with a wide audience of Spark
enthusiasts, and TAs will be formally listed as part of the course staff on
the course website. My course
<https://www.edx.org/course/v2/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x>
is an introduction to big data analysis using Spark, and Ameet‚Äôs course
<https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1xis about using Spark for Distributed Machine Learning applications. We are
also developing three more courses: an introductory Spark course, an
advanced Machine Learning course, and an advanced Spark course. The courses
will be taught in Python.

Last year‚Äôs offering of my course enrolled over 76,000 students with a 29%
engagement rate and 11.2% completion rate, and the course was rated by
students on Class Central
<https://www.class-central.com/report/best-free-online-courses-2015/> in
the top-ten of all MOOCs offered last year. Last year‚Äôs offering of Ameet‚Äôs
course enrolled over 56,000 students with a 29% engagement rate and 14.5%
completion rate.

We're looking for volunteer Teaching Assistants (TAs) with at least two of
the following skills: basic Python Spark programming and debugging
experience, basic ML knowledge, and basic operations skills (writing and
using scripts, helping with username/password issues, etc.).

We are looking for a time-commitment of roughly 10-20 hours per week
beginning in March and ending November.  The team will be distributed and
we‚Äôre flexible about start and end dates and specific working hours. We can
offer a stipend.

Please contact us (Anthony adj@berkeley.edu and cc Ameet ameet@cs.ucla.edu
and Brian Clapper bmc@databricks.com) if you are interested in working with
us.

Thanks,

Anthony, Ameet, and Brian
"
Sean Owen <sowen@cloudera.com>,"Sat, 23 Apr 2016 10:08:52 +0100","Re: Proposal of closing some PRs which at least one of committers
 suggested so","Hyukjin Kwon <gurwls223@gmail.com>, Patrick Wendell <patrick@databricks.com>","Except for the last one I think they're closeable. We can't close any
PR directly. It's possible to push an empty commit with comments like
""Closes #xxxx"" to make the ASF processes close them.

I'd swear we have a script for this but it's not in the Spark project,
and it used to be run regularly. It would generate messages like
""Automated closing of PRs"" and was triggered by text in the PRs like
""do you mind closing this PR"". I can't find where it is though. That
would be ideal to run again.

@pwendell am I crazy .. did you not make this script and even email
about it? I've lost it somehow.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 23 Apr 2016 11:05:35 -0700","Re: Proposal of closing some PRs which at least one of committers
 suggested so",Sean Owen <sowen@cloudera.com>,"I pushed a commit to close all but the last one.



"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Sun, 24 Apr 2016 13:50:57 +0000",Spark 2.0 vs 1.6.1 Query Time using Thrift server,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi,

I run a query using JDBC driver running it on version 1.6.1 it return after 5 - 6 min , the same query against version 2.0 fails after 2h (due to timeout)
Both versions run on the same hardware/configuration and data source

Is anyone else experiencing this?


Also I noticed that the thrift server on version had a change in commands.scala in spark-sql-2.11/execution/command


[cid:image001.png@01D19E48.9F15B950]

The external tool Microstrategy can see tables only with the reinserted code compiling spark version 2.0, it looks like a bug


Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 24 Apr 2016 07:10:25 -0700",Re: Spark 2.0 vs 1.6.1 Query Time using Thrift server,Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Spark 2.0 changes very fast.

The ShowTablesCommand is now in:
sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala

Consider opening a PR for the change shown below.

For the query run, can you share some of your code ?
Please pastebin related portion from Thrift server log.

Cheers


2h (due to
"
Caique Marques <caiquermarques95@gmail.com>,"Sun, 24 Apr 2016 13:39:43 -0300",net.razorvine.pickle.PickleException in Pyspark,dev@spark.apache.org,"Hello, everyone!

I'm trying to implement the association rules in Python. I got implement an
association by a frequent element, works as expected (example can be seen
here
<https://github.com/mrcaique/spark/blob/master/examples/src/main/python/mllib/fpgrowth_example.py#L36-L40>).


Now, my challenge is to implement by a custom RDD. I study the structure of
Spark and how it implement Python functions of machine learning algorithms.
The implementations can be seen in the fork
<https://github.com/mrcaique/spark>.

The example for a custom RDD for association rule can be seen here
<https://github.com/mrcaique/spark/blob/master/examples/src/main/python/mllib/association_rules_example.py>,
in the line 33 the output is:

MapPartitionsRDD[10] at mapPartitions at PythonMLLibAPI.scala:1533

It is ok. Testing the Scala example, the structure returned is a
MapPartitions. But, when I try use a *foreach* in this collection:

net.razorvine.pickle.PickleException: expected zero arguments for
construction of ClassDict (for numpy.core.multiarray._reconstruct)
    at
net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
    at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
    at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
    at
org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$2.apply(PythonMLLibAPI.scala:1547)
    at
org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$2.apply(PythonMLLibAPI.scala:1546)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396)
    at
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
    at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:77)
    at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:45)
    at org.apache.spark.scheduler.Task.run(Task.scala:81)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

What is this? What does mean? Any help or tip is welcome.

Thanks,
Caique.
"
Mike Hynes <91mbbh@gmail.com>,"Sun, 24 Apr 2016 19:17:27 -0400",Re: executor delay in Spark,Raghava Mutharaju <m.vijayaraghava@gmail.com>,"Could you change numPartitions to {16, 32, 64} and run your program for
each to see how many partitions are allocated to each worker? Let's see if
you experience an all-nothing imbalance that way; if so, my guess is that
something else is odd in your program logic or spark runtime environment,
but if not and your executors all receive at least *some* partitions, then
I still wouldn't rule out effects of scheduling delay. It's a simple test,
but it could give some insight.

Mike

his could still be a  scheduling  If only one has *all* partitions,  and
email me the log file? (If it's 10+ MB, just the first few thousand lines
are fine).

"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 25 Apr 2016 09:05:07 +0800",Re: executor delay in Spark,Mike Hynes <91mbbh@gmail.com>,"Maybe this is due to config spark.scheduler.minRegisteredResourcesRatio,
you can try set it as 1 to see the behavior.


// Submit tasks only after (registered resources / total expected resources)

// is equal to at least this value, that is double between 0 and 1.
var minRegisteredRatio =
  math.min(1, conf.getDouble(""spark.scheduler.minRegisteredResourcesRatio"", 0))





-- 
Best Regards

Jeff Zhang
"
"""FangFang Chen"" <lulynn_2015_spark@163.com>","Mon, 25 Apr 2016 12:49:48 +0800 (GMT+08:00)","Spark sql with large sql syntax job failed with outofmemory error
 and grows beyond 64k warn","""user@spark.apache.org"" <user@spark.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
With large sql command, job failed with following error. Please give your suggestion on how to resolve it. Thanks


Sql file size: 676k
Log:
16/04/25 10:55:00 WARN TaskSetManager: Lost task 84.0 in stage 0.0 (TID 6, BJHC-HADOOP-HERA-17493.jd.local): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.janino.JaninoRuntimeException: Code of method ""(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificUnsafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;)V"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB


public Object generate(org.apache.spark.sql.catalyst.expressions.Expression[] exprs) {
  return new SpecificUnsafeProjection(exprs);
}


class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {




......


java.lang.OutOfMemoryError: Java heap space
  at com.google.protobuf.ByteString.copyFrom(ByteString.java:192)
  at com.google.protobuf.CodedInputStream.readBytes(CodedInputStream.java:324)
  at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:6657)
  at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:6607)
  at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:6703)
  at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:6698)
  at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:141)
  at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:176)
  at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:188)
  at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:193)
  at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)
  at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:6821)
  at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:168)


  


∑¢◊‘ Õ¯“◊” œ‰¥Û ¶"
"""Praveen Devarao"" <praveendrl@in.ibm.com>","Mon, 25 Apr 2016 11:24:19 +0530",Do transformation functions on RDD invoke a Job [sc.runJob]?,"dev@spark.apache.org, user@spark.apache.org","Hi,

        I have a streaming program with the block as below [ref: 
https://github.com/agsachin/streamingBenchmark/blob/master/spark-benchmarks/src/main/scala/TwitterStreaming.scala
]

1 val lines = messages.map(_._2)
2 val hashTags = lines.flatMap(status => status.split("" ""
).filter(_.startsWith(""#"")))

3 val topCounts60 = hashTags.map((_, 1)).reduceByKey( _ + _ )
3a  .map { case (topic, count) => (count, topic) }
3b  .transform(_.sortByKey(false))

4a topCounts60.foreachRDD( rdd => {
4b  val topList = rdd.take( 10 )
})

        This batch is triggering 2 jobs...one at line 3b (sortByKey)  and 
the other at 4b (rdd.take) I agree that there is a Job triggered on line 
4b as take() is an action on RDD while as on line 3b sortByKey is just a 
transformation function which as per docs is lazy evaluation...but I see 
that this line uses a RangePartitioner and Rangepartitioner on 
initialization invokes a method called sketch() that invokes collect() 
triggering a Job.

        My question: Is it expected that sortByKey will invoke a Job...if 
yes, why is sortByKey listed as a transformation and not action. Are there 
any other functions like this that invoke a Job, though they are 
transformations and not actions?

        I am on Spark 1.6

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""

"
Reynold Xin <rxin@databricks.com>,"Sun, 24 Apr 2016 22:55:45 -0700",Re: Do transformation functions on RDD invoke a Job [sc.runJob]?,Praveen Devarao <praveendrl@in.ibm.com>,"Usually no - but sortByKey does because it needs the range boundary to be
built in order to have the RDD. It is a long standing problem that's
unfortunately very difficult to solve without breaking the RDD API.

In DataFrame/Dataset we don't have this issue though.



"
"""Praveen Devarao"" <praveendrl@in.ibm.com>","Mon, 25 Apr 2016 11:34:42 +0530",Re: Do transformation functions on RDD invoke a Job [sc.runJob]?,Reynold Xin <rxin@databricks.com>,"Thanks Reynold for the reason as to why sortBykey invokes a Job

When you say ""DataFrame/Dataset does not have this issue"" is it right to 
assume you are referring to Spark 2.0 or Spark 1.6 DF already has built-in 
it?

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""



From:   Reynold Xin <rxin@databricks.com>
To:     Praveen Devarao/India/IBM@IBMIN
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>, user 
<user@spark.apache.org>
Date:   25/04/2016 11:26 am
Subject:        Re: Do transformation functions on RDD invoke a Job 
[sc.runJob]?



Usually no - but sortByKey does because it needs the range boundary to be 
built in order to have the RDD. It is a long standing problem that's 
unfortunately very difficult to solve without breaking the RDD API.

In DataFrame/Dataset we don't have this issue though.


Hi,

        I have a streaming program with the block as below [ref: 
https://github.com/agsachin/streamingBenchmark/blob/master/spark-benchmarks/src/main/scala/TwitterStreaming.scala
]

1 val lines = messages.map(_._2)
2 val hashTags = lines.flatMap(status => status.split("" ""
).filter(_.startsWith(""#"")))

3 val topCounts60 = hashTags.map((_, 1)).reduceByKey( _ + _ )
3a .map { case (topic, count) => (count, topic) }
3b .transform(_.sortByKey(false))

4atopCounts60.foreachRDD( rdd => {
4b val topList = rdd.take( 10 )
})

        This batch is triggering 2 jobs...one at line 3b(sortByKey)  and 
the other at 4b (rdd.take) I agree that there is a Job triggered on line 
4b as take() is an action on RDD while as on line 3b sortByKey is just a 
transformation function which as per docs is lazy evaluation...but I see 
that this line uses a RangePartitioner and Rangepartitioner on 
initialization invokes a method called sketch() that invokes collect() 
triggering a Job.

        My question: Is it expected that sortByKey will invoke a Job...if 
yes, why is sortByKey listed as a transformation and not action. Are there 
any other functions like this that invoke a Job, though they are 
transformations and not actions?

        I am on Spark 1.6

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""




"
Cody Koeninger <cody@koeninger.org>,"Mon, 25 Apr 2016 09:23:32 -0500",Re: Spark streaming Kafka receiver WriteAheadLog question,Renyi Xiong <renyixiong0@gmail.com>,"If you want to refer back to Kafka based on offset ranges, why not use
createDirectStream?


---------------------------------------------------------------------


"
Patrick Woody <patrick.woody1@gmail.com>,"Mon, 25 Apr 2016 12:59:29 -0400",Re: Question about storage memory in unified memory manager,dev@spark.apache.org,"Hey all,

Just wondering if anyone has had issues with this or if it is expected that
the semantic around the memory management is different here.

Thanks
-Pat


"
Michael Armbrust <michael@databricks.com>,"Mon, 25 Apr 2016 13:29:16 -0400",Re: Do transformation functions on RDD invoke a Job [sc.runJob]?,Praveen Devarao <praveendrl@in.ibm.com>,"Spark SQL's query planner has always delayed building the RDD, so has never
needed to eagerly calculate the range boundaries (since Spark 1.0).


"
Ali Tootoonchian <ali@levyx.com>,"Mon, 25 Apr 2016 12:50:27 -0700 (MST)",Cache Shuffle Based Operation Before Sort,dev@spark.apache.org,"Caching shuffle RDD before the sort process improves system performance. SQL
planner can be intelligent to cache join, aggregate or sort data frame
before executing next sort process.

For any sort process two job is created by spark, first one is responsible
for producing range boundary for shuffle partition and second one complete
sort process by creating a new shuffle RDD.

When an input of sort process is output of other shuffle process then reduce
part of shuffle RDD is re-evaluated and the intermediate  shuffle data is
read twice. If input shuffle RDD (exchange based data frame) is saved, sort
process can be completed faster. Remember that Spark saves RDD in parquet
format which usually compressed and its size is smaller than original data.

Let‚Äôs look at an example,
The following query is modified version of q3 of TPCH test bench.
tpchQuery = 
       """"""        
        |select *
        |from
        | customer,
        | orders,
        | lineitem
        |where
        | c_mktsegment = 'MACHINERY'
        | and c_custkey = o_custkey
        | and l_orderkey = o_orderkey
        | and o_orderdate < '1995-03-15'
        | and l_shipdate > '1995-03-15'
        |order by
        | o_orderdate
       """""".stripMargin

The query can be executed in one step using current Spark SQL planner. The
other approach for execute this query is two steps. 
    Compute and cache output of join process
    Execute order by command
Following command show how second approach can be implemented

tpchQuery =
      """"""
        |select *
        |from
        | customer,
        | orders,
        | lineitem
        |where
        | c_mktsegment = 'MACHINERY'
        | and c_custkey = o_custkey
        | and l_orderkey = o_orderkey
        | and o_orderdate < '1995-03-15'
        | and l_shipdate > '1995-03-15'
      """""".stripMargin
val joinDf = sqlContext.sql(tpchQuery).cache
val queryRes = joinDf.sort(""o_orderdate"")

Let‚Äôs look at details of execution for 10 and 100 scale factor input


By comparing stage 4, 9, 10 and 15, 20, 21 of two approaches, you can find
out that amount of data is read during sort process can be reduced by factor
2.



--
3.nabble.com/Cache-Shuffle-Based-Operation-Before-Sort-tp17331.html
om.

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 25 Apr 2016 13:54:04 -0700",Re: Cache Shuffle Based Operation Before Sort,Ali Tootoonchian <ali@levyx.com>,"Interesting.

bq. details of execution for 10 and 100 scale factor input

Looks like some chart (or image) didn't go through.

FYI


e
e
rt
a.
e
put
d
ased-Operation-Before-Sort-tp17331.html
"
Joseph Bradley <joseph@databricks.com>,"Mon, 25 Apr 2016 15:59:33 -0700",Re: net.razorvine.pickle.PickleException in Pyspark,Caique Marques <caiquermarques95@gmail.com>,"Thanks for your work on this.  Can we continue discussing on the JIRA?


"
shane knapp <sknapp@berkeley.edu>,"Mon, 25 Apr 2016 16:50:56 -0700","[build system] short downtime wednesday morning (4-27-16), 7-9am","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","another project hosted on our jenkins (e-mission) needs anaconda scipy
upgraded from 0.15.1 to 0.17.0.  this will also upgrade a few other
libs, which i've included at the end of this email.

i've spoken w/josh @ databricks and we don't believe that this will
impact the spark builds at all.  if this causes serious breakage, i
will roll everything back to pre-update.

i have created a JIRA issue to look in to creating conda environments
for spark builds, something that we should have done long ago:

https://issues.apache.org/jira/browse/SPARK-14905

builds will be paused:  ~7am PDT
anaconda package updates:  ~8am
jenkins quiet time ends:  ~9am at the latest

i do not expect the downtime to last very long, and will update this
thread w/updates as they come.

here's what will be updated under anaconda:

The following NEW packages will be INSTALLED:

    libgfortran: 3.0-0
    mkl:         11.3.1-0
    wheel:       0.29.0-py27_0

The following packages will be UPDATED:

    conda:       3.10.1-py27_0     --> 4.0.5-py27_0
    conda-env:   2.1.4-py27_0      --> 2.4.5-py27_0
    numpy:       1.9.2-py27_0      --> 1.11.0-py27_0
    openssl:     1.0.1k-1          --> 1.0.2g-0
    pip:         6.1.1-py27_0      --> 8.1.1-py27_1
    python:      2.7.9-2           --> 2.7.11-0
    pyyaml:      3.11-py27_0       --> 3.11-py27_1
    requests:    2.6.0-py27_0      --> 2.9.1-py27_0
    scipy:       0.15.1-np19py27_0 --> 0.17.0-np111py27_2
    setuptools:  15.0-py27_0       --> 20.7.0-py27_0
    sqlite:      3.8.4.1-1         --> 3.9.2-0
    yaml:        0.1.4-0           --> 0.1.6-0

---------------------------------------------------------------------


"
"""Praveen Devarao"" <praveendrl@in.ibm.com>","Tue, 26 Apr 2016 07:39:50 +0530",Re: Do transformation functions on RDD invoke a Job [sc.runJob]?,Michael Armbrust <michael@databricks.com>,"Cool!! Thanks for the clarification Mike.

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""



From:   Michael Armbrust <michael@databricks.com>
To:     Praveen Devarao/India/IBM@IBMIN
Cc:     Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" 
<dev@spark.apache.org>, user <user@spark.apache.org>
Date:   25/04/2016 10:59 pm
Subject:        Re: Do transformation functions on RDD invoke a Job 
[sc.runJob]?



Spark SQL's query planner has always delayed building the RDD, so has 
never needed to eagerly calculate the range boundaries (since Spark 1.0).

Thanks Reynold for the reason as to why sortBykey invokes a Job

When you say ""DataFrame/Dataset does not have this issue"" is it right to 
assume you are referring to Spark 2.0 or Spark 1.6 DF already has built-in 
it?

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""



From:        Reynold Xin <rxin@databricks.com>
To:        Praveen Devarao/India/IBM@IBMIN
Cc:        ""dev@spark.apache.org"" <dev@spark.apache.org>, user <
user@spark.apache.org>
Date:        25/04/2016 11:26 am
Subject:        Re: Do transformation functions on RDD invoke a Job 
[sc.runJob]?




Usually no - but sortByKey does because it needs the range boundary to be 
built in order to have the RDD. It is a long standing problem that's 
unfortunately very difficult to solve without breaking the RDD API.

In DataFrame/Dataset we don't have this issue though.


Hi,

        I have a streaming program with the block as below [ref: 
https://github.com/agsachin/streamingBenchmark/blob/master/spark-benchmarks/src/main/scala/TwitterStreaming.scala
]

1 val lines = messages.map(_._2)
2 val hashTags = lines.flatMap(status => status.split("" ""
).filter(_.startsWith(""#"")))

3 val topCounts60 = hashTags.map((_, 1)).reduceByKey( _ + _ )
3a .map { case (topic, count) => (count, topic) }
3b .transform(_.sortByKey(false))

4atopCounts60.foreachRDD( rdd => {
4b val topList = rdd.take( 10 )
})

        This batch is triggering 2 jobs...one at line 3b(sortByKey) and 
the other at 4b (rdd.take) I agree that there is a Job triggered on line 
4b as take() is an action on RDD while as on line 3b sortByKey is just a 
transformation function which as per docs is lazy evaluation...but I see 
that this line uses a RangePartitioner and Rangepartitioner on 
initialization invokes a method called sketch() that invokes collect() 
triggering a Job.

        My question: Is it expected that sortByKey will invoke a Job...if 
yes, why is sortByKey listed as a transformation and not action. Are there 
any other functions like this that invoke a Job, though they are 
transformations and not actions?

        I am on Spark 1.6

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""







"
Jacek Laskowski <jacek@japila.pl>,"Tue, 26 Apr 2016 06:32:11 +0200",Re: spark git commit: [HOTFIX] Fix compilation,rxin@apache.org,"Thanks Reynold! I was going to ask about that one as it breaks the build for me.

[info] Compiling 1 Scala source to
/Users/jacek/dev/oss/spark/sql/hivecontext-compatibility/target/scala-2.11/classes...
[error] /Users/jacek/dev/oss/spark/sql/hivecontext-compatibility/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala:32:
overriding value sparkSession in class SQLContext of type
org.apache.spark.sql.SparkSession;
[error]  value sparkSession has weaker access privileges; it should
not be private
[error]     @transient private val sparkSession: SparkSession,
[error]                            ^
[error] one error found
[error] Compile failed at Apr 26, 2016 6:28:01 AM [0.449s]


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 26 Apr 2016 06:58:37 +0200",Re: spark git commit: [HOTFIX] Fix the problem for real this time.,dev <dev@spark.apache.org>,"[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10:53 min
[INFO] Finished at: 2016-04-26T06:56:49+02:00
[INFO] Final Memory: 107M/890M
[INFO] ------------------------------------------------------------------------

Thanks Reynold! :)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
"""Mario Ds Briggs"" <mario.briggs@in.ibm.com>","Tue, 26 Apr 2016 16:39:32 +0530",Re: Spark streaming Kafka receiver WriteAheadLog question,"cody@koeninger.org, renyixiong0@gmail.com","That was my initial thought as well. But then i was wondering if this
approach could help remove
 a - the little extra latency overhead we have with the DirectApproach
(compared to Receiver) and
 b - the data duplication in-efficiency (replication to WAL) and single
version of the truth of the offsets processed (under some failures) in the
Receiver approach.

thanks
Mario

----- Message from Cody Koeninger <cody@koeninger.org> on Mon, 25 Apr 2016
09:23:32 -0500 -----
                                                         
      To: Renyi Xiong <renyixiong0@gmail.com>            
                                                         
      cc: dev <dev@spark.apache.org>                     
                                                         
 Subject: Re: Spark streaming Kafka receiver             
          WriteAheadLog question                         
                                                         

If you want to refer back to Kafka based on offset ranges, why not use
createDirectStream?

to
understand?




"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 26 Apr 2016 18:10:38 +0000",Number of partitions for binaryFiles,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I have 100 binary files in local file system that I want to load into Spark RDD. I need the data from each file to be in a separate partition. However, I cannot make it happen:

scala> sc.binaryFiles(""/data/subset"").partitions.size
res5: Int = 66

The ""minPartitions"" parameter does not seems to help:
scala> sc.binaryFiles(""/data/subset"", minPartitions = 100).partitions.size
res8: Int = 66

At the same time, Spark produces the required number of partitions with sc.textFiles (though I cannot use it because my files are binary):
scala> sc.textFile(""/data/subset"").partitions.size
res9: Int = 100

Could you suggest how to force Spark to load binary files each in a separate partition?

Best regards, Alexander
"
Michael Gummelt <mgummelt@mesosphere.io>,"Tue, 26 Apr 2016 11:20:15 -0700",HDFS as Shuffle Service,dev@spark.apache.org,"Has there been any thought or work on this (or any other networked file
system)?  It would be valuable to support dynamic allocation without
depending on the shuffle service.

-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 26 Apr 2016 13:22:25 -0700",Re: Number of partitions for binaryFiles,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Here is the body of StreamFileInputFormat#setMinPartitions :

  def setMinPartitions(context: JobContext, minPartitions: Int) {
    val totalLen =
listStatus(context).asScala.filterNot(_.isDirectory).map(_.getLen).sum
    val maxSplitSize = math.ceil(totalLen / math.max(minPartitions,
1.0)).toLong
    super.setMaxSplitSize(maxSplitSize)

I guess what happened was that among the 100 files you had, there were ~60
files whose sizes were much bigger than the rest.
According to the way max split size is computed above, you ended up with
fewer partitions.

I just performed a test using local directory where 3 files were
significantly larger than the rest and reproduced what you observed.

Cheers


ize
"
Timothy Chen <tnachen@gmail.com>,"Tue, 26 Apr 2016 13:31:12 -0700",Re: HDFS as Shuffle Service,Michael Gummelt <mgummelt@mesosphere.io>,"Are you suggesting to have shuffle service persist and fetch data with hdfs, or skip shuffle service altogether and just write to hdfs?

Tim


te:
stem)?  It would be valuable to support dynamic allocation without depending on the shuffle service.

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 26 Apr 2016 21:27:06 +0000",RE: Number of partitions for binaryFiles,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

I have 36 files of size ~600KB and the rest 74 are about 400KB.

Is there a workaround rather than changing Sparks code?

Best regards, Alexander

From: Ted Yu [mailto:yuzhihong@gmail.com]
Sent: Tuesday, April 26, 2016 1:22 PM
To: Ulanov, Alexander <alexander.ulanov@hpe.com>
Cc: dev@spark.apache.org
Subject: Re: Number of partitions for binaryFiles

Here is the body of StreamFileInputFormat#setMinPartitions :

  def setMinPartitions(context: JobContext, minPartitions: Int) {
    val totalLen = listStatus(context).asScala.filterNot(_.isDirectory).map(_.getLen).sum
    val maxSplitSize = math.ceil(totalLen / math.max(minPartitions, 1.0)).toLong
    super.setMaxSplitSize(maxSplitSize)

I guess what happened was that among the 100 files you had, there were ~60 files whose sizes were much bigger than the rest.
According to the way max split size is computed above, you ended up with fewer partitions.

I just performed a test using local directory where 3 files were significantly larger than the rest and reproduced what you observed.

Cheers

On Tue, Apr 26, 2016 at 11:10 AM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I have 100 binary files in local file system that I want to load into Spark RDD. I need the data from each file to be in a separate partition. However, I cannot make it happen:

scala> sc.binaryFiles(""/data/subset"").partitions.size
res5: Int = 66

The ‚ÄúminPartitions‚Äù parameter does not seems to help:
scala> sc.binaryFiles(""/data/subset"", minPartitions = 100).partitions.size
res8: Int = 66

At the same time, Spark produces the required number of partitions with sc.textFiles (though I cannot use it because my files are binary):
scala> sc.textFile(""/data/subset"").partitions.size
res9: Int = 100

Could you suggest how to force Spark to load binary files each in a separate partition?

Best regards, Alexander

"
Ted Yu <yuzhihong@gmail.com>,"Tue, 26 Apr 2016 14:43:36 -0700",Re: Number of partitions for binaryFiles,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","end up with very small partitions.

In your case, look at the size of the cluster.
If 66 partitions can make good use of your cluster, it should be fine.

m

0
ize
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 26 Apr 2016 21:55:24 +0000",RE: Number of partitions for binaryFiles,Ted Yu <yuzhihong@gmail.com>,"The issue is that the data was specifically prepared in such a way that each file is a single partition computationally and logically. It seems strange that one cannot override the default behavior. It might be too expensive to perform another round of re-partitioning within Spark because it will involve shuffling.


From: Ted Yu [mailto PM
To: Ulanov, Alexander <alexander.ulanov@hpe.com>
Cc: dev@spark.apache.org
Subject: Re: Number of partitions for binaryFiles

From what I understand, Spark code was written this way because you don't end up with very small partitions.

In your case, look at the size of the cluster.
If 66 partitions can make good use of your cluster, it should be fine.

On Tue, Apr 26, 2016 at 2:27 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Hi Ted,

I have 36 files of size ~600KB and the rest 74 are about 400KB.

Is there a workaround rather than changing Sparks code?

Best regards, Alexander

From: Ted Yu [mailto:yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
Sent: Tuesday, April 26, 2016 1:22 PM
To: Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>>
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Number of partitions for binaryFiles

Here is the body of StreamFileInputFormat#setMinPartitions :

  def setMinPartitions(context: JobContext, minPartitions: Int) {
    val totalLen = listStatus(context).asScala.filterNot(_.isDirectory).map(_.getLen).sum
    val maxSplitSize = math.ceil(totalLen / math.max(minPartitions, 1.0)).toLong
    super.setMaxSplitSize(maxSplitSize)

I guess what happened was that among the 100 files you had, there were ~60 files whose sizes were much bigger than the rest.
According to the way max split size is computed above, you ended up with fewer partitions.

I just performed a test using local directory where 3 files were significantly larger than the rest and reproduced what you observed.

Cheers

On Tue, Apr 26, 2016 at 11:10 AM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I have 100 binary files in local file system that I want to load into Spark RDD. I need the data from each file to be in a separate partition. However, I cannot make it happen:

scala> sc.binaryFiles(""/data/subset"").partitions.size
res5: Int = 66

The ‚ÄúminPartitions‚Äù parameter does not seems to help:
scala> sc.binaryFiles(""/data/subset"", minPartitions = 100).partitions.size
res8: Int = 66

At the same time, Spark produces the required number of partitions with sc.textFiles (though I cannot use it because my files are binary):
scala> sc.textFile(""/data/subset"").partitions.size
res9: Int = 100

Could you suggest how to force Spark to load binary files each in a separate partition?

Best regards, Alexander


"
Saisai Shao <sai.sai.shao@gmail.com>,"Wed, 27 Apr 2016 11:27:41 +0800",Re: HDFS as Shuffle Service,Timothy Chen <tnachen@gmail.com>,"Quite curious about the benefits of using HDFS as shuffle service, also
what's the problem of using current shuffle service?


Thanks
Saisai


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Wed, 27 Apr 2016 12:59:11 +0900",Re: HDFS as Shuffle Service,Timothy Chen <tnachen@gmail.com>,"Hi, all

See SPARK-1529 for related discussion.

// maropu




-- 
---
Takeshi Yamamuro
"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Wed, 27 Apr 2016 13:46:53 +0200",Decrease shuffle in TreeAggregate with coalesce ?,dev@spark.apache.org,"Hi,

I've been looking at the code of RDD.treeAggregate, because we've seen a 
huge performance drop between 1.5.2 and 1.6.1 on a treeReduce. I think 
the treeAggregate code hasn't changed, so my message is not about the 
performance drop, but a more general remark about treeAggregate.

In treeAggregate, after the aggregate is applied inside original 
partitions, we enter the tree :


	while (numPartitions > scale + math.ceil(numPartitions.toDouble / 
scale)) {

	numPartitions /= scale

	val curNumPartitions = numPartitions

	*partiallyAggregated **=**partiallyAggregated.mapPartitionsWithIndex {*

	*(i, iter) **=>**iter.map((i **%**curNumPartitions, _))*

	}.reduceByKey(new HashPartitioner(curNumPartitions), cleanCombOp).values

	}


The two lines where the partitions are numbered then renumbered, then 
reducedByKey seems below optimality to me. There is a huge shuffle cost, 
while a simple coalesce followed by a partition-level aggregation would 
probably perfectly do the job.

Have I missed something that requires to do this reshuffle ?

Best regards
Guillaume Pitel
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Wed, 27 Apr 2016 09:29:19 -0300",Duplicated fit into TrainValidationSplit,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi guys, I was testing a pipeline here, and found a possible duplicated
call to fit method into the
org.apache.spark.ml.tuning.TrainValidationSplit
<https://github.com/apache/spark/blob/18c2c92580bdc27aa5129d9e7abda418a3633ea6/mllib/src/main/scala/org/apache/spark/ml/tuning/TrainValidationSplit.scala>
class
In line 110 there is a call to est.fit method that call fit in all
parameter combinations that we have setup.
Down in the line 128, after discovering which is the bestmodel, we call fit
aggain using the bestIndex, wouldn't be better to just access the result of
the already call fit method stored in the models val?

Kind regards,
Dirceu
"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 27 Apr 2016 14:37:03 +0000",Re: Duplicated fit into TrainValidationSplit,"Dirceu Semighini Filho <dirceu.semighini@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","You should find that the first set of fits are called on the training set,
and the resulting models evaluated on the validation set. The final best
model is then retrained on the entire dataset. This is standard practice -
usually the dataset passed to the train validation split is itself further
split into a training and test set, where the final best model is evaluated
against the test set.

"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Wed, 27 Apr 2016 12:01:38 -0300",Re: Duplicated fit into TrainValidationSplit,"""dev@spark.apache.org"" <dev@spark.apache.org>","Ok, thank you.

2016-04-27 11:37 GMT-03:00 Nick Pentreath <nick.pentreath@gmail.com>:

"
shane knapp <sknapp@berkeley.edu>,"Wed, 27 Apr 2016 08:50:06 -0700","Re: [build system] short downtime wednesday morning (4-27-16), 7-9am","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","this will be postponed due to the 2.0 code freeze.  sorry for the late notice.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 27 Apr 2016 10:19:37 -0700","Re: [build system] short downtime wednesday morning (4-27-16), 7-9am","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","we're going to go ahead and do this on monday.  i'll send out another
email later this week w/the details.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Wed, 27 Apr 2016 17:24:42 +0000",Re: HDFS as Shuffle Service,Takeshi Yamamuro <linguin.m.s@gmail.com>,"
> On 27 Apr 2016, at 04:59, Takeshi Yamamuro <linguin.m.s@gmail.com> wrote:
> 
> Hi, all
> 
> See SPARK-1529 for related discussion.
> 
> // maropu


I'd not seen that discussion.

I'm actually curious about why the 15% diff in performance between Java NIO and Hadoop FS APIs, and, if it is the case (Hadoop still uses the pre-NIO libraries, *has anyone thought of just fixing Hadoop Local FS codepath?*

It's not like anyone hasn't filed JIRAs on that ... it's just that nothing has ever got to a state where it was considered ready to adopt, where ""ready"" means: passes all unit and load tests against Linux, Unix, Windows filesystems. There's been some attempts, but they never quite got much engagement or support, especially as nio wasn't there properly until Java 7, ‚Äîand Hadoop was stuck on java 6 support until 2015. That's no longer a constraint: someone could do the work, using the existing JIRAs as starting points.


If someone did do this in RawLocalFS, it'd be nice if the patch also allowed you to turn off CRC creation and checking. 

That's not only part of the overhead, it means that flush() doesn't, not until you reach the end of a CRC32 block ... so breaking what few durability guarantees POSIX offers.



"
Joseph Bradley <joseph@databricks.com>,"Wed, 27 Apr 2016 10:41:48 -0700",Re: Decrease shuffle in TreeAggregate with coalesce ?,Guillaume Pitel <guillaume.pitel@exensa.com>,"Do you have code which can reproduce this performance drop in treeReduce?
It would be helpful to debug.  In the 1.6 release, we profiled it via the
various MLlib algorithms and did not see performance drops.

It's not just renumbering the partitions; it is reducing the number of
partitions by a factor of 1.0/scale (where scale > 1).  This creates a
""tree""-structured aggregation so that more of the work of merging during
aggregation is done on the workers, not the driver.


"
Michael Slavitch <slavitch@gmail.com>,"Wed, 27 Apr 2016 14:34:16 -0400",Error running  spark-sql-perf  version 0.3.2 against Spark 1.6,dev@spark.apache.org,"Hello;

I'm trying to run spark-sql-perf  version 0.3.2  (hash cb0347b) against Spark 1.6,  I get the following when running

 ./bin/run --benchmark DatsetPerformance 

Exception in thread ""main"" java.lang.ClassNotFoundException: com.databricks.spark.sql.perf.DatsetPerformance

Even though the classes are built.  How do I resolve this?


Another question:  Why does the current version of the benchmark run only against spark 2.0?




---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Wed, 27 Apr 2016 17:34:14 -0700",Re: HDFS as Shuffle Service,Steve Loughran <stevel@hortonworks.com>,"hdfs, or skip shuffle service altogether and just write to hdfs?

Skip shuffle service altogether.  Write to HDFS.

Mesos environments tend to be multi-tenant, and running the shuffle service
on all nodes could be extremely wasteful.  If you're running a 10K node
cluster, and you'd like to run a Spark job that consumes 100 nodes, you
would have to run the shuffle service on all 10K nodes out of band of Spark
(e.g. marathon).  I'd like a solution for dynamic allocation that doesn't
require this overhead.

I'll look at SPARK-1529.


g
s
longer a
ng


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Sean Owen <sowen@cloudera.com>,"Thu, 28 Apr 2016 09:36:46 +0100",Re: HDFS as Shuffle Service,Michael Gummelt <mgummelt@mesosphere.io>,"Why would you run the shuffle service on 10K nodes but Spark executors
on just 100 nodes? wouldn't you also run that service just on the 100
nodes?

What does plumbing it through HDFS buy you in comparison? There's some
additional overhead and if anything you lose some control over
locality, in a context where I presume HDFS itself is storing data on
much more than the 100 Spark nodes.

rote:
ce
rk
ng
ws
a 7,
nger a
ing
lity

---------------------------------------------------------------------


"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Thu, 28 Apr 2016 11:23:22 +0200",Re: Decrease shuffle in TreeAggregate with coalesce ?,Joseph Bradley <joseph@databricks.com>,"Le 27/04/2016 √† 19:41, Joseph Bradley a √©crit :
That would be difficult, but if we cannot find out, we'll design a small 
example to test that. I first have to check with latest git version. I 
have to recompile spark with lgpl version of netlib.

Sure,I get that, and it wasn't my point. I just think coalesce also 
reduces the number of partitions, without shuffle, right ?

_With Coalesce :_
Let's say we have 2 workers with 2 partitions each.

W0: p0,p1
W1: p1,p2

Since coalesce tries to reduce shuffling, coalesce(2) should group 
contents of p0 and p1 in p0' (on W0) and p2 and p3 in p1' (-on W1)

OTOH, _with current mapPartitionWithIndex + modulo + reduceByKey_, let's 
say partitions are numbered like that :

(0,p0),(1,p1),(2,p2),(3,p3)

Then after the modulo, (0,p0),(1,p1),(0,p2),(1,p3)

As a consequence, W1 will shuffle p2 to W0 and W0 will shuffle p1 to W1.

Guillaume



-- 
eXenSa

	
*Guillaume PITEL, Pr√©sident*
+33(0)626 222 431

eXenSa S.A.S. <http://www.exensa.com/>
41, rue P√©rier - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705

"
<Ioannis.Deligiannis@nomura.com>,"Thu, 28 Apr 2016 09:33:03 +0000",RDD.broadcast,<dev@spark.apache.org>,"Hi,

It is a common pattern to process an RDD, collect (typically a subset) to the driver and then broadcast back.

Adding an RDD method that can do that using the torrent broadcast mechanics would be much more efficient. In addition, it would not require the Driver to also utilize its Heap holding this broadcast.

I guess this can become complicated if the resulting broadcast is required to keep lineage information, but assuming a torrent distribution, once the broadcast is synced then lineage would not be required. I‚Äôd also expect the call to rdd.brodcast to be an action that eagerly distributes the broadcast and returns when the operation has succeeded.

Is this something that could be implemented or are there any reasons that prohibits this?

Thanks
Ioannis


This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm

"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Thu, 28 Apr 2016 12:53:29 +0200",Re: Decrease shuffle in TreeAggregate with coalesce ?,dev@spark.apache.org,"Long story short, regarding the performance issue, it appeared with 
recompiled version of the source TGZ downloaded from spark website.

Problem disappears with 1.6.2-SNAPSHOT (branch-1.6)

Guillaume

"
Marcin Tustin <mtustin@handybook.com>,"Thu, 28 Apr 2016 07:08:11 -0400",Re: RDD.broadcast,"""Ioannis.Deligiannis@nomura.com"" <Ioannis.Deligiannis@nomura.com>","Why would you ever need to do this? I'm genuinely curious. I view collects
as being solely for interactive work.


d
e
expect the
st
ed
ty
a
s
l:

-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
<Ioannis.Deligiannis@nomura.com>,"Thu, 28 Apr 2016 11:20:18 +0000",RE: RDD.broadcast,<mtustin@handybook.com>,"One example pattern we have it doing joins or filters based on two datasets. E.g.

1         Filter ‚Äìmultiple- RddB for a given set extracted from RddA (keyword here is multiple times)

a.       RddA -> keyBy -> distinct -> collect() to Set A;

b.      RddB -> Filter using Set A;

2         ‚ÄúJoin‚Äù using composition on executor (again multiple times)

a.       RddA -> filter by XYZ -> keyBy join attribute -> collectAsMap ->Broadcast MapA;

b.      RddB -> map (Broadcast<Map<K,V>> MapA;


The first use case might not be that common, but joining a large RDD with a small (reference) RDD is quite common and much faster than using ‚Äújoin‚Äù method.


From: Marcin Tustin [mailto:mtustin@handybook.com]
Sent: 28 April 2016 12:08
To: Deligiannis, Ioannis (UK)
Cc: dev@spark.apache.org
Subject: Re: RDD.broadcast

Why would you ever need to do this? I'm genuinely curious. I view collects as being solely for interactive work.

On Thursday, April 28, 2016, <Ioannis.Deligiannis@nomura.com<mailto:Ioannis.Deligiannis@nomura.com>> wrote:
Hi,

It is a common pattern to process an RDD, collect (typically a subset) to the driver and then broadcast back.

Adding an RDD method that can do that using the torrent broadcast mechanics would be much more efficient. In addition, it would not require the Driver to also utilize its Heap holding this broadcast.

I guess this can become complicated if the resulting broadcast is required to keep lineage information, but assuming a torrent distribution, once the broadcast is synced then lineage would not be required. I‚Äôd also expect the call to rdd.brodcast to be an action that eagerly distributes the broadcast and returns when the operation has succeeded.

Is this something that could be implemented or are there any reasons that prohibits this?

Thanks
Ioannis

This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.Nomura.com_email-5Fdisclaimer.htm&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=SLnOgTBJ2zAlhtvjcFRXfqUArds-4HSAZCgFXLgMCVY&e=>

Want to work at Handy? Check out our culture deck and open roles<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.handy.com_careers&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=WgDnCrSGv_qt66f2cabjugmMGU46gc5rSkt_gm7lEkQ&e=>
Latest news<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.handy.com_press&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=rfQxr8cDwVFK7Mql1_HdnvqAmXeiOHZgnjNtKXGn_Kg&e=> at Handy
Handy just raised $50m<https://urldefense.proofpoint.com/v2/url?u=http-3A__venturebeat.com_2015_11_02_on-2Ddemand-2Dhome-2Dservice-2Dhandy-2Draises-2D50m-2Din-2Dround-2Dled-2Dby-2Dfidelity_&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=RbQTDcalISb9w2WMxzXmRgR1mr7QiCaqpD2bLAkt-z4&e=> led by Fidelity

[Image removed by sender.]


This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm

"
Mike Hynes <91mbbh@gmail.com>,"Thu, 28 Apr 2016 07:24:24 -0400",Re: RDD.broadcast,Marcin Tustin <mtustin@handybook.com>,"I second knowing the use case for interest. I can imagine a case where
knowledge of the RDD key distribution would help local computations, for
relaticely few keys, but would be interested to hear your motive.

Essentially, are you trying to achieve what would be an all-reduce type
operation in MPI?

o
e
n,
ôd also
t
med
t
ity
 a
y
l
is
d
il:
m-in-round-led-by-fidelity/> led
"
Marcin Tustin <mtustin@handybook.com>,"Thu, 28 Apr 2016 07:26:51 -0400",Re: RDD.broadcast,"""Ioannis.Deligiannis@nomura.com"" <Ioannis.Deligiannis@nomura.com>","I don't know what your notation really means. I'm very much unclear on why
you can't use the filter method for 1. If you're talking about
splitting/bucketing rather filtering as such I think that is a specific
lacuna in spark's Api.

I've generally found the join api to be entirely adequate for my needs, so
I don't really have a comment on 2.


ddA
tiple times)
újoin‚Äù
s
d
e
expect the
st
ed
ty
a
s
l:
il-5Fdisclaimer.htm&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=SLnOgTBJ2zAlhtvjcFRXfqUArds-4HSAZCgFXLgMCVY&e=>
ers&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=WgDnCrSGv_qt66f2cabjugmMGU46gc5rSkt_gm7lEkQ&e=>
s&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=rfQxr8cDwVFK7Mql1_HdnvqAmXeiOHZgnjNtKXGn_Kg&e=> at
15_11_02_on-2Ddemand-2Dhome-2Dservice-2Dhandy-2Draises-2D50m-2Din-2Dround-2Dled-2Dby-2Dfidelity_&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=RbQTDcalISb9w2WMxzXmRgR1mr7QiCaqpD2bLAkt-z4&e=> led
ed
ty
a
s
l:

-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
<Ioannis.Deligiannis@nomura.com>,"Thu, 28 Apr 2016 12:51:55 +0000",RE: RDD.broadcast,<mtustin@handybook.com>,"I was aiming to show the operations with pseudo-code, but I apparently failed, so Java it is ‚ò∫
Assume the following 3 datasets on HDFS.

1.       RDD1: User (1 Million rows ‚Äì 2GB ) Columns: uid, locationId, (extra stuff)

2.       RDD2: Actions (1 Billion rows ‚Äì 500GB) Columns: uid_1, uid_2 (extra stuff)

3.       RDD3: Locations (10 Millions rows ‚Äì 10GB) Columns: locationId, (extra stuff)

So, what I am doing is shown below.
I assume that it is clear of why using a ‚Äújoin‚Äù would be really inefficient, but if not have a look of some of the reasons here<http://stackoverflow.com/questions/30412325/hoes-does-spark-schedule-a-join/30490619#30490619> .

//1 Filter actions
 > JavaRDD actionRdd = RDD2.filter(f->‚Ä¶.); //300GB

//2 Filter users & Broadcast
> JavaRDD userRdd =  RDD1.filter(f->‚Ä¶); //1GB
> Map m = userRdd.keyBy(f->g.getUid()).collectAsMap();
> Broadcast<Map> userMap = sparkCtx.broadcast(m);//1GB

//3 Filter locations & Broadcast
> JavaPairRDD locations = RDD3.filter(f->‚Ä¶.).keyBy(f->g.getLocationId());
> Broadcast<Map> locationMap = sparkCtx.broadcast(locations.collectAsMap());//1GB

//4 Process using data from all 3 data sets
> actionRdd.map(f-> {
User u1 = userMap.value.get( f.getUid1());
User u2 = userMap.value.get( f.getUid2());
Location l= locationMap.value.get(u.getLocationId();
Object result = method(f,u1,u2,l);//method implementation not important, but requires all 3 objects
return result;
          });


From: Marcin Tustin [mailto:mtustin@handybook.com]
Sent: 28 April 2016 12:27
To: Deligiannis, Ioannis (UK)
Cc: dev@spark.apache.org
Subject: Re: RDD.broadcast

I don't know what your notation really means. I'm very much unclear on why you can't use the filter method for 1. If you're talking about splitting/bucketing rather filtering as such I think that is a specific lacuna in spark's Api.

I've generally found the join api to be entirely adequate for my needs, so I don't really have a comment on 2.

On Thursday, April 28, 2016, <Ioannis.Deligiannis@nomura.com<mailto:Ioannis.Deligiannis@nomura.com>> wrote:
One example pattern we have it doing joins or filters based on two datasets. E.g.

1         Filter ‚Äìmultiple- RddB for a given set extracted from RddA (keyword here is multiple times)

a.       RddA -> keyBy -> distinct -> collect() to Set A;

b.      RddB -> Filter using Set A;

2         ‚ÄúJoin‚Äù using composition on executor (again multiple times)

a.       RddA -> filter by XYZ -> keyBy join attribute -> collectAsMap ->Broadcast MapA;

b.      RddB -> map (Broadcast<Map<K,V>> MapA;


The first use case might not be that common, but joining a large RDD with a small (reference) RDD is quite common and much faster than using ‚Äújoin‚Äù method.


From: Marcin Tustin [mailto:mtustin@handybook.com<javascript:_e(%7B%7D,'cvml','mtustin@handybook.com');>]
Sent: 28 April 2016 12:08
To: Deligiannis, Ioannis (UK)
Cc: dev@spark.apache.org<javascript:_e(%7B%7D,'cvml','dev@spark.apache.org');>
Subject: Re: RDD.broadcast

Why would you ever need to do this? I'm genuinely curious. I view collects as being solely for interactive work.

On Thursday, April 28, 2016, <Ioannis.Deligiannis@nomura.com<javascript:_e(%7B%7D,'cvml','Ioannis.Deligiannis@nomura.com');>> wrote:
Hi,

It is a common pattern to process an RDD, collect (typically a subset) to the driver and then broadcast back.

Adding an RDD method that can do that using the torrent broadcast mechanics would be much more efficient. In addition, it would not require the Driver to also utilize its Heap holding this broadcast.

I guess this can become complicated if the resulting broadcast is required to keep lineage information, but assuming a torrent distribution, once the broadcast is synced then lineage would not be required. I‚Äôd also expect the call to rdd.brodcast to be an action that eagerly distributes the broadcast and returns when the operation has succeeded.

Is this something that could be implemented or are there any reasons that prohibits this?

Thanks
Ioannis

This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.Nomura.com_email-5Fdisclaimer.htm&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=SLnOgTBJ2zAlhtvjcFRXfqUArds-4HSAZCgFXLgMCVY&e=>

Want to work at Handy? Check out our culture deck and open roles<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.handy.com_careers&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=WgDnCrSGv_qt66f2cabjugmMGU46gc5rSkt_gm7lEkQ&e=>
Latest news<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.handy.com_press&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=rfQxr8cDwVFK7Mql1_HdnvqAmXeiOHZgnjNtKXGn_Kg&e=> at Handy
Handy just raised $50m<https://urldefense.proofpoint.com/v2/url?u=http-3A__venturebeat.com_2015_11_02_on-2Ddemand-2Dhome-2Dservice-2Dhandy-2Draises-2D50m-2Din-2Dround-2Dled-2Dby-2Dfidelity_&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=RbQTDcalISb9w2WMxzXmRgR1mr7QiCaqpD2bLAkt-z4&e=> led by Fidelity

[Image removed by sender.]

This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.Nomura.com_email-5Fdisclaimer.htm&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=zRDuW7ua96biT1hpVLmf9v2kyB0mV0SzsqtLLTbsRl0&e=>

Want to work at Handy? Check out our culture deck and open roles<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.handy.com_careers&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=x2weVTVWoKPax6nWFBzezd9rEyFBLCr1tgL0NYQ8CkY&e=>
Latest news<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.handy.com_press&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=EX4TthvT0szzJSv8UOAPWUBq0vQMseqWuXColEdW9jw&e=> at Handy
Handy just raised $50m<https://urldefense.proofpoint.com/v2/url?u=http-3A__venturebeat.com_2015_11_02_on-2Ddemand-2Dhome-2Dservice-2Dhandy-2Draises-2D50m-2Din-2Dround-2Dled-2Dby-2Dfidelity_&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=vIRLq79Lr1dBdabX-d9eAi1uJsbpRVx50slHOT11RoQ&e=> led by Fidelity

[http://marketing-email-assets.handybook.com/smalllogo.png]


This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm

"
William Benton <willb@redhat.com>,"Thu, 28 Apr 2016 09:08:36 -0500",certification suite?,dev@spark.apache.org,"Hi all,

Does anyone happen to know what tests Databricks uses for the Spark
distribution certification suite?  Is it simply the tests that run as CI on
Spark pull requests, or is there something more involved?

The web site (
https://databricks.com/spark/certification/certified-spark-distribution)
says that ""the certification process is fully transparent with open-source
tests (developed and maintained by the community and used to test each
release of Apache Spark), lightweight, and 100% free.""  But that's all the
information I can find about the process, so I was hoping someone could
elucidate it for me.

It seems like community-based downstream distributions (e.g., BigTop,
Fedora, etc) could benefit from running a similar test suite to the
official Spark distribution certification suite in their regular CI, even
if the results were (necessarily) not official.


best,
wb
"
Renyi Xiong <renyixiong0@gmail.com>,"Thu, 28 Apr 2016 09:17:09 -0700",Spark streaming concurrent job scheduling question,dev <dev@spark.apache.org>,"Hi,

I am trying to run an I/O intensive RDD in parallel with CPU intensive RDD
within an application through a window like below:

var ssc = new StreamingContext(sc, 1min);
var ds1 = ...
var ds2 = ds1.Window(2min).ForeachRDD(...)
ds1.ForeachRDD(...)

I hope ds1 to start its job at 1min interval even if ds2's job not complete
yet.

but it is not the case when I run it - ds1's job won't start until ds2's
job completes.

I looked into document which mentions jobs within same SparkContext need to
be submitted in different thread in order to run in parallel.

is it true? then question becomes how should I submit the above 2 jobs in
different threads?

(I know there're concurrentJobs and receiver mode but both with some
particular issues)

Thanks a lot,
Renyi.
"
_na <nikhila.albert@seeq.com>,"Thu, 28 Apr 2016 09:34:17 -0700 (MST)",Using Spark when data definitions are unknowable at compile time,dev@spark.apache.org,"We are looking to incorporate Spark into a timeseries data investigation
application, but we are having a hard time transforming our workflow into
the required transformations-on-data model. The crux of the problem is that
we don‚Äôt know a priori which data will be required for our transformations.
 
For example, a common request might be `average($series2.within($ranges))`,
where in order to fetch the right sections of data from $series2, $ranges
will need to be computed first and then used to define data boundaries.
 
Is there a way to get around the need to define data first in Spark?



--
3.nabble.com/Using-Spark-when-data-definitions-are-unknowable-at-compile-time-tp17371.html
om.

---------------------------------------------------------------------


"
Dean Wampler <deanwampler@gmail.com>,"Thu, 28 Apr 2016 11:53:29 -0500",Re: Using Spark when data definitions are unknowable at compile time,_na <nikhila.albert@seeq.com>,"I would start with using DataFrames and the Row
<http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row>
API, because you can fetch fields by index. Presumably, you'll parse the
incoming data and determine what fields have what types, etc. Or, will
someone specify the schema dynamically some how?

Either way, once you know the types and indices of the fields you need for
a given query, you can fetch them using the Row methods.

HTH,

dean

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Lightbend <http://lightbend.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


at
ormations.
`,
n-data-definitions-are-unknowable-at-compile-time-tp17371.html
"
Michael Gummelt <mgummelt@mesosphere.io>,"Thu, 28 Apr 2016 10:46:37 -0700",Re: HDFS as Shuffle Service,Sean Owen <sowen@cloudera.com>,"on just 100 nodes? wouldn't you also run that service just on the 100
nodes?

We have to start the service beforehand, out of band, and we don't know a
priori where the Spark executors will land.  Those 100 executors could land
on any of the 10K nodes.


It drops the shuffle service requirement, which is HUGE.  It means Spark
can completely vacate the machine when it's not in use, which is crucial
for a large, multi-tenant cluster.  ShuffledRDDs can now read the map files
from HDFS, rather than the ancestor executors, which means we can shut
executors down immediately after the shuffle files are written.

over locality, in a context where I presume HDFS itself is storing data on
much more than the 100 Spark nodes.

Write locality would be sacrificed, but the descendent executors were
already doing a remote read (they have to read from multiple ancestor
executors), so there's no additional cost in read locality.  In fact, if we
take advantage of HDFS's favored node feature, we could make it likely that
all map files for a given partition land on the same node, so the
descendent executor would never have to do a remote read!  We'd effectively
shift the remote IO from read side to write side, for theoretically no
change in performance.

In summary:

Advantages:
- No shuffle service dependency (increased utilization, decreased
management cost)
- Shut executors down immediately after shuffle files are written, rather
than waiting for a timeout (increased utilization)
- HDFS is HA, so shuffle files survive a node failure, which isn't true for
the shuffle service (decreased latency during failures)
- Potential ability to parallelize shuffle file reads if we write a new
shuffle iterator (decreased latency)

Disadvantages
- Increased write latency (but potentially not if we implement it
efficiently.  See above).
- Would need some sort of GC on HDFS shuffle files






't
m
a
longer a
ot



-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 28 Apr 2016 11:08:47 -0700",Re: HDFS as Shuffle Service,Michael Gummelt <mgummelt@mesosphere.io>,"Yes, replicated and distributed shuffle materializations are key
requirement to maintain performance in a fully elastic cluster where
Executors aren't just reallocated across an essentially fixed number of
Worker nodes, but rather the number of Workers itself is dynamic.
Retaining the file interface to those shuffle materializations while also
using HDFS for the spark.local.dirs has a certain amount of attraction, but
I also wonder whether a typical HDFS deployment is really sufficient to
handle this kind of elastic cluster scaling.  For instance and assuming
HDFS co-located on worker nodes, if after a work-load burst your cluster
dynamically changes from 10000 workers to 1000, will the typical HDFS
replication factor be sufficient to retain access to the shuffle files in
HDFS, or will we instead be seeing numerous FetchFailure exceptions, Tasks
recomputed or Stages aborted, etc. so that the net effect is not all that
much different than if the shuffle files had not been relocated to HDFS and
the Executors or ShuffleService instances had just disappeared along with
the worker nodes?


nd
es
n
we
at
ly
h
e
u
va
h
 longer a
"
Michael Gummelt <mgummelt@mesosphere.io>,"Thu, 28 Apr 2016 11:15:57 -0700",Re: HDFS as Shuffle Service,Mark Hamstra <mark@clearstorydata.com>," > if after a work-load burst your cluster dynamically changes from 10000
workers to 1000, will the typical HDFS replication factor be sufficient to
retain access to the shuffle files in HDFS

HDFS isn't resizing.  Spark is.  HDFS files should be HA and durable.


ut
s
nd
a
and
les
on
 we
hat
ely
r
o>
th
de
ou
e
S
e
ch
o longer a


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Thu, 28 Apr 2016 11:17:18 -0700",Re: HDFS as Shuffle Service,Michael Gummelt <mgummelt@mesosphere.io>,"Hm while this is an attractive idea in theory, in practice I think you are
substantially overestimating HDFS' ability to handle a lot of small,
ephemeral files. It has never really been optimized for that use case.


o
o
but
n
ks
t
and
h
o
s
d
k
l
iles
l
 on
f we
that
vely
f
he
FS
re
l
no longer
o
,
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 28 Apr 2016 11:19:36 -0700",Re: HDFS as Shuffle Service,Michael Gummelt <mgummelt@mesosphere.io>,"So you are only considering the case where your set of HDFS nodes is
disjoint from your dynamic set of Spark Worker nodes?  That would seem to
be a pretty significant sacrifice of data locality.


o
o
but
n
ks
t
and
h
o
s
d
k
l
iles
l
 on
f we
that
vely
f
he
FS
re
l
no longer
o
,
"
Michael Gummelt <mgummelt@mesosphere.io>,"Thu, 28 Apr 2016 11:20:46 -0700",Re: HDFS as Shuffle Service,Reynold Xin <rxin@databricks.com>,"Yea, it's an open question.  I'm willing to create some benchmarks, but I'd
first like to know that the feature would be accepted assuming the results
are reasonable.  Can a committer give me a thumbs up?


e
to
so
 but
r
in
sks
at
 and
th
rs
w
ld
the
e can
ring
if we
 that
ively
e
w
s
e
m
 no
so


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Michael Gummelt <mgummelt@mesosphere.io>,"Thu, 28 Apr 2016 11:23:34 -0700",Re: HDFS as Shuffle Service,Mark Hamstra <mark@clearstorydata.com>,"Not disjoint.  Colocated.  By ""shrinking"", I don't mean any nodes are going
away.  I mean executors are decreasing in number, which is the case with
dynamic allocation.  HDFS nodes aren't decreasing in number though, and we
can still colocate on those nodes, as always.


to
so
 but
r
in
sks
at
 and
th
rs
w
ld
the
e can
ring
if we
 that
ively
e
w
s
e
m
 no
so


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Thu, 28 Apr 2016 11:33:44 -0700",Re: RDD.broadcast,Ioannis.Deligiannis@nomura.com,"This is a nice feature in broadcast join. It is just a little bit
complicated to do and as a result hasn't been prioritized as highly yet.



nId,
id_2
ionId,
really
oin/30490619#30490619>
tionId());
,
y
o
ddA
tiple times)
újoin‚Äù
s
d
e
expect the
st
ed
ty
a
s
l:
il-5Fdisclaimer.htm&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=SLnOgTBJ2zAlhtvjcFRXfqUArds-4HSAZCgFXLgMCVY&e=>
ers&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=WgDnCrSGv_qt66f2cabjugmMGU46gc5rSkt_gm7lEkQ&e=>
s&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=rfQxr8cDwVFK7Mql1_HdnvqAmXeiOHZgnjNtKXGn_Kg&e=> at
15_11_02_on-2Ddemand-2Dhome-2Dservice-2Dhandy-2Draises-2D50m-2Din-2Dround-2Dled-2Dby-2Dfidelity_&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=GAA5LZhuKEWXxozKzXPhWAYY4BSTpcXaf2lFg5JSPB0&s=RbQTDcalISb9w2WMxzXmRgR1mr7QiCaqpD2bLAkt-z4&e=> led
ed
ty
a
s
l:
il-5Fdisclaimer.htm&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=zRDuW7ua96biT1hpVLmf9v2kyB0mV0SzsqtLLTbsRl0&e=>
ers&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=x2weVTVWoKPax6nWFBzezd9rEyFBLCr1tgL0NYQ8CkY&e=>
s&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=EX4TthvT0szzJSv8UOAPWUBq0vQMseqWuXColEdW9jw&e=> at
15_11_02_on-2Ddemand-2Dhome-2Dservice-2Dhandy-2Draises-2D50m-2Din-2Dround-2Dled-2Dby-2Dfidelity_&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=3sSvB-R66BWR0E56L12fHFtbHcZLeqp2hdnrMncx1-U&s=vIRLq79Lr1dBdabX-d9eAi1uJsbpRVx50slHOT11RoQ&e=> led
ed
ty
a
s
l:
"
Koert Kuipers <koert@tresata.com>,"Thu, 28 Apr 2016 14:35:06 -0400",spark 2 segfault,"""dev@spark.apache.org"" <dev@spark.apache.org>","i tried for the first time to run our own in-house unit tests on spark 2,
and i get the error below.
has anyone seen this?

it is reproducible. i tried latest java 7 and it is still there.

# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7c3a4b1f54, pid=21939, tid=140171011417856
#
# JRE version: Java(TM) SE Runtime Environment (7.0_75-b13) (build
1.7.0_75-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.75-b04 mixed mode
linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x747f54]  _Copy_arrayof_conjoint_jlongs+0x44

more info:

Stack: [0x00007f7c1b47e000,0x00007f7c1b57f000],  sp=0x00007f7c1b57a9a8,
free space=1010k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native
code)
V  [libjvm.so+0x747f54]  _Copy_arrayof_conjoint_jlongs+0x44
j  sun.misc.Unsafe.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V+0
j
org.apache.spark.unsafe.Platform.copyMemory(Ljava/lang/Object;JLjava/lang/Object;JJ)V+34
j  org.apache.spark.unsafe.types.UTF8String.getBytes()[B+76
j  org.apache.spark.unsafe.types.UTF8String.toString()Ljava/lang/String;+5
j
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Ljava/lang/Object;)Ljava/lang/Object;+876
j
org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.fromRow(Lorg/apache/spark/sql/catalyst/InternalRow;)Ljava/lang/Object;+5
j
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1$$anonfun$apply$13.apply(Lorg/apache/spark/sql/catalyst/InternalRow;)Ljava/lang/Object;+11
j
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1$$anonfun$apply$13.apply(Ljava/lang/Object;)Ljava/lang/Object;+5
J 13277 C2
scala.collection.mutable.ArrayOps$ofRef.map(Lscala/Function1;Lscala/collection/generic/CanBuildFrom;)Ljava/lang/Object;
(7 bytes) @ 0x00007f7c25eeae08 [0x00007f7c25eead40+0xc8]
j
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply()Ljava/lang/Object;+43
j
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(Lorg/apache/spark/sql/SparkSession;Lorg/apache/spark/sql/execution/QueryExecution;Lscala/Function0;)Ljava/lang/Object;+106
j
org.apache.spark.sql.Dataset.withNewExecutionId(Lscala/Function0;)Ljava/lang/Object;+12
j  org.apache.spark.sql.Dataset.org
$apache$spark$sql$Dataset$$execute$1()Ljava/lang/Object;+9
j
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Lorg/apache/spark/sql/Dataset;)Ljava/lang/Object;+4
j
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Ljava/lang/Object;)Ljava/lang/Object;+5
j
org.apache.spark.sql.Dataset.withCallback(Ljava/lang/String;Lorg/apache/spark/sql/Dataset;Lscala/Function1;)Ljava/lang/Object;+25
j  org.apache.spark.sql.Dataset.org
$apache$spark$sql$Dataset$$collect(Z)Ljava/lang/Object;+20
j  org.apache.spark.sql.Dataset.collect()Ljava/lang/Object;+2
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 28 Apr 2016 11:36:07 -0700",Re: HDFS as Shuffle Service,Michael Gummelt <mgummelt@mesosphere.io>,"Ah, got it.  While that would be useful, it doesn't address the more
general (and potentially even more beneficial) case where the total number
of worker nodes is fully elastic.  That already starts to push you into the
direction of spitting Spark worker and HDFS data nodes into disjoint sets,
and to compensate for the loss of data locality you start wishing for some
kind of hierarchical storage where at least your hot data can be present on
the Spark workers.  Even without an elastic number of HDFS nodes, you might
well get into a similar kind of desire for hierarchical storage another
layer providing faster access to the shuffle files than is possible using
HDFS -- because I share Reynold's scepticism that HDFS by itself will be up
to demands of handling the shuffle files.  With such a hierarchical split
or Spark-node-local caching layer, considering the more general split
between data and fully elastic worker nodes becomes much more tractable.


o
o
0
 to
f
lso
, but
o
g
er
 in
asks
hat
S and
ith
rs
s
 the
we can
oring
 if we
y that
tively
o
:
rs
0
me
n
e
,
n
l
t
,
s no
s
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 28 Apr 2016 11:41:29 -0700",Re: spark 2 segfault,Koert Kuipers <koert@tresata.com>,"Are you able to pastebin a unit test which can reproduce the following ?

Thanks

nd i get the error below.
17856
5-b13)
md64 compressed oops)
 free space=1010k
native code)
ng/Object;JJ)V+34

ojection.apply(Ljava/lang/Object;)Ljava/lang/Object;+876
pache/spark/sql/catalyst/InternalRow;)Ljava/lang/Object;+5
cute$1$1$$anonfun$apply$13.apply(Lorg/apache/spark/sql/catalyst/InternalRow;)Ljava/lang/Object;+11
cute$1$1$$anonfun$apply$13.apply(Ljava/lang/Object;)Ljava/lang/Object;+5
cala/collection/generic/CanBuildFrom;)Ljava/lang/Object; (7 bytes) @ 0x00007f7c25eeae08 [0x00007f7c25eead40+0xc8]
cute$1$1.apply()Ljava/lang/Object;+43
ache/spark/sql/SparkSession;Lorg/apache/spark/sql/execution/QueryExecution;Lscala/Function0;)Ljava/lang/Object;+106
/lang/Object;+12
java/lang/Object;+9
lect$1.apply(Lorg/apache/spark/sql/Dataset;)Ljava/lang/Object;+4
lect$1.apply(Ljava/lang/Object;)Ljava/lang/Object;+5
/spark/sql/Dataset;Lscala/Function1;)Ljava/lang/Object;+25
ava/lang/Object;+20
"
Daniel Siegmann <daniel.siegmann@teamaol.com>,"Thu, 28 Apr 2016 15:06:42 -0400",Re: Spark ML - Scaling logistic regression for many features,Nick Pentreath <nick.pentreath@gmail.com>,"FYI: https://issues.apache.org/jira/browse/SPARK-14464

I have submitted a PR as well.


"
Andrew Ray <ray.andrew@gmail.com>,"Thu, 28 Apr 2016 12:41:39 -0700",Re: HDFS as Shuffle Service,Reynold Xin <rxin@databricks.com>,"Yes, HDFS has serious problems with creating lots of files. But we can
always just create a single merged file on HDFS per task.

Hm while this is an attractive idea in theory, in practice I think you are
substantially overestimating HDFS' ability to handle a lot of small,
ephemeral files. It has never really been optimized for that use case.


o
o
but
n
ks
t
and
h
o
s
d
k
l
iles
l
 on
f we
that
vely
f
he
FS
re
l
no longer
o
,
"
"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Thu, 28 Apr 2016 12:56:26 -0700 (MST)",Re: Tungsten off heap memory access for C++ libraries,dev@spark.apache.org,"Hi,

I'm coming from the particle physics community and I'm also very interested
in the development of this project. We have a huge C++ codebase and would
like to start using the higher-level abstractions of Spark in our data
analyses. To this end, I've been developing code that copies data from our
C++ framework, ROOT, into Scala:

https://github.com/diana-hep/rootconverter/tree/master/scaroot-reader
<https://github.com/diana-hep/rootconverter/tree/master/scaroot-reader>  

(Worth noting: the ROOT file format is too complex for a complete rewrite in
Java or Scala to be feasible. ROOT readers in Java and even Javascript
exist, but they only handle simple cases.)

I have a variety of options for how to lay out the bytes during this
transfer, and in all cases fill the constructor arguments of Scala classes
using macros. When I learned that you're moving the Spark data off-heap (at
the same time as I'm struggling to move it on-heap), I realized that you
must have chosen a serialization format for that data, and I should be using
/that/ serialization format.

Even though it's early, do you have any designs for that serialization
format? Have you picked a standard one? Most of the options, such as Avro,
don't make a lot of sense because they pack integers to minimize number of
bytes, rather than lay them out for efficient access (including any
byte-alignment considerations).

Also, are there any plans for an API that /fills/ an RDD or DataSet from the
C++ side, as I'm trying to do?

Thanks,
-- Jim


P.S. Concerning Java/C++ bindings, there are many. I tried JNI, JNA, BridJ,
and JavaCPP personally, but in the end picked JNA because of its
(comparatively) large user base. If Spark will be using Djinni, that could
be a symmetry-breaking consideration and I'll start using it for
consistency, maybe even interoperability.




--

---------------------------------------------------------------------


"
"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Thu, 28 Apr 2016 13:13:53 -0700 (MST)",Re: Tungsten off heap memory access for C++ libraries,dev@spark.apache.org,"jpivarski@gmail.com wrote

I think I misunderstood what Djinni is. JNA, BridJ, and JavaCPP provide
access to untyped bytes (except for common cases like java.lang.String), but
it looks like Djinni goes further and provides a type mapping--- exactly the
""serialization format"" or ""layout of bytes"" that I was asking about.

Is it safe to say that when Spark has off-heap caching, that it will be in
the format specified by Djinni? If I work to integrate ROOT with Djinni,
will this be a major step toward integrating it with Spark 2.0?

Even if the above answers my first question, I'd still like to know if the
new Spark API will allow RDDs to be /filled/ from the C++ side, as a data
source, rather than a derived dataset.




--

---------------------------------------------------------------------


"
Gayathri Murali <gayathri.m.softie@gmail.com>,"Thu, 28 Apr 2016 13:19:42 -0700",SparkR unit test failures on local master,dev@spark.apache.org,"Hi All,

I am running the sparkR unit test(./R/run-tests.sh) on a local master
branch and I am seeing the following issues with SparkR ML wrapper test
cases.

Failed
-------------------------------------------------------------------------
1. Error: glm and predict (@test_mllib.R#31)
-----------------------------------

1: glm(Sepal_Width ~ Sepal_Length, training, family = ""gaussian"") at
/Users/gayathri/spark/R/lib/SparkR/tests/testthat/test_mllib.R:31

Error: summary coefficients match with native glm (@test_mllib.R#79)
--------
error in evaluating the argument 'object' in selecting a method for
function 'summary':

Error: summary coefficients match with native glm of family 'binomial'
(@test_mllib.R#97)
error in evaluating the argument 'object' in selecting a method for
function 'summary':

10. Failure: SQL error message is returned from JVM (@test_sparkSQL.R#1820)
----
grepl(""Table not found: blah"", retError) not equal to TRUE.
1 element mismatch

Any thoughts on what could be causing this?

Thanks
Gayathri
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 28 Apr 2016 13:39:59 -0700",Re: SparkR unit test failures on local master,Gayathri Murali <gayathri.m.softie@gmail.com>,"I just ran the tests using a recently synced master branch and the
tests seemed to work fine. My guess is some of the Java classes
changed and you need to rebuild Spark ?

Thanks
Shivaram


---------------------------------------------------------------------


"
Hamel Kothari <hamelkothari@gmail.com>,"Thu, 28 Apr 2016 20:57:22 +0000",ConvertToSafe being done before functions.explode,dev <dev@spark.apache.org>,"Hi all,

I've been looking at some of my query plans and noticed that pretty much
every explode that I run (which is always over a column with ArrayData) is
prefixed with a ConvertToSafe call in the physical plan. Looking at
Generate.scala it looks like it doesn't override canProcessUnsafeRows in
SparkPlan which defaults to false. For more clarity, I'm using
functions.explode (which uses builtin Explode from generators.scala), not
DataFrame.explode (which requires a user function to be passed in).

Is this behavior correct? I suspect that unless we're using a
UserDefinedGenerator this isn't the right. Even in the case of
UserDefinedGenerator it seems the UserDefinedGenerator expression code
performs a manual convertToSafe. If my understanding is correct we should
be able to set ""canProcessUnsafeRows"" to be true in all cases. Can someone
who understands this part of the SQL code spot check me on this?

Thanks,
Hamel
"
Gayathri Murali <gayathri.m.softie@gmail.com>,"Thu, 28 Apr 2016 15:06:15 -0700",Re: SparkR unit test failures on local master,shivaram@eecs.berkeley.edu,"I just rebuild spark and tried to run the tests again. Same failure.


"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 28 Apr 2016 15:42:37 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Reynold Xin <rxin@apache.org>,"Just want to provide a quick update that we have submitted the ""Spark
Extras"" proposal for review by the Apache board (see link below with the
contents).

https://docs.google.com/document/d/1zRFGG4414LhbKlGbYncZ13nyX34Rw4sfWhZRA5YBtIE/edit?usp=sharing

Note that we are in the quest for a project name that does not have ""Spark""
as part of it, and we will provide an update here when we find a suitable
name. Suggestions are welcome (please send them directly to my inbox to
avoid flooding the mailing list).

Thanks



.
ng
y
ns
ot
ed
C
f
g
)
o



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
JaeSung Jun <jaesjun@gmail.com>,"Fri, 29 Apr 2016 14:38:24 +1000",Unit test error,dev@spark.apache.org,"Hi All,

I'm developing custom data source & relation provider based on spark 1.6.1.
Every unit test has its own Spark Context, and it runs successfully when
running one by one.
But when running in sbt(sbt:test), error pops up when initializing spark
contest like followings :

org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint:
spark://HeartbeatReceiver@192.168.123.101:54079

at
org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$asyncSetupEndpointRefByURI$1.apply(NettyRpcEnv.scala:148)

at
org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$asyncSetupEndpointRefByURI$1.apply(NettyRpcEnv.scala:144)

at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)

at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249)

at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)

at
org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)

at
scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)

at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)

at
scala.concurrent.impl.Promise$DefaultPromise.scala$concurrent$impl$Promise$DefaultPromise$$dispatchOrAddCallback(Promise.scala:280)

at
scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:270)

at scala.concurrent.Future$class.flatMap(Future.scala:249)

at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:153)

at
org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)

at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:97)

at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:106)

at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)

at org.apache.spark.executor.Executor.<init>(Executor.scala:115)

at
org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalBackend.scala:58)

at
org.apache.spark.scheduler.local.LocalBackend.start(LocalBackend.scala:125)

at
org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)

at org.apache.spark.SparkContext.<init>(SparkContext.scala:530)


Anyone any idea?


Thanks Jason
"
Rob Turner <robairrobair@gmail.com>,"Fri, 29 Apr 2016 13:36:56 +0100",unsubscribe,dev@spark.apache.org,"*spark*-unsubscribe-user=robair@bigfoot.com
"
Arun Allamsetty <arun.allamsetty@gmail.com>,"Fri, 29 Apr 2016 13:00:37 -0600",Requesting feedback for PR for SPARK-11962,dev@spark.apache.org,"Hi,

I have submitted a PR for SPARK-11962 (
https://github.com/apache/spark/pull/12708). I have most of it ready except
I am not able to pin point the cause for a particular bug.

In the PR I've added two major methods to Row, `attempt` and `getOption`.
The former returns a `Try` while the latter returns an `Option`.

   - `attempt` was added after a comment was made in PR #10247
   <https://github.com/apache/spark/pull/10247>, where it was suggested I
   not return `None` when certain exceptions are thrown. But in my opinion,
   throwing exceptions from a function which returns an `Option` is not a
   good use case.
   - I am not in love with the method name, `attempt`. Would welcome
   suggestions. I wanted to use `try` but it's a keyword.
   - So about the failing tests (which shouldn't fail in my opinion), in
   `RowTest`, all new tests testing for `ClassCastException` fail. I have
   modified tests as comments in the code blocks (with TODOs) which work, but
   should behave the same way as the actual test from what I can see. I am
   not sure what I am doing wrong in the code here.

Would appreciate some feedback.

Thanks,

Arun

P. S. I have sent similar emails to the dev mailing list before but I don't
see them in the dev archives. I am guessing they were not received as I was
not subscribed to the dev list. Hopefully this would work as I have
subscribed now.
"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Apr 2016 12:52:44 -0700","[build system] short downtime monday morning (5-2-16), 7-9am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","(copy-pasta of previous message)

another project hosted on our jenkins (e-mission) needs anaconda scipy
upgraded from 0.15.1 to 0.17.0.  this will also upgrade a few other
libs, which i've included at the end of this email.

i've spoken w/josh @ databricks and we don't believe that this will
impact the spark builds at all.  if this causes serious breakage, i
will roll everything back to pre-update.

i have created a JIRA issue to look in to creating conda environments
for spark builds, something that we should have done long ago:

https://issues.apache.org/jira/browse/SPARK-14905

builds will be paused:  ~7am PDT
anaconda package updates:  ~8am
jenkins quiet time ends:  ~9am at the latest

i do not expect the downtime to last very long, and will update this
thread w/updates as they come.

here's what will be updated under anaconda:

The following NEW packages will be INSTALLED:

    libgfortran: 3.0-0
    mkl:         11.3.1-0
    wheel:       0.29.0-py27_0

The following packages will be UPDATED:

    conda:       3.10.1-py27_0     --> 4.0.5-py27_0
    conda-env:   2.1.4-py27_0      --> 2.4.5-py27_0
    numpy:       1.9.2-py27_0      --> 1.11.0-py27_0
    openssl:     1.0.1k-1          --> 1.0.2g-0
    pip:         6.1.1-py27_0      --> 8.1.1-py27_1
    python:      2.7.9-2           --> 2.7.11-0
    pyyaml:      3.11-py27_0       --> 3.11-py27_1
    requests:    2.6.0-py27_0      --> 2.9.1-py27_0
    scipy:       0.15.1-np19py27_0 --> 0.17.0-np111py27_2
    setuptools:  15.0-py27_0       --> 20.7.0-py27_0
    sqlite:      3.8.4.1-1         --> 3.9.2-0
    yaml:        0.1.4-0           --> 0.1.6-0

---------------------------------------------------------------------


"
Wes Holler <wholler@algebraixdata.com>,"Fri, 29 Apr 2016 23:09:17 +0000",RPC Timeout and Abnormally Long JvmGcTime,dev <dev@spark.apache.org>,"Recently we switched to EMR 4.5/Spark 1.6.1 and have since encountered a new failure scenario.

The primary symptom is that the cluster appears to be stalled. The job has tException s (see below) are usually found towards the bottom of the log. Another observation is that, using the status API, I can see that the jvmGcTime value (see JSON snippet below) for some tasks in a stage that never finished is abnormally large (in the millions). Even for identical queries/data, this problem doesn't always happen, and when the cluster doesn't hang, I also do not see these large values for jvmGcTime.

Some questions:
1. What units is jvmGcTime in?
2. If it is in milliseconds, then that seems to indicate garbage collections times into the tens of minutes - far longer that the queries themselves usually take and also longer than the default spark.rpc.askTimeout value of 120s. Could this be blocking/starving the promise and causing the failure?
3. We are doing almost entirely standard SQL, no UDFs, and no use of the RDD API. I was under the impression that in Spark 1.6, pretty much everything DataFrames is using the custom Tungsten serialization and unsafe buffers, even in the actual processing (so no actual unroll). Is this correct? What could be causing such GC churn?

===============================================================================

The exception found in the log (several stack trace lines removed for brevity:

16/04/29 17:43:29 ERROR ContextCleaner: Error cleaning broadcast 0
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	.
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:68)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	.
	... 12 more
16/04/29 17:43:29 WARN BlockManagerMaster: Failed to remove broadcast 0 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	.
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
16/04/29 17:43:29 INFO ContextCleaner: Cleaned accumulator 2
16/04/29 19:33:23 INFO PackagesResourceConfig: Scanning for root resource and provider classes in the packages:
  org.apache.spark.status.api.v1
16/04/29 19:33:28 INFO ScanningResourceConfig: Root resource classes found:
  class org.apache.spark.status.api.v1.ApiRootResource
16/04/29 19:33:28 INFO ScanningResourceConfig: Provider classes found:
  class org.apache.spark.status.api.v1.JacksonMessageWriter
16/04/29 19:33:28 INFO WebApplicationImpl: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
16/04/29 19:34:09 WARN Errors: The following warnings have been detected with resource and/or provider classes:
  WARNING: A sub-resource method, public scala.collection.Seq org.apache.sps treated as a resource method

===============================================================================

A sample from the adasdasdasdasd:

""1446406"" : {
† † † ""taskId"" : 1446406,
† † † ""index"" : 593,
† † † ""attempt"" : 0,
† † † ""launchTime"" : ""2016-04-27T11:16:25.005GMT"",
† † † ""executorId"" : ""364"",
† † † ""host"" : ""ip-10-0-0-41.ec2.internal"",
† † † ""taskLocality"" : ""RACK_LOCAL"",
† † † ""speculative"" : false,
† † † ""accumulatorUpdates"" : [ ],
† † † ""taskMetrics"" : {
† † † † ""executorDeserializeTime"" : 0,
† † † † ""executorRunTime"" : 0,
† † † † ""resultSize"" : 0,
† † † † ""jvmGcTime"" : 1637765,
† † † † ""resultSerializationTime"" : 0,
† † † † ""memoryBytesSpilled"" : 0,
† † † † ""diskBytesSpilled"" : 0,
† † † † ""inputMetrics"" : {
† † † † † ""bytesRead"" : 40824341,
† † † † † ""recordsRead"" : 1130000
† † † † },
† † † † ""shuffleWriteMetrics"" : {
† † † † † ""bytesWritten"" : 0,
† † † † † ""writeTime"" : 0,
† † † † † ""recordsWritten"" : 0
† † † † }
† † † }
† † },



---------------------------------------------------------------------


"
Wes Holler <wholler@algebraixdata.com>,"Fri, 29 Apr 2016 23:11:16 +0000",RE: RPC Timeout and Abnormally Long JvmGcTime,dev <dev@spark.apache.org>,"Oops... The ""adasdasdasdasd"" below is JSON scraped from the status API endpoint.

w failure scenario.

The primary symptom is that the cluster appears to be stalled. The job has tException s (see below) are usually found towards the bottom of the log. Another observation is that, using the status API, I can see that the jvmGcTime value (see JSON snippet below) for some tasks in a stage that never finished is abnormally large (in the millions). Even for identical queries/data, this problem doesn't always happen, and when the cluster doesn't hang, I also do not see these large values for jvmGcTime.

Some questions:
1. What units is jvmGcTime in?
2. If it is in milliseconds, then that seems to indicate garbage collections times into the tens of minutes - far longer that the queries themselves usually take and also longer than the default spark.rpc.askTimeout value of 120s. Could this be blocking/starving the promise and causing the failure?
3. We are doing almost entirely standard SQL, no UDFs, and no use of the RDD API. I was under the impression that in Spark 1.6, pretty much everything DataFrames is using the custom Tungsten serialization and unsafe buffers, even in the actual processing (so no actual unroll). Is this correct? What could be causing such GC churn?

===============================================================================

The exception found in the log (several stack trace lines removed for brevity:

16/04/29 17:43:29 ERROR ContextCleaner: Error cleaning broadcast 0
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	.
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:68)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	.
	... 12 more
16/04/29 17:43:29 WARN BlockManagerMaster: Failed to remove broadcast 0 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	.
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
16/04/29 17:43:29 INFO ContextCleaner: Cleaned accumulator 2
16/04/29 19:33:23 INFO PackagesResourceConfig: Scanning for root resource and provider classes in the packages:
  org.apache.spark.status.api.v1
16/04/29 19:33:28 INFO ScanningResourceConfig: Root resource classes found:
  class org.apache.spark.status.api.v1.ApiRootResource
16/04/29 19:33:28 INFO ScanningResourceConfig: Provider classes found:
  class org.apache.spark.status.api.v1.JacksonMessageWriter
16/04/29 19:33:28 INFO WebApplicationImpl: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
16/04/29 19:34:09 WARN Errors: The following warnings have been detected with resource and/or provider classes:
  WARNING: A sub-resource method, public scala.collection.Seq org.apache.sps treated as a resource method

===============================================================================

A sample from the adasdasdasdasd:

""1446406"" : {
† † † ""taskId"" : 1446406,
† † † ""index"" : 593,
† † † ""attempt"" : 0,
† † † ""launchTime"" : ""2016-04-27T11:16:25.005GMT"",
† † † ""executorId"" : ""364"",
† † † ""host"" : ""ip-10-0-0-41.ec2.internal"",
† † † ""taskLocality"" : ""RACK_LOCAL"",
† † † ""speculative"" : false,
† † † ""accumulatorUpdates"" : [ ],
† † † ""taskMetrics"" : {
† † † † ""executorDeserializeTime"" : 0,
† † † † ""executorRunTime"" : 0,
† † † † ""resultSize"" : 0,
† † † † ""jvmGcTime"" : 1637765,
† † † † ""resultSerializationTime"" : 0,
† † † † ""memoryBytesSpilled"" : 0,
† † † † ""diskBytesSpilled"" : 0,
† † † † ""inputMetrics"" : {
† † † † † ""bytesRead"" : 40824341,
† † † † † ""recordsRead"" : 1130000
† † † † },
† † † † ""shuffleWriteMetrics"" : {
† † † † † ""bytesWritten"" : 0,
† † † † † ""writeTime"" : 0,
† † † † † ""recordsWritten"" : 0
† † † † }
† † † }
† † },



---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Renyi Xiong <renyixiong0@gmail.com>,"Sat, 30 Apr 2016 15:52:08 -0700",persist versus checkpoint,dev <dev@spark.apache.org>,"Hi,

Is RDD.persist equivalent to RDD.checkpoint If they save same number of
copies (say 3) to disk?

(I assume persist saves copies on different machines ?)

thanks,
Renyi.
"
