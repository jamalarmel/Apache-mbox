Joseph Bradley <joseph@databricks.com>,"Mon, 30 Nov 2015 16:33:37 -0800",Re: Problem in running MLlib SVM,Fazlan Nazeem <fazlann@wso2.com>,"model.predict should return a 0/1 predicted label.  The example code is
misleading when it calls the prediction a ""score.""


herwise. I
 the
I
= 0))
g
or
is
ns);
== 0))
;
"
Joseph Bradley <joseph@databricks.com>,"Mon, 30 Nov 2015 16:34:02 -0800",Re: Grid search with Random Forest,Ndjido Ardo Bar <ndjido@gmail.com>,"It should work with 1.5+.


"
Ndjido Ardo BAR <ndjido@gmail.com>,"Tue, 01 Dec 2015 02:57:27 +0000",Re: Grid search with Random Forest,Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

Yes Random Forest support Grid Search on Spark 1.5.+ . But I'm getting a
""rawPredictionCol field does not exist exception"" on Spark 1.5.2 for
Gradient Boosting Trees classifier.


Ardo

"
Roman Shaposhnik <rvs@apache.org>,"Mon, 30 Nov 2015 22:30:21 -0800",FOSDEM 2016 - take action by 4th of December 2015,rvs@apache.org,"As most of you probably know FOSDEM 2016 (the biggest,
100% free open source developer conference) is right 
around the corner:
   https://fosdem.org/2016/

We hope to have an ASF booth and we would love to see as
many ASF projects as possible present at various tracks
(AKA Developer rooms):
   https://fosdem.org/2016/schedule/#devrooms

This year, for the first time, we are running a dedicated
Big Data and HPC Developer Room and given how much of that
open source development is done at ASF it would be great
to have folks submit talks to:
   https://hpc-bigdata-fosdem16.github.io

While the CFPs for different Developer Rooms follow slightly 
different schedules, but if you submit by the end of this week 
you should be fine.

Finally if you don't want to fish for CFP submission URL,
here it is:
   https://fosdem.org/submit

If you have any questions -- please email me *directly* and
hope to see as many of you as possible in two months! 

Thanks,
Roman.

---------------------------------------------------------------------


"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Tue, 1 Dec 2015 08:32:08 +0100",Re: Grid search with Random Forest,Ndjido Ardo BAR <ndjido@gmail.com>,"Hi Ndjido,

This is because GBTClassifier doesn't yet have a rawPredictionCol like the.
RandomForestClassifier has.
Cf:
http://spark.apache.org/docs/latest/ml-ensembles.html#output-columns-predictions-1

"
Alexander Pivovarov <apivovarov@gmail.com>,"Mon, 30 Nov 2015 23:38:37 -0800",Re: How to add 1.5.2 support to ec2/spark_ec2.py ?,dev@spark.apache.org,"just want to follow up

"
Ndjido Ardo BAR <ndjido@gmail.com>,"Tue, 01 Dec 2015 07:43:46 +0000",Re: Grid search with Random Forest,Benjamin Fradet <benjamin.fradet@gmail.com>,"Hi Benjamin,

Thanks, the documentation you sent is clear.
Is there any other way to perform a Grid Search with GBT?


Ndjido

"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 1 Dec 2015 00:22:47 -0800",Re: How to add 1.5.2 support to ec2/spark_ec2.py ?,Alexander Pivovarov <apivovarov@gmail.com>,"Yeah we just need to add 1.5.2 as in
https://github.com/apache/spark/commit/97956669053646f00131073358e53b05d0c3d5d0#diff-ada66bbeb2f1327b508232ef6c3805a5
to the master branch as well

Thanks
Shivaram




---------------------------------------------------------------------


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 1 Dec 2015 00:32:14 -0800",Re: How to add 1.5.2 support to ec2/spark_ec2.py ?,shivaram@eecs.berkeley.edu,"Thank you,
I looked at master branch. I did not realize that it's behind branch-1.5
BTW, line 54 still has SPARK_EC2_VERSION = ""1.5.1""


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 1 Dec 2015 00:49:27 -0800",Re: How to add 1.5.2 support to ec2/spark_ec2.py ?,Alexander Pivovarov <apivovarov@gmail.com>,"Yeah - that needs to be changed as well. Could you send a PR to fix this ?

Shivaram


---------------------------------------------------------------------


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 1 Dec 2015 00:50:13 -0800",Re: How to add 1.5.2 support to ec2/spark_ec2.py ?,shivaram@eecs.berkeley.edu,"Did 6 min ago


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 1 Dec 2015 10:50:05 +0100",Re: Bringing up JDBC Tests to trunk,Josh Rosen <joshrosen@databricks.com>,"

Hi Josh,

Could you elaborate a little more on what is really required and how
to verify requested changes (or at least what is failing so once it's
not, it's supposed to be a solution)? The little magic word ""sbt""
triggered me thinking I could help here and there? :-)

If there's a JIRA task for it, let me know. Thanks!

Jacek

---------------------------------------------------------------------


"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Tue, 1 Dec 2015 14:10:48 +0100",Re: Grid search with Random Forest,Ndjido Ardo BAR <ndjido@gmail.com>,"Someone correct me if I'm wrong but no there isn't one that I am aware of.

Unless someone is willing to explain how to obtain the raw prediction
column with the GBTClassifier. In this case I'd be happy to work on a PR.

"
Joseph Bradley <joseph@databricks.com>,"Tue, 1 Dec 2015 10:29:55 -0800",Re: Grid search with Random Forest,Benjamin Fradet <benjamin.fradet@gmail.com>,"You can do grid search if you set the evaluator to a
MulticlassClassificationEvaluator, which expects a prediction column, not a
rawPrediction column.  There's a JIRA for making
BinaryClassificationEvaluator accept prediction instead of rawPrediction.
Joseph


"
Ndjido Ardo BAR <ndjido@gmail.com>,"Tue, 01 Dec 2015 18:50:18 +0000",Re: Grid search with Random Forest,"Joseph Bradley <joseph@databricks.com>, Benjamin Fradet <benjamin.fradet@gmail.com>","Thanks for the clarification. Gonna test that and give you feedbacks.

Ndjido

"
Tarek Elgamal <tarek.elgamal@gmail.com>,"Tue, 1 Dec 2015 14:02:55 -0600",Re: Problem in running MLlib SVM,Joseph Bradley <joseph@databricks.com>,"Thanks, actually model.predict() gives a number between 0 and 1. However,
model.predictPoint gives me a number from 0/1 but the accuracy is still
very low. I am using the training data just to make sure that I am using it
right. But it still seems not to work for me.
@Joseph, do you have any benchmark data that you tried SVM on. I am
attaching my toy data with just 100 examples. I tried it with different
data and bigger data and still getting accuracy around 57% on training set.


e
therwise. I
d the
 I
= 0))
 I used
code is
== 0))
);

---------------------------------------------------------------------"
Joseph Bradley <joseph@databricks.com>,"Tue, 1 Dec 2015 12:43:20 -0800",Re: Problem in running MLlib SVM,Tarek Elgamal <tarek.elgamal@gmail.com>,"Oh, sorry about that.  I forgot that's the behavior when the threshold is
not set.  My guess would be that you need more iterations, or that the
regParam needs to be tuned.

I'd recommend testing on some of the LibSVM datasets.  They have a lot, and
you can find existing examples (and results) for many of them.


it
t.
t
m
otherwise. I
nd the
d I
s
= 0))
a I used
 code is
 == 0))
,
));
"
Robert Dodier <robert.dodier@gmail.com>,"Tue, 1 Dec 2015 16:35:46 -0700 (MST)",Re: Problem in running MLlib SVM,dev@spark.apache.org,"Tarek,

first computes dot(w, x) + b where w is the SVM weight vector, x is the
input vector, and b is a constant. If there is a threshold defined, then the
output is 1 if that's greater than the threshold and 0 otherwise. If there
is no threshold, then it just returns dot(w, x) + b. There is no requirement
that the output be constrained to a specific range. 

For a little problem I was working on, I investigated the outputs a little
bit; here's a snippet of some stuff you could put in spark-shell:

    model.clearThreshold
    val foo = x.map (p => (p.label, model.predict (p.features)))
    import org.apache.spark.mllib.stat.Statistics
    val summary = Statistics.colStats (foo.map {case (a, b) => Vectors.dense
(a, b)})
    summary.mean
    summary.min
    summary.max

When I tried that, I found a very large range of outputs -- something like
-6*10^6 to -400, with a mean of about -30000. If you look into it, let us
know what you find, I would be interested to hear about it.

best,

Robert Dodier



--

---------------------------------------------------------------------


"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Wed, 02 Dec 2015 11:21:42 +0800",query on SVD++,<dev@spark.apache.org>,"Hi All,

 

I came across the SVD++ algorithm implementation in Spark code base, but I
was wondering why we didn't expose the scala api interface to python?

Any plan to do this?

 

BR,

-Allen Zhang

"
caiquermarques95 <caiquermarques95@gmail.com>,"Wed, 2 Dec 2015 05:51:05 -0700 (MST)",Python API for Association Rules,dev@spark.apache.org,"Hello everyone!
I'm developing to the Python API for association rules (
https://issues.apache.org/jira/browse/SPARK-8855), but I found a doubt.

Following the description of the issue, it says that a important method is ""
*FPGrowthModel.generateAssociationRules()*"", of course. However, is not
clear if a wrapper for the association rules it will be in ""
*FPGrowthModelWrapper.scala*"" and this is the problem.

My idea is the following:
1) In the fpm.py file; class ""Association Rules"" with one method and a
class:
1.1) Method train(data, minConfidence), that will generate the association
rules for a data with a minConfidence specified (0.6 default). This method
will call the ""trainAssociationRules"" from the *PythonMLLibAPI* with the
parameters data and minConfidence. Later. will return a FPGrowthModel.
1.2) Class Rule, that will a namedtuple, represents an (antecedent,
consequent) tuple.

2) Still in fpm.py, in the class FPGrowthModel, a new method will be added,
called generateAssociationRules, that will map the Rules generated calling
the method ""getAssociationRule"" from FPGrowthModelWrapper to the namedtuple.

Now is my doubt, how to make trainAssociationRules returns a FGrowthModel
to the Wrapper just maps the rule received to the antecedent/consequent? I
could not do the method trainAssociationRules returns a FPGrowthModel. The
wrapper for association rules is in FPGrowthModelWrapper, right?

For illustration, I think something like this in *PythonMLLibAPI:*

def trainAssociationRules(
      data: JavaRDD[FPGrowth.FreqItemset[Any]],
      minConfidence: Double): [return type] = {

    val model = new FPGrowthModel(data.rdd)
      .generateAssociationRules(minConfidence)

    new FPGrowthModelWrapper(model)
  }

And in FPGrowthModelWrapper, something like:

 def getAssociationRules: [return type] = {
    SerDe.fromTuple2RDD(rule.map(x => (x.javaAntecedent, x.javaConsequent)))
 }

I know that will fail, but, what is wrong with my idea?
Any suggestions?

Thanks for the help and the tips.
Caique.




--"
Sean Owen <sowen@cloudera.com>,"Wed, 2 Dec 2015 15:47:09 +0000",IntelliJ license for committers?,dev <dev@spark.apache.org>,"I'm aware that IntelliJ has (at least in the past) made licenses
available to committers in bona fide open source projects, and I
recall they did the same for Spark. I believe I'm using that license
now, but it seems to have expired? If anyone knows the status of that
(or of any renewals to the license), I wonder if you could share that
with me, offline of course.

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Wed, 2 Dec 2015 10:18:07 -0800",Re: IntelliJ license for committers?,Sean Owen <sowen@cloudera.com>,"I think they can renew your license. In
https://www.jetbrains.com/buy/opensource/?product=idea, you can find
""Update Open Source License"".


"
Sean Owen <sowen@cloudera.com>,"Wed, 2 Dec 2015 18:24:51 +0000",Re: IntelliJ license for committers?,"Yin Huai <yhuai@databricks.com>, Josh Rosen <joshrosen@databricks.com>","Thanks, yes I've seen this, though I recall from another project that
at some point they said, wait, we already gave your project a license!
and I had to track down who had it. I think Josh might be the keeper?
Not a big deal, just making sure I didn't miss an update there.


---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 2 Dec 2015 12:09:29 -0800",Re: IntelliJ license for committers?,Sean Owen <sowen@cloudera.com>,"Yep, I'm the point of contact between us and JetBrains. I forwarded the
2015 license renewal email to the private@ list, so it should be accessible
via the archives. I'll go ahead and forward you a copy of our project
license, which will have to be renewed in January of next year.


"
Michael Armbrust <michael@databricks.com>,"Wed, 2 Dec 2015 12:26:53 -0800",[VOTE] Release Apache Spark 1.6.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.0!

The vote is open until Saturday, December 5, 2015 at 21:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is *v1.6.0-rc1
(bf525845cef159d2d4c9f4d64e158f037179b5c4)
<https://github.com/apache/spark/tree/v1.6.0-rc1>*

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-v1.6.0-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1165/

The test repository (versioned as v1.6.0-rc1) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1164/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc1-docs/


=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.

===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.


==================================================
== Major changes to help you focus your testing ==
==================================================

Spark SQL

   - SPARK-10810 <https://issues.apache.org/jira/browse/SPARK-10810>
   Session Management - The ability to create multiple isolated SQL
   Contexts that have their own configuration and default database.  This is
   turned on by default in the thrift server.
   - SPARK-9999  <https://issues.apache.org/jira/browse/SPARK-9999> Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000 <https://issues.apache.org/jira/browse/SPARK-10000> Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197 <https://issues.apache.org/jira/browse/SPARK-11197> SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745 <https://issues.apache.org/jira/browse/SPARK-11745> Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 <https://issues.apache.org/jira/browse/SPARK-10412>
Per-operator
   Metics for SQL Execution - Display statistics on a per-operator basis
   for memory usage and spilled data size.
   - SPARK-11329 <https://issues.apache.org/jira/browse/SPARK-11329> Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 <https://issues.apache.org/jira/browse/SPARK-10917>,
   SPARK-11149 <https://issues.apache.org/jira/browse/SPARK-11149> In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111 <https://issues.apache.org/jira/browse/SPARK-11111> Fast
   null-safe joins - Joins using null-safe equality (<=>) will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389 <https://issues.apache.org/jira/browse/SPARK-11389> SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978 <https://issues.apache.org/jira/browse/SPARK-10978> Datasource
   API Avoid Double Filter - When implementing a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849  <https://issues.apache.org/jira/browse/SPARK-4849> Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858  <https://issues.apache.org/jira/browse/SPARK-9858> Adaptive
   query execution - Initial support for automatically selecting the number
   of reducers for joins and aggregations.

Spark Streaming

   - API Updates
      - SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629> New
      improved state management - trackStateByKey - a DStream
      transformation for stateful stream processing, supersedes
      updateStateByKey in functionality and performance.
      - SPARK-11198 <https://issues.apache.org/jira/browse/SPARK-11198> Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891 <https://issues.apache.org/jira/browse/SPARK-10891> Kinesis
      message handler function - Allows arbitrary function to be applied to
      a Kinesis record in the Kinesis receiver before to customize what data is
      to be stored in memory.
      - SPARK-6328  <https://issues.apache.org/jira/browse/SPARK-6328>
       Python Streaming Listener API - Get streaming statistics (scheduling
      delays, batch processing times, etc.) in streaming.


   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress bars

MLlibNew algorithms/models

   - SPARK-8518  <https://issues.apache.org/jira/browse/SPARK-8518> Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834  <https://issues.apache.org/jira/browse/SPARK-9834> Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930  <https://issues.apache.org/jira/browse/SPARK-9930> New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517  <https://issues.apache.org/jira/browse/SPARK-6517> Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means

API improvements

   - ML Pipelines
      - SPARK-6725  <https://issues.apache.org/jira/browse/SPARK-6725> Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.ml algorithms
      - SPARK-5565  <https://issues.apache.org/jira/browse/SPARK-5565> LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836  <https://issues.apache.org/jira/browse/SPARK-9836> R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681  <https://issues.apache.org/jira/browse/SPARK-9681> Feature
      interactions in R formula - Interaction operator "":"" in R formula
   - Python API - Many improvements to Python API to approach feature parity

Misc improvements

   - SPARK-7685  <https://issues.apache.org/jira/browse/SPARK-7685>,
   SPARK-9642  <https://issues.apache.org/jira/browse/SPARK-9642> Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 <https://issues.apache.org/jira/browse/SPARK-10384>,
   SPARK-10385 <https://issues.apache.org/jira/browse/SPARK-10385> Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117 <https://issues.apache.org/jira/browse/SPARK-10117> LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751  <https://issues.apache.org/jira/browse/SPARK-7751> @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337 <https://issues.apache.org/jira/browse/SPARK-11337> Testable
   example code - Automated testing for code in user guide examples

Deprecations

   - In spark.mllib.clustering.KMeans, the ""runs"" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the ""weights"" field has been
   deprecated, in favor of the new name ""coefficients."" This helps
   disambiguate from instance (row) weights given to algorithms.

Changes of behavior

   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
"
Sean Owen <sowen@cloudera.com>,"Wed, 2 Dec 2015 20:28:30 +0000",When to cut RCs,dev <dev@spark.apache.org>,"Moving this to dev --

That's good, that'll help. Technically there's still a Blocker bug:
https://issues.apache.org/jira/browse/SPARK-12000

The 'race' doesn't matter that much, but release planning remains the
real bug-bear here. There are still, for instance, 52 issues targeted
at 1.6.0, 42 of which were raised and targeted by committers. For
example, I count 6 'umbrella' JIRAs in ML alone that are still open.
The release is theoretically several weeks behind plan on what's
intended to be a fixed release cycle too. This is why I'm not sure why
today it's suddenly potentially ready for release.

I'm just curious, am I the only one that thinks this isn't roughly
normal, or do other people manage releases this way? I know the real
world is messy and this is better than in the past, but I still get
surprised by how each 1.x release actually comes about.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 02 Dec 2015 20:39:06 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),"Michael Armbrust <michael@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","-0

If spark-ec2 is still a supported part of the project, then we should
update its version lists as new releases are made. 1.5.2 had the same issue.

https://github.com/apache/spark/blob/v1.6.0-rc1/ec2/spark_ec2.py#L54-L91

(I guess as part of the 2.0 d"
Michael Armbrust <michael@databricks.com>,"Wed, 2 Dec 2015 13:06:14 -0800",Re: When to cut RCs,Sean Owen <sowen@cloudera.com>,"Thanks for bringing this up Sean. I think we are all happy to adopt
concrete suggestions to make the release process more transparent,
including pinging the list before kicking off the release build.

Technically there's still a Blocker bug:


Sorry, I misprioritized this particular issue when I thought that it was
going to block the release by causing the doc build to fail. When I
realized the failure was non-deterministic and isolated to OSX (i.e. the
official release build on jenkins is not affected) I failed to update the
issue.  It doesn't show up on the dashboard that I've been using to track
the release
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20component%20not%20in%20(Tests%2C%20Documentation)%20AND%20%22Target%20Version%2Fs%22%20%3D%201.6.0%20AND%20(fixVersion%20is%20EMPTY%20OR%20fixVersion%20!%3D%201.6.0)%20AND%20(status%20%3D%20Open%20OR%20status%20%3D%20%22In%20Progress%22%20OR%20status%20%3D%20Reopened)%20ORDER%20BY%20priority%20DESCsince
its labeled documentation.



This can be debated, but I explicitly ignored test and documentation
issues.  Since the docs are published separately and easy to update, I
don't think its worth further disturbing the release cadence for these
JIRAs.



Up until today various committers have told me that there were known issues
with branch-1.6 that would cause them to -1 the release.  Whenever this
happened, I asked them to ensure there was a properly targeted blocker JIRA
open so people could publicly track the status of the release.  As long as
such issues were open, I only published a preview since making an RC is
pretty high cost.

I'm sorry that it felt sudden to you, but as of last night all such known
issues were resolved and thus I cut a release as soon as this was the case.

I'm just curious, am I the only one that thinks this isn't roughly

I actually did spent quite a bit of time asking people to close various
umbrella issues, and I was pretty strict about watching JIRA throughout the
process.  Perhaps as an additional step, future preview releases or branch
cuts can include a link to an authoritative dashboard that we will use to
decide when we are ready to make an RC.  I'm also open to other suggestions.

Michael
"
Joseph Bradley <joseph@databricks.com>,"Wed, 2 Dec 2015 13:12:56 -0800",Re: Python API for Association Rules,caiquermarques95 <caiquermarques95@gmail.com>,"If you're working on a feature, please comment on the JIRA first (to avoid
conflicts / duplicate work).  Could you please copy what your wrote to the
JIRA to discuss there?
Thanks,
Joseph


"
Sean Busbey <busbey@cloudera.com>,"Wed, 2 Dec 2015 15:19:04 -0600",Re: When to cut RCs,Michael Armbrust <michael@databricks.com>,"

It would help me, and I'm sure other less-active folks, if this kind of
feedback cycle were visible on dev@spark. It would also have the benefit of
getting feedback from non-committers who might e.g. have some issue that
they see in their own production deployments.



-- 
Sean
"
Sean Busbey <busbey@cloudera.com>,"Wed, 2 Dec 2015 15:20:28 -0600",Re: When to cut RCs,Michael Armbrust <michael@databricks.com>,"
Sorry for a second email so soon. I meant to also ask, what keeps the cost
of making an RC high? Can we bring it down with better tooling?


-- 
Sean
"
Michael Armbrust <michael@databricks.com>,"Wed, 2 Dec 2015 13:24:23 -0800",Re: When to cut RCs,Sean Busbey <busbey@cloudera.com>,"
There is a lot of tooling:
https://amplab.cs.berkeley.edu/jenkins/view/Spark-Packaging/

Still you have check JIRA, sync with people who have been working on known
issues, run the jenkins jobs (which take 1+ hours) and then write that
email which has a bunch of links in it.  Short of automating the creation
of the email (PRs welcome!) I'm not sure what else you would automate.
That said, this is all I have done since I came into work today.
"
Sean Owen <sowen@cloudera.com>,"Wed, 2 Dec 2015 21:25:10 +0000",Re: When to cut RCs,Michael Armbrust <michael@databricks.com>,"
It makes sense to not hold up an RC since they don't affect testing of
functionality. Prior releases have ultimately gone out with doc issues
still outstanding (and bugs) though. This doesn't seem to be on
anyone's release checklist, and maybe part of it is because they're
let slide for RCs.  Your suggestion to check-point release status
below sounds spot on; I sort of tried to do that earlier.



Makes sense if these are all getting translated into Blockers and
resolved before an RC. It's the simplest mechanism to communicate and
track this in a distributed way.

""No blockers"" is a minimal criterion for release. It still seems funny
to release with so many issues targeted for 1.6.0, including issues
that aren't critical or bugs. Sure, that's just hygiene. But without
it, do people take ""Target Version"" seriously? if they don't, is there
any force guiding people to prioritize or decide what to (not) work
on? I'm sure the communication happens, just doesn't seem like it's
fully on JIRA, which is ultimately suboptimal.



Yes, that's great. It takes the same effort from everyone. Having a
green light on a dashboard at release time is only the symptom of
decent planning. The effect I think it really needs to have occurs
now: what's really probably on the menu for 1.7? and periodically
track against that goal. Then the release process is with any luck
just a formality with no surprises.

---------------------------------------------------------------------


"
Sean Busbey <busbey@cloudera.com>,"Wed, 2 Dec 2015 15:26:22 -0600",Re: IntelliJ license for committers?,Josh Rosen <joshrosen@databricks.com>,"the private@spark list is only available to PMC members[1]. Could we
document somewhere (the IntelliJ section of the wiki[2]?) both the current
point of contact and a list of what happens when things get renewed each
year?

That way we could include either a note that the POC should email the key
to all committers or that committers should reach out to the POC at a given
time?

[1]: PMC members and ASF Members, so Sean O specifically can search the
archives. but that won't solve the general problem.
[2]:
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ




-- 
Sean
"
Sean Owen <sowen@cloudera.com>,"Wed, 2 Dec 2015 21:37:27 +0000",Re: IntelliJ license for committers?,Sean Busbey <busbey@cloudera.com>,"Yeah I can see the PMC list as it happens; technically there are
committers that aren't PMC / ASF members though, yeah. Josh did update
the list when the last one expired, and the current license hasn't
expired yet, though it was no longer working for me. It turns out to
not be valid for the very newest IJ.

Josh actually gave me a better solution: just use the free IntelliJ.
Actually, none of the features we're likely to need are omitted from
the community edition.

Sure, it's cool if the existence of an update can be disseminated on
dev@ and I'm guessing it would certainly be when it came up. My
problem was actually slightly different and should have clarified I
was curious if anyone else had license problems 'early' and if I'd
missed an update, but I hadn't.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 3 Dec 2015 07:02:00 +0800",Re: IntelliJ license for committers?,Sean Owen <sowen@cloudera.com>,"For IntelliJ I think the free version is sufficient for Spark development.


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 2 Dec 2015 15:28:06 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Nicholas Chammas <nicholas.chammas@gmail.com>,"I tried to run test suite and encountered the following:

http://pastebin.com/DPnwMGrm

FYI


"
Michael Armbrust <michael@databricks.com>,"Wed, 2 Dec 2015 16:23:47 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Ted Yu <yuzhihong@gmail.com>,"I'm going to kick the voting off with a +1 (binding).  We ran TPC-DS and
most queries are faster than 1.5.  We've also ported several production
pipelines to 1.6.
"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 2 Dec 2015 17:11:56 -0800",Re: IntelliJ license for committers?,Reynold Xin <rxin@databricks.com>,"
And I believe anyone with a @apache.org e-mail address can request their
own personal license for InteliJ. That's what  I personally did.

-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 2 Dec 2015 18:13:33 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Michael Armbrust <michael@databricks.com>,"+1

Ran through test suite (minus docker-integration-tests) which passed.

Overall experience was much better compared with some of the prior RC's.

[INFO] Spark Project External Kafka ....................... SUCCESS [
53.956 s]
[INFO] Spark Project Examp"
Yanbo Liang <ybliang8@gmail.com>,"Thu, 3 Dec 2015 10:29:59 +0800",Re: query on SVD++,=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?= <zzq98736@alibaba-inc.com>,"You means the SVDPlusPlus in GraphX? If you want to use SVD++ to train CF
model, I recommend you to use ALS which is more efficiency and has python
interface.

2015-12-02 11:21 GMT+08:00 Âº†ÂøóÂº∫(Êó∫ËΩ©) <zzq98736@alibaba-inc.com>:

I
hon?
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 2 Dec 2015 19:07:45 -0800",Re: When to cut RCs,Sean Owen <sowen@cloudera.com>,"In terms of advertising to people the status of the release and whether an
RC is likely to go out, the best mechanism I can think of is our current
mechanism of using JIRA and respecting the semantics of a blocker JIRA. We
could do a better job though creating a JIRA dashboard for each release and
linking to it publicly so it's very clear to people what is going on. I
have always used one privately when managing previous releases, but no
reason we can't put one up on the website or wiki.

IMO a mailing list is not a great mechanism for the fine-grained work of
release management because of the sheer complexity and volume of finalizing
a spark release. Being a release manager means tracking over a course of
several weeks typically dozens of distinct issues and trying to prioritize
them, get more clarity from the report of those issues, possibly reaching
out to people on the phone or in person to get more details, etc. You want
a mutable dashboard where you can convey the current status clearly.

What might be good in the early stages is a weekly e-mail to the dev@ list
just refreshing what is on the JIRA and letting people know how things are
looking. So someone just passing by has some idea of how things are going
and can chime in, etc.

discussion. At that point the number of known issues is small enough I
think to discuss in an all-to-all fashion.

- Patrick


"
shane knapp <sknapp@berkeley.edu>,"Wed, 2 Dec 2015 19:20:37 -0800","[build system] jenkins downtime, thursday 12/10/15 7am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","there's Yet Another Jenkins Security Advisory[tm], and a big release
to patch it all coming out next wednesday.

to that end i will be performing a jenkins update, as well as
performing the work to resolve the following jira issue:
https://issues.apache.org/jira/browse/SPARK-11255

i will put jenkins in to quiet mode around 6am, start work around 7am
and expect everything to be back up and building before 9am.  i'll
post updates as things progress.

please let me know ASAP if there's any problem with this schedule.

shane

---------------------------------------------------------------------


"
Caique Marques <caiquermarques95@gmail.com>,"Thu, 3 Dec 2015 02:07:32 -0200",Re: Python API for Association Rules,Joseph Bradley <joseph@databricks.com>,"Hi Joseph.
Sorry for my fail, I will comment on Jira.

Thanks.
Caique.

2015-12-02 19:12 GMT-02:00 Joseph Bradley <joseph@databricks.com>:

"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 3 Dec 2015 12:02:25 +0530",Re: Multiplication on decimals in a dataframe query,Philip Dodds <philip.dodds@gmail.com>,"Not quiet sure whats happening, but its not an issue with multiplication i
guess as the following query worked for me:

trades.select(trades(""price"")*9.5).show
+-------------+
|(price * 9.5)|
+-------------+
|        199.5|
|        228.0|
|        190.0|
|        199.5|
|        190.0|
|        256.5|
|        218.5|
|        275.5|
|        218.5|
......
......


Could it be with the precision? ccing dev list, may be you can open up a
jira for this as it seems to be a bug.

Thanks
Best Regards


"
Sean Owen <sowen@cloudera.com>,"Thu, 3 Dec 2015 08:47:27 +0000",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"Reynold, did you (or someone else) delete version 1.7.0 in JIRA? I
think that's premature. If there's a 1.7.0 then we've lost info about
what it would contain. It's trivial at any later point to merge the
versions. And, since things change and there's not a pressing need to
decide one way or the other, it seems fine to at least collect this
info like we have things like ""1.4.3"" that may never be released. I'd
like to add it back?

n
n
ay on
their
r
hat
the
m>
ala
uld
tter
 once
te:
and
l on
want
y
ays
rt
the
map
k this
s
.0 and
has
m>
m>
 in
ging the
nt to
ing to
ataset
ble
 we
I'd like to
llow us to
n
given what
1.x release
This might
d thing.
.
 the
egory, as we
.
ented to new
 Until the
 best
nces, ...,
om>
for
 scale.
d seem
ns.  AFAIK
to the
native RDD
t, but
API came
oductory
 emphasize
is that with
ze and
be
nted as the
trast, would
 API that
mes or
but
ainful issue for
or
th DataFrame
rg;
ing
on with user
sPathFirst,
ependencies
a whitelist
plicitly
s also
ul
cture of two
g.
f
olve with
y. That way
deprecate to be
al
API and remove
ppy to have that
ul
cture of two
g.
f
olve with
t
ponent of
s
ºåMatei Zaharia <matei.zaharia@gmail.com> ÂÜôÈÅìÔºö
too
en be useful
e worth
,
 added later
 I imagine
ed. Keeping
t.
e:
om>
are
 a
e,
ss
to
a
1
re
g a
(?
-----
-----
-----

---------------------------------------------------------------------


"
taishi takahashi <takahashi.spark@gmail.com>,"Thu, 3 Dec 2015 18:48:26 +0900",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Michael Armbrust <michael@databricks.com>,"Excuse me,

I'm working on SPARK-10259.(this parent issue is SPARK-7751.)
https://issues.apache.org/jira/browse/SPARK-10259

This issues's purpose is to add @Since annotation to stable and experimenal
methods in MLlib.

in SPARK-7751, this and this children issues' target version is v.1.6.0,
but some issue is in progress.
Jenkins's test.)

if these issues is merged in v.1.6.0, please run Jenkins's test for
SPARK-10259.

Thanks,
Hiroshi Takahashi

2015-12-03 11:13 GMT+09:00 Ted Yu <yuzhihong@gmail.com>:

2015-12-03 5:26 GMT+09:00 Michael Armbrust <michael@databricks.com>:

"
Sean Owen <sowen@cloudera.com>,"Thu, 3 Dec 2015 15:20:06 +0000",Re: A proposal for Spark 2.0,Koert Kuipers <koert@tresata.com>,"Pardon for tacking on one more message to this thread, but I'm
reminded of one more issue when building the RC today: Scala 2.10 does
not in general try to work with Java 8, and indeed I can never fully
compile it with Java 8 on Ubuntu or OS X, due to scalac assertion
errors. 2.11 is the first that's supposed to work with Java 8. This
may be a good reason to drop 2.10 by the time this comes up.


---------------------------------------------------------------------


"
robineast <robin.east@xense.co.uk>,"Thu, 3 Dec 2015 09:39:56 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),dev@spark.apache.org,"+1

OSX 10.10.5, java version ""1.8.0_40"", scala 2.10

mvn clean package -DskipTests

[INFO] Spark Project External Kafka ....................... SUCCESS [ 18.161
s]
[INFO] Spark Project Examples ............................. SUCCESS [01:18
min]
[INFO] Spa"
mkhaitman <mark.khaitman@chango.com>,"Thu, 3 Dec 2015 09:45:15 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),dev@spark.apache.org,"I reported this in the 1.6 preview thread, but wouldn't mind if someone can
confirm that ctrl-c is not keyboard interrupting / clearing the current line
of input anymore in the pyspark shell. I saw the change that would kill the
currently running job when using ctrl+c, but now the only way to clear the
current line of input is to simply hit enter (throwing an exception). Anyone
else seeing this?





--

---------------------------------------------------------------------


"
dodobidu <ricardo.almeida@actnowib.com>,"Thu, 3 Dec 2015 10:25:35 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),dev@spark.apache.org,"+1 (non binding) 

Tested our pipelines on a Spark 1.6.0 standalone cluster (Python only):
- Pyspark package
- Spark SQL
- Dataframes
- Spark MLlib

No major issues, good performance.


Just a minor distinct behavior from version 1.4.1 using a SQLContext:"
"""Mario Ds Briggs"" <mario.briggs@in.ibm.com>","Thu, 3 Dec 2015 23:00:38 +0530","Spark Streaming Kafka - DirectKafkaInputDStream: Using the new Kafka
 Consumer API",dev@spark.apache.org,"

Hi,

Wanted to pick Cody's mind on what he thinks about
DirectKafkaInputDStream/KafkaRDD internally using the new Kafka consumer
API. I know the latter is documented as beta-quality, but yet wanted to
side the consideration is that kafka 0.9.0.0 introduced Authentication and
Encryption (beta again) between clients & brokers, but this is available
only newer Consumer API's and not in the older Low-level/High-level API's.

DirectKafkaInputDStream/KafkaRDD and new Consumer API, my thinking is that
it is possible to support the exact current implementation you have using
the new API's.
offsetRange (I did read about the deterministic feature you were after) and
i couldnt find a direct method in the new Consumer API to get the current
'latest' offset - however one can do a consumer.seekToEnd() and then call a
consumer.position().
 Of course one other benefit is that the new Consumer API's abstracts away
having to deal with finding the leader for a partition, so can get rid of
that code

Would be great to get your thoughts.

thanks in advance
Mario
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 3 Dec 2015 10:03:44 -0800",Re: A proposal for Spark 2.0,Sean Owen <sowen@cloudera.com>,"Reynold's post fromNov. 25:

I don't think we should drop support for Scala 2.10, or make it harder in


to
t
n
ot
,
y
e
n
e
g
se
.
e
a
.
m
o
t
t
d
e
D
g
d
,
 painful
e
te
o
t API and
happy to
ºåMatei Zaharia <matei.zaharia@gmail.com> ÂÜôÈÅìÔºö
t
he
t
s
x
f
o
"
Cody Koeninger <cody@koeninger.org>,"Thu, 3 Dec 2015 12:45:33 -0600","Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the new
 Kafka Consumer API",Mario Ds Briggs <mario.briggs@in.ibm.com>,"Honestly my feeling on any new API is to wait for a point release before
taking it seriously :)

Auth and encryption seem like the only compelling reason to move, but
forcing people on kafka 8.x to upgrade their brokers is questionable.


"
Davies Liu <davies@databricks.com>,"Thu, 3 Dec 2015 11:03:23 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),mkhaitman <mark.khaitman@chango.com>,"Does this https://github.com/apache/spark/pull/10134 is valid fix?
(still worse than 1.5)


---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Thu, 3 Dec 2015 15:28:04 -0500",Re: A proposal for Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"spark 1.x has been supporting scala 2.11 for 3 or 4 releases now. seems to
me you already provide a clear upgrade path: get on scala 2.11 before
upgrading to spark 2.x

from scala team when scala 2.10.6 came out:
We strongly encourage you to upgrade to the latest stable version of Scala
2.11.x, as the 2.10.x series is no longer actively maintained.






n
r
0
I
r
re
e
t
n
ug
w
l
to
at
e
a painful
I
e
3
e
to
nt API
 happy to
e
:
ÔºåMatei Zaharia <matei.zaharia@gmail.com>
o
es
.
of
d
"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Thu, 3 Dec 2015 21:42:28 +0000",SparkStreaming is failing to process Kafka jobs under load....,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

I am running spark streaming application in local mode.  The application is getting data from a Kafka queue.  Everything seems to work fine for a few days then I see following message in the log but Spark is not processing any event.  Any help is greatly appreciated...

2015-12-02 14:45:16 INFO  PhishTank:30 - &&&&&  PhishtankCheck processing key PhishTank on url update-account.log.cgi.cust.mpp.singin.ssl.cussusmpp.com/update/logcheck.php got score 0.0
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8094
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8094
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8094 of size 6576 dropped from memory (free 240797255)
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8094_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8094_piece0 of size 2219 dropped from memory (free 240799474)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8094_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8094_piece0
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8094
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8093
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8093
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8093 of size 74136 dropped from memory (free 240873610)
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8093_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8093_piece0 of size 25655 dropped from memory (free 240899265)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8093_piece0 on localhost:54856 in memory (size: 25.1 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8093_piece0
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8093
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8092
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8092_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8092_piece0 of size 2235 dropped from memory (free 240901500)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8092_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8092_piece0
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8092
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8092 of size 6600 dropped from memory (free 240908100)
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8092
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8091
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8091_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8091_piece0 of size 2235 dropped from memory (free 240910335)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8091_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8091_piece0
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8091
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8091 of size 6600 dropped from memory (free 240916935)
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8091
2015-12-02 14:45:20 INFO  JobScheduler:59 - Added jobs for time 1449085520000 ms
2015-12-02 14:45:30 INFO  JobScheduler:59 - Added jobs for time 1449085530000 ms
2015-12-02 14:45:40 INFO  JobScheduler:59 - Added jobs for time 1449085540000 ms
2015-12-02 14:45:50 INFO  JobScheduler:59 - Added jobs for time 1449085550000 ms
2015-12-02 14:46:00 INFO  JobScheduler:59 - Added jobs for time 1449085560000 ms
2015-12-02 14:46:09 INFO  MemoryStore:59 - ensureFreeSpace(3831) called with curMem=37385621, maxMem=278302556
2015-12-02 14:46:09 INFO  MemoryStore:59 - Block input-0-1449085569600 stored as bytes in memory (estimated size 3.7 KB, free 229.8 MB)
2015-12-02 14:46:09 INFO  BlockManagerInfo:59 - Added input-0-1449085569600 in memory on localhost:54856 (size: 3.7 KB, free: 231.3 MB)
2015-12-02 14:46:09 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085569600
2015-12-02 14:46:09 WARN  BlockManager:71 - Block input-0-1449085569600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:09 INFO  BlockGenerator:59 - Pushed block input-0-1449085569600
2015-12-02 14:46:10 INFO  JobScheduler:59 - Added jobs for time 1449085570000 ms
2015-12-02 14:46:20 INFO  JobScheduler:59 - Added jobs for time 1449085580000 ms
2015-12-02 14:46:30 INFO  JobScheduler:59 - Added jobs for time 1449085590000 ms
2015-12-02 14:46:30 INFO  MemoryStore:59 - ensureFreeSpace(23388) called with curMem=37389452, maxMem=278302556
2015-12-02 14:46:30 INFO  MemoryStore:59 - Block input-0-1449085590200 stored as bytes in memory (estimated size 22.8 KB, free 229.7 MB)
2015-12-02 14:46:30 INFO  BlockManagerInfo:59 - Added input-0-1449085590200 in memory on localhost:54856 (size: 22.8 KB, free: 231.3 MB)
2015-12-02 14:46:30 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085590200
2015-12-02 14:46:30 WARN  BlockManager:71 - Block input-0-1449085590200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:30 INFO  BlockGenerator:59 - Pushed block input-0-1449085590200
2015-12-02 14:46:38 INFO  MemoryStore:59 - ensureFreeSpace(126359) called with curMem=37412840, maxMem=278302556
2015-12-02 14:46:38 INFO  MemoryStore:59 - Block input-0-1449085598600 stored as bytes in memory (estimated size 123.4 KB, free 229.6 MB)
2015-12-02 14:46:38 INFO  BlockManagerInfo:59 - Added input-0-1449085598600 in memory on localhost:54856 (size: 123.4 KB, free: 231.2 MB)
2015-12-02 14:46:38 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085598600
2015-12-02 14:46:38 WARN  BlockManager:71 - Block input-0-1449085598600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:38 INFO  BlockGenerator:59 - Pushed block input-0-1449085598600
2015-12-02 14:46:40 INFO  JobScheduler:59 - Added jobs for time 1449085600000 ms
2015-12-02 14:46:42 INFO  MemoryStore:59 - ensureFreeSpace(50266) called with curMem=37539199, maxMem=278302556
2015-12-02 14:46:42 INFO  MemoryStore:59 - Block input-0-1449085602000 stored as bytes in memory (estimated size 49.1 KB, free 229.6 MB)
2015-12-02 14:46:42 INFO  BlockManagerInfo:59 - Added input-0-1449085602000 in memory on localhost:54856 (size: 49.1 KB, free: 231.2 MB)
2015-12-02 14:46:42 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085602000
2015-12-02 14:46:42 WARN  BlockManager:71 - Block input-0-1449085602000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:42 INFO  BlockGenerator:59 - Pushed block input-0-1449085602000
2015-12-02 14:46:50 INFO  JobScheduler:59 - Added jobs for time 1449085610000 ms
2015-12-02 14:47:00 INFO  JobScheduler:59 - Added jobs for time 1449085620000 ms
2015-12-02 14:47:10 INFO  JobScheduler:59 - Added jobs for time 1449085630000 ms
2015-12-02 14:47:20 INFO  JobScheduler:59 - Added jobs for time 1449085640000 ms
2015-12-02 14:47:27 INFO  MemoryStore:59 - ensureFreeSpace(4677) called with curMem=37589465, maxMem=278302556
2015-12-02 14:47:27 INFO  MemoryStore:59 - Block input-0-1449085647600 stored as bytes in memory (estimated size 4.6 KB, free 229.6 MB)
2015-12-02 14:47:27 INFO  BlockManagerInfo:59 - Added input-0-1449085647600 in memory on localhost:54856 (size: 4.6 KB, free: 231.1 MB)
2015-12-02 14:47:27 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085647600
2015-12-02 14:47:27 WARN  BlockManager:71 - Block input-0-1449085647600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:47:27 INFO  BlockGenerator:59 - Pushed block input-0-1449085647600
2015-12-02 14:47:30 INFO  JobScheduler:59 - Added jobs for time 1449085650000 ms
2015-12-02 14:47:36 INFO  MemoryStore:59 - ensureFreeSpace(5272) called with curMem=37594142, maxMem=278302556
2015-12-02 14:47:36 INFO  MemoryStore:59 - Block input-0-1449085656000 stored as bytes in memory (estimated size 5.1 KB, free 229.6 MB)
2015-12-02 14:47:36 INFO  BlockManagerInfo:59 - Added input-0-1449085656000 in memory on localhost:54856 (size: 5.1 KB, free: 231.1 MB)
2015-12-02 14:47:36 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085656000
2015-12-02 14:47:36 WARN  BlockManager:71 - Block input-0-1449085656000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:47:36 INFO  BlockGenerator:59 - Pushed block input-0-1449085656000
2015-12-02 14:47:40 INFO  JobScheduler:59 - Added jobs for time 1449085660000 ms
2015-12-02 14:47:50 INFO  JobScheduler:59 - Added jobs for time 1449085670000 ms
2015-12-02 14:48:00 INFO  JobScheduler:59 - Added jobs for time 1449085680000 ms
2015-12-02 14:48:05 INFO  MemoryStore:59 - ensureFreeSpace(50082) called with curMem=37599414, maxMem=278302556
2015-12-02 14:48:05 INFO  MemoryStore:59 - Block input-0-1449085685600 stored as bytes in memory (estimated size 48.9 KB, free 229.5 MB)
2015-12-02 14:48:05 INFO  BlockManagerInfo:59 - Added input-0-1449085685600 in memory on localhost:54856 (size: 48.9 KB, free: 231.1 MB)
2015-12-02 14:48:05 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085685600
2015-12-02 14:48:05 WARN  BlockManager:71 - Block input-0-1449085685600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:05 INFO  BlockGenerator:59 - Pushed block input-0-1449085685600
2015-12-02 14:48:06 INFO  MemoryStore:59 - ensureFreeSpace(5143) called with curMem=37649496, maxMem=278302556
2015-12-02 14:48:06 INFO  MemoryStore:59 - Block input-0-1449085686600 stored as bytes in memory (estimated size 5.0 KB, free 229.5 MB)
2015-12-02 14:48:06 INFO  BlockManagerInfo:59 - Added input-0-1449085686600 in memory on localhost:54856 (size: 5.0 KB, free: 231.1 MB)
2015-12-02 14:48:06 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085686600
2015-12-02 14:48:06 WARN  BlockManager:71 - Block input-0-1449085686600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:06 INFO  BlockGenerator:59 - Pushed block input-0-1449085686600
2015-12-02 14:48:10 INFO  JobScheduler:59 - Added jobs for time 1449085690000 ms
2015-12-02 14:48:20 INFO  JobScheduler:59 - Added jobs for time 1449085700000 ms
2015-12-02 14:48:30 INFO  JobScheduler:59 - Added jobs for time 1449085710000 ms
2015-12-02 14:48:40 INFO  JobScheduler:59 - Added jobs for time 1449085720000 ms
2015-12-02 14:48:41 INFO  MemoryStore:59 - ensureFreeSpace(66613) called with curMem=37654639, maxMem=278302556
2015-12-02 14:48:41 INFO  MemoryStore:59 - Block input-0-1449085721200 stored as bytes in memory (estimated size 65.1 KB, free 229.4 MB)
2015-12-02 14:48:41 INFO  BlockManagerInfo:59 - Added input-0-1449085721200 in memory on localhost:54856 (size: 65.1 KB, free: 231.0 MB)
2015-12-02 14:48:41 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085721200
2015-12-02 14:48:41 WARN  BlockManager:71 - Block input-0-1449085721200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:41 INFO  BlockGenerator:59 - Pushed block input-0-1449085721200
2015-12-02 14:48:46 INFO  MemoryStore:59 - ensureFreeSpace(3757) called with curMem=37721252, maxMem=278302556
2015-12-02 14:48:46 INFO  MemoryStore:59 - Block input-0-1449085726400 stored as bytes in memory (estimated size 3.7 KB, free 229.4 MB)
2015-12-02 14:48:46 INFO  BlockManagerInfo:59 - Added input-0-1449085726400 in memory on localhost:54856 (size: 3.7 KB, free: 231.0 MB)
2015-12-02 14:48:46 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085726400
2015-12-02 14:48:46 WARN  BlockManager:71 - Block input-0-1449085726400 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:46 INFO  BlockGenerator:59 - Pushed block input-0-1449085726400
2015-12-02 14:48:47 INFO  MemoryStore:59 - ensureFreeSpace(49499) called with curMem=37725009, maxMem=278302556
2015-12-02 14:48:47 INFO  MemoryStore:59 - Block input-0-1449085726800 stored as bytes in memory (estimated size 48.3 KB, free 229.4 MB)
2015-12-02 14:48:47 INFO  BlockManagerInfo:59 - Added input-0-1449085726800 in memory on localhost:54856 (size: 48.3 KB, free: 231.0 MB)
2015-12-02 14:48:47 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085726800
2015-12-02 14:48:47 WARN  BlockManager:71 - Block input-0-1449085726800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:47 INFO  BlockGenerator:59 - Pushed block input-0-1449085726800
2015-12-02 14:48:50 INFO  JobScheduler:59 - Added jobs for time 1449085730000 ms
2015-12-02 14:49:00 INFO  JobScheduler:59 - Added jobs for time 1449085740000 ms
2015-12-02 14:49:05 INFO  MemoryStore:59 - ensureFreeSpace(2311) called with curMem=37774508, maxMem=278302556
2015-12-02 14:49:05 INFO  MemoryStore:59 - Block input-0-1449085745000 stored as bytes in memory (estimated size 2.3 KB, free 229.4 MB)
2015-12-02 14:49:05 INFO  BlockManagerInfo:59 - Added input-0-1449085745000 in memory on localhost:54856 (size: 2.3 KB, free: 231.0 MB)
2015-12-02 14:49:05 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085745000
2015-12-02 14:49:05 WARN  BlockManager:71 - Block input-0-1449085745000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:49:05 INFO  BlockGenerator:59 - Pushed block input-0-1449085745000
2015-12-02 14:49:10 INFO  MemoryStore:59 - ensureFreeSpace(7455) called with curMem=37776819, maxMem=278302556
2015-12-02 14:49:10 INFO  MemoryStore:59 - Block input-0-1449085749800 stored as bytes in memory (estimated size 7.3 KB, free 229.4 MB)
2015-12-02 14:49:10 INFO  JobScheduler:59 - Added jobs for time 1449085750000 ms
2015-12-02 14:49:10 INFO  BlockManagerInfo:59 - Added input-0-1449085749800 in memory on localhost:54856 (size: 7.3 KB, free: 231.0 MB)
2015-12-02 14:49:10 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085749800
2015-12-02 14:49:10 WARN  BlockManager:71 - Block input-0-1449085749800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:49:10 INFO  BlockGenerator:59 - Pushed block input-0-1449085749800
2015-12-02 14:49:20 INFO  JobScheduler:59 - Added jobs for time 1449085760000 ms
2015-12-02 14:49:30 INFO  JobScheduler:59 - Added jobs for time 1449085770000 ms
2015-12-02 14:49:40 INFO  JobScheduler:59 - Added jobs for time 1449085780000 ms
2015-12-02 14:49:50 INFO  JobScheduler:59 - Added jobs for time 1449085790000 ms
2015-12-02 14:50:00 INFO  JobScheduler:59 - Added jobs for time 1449085800000 ms
2015-12-02 14:50:03 INFO  MemoryStore:59 - ensureFreeSpace(50011) called with curMem=37784274, maxMem=278302556
2015-12-02 14:50:03 INFO  MemoryStore:59 - Block input-0-1449085803200 stored as bytes in memory (estimated size 48.8 KB, free 229.3 MB)
2015-12-02 14:50:03 INFO  BlockManagerInfo:59 - Added input-0-1449085803200 in memory on localhost:54856 (size: 48.8 KB, free: 230.9 MB)
2015-12-02 14:50:03 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085803200
2015-12-02 14:50:03 WARN  BlockManager:71 - Block input-0-1449085803200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:50:03 INFO  BlockGenerator:59 - Pushed block input-0-1449085803200
2015-12-02 14:50:10 INFO  JobScheduler:59 - Added jobs for time 1449085810000 ms
2015-12-02 14:50:20 INFO  JobScheduler:59 - Added jobs for time 1449085820000 ms
2015-12-02 14:50:30 INFO  JobScheduler:59 - Added jobs for time 1449085830000 ms
2015-12-02 14:50:40 INFO  JobScheduler:59 - Added jobs for time 1449085840000 ms
2015-12-02 14:50:50 INFO  JobScheduler:59 - Added jobs for time 1449085850000 ms
2015-12-02 14:51:00 INFO  JobScheduler:59 - Added jobs for time 1449085860000 ms
2015-12-02 14:51:10 INFO  JobScheduler:59 - Added jobs for time 1449085870000 ms
2015-12-02 14:51:20 INFO  JobScheduler:59 - Added jobs for time 1449085880000 ms
2015-12-02 14:51:30 INFO  JobScheduler:59 - Added jobs for time 1449085890000 ms
2015-12-02 14:51:40 INFO  JobScheduler:59 - Added jobs for time 1449085900000 ms
2015-12-02 14:51:50 INFO  JobScheduler:59 - Added jobs for time 1449085910000 ms
2015-12-02 14:51:53 INFO  MemoryStore:59 - ensureFreeSpace(3938) called with curMem=37834285, maxMem=278302556
2015-12-02 14:51:53 INFO  MemoryStore:59 - Block input-0-1449085912800 stored as bytes in memory (estimated size 3.8 KB, free 229.3 MB)
2015-12-02 14:51:53 INFO  BlockManagerInfo:59 - Added input-0-1449085912800 in memory on localhost:54856 (size: 3.8 KB, free: 230.9 MB)
2015-12-02 14:51:53 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085912800
2015-12-02 14:51:53 WARN  BlockManager:71 - Block input-0-1449085912800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:51:53 INFO  BlockGenerator:59 - Pushed block input-0-1449085912800
2015-12-02 14:52:00 INFO  JobScheduler:59 - Added jobs for time 1449085920000 ms
2015-12-02 14:52:08 INFO  MemoryStore:59 - ensureFreeSpace(153803) called with curMem=37838223, maxMem=278302556
2015-12-02 14:52:08 INFO  MemoryStore:59 - Block input-0-1449085928600 stored as bytes in memory (estimated size 150.2 KB, free 229.2 MB)
2015-12-02 14:52:08 INFO  BlockManagerInfo:59 - Added input-0-1449085928600 in memory on localhost:54856 (size: 150.2 KB, free: 230.8 MB)
2015-12-02 14:52:08 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085928600
2015-12-02 14:52:08 WARN  BlockManager:71 - Block input-0-1449085928600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:52:08 INFO  BlockGenerator:59 - Pushed block input-0-1449085928600
2015-12-02 14:52:10 INFO  JobScheduler:59 - Added jobs for time 1449085930000 ms
2015-12-02 14:52:20 INFO  JobScheduler:59 - Added jobs for time 1449085940000 ms
2015-12-02 14:52:21 INFO  MemoryStore:59 - ensureFreeSpace(39225) called with curMem=37992026, maxMem=278302556
2015-12-02 14:52:21 INFO  MemoryStore:59 - Block input-0-1449085941000 stored as bytes in memory (estimated size 38.3 KB, free 229.1 MB)
2015-12-02 14:52:21 INFO  BlockManagerInfo:59 - Added input-0-1449085941000 in memory on localhost:54856 (size: 38.3 KB, free: 230.7 MB)
2015-12-02 14:52:21 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085941000
2015-12-02 14:52:21 WARN  BlockManager:71 - Block input-0-1449085941000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:52:21 INFO  BlockGenerator:59 - Pushed block input-0-1449085941000
2015-12-02 14:52:25 INFO  MemoryStore:59 - ensureFreeSpace(39106) called with curMem=38031251, maxMem=278302556
2015-12-02 14:52:25 INFO  MemoryStore:59 - Block input-0-1449085945600 stored as bytes in memory (estimated size 38.2 KB, free 229.1 MB)
2015-12-02 14:52:25 INFO  BlockManagerInfo:59 - Added input-0-1449085945600 in memory on localhost:54856 (size: 38.2 KB, free: 230.7 MB)
2015-12-02 14:52:25 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085945600
2015-12-02 14:52:25 WARN  BlockManager:71 - Block input-0-1449085945600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:52:25 INFO  BlockGenerator:59 - Pushed block input-0-1449085945600
2015-12-02 14:52:30 INFO  JobScheduler:59 - Added jobs for time 1449085950000 ms
2015-12-02 14:52:40 INFO  JobScheduler:59 - Added jobs for time 1449085960000 ms
2015-12-02 14:52:50 INFO  JobScheduler:59 - Added jobs for time 1449085970000 ms
2015-12-02 14:53:00 INFO  JobScheduler:59 - Added jobs for time 1449085980000 ms
2015-12-02 14:53:10 INFO  JobScheduler:59 - Added jobs for time 1449085990000 ms
2015-12-02 14:53:20 INFO  JobScheduler:59 - Added jobs for time 1449086000000 ms
2015-12-02 14:53:30 INFO  JobScheduler:59 - Added jobs for time 1449086010000 ms
2015-12-02 14:53:35 INFO  MemoryStore:59 - ensureFreeSpace(880737) called with curMem=38070357, maxMem=278302556
2015-12-02 14:53:35 INFO  MemoryStore:59 - Block input-0-1449086015200 stored as bytes in memory (estimated size 860.1 KB, free 228.3 MB)
2015-12-02 14:53:35 INFO  BlockManagerInfo:59 - Added input-0-1449086015200 in memory on localhost:54856 (size: 860.1 KB, free: 229.9 MB)
2015-12-02 14:53:35 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086015200
2015-12-02 14:53:35 WARN  BlockManager:71 - Block input-0-1449086015200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:53:35 INFO  BlockGenerator:59 - Pushed block input-0-1449086015200
2015-12-02 14:53:40 INFO  JobScheduler:59 - Added jobs for time 1449086020000 ms
2015-12-02 14:53:50 INFO  JobScheduler:59 - Added jobs for time 1449086030000 ms
2015-12-02 14:54:00 INFO  JobScheduler:59 - Added jobs for time 1449086040000 ms
2015-12-02 14:54:10 INFO  JobScheduler:59 - Added jobs for time 1449086050000 ms
2015-12-02 14:54:20 INFO  JobScheduler:59 - Added jobs for time 1449086060000 ms
2015-12-02 14:54:30 INFO  JobScheduler:59 - Added jobs for time 1449086070000 ms
2015-12-02 14:54:40 INFO  JobScheduler:59 - Added jobs for time 1449086080000 ms
2015-12-02 14:54:47 INFO  MemoryStore:59 - ensureFreeSpace(994936) called with curMem=38951094, maxMem=278302556
2015-12-02 14:54:47 INFO  MemoryStore:59 - Block input-0-1449086087400 stored as bytes in memory (estimated size 971.6 KB, free 227.3 MB)
2015-12-02 14:54:47 INFO  BlockManagerInfo:59 - Added input-0-1449086087400 in memory on localhost:54856 (size: 971.6 KB, free: 228.9 MB)
2015-12-02 14:54:47 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086087400
2015-12-02 14:54:47 WARN  BlockManager:71 - Block input-0-1449086087400 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:54:47 INFO  BlockGenerator:59 - Pushed block input-0-1449086087400
2015-12-02 14:54:50 INFO  JobScheduler:59 - Added jobs for time 1449086090000 ms
2015-12-02 14:54:56 INFO  MemoryStore:59 - ensureFreeSpace(27364) called with curMem=39946030, maxMem=278302556
2015-12-02 14:54:56 INFO  MemoryStore:59 - Block input-0-1449086096600 stored as bytes in memory (estimated size 26.7 KB, free 227.3 MB)
2015-12-02 14:54:56 INFO  BlockManagerInfo:59 - Added input-0-1449086096600 in memory on localhost:54856 (size: 26.7 KB, free: 228.9 MB)
2015-12-02 14:54:56 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086096600
2015-12-02 14:54:56 WARN  BlockManager:71 - Block input-0-1449086096600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:54:56 INFO  BlockGenerator:59 - Pushed block input-0-1449086096600
2015-12-02 14:55:00 INFO  JobScheduler:59 - Added jobs for time 1449086100000 ms
2015-12-02 14:55:10 INFO  JobScheduler:59 - Added jobs for time 1449086110000 ms
2015-12-02 14:55:20 INFO  JobScheduler:59 - Added jobs for time 1449086120000 ms
2015-12-02 14:55:29 INFO  MemoryStore:59 - ensureFreeSpace(1862) called with curMem=39973394, maxMem=278302556
2015-12-02 14:55:29 INFO  MemoryStore:59 - Block input-0-1449086129200 stored as bytes in memory (estimated size 1862.0 B, free 227.3 MB)
2015-12-02 14:55:29 INFO  BlockManagerInfo:59 - Added input-0-1449086129200 in memory on localhost:54856 (size: 1862.0 B, free: 228.9 MB)
2015-12-02 14:55:29 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086129200
2015-12-02 14:55:29 WARN  BlockManager:71 - Block input-0-1449086129200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:55:29 INFO  BlockGenerator:59 - Pushed block input-0-1449086129200
2015-12-02 14:55:30 INFO  JobScheduler:59 - Added jobs for time 1449086130000 ms
2015-12-02 14:55:34 INFO  MemoryStore:59 - ensureFreeSpace(304679) called with curMem=39975256, maxMem=278302556
2015-12-02 14:55:34 INFO  MemoryStore:59 - Block input-0-1449086133800 stored as bytes in memory (estimated size 297.5 KB, free 227.0 MB)
2015-12-02 14:55:34 INFO  BlockManagerInfo:59 - Added input-0-1449086133800 in memory on localhost:54856 (size: 297.5 KB, free: 228.6 MB)
2015-12-02 14:55:34 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086133800
2015-12-02 14:55:34 WARN  BlockManager:71 - Block input-0-1449086133800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:55:34 INFO  BlockGenerator:59 - Pushed block input-0-1449086133800
2015-12-02 14:55:40 INFO  JobScheduler:59 - Added jobs for time 1449086140000 ms
2015-12-02 14:55:50 INFO  JobScheduler:59 - Added jobs for time 1449086150000 ms
2015-12-02 14:55:55 WARN  HttpUtil:187 - SocketTimeout-related exception occured. See antecedent stacktrace
2015-12-02 14:55:55 INFO  HttpUtil:213 - interrupted in delay()
java.lang.InterruptedException: sleep interrupted
                at java.lang.Thread.sleep(Native Method)
                at com.markmonitor.antifraud.ce.util.HttpUtil.delay(HttpUtil.java:211)
                at com.markmonitor.antifraud.ce.util.HttpUtil.getHttpGetResponse(HttpUtil.java:188)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:18)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:9)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)
2015-12-02 14:55:56 INFO  MemoryStore:59 - ensureFreeSpace(10975) called with curMem=40279935, maxMem=278302556
2015-12-02 14:55:56 INFO  MemoryStore:59 - Block input-0-1449086155800 stored as bytes in memory (estimated size 10.7 KB, free 227.0 MB)
2015-12-02 14:55:56 INFO  BlockManagerInfo:59 - Added input-0-1449086155800 in memory on localhost:54856 (size: 10.7 KB, free: 228.6 MB)
2015-12-02 14:55:56 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086155800
2015-12-02 14:55:56 WARN  BlockManager:71 - Block input-0-1449086155800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:55:56 INFO  BlockGenerator:59 - Pushed block input-0-1449086155800
2015-12-02 14:56:00 INFO  JobScheduler:59 - Added jobs for time 1449086160000 ms
2015-12-02 14:56:10 INFO  JobScheduler:59 - Added jobs for time 1449086170000 ms
2015-12-02 14:56:20 INFO  JobScheduler:59 - Added jobs for time 1449086180000 ms
2015-12-02 14:56:30 INFO  JobScheduler:59 - Added jobs for time 1449086190000 ms
2015-12-02 14:56:31 INFO  MemoryStore:59 - ensureFreeSpace(18836) called with curMem=40290910, maxMem=278302556
2015-12-02 14:56:31 INFO  MemoryStore:59 - Block input-0-1449086190800 stored as bytes in memory (estimated size 18.4 KB, free 227.0 MB)
2015-12-02 14:56:31 INFO  BlockManagerInfo:59 - Added input-0-1449086190800 in memory on localhost:54856 (size: 18.4 KB, free: 228.6 MB)
2015-12-02 14:56:31 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086190800
2015-12-02 14:56:31 WARN  BlockManager:71 - Block input-0-1449086190800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:56:31 INFO  BlockGenerator:59 - Pushed block input-0-1449086190800
2015-12-02 14:56:40 INFO  JobScheduler:59 - Added jobs for time 1449086200000 ms
2015-12-02 14:56:50 INFO  JobScheduler:59 - Added jobs for time 1449086210000 ms
2015-12-02 14:57:00 INFO  JobScheduler:59 - Added jobs for time 1449086220000 ms
2015-12-02 14:57:10 INFO  JobScheduler:59 - Added jobs for time 1449086230000 ms
2015-12-02 14:57:13 INFO  MemoryStore:59 - ensureFreeSpace(1690) called with curMem=40309746, maxMem=278302556
2015-12-02 14:57:13 INFO  MemoryStore:59 - Block input-0-1449086232800 stored as bytes in memory (estimated size 1690.0 B, free 227.0 MB)
2015-12-02 14:57:13 INFO  BlockManagerInfo:59 - Added input-0-1449086232800 in memory on localhost:54856 (size: 1690.0 B, free: 228.6 MB)
2015-12-02 14:57:13 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086232800
2015-12-02 14:57:13 WARN  BlockManager:71 - Block input-0-1449086232800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:57:13 INFO  BlockGenerator:59 - Pushed block input-0-1449086232800
2015-12-02 14:57:20 INFO  JobScheduler:59 - Added jobs for time 1449086240000 ms
2015-12-02 14:57:25 WARN  HttpUtil:187 - SocketTimeout-related exception occured. See antecedent stacktrace
2015-12-02 14:57:25 INFO  HttpUtil:213 - interrupted in delay()
java.lang.InterruptedException: sleep interrupted
                at java.lang.Thread.sleep(Native Method)
                at com.markmonitor.antifraud.ce.util.HttpUtil.delay(HttpUtil.java:211)
                at com.markmonitor.antifraud.ce.util.HttpUtil.getHttpGetResponse(HttpUtil.java:188)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:18)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:9)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)
2015-12-02 14:57:29 INFO  MemoryStore:59 - ensureFreeSpace(35769) called with curMem=40311436, maxMem=278302556
2015-12-02 14:57:29 INFO  MemoryStore:59 - Block input-0-1449086249200 stored as bytes in memory (estimated size 34.9 KB, free 226.9 MB)
2015-12-02 14:57:29 INFO  BlockManagerInfo:59 - Added input-0-1449086249200 in memory on localhost:54856 (size: 34.9 KB, free: 228.5 MB)
2015-12-02 14:57:29 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086249200
2015-12-02 14:57:29 WARN  BlockManager:71 - Block input-0-1449086249200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:57:29 INFO  BlockGenerator:59 - Pushed block input-0-1449086249200
2015-12-02 14:57:30 INFO  JobScheduler:59 - Added jobs for time 1449086250000 ms
2015-12-02 14:57:31 INFO  MemoryStore:59 - ensureFreeSpace(35771) called with curMem=40347205, maxMem=278302556
2015-12-02 14:57:31 INFO  MemoryStore:59 - Block input-0-1449086251400 stored as bytes in memory (estimated size 34.9 KB, free 226.9 MB)
2015-12-02 14:57:31 INFO  BlockManagerInfo:59 - Added input-0-1449086251400 in memory on localhost:54856 (size: 34.9 KB, free: 228.5 MB)
2015-12-02 14:57:31 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086251400
2015-12-02 14:57:31 WARN  BlockManager:71 - Block input-0-1449086251400 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:57:31 INFO  BlockGenerator:59 - Pushed block input-0-1449086251400
2015-12-02 14:57:40 INFO  JobScheduler:59 - Added jobs for time 1449086260000 ms
2015-12-02 14:57:50 INFO  JobScheduler:59 - Added jobs for time 1449086270000 ms
2015-12-02 14:58:00 INFO  JobScheduler:59 - Added jobs for time 1449086280000 ms
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing broadcast 8090
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8090
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8090 of size 6576 dropped from memory (free 237926156)
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8090_piece0
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8090_piece0 of size 2219 dropped from memory (free 237928375)
2015-12-02 14:58:03 INFO  BlockManagerInfo:59 - Removed broadcast_8090_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 228.5 MB)
2015-12-02 14:58:03 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8090_piece0
2015-12-02 14:58:03 INFO  ContextCleaner:59 - Cleaned broadcast 8090
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing broadcast 8089
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8089
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8089 of size 74136 dropped from memory (free 238002511)
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8089_piece0
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8089_piece0 of size 25655 dropped from memory (free 238028166)
2015-12-02 14:58:03 INFO  BlockManagerInfo:59 - Removed broadcast_8089_piece0 on localhost:54856 in memory (size: 25.1 KB, free: 228.5 MB)
2015-12-02 14:58:03 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8089_piece0
2015-12-02 14:58:03 INFO  ContextCleaner:59 - Cleaned broadcast 8089
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing broadcast 8086
2015-12-02"
"
Sean Owen <sowen@cloudera.com>,Thu"," 3 Dec 2015 21:57:37 +0000""",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Michael Armbrust <michael@databricks.com>,"Licenses and signature are all fine.

Docker integration tests consistently fail for me with Java 7 / Ubuntu
and ""-Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver""

*** RUN ABORTED ***
  java.lang.NoSuchMethodError:
org.apache.http.impl.client.HttpClientBuilder.setConnectionManagerShared(Z)Lorg/apache/http/impl/client/HttpClientBuilder;
  at org.glassfish.jersey.apache.connector.ApacheConnector.<init>(ApacheConnector.java:240)
  at org.glassfish.jersey.apache.connector.ApacheConnectorProvider.getConnector(ApacheConnectorProvider.java:115)
  at org.glassfish.jersey.client.ClientConfig$State.initRuntime(ClientConfig.java:418)
  at org.glassfish.jersey.client.ClientConfig$State.access$000(ClientConfig.java:88)
  at org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:120)
  at org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:117)
  at org.glassfish.jersey.internal.util.collection.Values$LazyValueImpl.get(Values.java:340)
  at org.glassfish.jersey.client.ClientConfig.getRuntime(ClientConfig.java:726)
  at org.glassfish.jersey.client.ClientRequest.getConfiguration(ClientRequest.java:285)
  at org.glassfish.jersey.client.JerseyInvocation.validateHttpMethodAndEntity(JerseyInvocation.java:126)

I also get this failure consistently:

DirectKafkaStreamSuite
- offset recovery *** FAILED ***
  recoveredOffsetRanges.forall(((or: (org.apache.spark.streaming.Time,
Array[org.apache.spark.streaming.kafka.OffsetRange])) =>
earlierOffsetRangesAsSets.contains(scala.Tuple2.apply[org.apache.spark.streaming.Time,
scala.collection.immutable.Set[org.apache.spark.streaming.kafka.OffsetRange]](or._1,
scala.this.Predef.refArrayOps[org.apache.spark.streaming.kafka.OffsetRange](or._2).toSet[org.apache.spark.streaming.kafka.OffsetRange]))))
was false Recovered ranges are not the same as the ones generated
(DirectKafkaStreamSuite.scala:301)


---------------------------------------------------------------------


"
Rad Gruchalski <radek@gruchalski.com>,"Thu, 3 Dec 2015 23:59:35 +0100",Re: A proposal for Spark 2.0,Koert Kuipers <koert@tresata.com>,"There was a talk in this thread about removing the fine-grained Mesos scheduler. I think it would a loss to lose it completely, however, I understand that it might be a burden to keep it under development for Mesos only.  
Having been thinking about it for a while, it would be great if the schedulers were pluggable. If Spark 2 could offer a way of registering a scheduling mechanism then the Mesos fine-grained scheduler could be moved to a separate project and, possibly, maintained by a separate community.
This would also enable people to add more schedulers in the future - Kubernetes comes into mind but also Docker Swarm would become an option. This would allow growing the ecosystem a bit.

I‚Äôd be very interested in working on such a feature.










Kind regards,‚Ä®
Radek Gruchalski
‚Ä®radek@gruchalski.com (mailto:radek@gruchalski.com)‚Ä® (mailto:radek@gruchalski.com)
de.linkedin.com/in/radgruchalski/ (http://de.linkedin.com/in/radgruchalski/)

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.




 to me you already provide a clear upgrade path: get on scala 2.11 before upgrading to spark 2.x
ala 2.11.x, as the 2.10.x series is no longer actively maintained.
der in terms of operations for people to upgrade.
ersion and retarget things to 2.0 on JIRA.
I
ut

to

d. I'd
which
ion.
that way,
pdate
porting
 the
o
ough
ate
ndeed
a
ased.
nues
xing.
ng
 2.x
harder in
7 version
era.com (mailto:sandy.ryza@cloudera.com)>
ctant to
ed to stay on
d to port their
pport for
in 2.0 that
ases in the
gmail.com (mailto:matei.zaharia@gmail.com)>
 we're not
ainst Scala
eople would
 it's better
o update once
ed API
 users, and
are still on
 probably want
API so they
 other ways
ch more
o backport
 API in the
ks.com (mailto:rxin@databricks.com)>
. The
he
DataFrame.map
her break this
damage is
l or not
tal in 2.0 and
l"", there has
orydata.com (mailto:mark@clearstorydata.com)>
ot just bug
loudera.com (mailto:kostas@cloudera.com)>
nge APIs in
 be changing the
 important to
fore moving to
he new Dataset
ncompatible
 release
te:
re - yes we
park 2.0. I'd like to
s will allow us to
6:
ransition
upgrade given what
 having a 1.x release
ficial. This might
ily a bad thing.
intel.com (mailto:hao.cheng@intel.com)>
in DF/DS.
have to
h, like, the
n this category, as we
erformance.
]
g)
ng that
ing presented to new
Frames.... Until the
 you the best
ircumstances, ...,
@gmail.com (mailto:javadba@gmail.com)>
support for
ation at scale.
S  it would seem
dev.
ource
xpressions.  AFAIK
applied to the
 in the native RDD
rydata.com (mailto:mark@clearstorydata.com)>:
een
recate it, but
The RDD API came
ides, introductory
ended to emphasize
hinking is that with
-emphasize and
ets would be
be presented as the
, in contrast, would
he-metal API that
 DataFrames or
intel.com (mailto:hao.cheng@intel.com)>
roblem, but
t is a painful issue for
DD API (or
erlapping with DataFrame

itgo@qq.com); dev@spark.apache.org (mailto:dev@spark.apache.org);
 I'm hoping
 isolation with user
or}.userClassPathFirst,
sitive dependencies
 can have a whitelist
ps to explicitly
Hadoop 3 is also
ove useful
ent structure of two
confusing.
he use of
raphX evolve with
eprecate
mediately. That way
want to deprecate to be
h the goal
current API and remove
ld be happy to have that
ove useful
ent structure of two
confusing.
he use of
raphX evolve with
g)
agreement
of a component of
me features
r.
:32ÔºåMatei Zaharia <matei.zaharia@gmail.com (mailto:matei.zaharia@gmail.com)> ÂÜôÈÅìÔºö
mponent too
might even be useful
ges to be worth
entually,
t can be added later
n there? I imagine
uff.
ajor""
d proposed. Keeping
important.
tabricks.com (mailto:rxin@databricks.com)>
ry
res. The
gs that are
xamples
 also be a
a new one,
er, unless
 in 1.x to
d big.
ort Scala
ion, 2.11
. I'd
.
d 2.1 were
- shading a
 light of
 etc?
e proposed
 another repo (?
-------------
rg (mailto:dev-unsubscribe@spark.apache.org)
he.org (mailto:dev-help@spark.apache.org)
-------------
rg (mailto:dev-unsubscribe@spark.apache.org)
he.org (mailto:dev-help@spark.apache.org)
-------------
rg (mailto:dev-unsubscribe@spark.apache.org)
he.org (mailto:dev-help@spark.apache.org)
--
dev-unsubscribe@spark.apache.org)
lto:dev-help@spark.apache.org)

"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Thu, 3 Dec 2015 23:22:33 +0000",RE: SparkStreaming is failing to process Kafka jobs under load....,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

I am running spark streaming application in local mode.  The application is getting data from a Kafka queue.  Everything seems to work fine for a few days then I see following message in the log but Spark is not processing any event.  I am running in local mode but number of core is set to 8 and the flow works fine for a few days then I am getting this error.

Any help is greatly appreciated...

2015-12-02 14:45:16 INFO  PhishTank:30 - &&&&&  PhishtankCheck processing key PhishTank on url update-account.log.cgi.cust.mpp.singin.ssl.cussusmpp.com/update/logcheck.php got score 0.0
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8094
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8094
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8094 of size 6576 dropped from memory (free 240797255)
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8094_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8094_piece0 of size 2219 dropped from memory (free 240799474)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8094_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8094_piece0
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8094
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8093
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8093
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8093 of size 74136 dropped from memory (free 240873610)
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8093_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8093_piece0 of size 25655 dropped from memory (free 240899265)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8093_piece0 on localhost:54856 in memory (size: 25.1 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8093_piece0
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8093
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8092
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8092_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8092_piece0 of size 2235 dropped from memory (free 240901500)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8092_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8092_piece0
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8092
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8092 of size 6600 dropped from memory (free 240908100)
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8092
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing broadcast 8091
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8091_piece0
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8091_piece0 of size 2235 dropped from memory (free 240910335)
2015-12-02 14:45:17 INFO  BlockManagerInfo:59 - Removed broadcast_8091_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 231.3 MB)
2015-12-02 14:45:17 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8091_piece0
2015-12-02 14:45:17 INFO  BlockManager:59 - Removing block broadcast_8091
2015-12-02 14:45:17 INFO  MemoryStore:59 - Block broadcast_8091 of size 6600 dropped from memory (free 240916935)
2015-12-02 14:45:17 INFO  ContextCleaner:59 - Cleaned broadcast 8091
2015-12-02 14:45:20 INFO  JobScheduler:59 - Added jobs for time 1449085520000 ms
2015-12-02 14:45:30 INFO  JobScheduler:59 - Added jobs for time 1449085530000 ms
2015-12-02 14:45:40 INFO  JobScheduler:59 - Added jobs for time 1449085540000 ms
2015-12-02 14:45:50 INFO  JobScheduler:59 - Added jobs for time 1449085550000 ms
2015-12-02 14:46:00 INFO  JobScheduler:59 - Added jobs for time 1449085560000 ms
2015-12-02 14:46:09 INFO  MemoryStore:59 - ensureFreeSpace(3831) called with curMem=37385621, maxMem=278302556
2015-12-02 14:46:09 INFO  MemoryStore:59 - Block input-0-1449085569600 stored as bytes in memory (estimated size 3.7 KB, free 229.8 MB)
2015-12-02 14:46:09 INFO  BlockManagerInfo:59 - Added input-0-1449085569600 in memory on localhost:54856 (size: 3.7 KB, free: 231.3 MB)
2015-12-02 14:46:09 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085569600
2015-12-02 14:46:09 WARN  BlockManager:71 - Block input-0-1449085569600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:09 INFO  BlockGenerator:59 - Pushed block input-0-1449085569600
2015-12-02 14:46:10 INFO  JobScheduler:59 - Added jobs for time 1449085570000 ms
2015-12-02 14:46:20 INFO  JobScheduler:59 - Added jobs for time 1449085580000 ms
2015-12-02 14:46:30 INFO  JobScheduler:59 - Added jobs for time 1449085590000 ms
2015-12-02 14:46:30 INFO  MemoryStore:59 - ensureFreeSpace(23388) called with curMem=37389452, maxMem=278302556
2015-12-02 14:46:30 INFO  MemoryStore:59 - Block input-0-1449085590200 stored as bytes in memory (estimated size 22.8 KB, free 229.7 MB)
2015-12-02 14:46:30 INFO  BlockManagerInfo:59 - Added input-0-1449085590200 in memory on localhost:54856 (size: 22.8 KB, free: 231.3 MB)
2015-12-02 14:46:30 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085590200
2015-12-02 14:46:30 WARN  BlockManager:71 - Block input-0-1449085590200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:30 INFO  BlockGenerator:59 - Pushed block input-0-1449085590200
2015-12-02 14:46:38 INFO  MemoryStore:59 - ensureFreeSpace(126359) called with curMem=37412840, maxMem=278302556
2015-12-02 14:46:38 INFO  MemoryStore:59 - Block input-0-1449085598600 stored as bytes in memory (estimated size 123.4 KB, free 229.6 MB)
2015-12-02 14:46:38 INFO  BlockManagerInfo:59 - Added input-0-1449085598600 in memory on localhost:54856 (size: 123.4 KB, free: 231.2 MB)
2015-12-02 14:46:38 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085598600
2015-12-02 14:46:38 WARN  BlockManager:71 - Block input-0-1449085598600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:38 INFO  BlockGenerator:59 - Pushed block input-0-1449085598600
2015-12-02 14:46:40 INFO  JobScheduler:59 - Added jobs for time 1449085600000 ms
2015-12-02 14:46:42 INFO  MemoryStore:59 - ensureFreeSpace(50266) called with curMem=37539199, maxMem=278302556
2015-12-02 14:46:42 INFO  MemoryStore:59 - Block input-0-1449085602000 stored as bytes in memory (estimated size 49.1 KB, free 229.6 MB)
2015-12-02 14:46:42 INFO  BlockManagerInfo:59 - Added input-0-1449085602000 in memory on localhost:54856 (size: 49.1 KB, free: 231.2 MB)
2015-12-02 14:46:42 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085602000
2015-12-02 14:46:42 WARN  BlockManager:71 - Block input-0-1449085602000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:46:42 INFO  BlockGenerator:59 - Pushed block input-0-1449085602000
2015-12-02 14:46:50 INFO  JobScheduler:59 - Added jobs for time 1449085610000 ms
2015-12-02 14:47:00 INFO  JobScheduler:59 - Added jobs for time 1449085620000 ms
2015-12-02 14:47:10 INFO  JobScheduler:59 - Added jobs for time 1449085630000 ms
2015-12-02 14:47:20 INFO  JobScheduler:59 - Added jobs for time 1449085640000 ms
2015-12-02 14:47:27 INFO  MemoryStore:59 - ensureFreeSpace(4677) called with curMem=37589465, maxMem=278302556
2015-12-02 14:47:27 INFO  MemoryStore:59 - Block input-0-1449085647600 stored as bytes in memory (estimated size 4.6 KB, free 229.6 MB)
2015-12-02 14:47:27 INFO  BlockManagerInfo:59 - Added input-0-1449085647600 in memory on localhost:54856 (size: 4.6 KB, free: 231.1 MB)
2015-12-02 14:47:27 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085647600
2015-12-02 14:47:27 WARN  BlockManager:71 - Block input-0-1449085647600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:47:27 INFO  BlockGenerator:59 - Pushed block input-0-1449085647600
2015-12-02 14:47:30 INFO  JobScheduler:59 - Added jobs for time 1449085650000 ms
2015-12-02 14:47:36 INFO  MemoryStore:59 - ensureFreeSpace(5272) called with curMem=37594142, maxMem=278302556
2015-12-02 14:47:36 INFO  MemoryStore:59 - Block input-0-1449085656000 stored as bytes in memory (estimated size 5.1 KB, free 229.6 MB)
2015-12-02 14:47:36 INFO  BlockManagerInfo:59 - Added input-0-1449085656000 in memory on localhost:54856 (size: 5.1 KB, free: 231.1 MB)
2015-12-02 14:47:36 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085656000
2015-12-02 14:47:36 WARN  BlockManager:71 - Block input-0-1449085656000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:47:36 INFO  BlockGenerator:59 - Pushed block input-0-1449085656000
2015-12-02 14:47:40 INFO  JobScheduler:59 - Added jobs for time 1449085660000 ms
2015-12-02 14:47:50 INFO  JobScheduler:59 - Added jobs for time 1449085670000 ms
2015-12-02 14:48:00 INFO  JobScheduler:59 - Added jobs for time 1449085680000 ms
2015-12-02 14:48:05 INFO  MemoryStore:59 - ensureFreeSpace(50082) called with curMem=37599414, maxMem=278302556
2015-12-02 14:48:05 INFO  MemoryStore:59 - Block input-0-1449085685600 stored as bytes in memory (estimated size 48.9 KB, free 229.5 MB)
2015-12-02 14:48:05 INFO  BlockManagerInfo:59 - Added input-0-1449085685600 in memory on localhost:54856 (size: 48.9 KB, free: 231.1 MB)
2015-12-02 14:48:05 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085685600
2015-12-02 14:48:05 WARN  BlockManager:71 - Block input-0-1449085685600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:05 INFO  BlockGenerator:59 - Pushed block input-0-1449085685600
2015-12-02 14:48:06 INFO  MemoryStore:59 - ensureFreeSpace(5143) called with curMem=37649496, maxMem=278302556
2015-12-02 14:48:06 INFO  MemoryStore:59 - Block input-0-1449085686600 stored as bytes in memory (estimated size 5.0 KB, free 229.5 MB)
2015-12-02 14:48:06 INFO  BlockManagerInfo:59 - Added input-0-1449085686600 in memory on localhost:54856 (size: 5.0 KB, free: 231.1 MB)
2015-12-02 14:48:06 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085686600
2015-12-02 14:48:06 WARN  BlockManager:71 - Block input-0-1449085686600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:06 INFO  BlockGenerator:59 - Pushed block input-0-1449085686600
2015-12-02 14:48:10 INFO  JobScheduler:59 - Added jobs for time 1449085690000 ms
2015-12-02 14:48:20 INFO  JobScheduler:59 - Added jobs for time 1449085700000 ms
2015-12-02 14:48:30 INFO  JobScheduler:59 - Added jobs for time 1449085710000 ms
2015-12-02 14:48:40 INFO  JobScheduler:59 - Added jobs for time 1449085720000 ms
2015-12-02 14:48:41 INFO  MemoryStore:59 - ensureFreeSpace(66613) called with curMem=37654639, maxMem=278302556
2015-12-02 14:48:41 INFO  MemoryStore:59 - Block input-0-1449085721200 stored as bytes in memory (estimated size 65.1 KB, free 229.4 MB)
2015-12-02 14:48:41 INFO  BlockManagerInfo:59 - Added input-0-1449085721200 in memory on localhost:54856 (size: 65.1 KB, free: 231.0 MB)
2015-12-02 14:48:41 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085721200
2015-12-02 14:48:41 WARN  BlockManager:71 - Block input-0-1449085721200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:41 INFO  BlockGenerator:59 - Pushed block input-0-1449085721200
2015-12-02 14:48:46 INFO  MemoryStore:59 - ensureFreeSpace(3757) called with curMem=37721252, maxMem=278302556
2015-12-02 14:48:46 INFO  MemoryStore:59 - Block input-0-1449085726400 stored as bytes in memory (estimated size 3.7 KB, free 229.4 MB)
2015-12-02 14:48:46 INFO  BlockManagerInfo:59 - Added input-0-1449085726400 in memory on localhost:54856 (size: 3.7 KB, free: 231.0 MB)
2015-12-02 14:48:46 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085726400
2015-12-02 14:48:46 WARN  BlockManager:71 - Block input-0-1449085726400 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:46 INFO  BlockGenerator:59 - Pushed block input-0-1449085726400
2015-12-02 14:48:47 INFO  MemoryStore:59 - ensureFreeSpace(49499) called with curMem=37725009, maxMem=278302556
2015-12-02 14:48:47 INFO  MemoryStore:59 - Block input-0-1449085726800 stored as bytes in memory (estimated size 48.3 KB, free 229.4 MB)
2015-12-02 14:48:47 INFO  BlockManagerInfo:59 - Added input-0-1449085726800 in memory on localhost:54856 (size: 48.3 KB, free: 231.0 MB)
2015-12-02 14:48:47 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085726800
2015-12-02 14:48:47 WARN  BlockManager:71 - Block input-0-1449085726800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:48:47 INFO  BlockGenerator:59 - Pushed block input-0-1449085726800
2015-12-02 14:48:50 INFO  JobScheduler:59 - Added jobs for time 1449085730000 ms
2015-12-02 14:49:00 INFO  JobScheduler:59 - Added jobs for time 1449085740000 ms
2015-12-02 14:49:05 INFO  MemoryStore:59 - ensureFreeSpace(2311) called with curMem=37774508, maxMem=278302556
2015-12-02 14:49:05 INFO  MemoryStore:59 - Block input-0-1449085745000 stored as bytes in memory (estimated size 2.3 KB, free 229.4 MB)
2015-12-02 14:49:05 INFO  BlockManagerInfo:59 - Added input-0-1449085745000 in memory on localhost:54856 (size: 2.3 KB, free: 231.0 MB)
2015-12-02 14:49:05 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085745000
2015-12-02 14:49:05 WARN  BlockManager:71 - Block input-0-1449085745000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:49:05 INFO  BlockGenerator:59 - Pushed block input-0-1449085745000
2015-12-02 14:49:10 INFO  MemoryStore:59 - ensureFreeSpace(7455) called with curMem=37776819, maxMem=278302556
2015-12-02 14:49:10 INFO  MemoryStore:59 - Block input-0-1449085749800 stored as bytes in memory (estimated size 7.3 KB, free 229.4 MB)
2015-12-02 14:49:10 INFO  JobScheduler:59 - Added jobs for time 1449085750000 ms
2015-12-02 14:49:10 INFO  BlockManagerInfo:59 - Added input-0-1449085749800 in memory on localhost:54856 (size: 7.3 KB, free: 231.0 MB)
2015-12-02 14:49:10 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085749800
2015-12-02 14:49:10 WARN  BlockManager:71 - Block input-0-1449085749800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:49:10 INFO  BlockGenerator:59 - Pushed block input-0-1449085749800
2015-12-02 14:49:20 INFO  JobScheduler:59 - Added jobs for time 1449085760000 ms
2015-12-02 14:49:30 INFO  JobScheduler:59 - Added jobs for time 1449085770000 ms
2015-12-02 14:49:40 INFO  JobScheduler:59 - Added jobs for time 1449085780000 ms
2015-12-02 14:49:50 INFO  JobScheduler:59 - Added jobs for time 1449085790000 ms
2015-12-02 14:50:00 INFO  JobScheduler:59 - Added jobs for time 1449085800000 ms
2015-12-02 14:50:03 INFO  MemoryStore:59 - ensureFreeSpace(50011) called with curMem=37784274, maxMem=278302556
2015-12-02 14:50:03 INFO  MemoryStore:59 - Block input-0-1449085803200 stored as bytes in memory (estimated size 48.8 KB, free 229.3 MB)
2015-12-02 14:50:03 INFO  BlockManagerInfo:59 - Added input-0-1449085803200 in memory on localhost:54856 (size: 48.8 KB, free: 230.9 MB)
2015-12-02 14:50:03 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085803200
2015-12-02 14:50:03 WARN  BlockManager:71 - Block input-0-1449085803200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:50:03 INFO  BlockGenerator:59 - Pushed block input-0-1449085803200
2015-12-02 14:50:10 INFO  JobScheduler:59 - Added jobs for time 1449085810000 ms
2015-12-02 14:50:20 INFO  JobScheduler:59 - Added jobs for time 1449085820000 ms
2015-12-02 14:50:30 INFO  JobScheduler:59 - Added jobs for time 1449085830000 ms
2015-12-02 14:50:40 INFO  JobScheduler:59 - Added jobs for time 1449085840000 ms
2015-12-02 14:50:50 INFO  JobScheduler:59 - Added jobs for time 1449085850000 ms
2015-12-02 14:51:00 INFO  JobScheduler:59 - Added jobs for time 1449085860000 ms
2015-12-02 14:51:10 INFO  JobScheduler:59 - Added jobs for time 1449085870000 ms
2015-12-02 14:51:20 INFO  JobScheduler:59 - Added jobs for time 1449085880000 ms
2015-12-02 14:51:30 INFO  JobScheduler:59 - Added jobs for time 1449085890000 ms
2015-12-02 14:51:40 INFO  JobScheduler:59 - Added jobs for time 1449085900000 ms
2015-12-02 14:51:50 INFO  JobScheduler:59 - Added jobs for time 1449085910000 ms
2015-12-02 14:51:53 INFO  MemoryStore:59 - ensureFreeSpace(3938) called with curMem=37834285, maxMem=278302556
2015-12-02 14:51:53 INFO  MemoryStore:59 - Block input-0-1449085912800 stored as bytes in memory (estimated size 3.8 KB, free 229.3 MB)
2015-12-02 14:51:53 INFO  BlockManagerInfo:59 - Added input-0-1449085912800 in memory on localhost:54856 (size: 3.8 KB, free: 230.9 MB)
2015-12-02 14:51:53 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085912800
2015-12-02 14:51:53 WARN  BlockManager:71 - Block input-0-1449085912800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:51:53 INFO  BlockGenerator:59 - Pushed block input-0-1449085912800
2015-12-02 14:52:00 INFO  JobScheduler:59 - Added jobs for time 1449085920000 ms
2015-12-02 14:52:08 INFO  MemoryStore:59 - ensureFreeSpace(153803) called with curMem=37838223, maxMem=278302556
2015-12-02 14:52:08 INFO  MemoryStore:59 - Block input-0-1449085928600 stored as bytes in memory (estimated size 150.2 KB, free 229.2 MB)
2015-12-02 14:52:08 INFO  BlockManagerInfo:59 - Added input-0-1449085928600 in memory on localhost:54856 (size: 150.2 KB, free: 230.8 MB)
2015-12-02 14:52:08 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085928600
2015-12-02 14:52:08 WARN  BlockManager:71 - Block input-0-1449085928600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:52:08 INFO  BlockGenerator:59 - Pushed block input-0-1449085928600
2015-12-02 14:52:10 INFO  JobScheduler:59 - Added jobs for time 1449085930000 ms
2015-12-02 14:52:20 INFO  JobScheduler:59 - Added jobs for time 1449085940000 ms
2015-12-02 14:52:21 INFO  MemoryStore:59 - ensureFreeSpace(39225) called with curMem=37992026, maxMem=278302556
2015-12-02 14:52:21 INFO  MemoryStore:59 - Block input-0-1449085941000 stored as bytes in memory (estimated size 38.3 KB, free 229.1 MB)
2015-12-02 14:52:21 INFO  BlockManagerInfo:59 - Added input-0-1449085941000 in memory on localhost:54856 (size: 38.3 KB, free: 230.7 MB)
2015-12-02 14:52:21 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085941000
2015-12-02 14:52:21 WARN  BlockManager:71 - Block input-0-1449085941000 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:52:21 INFO  BlockGenerator:59 - Pushed block input-0-1449085941000
2015-12-02 14:52:25 INFO  MemoryStore:59 - ensureFreeSpace(39106) called with curMem=38031251, maxMem=278302556
2015-12-02 14:52:25 INFO  MemoryStore:59 - Block input-0-1449085945600 stored as bytes in memory (estimated size 38.2 KB, free 229.1 MB)
2015-12-02 14:52:25 INFO  BlockManagerInfo:59 - Added input-0-1449085945600 in memory on localhost:54856 (size: 38.2 KB, free: 230.7 MB)
2015-12-02 14:52:25 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449085945600
2015-12-02 14:52:25 WARN  BlockManager:71 - Block input-0-1449085945600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:52:25 INFO  BlockGenerator:59 - Pushed block input-0-1449085945600
2015-12-02 14:52:30 INFO  JobScheduler:59 - Added jobs for time 1449085950000 ms
2015-12-02 14:52:40 INFO  JobScheduler:59 - Added jobs for time 1449085960000 ms
2015-12-02 14:52:50 INFO  JobScheduler:59 - Added jobs for time 1449085970000 ms
2015-12-02 14:53:00 INFO  JobScheduler:59 - Added jobs for time 1449085980000 ms
2015-12-02 14:53:10 INFO  JobScheduler:59 - Added jobs for time 1449085990000 ms
2015-12-02 14:53:20 INFO  JobScheduler:59 - Added jobs for time 1449086000000 ms
2015-12-02 14:53:30 INFO  JobScheduler:59 - Added jobs for time 1449086010000 ms
2015-12-02 14:53:35 INFO  MemoryStore:59 - ensureFreeSpace(880737) called with curMem=38070357, maxMem=278302556
2015-12-02 14:53:35 INFO  MemoryStore:59 - Block input-0-1449086015200 stored as bytes in memory (estimated size 860.1 KB, free 228.3 MB)
2015-12-02 14:53:35 INFO  BlockManagerInfo:59 - Added input-0-1449086015200 in memory on localhost:54856 (size: 860.1 KB, free: 229.9 MB)
2015-12-02 14:53:35 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086015200
2015-12-02 14:53:35 WARN  BlockManager:71 - Block input-0-1449086015200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:53:35 INFO  BlockGenerator:59 - Pushed block input-0-1449086015200
2015-12-02 14:53:40 INFO  JobScheduler:59 - Added jobs for time 1449086020000 ms
2015-12-02 14:53:50 INFO  JobScheduler:59 - Added jobs for time 1449086030000 ms
2015-12-02 14:54:00 INFO  JobScheduler:59 - Added jobs for time 1449086040000 ms
2015-12-02 14:54:10 INFO  JobScheduler:59 - Added jobs for time 1449086050000 ms
2015-12-02 14:54:20 INFO  JobScheduler:59 - Added jobs for time 1449086060000 ms
2015-12-02 14:54:30 INFO  JobScheduler:59 - Added jobs for time 1449086070000 ms
2015-12-02 14:54:40 INFO  JobScheduler:59 - Added jobs for time 1449086080000 ms
2015-12-02 14:54:47 INFO  MemoryStore:59 - ensureFreeSpace(994936) called with curMem=38951094, maxMem=278302556
2015-12-02 14:54:47 INFO  MemoryStore:59 - Block input-0-1449086087400 stored as bytes in memory (estimated size 971.6 KB, free 227.3 MB)
2015-12-02 14:54:47 INFO  BlockManagerInfo:59 - Added input-0-1449086087400 in memory on localhost:54856 (size: 971.6 KB, free: 228.9 MB)
2015-12-02 14:54:47 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086087400
2015-12-02 14:54:47 WARN  BlockManager:71 - Block input-0-1449086087400 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:54:47 INFO  BlockGenerator:59 - Pushed block input-0-1449086087400
2015-12-02 14:54:50 INFO  JobScheduler:59 - Added jobs for time 1449086090000 ms
2015-12-02 14:54:56 INFO  MemoryStore:59 - ensureFreeSpace(27364) called with curMem=39946030, maxMem=278302556
2015-12-02 14:54:56 INFO  MemoryStore:59 - Block input-0-1449086096600 stored as bytes in memory (estimated size 26.7 KB, free 227.3 MB)
2015-12-02 14:54:56 INFO  BlockManagerInfo:59 - Added input-0-1449086096600 in memory on localhost:54856 (size: 26.7 KB, free: 228.9 MB)
2015-12-02 14:54:56 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086096600
2015-12-02 14:54:56 WARN  BlockManager:71 - Block input-0-1449086096600 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:54:56 INFO  BlockGenerator:59 - Pushed block input-0-1449086096600
2015-12-02 14:55:00 INFO  JobScheduler:59 - Added jobs for time 1449086100000 ms
2015-12-02 14:55:10 INFO  JobScheduler:59 - Added jobs for time 1449086110000 ms
2015-12-02 14:55:20 INFO  JobScheduler:59 - Added jobs for time 1449086120000 ms
2015-12-02 14:55:29 INFO  MemoryStore:59 - ensureFreeSpace(1862) called with curMem=39973394, maxMem=278302556
2015-12-02 14:55:29 INFO  MemoryStore:59 - Block input-0-1449086129200 stored as bytes in memory (estimated size 1862.0 B, free 227.3 MB)
2015-12-02 14:55:29 INFO  BlockManagerInfo:59 - Added input-0-1449086129200 in memory on localhost:54856 (size: 1862.0 B, free: 228.9 MB)
2015-12-02 14:55:29 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086129200
2015-12-02 14:55:29 WARN  BlockManager:71 - Block input-0-1449086129200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:55:29 INFO  BlockGenerator:59 - Pushed block input-0-1449086129200
2015-12-02 14:55:30 INFO  JobScheduler:59 - Added jobs for time 1449086130000 ms
2015-12-02 14:55:34 INFO  MemoryStore:59 - ensureFreeSpace(304679) called with curMem=39975256, maxMem=278302556
2015-12-02 14:55:34 INFO  MemoryStore:59 - Block input-0-1449086133800 stored as bytes in memory (estimated size 297.5 KB, free 227.0 MB)
2015-12-02 14:55:34 INFO  BlockManagerInfo:59 - Added input-0-1449086133800 in memory on localhost:54856 (size: 297.5 KB, free: 228.6 MB)
2015-12-02 14:55:34 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086133800
2015-12-02 14:55:34 WARN  BlockManager:71 - Block input-0-1449086133800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:55:34 INFO  BlockGenerator:59 - Pushed block input-0-1449086133800
2015-12-02 14:55:40 INFO  JobScheduler:59 - Added jobs for time 1449086140000 ms
2015-12-02 14:55:50 INFO  JobScheduler:59 - Added jobs for time 1449086150000 ms
2015-12-02 14:55:55 WARN  HttpUtil:187 - SocketTimeout-related exception occured. See antecedent stacktrace
2015-12-02 14:55:55 INFO  HttpUtil:213 - interrupted in delay()
java.lang.InterruptedException: sleep interrupted
                at java.lang.Thread.sleep(Native Method)
                at com.markmonitor.antifraud.ce.util.HttpUtil.delay(HttpUtil.java:211)
                at com.markmonitor.antifraud.ce.util.HttpUtil.getHttpGetResponse(HttpUtil.java:188)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:18)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:9)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)
2015-12-02 14:55:56 INFO  MemoryStore:59 - ensureFreeSpace(10975) called with curMem=40279935, maxMem=278302556
2015-12-02 14:55:56 INFO  MemoryStore:59 - Block input-0-1449086155800 stored as bytes in memory (estimated size 10.7 KB, free 227.0 MB)
2015-12-02 14:55:56 INFO  BlockManagerInfo:59 - Added input-0-1449086155800 in memory on localhost:54856 (size: 10.7 KB, free: 228.6 MB)
2015-12-02 14:55:56 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086155800
2015-12-02 14:55:56 WARN  BlockManager:71 - Block input-0-1449086155800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:55:56 INFO  BlockGenerator:59 - Pushed block input-0-1449086155800
2015-12-02 14:56:00 INFO  JobScheduler:59 - Added jobs for time 1449086160000 ms
2015-12-02 14:56:10 INFO  JobScheduler:59 - Added jobs for time 1449086170000 ms
2015-12-02 14:56:20 INFO  JobScheduler:59 - Added jobs for time 1449086180000 ms
2015-12-02 14:56:30 INFO  JobScheduler:59 - Added jobs for time 1449086190000 ms
2015-12-02 14:56:31 INFO  MemoryStore:59 - ensureFreeSpace(18836) called with curMem=40290910, maxMem=278302556
2015-12-02 14:56:31 INFO  MemoryStore:59 - Block input-0-1449086190800 stored as bytes in memory (estimated size 18.4 KB, free 227.0 MB)
2015-12-02 14:56:31 INFO  BlockManagerInfo:59 - Added input-0-1449086190800 in memory on localhost:54856 (size: 18.4 KB, free: 228.6 MB)
2015-12-02 14:56:31 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086190800
2015-12-02 14:56:31 WARN  BlockManager:71 - Block input-0-1449086190800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:56:31 INFO  BlockGenerator:59 - Pushed block input-0-1449086190800
2015-12-02 14:56:40 INFO  JobScheduler:59 - Added jobs for time 1449086200000 ms
2015-12-02 14:56:50 INFO  JobScheduler:59 - Added jobs for time 1449086210000 ms
2015-12-02 14:57:00 INFO  JobScheduler:59 - Added jobs for time 1449086220000 ms
2015-12-02 14:57:10 INFO  JobScheduler:59 - Added jobs for time 1449086230000 ms
2015-12-02 14:57:13 INFO  MemoryStore:59 - ensureFreeSpace(1690) called with curMem=40309746, maxMem=278302556
2015-12-02 14:57:13 INFO  MemoryStore:59 - Block input-0-1449086232800 stored as bytes in memory (estimated size 1690.0 B, free 227.0 MB)
2015-12-02 14:57:13 INFO  BlockManagerInfo:59 - Added input-0-1449086232800 in memory on localhost:54856 (size: 1690.0 B, free: 228.6 MB)
2015-12-02 14:57:13 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086232800
2015-12-02 14:57:13 WARN  BlockManager:71 - Block input-0-1449086232800 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:57:13 INFO  BlockGenerator:59 - Pushed block input-0-1449086232800
2015-12-02 14:57:20 INFO  JobScheduler:59 - Added jobs for time 1449086240000 ms
2015-12-02 14:57:25 WARN  HttpUtil:187 - SocketTimeout-related exception occured. See antecedent stacktrace
2015-12-02 14:57:25 INFO  HttpUtil:213 - interrupted in delay()
java.lang.InterruptedException: sleep interrupted
                at java.lang.Thread.sleep(Native Method)
                at com.markmonitor.antifraud.ce.util.HttpUtil.delay(HttpUtil.java:211)
                at com.markmonitor.antifraud.ce.util.HttpUtil.getHttpGetResponse(HttpUtil.java:188)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:18)
                at com.markmonitor.feature.handler.HttpEndPointCallable.call(HttpEndPointCallable.java:9)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)
2015-12-02 14:57:29 INFO  MemoryStore:59 - ensureFreeSpace(35769) called with curMem=40311436, maxMem=278302556
2015-12-02 14:57:29 INFO  MemoryStore:59 - Block input-0-1449086249200 stored as bytes in memory (estimated size 34.9 KB, free 226.9 MB)
2015-12-02 14:57:29 INFO  BlockManagerInfo:59 - Added input-0-1449086249200 in memory on localhost:54856 (size: 34.9 KB, free: 228.5 MB)
2015-12-02 14:57:29 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086249200
2015-12-02 14:57:29 WARN  BlockManager:71 - Block input-0-1449086249200 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:57:29 INFO  BlockGenerator:59 - Pushed block input-0-1449086249200
2015-12-02 14:57:30 INFO  JobScheduler:59 - Added jobs for time 1449086250000 ms
2015-12-02 14:57:31 INFO  MemoryStore:59 - ensureFreeSpace(35771) called with curMem=40347205, maxMem=278302556
2015-12-02 14:57:31 INFO  MemoryStore:59 - Block input-0-1449086251400 stored as bytes in memory (estimated size 34.9 KB, free 226.9 MB)
2015-12-02 14:57:31 INFO  BlockManagerInfo:59 - Added input-0-1449086251400 in memory on localhost:54856 (size: 34.9 KB, free: 228.5 MB)
2015-12-02 14:57:31 INFO  BlockManagerMaster:59 - Updated info of block input-0-1449086251400
2015-12-02 14:57:31 WARN  BlockManager:71 - Block input-0-1449086251400 replicated to only 0 peer(s) instead of 1 peers
2015-12-02 14:57:31 INFO  BlockGenerator:59 - Pushed block input-0-1449086251400
2015-12-02 14:57:40 INFO  JobScheduler:59 - Added jobs for time 1449086260000 ms
2015-12-02 14:57:50 INFO  JobScheduler:59 - Added jobs for time 1449086270000 ms
2015-12-02 14:58:00 INFO  JobScheduler:59 - Added jobs for time 1449086280000 ms
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing broadcast 8090
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8090
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8090 of size 6576 dropped from memory (free 237926156)
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8090_piece0
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8090_piece0 of size 2219 dropped from memory (free 237928375)
2015-12-02 14:58:03 INFO  BlockManagerInfo:59 - Removed broadcast_8090_piece0 on localhost:54856 in memory (size: 2.2 KB, free: 228.5 MB)
2015-12-02 14:58:03 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8090_piece0
2015-12-02 14:58:03 INFO  ContextCleaner:59 - Cleaned broadcast 8090
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing broadcast 8089
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8089
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8089 of size 74136 dropped from memory (free 238002511)
2015-12-02 14:58:03 INFO  BlockManager:59 - Removing block broadcast_8089_piece0
2015-12-02 14:58:03 INFO  MemoryStore:59 - Block broadcast_8089_piece0 of size 25655 dropped from memory (free 238028166)
2015-12-02 14:58:03 INFO  BlockManagerInfo:59 - Removed broadcast_8089_piece0 on localhost:54856 in memory (size: 25.1 KB, free: 228.5 MB)
2015-12-02 14:58:03 INFO  BlockManagerMaster:59 - Updated info of block broadcast_8089_piece0
2015-12-02 14:58:03"
"
Matt Cheah <mcheah@palantir.com>,Fri"," 4 Dec 2015 01:27:40 +0000""",Quick question regarding Maven and Spark Assembly jar,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

A very brief question out of curiosity ≠ is there any particular reason why
we donπt publish the Spark assembly jar on the Maven repository?

Thanks,

-Matt Cheah


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 3 Dec 2015 17:35:22 -0800",Re: Quick question regarding Maven and Spark Assembly jar,Matt Cheah <mcheah@palantir.com>,"Try to read this before Marcelo gets to you.
https://issues.apache.org/jira/browse/SPARK-11157


reason
ry?
"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 3 Dec 2015 17:55:29 -0800",Re: Bringing up JDBC Tests to trunk,Josh Rosen <joshrosen@databricks.com>,"

So, the issue I am having now is that the DB2 JDBC is not available in any
maven public repository, so the plan I am going in with is :

- Before running the DB2 Docker Tests, the client machine needs to download
the jdbc driver locally and install it to it's local maven repository (or
sbt equivalent)  (instructions to be provided in either readme or pom file)

- We would need help with installing the DB2 JDBC on the Jenkins slaves
machines

- We could also create a new profile for the DB2 Docker Tests, so that this
tests are running when this profile is enabled.

I could probably think about other options, but they would sound a lot
hacky.....

Thoughts ? Some suggestions ?

-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 4 Dec 2015 10:40:01 +0800",Spark doesn't unset HADOOP_CONF_DIR when testing ?,dev <dev@spark.apache.org>,"I try to do test on HiveSparkSubmitSuite on local box, but fails. The cause
is that spark is still using my local single node cluster hadoop when doing
the unit test. I don't think it make sense to do that. These environment
variable should be unset before the testing. And I suspect dev/run-tests
also
didn't do that either.

Here's the error message:

Cause: java.lang.RuntimeException: java.lang.RuntimeException: The root
scratch dir: /tmp/hive on HDFS should be writable. Current permissions are:
rwxr-xr-x
[info]   at
org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
[info]   at
org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:171)
[info]   at
org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:162)
[info]   at
org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:160)



-- 
Best Regards

Jeff Zhang
"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 3 Dec 2015 19:14:51 -0800",Re: A proposal for Spark 2.0,Rad Gruchalski <radek@gruchalski.com>,"There was a proposal to make schedulers pluggable in context of adding one
which leverages Apache Tez : IIRC it was a abandoned - but the jira might
be a good starting point.

Regards
Mridul

os
o
a
to
t
n
ot
,
y
e
n
e
g
se
.
e
a
.
m
o
t
t
d
e
D
g
d
,
 painful
e
te
o
t API and
happy to
ºåMatei Zaharia <matei.zaharia@gmail.com> ÂÜôÈÅìÔºö
t
he
t
s
x
f
o
"
"""Mario Ds Briggs"" <mario.briggs@in.ibm.com>","Fri, 4 Dec 2015 14:42:06 +0530","Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the new Kafka
 Consumer API",Cody Koeninger <cody@koeninger.org>,"
 forcing people on kafka 8.x to upgrade their brokers is questionable.
<<

I agree and i was more thinking maybe there is a way to support both for a
period of time (of course means some more code to maintain :-)).


thanks
Mario



From:	Cody Koeninger <cody@koeninger.org>
To:	Mario Ds Briggs/India/IBM@IBMIN
Cc:	""dev@spark.apache.org"" <dev@spark.apache.org>
Date:	04/12/2015 12:15 am
Subject:	Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the
            new Kafka Consumer API



Honestly my feeling on any new API is to wait for a point release before
taking it seriously :)

Auth and encryption seem like the only compelling reason to move, but
forcing people on kafka 8.x to upgrade their brokers is questionable.

  Hi,

  Wanted to pick Cody's mind on what he thinks about
  DirectKafkaInputDStream/KafkaRDD internally using the new Kafka consumer
  API. I know the latter is documented as beta-quality, but yet wanted to
  side the consideration is that kafka 0.9.0.0 introduced Authentication
  and Encryption (beta again) between clients & brokers, but this is
  available only newer Consumer API's and not in the older
  Low-level/High-level API's.

  From briefly studying the implementation of
  DirectKafkaInputDStream/KafkaRDD and new Consumer API, my thinking is
  that it is possible to support the exact current implementation you have
  using the new API's.
  offsetRange (I did read about the deterministic feature you were after)
  and i couldnt find a direct method in the new Consumer API to get the
  current 'latest' offset - however one can do a consumer.seekToEnd() and
  then call a consumer.position().
  Of course one other benefit is that the new Consumer API's abstracts away
  having to deal with finding the leader for a partition, so can get rid of
  that code

  Would be great to get your thoughts.

  thanks in advance
  Mario








"
Sean Owen <sowen@cloudera.com>,"Fri, 4 Dec 2015 11:28:42 +0000",Re: A proposal for Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"To be clear-er, I don't think it's clear yet whether a 1.7 release
should exist or not. I could see both making sense. It's also not
really necessary to decide now, well before a 1.6 is even out in the
field. Deleting the version lost information, and I would not have
done that given my reply. Reynold maybe I can take this up with you
offline.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 4 Dec 2015 12:05:24 +0000",Re: Quick question regarding Maven and Spark Assembly jar,Matt Cheah <mcheah@palantir.com>,"I think one problem is that the assembly by nature includes a bunch of
it would be unlikely to be the right flavor of assembly for any given
user.

reason why

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 4 Dec 2015 09:14:52 -0600","Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the new
 Kafka Consumer API",Mario Ds Briggs <mario.briggs@in.ibm.com>,"Brute force way to do it might be to just have a separate
streaming-kafka-new-consumer subproject, or something along those lines.


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 4 Dec 2015 15:04:33 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Michael Armbrust <michael@databricks.com>,"0

Currently figuring out who is responsible for the regression that I am
seeing in some user code ScalaUDFs that make use of Timestamps and where
NULL from a CSV file read in via a TestHive#registerTestTable is now
producing 1969-12-31 23:59:59.999999 instead of null.


"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Sat, 5 Dec 2015 14:02:34 +0100",[ML] Missing documentation for the IndexToString feature transformer,dev@spark.apache.org,"Hi,

I was wondering why the IndexToString
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/StringIndexer.scala#L245>
label
transformer was not documented in ml-features.md
<https://github.com/apache/spark/blob/master/docs/ml-features.md>.

If it's not intentional, having used it a few times, I'd be happy to submit
a jira and the pr associated.

Best,
Ben.

-- 
Ben Fradet.
"
Justin Uang <justin.uang@gmail.com>,"Sat, 05 Dec 2015 15:03:39 +0000",Returning numpy types from udfs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have fallen into the trap of returning numpy types from udfs, such as
np.float64 and np.int. It's hard to find the issue because they behave
pretty much as regular pure Python floats and doubles, so can we make
PYSPARK automatically translate them?

If so, I'll create a Jira ticket.

Justin
"
Reynold Xin <rxin@databricks.com>,"Sat, 5 Dec 2015 23:08:25 +0800",Re: Returning numpy types from udfs,Justin Uang <justin.uang@gmail.com>,"Not aware of any jira ticket, but it does sound like a great idea.



"
Justin Uang <justin.uang@gmail.com>,"Sat, 05 Dec 2015 16:54:16 +0000",Re: Returning numpy types from udfs,Reynold Xin <rxin@databricks.com>,"Filed here:

https://issues.apache.org/jira/browse/SPARK-12157


"
Joseph Bradley <joseph@databricks.com>,"Sat, 5 Dec 2015 14:21:37 -0800",Re: [ML] Missing documentation for the IndexToString feature transformer,Benjamin Fradet <benjamin.fradet@gmail.com>,"Thanks for reporting this!  I just added a JIRA:
https://issues.apache.org/jira/browse/SPARK-12159
That would be great if you could send a PR for it; thanks!
Joseph


"
Holden Karau <holden@pigscanfly.ca>,"Sat, 5 Dec 2015 14:26:23 -0800",Re: [ML] Missing documentation for the IndexToString feature transformer,Joseph Bradley <joseph@databricks.com>,"I'd be more than happy to help review the docs if that would be useful :)




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
jatinganhotra <jatin.ganhotra@gmail.com>,"Sat, 5 Dec 2015 19:57:21 -0700 (MST)",How to debug Spark source using IntelliJ/ Eclipse,dev@spark.apache.org,"Hi,

I am trying to understand Spark internal code and wanted to debug Spark
source, to add a new feature. I have tried the steps lined out here on the 
Spark Wiki page IDE setup
<https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IDESetup> 
, but they don't work.

I also found other posts in the Dev mailing list such as - 

1.  Spark-1-5-0-setting-up-debug-env
<http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-5-0-setting-up-debug-env-td14056.html> 
, and

2.  using-IntelliJ-to-debug-SPARK-1-1-Apps-with-mvn-sbt-for-beginners
<http://apache-spark-developers-list.1001551.n3.nabble.com/Intro-to-using-IntelliJ-to-debug-SPARK-1-1-Apps-with-mvn-sbt-for-beginners-td9429.html>  

But, I found many issues with both the links. I have tried both these
articles many times, often re-starting the whole process from scratch after
deleting everything and re-installing again, but I always face some
dependency issues.

It would be great if someone from the Spark developers group could point me
to the steps for setting up Spark debug environment.



--

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Sun, 06 Dec 2015 20:32:52 +0000",Re: Spark doesn't unset HADOOP_CONF_DIR when testing ?,"Jeff Zhang <zjffdu@gmail.com>, dev <dev@spark.apache.org>","I agree that we should unset this in our tests. Want to file a JIRA and
submit a PR to do this?


"
Jia <jacquelinezou@gmail.com>,"Sun, 6 Dec 2015 14:43:24 -0600",Shared memory between C++ process and Spark,dev@spark.apache.org,"Dears, for one project, I need to implement something so Spark can read data from a C++ process. 
To provide high performance, I really hope to implement this through shared memory between the C++ process and Java JVM process.
It seems it may be possible to use named memory mapped files and JNI to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
Thank you very much!

Best Regards,
Jia


---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Sun, 06 Dec 2015 20:46:04 +0000",Re: Bringing up JDBC Tests to trunk,Luciano Resende <luckbr1975@gmail.com>,"Can you write a script to download and install the JDBC driver to the local
Maven repository if it's not already present? If we had that, we could just
invoke it as part of dev/run-tests.


"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 7 Dec 2015 09:38:37 +0800",Re: Spark doesn't unset HADOOP_CONF_DIR when testing ?,Josh Rosen <joshrosen@databricks.com>,"Thanks Josh, created https://issues.apache.org/jira/browse/SPARK-12166






-- 
Best Regards

Jeff Zhang
"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Mon, 07 Dec 2015 10:16:52 +0800",Re: query on SVD++,"""'Yanbo Liang'"" <ybliang8@gmail.com>","Yes,

 

We already used ALS in our production environment, we also want to try SVD++ but it has no python interface.

 

Any idea? Thanks

 

-Allen

 

Âèë‰ª∂‰∫∫: Yanbo Liang [mailto:ybliang8@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà3Êó• 10:30
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: query on SVD++

 

You means the SVDPlusPlus in GraphX? If you want to use SVD++ to train CF model, I recommend you to use ALS which is more efficiency and has python interface.

 

2015-12-02 11:21 GMT+08:00 Âº†ÂøóÂº∫(Êó∫ËΩ©) <zzq98736@alibaba-inc.com>:

Hi All,

 

I came across the SVD++ algorithm implementation in Spark code base, but I was wondering why we didn‚Äôt expose the scala api interface to python?

Any plan to do this?

 

BR,

-Allen Zhang

 

"
Yin Huai <yhuai@databricks.com>,"Sun, 6 Dec 2015 18:24:00 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Mark Hamstra <mark@clearstorydata.com>,"-1

Tow blocker bugs have been found after this RC.
https://issues.apache.org/jira/browse/SPARK-12089 can cause data corruption
when an external sorter spills data.
https://issues.apache.org/jira/browse/SPARK-12155 can prevent tasks from
acquiring memory "
Robin East <robin.east@xense.co.uk>,"Mon, 7 Dec 2015 08:08:10 +0000",Re: query on SVD++,=?utf-8?B?IuW8oOW/l+W8uijml7rovakpIg==?= <zzq98736@alibaba-inc.com>,"Python bindings for GraphX are still in development - see https://issues.apache.org/jira/browse/SPARK-3789?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20%3D%20GraphX%20ORDER%20BY%20updated%20DESC <https://issues.apache.org/jira/browse/SPARK-3789?jql=project%20=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20component%20=%20GraphX%20ORDER%20BY%20updated%20DESC> for the latest.


-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action <http://www.manning.com/books/spark-graphx-in-action>





SVD++ but it has no python interface.
<mailto:ybliang8@gmail.com>] 
 10:30
©)
CF model, I recommend you to use ALS which is more efficiency and has python interface.
) <zzq98736@alibaba-inc.com <mailto:zzq98736@alibaba-inc.com>>:
but I was wondering why we didn‚Äôt expose the scala api interface to python?

"
"""wei.zhu@kaiyuandao.com"" <wei.zhu@kaiyuandao.com>","Mon, 7 Dec 2015 16:43:04 +0800",mlib compilation errors,dev <dev@spark.apache.org>,"hi£¨ when I was compiling the mlib project in Intellij, it has the following errors. If I run mvn from command line, it works well. anyone came to the same issue? thanks



"
"""Mario Ds Briggs"" <mario.briggs@in.ibm.com>","Mon, 7 Dec 2015 15:58:56 +0530","Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the new Kafka
 Consumer API",Cody Koeninger <cody@koeninger.org>,"

sounds sane for a first cut.

Since all creation methods take a KafkaParams, i was thinking along lines
of maybe  a temp property in there which trigger usage of new consumer.

thanks
Mario



From:	Cody Koeninger <cody@koeninger.org>
To:	Mario Ds Briggs/India/IBM@IBMIN
Cc:	""dev@spark.apache.org"" <dev@spark.apache.org>
Date:	04/12/2015 08:45 pm
Subject:	Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the
            new Kafka Consumer API



Brute force way to do it might be to just have a separate
streaming-kafka-new-consumer subproject, or something along those lines.

  >>
  forcing people on kafka 8.x to upgrade their brokers is questionable.
  <<

  I agree and i was more thinking maybe there is a way to support both for
  a period of time (of course means some more code to maintain :-)).


  thanks
  Mario

  Inactive hide details for Cody Koeninger ---04/12/2015 12:15:55
  am---Honestly my feeling on any new API is to wait for a point Cody
  Koeninger ---04/12/2015 12:15:55 am---Honestly my feeling on any new API
  is to wait for a point release before taking it seriously :)

  From: Cody Koeninger <cody@koeninger.org>
  To: Mario Ds Briggs/India/IBM@IBMIN
  Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>
  Date: 04/12/2015 12:15 am
  Subject: Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the
  new Kafka Consumer API



  Honestly my feeling on any new API is to wait for a point release before
  taking it seriously :)

  Auth and encryption seem like the only compelling reason to move, but
  forcing people on kafka 8.x to upgrade their brokers is questionable.

        Hi,

        Wanted to pick Cody's mind on what he thinks about
        DirectKafkaInputDStream/KafkaRDD internally using the new Kafka
        consumer API. I know the latter is documented as beta-quality, but
        yet wanted to know if he sees any blockers as to why shouldn't go
        introduced Authentication and Encryption (beta again) between
        clients & brokers, but this is available only newer Consumer API's
        and not in the older Low-level/High-level API's.

        From briefly studying the implementation of
        DirectKafkaInputDStream/KafkaRDD and new Consumer API, my thinking
        is that it is possible to support the exact current implementation
        you have using the new API's.
        fixes the offsetRange (I did read about the deterministic feature
        you were after) and i couldnt find a direct method in the new
        Consumer API to get the current 'latest' offset - however one can
        do a consumer.seekToEnd() and then call a consumer.position().
        Of course one other benefit is that the new Consumer API's
        abstracts away having to deal with finding the leader for a
        partition, so can get rid of that code

        Would be great to get your thoughts.

        thanks in advance
        Mario












"
"""Jagadeesan A.S."" <lijugan92@gmail.com>","Mon, 7 Dec 2015 19:25:40 +0530",java.lang.OutOfMemoryError: Java heap space,dev@spark.apache.org,"Hi dev,

We are testing spark performance on Spark-perf. While generating output for
python_mllib-perf we are getting following issue.

https://github.com/databricks/spark-perf/issues/92

Max. Heap Size (Estimated): 8.00G
------------------------------

Following changes we made in spark-perf-master/config/config.py file.

SPARK_HOME_DIR = ""/home/test/spark-1.5.1""
RUN_SPARK_TESTS = False
RUN_PYSPARK_TESTS =False
RUN_STREAMING_TESTS =False
RUN_MLLIB_TESTS = False
RUN_PYTHON_MLLIB_TESTS = True

PREP_SPARK_TESTS = False
PREP_PYSPARK_TESTS = False
PREP_STREAMING_TESTS =False
PREP_MLLIB_TESTS = False

COMMON_JAVA_OPTS = [ JavaOptionSet(""spark.storage.memoryFraction"", [0.66]),
JavaOptionSet(""spark.serializer"",
[""org.apache.spark.serializer.JavaSerializer""]),
JavaOptionSet(""spark.executor.memory"", [""16g""]),
JavaOptionSet(""spark.locality.wait"", [str(60 * 1000 * 1000)]) ]

SPARK_DRIVER_MEMORY = ""4g""
MLLIB_SPARK_VERSION = 1.5


Thanks & Regards

Jagadeesan A S
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 7 Dec 2015 15:24:05 +0100",Re: How to debug Spark source using IntelliJ/ Eclipse,jatinganhotra <jatin.ganhotra@gmail.com>,"What errors do you see? I‚Äôm using Eclipse and things work pretty much as
described (I‚Äôm using Scala 2.11 so there‚Äôs a slight difference for that,
but if you‚Äôre fine using Scala 2.10 it should be good to go).

instead run:

build/sbt eclipse

iulian
‚Äã


e
UsefulDeveloperTools-IDESetup
ting-up-debug-env-td14056.html
IntelliJ-to-debug-SPARK-1-1-Apps-with-mvn-sbt-for-beginners-td9429.html
er
me
ark-source-using-IntelliJ-Eclipse-tp15477.html


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Robin East <robin.east@xense.co.uk>,"Mon, 7 Dec 2015 16:54:07 +0000",Re: Shared memory between C++ process and Spark,"Jia <jacquelinezou@gmail.com>,
 ""user @spark"" <user@spark.apache.org>","-dev, +user (this is not a question about development of Spark itself so you‚Äôll get more answers in the user mailing list)

First up let me say that I don‚Äôt really know how this could be done - I‚Äôm sure it would be possible with enough tinkering but"
Jia <jacquelinezou@gmail.com>,"Mon, 7 Dec 2015 11:32:46 -0600",Re: Shared memory between C++ process and Spark,Robin East <robin.east@xense.co.uk>,"Hi, Robin, 
Thanks for your reply and thanks for copying my question to user mailing list.
Yes, we have a distributed C++ application, that will store data on each node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, thatís why we want shared memory.
Suggestions will be highly appreciated!

Best Regards,
Jia


so youíll get more answers in the user mailing list)
- Iím sure it would be possible with enough tinkering but itís not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesnít make architectural sense. 
-------------------------------------------------------------------------------
read data from a C++ process. 
shared memory between the C++ process and Java JVM process.
to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?

"
Dewful <dewful@gmail.com>,"Mon, 7 Dec 2015 10:46:38 -0700",Re: Shared memory between C++ process and Spark,Jia <jacquelinezou@gmail.com>,"Maybe looking into something like Tachyon would help, I see some sample c++
bindings, not sure how much of the current functionality they support...
Hi, Robin,
Thanks for your reply and thanks for copying my question to user mailing
list.
Yes, we have a distributed C++ application, that will store data on each
node in the cluster, and we hope to leverage Spark to do more fancy
analytics on those data. But we need high performance, that‚Äôs why we want
shared memory.
Suggestions will be highly appreciated!

Best Regards,
Jia


-dev, +user (this is not a question about development of Spark itself so
you‚Äôll get more answers in the user mailing list)

First up let me say that I don‚Äôt really know how this could be done - I‚Äôm
sure it would be possible with enough tinkering but it‚Äôs not clear what you
are trying to achieve. Spark is a distributed processing system, it has
multiple JVMs running on different machines that each run a small part of
the overall processing. Unless you have some sort of idea to have multiple
C++ processes collocated with the distributed JVMs using named memory
mapped files doesn‚Äôt make architectural sense.
-------------------------------------------------------------------------------
Robin East
*Spark GraphX in Action* Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action






Dears, for one project, I need to implement something so Spark can read
data from a C++ process.
To provide high performance, I really hope to implement this through shared
memory between the C++ process and Java JVM process.
It seems it may be possible to use named memory mapped files and JNI to do
this, but I wonder whether there is any existing efforts or more efficient
approach to do this?
Thank you very much!

Best Regards,
Jia


---------------------------------------------------------------------
"
Jia <jacquelinezou@gmail.com>,"Mon, 7 Dec 2015 12:11:59 -0600",Re: Shared memory between C++ process and Spark,Dewful <dewful@gmail.com>,"Thanks, Dewful!

My impression is that Tachyon is a very nice in-memory file system that can connect to multiple storages.
However, because our data is also hold in memory, I suspect that connecting to Spark directly may be more efficient in performance.
But definitely I need to look at Tachyon more carefully, in case it has a very efficient C++ binding mechanism.

Best Regards,
Jia


sample c++ bindings, not sure how much of the current functionality they support...
mailing list.
each node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, thatís why we want shared memory.
so youíll get more answers in the user mailing list)
- Iím sure it would be possible with enough tinkering but itís not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesnít make architectural sense. 
-------------------------------------------------------------------------------
read data from a C++ process. 
shared memory between the C++ process and Java JVM process.
to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
---------------------------------------------------------------------

"
Annabel Melongo <melongo_annabel@yahoo.com.INVALID>,"Mon, 7 Dec 2015 18:26:34 +0000 (UTC)",Re: Shared memory between C++ process and Spark,"Jia <jacquelinezou@gmail.com>, Dewful <dewful@gmail.com>","My guess is that Jia wants to run C++ on top of Spark. If that's the case, I'm afraid this is not possible. Spark has support for Java, Python, Scala and R.
The best way to achieve this is to run your application in C++ and used the data created by said application to do manipulation within Spark. 


e:
 

 Thanks, Dewful!
My impression is that Tachyon is a very nice in-memory file system that can connect to multiple storages.However, because our data is also hold in memory, I suspect that connecting to Spark directly may be more efficient in performance.But definitely I need to look at Tachyon more carefully, in case it has a very efficient C++ binding mechanism.
Best Regards,Jia

Maybe looking into something like Tachyon would help, I see some sample c++ bindings, not sure how much of the current functionality they support...Hi, Robin,¬†Thanks for your reply and thanks for copying my question to user mailing list.Yes, we have a distributed C++ application, that will store data on each node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, that‚Äôs why we want shared memory.Suggestions will be highly appreciated!
Best Regards,Jia

-dev, +user (this is not a question about development of Spark itself so you‚Äôll get more answers in the user mailing list)
First up let me say that I don‚Äôt really know how this could be done - I‚Äôm sure it would be possible with enough tinkering but it‚Äôs not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesn‚Äôt make architectural sense.¬†
-------------------------------------------------------------------------------Robin EastSpark GraphX in Action¬†Michael Malak and Robin EastManning Publications Co.http://www.manning.com/books/spark-graphx-in-action





Dears, for one project, I need to implement something so Spark can read data from a C++ process. 
To provide high performance, I really hope to implement this through shared memory between the C++ process and Java JVM process.
It seems it may be possible to use named memory mapped files and JNI to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
Thank you very much!

Best Regards,
Jia


---------------------------------------------------------------------









  "
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Tue, 8 Dec 2015 03:26:57 +0900",Re: Shared memory between C++ process and Spark,Jia <jacquelinezou@gmail.com>,"Is this JIRA entry related to what you want?
https://issues.apache.org/jira/browse/SPARK-10399

Regards,
Kazuaki Ishizaki



From:   Jia <jacquelinezou@gmail.com>
To:     Dewful <dewful@gmail.com>
Cc:     ""user @spark"" <user@spark.apache.org>, dev@spark.apache.org, Robin 
East <robin.east@xense.co.uk>
Date:   2015/12/08 03:17
Subject:        Re: Shared memory between C++ process and Spark



Thanks, Dewful!

My impression is that Tachyon is a very nice in-memory file system that 
can connect to multiple storages.
However, because our data is also hold in memory, I suspect that 
connecting to Spark directly may be more efficient in performance.
But definitely I need to look at Tachyon more carefully, in case it has a 
very efficient C++ binding mechanism.

Best Regards,
Jia


Maybe looking into something like Tachyon would help, I see some sample 
c++ bindings, not sure how much of the current functionality they 
support...
Hi, Robin, 
Thanks for your reply and thanks for copying my question to user mailing 
list.
Yes, we have a distributed C++ application, that will store data on each 
node in the cluster, and we hope to leverage Spark to do more fancy 
analytics on those data. But we need high performance, that$B!G(Bs why we want 
shared memory.
Suggestions will be highly appreciated!

Best Regards,
Jia


-dev, +user (this is not a question about development of Spark itself so 
you$B!G(Bll get more answers in the user mailing list)

First up let me say that I don$B!G(Bt really know how this could be done - I$B!G(B
m sure it would be possible with enough tinkering but it$B!G(Bs not clear what 
you are trying to achieve. Spark is a distributed processing system, it 
has multiple JVMs running on different machines that each run a small part 
of the overall processing. Unless you have some sort of idea to have 
multiple C++ processes collocated with the distributed JVMs using named 
memory mapped files doesn$B!G(Bt make architectural sense. 
-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action






Dears, for one project, I need to implement something so Spark can read 
data from a C++ process. 
To provide high performance, I really hope to implement this through 
shared memory between the C++ process and Java JVM process.
It seems it may be possible to use named memory mapped files and JNI to do 
this, but I wonder whether there is any existing efforts or more efficient 
approach to do this?
Thank you very much!

Best Regards,
Jia


---------------------------------------------------------------------






"
Jia <jacquelinezou@gmail.com>,"Mon, 7 Dec 2015 12:40:05 -0600",Re: Shared memory between C++ process and Spark,Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>,"Hi, Kazuaki,

It$B!G(Bs very similar with my requirement, thanks!
It seems they want to write to a C++ process with zero copy, and I want to do both read/write with zero copy.
Any one knows how to obtain more information like current status of this JIRA entry?

Best Regards,
Jia





dev@spark.apache.org, Robin East <robin.east@xense.co.uk>
that can connect to multiple storages.
connecting to Spark directly may be more efficient in performance.
has a very efficient C++ binding mechanism.
sample c++ bindings, not sure how much of the current functionality they support...
mailing list.
each node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, that$B!G(Bs why we want shared memory.
so you$B!G(Bll get more answers in the user mailing list)
be done - I$B!G(Bm sure it would be possible with enough tinkering but it$B!G(Bs not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesn$B!G(Bt make architectural sense. 
-------------------------------------------------------------------------------
read data from a C++ process. 
shared memory between the C++ process and Java JVM process.
to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?

"
Jia <jacquelinezou@gmail.com>,"Mon, 7 Dec 2015 12:42:54 -0600",Re: Shared memory between C++ process and Spark,Annabel Melongo <melongo_annabel@yahoo.com>,"Thanks, Annabel, but I may need to clarify that I have no intention to write and run Spark UDF in C++, I'm just wondering whether Spark can read and write data to a C++ process with zero copy.

Best Regards,
Jia
 



case, I'm afraid this is not possible. Spark has support for Java, Python, Scala and R.
used the data created by said application to do manipulation within Spark.
that can connect to multiple storages.
connecting to Spark directly may be more efficient in performance.
has a very efficient C++ binding mechanism.
sample c++ bindings, not sure how much of the current functionality they support...
mailing list.
each node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, thatís why we want shared memory.
itself so youíll get more answers in the user mailing list)
done - Iím sure it would be possible with enough tinkering but itís not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesnít make architectural sense. 
-------------------------------------------------------------------------------
read data from a C++ process. 
through shared memory between the C++ process and Java JVM process.
JNI to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
---------------------------------------------------------------------

"
Robin East <robin.east@xense.co.uk>,"Mon, 7 Dec 2015 18:57:26 +0000",Re: Shared memory between C++ process and Spark,Annabel Melongo <melongo_annabel@yahoo.com>,"Annabel

Spark works very well with data stored in HDFS but is certainly not tied to it. Have a look at the wide variety of connectors to things like Cassandra, HBase, etc.

Robin

Sent from my iPhone

:
. What you're requesting, reading and writing to a C++ process, is not part of that requirement.
te and run Spark UDF in C++, I'm just wondering whether Spark can read and write data to a C++ process with zero copy.
rote:
, I'm afraid this is not possible. Spark has support for Java, Python, Scala and R.
he data created by said application to do manipulation within Spark.

an connect to multiple storages.
ng to Spark directly may be more efficient in performance.
 very efficient C++ binding mechanism.
++ bindings, not sure how much of the current functionality they support...
 list.
 node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, that‚Äôs why we want shared memory.

o you‚Äôll get more answers in the user mailing list)
one - I‚Äôm sure it would be possible with enough tinkering but it‚Äôs not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesn‚Äôt make architectural sense. 
--------
d data from a C++ process. 
hared memory between the C++ process and Java JVM process.
o do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
"
Jakob Odersky <jodersky@gmail.com>,"Mon, 7 Dec 2015 11:07:25 -0800",Re: Fastest way to build Spark from scratch,Nicholas Chammas <nicholas.chammas@gmail.com>,"make-distribution and the second code snippet both create a distribution
from a clean state. They therefore require that every source file be
compiled and that takes time (you can maybe tweak some settings or use a
newer compiler to gain some speed).

I'm inferring from your question that for your use-case deployment speed is
a critical issue, furthermore you'd like to build Spark for lots of
(every?) commit in a systematic way. In that case I would suggest you try
using the second code snippet without the `clean` task and only resort to
it if the build fails.


regards,
--Jakob




714798ec3/spark/init.sh#L21-L22>
"
Robin East <robin.east@xense.co.uk>,"Mon, 7 Dec 2015 19:49:48 +0000",Re: Shared memory between C++ process and Spark,Annabel Melongo <melongo_annabel@yahoo.com>,"Hi Annabel

I certainly did read your post. My point was that Spark can read from HDFS but is in no way tied to that storage layer . A very interesting use case that sounds very similar to Jia's (as mentioned by another poster) is contained in https://issues.apache.org/jira/browse/SPARK-10399. The comments section provides a specific example of processing very large images using a pre-existing c++ library.

Robin

Sent from my iPhone

. What you're requesting, reading and writing to a C++ process, is not part of that requirement.
te and run Spark UDF in C++, I'm just wondering whether Spark can read and write data to a C++ process with zero copy.
rote:
, I'm afraid this is not possible. Spark has support for Java, Python, Scala and R.
he data created by said application to do manipulation within Spark.

an connect to multiple storages.
ng to Spark directly may be more efficient in performance.
 very efficient C++ binding mechanism.
++ bindings, not sure how much of the current functionality they support...
 list.
 node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, that‚Äôs why we want shared memory.

o you‚Äôll get more answers in the user mailing list)
one - I‚Äôm sure it would be possible with enough tinkering but it‚Äôs not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesn‚Äôt make architectural sense. 
--------
d data from a C++ process. 
hared memory between the C++ process and Java JVM process.
o do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
"
Robin East <robin.east@xense.co.uk>,"Mon, 7 Dec 2015 20:09:39 +0000",Re: Shared memory between C++ process and Spark,Annabel Melongo <melongo_annabel@yahoo.com>,"I‚Äôm not sure what point you‚Äôre trying to prove and I‚Äôm not particularly interested in getting into a protracted on top of HDFS. I interpreted that as a statement implying that Spark has to run on HDFS which is definitely not the case. If you didn‚Äôt mean then we are both in agreement.
-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action <http://www.manning.com/books/spark-graphx-in-action>





implementation stage.
HDFS but is in no way tied to that storage layer . A very interesting use case that sounds very similar to Jia's (as mentioned by another poster) is contained in https://issues.apache.org/jira/browse/SPARK-10399 <https://issues.apache.org/jira/browse/SPARK-10399>. The comments section provides a specific example of processing very large images using a pre-existing c++ library.
<melongo_annabel@yahoo.com.INVALID of HDFS. What you're requesting, reading and writing to a C++ process, is not part of that requirement.
to write and run Spark UDF in C++, I'm just wondering whether Spark can read and write data to a C++ process with zero copy.
case, I'm afraid this is not possible. Spark has support for Java, Python, Scala and R.
used the data created by said application to do manipulation within Spark.
that can connect to multiple storages.
connecting to Spark directly may be more efficient in performance.
has a very efficient C++ binding mechanism.
sample c++ bindings, not sure how much of the current functionality they support...
mailing list.
each node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, that‚Äôs why we want shared memory.
itself so you‚Äôll get more answers in the user mailing list)
could be done - I‚Äôm sure it would be possible with enough tinkering but it‚Äôs not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesn‚Äôt make architectural sense. 
-------------------------------------------------------------------------------
<http://www.manning.com/books/spark-graphx-in-action>
can read data from a C++ process. 
through shared memory between the C++ process and Java JVM process.
JNI to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Jian Feng <freedafeng@yahoo.com.INVALID>,"Mon, 7 Dec 2015 21:17:04 +0000 (UTC)",Re: Shared memory between C++ process and Spark,"Robin East <robin.east@xense.co.uk>, 
	Annabel Melongo <melongo_annabel@yahoo.com>","The only way I can think of is through some kind of wrapper. For java/scala, use JNI. For Python, use extensions. There should not be a lot of work if you know these tools.¬†

      From: Robin East <robin.east@xense.co.uk>
 To: Annabel Melongo <melongo_annabel@yahoo.com> 
Cc: Jia <jacquelinezou@gmail.com>; Dewful <dewful@gmail.com>; ""user @spark"" <user@spark.apache.org>; ""dev@spark.apache.org"" <dev@spark.apache.org>
 Sent: Monday, December 7, 2015 10:57 AM
 Subject: Re: Shared memory between C++ process and Spark
   
Annabel
Spark works very well with data stored in HDFS but is certainly not tied to it. Have a look at the wide variety of connectors to things like Cassandra, HBase, etc.
Robin

Sent from my iPhone


Jia,
I'm so confused on this. The architecture of Spark is to run on top of HDFS. What you're requesting, reading and writing to a C++ process, is not part of that requirement.

 


e:
 

 Thanks, Annabel, but I may need to clarify that I have no intention to write and run Spark UDF in C++, I'm just wondering whether Spark can read and write data to a C++ process with zero copy.
Best Regards,Jia¬†



ote:

My guess is that Jia wants to run C++ on top of Spark. If that's the case, I'm afraid this is not possible. Spark has support for Java, Python, Scala and R.
The best way to achieve this is to run your application in C++ and used the data created by said application to do manipulation within Spark. 


e:
 

 Thanks, Dewful!
My impression is that Tachyon is a very nice in-memory file system that can connect to multiple storages.However, because our data is also hold in memory, I suspect that connecting to Spark directly may be more efficient in performance.But definitely I need to look at Tachyon more carefully, in case it has a very efficient C++ binding mechanism.
Best Regards,Jia

Maybe looking into something like Tachyon would help, I see some sample c++ bindings, not sure how much of the current functionality they support...Hi, Robin,¬†Thanks for your reply and thanks for copying my question to user mailing list.Yes, we have a distributed C++ application, that will store data on each node in the cluster, and we hope to leverage Spark to do more fancy analytics on those data. But we need high performance, that‚Äôs why we want shared memory.Suggestions will be highly appreciated!
Best Regards,Jia

-dev, +user (this is not a question about development of Spark itself so you‚Äôll get more answers in the user mailing list)
First up let me say that I don‚Äôt really know how this could be done - I‚Äôm sure it would be possible with enough tinkering but it‚Äôs not clear what you are trying to achieve. Spark is a distributed processing system, it has multiple JVMs running on different machines that each run a small part of the overall processing. Unless you have some sort of idea to have multiple C++ processes collocated with the distributed JVMs using named memory mapped files doesn‚Äôt make architectural sense.¬†
-------------------------------------------------------------------------------Robin EastSpark GraphX in Action¬†Michael Malak and Robin EastManning Publications Co.http://www.manning.com/books/spark-graphx-in-action





Dears, for one project, I need to implement something so Spark can read data from a C++ process. 
To provide high performance, I really hope to implement this through shared memory between the C++ process and Java JVM process.
It seems it may be possible to use named memory mapped files and JNI to do this, but I wonder whether there is any existing efforts or more efficient approach to do this?
Thank you very much!

Best Regards,
Jia


---------------------------------------------------------------------









   



   


 "
Disha Shrivastava <dishu.905@gmail.com>,"Tue, 8 Dec 2015 14:12:53 +0530",Data and Model Parallelism in MLPC,"dev@spark.apache.org, ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi,

I would like to know if the implementation of MLPC in the latest released
version of Spark ( 1.5.2 ) implements model parallelism and data
parallelism as done in the DistBelief model implemented by Google
http://static.googleusercontent.com/media/research.google.com/hi//archive/large_deep_networks_nips2012.pdf


Thanks And Regards,
Disha
"
"""wei.zhu@kaiyuandao.com"" <wei.zhu@kaiyuandao.com>","Tue, 8 Dec 2015 16:44:25 +0800",=?GB2312?B?u9i4tDogbWxpYiBjb21waWxhdGlvbiBlcnJvcnM=?=,"=?GB2312?B?1uzOsA==?= <wei.zhu@kaiyuandao.com>, 
	dev <dev@spark.apache.org>","probably it is because I ran ""./dev/change-scala-version.sh 2.11"" after importing these projects in intellij. I reimported these projects later. it works fine.  

closed for this thread. thanks



 
∑¢º˛»À£∫ wei.zhu@kaiyuandao.com
∑¢ÀÕ ±º‰£∫ 2015-12-07 16:43
 ’º˛»À£∫ dev
÷˜Ã‚£∫ mlib compilation errors
hi£¨ when I was compiling the mlib project in Intellij, it has the following errors. If I run mvn from command line, it works well. anyone came to the same issue? thanks



"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Tue, 8 Dec 2015 17:25:53 +0800",Failed to generate predicate Error when using dropna,dev <dev@spark.apache.org>,"spark version: spark-1.5.2-bin-hadoop2.6
python version: 2.7.9
os: ubuntu 14.04

code to reproduce error

# write.py

import pyspark
sc = pyspark.SparkContext()
sqlc = pyspark.SQLContext(sc)
df = sqlc.range(10)
df1 = df.withColumn('a', df['id'] * 2)
df1.write.partitionBy('id').parquet('./data')


# read.py

import pyspark
sc = pyspark.SparkContext()
sqlc = pyspark.SQLContext(sc)
df2 = sqlc.read.parquet('./data')
df2.dropna().count()


$ spark-submit write.py
$ spark-submit read.py

# error message

15/12/08 17:20:34 ERROR Filter: Failed to generate predicate, fallback to
interpreted org.apache.spark.sql.catalyst.errors.package$TreeNodeException:
Binding attribute, tree: a#0L
...

If write data without partitionBy, the error won't happen
any suggestion?
Thanks!

-- 
-- ÂºµÈõÖËªí
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 8 Dec 2015 12:01:14 +0000",Re: Fastest way to build Spark from scratch,,"
On 7 Dec 2015, at 19:07, Jakob Odersky <jodersky@gmail.com<mailto:jodersky@gmail.com>> wrote:

make-distribution and the second code snippet both create a distribution from a clean state. They therefore require that every source file be compiled and that takes time (you can maybe tweak some settings or use a newer compiler to gain some speed).

I'm inferring from your question that for your use-case deployment speed is a critical issue, furthermore you'd like to build Spark for lots of (every?) commit in a systematic way. In that case I would suggest you try using the second code snippet without the `clean` task and only resort to it if the build fails.

On my local machine, an assembly without a clean drops from 6 minutes to 2.

regards,
--Jakob

1. you can use zinc -where possible- to speed up scala compilations
2. you might also consider setting up a local jenkins VM, hooked to whatever git repo & branch you are working off, and have it do the builds and tests for you. Not so great for interactive dev,

finally, on the mac, the ""say"" command is pretty handy at letting you know when some work in a terminal is ready, so you can do the first-thing-in-the morning build-of-the-SNAPSHOTS

mvn install -DskipTests -Pyarn,hadoop-2.6 -Dhadoop.version=2.7.1; say moo

After that you can work on the modules you care about (via the -pl) option). That doesn't work if you are running on an EC2 instance though




On 23 November 2015 at 20:18, Nicholas Chammas <nicholas.chammas@gmail.com<mailto:nicholas.chammas@gmail.com>> wrote:

Say I want to build a complete Spark distribution against Hadoop 2.6+ as fast as possible from scratch.

This is what I‚Äôm doing at the moment:

./make-distribution.sh -T 1C -Phadoop-2.6


-T 1C instructs Maven to spin up 1 thread per available core. This takes around 20 minutes on an m3.large instance.

I see that spark-ec2, on the other hand, builds Spark as follows<https://github.com/amplab/spark-ec2/blob/a990752575cd8b0ab25731d7820a55c714798ec3/spark/init.sh#L21-L22> when you deploy Spark at a specific git commit:

sbt/sbt clean assembly
sbt/sbt publish-local


This seems slower than using make-distribution.sh, actually.

Is there a faster way to do this?

Nick

‚Äã


"
Reynold Xin <rxin@databricks.com>,"Tue, 8 Dec 2015 21:59:40 +0800",Re: Failed to generate predicate Error when using dropna,Chang Ya-Hsuan <sumtiogo@gmail.com>,"Can you create a JIRA ticket for this? Thanks.



n:
"
"""=?ISO-8859-1?B?dmVjdG9y?="" <799203320@qq.com>","Tue, 8 Dec 2015 21:58:51 +0800",Filte the null before InnerJoin to solve the problem of data skew,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","when i join two tables, i find a table has the problem of data skew, and the skewing value of the field is null. so i want to filte  the null before InnerJoin. like that


a.key is skewed and the skewing value is null


Change


""select * from a join b on a.key = b.key""


to


""select * from a join b on a.key = b.key and a.key is not null""


The idea is feasible ?"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 08 Dec 2015 17:16:43 +0000",Re: Fastest way to build Spark from scratch,Steve Loughran <stevel@hortonworks.com>,"Thanks for the tips, Jakob and Steve.

It looks like my original approach is the best for me since I'm installing
Spark on newly launched EC2 instances and can't take advantage of
incremental compilation.

Nick


2.
w
he
oo
m
c714798ec3/spark/init.sh#L21-L22>
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 8 Dec 2015 09:33:07 -0800",Re: Fastest way to build Spark from scratch,Nicholas Chammas <nicholas.chammas@gmail.com>,"@Nick, on a fresh EC2 instance a significant chunk of the initial build
time might be due to artifact resolution + downloading. Putting
pre-populated Ivy and Maven caches onto your EC2 machine could shave a
decent chunk of time off that first build.

m

g"
Stephen Boesch <javadba@gmail.com>,"Tue, 8 Dec 2015 09:54:48 -0800",Re: Fastest way to build Spark from scratch,Josh Rosen <joshrosen@databricks.com>,"I will echo Steve L's comment about having zinc running (with --nailed).
That provides at least a 2X speedup - sometimes without it spark simply
does not build for me.

2015-12-08 9:33 GMT-08:00 Josh Rosen <joshrosen@databricks.com>:

e
n
a
d
ry
to
o
ds
 moo
55c714798ec3/spark/init.sh#L21-L22>
"
Michael Armbrust <michael@databricks.com>,"Tue, 8 Dec 2015 10:31:41 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),Yin Huai <yhuai@databricks.com>,"An update: the vote fails due to the -1.   I'll post another RC as soon as
we've resolved these issues.  In the mean time I encourage people to
continue testing and post any problems they encounter here.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 08 Dec 2015 18:32:04 +0000",Re: Fastest way to build Spark from scratch,Josh Rosen <joshrosen@databricks.com>,"Interesting. As long as Spark's dependencies don't change that often, the
same caches could save ""from scratch"" build time over many months of Spark
development. Is that right?

:

e
n
a
d
ry
to
o
ds
 moo
55c714798ec3/spark/init.sh#L21-L22>
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 8 Dec 2015 19:06:01 +0000",RE: Data and Model Parallelism in MLPC,"Disha Shrivastava <dishu.905@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Disha,

Multilayer perceptron classifier in Spark implements data parallelism.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com]
Sent: Tuesday, December 08, 2015 12:43 AM
To: dev@spark.apache.org; Ulanov, Alexander
Subject: Data and Model Parallelism in MLPC

Hi,
I would like to know if the implementation of MLPC in the latest released version of Spark ( 1.5.2 ) implements model parallelism and data parallelism as done in the DistBelief model implemented by Google  http://static.googleusercontent.com/media/research.google.com/hi//archive/large_deep_networks_nips2012.pdf<http://static.googleusercontent.com/media/research.google.com/hi/archive/large_deep_networks_nips2012.pdf>
Thanks And Regards,
Disha
"
Disha Shrivastava <dishu.905@gmail.com>,"Wed, 9 Dec 2015 00:49:13 +0530",Re: Data and Model Parallelism in MLPC,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi Alexander,

Thanks for your response. Can you suggest ways to incorporate Model
Parallelism in MPLC? I am trying to do the same in Spark. I got hold of
your post
http://apache-spark-developers-list.1001551.n3.nabble.com/Model-parallelism-with-RDD-td13141.html
where you have divided the weight matrix into different worker machines. I
have two basic questions in this regard:

1. How to actually visualize/analyze and control how nodes of the neural
network/ weights are divided across different workers?

2. Is there any alternate way to achieve model parallelism for MPLC in
Spark? I believe we need to have some kind of synchronization and control
for the updation of weights shared across different workers during
backpropagation.

Looking forward for your views on this.

Thanks and Regards,
Disha


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 8 Dec 2015 19:59:56 +0000",RE: Data and Model Parallelism in MLPC,Disha Shrivastava <dishu.905@gmail.com>,"Hi Disha,

Which use case do you have in mind that would require model parallelism? It should have large number of weights, so it could not fit into the memory of a single machine. For example, multilayer perceptron topologies, that are used for speech recognition, have up to 100M of weights. Present hardware is capable of accommodating this in the main memory. That might be a problem for GPUs, but this is a different topic.

The straightforward way of model parallelism for fully connected neural networks is to distribute horizontal (or vertical) blocks of weight matrices across several nodes. That means that the input data has to be reproduced on all these nodes. The forward and the backward passes will require re-assembling the outputs and the errors on each of the nodes after each layer, because each of the node can produce only partial results since it holds a part of weights. According to my estimations, this is inefficient due to large intermediate traffic between the nodes and should be used only if the model does not fit in memory of a single machine. Another way of model parallelism would be to represent the network as the graph and use GraphX to write forward and back propagation. However, this option does not seem very practical to me.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com]
Sent: Tuesday, December 08, 2015 11:19 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Data and Model Parallelism in MLPC

Hi Alexander,
Thanks for your response. Can you suggest ways to incorporate Model Parallelism in MPLC? I am trying to do the same in Spark. I got hold of your post http://apache-spark-developers-list.1001551.n3.nabble.com/Model-parallelism-with-RDD-td13141.html where you have divided the weight matrix into different worker machines. I have two basic questions in this regard:
1. How to actually visualize/analyze and control how nodes of the neural network/ weights are divided across different workers?
2. Is there any alternate way to achieve model parallelism for MPLC in Spark? I believe we need to have some kind of synchronization and control for the updation of weights shared across different workers during backpropagation.
Looking forward for your views on this.
Thanks and Regards,
Disha

On Wed, Dec 9, 2015 at 12:36 AM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Hi Disha,

Multilayer perceptron classifier in Spark implements data parallelism.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com<mailto:dishu.905@gmail.com>]
Sent: Tuesday, December 08, 2015 12:43 AM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>; Ulanov, Alexander
Subject: Data and Model Parallelism in MLPC

Hi,
I would like to know if the implementation of MLPC in the latest released version of Spark ( 1.5.2 ) implements model parallelism and data parallelism as done in the DistBelief model implemented by Google  http://static.googleusercontent.com/media/research.google.com/hi//archive/large_deep_networks_nips2012.pdf<http://static.googleusercontent.com/media/research.google.com/hi/archive/large_deep_networks_nips2012.pdf>
Thanks And Regards,
Disha

"
Kostas Sakellis <kostas@cloudera.com>,"Tue, 8 Dec 2015 13:40:57 -0800",Re: A proposal for Spark 2.0,Sean Owen <sowen@cloudera.com>,"I'd also like to make it a requirement that Spark 2.0 have a stable
dataframe and dataset API - we should not leave these APIs experimental in
the 2.0 release. We already know of at least one breaking change we need to
make to dataframes, now's the time to make any other changes we need to
stabilize these APIs. Anything we can do to make us feel more comfortable
about the dataset and dataframe APIs before the 2.0 release?

I've also been thinking that in Spark 2.0, we might want to consider strict
classpath isolation for user applications. Hadoop 3 is moving in this
direction. We could, for instance, run all user applications in their own
classloader that only inherits very specific classes from Spark (ie. public
APIs). This will require user apps to explicitly declare their dependencies
as there won't be any accidental class leaking anymore. We do something
like this for *userClasspathFirst option but it is not as strict as what I
described. This is a breaking change but I think it will help with
eliminating weird classpath incompatibility issues between user
applications and Spark system dependencies.

Thoughts?

Kostas



"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Wed, 9 Dec 2015 12:21:36 +0800",Re: Failed to generate predicate Error when using dropna,Reynold Xin <rxin@databricks.com>,"https://issues.apache.org/jira/browse/SPARK-12231

this is my first time to create JIRA ticket.
is this ticket proper?
thanks


:
o
on:


-- 
-- ÂºµÈõÖËªí
"
Fengdong Yu <fengdongy@everstring.com>,"Wed, 9 Dec 2015 13:23:25 +0800",I filed SPARK-12233,dev@spark.apache.org,"Hi,

I filed an issue, please take a look:

https://issues.apache.org/jira/browse/SPARK-12233


It definitely can be reproduced.







"
kostas papageorgopoylos <p02096@gmail.com>,"Wed, 9 Dec 2015 10:13:21 +0200",Re: A proposal for Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"Hi Kostas

With regards to your *second* point. I believe that requiring from the user
apps to explicitly declare their dependencies is the most clear API
approach when it comes to classpath and classloading.

However what about the following API: *SparkContext.addJar(String
pathToJar)* . *Is this going to change or affected in someway?*
Currently i use spark 1.5.2 in a Java application and i have built a
utility class that finds the correct path of a Dependency
(myPathOfTheJarDependency=Something like SparkUtils.getJarFullPathFromClass
(EsSparkSQL.class, ""^elasticsearch-hadoop-2.2.0-beta1.*\\.jar$"");), Which
is not something beatiful but i can live with.

Then i  use *javaSparkContext.addJar(myPathOfTheJarDependency)* ; after i
have initiated the javaSparkContext. In that way i do not require my
SparkCluster to have configuration on the classpath of my application and i
explicitly define the dependencies during runtime of my app after each time
i initiate a sparkContext.
I would be happy and i believe many other users also if i could could
continue having the same or similar approach with regards to dependencies


Regards

2015-12-08 23:40 GMT+02:00 Kostas Sakellis <kostas@cloudera.com>:

"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 9 Dec 2015 18:01:12 +0900","Differences between Spark APIs for Hadoop 1.x and Hadoop 2.x in terms
 of performance, progress reporting and IO metrics.","user@spark.apache.org, dev@spark.apache.org","Hi all,

I am writing this email to both user-group and dev-group since this is
applicable to both.

I am now working on Spark XML datasource (
https://github.com/databricks/spark-xml).
This uses a InputFormat implementation which I downgraded to Hadoop 1.x for
version compatibility.

However, I found all the internal JSON datasource and others in Databricks
use Hadoop 2.x API dealing with TaskAttemptContextImpl by reflecting the
method for this because TaskAttemptContext is a class in Hadoop 1.x and an
interface in Hadoop 2.x.

So, I looked through the codes for some advantages for Hadoop 2.x API but I
couldn't.
I wonder if there are some advantages for using Hadoop 2.x API.

I understand that it is still preferable to use Hadoop 2.x APIs at least
for future differences but somehow I feel like it might not have to use
Hadoop 2.x by reflecting a method.

I would appreciate that if you leave a comment here
https://github.com/databricks/spark-xml/pull/14 as well as sending back a
reply if there is a good explanation

Thanks!
"
Fengdong Yu <fengdongy@everstring.com>,"Wed, 9 Dec 2015 17:20:28 +0800","Re: Differences between Spark APIs for Hadoop 1.x and Hadoop 2.x in terms of performance, progress reporting and IO metrics.",Hyukjin Kwon <gurwls223@gmail.com>,"I don‚Äôt think there is performance difference between 1.x API and 2.x API.

but it‚Äôs not a big issue for your change, only com.databricks.hadoop.mapreduce.lib.input.XmlInputFormat.java <https://github.com/databricks/spark-xml/blob/master/src/main/java/com/databricks/hadoop/mapreduce/lib/input/XmlInputFormat.java> need to change, right?

It‚Äôs not a big change to 2.x API. if you agree, I can do, but I cannot promise the time within one or two weeks because of my daily job.





applicable to both.
(https://github.com/databricks/spark-xml <https://github.com/databricks/spark-xml>).
1.x for version compatibility.
Databricks use Hadoop 2.x API dealing with TaskAttemptContextImpl by reflecting the method for this because TaskAttemptContext is a class in Hadoop 1.x and an interface in Hadoop 2.x.
but I couldn't.
least for future differences but somehow I feel like it might not have to use Hadoop 2.x by reflecting a method.
https://github.com/databricks/spark-xml/pull/14 <https://github.com/databricks/spark-xml/pull/14> as well as sending back a reply if there is a good explanation

"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 9 Dec 2015 18:28:12 +0900","Re: Differences between Spark APIs for Hadoop 1.x and Hadoop 2.x in
 terms of performance, progress reporting and IO metrics.",Fengdong Yu <fengdongy@everstring.com>,"Thank you for your reply!

I have already done the change locally. So for changing it would be fine.

I just wanted to be sure which way is correct.

 2.x API.
tabricks/hadoop/mapreduce/lib/input/XmlInputFormat.java>
annot
s
n
"
Cristian O <cristian.b.opris@googlemail.com>,"Wed, 9 Dec 2015 16:34:06 +0000",SQL language vs DataFrame API,dev@spark.apache.org,"Hi,

I was wondering what the ""official"" view is on feature parity between SQL
and DF apis. Docs are pretty sparse on the SQL front, and it seems that
some features are only supported at various times in only one of Spark SQL
dialect, HiveQL dialect and DF API. DF.cube(), DISTRIBUTE BY, CACHE LAZY
are some examples

Is there an explicit goal of having consistent support for all features in
both DF and SQL ?

Thanks,
Cristian
"
shane knapp <sknapp@berkeley.edu>,"Wed, 9 Dec 2015 09:55:51 -0800","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","reminder!  this is happening tomorrow morning.


---------------------------------------------------------------------


"
Chris Freeman <cfreeman@alteryx.com>,"Wed, 9 Dec 2015 18:11:22 +0000",Specifying Scala types when calling methods from SparkR,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey everyone,

I‚Äôm currently looking at ways to save out SparkML model objects from SparkR and I‚Äôve had some luck putting the model into an RDD and then saving the RDD as an Object File. Once it‚Äôs saved, I‚Äôm able to load it back in with something like:

sc.objectFile[LinearRegressionModel](‚Äúpath/to/model‚Äù)

I‚Äôd like to try and replicate this same process from SparkR using the JVM backend APIs (e.g. ‚ÄúcallJMethod‚Äù), but so far I haven‚Äôt been able to replicate my success and I‚Äôm guessing that it‚Äôs (at least in part) due to the necessity of specifying the type when calling the objectFile method.

Does anyone know if this is actually possible? For example, here‚Äôs what I‚Äôve come up with so far:

loadModel <- function(sc, modelPath) {
  modelRDD <- SparkR:::callJMethod(sc,
                                                            ""objectFile[PipelineModel]"",
                                modelPath,
        SparkR:::getMinPartitions(sc, NULL))
  return(modelRDD)
}

Any help is appreciated!

--
Chris Freeman

"
Michael Armbrust <michael@databricks.com>,"Wed, 9 Dec 2015 10:36:52 -0800",Re: SQL language vs DataFrame API,Cristian O <cristian.b.opris@googlemail.com>,"I think that it is generally good to have parity when the functionality is
useful.  However, in some cases various features are there just to maintain
compatibility with other system.  For example CACHE TABLE is eager because
Shark's cache table was.  df.cache() is lazy because Spark's cache is.
Does that mean that we need to add some eager caching mechanism to
dataframes to have parity?  Probably not, users can just call .count() if
they want to force materialization.

Regarding the differences between HiveQL and the SQLParser, I think we
should get rid of the SQL parser.  Its kind of a hack that I built just so
that there was some SQL story for people who didn't compile with Hive.
Moving forward, I'd like to see the distinction between the HiveContext and
SQLContext removed and we can standardize on a single parser.  For this
reason I'd be opposed to spending a lot of dev/reviewer time on adding
features there.


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 09 Dec 2015 18:42:21 +0000",Re: Fastest way to build Spark from scratch,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yeah, this is the same idea behind having Travis cache the ivy2 folder to
speed up builds. In Amplab Jenkins each individual build workspace has its
own individual Ivy cache which is preserved across build runs but which is
only used by one active run at a time in order to avoid SBT ivy lock
contention (this shouldn't be an issue in most environments though).
m>

k
ge
rce
s or
ots
ou
sort
lds
y
h
a55c714798ec3/spark/init.sh#L21-L22>
"
Xiao Li <gatorsmile@gmail.com>,"Wed, 9 Dec 2015 11:02:13 -0800",Re: SQL language vs DataFrame API,Michael Armbrust <michael@databricks.com>,"Hi, Michael,

Does that mean SqlContext will be built on HiveQL in the near future?

Thanks,

Xiao Li


2015-12-09 10:36 GMT-08:00 Michael Armbrust <michael@databricks.com>:

"
Michael Armbrust <michael@databricks.com>,"Wed, 9 Dec 2015 11:31:51 -0800",Re: SQL language vs DataFrame API,Xiao Li <gatorsmile@gmail.com>,"I don't plan to abandon HiveQL compatibility, but I'd like to see us move
towards something with more SQL compliance (perhaps just newer versions of
the HiveQL parser).  Exactly which parser will do that for us is under
investigation.


"
Xiao Li <gatorsmile@gmail.com>,"Wed, 9 Dec 2015 11:41:49 -0800",Re: SQL language vs DataFrame API,Michael Armbrust <michael@databricks.com>,"That sounds great! When it is decided, please let us know and we can add
more features and make it ANSI SQL compliant.

Thank you!

Xiao Li


2015-12-09 11:31 GMT-08:00 Michael Armbrust <michael@databricks.com>:

"
shane knapp <sknapp@berkeley.edu>,"Wed, 9 Dec 2015 11:56:01 -0800","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","here's the security advisory for the update:
https://wiki.jenkins-ci.org/display/SECURITY/Jenkins+Security+Advisory+2015-12-09


---------------------------------------------------------------------


"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 9 Dec 2015 12:45:45 -0800",DStream not initialized SparkException,dev@spark.apache.org,"hi,

I met following exception when the driver program tried to recover from
checkpoint, looks like the logic relies on zeroTime being set which doesn't
seem to happen here. am I missing anything or is it a bug in 1.4.1?

org.apache.spark.SparkException:
org.apache.spark.streaming.api.csharp.CSharpTransformed2DStream@161f0d27
has not been initialized
        at
org.apache.spark.streaming.dstream.DStream.isTimeValid(DStream.scala:321)
        at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)
        at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)
        at scala.Option.orElse(Option.scala:257)
        at
org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)
        at
org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)
        at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)
        at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)
        at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at
scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
        at
scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
        at
org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:120)
        at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$restart$4.apply(JobGenerator.scala:227)
        at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$restart$4.apply(JobGenerator.scala:222)
        at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at
scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at
org.apache.spark.streaming.scheduler.JobGenerator.restart(JobGenerator.scala:222)
        at
org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:92)
        at
org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:73)
        at
org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:596)
        at
org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:594)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.api.csharp.CSharpBackendHandler.handleMethodCall(CSharpBackendHandler.scala:145)
        at
org.apache.spark.api.csharp.CSharpBackendHandler.channelRead0(CSharpBackendHandler.scala:90)
        at
org.apache.spark.api.csharp.CSharpBackendHandler.channelRead0(CSharpBackendHandler.scala:25)
        at
io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at
io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
        at
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at
io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:163)
        at
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at
io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)
        at
io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)
        at
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
        at java.lang.Thread.run(Thread.java:724)
"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 9 Dec 2015 13:09:36 -0800",Re: let spark streaming sample come to stop,Bryan Cutler <cutlerb@gmail.com>,"I see, thanks a lot


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Wed, 9 Dec 2015 21:45:00 +0000",Cause of akka.pattern.AskTimeoutException,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

I have a spark process that is running successfully for few days then suddenly the process stop with following error.  Do we know the cause of this error and how to resolve it.

WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1c50c2a8,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)

Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@357039d9,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@357039d9,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@357039d9,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@357039d9,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@a45e872,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@a45e872,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@a45e872,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@a45e872,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5d0f05af,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5d0f05af,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5d0f05af,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5d0f05af,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6728837f,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6728837f,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6728837f,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6728837f,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@27314434,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@27314434,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@27314434,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@27314434,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@42b7b5f3,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@42b7b5f3,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@42b7b5f3,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@42b7b5f3,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dc790e1,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dc790e1,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dc790e1,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dc790e1,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@47969bc5,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@47969bc5,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@47969bc5,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@47969bc5,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@57cca1d5,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@57cca1d5,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@57cca1d5,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@57cca1d5,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@46ae2f22,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@46ae2f22,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@46ae2f22,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@46ae2f22,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6e7da9d8,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6e7da9d8,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6e7da9d8,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@6e7da9d8,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@cd4d4ac,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@cd4d4ac,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@cd4d4ac,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@cd4d4ac,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                ... 1 more
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1237dcce,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1237dcce,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1237dcce,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
akka.pattern.AskTimeoutException: Recipient[Actor[akka://sparkDriver/user/HeartbeatReceiver#122822265]] had already been terminated.
                at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:194)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
WARN : org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1237dcce,BlockManagerId(<driver>, localhost, 42507))]
                at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:209)
                at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
Caused by: akka."
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@45e5527e,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@45e5527e,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@45e5527e,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@45e5527e,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@737a14e,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@737a14e,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@737a14e,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@737a14e,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@543aff2a,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@543aff2a,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@543aff2a,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@543aff2a,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@d55fd64,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@d55fd64,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@d55fd64,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@d55fd64,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3759cf46,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3759cf46,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3759cf46,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3759cf46,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@629a9984,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@629a9984,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@629a9984,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@629a9984,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3ecaf226,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3ecaf226,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3ecaf226,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3ecaf226,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3db42d26,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3db42d26,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3db42d26,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@3db42d26,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@679c5979,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@679c5979,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@679c5979,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@679c5979,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5f328085,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5f328085,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5f328085,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@5f328085,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dbcf1a,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dbcf1a,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dbcf1a,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@4dbcf1a,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@784bd62b,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@784bd62b,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@784bd62b,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@784bd62b,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1d76884a,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1d76884a,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1d76884a,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@1d76884a,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@270e056,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@270e056,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@270e056,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@270e056,BlockManagerId(<driver>, localhost, 42507))]
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@37aab060,BlockManagerId(<driver>, localhost, 42507))] in 1 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@37aab060,BlockManagerId(<driver>, localhost, 42507))] in 2 attempts
WARN : org.apache.spark.util.AkkaUtils - Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@37aab060,BlockManagerId(<driver>, localhost, 42507))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = Heartbeat(<driver>,[Lscala.Tuple2;@37aab060,BlockManagerId(<driver>, localhost, 42507))]
"
Renyi Xiong <renyixiong0@gmail.com>,Wed"," 9 Dec 2015 15:32:08 -0800""",Re: DStream not initialized SparkException,dev@spark.apache.org,"following scala program throws same exception, I know people are running
streaming jobs against kafka, I must be missing something. any idea why?

package org.apache.spark.streaming.api.csharp

import java.util.HashMap

import kafka.serializer.{DefaultDecoder, Decoder, StringDecoder}

import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._
import org.apache.spark.SparkConf

object ScalaSML {
  def main(args: Array[String]) {

 val checkpointPath =
""hdfs://SparkMasterVIP.AdsOISCP-Sandbox-Ch1d.CH1D.ap.gbl/checkpoint/ScalaSML/HK2""
    val sparkConf = new SparkConf().setAppName(""ScalaSML"")
    val ssc = StreamingContext.getOrCreate(checkpointPath, () => {
  val context = new StreamingContext(sparkConf, Seconds(60))
  context.checkpoint(checkpointPath)
  context
  })

 val kafkaParams = Map(""metadata.broker.list"" ->  ""..."",
      ""auto.offset.reset"" -> ""largest"")

 val topics = Set(""topic"")

    val ds = KafkaUtils.createDirectStream[Array[Byte], Array[Byte],
DefaultDecoder, DefaultDecoder](ssc, kafkaParams, topics)
 ds.foreachRDD((rdd, time) => println(""Time: "" + time + "" Count: "" +
rdd.count()))

    ssc.start()
    ssc.awaitTermination()
  }
}

15/12/09 15:22:43 ERROR StreamingContext: Error starting the context,
marking it as stopped
org.apache.spark.SparkException:
org.apache.spark.streaming.kafka.DirectKafkaInputDStream@15ce2c0 has not
been initialized
        at
org.apache.spark.streaming.dstream.DStream.isTimeValid(DStream.scala:321)
        at
org.apache.spark.streaming.dstream.InputDStream.isTimeValid(InputDStream.scala:83)
        at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)
        at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)
        at scala.Option.orElse(Option.scala:257)
        at
org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)
        at
org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)
        at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)
        at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)
        at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at
scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
        at
scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
        at
org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:120)
        at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$restart$4.apply(JobGenerator.scala:227)
        at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$restart$4.apply(JobGenerator.scala:222)
        at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at
scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at
org.apache.spark.streaming.scheduler.JobGenerator.restart(JobGenerator.scala:222)
        at
org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:92)
        at
org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:73)
        at
org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:593)
        at
org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:591)
        at
org.apache.spark.streaming.api.csharp.ScalaSML$.main(ScalaSML.scala:48)
        at
org.apache.spark.streaming.api.csharp.ScalaSML.main(ScalaSML.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
        at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
        at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 9 Dec 2015 15:57:02 -0800",Re: DStream not initialized SparkException,dev@spark.apache.org,"never mind, one of my peers correct the driver program for me - all dstream
operations need to be within the scope of getOrCreate API


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 9 Dec 2015 16:20:36 -0800",Re: Specifying Scala types when calling methods from SparkR,Chris Freeman <cfreeman@alteryx.com>,"The SparkR callJMethod can only invoke methods as they show up in the
Java byte code. So in this case you'll need to check the SparkContext
byte code (with javap or something like that) to see how that method
looks. My guess is the type is passed in as a class tag argument, so
you'll need to do something like create a class tag for the
LinearRegressionModel and pass that in as the first or last argument
etc.

Thanks
Shivaram

:
rom SparkR
ing the RDD
ack in with
 the JVM
Äôt been able to
 in part) due to
s what I‚Äôve

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 9 Dec 2015 16:29:49 -0800",Re: SQL language vs DataFrame API,Cristian Opris <cristian.b.opris@gmail.com>,"Yeah, I would like to address any actual gaps in functionality that are
present.


"
Stephen Boesch <javadba@gmail.com>,"Wed, 9 Dec 2015 17:02:07 -0800",Re: SQL language vs DataFrame API,Michael Armbrust <michael@databricks.com>,"Is this a candidate for the version 1.X/2.0 split?

2015-12-09 16:29 GMT-08:00 Michael Armbrust <michael@databricks.com>:

"
"""Sun, Rui"" <rui.sun@intel.com>","Thu, 10 Dec 2015 01:59:37 +0000",RE: Specifying Scala types when calling methods from SparkR,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>, Chris Freeman
	<cfreeman@alteryx.com>","Hi,

Just use  """"objectFile"" instead of ""objectFile[PipelineModel]"" for callJMethod. You can take the objectFile() in context.R as example.

Since the SparkContext created in SparkR is actually a JavaSparkContext, there is no need to pass the implicit ClassTag.

-----Original Message-----
From: Shivaram Venkataraman [mailto:shivaram@eecs.berkeley.edu] 
Sent: Thursday, December 10, 2015 8:21 AM
To: Chris Freeman
Cc: dev@spark.apache.org
Subject: Re: Specifying Scala types when calling methods from SparkR

The SparkR callJMethod can only invoke methods as they show up in the Java byte code. So in this case you'll need to check the SparkContext byte code (with javap or something like that) to see how that method looks. My guess is the type is passed in as a class tag argument, so you'll need to do something like create a class tag for the LinearRegressionModel and pass that in as the first or last argument etc.

Thanks
Shivaram

On Wed, Dec 9, 2015 at 10:11 AM, Chris Freeman <cfreeman@alteryx.com> wrote:
> Hey everyone,
>
> I‚Äôm currently looking at ways to save out SparkML model objects from 
> SparkR and I‚Äôve had some luck putting the model into an RDD and then 
> saving the RDD as an Object File. Once it‚Äôs saved, I‚Äôm able to load it 
> back in with something like:
>
> sc.objectFile[LinearRegressionModel](‚Äúpath/to/model‚Äù)
>
> I‚Äôd like to try and replicate this same process from SparkR using the 
> JVM backend APIs (e.g. ‚ÄúcallJMethod‚Äù), but so far I haven‚Äôt been able 
> to replicate my success and I‚Äôm guessing that it‚Äôs (at least in part) 
> due to the necessity of specifying the type when calling the objectFile method.
>
> Does anyone know if this is actually possible? For example, here‚Äôs 
> what I‚Äôve come up with so far:
>
> loadModel <- function(sc, modelPath) {
>   modelRDD <- SparkR:::callJMethod(sc,
>
> ""objectFile[PipelineModel]"",
>                                 modelPath,
>         SparkR:::getMinPartitions(sc, NULL))
>   return(modelRDD)
> }
>
> Any help is appreciated!
>
> --
> Chris Freeman
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org

"
Jacek Laskowski <jacek@japila.pl>,"Thu, 10 Dec 2015 09:22:18 +0100",A bug in Spark standalone? Worker registration and deregistration,dev <dev@spark.apache.org>,"Hi,

While toying with Spark Standalone I've noticed the following messages
in the logs of the master:

INFO Master: Registering worker 192.168.1.6:59919 with 2 cores, 2.0 GB RAM
INFO Master: localhost:59920 got disassociated, removing it.
...
WARN Master: Removing worker-20151210090708-192.168.1.6-59919 because
we got no heartbeat in 60 seconds
INFO Master: Removing worker worker-20151210090708-192.168.1.6-59919
on 192.168.1.6:59919

Why does the message ""WARN Master: Removing
worker-20151210090708-192.168.1.6-59919 because we got no heartbeat in
60 seconds"" appear when the worker should've been gone already (as
pointed out in ""INFO Master: localhost:59920 got disassociated,
removing it."")?

Could it be that the ids are different - 192.168.1.6:59919 vs localhost:59920?

I started master using ""./sbin/start-master.sh -h localhost"" and the
workers ""./sbin/start-slave.sh spark://localhost:7077"".

p.s. Are such questions appropriate for this mailing list?

Pozdrawiam,
Jacek

--
Jacek Laskowski | https://medium.com/@jaceklaskowski/ |
http://blog.jaceklaskowski.pl
Mastering Spark https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 10 Dec 2015 10:26:15 +0000","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT",,"Note also that ASF Jira goes down at 19:00 UTC on friday, it *should* be back up ... the @infrabot twitter feed will the best channel for updates


---------- Forwarded message ----------
From: Daniel Takamori <pono@apache.org>
Date: 8 December 2015 at 18:59
Subject: Jira reboot 11-12-2015 at 19:00 UTC
To: operations@apache.org, infrastructure@apache.org


There will be a planned reboot of Jira on Friday 11th December at 19:00 UTC.

This is 72 hours notice as recommended in our Core Services planned downtime SLA.

Currently, Jira requires a reboot when adding new projects to it. There is an outstanding ticket with Atlassian about this. They require logs and so these will be gathered at the time of the planned reboot. 

Projects being added to Jira at this time will include:-

INFRA-10905 - New JIRA for Metron

and any more that get requested between now and downtime.

Any projects requiring issues to be imported from other issue trackers will NOT be done at this time.

A tweet via @infrabot will be tweeted 24 hrs and 1 hr before.
A planned maintenance notice will be posted on status.apache.org.

Actual downtime should be no more than 10 minutes all being well.

The next email about this will be after the service has resumed from the planned downtime.

Thanks!

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 10 Dec 2015 11:45:54 +0100",Re: A bug in Spark standalone? Worker registration and deregistration,Sasaki Kai <sasaki@treasure-data.com>,"Hi,

I'm on yesterday's master HEAD.

Pozdrawiam,
Jacek

--
Jacek Laskowski | https://medium.com/@jaceklaskowski/ |
http://blog.jaceklaskowski.pl
Mastering Spark https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski



---------------------------------------------------------------------


"
Inosh Goonewardena <inoshmrt@gmail.com>,"Thu, 10 Dec 2015 16:40:06 +0530",Re: releasing Spark 1.4.2,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I can see that there is 1.4.2 rc release prepared already[1], but wasn't
called the vote for some reason. Was there any particular reason behind for
stop the voting process? Just wondering whether we can get 1.4.2 released?

[1]
https://github.com/apache/spark/commit/0b22a3c7a3a40ff63a2e740ecab152141271b30d




"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 10 Dec 2015 19:49:13 +0800","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT",shane knapp <sknapp@berkeley.edu>,"Hi Shane,

I found that Jenkins has been in the status of ""Jenkins is going to shut
down"" for at least 4 hours (from ~23:30 Dec 9 to 3:45 Dec 10, PDT). Not
sure whether this is part of the schedule or related?

Cheng


"
JaeSung Jun <jaesjun@gmail.com>,"Fri, 11 Dec 2015 00:19:53 +1100","Does RDD[Type1, Iterable[Type2]] split into multiple partitions?",dev@spark.apache.org,"Hi,

I'm currently working on Iterable type of RDD, which is like :

val keyValueIterableRDD[CaseClass1, Iterable[CaseClass2]] = buildRDD(...)

If there is only one unique key and Iterable is big enough, would this
Iterable be partitioned across all executors like followings ?

(executor1)
(xxx, iterator from 0 to 10,000)

(executor2)
(xxx, iterator from 10,001 to 20,000)

(executor2)
(xxx, iterator from 20,001 to 30,000)

...

Thanks
Jason
"
Reynold Xin <rxin@databricks.com>,"Thu, 10 Dec 2015 21:29:18 +0800","Re: Does RDD[Type1, Iterable[Type2]] split into multiple partitions?",JaeSung Jun <jaesjun@gmail.com>,"No, since the signature itself limits it.



"
shane knapp <sknapp@berkeley.edu>,"Thu, 10 Dec 2015 06:35:24 -0800","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 10 Dec 2015 06:54:36 -0800","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","jenkins is done, but we'll also be updating the firewall.  this
shouldn't take very long and i'll let everyone know when we're done.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 10 Dec 2015 07:03:48 -0800","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","and we're done!  this was a quick one.  :)


---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 10 Dec 2015 17:04:27 +0200",Spark Streaming Kinesis - DynamoDB Streams compatability,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark users & devs

I was just wondering if anyone out there has interest in DynamoDB Streams (
http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html)
as an input source for Spark Streaming Kinesis?

Because DynamoDB Streams provides an adaptor client that works with the
KCL, making this work is fairly straightforward, but would require a little
bit of work to add it to Spark Streaming Kinesis as an option. It also
requires updating the AWS SDK version.

For those using AWS heavily, there are other ways of achieving the same
outcome indirectly, the easiest of which I've found so far is using AWS
Lambdas to read from the DynamoDB Stream, (optionally) transform the
events, and write to a Kinesis stream, allowing one to just use the
existing Spark integration. Still, I'd like to know if there is sufficient
interest or demand for this among the user base to work on a PR adding
DynamoDB Streams support to Spark.

(At the same time, the implementation details happen to provide an
opportunity to address https://issues.apache.org/jira/browse/SPARK-10969,
though not sure how much need there is for that either?)

N
"
shane knapp <sknapp@berkeley.edu>,"Thu, 10 Dec 2015 08:15:16 -0800","Re: [build system] jenkins downtime, thursday 12/10/15 7am PDT",Cheng Lian <lian.cs.zju@gmail.com>,"that probably was me, sorry.  i pulled up the rest api command on my
phone before i fell asleep and must have accidentally put jenkins in
to quiet mode.  sorry about that!


---------------------------------------------------------------------


"
Bryan Cutler <cutlerb@gmail.com>,"Thu, 10 Dec 2015 10:09:40 -0800",Re: A bug in Spark standalone? Worker registration and deregistration,Jacek Laskowski <jacek@japila.pl>,"Hi Jacek,

I also recently noticed those messages, and some others, and am wondering
if there is an issue.  I am also seeing the following when I have event
logging enabled.  The first application is submitted and executes fine, but
all subsequent attempts produce an error log, but the master fails to load
it.  Not sure if this is related to the messages you see, but I would also
like to know if others can reproduce.  Here are the logs

MASTER
15/12/09 21:19:10 INFO Master: Registering app Spark Pi
15/12/09 21:19:10 INFO Master: Registered app Spark Pi with ID
app-20151209211910-0001
15/12/09 21:19:10 INFO Master: Launching executor app-20151209211910-0001/0
on worker worker-20151209211739-***
15/12/09 21:19:14 INFO Master: Received unregister request from application
app-20151209211910-0001
15/12/09 21:19:14 INFO Master: Removing app app-20151209211910-0001
15/12/09 21:19:14 WARN Master: Application Spark Pi is still in progress,
it may be terminated abnormally.
15/12/09 21:19:14 WARN Master: No event logs found for application Spark Pi
in file:/home/bryan/git/spark/logs/.
15/12/09 21:19:14 INFO Master: localhost.localdomain:54174 got
disassociated, removing it.
15/12/09 21:19:14 WARN Master: Got status update for unknown executor
app-20151209211910-0001/0
15/12/09 21:21:59 WARN Master: Got status update for unknown executor
app-20151209211830-0000/0
15/12/09 21:22:00 INFO Master: localhost.localdomain:54163 got
disassociated, removing it.

WORKER
15/12/09 21:19:14 INFO Worker: Asked to kill executor
app-20151209211910-0001/0
15/12/09 21:19:14 INFO ExecutorRunner: Runner thread for executor
app-20151209211910-0001/0 interrupted
15/12/09 21:19:14 INFO ExecutorRunner: Killing process!
15/12/09 21:19:14 ERROR FileAppender: Error writing stream to file
/home/bryan/git/spark/work/app-20151209211910-0001/0/stderr
java.io.IOException: Stream closed
at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
at java.io.FilterInputStream.read(FilterInputStream.java:107)
at
org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
at
org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
at
org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
at
org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1730)
at
org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
15/12/09 21:19:14 INFO Worker: Executor app-20151209211910-0001/0 finished
with state KILLED exitStatus 143
15/12/09 21:19:14 INFO Worker: Cleaning up local directories for
application app-20151209211910-0001
15/12/09 21:19:14 INFO ExternalShuffleBlockResolver: Application
app-20151209211910-0001 removed, cleanupLocalDirs = true



"
Chris Freeman <cfreeman@alteryx.com>,"Thu, 10 Dec 2015 18:47:07 +0000",RE: Specifying Scala types when calling methods from SparkR,"""Sun, Rui"" <rui.sun@intel.com>, ""shivaram@eecs.berkeley.edu""
	<shivaram@eecs.berkeley.edu>","Hi Sun Rui,

I‚Äôve had some luck simply using ‚ÄúobjectFile‚Äù when saving from SparkR directly. The problem is that if you do it that way, the model object will only work if you continue to use the current Spark Context, and I think model persistence should really enable you to use the model at a later time. That‚Äôs where I found that I could drop down to the JVM level and interact with the Scala object directly, but that seems to only work if you specify the type.



On December 9, 2015 at 7:59:43 PM, Sun, Rui (rui.sun@intel.com<mailto:rui.sun@intel.com>) wrote:

Hi,

Just use """"objectFile"" instead of ""objectFile[PipelineModel]"" for callJMethod. You can take the objectFile() in context.R as example.

Since the SparkContext created in SparkR is actually a JavaSparkContext, there is no need to pass the implicit ClassTag.

-----Original Message-----
From: Shivaram Venkataraman [mailto:shivaram@eecs.berkeley.edu]
Sent: Thursday, December 10, 2015 8:21 AM
To: Chris Freeman
Cc: dev@spark.apache.org
Subject: Re: Specifying Scala types when calling methods from SparkR

The SparkR callJMethod can only invoke methods as they show up in the Java byte code. So in this case you'll need to check the SparkContext byte code (with javap or something like that) to see how that method looks. My guess is the type is passed in as a class tag argument, so you'll need to do something like create a class tag for the LinearRegressionModel and pass that in as the first or last argument etc.

Thanks
Shivaram

On Wed, Dec 9, 2015 at 10:11 AM, Chris Freeman <cfreeman@alteryx.com> wrote:
> Hey everyone,
>
> I‚Äôm currently looking at ways to save out SparkML model objects from
> SparkR and I‚Äôve had some luck putting the model into an RDD and then
> saving the RDD as an Object File. Once it‚Äôs saved, I‚Äôm able to load it
> back in with something like:
>
> sc.objectFile[LinearRegressionModel](‚Äúpath/to/model‚Äù)
>
> I‚Äôd like to try and replicate this same process from SparkR using the
> JVM backend APIs (e.g. ‚ÄúcallJMethod‚Äù), but so far I haven‚Äôt been able
> to replicate my success and I‚Äôm guessing that it‚Äôs (at least in part)
> due to the necessity of specifying the type when calling the objectFile method.
>
> Does anyone know if this is actually possible? For example, here‚Äôs
> what I‚Äôve come up with so far:
>
> loadModel <- function(sc, modelPath) {
> modelRDD <- SparkR:::callJMethod(sc,
>
> ""objectFile[PipelineModel]"",
> modelPath,
> SparkR:::getMinPartitions(sc, NULL))
> return(modelRDD)
> }
>
> Any help is appreciated!
>
> --
> Chris Freeman
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org

"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 10 Dec 2015 11:10:08 -0800",Re: A bug in Spark standalone? Worker registration and deregistration,Jacek Laskowski <jacek@japila.pl>,"Jacek, could you create a JIRA for it? I just reproduced it. It's a bug in
how Master handles the Worker disconnection.

Best Regards,
Shixiong Zhu

2015-12-10 2:45 GMT-08:00 Jacek Laskowski <jacek@japila.pl>:

"
Jacek Laskowski <jacek@japila.pl>,"Thu, 10 Dec 2015 20:23:33 +0100",Re: A bug in Spark standalone? Worker registration and deregistration,Shixiong Zhu <zsxwing@gmail.com>,"
Hi Shixiong,

I'm saved. Kept thinking I'm lost in the sources and see ghosts :-)

https://issues.apache.org/jira/browse/SPARK-12267

Pozdrawiam,
Jacek

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Dec 2015 12:59:29 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","We are getting close to merging patches for SPARK-12155
<https://issues.apache.org/jira/browse/SPARK-12155> and SPARK-12253
<https://issues.apache.org/jira/browse/SPARK-12253>.  I'll be cutting RC2
shortly after that.

Michael


"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Dec 2015 18:44:16 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Cutting RC2 now.


"
"""Sun, Rui"" <rui.sun@intel.com>","Fri, 11 Dec 2015 02:48:55 +0000",RE: Specifying Scala types when calling methods from SparkR,"Chris Freeman <cfreeman@alteryx.com>, ""shivaram@eecs.berkeley.edu""
	<shivaram@eecs.berkeley.edu>","Hi, Chris,

I know your point: objectFile and saveAsObjectFile pair in SparkR can only be used in SparkR context, as the content of RDD is assumed to be serialized R objects.

It‚Äôs fine to drop down to JVM level in the case the model is saved as objectFile in Scala, and load it in SparkR. But I don‚Äôt understand ‚Äúbut that seems to only work if you specify the type‚Äù, seems no need to specify type because of type erasure?

Did you try something like: convert the RDD to DataFrame, save it , and load it as a DataFrame in SparkR and then to RDD?

From: Chris Freeman [mailto:cfreeman@alteryx.com]
Sent: Friday, December 11, 2015 2:47 AM
To: Sun, Rui; shivaram@eecs.berkeley.edu
Cc: dev@spark.apache.org
Subject: RE: Specifying Scala types when calling methods from SparkR

Hi Sun Rui,

I‚Äôve had some luck simply using ‚ÄúobjectFile‚Äù when saving from SparkR directly. The problem is that if you do it that way, the model object will only work if you continue to use the current Spark Context, and I think model persistence should really enable you to use the model at a later time. That‚Äôs where I found that I could drop down to the JVM level and interact with the Scala object directly, but that seems to only work if you specify the type.



On December 9, 2015 at 7:59:43 PM, Sun, Rui (rui.sun@intel.com<mailto:rui.sun@intel.com>) wrote:
Hi,

Just use """"objectFile"" instead of ""objectFile[PipelineModel]"" for callJMethod. You can take the objectFile() in context.R as example.

Since the SparkContext created in SparkR is actually a JavaSparkContext, there is no need to pass the implicit ClassTag.

-----Original Message-----
From: Shivaram Venkataraman [mailto:shivaram@eecs.berkeley.edu]
Sent: Thursday, December 10, 2015 8:21 AM
To: Chris Freeman
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Specifying Scala types when calling methods from SparkR

The SparkR callJMethod can only invoke methods as they show up in the Java byte code. So in this case you'll need to check the SparkContext byte code (with javap or something like that) to see how that method looks. My guess is the type is passed in as a class tag argument, so you'll need to do something like create a class tag for the LinearRegressionModel and pass that in as the first or last argument etc.

Thanks
Shivaram

On Wed, Dec 9, 2015 at 10:11 AM, Chris Freeman <cfreeman@alteryx.com<mailto:cfreeman@alteryx.com>> wrote:
> Hey everyone,
>
> I‚Äôm currently looking at ways to save out SparkML model objects from
> SparkR and I‚Äôve had some luck putting the model into an RDD and then
> saving the RDD as an Object File. Once it‚Äôs saved, I‚Äôm able to load it
> back in with something like:
>
> sc.objectFile[LinearRegressionModel](‚Äúpath/to/model‚Äù)
>
> I‚Äôd like to try and replicate this same process from SparkR using the
> JVM backend APIs (e.g. ‚ÄúcallJMethod‚Äù), but so far I haven‚Äôt been able
> to replicate my success and I‚Äôm guessing that it‚Äôs (at least in part)
> due to the necessity of specifying the type when calling the objectFile method.
>
> Does anyone know if this is actually possible? For example, here‚Äôs
> what I‚Äôve come up with so far:
>
> loadModel <- function(sc, modelPath) {
> modelRDD <- SparkR:::callJMethod(sc,
>
> ""objectFile[PipelineModel]"",
> modelPath,
> SparkR:::getMinPartitions(sc, NULL))
> return(modelRDD)
> }
>
> Any help is appreciated!
>
> --
> Chris Freeman
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org> For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
"
Hyukjin Kwon <gurwls223@gmail.com>,"Fri, 11 Dec 2015 15:56:20 +0900",coalesce at DataFrame missing argument for shuffle.,dev@spark.apache.org,"Hi all,

I accidentally met coalesce() function and found this taking arguments
different for RDD and DataFrame.

It looks shuffle option is missing for DataFrame.

I understand repartition() exactly works as coalesce() with shuffling but
it looks a bit weird that the same functions take different argument which
can be easily done just by adding single argument.

Could anybody tell me if this is intendedly missing or not?

Thanks!
‚Äã
"
Lars Francke <lars.francke@gmail.com>,"Fri, 11 Dec 2015 08:40:34 +0100",JIRA: Wrong dates from imported JIRAs,"""user@spark.apache.org"" <dev@spark.apache.org>","Hi,

I've been digging into JIRA a bit and found a couple of old issues (~250)
and I just assume that they are all from the old JIRA.

Here's one example:

Old: <https://spark-project.atlassian.net/rest/api/2/issue/SPARK-378>
New: <https://issues.apache.org/jira/rest/api/2/issue/SPARK-378>

created"": ""0012-08-21T09:03:00.000-0800"",

That's quite impressive but wrong :)

That means when you sort all Apache JIRAs by creation date Spark comes
first: <
https://issues.apache.org/jira/issues/?jql=order%20By%20createdDate%20ASC&startIndex=250

The dates were already wrong in the source JIRA.

Now it seems as if those can be fixed using a CSV import. I still remember
how painful the initial import was but this looks relatively straight
forward <
https://confluence.atlassian.com/display/JIRAKB/How+to+change+the+issue+creation+date+using+CSV+import

If everyone's okay with it I'd raise it with INFRA (and would prepare the
necessary CSV file) but as I'm not a committer it'd be great if one/some of
the committers could give me a +1

Cheers,
Lars
"
Reynold Xin <rxin@databricks.com>,"Fri, 11 Dec 2015 16:05:26 +0800",Re: JIRA: Wrong dates from imported JIRAs,Lars Francke <lars.francke@gmail.com>,"Thanks for looking at this. Is it worth fixing? Is there a risk (although
small) that the re-import would break other things?

Most of those are done and I don't know how often people search JIRAs by
date across projects.


"
Reynold Xin <rxin@databricks.com>,"Fri, 11 Dec 2015 16:16:57 +0800",Re: coalesce at DataFrame missing argument for shuffle.,Hyukjin Kwon <gurwls223@gmail.com>,"I am not sure if we need it. The RDD API has way too many methods and
parameters. As you said, it is simply ""repartition"".



h
"
Fengdong Yu <fengdongy@everstring.com>,"Fri, 11 Dec 2015 16:17:12 +0800",A very Minor typo in the Spark paper,dev@spark.apache.org,"Hi,

I found a very minor typo in:
http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf


Page 4:
We  complement the data mining example in Section 2.2.1 with two iterative applications: logistic regression and PageRank.


I read back to section 2.2.1, there is no these two examples. actually, It should be Section 3.2.1 and 3.2.2

@Matei


Thanks




"
Lars Francke <lars.francke@gmail.com>,"Fri, 11 Dec 2015 09:54:39 +0100",Re: JIRA: Wrong dates from imported JIRAs,Reynold Xin <rxin@databricks.com>,"That's a good point. I assume there's always a small risk but it's at least
the documented way from Atlassian to change the creation date so I'd hope
it should be okay. I'd build the minimal CSV file.

I agree that probably not a lot of people are going to search across
projects but on the other hand it's a one-time fix and who knows how long
the Apache Jira is going to live :)


"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 11 Dec 2015 12:07:30 +0200",Re: Spark streaming with Kinesis broken?,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","cc'ing dev list

Ok, looks like when the KCL version was updated in
https://github.com/apache/spark/pull/8957, the AWS SDK version was not,
probably leading to dependency conflict, though as Burak mentions its hard
to debug as no exceptions seem to get thrown... I've tested 1.5.2 locally
and on my 1.5.2 EC2 cluster, and no data is received, and nothing shows up
in driver or worker logs, so any exception is getting swallowed somewhere.

Run starting. Expected test count is: 4
KinesisStreamSuite:
Using endpoint URL https://kinesis.eu-west-1.amazonaws.com for creating
Kinesis streams for tests.
- KinesisUtils API
- RDD generation
- basic operation *** FAILED ***
  The code passed to eventually never returned normally. Attempted 13 times
over 2.047777 minutes. Last failure message: Set() did not equal Set(5, 10,
1, 6, 9, 2, 7, 3, 8, 4)
  Data received does not match data sent. (KinesisStreamSuite.scala:188)
- failure recovery *** FAILED ***
  The code passed to eventually never returned normally. Attempted 63 times
over 2.0286383166666666 minutes. Last failure message: isCheckpointPresent
was true, but 0 was not greater than 10. (KinesisStreamSuite.scala:228)
Run completed in 5 minutes, 0 seconds.
Total number of tests run: 4
Suites: completed 1, aborted 0
Tests: succeeded 2, failed 2, canceled 0, ignored 0, pending 0
*** 2 TESTS FAILED ***
[INFO]
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO]
------------------------------------------------------------------------


KCL 1.3.0 depends on *1.9.37* SDK (
https://github.com/awslabs/amazon-kinesis-client/blob/1.3.0/pom.xml#L26)
while the Spark Kinesis dependency was kept at *1.9.16.*

I've run the integration tests on branch-1.5 (1.5.3-SNAPSHOT) with AWS SDK
1.9.37 and everything works.

Run starting. Expected test count is: 28
KinesisBackedBlockRDDSuite:
Using endpoint URL https://kinesis.eu-west-1.amazonaws.com for creating
Kinesis streams for tests.
- Basic reading from Kinesis
- Read data available in both block manager and Kinesis
- Read data available only in block manager, not in Kinesis
- Read data available only in Kinesis, not in block manager
- Read data available partially in block manager, rest in Kinesis
- Test isBlockValid skips block fetching from block manager
- Test whether RDD is valid after removing blocks from block anager
KinesisStreamSuite:
- KinesisUtils API
- RDD generation
- basic operation
- failure recovery
KinesisReceiverSuite:
- check serializability of SerializableAWSCredentials
- process records including store and checkpoint
- shouldn't store and checkpoint when receiver is stopped
- shouldn't checkpoint when exception occurs during store
- should set checkpoint time to currentTime + checkpoint interval upon
instantiation
- should checkpoint if we have exceeded the checkpoint interval
- shouldn't checkpoint if we have not exceeded the checkpoint interval
- should add to time when advancing checkpoint
- shutdown should checkpoint if the reason is TERMINATE
- shutdown should not checkpoint if the reason is something other than
TERMINATE
- retry success on first attempt
- retry success on second attempt after a Kinesis throttling exception
- retry success on second attempt after a Kinesis dependency exception
- retry failed after a shutdown exception
- retry failed after an invalid state exception
- retry failed after unexpected exception
- retry failed after exhausing all retries
Run completed in 3 minutes, 28 seconds.
Total number of tests run: 28
Suites: completed 4, aborted 0
Tests: succeeded 28, failed 0, canceled 0, ignored 0, pending 0
All tests passed.

So this is a regression in Spark Streaming Kinesis 1.5.2 - @Brian can you
file a JIRA for this?

@dev-list, since KCL brings in AWS SDK dependencies itself, is it necessary
to declare an explicit dependency on aws-java-sdk in the Kinesis POM? Also,
from KCL 1.5.0+, only the relevant components used from the AWS SDKs are
brought in, making things a bit leaner (this can be upgraded in Spark
1.7/2.0 perhaps). All local tests (and integration tests) pass with
removing the explicit dependency and only depending on KCL. Is aws-java-sdk
used anywhere else (AFAIK it is not, but in case I missed something let me
know any good reason to keep the explicit dependency)?

N




e
c86e6a3
m
as
ause
m
g
ax.net>
1
he
--
"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Fri, 11 Dec 2015 07:42:42 -0800 (PST)",Re: Spark streaming with Kinesis broken?,"""Brian London"" <brianmlondon@gmail.com>","Is that PR against master branch?




S3 read comes from Hadoop / jet3t afaik



‚Äî
Sent from Mailbox


the
for
com>
hard
locally
up
somewhere.
scala:188)


xml#L26)
SDK
you
Kinesis
 AWS
in
with
aws-java-sdk
me
com>
 the
fac86e6a3
 was
tests
com>
 cause
conflicts,
working
 ?
what
2?
me
earlier
----"
Ted Yu <yuzhihong@gmail.com>,"Sat, 12 Dec 2015 02:27:51 +0800",Maven build against Hadoop 2.4 times out,dev@spark.apache.org,"Hi,
You may have noticed that maven build against Hadoop 2.4 times out on Jenkins. 

The last module is spark-hive-thriftserver

This seemed to start with build #4440

FYI 
---------------------------------------------------------------------


"
Zhan Zhang <zzhang@hortonworks.com>,"Fri, 11 Dec 2015 18:46:05 +0000",Multi-core support per task in Spark,"user <user@spark.apache.org>, Dev <dev@spark.apache.org>","Hi Folks,

Is it possible to assign multiple core per task and how? Suppose we have some scenario, in which some tasks are really heavy processing each record and require multi-threading, and we want to avoid similar tasks assigned to the same executors/hosts. 

If it is not supported, does it make sense to add this feature. It may seems make user worry about more configuration, but by default we can still do 1 core per task and only advanced users need to be aware of this feature.

Thanks.

Zhan Zhang

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 11 Dec 2015 11:22:59 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Trying again now that eec36607
<https://github.com/apache/spark/commit/eec36607f9fc92b6c4d306e3930fcf03961625eb>
is
merged.


"
Zhan Zhang <zzhang@hortonworks.com>,"Fri, 11 Dec 2015 19:39:33 +0000",Re: Multi-core support per task in Spark,Zhan Zhang <zzhang@hortonworks.com>,"I noticed that it is configurable in job level spark.task.cpus.  Anyway to support on task level?

Thanks.

Zhan Zhang



some scenario, in which some tasks are really heavy processing each record and require multi-threading, and we want to avoid similar tasks assigned to the same executors/hosts. 
ems make user worry about more configuration, but by default we can still do 1 core per task and only advanced users need to be aware of this feature.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Sat, 12 Dec 2015 09:39:21 -0800",[VOTE] Release Apache Spark 1.6.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.0!

The vote is open until Tuesday, December 15, 2015 at 6:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is *v1.6.0-rc2
(23f8dfd45187cb8f2216328ab907ddb5fbdffd0b)
<https://github.com/apache/spark/tree/v1.6.0-rc2>*

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1169/

The test repository (versioned as v1.6.0-rc2) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1168/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-docs/

=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.

===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.


==================================================
== Major changes to help you focus your testing ==
==================================================

Spark 1.6.0 PreviewNotable changes since 1.6 RC1Spark Streaming

   - SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629>
   trackStateByKey has been renamed to mapWithState

Spark SQL

   - SPARK-12165 <https://issues.apache.org/jira/browse/SPARK-12165>
   SPARK-12189 <https://issues.apache.org/jira/browse/SPARK-12189> Fix bugs
   in eviction of storage memory by execution.
   - SPARK-12258 <https://issues.apache.org/jira/browse/SPARK-12258> correct
   passing null into ScalaUDF

Notable Features Since 1.5Spark SQL

   - SPARK-11787 <https://issues.apache.org/jira/browse/SPARK-11787> Parquet
   Performance - Improve Parquet scan performance when using flat schemas.
   - SPARK-10810 <https://issues.apache.org/jira/browse/SPARK-10810>
   Session Management - Isolated devault database (i.e USE mydb) even on
   shared clusters.
   - SPARK-9999  <https://issues.apache.org/jira/browse/SPARK-9999> Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000 <https://issues.apache.org/jira/browse/SPARK-10000> Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197 <https://issues.apache.org/jira/browse/SPARK-11197> SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745 <https://issues.apache.org/jira/browse/SPARK-11745> Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 <https://issues.apache.org/jira/browse/SPARK-10412>
Per-operator
   Metrics for SQL Execution - Display statistics on a peroperator basis
   for memory usage and spilled data size.
   - SPARK-11329 <https://issues.apache.org/jira/browse/SPARK-11329> Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 <https://issues.apache.org/jira/browse/SPARK-10917>,
   SPARK-11149 <https://issues.apache.org/jira/browse/SPARK-11149> In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111 <https://issues.apache.org/jira/browse/SPARK-11111> Fast
   null-safe joins - Joins using null-safe equality (<=>) will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389 <https://issues.apache.org/jira/browse/SPARK-11389> SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978 <https://issues.apache.org/jira/browse/SPARK-10978> Datasource
   API Avoid Double Filter - When implemeting a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849  <https://issues.apache.org/jira/browse/SPARK-4849> Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858  <https://issues.apache.org/jira/browse/SPARK-9858> Adaptive
   query execution - Intial support for automatically selecting the number
   of reducers for joins and aggregations.
   - SPARK-9241  <https://issues.apache.org/jira/browse/SPARK-9241> Improved
   query planner for queries having distinct aggregations - Query plans of
   distinct aggregations are more robust when distinct columns have high
   cardinality.

Spark Streaming

   - API Updates
      - SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629> New
      improved state management - mapWithState - a DStream transformation
      for stateful stream processing, supercedes updateStateByKey in
      functionality and performance.
      - SPARK-11198 <https://issues.apache.org/jira/browse/SPARK-11198> Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891 <https://issues.apache.org/jira/browse/SPARK-10891> Kinesis
      message handler function - Allows arbitraray function to be applied
      to a Kinesis record in the Kinesis receiver before to customize what data
      is to be stored in memory.
      - SPARK-6328  <https://issues.apache.org/jira/browse/SPARK-6328> Python
      Streamng Listener API - Get streaming statistics (scheduling delays,
      batch processing times, etc.) in streaming.


   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress
      bars.

MLlibNew algorithms/models

   - SPARK-8518  <https://issues.apache.org/jira/browse/SPARK-8518> Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834  <https://issues.apache.org/jira/browse/SPARK-9834> Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930  <https://issues.apache.org/jira/browse/SPARK-9930> New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517  <https://issues.apache.org/jira/browse/SPARK-6517> Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means

API improvements

   - ML Pipelines
      - SPARK-6725  <https://issues.apache.org/jira/browse/SPARK-6725> Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.ml algorithms
      - SPARK-5565  <https://issues.apache.org/jira/browse/SPARK-5565> LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836  <https://issues.apache.org/jira/browse/SPARK-9836> R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681  <https://issues.apache.org/jira/browse/SPARK-9681> Feature
      interactions in R formula - Interaction operator "":"" in R formula
   - Python API - Many improvements to Python API to approach feature parity

Misc improvements

   - SPARK-7685  <https://issues.apache.org/jira/browse/SPARK-7685>,
   SPARK-9642  <https://issues.apache.org/jira/browse/SPARK-9642> Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 <https://issues.apache.org/jira/browse/SPARK-10384>,
   SPARK-10385 <https://issues.apache.org/jira/browse/SPARK-10385> Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117 <https://issues.apache.org/jira/browse/SPARK-10117> LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751  <https://issues.apache.org/jira/browse/SPARK-7751> @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337 <https://issues.apache.org/jira/browse/SPARK-11337> Testable
   example code - Automated testing for code in user guide examples

Deprecations

   - In spark.mllib.clustering.KMeans, the ""runs"" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the ""weights"" field has been
   deprecated, in favor of the new name ""coefficients."" This helps
   disambiguate from instance (row) weights given to algorithms.

Changes of behavior

   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
   - Spark SQL's partition discovery has been changed to only discover
   partition directories that are children of the given path. (i.e. if
   path=""/my/data/x=1"" then x=1 will no longer be considered a partition
   but only children of x=1.) This behavior can be overridden by manually
   specifying the basePath that partitioning discovery should start with (
   SPARK-11678 <https://issues.apache.org/jira/browse/SPARK-11678>).
   - When casting a value of an integral type to timestamp (e.g. casting a
   long value to timestamp), the value is treated as being in seconds instead
   of milliseconds (SPARK-11724
   <https://issues.apache.org/jira/browse/SPARK-11724>).
   - With the improved query planner for queries having distinct
   aggregations (SPARK-9241
   <https://issues.apache.org/jira/browse/SPARK-9241>), the plan of a query
   having a single distinct aggregation has been changed to a more robust
   version. To switch back to the plan generated by Spark 1.5's planner,
   please set spark.sql.specializeSingleDistinctAggPlanning to true (
   SPARK-12077 <https://issues.apache.org/jira/browse/SPARK-12077>).
"
Michael Armbrust <michael@databricks.com>,"Sat, 12 Dec 2015 09:39:54 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'll kick off the voting with a +1.


"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Sat, 12 Dec 2015 18:44:28 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"-1

For me the docs are not displaying except for the first page, for example
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-docs/mllib-guide.html
is
a blank page.
This is because of SPARK-12199 <https://github.com/apache/spark/pull/101"
Michael Armbrust <michael@databricks.com>,"Sat, 12 Dec 2015 10:31:19 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Benjamin Fradet <benjamin.fradet@gmail.com>,"Thanks Ben, but as I said in the first email, docs are published separately
from the release, so this isn't a valid reason to down vote the RC.  We
just provide them to help with testing.

I'll ask the mllib guys to take a look at that patch though.

"
,"Sat, 12 Dec 2015 20:09:45 +0100",RE: [VOTE] Release Apache Spark 1.6.0 (RC2),"Michael Armbrust <michael@databricks.com>, ""dev@spark.apache.org""
 <dev@spark.apache.org>","
    
+1 (non binding)
Tested with different samples.
RegardsJB¬†


Sent from my Samsung device

-------- Original message --------
From: Michael Armbrust <michael@databricks.com> 
Date: 12/12/2015  18:39  (GMT+01:00) 
To: dev@spark.apache.org 
Subject: [VOTE] Release Apache Spark 1.6.0 (RC2) 

Please vote on releasing the following candidate as Apache Spark version 1.6.0!
The vote is open until Tuesday, December 15, 2015¬†at 6:00 UTC and passes if a majority¬†of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.0[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see¬†http://spark.apache.org/
The tag to be voted on is¬†v1.6.0-rc2 (23f8dfd45187cb8f2216328ab907ddb5fbdffd0b)
The release files, including signatures, digests, etc. can be found at:http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-bin/
Release artifacts are signed with the following key:https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:https://repository.apache.org/content/repositories/orgapachespark-1169/
The test repository (versioned as v1.6.0-rc2) for this release can be found at:https://repository.apache.org/content/repositories/orgapachespark-1168/
The documentation corresponding to this release can be found at:http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-docs/
========================================= How can I help test this release? =========================================If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.
================================================== What justifies a -1 vote for this release? ==================================================This vote is happening towards the end of the 1.6 QA period, so -1 votes should only occur for significant regressions from 1.5. Bugs already present in 1.5, minor regressions, or bugs related to new features will not block this release.
================================================================= What should happen to JIRA tickets still targeting 1.6.0? =================================================================1. It is OK for documentation patches to target 1.6.0 and still go into branch-1.6, since documentations will be published separately from the release.2. New features for non-alpha-modules should target 1.7+.3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target version.

==================================================== Major changes to help you focus your testing ====================================================
Spark 1.6.0 PreviewNotable changes since 1.6 RC1Spark StreamingSPARK-2629¬†¬†trackStateByKey¬†has been renamed to¬†mapWithStateSpark SQLSPARK-12165¬†SPARK-12189¬†Fix bugs in eviction of storage memory by execution.SPARK-12258¬†correct passing null into ScalaUDFNotable Features Since 1.5Spark SQLSPARK-11787¬†Parquet Performance¬†- Improve Parquet scan performance when using flat schemas.SPARK-10810¬†Session¬†Management¬†- Isolated devault database (i.e¬†USE mydb) even on shared clusters.SPARK-9999¬†¬†Dataset API¬†- A type-safe API (similar to RDDs) that performs many operations on serialized binary data and code generation (i.e. Project Tungsten).SPARK-10000¬†Unified Memory Management¬†- Shared memory for execution and caching instead of exclusive division of the regions.SPARK-11197¬†SQL Queries on Files¬†- Concise syntax for running SQL queries over files of any supported format without registering a table.SPARK-11745¬†Reading non-standard JSON files¬†- Added options to read non-standard JSON files (e.g. single-quotes, unquoted attributes)SPARK-10412¬†Per-operator Metrics for SQL Execution¬†- Display statistics on a peroperator basis for memory usage and spilled data size.SPARK-11329¬†Star (*) expansion for StructTypes¬†- Makes it easier to nest and unest arbitrary numbers of columnsSPARK-10917,¬†SPARK-11149¬†In-memory Columnar Cache Performance¬†- Significant (up to 14x) speed up when caching data that contains complex types in DataFrames or SQL.SPARK-11111¬†Fast null-safe joins¬†- Joins using null-safe equality (<=>) will now execute using SortMergeJoin instead of computing a cartisian product.SPARK-11389¬†SQL Execution Using Off-Heap Memory¬†- Support for configuring query execution to occur using off-heap memory to avoid GC overheadSPARK-10978¬†Datasource API Avoid Double Filter¬†- When implemeting a datasource with filter pushdown, developers can now tell Spark SQL to avoid double evaluating a pushed-down filter.SPARK-4849¬†¬†Advanced Layout of Cached Data¬†- storing partitioning and ordering schemes in In-memory table scan, and adding distributeBy and localSort to DF APISPARK-9858¬†¬†Adaptive query execution¬†- Intial support for automatically selecting the number of reducers for joins and aggregations.SPARK-9241¬†¬†Improved query planner for queries having distinct aggregations¬†- Query plans of distinct aggregations are more robust when distinct columns have high cardinality.Spark StreamingAPI UpdatesSPARK-2629¬†¬†New improved state management¬†-¬†mapWithState¬†- a DStream transformation for stateful stream processing, supercedes¬†updateStateByKey¬†in functionality and performance.SPARK-11198¬†Kinesis record deaggregation¬†- Kinesis streams have been upgraded to use KCL 1.4.0 and supports transparent deaggregation of KPL-aggregated records.SPARK-10891¬†Kinesis message handler function¬†- Allows arbitraray function to be applied to a Kinesis record in the Kinesis receiver before to customize what data is to be stored in memory.SPARK-6328¬†¬†Python Streamng Listener API¬†- Get streaming statistics (scheduling delays, batch processing times, etc.) in streaming.UI ImprovementsMade failures visible in the streaming tab, in the timelines, batch list, and batch details page.Made output operations visible in the streaming tab as progress bars.MLlibNew algorithms/modelsSPARK-8518¬†¬†Survival analysis¬†- Log-linear model for survival analysisSPARK-9834¬†¬†Normal equation for least squares¬†- Normal equation solver, providing R-like model summary statisticsSPARK-3147¬†¬†Online hypothesis testing¬†- A/B testing in the Spark Streaming frameworkSPARK-9930¬†¬†New feature transformers¬†- ChiSqSelector, QuantileDiscretizer, SQL transformerSPARK-6517¬†¬†Bisecting K-Means clustering¬†- Fast top-down clustering variant of K-MeansAPI improvementsML PipelinesSPARK-6725¬†¬†Pipeline persistence¬†- Save/load for ML Pipelines, with partial coverage of spark.ml algorithmsSPARK-5565¬†¬†LDA in ML Pipelines¬†- API for Latent Dirichlet Allocation in ML PipelinesR APISPARK-9836¬†¬†R-like statistics for GLMs¬†- (Partial) R-like stats for ordinary least squares via summary(model)SPARK-9681¬†¬†Feature interactions in R formula¬†- Interaction operator "":"" in R formulaPython API¬†- Many improvements to Python API to approach feature parityMisc improvementsSPARK-7685¬†,¬†SPARK-9642¬†¬†Instance weights for GLMs¬†- Logistic and Linear Regression can take instance weightsSPARK-10384,¬†SPARK-10385¬†Univariate and bivariate statistics in DataFrames¬†- Variance, stddev, correlations, etc.SPARK-10117¬†LIBSVM data source¬†- LIBSVM as a SQL data sourceDocumentation improvementsSPARK-7751¬†¬†@since versions¬†- Documentation includes initial version when classes and methods were addedSPARK-11337¬†Testable example code¬†- Automated testing for code in user guide examplesDeprecationsIn spark.mllib.clustering.KMeans, the ""runs"" parameter has been deprecated.In spark.ml.classification.LogisticRegressionModel and spark.ml.regression.LinearRegressionModel, the ""weights"" field has been deprecated, in favor of the new name ""coefficients."" This helps disambiguate from instance (row) weights given to algorithms.Changes of behaviorspark.mllib.tree.GradientBoostedTrees validationTol has changed semantics in 1.6. Previously, it was a threshold for absolute change in error. Now, it resembles the behavior of GradientDescent convergenceTol: For large errors, it uses relative error (relative to the previous error); for small errors (< 0.01), it uses absolute error.spark.ml.feature.RegexTokenizer: Previously, it did not convert strings to lowercase before tokenizing. Now, it converts to lowercase by default, with an option not to. This matches the behavior of the simpler Tokenizer transformer.Spark SQL's partition discovery has been changed to only discover partition directories that are children of the given path. (i.e. if¬†path=""/my/data/x=1""¬†then¬†x=1¬†will no longer be considered a partition but only children of¬†x=1.) This behavior can be overridden by manually specifying the¬†basePath¬†that partitioning discovery should start with (SPARK-11678).When casting a value of an integral type to timestamp (e.g. casting a long value to timestamp), the value is treated as being in seconds instead of milliseconds (SPARK-11724).With the improved query planner for queries having distinct aggregations (SPARK-9241), the plan of a query having a single distinct aggregation has been changed to a more robust version. To switch back to the plan generated by Spark 1.5's planner, please set¬†spark.sql.specializeSingleDistinctAggPlanning¬†to¬†true¬†(SPARK-12077).

"
Yin Huai <yhuai@databricks.com>,"Sat, 12 Dec 2015 12:55:02 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"+1

Critical and blocker issues of SQL have been addressed.


"
Sean Owen <sowen@cloudera.com>,"Sat, 12 Dec 2015 21:50:43 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"I've heard this argument before, but don't quite get it. Documentation is
part of a release, and I believe is something we're voting on here too, and
therefore needs to 'work' as documentation. We could not release this HTML
to the Apache site, so I think that does actually mean the artifacts
including docs don't work as a release.

Yes, I can see that the non-code artifacts can be released a little bit
after the code artifacts with last minute fixes. But, the whole release can
just happen later too. Why wouldn't this be a valid reason to block the
release?


"
Michael Armbrust <michael@databricks.com>,"Sat, 12 Dec 2015 15:17:21 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Sean Owen <sowen@cloudera.com>,"Sean, if you would like to -1 the release you are certainly entitled to,
but in the past we have never held a release for documentation only
issues.  If you'd like to change the policy of the project I'm not sure
that a voting thread is the right place to do it.

I think the right question here, is ""How are users going to be affected by
this temporary issue?"".  Given that I'm pretty certain that no users build
the documentation from the release themselves and instead consume it from
the published documentation, the docs contained in the release seem less
important as far as voting on the artifacts is concerned.

In contrast, there have been several threads on the users list asking when
the release is going to happen.  Should we make them wait longer for
something that isn't going to affect their usage of the release?  I would
vote no.  That doesn't mean that we shouldn't fix the documentation issue.
It just means we shouldn't add unnecessary coupling where it has no benefit.


"
Sean Owen <sowen@cloudera.com>,"Sat, 12 Dec 2015 23:48:53 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"(I can't -1 this.) I do agree that docs have been treated as if separate
from releases in the past. With more maturity in the release process, I'm
questioning that now, as I don't think it's normal. It would be a reason to
release or not release this particular tarball, so a vote thread is the
right place to discuss it.

I'm surprised you're suggesting there's not a coupling between a release's
code and the docs for that release. If a release happens and some time
later docs come out, that has some effect on people's usage. Surely, the
ideal is for docs for x.y to come from the bits for x.y, and thus are
available at the same time.

Reality is something else, and your argument is practical, that the release
is again behind and so shouldn't we overlook this minor problem to get it
out? This particular problem has to get fixed, soon, we agree. It's minor
by virtue of being hopefully temporary.

But if it can/will be fixed quickly, what's the hurry? I get it, people
want a releases sooner than later all else equal, but this is always true.
It'd be nice to talk about what behaviors have led to being behind schedule
and this perceived rush to finish now, since this same thing has happened
in 1.5, 1.4. I'd rather at least collect some opinions on it than
invalidate the question.


"
Michael Armbrust <michael@databricks.com>,"Sat, 12 Dec 2015 15:58:02 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Sean Owen <sowen@cloudera.com>,"
I'm only suggesting that we shouldn't delay testing of the actual bits, or
wait to iterate on another RC.  Ideally docs should come out with the
actual release announcement (and I'll do everything in my power to make
this happen).  The should also be updated regularly as small issues are
found.

But if it can/will be fixed quickly, what's the hurry? I get it, people

I'm happy to debate concrete process suggestions on another thread.
"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 12 Dec 2015 16:16:51 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,1
Yin Huai <yhuai@databricks.com>,"Sat, 12 Dec 2015 18:19:54 -0800",Re: Maven build against Hadoop 2.4 times out,Ted Yu <yuzhihong@gmail.com>,"Ted,

Looks like thrift server tests were just hanging. See
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/Spark-Master-Maven-with-YARN/4453/HADOOP_PROFILE=hadoop-2.4,label=spark-test/artifact/sql/hive-thriftserver/target/unit-tests.log.
If it is caused by a recent commit, it is also possible that a commit
listed in build 4438 or 4439 that caused it since 4438 and 4439 were failed
way before the thrift server tests.


"
Burak Yavuz <brkyvz@gmail.com>,"Sat, 12 Dec 2015 18:58:18 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Mark Hamstra <mark@clearstorydata.com>,"+1 tested SparkSQL and Streaming on some production sized workloads


"
Joseph Bradley <joseph@databricks.com>,"Sat, 12 Dec 2015 22:21:43 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1
Ran all tests locally on Mac OS X, and MLlib with large workloads on a
cluster.


"
Sean Owen <sowen@cloudera.com>,"Sun, 13 Dec 2015 07:50:49 +0000",Doc readiness vs releasability,Michael Armbrust <michael@databricks.com>,"-> new subject
I'll send my result of testing the RC separately


Certainly there is no reason to stop testing, like nobody would stop if any
other issue were uncovered. You argued against a (non-binding) -1 on the
grounds that doc problems aren't that im"
Ted Yu <yuzhihong@gmail.com>,"Sun, 13 Dec 2015 15:38:49 -0800",Re: Maven build against Hadoop 2.4 times out,Yin Huai <yhuai@databricks.com>,"Thanks for checking, Yin.

Looks like the cause might be in one of the commits for build #4438

Cheers


"
Yin Huai <yhuai@databricks.com>,"Sun, 13 Dec 2015 22:03:51 -0800",Re: Maven build against Hadoop 2.4 times out,Ted Yu <yuzhihong@gmail.com>,"Can you reproduce the problem in your local environment? Our 1.6 hadoop 2.4
maven build looks pretty good. Since our 1.6 is pretty close to master, I
am wondering if there is any environment related issue.


"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 14 Dec 2015 15:58:09 +0800",[SparkR] Any reason why saveDF's mode is append by default ?,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","It is inconsistent with scala api which is error by default. Any reason for
that ? Thanks



-- 
Best Regards

Jeff Zhang
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 14 Dec 2015 00:13:21 -0800",Re: [SparkR] Any reason why saveDF's mode is append by default ?,Jeff Zhang <zjffdu@gmail.com>,"I think its just a bug -- I think we originally followed the Python
API (in the original PR [1]) but the Python API seems to have been
changed to match Scala / Java in
https://issues.apache.org/jira/browse/SPARK-6366

Feel free to open a JIRA / PR for this.

Thanks
Shivaram

[1] https://github.com/amplab-extras/SparkR-pkg/pull/199/files


---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 14 Dec 2015 16:30:31 +0800",Re: [SparkR] Any reason why saveDF's mode is append by default ?,shivaram@eecs.berkeley.edu,"Thanks Shivaram, created https://issues.apache.org/jira/browse/SPARK-12318
I will work on it.





-- 
Best Regards

Jeff Zhang
"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Mon, 14 Dec 2015 11:26:51 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non binding)

Tested our workloads on a standalone cluster:
- Spark Core
- Spark SQL
- Spark MLlib
- Python API




"
Sean Owen <sowen@cloudera.com>,"Mon, 14 Dec 2015 14:43:05 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"With Java 7 / Ubuntu 15, and ""-Pyarn -Phadoop-2.6 -Phive
-Phive-thriftserver"", I still see the Docker tests fail every time. Is
anyone else seeing them fail (or running them)?

The Hive CliSuite also fails (stack trace at the bottom).

Same deal -- if people are running this test and it's not failing,
this is probably just flakiness of some form.

There's the aforementioned doc generation issue too.

Other than that it compiled and ran all tests for me.

JIRA score: 28 issues, of which 11 bugs, of which 5 critical (listed
below), of which 0 blockers. OK there.



Critical bugs:
SPARK-8447 Test external shuffle service with all shuffle managers
SPARK-10680 Flaky test:
network.RequestTimeoutIntegrationSuite.timeoutInactiveRequests
SPARK-11224 Flaky test: o.a.s.ExternalShuffleServiceSuite
SPARK-11266 Peak memory tests swallow failures
SPARK-11293 Spillable collections leak shuffle memory



- Simple commands *** FAILED ***
  =======================
  CliSuite failure output
  =======================
  Spark SQL CLI command line: ../../bin/spark-sql --master local
--driver-java-options -Dderby.system.durability=test --conf
spark.ui.enabled=false --hiveconf
javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/home/srowen/spark-1.6.0/sql/hive-thriftserver/target/tmp/spark-240e9e22-8fe8-408b-a116-2a894b3cbf1f;create=true
--hiveconf hive.metastore.warehouse.dir=/home/srowen/spark-1.6.0/sql/hive-thriftserver/target/tmp/spark-c336bc67-8e51-4284-b574-e8b79d0d4fce
--hiveconf hive.exec.scratchdir=/home/srowen/spark-1.6.0/sql/hive-thriftserver/target/tmp/spark-3a4f9564-d9f1-467f-8016-d4c95389e568
  Exception: java.util.concurrent.TimeoutException: Futures timed out
after [3 minutes]
  Executed query 0 ""CREATE TABLE hive_test(key INT, val STRING);"",
  But failed to capture expected output ""OK"" within 3 minutes.

  2015-12-14 13:47:23.07 - stderr> SLF4J: Class path contains multiple
SLF4J bindings.
  2015-12-14 13:47:23.07 - stderr> SLF4J: Found binding in
[jar:file:/home/srowen/spark-1.6.0/assembly/target/scala-2.10/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
  2015-12-14 13:47:23.07 - stderr> SLF4J: Found binding in
[jar:file:/home/srowen/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
  2015-12-14 13:47:23.07 - stderr> SLF4J: See
http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
  2015-12-14 13:47:23.074 - stderr> SLF4J: Actual binding is of type
[org.slf4j.impl.Log4jLoggerFactory]
  2015-12-14 13:47:39.36 - stdout> SET spark.sql.hive.version=1.2.1
  ===========================
  End CliSuite failure output
  =========================== (CliSuite.scala:151)





---------------------------------------------------------------------


"
Al Pivonka <alpivonka@gmail.com>,"Mon, 14 Dec 2015 10:22:05 -0500",Dev Environment (again),dev@spark.apache.org,"I've read through the mail archives and read the different threads.



I believe there is a great deal of value in teaching others.



I'm a 14yr vet of Java and would like to contribute to different Spark
projects. Here are my dilemmas:

1)    How does one quickly get a working environment up and running?



What do I mean by environment?


   1.  I have an IDE ‚Äì Not a problem I can build using sbt.
   2. Environment to me is a working standalone spark cluster (Docker)
   which I can take what I build from #1 and re-deploy to test out my changes
   etc.
   3. What are the dependencies between projects internal to Spark?



How to on-board a new developer and make them productive as soon as
possible.



Not looking for answers to just these questions/dilemmas.



There is a wealth of knowledge here (existing Spark & sub-projects
developers/Architects).



My proposal, is to document the onboarding process and dependencies for a
new contributor, what someone will need in order to get working dev
environment up and running for the purposes of being able to add/test new
functionality to the Spark project. Also document how to setup a test
environment in order to deploy and test out said new functionality
(Docker/Standalone).





Suggestions?
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 14 Dec 2015 07:42:40 -0800",Re: Maven build against Hadoop 2.4 times out,Yin Huai <yhuai@databricks.com>,"Attached was the tail of test suite output from local run.
I got test failure.

FYI



---------------------------------------------------------------------"
=?UTF-8?B?RMW+ZW5hbiBTb2Z0acSH?= <dzeno10@gmail.com>,"Mon, 14 Dec 2015 16:56:58 +0100",BIRCH clustering algorithm,dev@spark.apache.org,"Hi,

As a part of the project, we are trying to create parallel implementation
of BIRCH clustering algorithm [1]. We are mostly getting idea how to do it
from this paper, which used CUDA to make BIRCH parallel [2]. ([2] is short
paper, just section 4. is relevant).

We would like to implement BIRCH on Spark. Would this be an interesting
contribution for MLlib? Is there anyone already who tried to implement
BIRCH on Spark?

Any suggestions for implementation itself would be very much appreciated!


[1] http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
[2] http://boyuan.global-optimization.com/Mypaper/IDEAL2013-88.pdf


Best,
Dzeno
"
Michael Armbrust <michael@databricks.com>,"Mon, 14 Dec 2015 09:56:55 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Here are a fixed version of the docs for 1.6:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-docsfixed-docs

There still might be some minor rendering issues of the ML page, but people
are investigating.


"
shane knapp <sknapp@berkeley.edu>,"Mon, 14 Dec 2015 10:03:12 -0800",Re: Maven build against Hadoop 2.4 times out,"Ted Yu <yuzhihong@gmail.com>, Josh Rosen <joshrosen@databricks.com>","++joshrosen

This Is Known[tm], and we have a bug open against it:
https://issues.apache.org/jira/browse/SPARK-11823


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 14 Dec 2015 10:31:33 -0800",[build system] brief downtime right now,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","last week i forgot to downgrade R to 3.1.1, and since there's not much
activity right now, i'm going to take jenkins down and finish up the
ticket.

https://issues.apache.org/jira/browse/SPARK-11255

we should be back up and running within 30 minutes.

thanks!

shane

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Dec 2015 10:45:28 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"+1

Tested some dataframe operations on my Mac.


"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Mon, 14 Dec 2015 21:52:22 +0300",SparkML algos limitations question.,"user <user@spark.apache.org>, dev@spark.apache.org","Hello!

I'm currently working on POC and try to use Random Forest (classification
and regression). I also have to check SVM and Multiclass perceptron (other
algos are less important at the moment). So far I've discovered that Random
Forest has a limitation of maxDepth for trees and just out of curiosity I
wonder why such a limitation has been introduced?

An actual question is that I'm going to use Spark ML in production next
year and would like to know if there are other limitations like maxDepth in
RF for other algorithms: Logistic Regression, Perceptron, SVM, etc.

Thanks in advance for your time.
--
Be well!
Jean Morozov
"
Michael Segel <msegel_hadoop@hotmail.com>,"Mon, 14 Dec 2015 11:58:46 -0700",Secondary Indexing of RDDs?,dev@spark.apache.org,"Hi, 

This may be a silly question‚Ä¶ couldn‚Äôt find the answer on my own‚Ä¶ 

I‚Äôm trying to find out if anyone has implemented secondary indexing on Spark‚Äôs RDDs.

If anyone could point me to some references, it would be helpful. 

I‚Äôve seen some stuff on Succinct Spark (see: https://amplab.cs.berkeley.edu/succinct-spark-queries-on-compressed-rdds/ <https://amplab.cs.berkeley.edu/succinct-spark-queries-on-compressed-rdds/but was more interested in integration with SparkSQL and SparkSQL support for secondary indexing. 

Also the reason I‚Äôm posting this to the dev list is that there‚Äôs more to this question ‚Ä¶ 


Thx 

-Mike

"
Andrew Or <andrew@databricks.com>,"Mon, 14 Dec 2015 11:04:42 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Reynold Xin <rxin@databricks.com>,"+1

Ran PageRank on standalone mode with 4 nodes and noticed a speedup after
the specific commits that were in RC2 but not RC1:

c247b6a Dec 10 [SPARK-12155][SPARK-12253] Fix executor OOM in unified
memory management
05e441e Dec 9 [SPARK-12165][SPARK-1218"
shane knapp <sknapp@berkeley.edu>,"Mon, 14 Dec 2015 11:26:22 -0800",Re: [build system] brief downtime right now,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok, we're back up and building.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 14 Dec 2015 11:37:18 -0800",Re: [build system] brief downtime right now,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","after killing and restarting jenkins, things seem to be VERY slow.
i'm gonna kick jenkins again and see if that helps.




---------------------------------------------------------------------


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Tue, 15 Dec 2015 04:41:07 +0900",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"+1 (non-binding)

Tested some workloads using basic API and DataFrame API on my 4-nodes 
YARN cluster (1 master and 3 slaves).
I also tested the Web UI.

(I'm resending this mail just in case because it seems that I failed to 
send the mail to dev@)

"
shane knapp <sknapp@berkeley.edu>,"Mon, 14 Dec 2015 11:51:32 -0800",Re: [build system] brief downtime right now,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","something is up w/apache.  looking.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 14 Dec 2015 11:55:21 -0800",Re: [build system] brief downtime right now,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","...and we're back.  we were getting reverse proxy timeouts, which seem
to have been caused by jenkins churning and doing a lot of IO.  i'll
dig in to the logs and see if i can find out what happened.

weird.

shane


---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Mon, 14 Dec 2015 15:05:35 -0800",Re: [build system] brief downtime right now,shane knapp <sknapp@berkeley.edu>,"Hi Shane,

Seems Spark's lint-r started to fail from
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/Spark-Master-SBT/4260/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=spark-test/console.
Is it related to the upgrade work of R?

Thanks,

Yin


"
shane knapp <sknapp@berkeley.edu>,"Mon, 14 Dec 2015 15:12:43 -0800",Re: [build system] brief downtime right now,Yin Huai <yhuai@databricks.com>,"that looks like the lintr checks failed, causing the build to fail.


---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 14 Dec 2015 18:51:12 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"Guys,
   The sc.version gives 1.6.0-SNAPSHOT. Need to change to 1.6.0. Can you pl
verify ?
Cheers
<k/>


"
Nitin Goyal <nitin2goyal@gmail.com>,"Tue, 15 Dec 2015 08:57:00 +0530",Re: Secondary Indexing of RDDs?,Michael Segel <msegel_hadoop@hotmail.com>,"Spar SQL's in-memory cache stores statistics per column which in turn is
used to skip batches(default size 10000) within partition

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala#L25

Hope this helps

Thanks
-Nitin


 my own‚Ä¶
ng on
Äôs more to
"
chao chu <chuchao333@gmail.com>,"Tue, 15 Dec 2015 12:24:48 +0800","Re: Problem using User Defined Predicate pushdown with core RDD and
 parquet - UDP class not found","dev@spark.apache.org, user@spark.apache.org","+spark user mailing list

Hi there,

I have exactly the same problem as mentioned below. My current work around
is to add the jar containing my UDP in one of the system classpath (for
example, put it under the same path as
/opt/cloudera/parcels/CDH-5.4.2-"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 14 Dec 2015 21:31:26 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Krishna Sankar <ksankar42@gmail.com>,"I'm afraid you're correct, Krishna:

core/src/main/scala/org/apache/spark/package.scala:  val SPARK_VERSION =
""1.6.0-SNAPSHOT""
docs/_config.yml:SPARK_VERSION: 1.6.0-SNAPSHOT


"
Sachin Aggarwal <different.sachin@gmail.com>,"Tue, 15 Dec 2015 11:59:55 +0530",status of 2.11 support?,dev@spark.apache.org,"Hi,


adding question from user group to dev group need expert advice
please help us decide which version to choose for production as standard.

http://apache-spark-user-list.1001560.n3.nabble.com/Status-of-2-11-support-tp25362.html

thanks

-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Tue, 15 Dec 2015 17:23:27 +0800",spark with label nodes in yarn,<dev@spark.apache.org>,"Hi all,

 

Has anyone tried label based scheduling via spark on yarn? I've tried that,
it didn't work, spark 1.4.1 + apache hadoop 2.6.0

 

Any feedbacks are welcome.

 

Thanks

Allen

"
Ted Yu <yuzhihong@gmail.com>,"Tue, 15 Dec 2015 01:38:43 -0800",Re: spark with label nodes in yarn,=?utf-8?B?IuW8oOW/l+W8uijml7rovakpIg==?= <zzq98736@alibaba-inc.com>,"Please take a look at:
https://issues.apache.org/jira/browse/SPARK-7173

Cheers

ied that, it didn‚Äôt work, spark 1.4.1 + apache hadoop 2.6.0
"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Tue, 15 Dec 2015 17:55:00 +0800",Re: spark with label nodes in yarn,"""'Ted Yu'"" <yuzhihong@gmail.com>","Hi Ted,

 

Thanks for your quick response, but I think the link you gave it to me is more advanced feature.

Yes, I noticed SPARK-6470(https://issues.apache.org/jira/browse/SPARK-6470) 

And I just tried for this feature with spark 1.5.0, what happened to me was I was blocked to get the YARN containers by setting spark.yarn.executor.nodeLabelExpression property. My question, https://issues.apache.org/jira/browse/SPARK-7173 will fix this?

 

Thanks

Allen

 

 

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 17:39
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

Please take a look at:

https://issues.apache.org/jira/browse/SPARK-7173

 

Cheers


Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi all,

 

Has anyone tried label based scheduling via spark on yarn? I‚Äôve tried that, it didn‚Äôt work, spark 1.4.1 + apache hadoop 2.6.0

 

Any feedbacks are welcome.

 

Thanks

Allen

"
Ted Yu <yuzhihong@gmail.com>,"Tue, 15 Dec 2015 01:55:44 -0800",Re: status of 2.11 support?,Sachin Aggarwal <different.sachin@gmail.com>,"Please see related JIRA:
https://issues.apache.org/jira/browse/SPARK-8013

This question is better suited for user mailing list.

Thanks


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 15 Dec 2015 02:01:22 -0800",Re: spark with label nodes in yarn,=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?= <zzq98736@alibaba-inc.com>,"SPARK-6470 was integrated to 1.5.0 release.

Please use 1.5.0 or newer release.

SPARK-7173 <https://issues.apache.org/jira/browse/SPARK-7173> adds support
for setting ""spark.yarn.am.nodeLabelExpression""

Cheers


0)
• 17:39
Ω©)
ried
"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 15 Dec 2015 18:06:54 +0800",Re: spark with label nodes in yarn,=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?= <zzq98736@alibaba-inc.com>,"SPARK-6470 only supports node label expression for executors.
SPARK-7173 supports node label expression for AM (will be in 1.6).

If you want to schedule your whole application through label expression,
you have to configure both am and executor label expression. If you only
want to schedule executors through label expression, the executor
configuration is enough, but you have to make sure your cluster has some
nodes with no label.

You can refer to this document (
http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_yarn_resource_mgt/content/configuring_node_labels.html
).

Thanks
Saisai



0)
• 17:39
Ω©)
ried
"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Tue, 15 Dec 2015 18:20:44 +0800",Re: spark with label nodes in yarn,"""'Saisai Shao'"" <sai.sai.shao@gmail.com>","Hi SaiSai,

 

OK, it make sense to me , what I need is just to schedule the executors, AND I leave one nodemanager at least with no any labels.

 

It‚Äôs weird to me that YARN page shows my application is running, but actually it is still waiting for its executor

 

See the attached.

 

Thanks,

Allen

 

Âèë‰ª∂‰∫∫: Saisai Shao [mailto:sai.sai.shao@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 18:07
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: Ted Yu; dev
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

SPARK-6470 only supports node label expression for executors.

SPARK-7173 supports node label expression for AM (will be in 1.6).

 

If you want to schedule your whole application through label expression, you have to configure both am and executor label expression. If you only want to schedule executors through label expression, the executor configuration is enough, but you have to make sure your cluster has some nodes with no label.

 

You can refer to this document (http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_yarn_resource_mgt/content/configuring_node_labels.html).

 

Thanks

Saisai

 

 

Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi Ted,

 

Thanks for your quick response, but I think the link you gave it to me is more advanced feature.

Yes, I noticed SPARK-6470(https://issues.apache.org/jira/browse/SPARK-6470) 

And I just tried for this feature with spark 1.5.0, what happened to me was I was blocked to get the YARN containers by setting spark.yarn.executor.nodeLabelExpression property. My question, https://issues.apache.org/jira/browse/SPARK-7173 will fix this?

 

Thanks

Allen

 

 

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 17:39
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

Please take a look at:

https://issues.apache.org/jira/browse/SPARK-7173

 

Cheers


Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi all,

 

Has anyone tried label based scheduling via spark on yarn? I‚Äôve tried that, it didn‚Äôt work, spark 1.4.1 + apache hadoop 2.6.0

 

Any feedbacks are welcome.

 

Thanks

Allen

 


---------------------------------------------------------------------"
Ted Yu <yuzhihong@gmail.com>,"Tue, 15 Dec 2015 06:59:07 -0800",Re: spark with label nodes in yarn,=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?= <zzq98736@alibaba-inc.com>,"Please upgrade to Spark 1.5.x

1.4.1 didn't support node label feature.

Cheers


but
]
• 18:07
Ω©)
mgt/content/configuring_node_labels.html
0)
• 17:39
Ω©)
ried
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 15 Dec 2015 16:49:10 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Mark Hamstra <mark@clearstorydata.com>,"-1 (non-binding)

Cluster mode on Mesos is broken (regression compared to 1.5.2). It seems to
be related to the way SPARK_HOME is handled. In the driver logs I see:

I1215 15:00:39.411212 28032 exec.cpp:134] Version: 0.25.0
I1215 15:00:39.413512 28037 exe"
Michael Segel <msegel_hadoop@hotmail.com>,"Tue, 15 Dec 2015 14:49:44 -0700",Re: Secondary Indexing of RDDs?,Nitin Goyal <nitin2goyal@gmail.com>,"Hi, 

Not exactly what I was looking for‚Ä¶. 

Think more along the idea of indexes like an inverted table  where you have (K,V) ‚Äî> (V,K) transformation or a more ‚Äòtraditional‚Äô index like a B-Tree, R-Tree, etc ‚Ä¶ type of secondary indexing. 

To give you an example‚Ä¶ suppose you have a database of all of the insurance claims for automobile accidents and you wanted to find the average cost of fixing a front end collision to a car, and then group / average by make and model.

Having a couple of indexes would be helpful ‚Ä¶ e.g. an index on make/model along with an index on type of collision. 

RDDs aren‚Äôt indexed and there is Apache Ignite‚Ä¶ was looking for more ideas and to see what‚Äôs being baked in to sparkSQL. 

I mean does it make sense for Spark to store the relationship between data objects (RDDs) using a GraphX ‚Äòdatabase‚Äô ? (This could end up being very small) 
Has anyone looked at RDD modeling?  (Sorry, I‚Äôm still new to Spark and there‚Äôs a lot of people doing a lot of different things‚Ä¶)

Thx

is used to skip batches(default size 10000) within partition
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala#L25 <https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala#L25>
on my own‚Ä¶ 
indexing on Spark‚Äôs RDDs.
https://amplab.cs.berkeley.edu/succinct-spark-queries-on-compressed-rdds/ <https://amplab.cs.berkeley.edu/succinct-spark-queries-on-compressed-rdds/support for secondary indexing. 
there‚Äôs more to this question ‚Ä¶ 

"
Joseph Bradley <joseph@databricks.com>,"Tue, 15 Dec 2015 13:58:15 -0800",Re: BIRCH clustering algorithm,=?UTF-8?B?RMW+ZW5hbiBTb2Z0acSH?= <dzeno10@gmail.com>,"Hi Dzeno,

I'm not familiar with the algorithm myself, but if you have an important
use case for it, you could open a JIRA to discuss it.  However, if it is a
less common algorithm, I'd recommend first submitting it as a Spark package
(but publicizing the package on the user list).  If it gains traction, then
it could become a higher priority item for MLlib.

Thanks,
Joseph


t
t
"
Joseph Bradley <joseph@databricks.com>,"Tue, 15 Dec 2015 14:00:55 -0800",Re: SparkML algos limitations question.,Eugene Morozov <evgeny.a.morozov@gmail.com>,"Hi Eugene,

The maxDepth parameter exists because the implementation uses Integer node
IDs which correspond to positions in the binary tree.  This simplified the
implementation.  I'd like to eventually modify it to avoid depending on
tree node IDs, but that is not yet on the roadmap.

There is not an analogous limit for the GLMs you listed, but I'm not very
familiar with the perceptron implementation.

Joseph


"
Michael Armbrust <michael@databricks.com>,"Tue, 15 Dec 2015 14:40:47 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"This vote is canceled due to the issue with the incorrect version.  This
issue will be fixed by https://github.com/apache/spark/pull/10317

We can wait a little bit for a fix to
https://issues.apache.org/jira/browse/SPARK-12345.  However if it looks
like there is not an easy fix coming soon, I'm planning to move forward
with RC3.


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 15 Dec 2015 23:58:30 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC2),Michael Armbrust <michael@databricks.com>,"Thanks for the heads up.




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Tue, 15 Dec 2015 23:23:03 +0000","java.lang.NoSuchMethodError while saving a random forest model
 Spark version 1.5","""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I have recently upgraded spark version but when I try to run save a random forest model using model save command I am getting nosuchmethoderror.  My code works fine with 1.3x version.



model.save(sc.sc(), ""modelsavedir"");


ERROR: org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation - Aborting job.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 230, localhost): java.lang.NoSuchMethodError: parquet.schema.Types$GroupBuilder.addField(Lparquet/schema/Type;)Lparquet/schema/Types$BaseGroupBuilder;
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convertField$1.apply(CatalystSchemaConverter.scala:517)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convertField$1.apply(CatalystSchemaConverter.scala:516)
                at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
                at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
                at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter.convertField(CatalystSchemaConverter.scala:516)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter.convertField(CatalystSchemaConverter.scala:312)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convert$1.apply(CatalystSchemaConverter.scala:305)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convert$1.apply(CatalystSchemaConverter.scala:305)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.Iterator$class.foreach(Iterator.scala:727)
                at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
                at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
                at org.apache.spark.sql.types.StructType.foreach(StructType.scala:92)
                at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
                at org.apache.spark.sql.types.StructType.map(StructType.scala:92)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter.convert(CatalystSchemaConverter.scala:305)
                at org.apache.spark.sql.execution.datasources.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetTypesConverter.scala:58)
                at org.apache.spark.sql.execution.datasources.parquet.RowWriteSupport.init(ParquetTableSupport.scala:55)
                at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:287)
                at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:261)
                at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetRelation.scala:94)
                at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anon$3.newInstance(ParquetRelation.scala:272)
                at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:233)
                at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
                at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
                at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
                at org.apache.spark.scheduler.Task.run(Task.scala:88)
                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)

"
syepes <syepes@gmail.com>,"Tue, 15 Dec 2015 16:31:55 -0700 (MST)",=?UTF-8?Q?=E2=80=8BSpark_1.6_-_H=E2=80=8Bive_remote_metastore_not_working?=,dev@spark.apache.org,"‚ÄãHello,

I am testing out the 1.6 branch (#08aa3b4) and I have just noticed that
spark-shell ""HiveContext"" is no longer able to connect to my remote
metastore.
Using the same build options and configuration files with 1.5 (#0fdf554) it
works. 

Does anyone know if there have been any mayor changes on this component or
any new config that's needed to make this work?

spark-shell:
------------------------------
...
15/12/16 00:06:06 INFO Persistence: Property
hive.metastore.integral.jdo.pushdown unknown - will be ignored
15/12/16 00:06:06 INFO Persistence: Property datanucleus.cache.level2
unknown - will be ignored
15/12/16 00:06:06 WARN Connection: BoneCP specified but not present in
CLASSPATH (or one of dependencies)
15/12/16 00:06:06 WARN Connection: BoneCP specified but not present in
CLASSPATH (or one of dependencies)
15/12/16 00:06:08 INFO ObjectStore: Setting MetaStore object pin classes
with
hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Dat
abase,Type,FieldSchema,Order""
15/12/16 00:06:09 INFO Datastore: The class
""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as
""embedded-only"" so does not have its own datasto
re table.
15/12/16 00:06:09 INFO Datastore: The class
""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only""
so does not have its own datastore tab
le.
15/12/16 00:06:11 INFO Datastore: The class
""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as
""embedded-only"" so does not have its own datasto
re table.
15/12/16 00:06:11 INFO Datastore: The class
""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only""
so does not have its own datastore tab
le.
15/12/16 00:06:11 INFO MetaStoreDirectSql: Using direct SQL, underlying DB
is DERBY
15/12/16 00:06:11 INFO ObjectStore: Initialized ObjectStore
15/12/16 00:06:11 WARN ObjectStore: Version information not found in
metastore. hive.metastore.schema.verification is not enabled so recording
the schema versi
on 1.2.0
15/12/16 00:06:11 WARN ObjectStore: Failed to get database default,
returning NoSuchObjectException
15/12/16 00:06:11 INFO HiveMetaStore: Added admin role in metastore
..
..
15/12/16 00:06:12 INFO HiveContext: Initializing HiveMetastoreConnection
version 1.2.1 using Spark classes.
15/12/16 00:06:12 INFO ClientWrapper: Inspected Hadoop version: 2.7.1
15/12/16 00:06:12 INFO ClientWrapper: Loaded
org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.7.1
15/12/16 00:06:13 INFO metastore: Trying to connect to metastore with URI
thrift://remoteNode:9083
15/12/16 00:06:14 INFO metastore: Connected to metastore.
15/12/16 00:06:14 INFO SessionState: Created local directory:
/tmp/c3a4afbb-e4cf-4d20-85a0-01a53074efc8_resources
15/12/16 00:06:14 INFO SessionState: Created HDFS directory:
/tmp/hive/syepes/c3a4afbb-e4cf-4d20-85a0-01a53074efc8
15/12/16 00:06:14 INFO SessionState: Created local directory:
/tmp/root/c3a4afbb-e4cf-4d20-85a0-01a53074efc8
15/12/16 00:06:14 INFO SessionState: Created HDFS directory:
/tmp/hive/syepes/c3a4afbb-e4cf-4d20-85a0-01a53074efc8/_tmp_space.db
15/12/16 00:06:14 INFO SparkILoop: Created sql context (with Hive support)..
SQL context available as sqlContext.
..
------------------------------

hive-site.xml
---
<configuration>
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://remoteNode:9083</value>
  </property>
</configuration>
---


Regards and thanks in advance for any info,
Sebastian



--
3.nabble.com/Spark-1-6-H-ive-remote-metastore-not-working-tp15634.html
om.

---------------------------------------------------------------------


"
Judy Nash <judynash@exchange.microsoft.com>,"Wed, 16 Dec 2015 01:16:21 +0000",security testing on spark ? ,"User <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi all,

Does anyone know of any effort from the community on security testing spark clusters.
I.e.
Static source code analysis to find security flaws
Penetration testing to identify ways to compromise spark cluster
Fuzzing to crash spark

Thanks,
Judy

"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Wed, 16 Dec 2015 09:21:13 +0800",Re: spark with label nodes in yarn,"""'Ted Yu'"" <yuzhihong@gmail.com>","Oops...

 

I do use spark 1.5.0 and apache hadoop 2.6.0 (spark 1.4.1 + apache hadoop 2.6.0 is a typo), sorry

 

Thanks,

Allen

 

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 22:59
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: Saisai Shao; dev
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

Please upgrade to Spark 1.5.x

 

1.4.1 didn't support node label feature.

 

Cheers

 

Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi SaiSai,

 

OK, it make sense to me , what I need is just to schedule the executors, AND I leave one nodemanager at least with no any labels.

 

It‚Äôs weird to me that YARN page shows my application is running, but actually it is still waiting for its executor

 

See the attached.

 

Thanks,

Allen

 

Âèë‰ª∂‰∫∫: Saisai Shao [mailto:sai.sai.shao@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 18:07
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: Ted Yu; dev

‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

SPARK-6470 only supports node label expression for executors.

SPARK-7173 supports node label expression for AM (will be in 1.6).

 

If you want to schedule your whole application through label expression, you have to configure both am and executor label expression. If you only want to schedule executors through label expression, the executor configuration is enough, but you have to make sure your cluster has some nodes with no label.

 

You can refer to this document (http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_yarn_resource_mgt/content/configuring_node_labels.html).

 

Thanks

Saisai

 

 

Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi Ted,

 

Thanks for your quick response, but I think the link you gave it to me is more advanced feature.

Yes, I noticed SPARK-6470(https://issues.apache.org/jira/browse/SPARK-6470) 

And I just tried for this feature with spark 1.5.0, what happened to me was I was blocked to get the YARN containers by setting spark.yarn.executor.nodeLabelExpression property. My question, https://issues.apache.org/jira/browse/SPARK-7173 will fix this?

 

Thanks

Allen

 

 

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 17:39
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

Please take a look at:

https://issues.apache.org/jira/browse/SPARK-7173

 

Cheers


Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi all,

 

Has anyone tried label based scheduling via spark on yarn? I‚Äôve tried that, it didn‚Äôt work, spark 1.4.1 + apache hadoop 2.6.0

 

Any feedbacks are welcome.

 

Thanks

Allen

 

 

"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Wed, 16 Dec 2015 10:41:56 +0800",Re: spark with label nodes in yarn,"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>,
  ""'Ted Yu'"" <yuzhihong@gmail.com>","one more question , do I have to configure label for my capacity scheduler? is this mandatory?

 

Âèë‰ª∂‰∫∫: AllenZ [mailto:zzq98736@alibaba-inc.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà16Êó• 9:21
Êî∂‰ª∂‰∫∫: 'Ted Yu'
ÊäÑÈÄÅ: 'Saisai Shao'; 'dev'
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

Oops...

 

I do use spark 1.5.0 and apache hadoop 2.6.0 (spark 1.4.1 + apache hadoop 2.6.0 is a typo), sorry

 

Thanks,

Allen

 

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 22:59
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: Saisai Shao; dev
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

Please upgrade to Spark 1.5.x

 

1.4.1 didn't support node label feature.

 

Cheers

 

Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi SaiSai,

 

OK, it make sense to me , what I need is just to schedule the executors, AND I leave one nodemanager at least with no any labels.

 

It‚Äôs weird to me that YARN page shows my application is running, but actually it is still waiting for its executor

 

See the attached.

 

Thanks,

Allen

 

Âèë‰ª∂‰∫∫: Saisai Shao [mailto:sai.sai.shao@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 18:07
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: Ted Yu; dev

‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

SPARK-6470 only supports node label expression for executors.

SPARK-7173 supports node label expression for AM (will be in 1.6).

 

If you want to schedule your whole application through label expression, you have to configure both am and executor label expression. If you only want to schedule executors through label expression, the executor configuration is enough, but you have to make sure your cluster has some nodes with no label.

 

You can refer to this document (http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_yarn_resource_mgt/content/configuring_node_labels.html).

 

Thanks

Saisai

 

 

Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi Ted,

 

Thanks for your quick response, but I think the link you gave it to me is more advanced feature.

Yes, I noticed SPARK-6470(https://issues.apache.org/jira/browse/SPARK-6470) 

And I just tried for this feature with spark 1.5.0, what happened to me was I was blocked to get the YARN containers by setting spark.yarn.executor.nodeLabelExpression property. My question, https://issues.apache.org/jira/browse/SPARK-7173 will fix this?

 

Thanks

Allen

 

 

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com] 
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà15Êó• 17:39
Êî∂‰ª∂‰∫∫: Âº†ÂøóÂº∫(Êó∫ËΩ©)
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: spark with label nodes in yarn

 

Please take a look at:

https://issues.apache.org/jira/browse/SPARK-7173

 

Cheers


Âº†ÂøóÂº∫(Êó∫ËΩ©) 
Hi all,

 

Has anyone tried label based scheduling via spark on yarn? I‚Äôve tried that, it didn‚Äôt work, spark 1.4.1 + apache hadoop 2.6.0

 

Any feedbacks are welcome.

 

Thanks

Allen

 

 

"
Saisai Shao <sai.sai.shao@gmail.com>,"Wed, 16 Dec 2015 10:43:15 +0800",Re: spark with label nodes in yarn,=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?= <zzq98736@alibaba-inc.com>,"Yes, of course, capacity scheduler also needs to be configured.


• 9:21
• 22:59
Ω©)
but
]
• 18:07
Ω©)
mgt/content/configuring_node_labels.html
0)
• 17:39
Ω©)
ried
"
"""Allen Zhang"" <allenzhang010@126.com>","Wed, 16 Dec 2015 16:32:04 +0800 (CST)",does spark really support label expr like && or || ?,dev@spark.apache.org,"Hi All,


does spark label expression really support ""&&"" or ""||"" or even ""!"" for label based schedulering?
I tried that but it does NOT work.


Best Regards,
Allen

"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Wed, 16 Dec 2015 17:00:42 +0800",Re: does spark really support label expr like && or || ?,Allen Zhang <allenzhang010@126.com>,"are you trying to do dataframe boolean expression?
please use '&' for 'and', '|' for 'or', '~' for 'not' when building
DataFrame boolean expressions.

example:

DataFrame[id: bigint]





-- 
-- ÂºµÈõÖËªí
"
"""Allen Zhang"" <allenzhang010@126.com>","Wed, 16 Dec 2015 17:25:33 +0800 (CST)",Re:Re: does spark really support label expr like && or || ?,"""Chang Ya-Hsuan"" <sumtiogo@gmail.com>","yes, I've tried that as well. NOT work at all.


any feedback are welcome.
Thanks,
Allen






At 2015-12-16 17:00:42, ""Chang Ya-Hsuan"" <sumtiogo@gmail.com> wrote:

are you trying to do dataframe boolean expression?
please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.



example:


>>> df = sqlContext.range(10)
>>> df.where( (df.id==1) | ~(df.id==1))
DataFrame[id: bigint]




On Wed, Dec 16, 2015 at 4:32 PM, Allen Zhang <allenzhang010@126.com> wrote:

Hi All,


does spark label expression really support ""&&"" or ""||"" or even ""!"" for label based schedulering?
I tried that but it does NOT work.


Best Regards,
Allen







--

-- èà—≈‹é"
Dzeno <dzeno10@gmail.com>,"Wed, 16 Dec 2015 10:44:45 +0100",Re: BIRCH clustering algorithm,Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

Thank you for your tips. 

Thanks,
Dzeno



:
se case for it, you could open a JIRA to discuss it.  However, if it is a less common algorithm, I'd recommend first submitting it as a Spark package (but publicizing the package on the user list).  If it gains traction, then it could become a higher priority item for MLlib.
 of BIRCH clustering algorithm [1]. We are mostly getting idea how to do it from this paper, which used CUDA to make BIRCH parallel [2]. ([2] is short paper, just section 4. is relevant). 
ontribution for MLlib? Is there anyone already who tried to implement BIRCH on Spark? 
 
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 16 Dec 2015 02:24:08 -0800",Re: does spark really support label expr like && or || ?,Chang Ya-Hsuan <sumtiogo@gmail.com>,"Allen:
Since you mentioned scheduling, I assume you were talking about node label support in YARN. 
If that is the case, can you give us some more information:
How node labels are setup in YARN cluster
How you specified node labels in application
Hadoop and Spark releases you are using

Cheers

me boolean expressions.
e:
abel based schedulering?
"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Wed, 16 Dec 2015 15:16:26 +0300","A bug in Spark ML? NoSuchElementException while using RandomForest
 for regression.",dev@spark.apache.org,"Hi!

I've looked through issues and haven't found anything like that, so I've
created a new one. Everything to reproduce is attached to it:
https://issues.apache.org/jira/browse/SPARK-12367

Could you, please, take a look and if possible advice any workaround.
Thank you in advance.
--
Be well!
Jean Morozov
"
Aaron <aarongmldt@gmail.com>,"Wed, 16 Dec 2015 08:00:55 -0500","Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set for
 client mode",dev@spark.apache.org,"In going through running various Spark jobs, both Spark 1.5.2 and the
new Spark 1.6 SNAPSHOTs, on a Mesos cluster (currently 0.25), we
noticed that is in order to run the Spark shells (both python and
scala), we needed to set the LIBPROCESS_IP environment variable before
running.

Was curious if the Spark on Mesos docs should be updated, under the
Client Mode section, to include setting this environment variable?

Cheers
Aaron

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Wed, 16 Dec 2015 17:00:45 +0100","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",Aaron <aarongmldt@gmail.com>,"Hi Aaron,

I never had to use that variable. What is it for?




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Aaron <aarongmldt@gmail.com>,"Wed, 16 Dec 2015 11:04:16 -0500","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",dev@spark.apache.org,"The more I read/look into this..it's because the Spark Mesos Scheduler
resolves to something can't be reached (e.g. localhost).  So, maybe
this just needs to be added to the docs, if your host cannot or does
not resolve to the IP address you want.  Maybe just a footnote or
something?




---------------------------------------------------------------------


"
Aaron <aarongmldt@gmail.com>,"Wed, 16 Dec 2015 11:07:09 -0500","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Found this thread that talked about it to help understand it better:

https://mail-archives.apache.org/mod_mbox/mesos-user/201507.mbox/%3CCAJQ68qf9pEJgNWOmasM2DQchYaxPcAovnFKFgGgxXpZJ2JOAfQ@mail.gmail.com%3E


Cheers,
Aaron


---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Wed, 16 Dec 2015 17:09:28 +0100","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",Aaron <aarongmldt@gmail.com>,"LIBPROCESS_IP has zero hits in the Spark code base. This seems to be a
Mesos-specific setting.

Have you tried setting SPARK_LOCAL_IP?


8qf9pEJgNWOmasM2DQchYaxPcAovnFKFgGgxXpZJ2JOAfQ@mail.gmail.com%3E
t
e
t



-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Aaron <aarongmldt@gmail.com>,"Wed, 16 Dec 2015 11:11:21 -0500","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Basically, my hostname doesn't resolve to an ""accessible"" IP
address...which isn't a big deal, I normally set SPARK_LOCAL_IP when I
am doing things on a YARN cluster.  But, we've moved to a Mesos
Cluster recently, and had to track down when it wasn't working...I
assumed (badly obviously) that setting SPARK_LOCAL_IP was
sufficient...need to tell the Mesos scheduler as well I guess.

Not sure if you if would be a good idea to put in actual code,
something like:  ""when SPARK_LCOAL_IP is set, and using mesos:// as
the master..set LIBPROCESS_IP,"" but, some kind of documentation about
this possible issue would have saved me some time.

Cheers,
Aaron

8qf9pEJgNWOmasM2DQchYaxPcAovnFKFgGgxXpZJ2JOAfQ@mail.gmail.com%3E

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Wed, 16 Dec 2015 17:16:06 +0100","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",Aaron <aarongmldt@gmail.com>,"Sure, documenting this would be great, I just wanted to understand the
context. There is a related ticket: SPARK-5488
<https://issues.apache.org/jira/browse/SPARK-5488>.

Would you mind opening a PR?


8qf9pEJgNWOmasM2DQchYaxPcAovnFKFgGgxXpZJ2JOAfQ@mail.gmail.com%3E
lt
se
it
r
e



-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Timothy Chen <tnachen@gmail.com>,"Wed, 16 Dec 2015 08:33:57 -0800",Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set for client mode,=?utf-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Yes if want to manually override what IP to use to be contacted by the master you can set LIPROCESS_IP and LIBPROCESS_PORT.

It is a Mesos specific settings. We can definitely update the docs.

Note that in the future as we move to use the new Mesos Http API these configurations won't be needed (also libmesos!).

Tim

os-specific setting.
8qf9pEJgNWOmasM2DQchYaxPcAovnFKFgGgxXpZJ2JOAfQ@mail.gmail.com%3E
t
e
t


"
Aaron <aarongmldt@gmail.com>,"Wed, 16 Dec 2015 11:42:44 -0500","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",Timothy Chen <tnachen@gmail.com>,"Wrt to PR, sure, let me update the documentation, i'll send it out
shortly.  My Fork is on Github..is the PR from there ok?

Cheers,
Aaron

om>
68qf9pEJgNWOmasM2DQchYaxPcAovnFKFgGgxXpZJ2JOAfQ@mail.gmail.com%3E
lt
se
it
r
e

---------------------------------------------------------------------


"
Lars Francke <lars.francke@gmail.com>,"Wed, 16 Dec 2015 17:45:48 +0100",Re: JIRA: Wrong dates from imported JIRAs,Reynold Xin <rxin@databricks.com>,"Any other opinions on this?


"
Yin Huai <yhuai@databricks.com>,"Wed, 16 Dec 2015 11:02:27 -0800","=?UTF-8?Q?Re=3A_=E2=80=8BSpark_1=2E6_=2D_H=E2=80=8Bive_remote_metastore_not_work?=
	=?UTF-8?Q?ing?=",syepes <syepes@gmail.com>,"I see

15/12/16 00:06:13 INFO metastore: Trying to connect to metastore with URI
thrift://remoteNode:9083
15/12/16 00:06:14 INFO metastore: Connected to metastore.

Looks like you were connected to your remote metastore.


it
r
tition,Dat
B
-remote-metastore-not-working-tp15634.html
"
syepes <syepes@gmail.com>,"Wed, 16 Dec 2015 13:05:39 -0700 (MST)","=?UTF-8?Q?Re:_=E2=80=8BSpark_1.6_-_H=E2=80=8Bive_re?=
 =?UTF-8?Q?mote_metastore_not_working?=",dev@spark.apache.org,"Thanks for the reply.

The thing is that with 1.5 it never showed messages like the following:

15/12/16 00:06:11 INFO MetaStoreDirectSql: Using direct SQL, underlying DB
is DERBY
15/12/16 00:06:11 WARN ObjectStore: Failed to get database default,
returning NoSuchObjectException

This is a bit misleading.



--

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Wed, 16 Dec 2015 12:21:21 -0800","=?UTF-8?Q?Re=3A_=E2=80=8BSpark_1=2E6_=2D_H=E2=80=8Bive_remote_metastore_not_work?=
	=?UTF-8?Q?ing?=",syepes <syepes@gmail.com>,"oh i see. In your log, I guess you can find a line like ""Initializing
execution hive, version"". The line you showed is actually associated with
execution hive, which is a fake metastore that used by spark sql
internally. Logs related to the real metastore (the metastore storing table
metadata and etc.) starts from the line of ""Initializing
HiveMetastoreConnection version 1.2.1 using Spark classes.""

Hope this is helpful.






"
syepes <syepes@gmail.com>,"Wed, 16 Dec 2015 13:31:53 -0700 (MST)","=?UTF-8?Q?Re:_=E2=80=8BSpark_1.6_-_H=E2=80=8Bive_re?=
 =?UTF-8?Q?mote_metastore_not_working?=",dev@spark.apache.org,"Thanks for the the info.



--

---------------------------------------------------------------------


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Wed, 16 Dec 2015 21:01:28 +0000","RandomForestModel Save is throwing NoSuchMethodError with Spark
 Version 1.5x","""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","final RandomForestModel model = RandomForest.trainClassifier(trainrdd, NUM_CLASSES,
                          categoricalFeaturesInfo, NUM_TREES, FEATURESUBSETSTRATEGY, IMPURITY, MAX_TREE_DEPTH, MAX_BIN, SEED);
model.save(sc.sc(), MODEL_DIRECTORY);

I have just these two packages in my maven folder

<dependency>
                                    <groupId>org.apache.spark</groupId>
                                    <artifactId>spark-streaming-kafka_2.11</artifactId>
                                    <version>${spark.version}</version>
                        </dependency>
                        <dependency>
                                    <groupId>org.apache.spark</groupId>
                                    <artifactId>spark-mllib_2.11</artifactId>
                                    <version>${spark.version}</version>
                        </dependency>
            </dependencies>

Maven dependency Tree:

+- org.apache.spark:spark-streaming-kafka_2.11:jar:1.5.2:compile
[INFO] |  +- org.apache.kafka:kafka_2.11:jar:0.8.2.1:compile
[INFO] |  |  +- org.scala-lang.modules:scala-xml_2.11:jar:1.0.2:compile
[INFO] |  |  +- com.yammer.metrics:metrics-core:jar:2.2.0:compile
[INFO] |  |  +- org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.0.2:compile
[INFO] |  |  +- com.101tec:zkclient:jar:0.3:compile
[INFO] |  |  +- org.scala-lang:scala-library:jar:2.11.5:compile
[INFO] |  |  \- org.apache.kafka:kafka-clients:jar:0.8.2.1:compile
[INFO] |  \- org.spark-project.spark:unused:jar:1.0.0:compile
[INFO] \- org.apache.spark:spark-mllib_2.11:jar:1.5.2:compile
[INFO]    +- org.apache.spark:spark-core_2.11:jar:1.5.2:compile
[INFO]    |  +- org.apache.avro:avro-mapred:jar:hadoop2:1.7.7:compile
[INFO]    |  |  +- org.apache.avro:avro-ipc:jar:1.7.7:compile
[INFO]    |  |  |  \- org.apache.avro:avro:jar:1.7.7:compile
[INFO]    |  |  +- org.apache.avro:avro-ipc:jar:tests:1.7.7:compile
[INFO]    |  |  +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile
[INFO]    |  |  \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile
[INFO]    |  +- com.twitter:chill_2.11:jar:0.5.0:compile
[INFO]    |  |  \- com.esotericsoftware.kryo:kryo:jar:2.21:compile
[INFO]    |  |     +- com.esotericsoftware.reflectasm:reflectasm:jar:shaded:1.07:compile
[INFO]    |  |     +- com.esotericsoftware.minlog:minlog:jar:1.2:compile
[INFO]    |  |     \- org.objenesis:objenesis:jar:1.2:compile
[INFO]    |  +- com.twitter:chill-java:jar:0.5.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-common:jar:2.2.0:compile
[INFO]    |  |  |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO]    |  |  |  +- org.apache.commons:commons-math:jar:2.1:compile
[INFO]    |  |  |  +- xmlenc:xmlenc:jar:0.52:compile
[INFO]    |  |  |  +- commons-configuration:commons-configuration:jar:1.6:compile
[INFO]    |  |  |  |  +- commons-collections:commons-collections:jar:3.2.1:compile
[INFO]    |  |  |  |  +- commons-digester:commons-digester:jar:1.8:compile
[INFO]    |  |  |  |  |  \- commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO]    |  |  |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO]    |  |  |  +- org.apache.hadoop:hadoop-auth:jar:2.2.0:compile
[INFO]    |  |  |  \- org.apache.commons:commons-compress:jar:1.4.1:compile
[INFO]    |  |  |     \- org.tukaani:xz:jar:1.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-hdfs:jar:2.2.0:compile
[INFO]    |  |  |  \- org.mortbay.jetty:jetty-util:jar:6.1.26:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.2.0:compile
[INFO]    |  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.2.0:compile
[INFO]    |  |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.2.0:compile
[INFO]    |  |  |  |  \- org.apache.hadoop:hadoop-yarn-server-common:jar:2.2.0:compile
[INFO]    |  |  |  \- org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.2.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.2.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.2.0:compile
[INFO]    |  |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.2.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.2.0:compile
[INFO]    |  |  \- org.apache.hadoop:hadoop-annotations:jar:2.2.0:compile
[INFO]    |  +- org.apache.spark:spark-launcher_2.11:jar:1.5.2:compile
[INFO]    |  +- org.apache.spark:spark-network-common_2.11:jar:1.5.2:compile
[INFO]    |  +- org.apache.spark:spark-network-shuffle_2.11:jar:1.5.2:compile
[INFO]    |  +- org.apache.spark:spark-unsafe_2.11:jar:1.5.2:compile
[INFO]    |  +- net.java.dev.jets3t:jets3t:jar:0.7.1:compile
[INFO]    |  |  \- commons-httpclient:commons-httpclient:jar:3.1:compile
[INFO]    |  +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO]    |  |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO]    |  |  +- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO]    |  |  |  \- jline:jline:jar:0.9.94:compile
[INFO]    |  |  \- com.google.guava:guava:jar:14.0.1:compile
[INFO]    |  +- org.eclipse.jetty.orbit:javax.servlet:jar:3.0.0.v201112011016:compile
[INFO]    |  +- org.apache.commons:commons-lang3:jar:3.3.2:compile
[INFO]    |  +- org.slf4j:jul-to-slf4j:jar:1.7.10:compile
[INFO]    |  +- org.slf4j:jcl-over-slf4j:jar:1.7.10:compile
[INFO]    |  +- com.ning:compress-lzf:jar:1.0.3:compile
[INFO]    |  +- org.xerial.snappy:snappy-java:jar:1.1.1.7:compile
[INFO]    |  +- net.jpountz.lz4:lz4:jar:1.3.0:compile
[INFO]    |  +- org.roaringbitmap:RoaringBitmap:jar:0.4.5:compile
[INFO]    |  +- commons-net:commons-net:jar:2.2:compile
[INFO]    |  +- com.typesafe.akka:akka-remote_2.11:jar:2.3.11:compile
[INFO]    |  |  +- com.typesafe.akka:akka-actor_2.11:jar:2.3.11:compile
[INFO]    |  |  |  \- com.typesafe:config:jar:1.2.1:compile
[INFO]    |  |  +- io.netty:netty:jar:3.8.0.Final:compile
[INFO]    |  |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile
[INFO]    |  |  \- org.uncommons.maths:uncommons-maths:jar:1.2.2a:compile
[INFO]    |  +- com.typesafe.akka:akka-slf4j_2.11:jar:2.3.11:compile
[INFO]    |  +- org.json4s:json4s-jackson_2.11:jar:3.2.10:compile
[INFO]    |  |  \- org.json4s:json4s-core_2.11:jar:3.2.10:compile
[INFO]    |  |     +- org.json4s:json4s-ast_2.11:jar:3.2.10:compile
[INFO]    |  |     \- org.scala-lang:scalap:jar:2.11.0:compile
[INFO]    |  |        \- org.scala-lang:scala-compiler:jar:2.11.0:compile
[INFO]    |  +- com.sun.jersey:jersey-server:jar:1.9:compile
[INFO]    |  |  \- asm:asm:jar:3.1:compile
[INFO]    |  +- com.sun.jersey:jersey-core:jar:1.9:compile
[INFO]    |  +- org.apache.mesos:mesos:jar:shaded-protobuf:0.21.1:compile
[INFO]    |  +- io.netty:netty-all:jar:4.0.29.Final:compile
[INFO]    |  +- com.clearspring.analytics:stream:jar:2.7.0:compile
[INFO]    |  +- io.dropwizard.metrics:metrics-core:jar:3.1.2:compile
[INFO]    |  +- io.dropwizard.metrics:metrics-jvm:jar:3.1.2:compile
[INFO]    |  +- io.dropwizard.metrics:metrics-json:jar:3.1.2:compile
[INFO]    |  +- io.dropwizard.metrics:metrics-graphite:jar:3.1.2:compile
[INFO]    |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.4.4:compile
[INFO]    |  |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.4.0:compile
[INFO]    |  |  \- com.fasterxml.jackson.core:jackson-core:jar:2.4.4:compile
[INFO]    |  +- com.fasterxml.jackson.module:jackson-module-scala_2.11:jar:2.4.4:compile
[INFO]    |  |  +- org.scala-lang:scala-reflect:jar:2.11.2:compile
[INFO]    |  |  \- com.thoughtworks.paranamer:paranamer:jar:2.6:compile
[INFO]    |  +- org.apache.ivy:ivy:jar:2.4.0:compile
[INFO]    |  +- oro:oro:jar:2.0.8:compile
[INFO]    |  +- org.tachyonproject:tachyon-client:jar:0.7.1:compile
[INFO]    |  |  +- commons-lang:commons-lang:jar:2.4:compile
[INFO]    |  |  +- commons-io:commons-io:jar:2.4:compile
[INFO]    |  |  +- org.apache.curator:curator-client:jar:2.1.0-incubating:compile
[INFO]    |  |  +- org.tachyonproject:tachyon-underfs-hdfs:jar:0.7.1:compile
[INFO]    |  |  \- org.tachyonproject:tachyon-underfs-local:jar:0.7.1:compile
[INFO]    |  +- net.razorvine:pyrolite:jar:4.4:compile
[INFO]    |  \- net.sf.py4j:py4j:jar:0.8.2.1:compile
[INFO]    +- org.apache.spark:spark-streaming_2.11:jar:1.5.2:compile
[INFO]    +- org.apache.spark:spark-sql_2.11:jar:1.5.2:compile
[INFO]    |  +- org.apache.spark:spark-catalyst_2.11:jar:1.5.2:compile
[INFO]    |  |  \- org.codehaus.janino:janino:jar:2.7.8:compile
[INFO]    |  |     \- org.codehaus.janino:commons-compiler:jar:2.7.8:compile
[INFO]    |  +- org.apache.parquet:parquet-column:jar:1.7.0:compile
[INFO]    |  |  +- org.apache.parquet:parquet-common:jar:1.7.0:compile
[INFO]    |  |  \- org.apache.parquet:parquet-encoding:jar:1.7.0:compile
[INFO]    |  |     \- org.apache.parquet:parquet-generator:jar:1.7.0:compile
[INFO]    |  \- org.apache.parquet:parquet-hadoop:jar:1.7.0:compile
[INFO]    |     +- org.apache.parquet:parquet-format:jar:2.3.0-incubating:compile
[INFO]    |     \- org.apache.parquet:parquet-jackson:jar:1.7.0:compile
[INFO]    +- org.apache.spark:spark-graphx_2.11:jar:1.5.2:compile
[INFO]    |  +- com.github.fommil.netlib:core:jar:1.1.2:compile
[INFO]    |  \- net.sourceforge.f2j:arpack_combined_all:jar:0.1:compile
[INFO]    +- org.scalanlp:breeze_2.11:jar:0.11.2:compile
[INFO]    |  +- org.scalanlp:breeze-macros_2.11:jar:0.11.2:compile
[INFO]    |  +- net.sf.opencsv:opencsv:jar:2.3:compile
[INFO]    |  +- com.github.rwl:jtransforms:jar:2.4.0:compile
[INFO]    |  \- org.spire-math:spire_2.11:jar:0.7.4:compile
[INFO]    |     \- org.spire-math:spire-macros_2.11:jar:0.7.4:compile
[INFO]    +- org.apache.commons:commons-math3:jar:3.4.1:compile
[INFO]    \- org.jpmml:pmml-model:jar:1.1.15:compile
[INFO]       +- org.jpmml:pmml-agent:jar:1.1.15:compile
[INFO]       +- org.jpmml:pmml-schema:jar:1.1.15:compile
[INFO]       \- com.sun.xml.bind:jaxb-impl:jar:2.2.7:compile
[INFO]          \- com.sun.xml.bind:jaxb-core:jar:2.2.7:compile
[INFO]             \- javax.xml.bind:jaxb-api:jar:2.2.7:compile

Exception Details:

ERROR: org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation - Aborting job.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 230, localhost): java.lang.NoSuchMethodError: parquet.schema.Types$GroupBuilder.addField(Lparquet/schema/Type;)Lparquet/schema/Types$BaseGroupBuilder;
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convertField$1.apply(CatalystSchemaConverter.scala:517)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convertField$1.apply(CatalystSchemaConverter.scala:516)
                at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
                at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
                at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter.convertField(CatalystSchemaConverter.scala:516)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter.convertField(CatalystSchemaConverter.scala:312)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convert$1.apply(CatalystSchemaConverter.scala:305)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter$$anonfun$convert$1.apply(CatalystSchemaConverter.scala:305)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.Iterator$class.foreach(Iterator.scala:727)
                at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
                at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
                at org.apache.spark.sql.types.StructType.foreach(StructType.scala:92)
                at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
                at org.apache.spark.sql.types.StructType.map(StructType.scala:92)
                at org.apache.spark.sql.execution.datasources.parquet.CatalystSchemaConverter.convert(CatalystSchemaConverter.scala:305)
                at org.apache.spark.sql.execution.datasources.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetTypesConverter.scala:58)
                at org.apache.spark.sql.execution.datasources.parquet.RowWriteSupport.init(ParquetTableSupport.scala:55)
                at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:287)
                at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:261)
                at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetRelation.scala:94)
                at org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anon$3.newInstance(ParquetRelation.scala:272)
                at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:233)
                at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
                at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
                at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
                at org.apache.spark.scheduler.Task.run(Task.scala:88)
                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)

"
Michael Armbrust <michael@databricks.com>,"Wed, 16 Dec 2015 13:32:14 -0800",[VOTE] Release Apache Spark 1.6.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.0!

The vote is open until Saturday, December 19, 2015 at 18:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is *v1.6.0-rc3
(168c89e07c51fa24b0bb88582c739cec0acb44d7)
<https://github.com/apache/spark/tree/v1.6.0-rc3>*

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1174/

The test repository (versioned as v1.6.0-rc3) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1173/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-docs/

=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.

===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.


==================================================
== Major changes to help you focus your testing ==
==================================================

Notable changes since 1.6 RC2
- SPARK_VERSION has been set correctly
- SPARK-12199 ML Docs are publishing correctly
- SPARK-12345 Mesos cluster mode has been fixed

Notable changes since 1.6 RC1
Spark Streaming

   - SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629>
   trackStateByKey has been renamed to mapWithState

Spark SQL

   - SPARK-12165 <https://issues.apache.org/jira/browse/SPARK-12165>
   SPARK-12189 <https://issues.apache.org/jira/browse/SPARK-12189> Fix bugs
   in eviction of storage memory by execution.
   - SPARK-12258 <https://issues.apache.org/jira/browse/SPARK-12258> correct
   passing null into ScalaUDF

Notable Features Since 1.5Spark SQL

   - SPARK-11787 <https://issues.apache.org/jira/browse/SPARK-11787> Parquet
   Performance - Improve Parquet scan performance when using flat schemas.
   - SPARK-10810 <https://issues.apache.org/jira/browse/SPARK-10810>
   Session Management - Isolated devault database (i.e USE mydb) even on
   shared clusters.
   - SPARK-9999  <https://issues.apache.org/jira/browse/SPARK-9999> Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000 <https://issues.apache.org/jira/browse/SPARK-10000> Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197 <https://issues.apache.org/jira/browse/SPARK-11197> SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745 <https://issues.apache.org/jira/browse/SPARK-11745> Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 <https://issues.apache.org/jira/browse/SPARK-10412>
Per-operator
   Metrics for SQL Execution - Display statistics on a peroperator basis
   for memory usage and spilled data size.
   - SPARK-11329 <https://issues.apache.org/jira/browse/SPARK-11329> Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 <https://issues.apache.org/jira/browse/SPARK-10917>,
   SPARK-11149 <https://issues.apache.org/jira/browse/SPARK-11149> In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111 <https://issues.apache.org/jira/browse/SPARK-11111> Fast
   null-safe joins - Joins using null-safe equality (<=>) will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389 <https://issues.apache.org/jira/browse/SPARK-11389> SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978 <https://issues.apache.org/jira/browse/SPARK-10978> Datasource
   API Avoid Double Filter - When implemeting a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849  <https://issues.apache.org/jira/browse/SPARK-4849> Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858  <https://issues.apache.org/jira/browse/SPARK-9858> Adaptive
   query execution - Intial support for automatically selecting the number
   of reducers for joins and aggregations.
   - SPARK-9241  <https://issues.apache.org/jira/browse/SPARK-9241> Improved
   query planner for queries having distinct aggregations - Query plans of
   distinct aggregations are more robust when distinct columns have high
   cardinality.

Spark Streaming

   - API Updates
      - SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629> New
      improved state management - mapWithState - a DStream transformation
      for stateful stream processing, supercedes updateStateByKey in
      functionality and performance.
      - SPARK-11198 <https://issues.apache.org/jira/browse/SPARK-11198> Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891 <https://issues.apache.org/jira/browse/SPARK-10891> Kinesis
      message handler function - Allows arbitraray function to be applied
      to a Kinesis record in the Kinesis receiver before to customize what data
      is to be stored in memory.
      - SPARK-6328  <https://issues.apache.org/jira/browse/SPARK-6328> Python
      Streamng Listener API - Get streaming statistics (scheduling delays,
      batch processing times, etc.) in streaming.


   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress
      bars.

MLlibNew algorithms/models

   - SPARK-8518  <https://issues.apache.org/jira/browse/SPARK-8518> Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834  <https://issues.apache.org/jira/browse/SPARK-9834> Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930  <https://issues.apache.org/jira/browse/SPARK-9930> New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517  <https://issues.apache.org/jira/browse/SPARK-6517> Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means

API improvements

   - ML Pipelines
      - SPARK-6725  <https://issues.apache.org/jira/browse/SPARK-6725> Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.mlalgorithms
      - SPARK-5565  <https://issues.apache.org/jira/browse/SPARK-5565> LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836  <https://issues.apache.org/jira/browse/SPARK-9836> R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681  <https://issues.apache.org/jira/browse/SPARK-9681> Feature
      interactions in R formula - Interaction operator "":"" in R formula
   - Python API - Many improvements to Python API to approach feature parity

Misc improvements

   - SPARK-7685  <https://issues.apache.org/jira/browse/SPARK-7685>,
   SPARK-9642  <https://issues.apache.org/jira/browse/SPARK-9642> Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 <https://issues.apache.org/jira/browse/SPARK-10384>,
   SPARK-10385 <https://issues.apache.org/jira/browse/SPARK-10385> Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117 <https://issues.apache.org/jira/browse/SPARK-10117> LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751  <https://issues.apache.org/jira/browse/SPARK-7751> @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337 <https://issues.apache.org/jira/browse/SPARK-11337> Testable
   example code - Automated testing for code in user guide examples

Deprecations

   - In spark.mllib.clustering.KMeans, the ""runs"" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the ""weights"" field has been
   deprecated, in favor of the new name ""coefficients."" This helps
   disambiguate from instance (row) weights given to algorithms.

Changes of behavior

   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
   - Spark SQL's partition discovery has been changed to only discover
   partition directories that are children of the given path. (i.e. if
   path=""/my/data/x=1"" then x=1 will no longer be considered a partition
   but only children of x=1.) This behavior can be overridden by manually
   specifying the basePath that partitioning discovery should start with (
   SPARK-11678 <https://issues.apache.org/jira/browse/SPARK-11678>).
   - When casting a value of an integral type to timestamp (e.g. casting a
   long value to timestamp), the value is treated as being in seconds instead
   of milliseconds (SPARK-11724
   <https://issues.apache.org/jira/browse/SPARK-11724>).
   - With the improved query planner for queries having distinct
   aggregations (SPARK-9241
   <https://issues.apache.org/jira/browse/SPARK-9241>), the plan of a query
   having a single distinct aggregation has been changed to a more robust
   version. To switch back to the plan generated by Spark 1.5's planner,
   please set spark.sql.specializeSingleDistinctAggPlanning to true (
   SPARK-12077 <https://issues.apache.org/jira/browse/SPARK-12077>).
"
=?UTF-8?B?SmnFmcOtIFN5cm92w70=?= <syrovy.jiri@gmail.com>,"Wed, 16 Dec 2015 23:25:32 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 Tested in standalone mode and so far seems to be fairly stable.

2015-12-16 22:32 GMT+01:00 Michael Armbrust <michael@databricks.com>:

"
singinpirate <thesinginpirate@gmail.com>,"Wed, 16 Dec 2015 22:35:24 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","-0 (non-binding)

I have observed that when we set spark.executor.port in 1.6, we get thrown
a NPE in SparkEnv$.create(SparkEnv.scala:259). It used to work in 1.5.2. Is
anyone else seeing this?


===============
===============
========================
=="
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 16 Dec 2015 14:43:09 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),singinpirate <thesinginpirate@gmail.com>,"I was going to say that spark.executor.port is not used anymore in
1.6, but damn, there's still that akka backend hanging around there
even when netty is being used... we should fix this, should be a
simple one-liner.


-- 
Marcelo

---------------------------------------------------------------------


"
Rad Gruchalski <radek@gruchalski.com>,"Thu, 17 Dec 2015 00:16:57 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Marcelo Vanzin <vanzin@cloudera.com>,"I also noticed that spark.replClassServer.host and spark.replClassServer.port aren‚Äôt used anymore. The transport now happens over the main RpcEnv.










Kind regards,‚Ä®
Radek Gruchalski
‚Ä®radek@gruchalski.com (mailto:radek@gruchalski.com)‚Ä® (mailto:radek@gruchalski.com)
de.linkedin.com/in/radgruchalski/ (http://de.linkedin.com/in/radgruchalski/)

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.




hrown a
.2. Is
unsubscribe@spark.apache.org)
dev-help@spark.apache.org)


"
Andrew Or <andrew@databricks.com>,"Wed, 16 Dec 2015 16:37:26 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Rad Gruchalski <radek@gruchalski.com>,"+1

Mesos cluster mode regression in RC2 is now fixed (SPARK-12345
<https://issues.apache.org/jira/browse/SPARK-12345> / PR10332
<https://github.com/apache/spark/pull/10332>).

Also tested on standalone client and cluster mode. No problems.

2015-12-16 15"
Josh Rosen <joshrosen@databricks.com>,"Wed, 16 Dec 2015 16:39:06 -0800",Re: JIRA: Wrong dates from imported JIRAs,Lars Francke <lars.francke@gmail.com>,"Personally, I'd rather avoid the risk of breaking things during the
reimport. In my experience we've had a lot of unforeseen problems with JIRA
import/export and the benefit here doesn't seem huge (this issue only
impacts people that are searching for the oldest JIRAs across all projects,
which I think is pretty uncommon). Just my two cents.

- Josh


"
Michael Armbrust <michael@databricks.com>,"Wed, 16 Dec 2015 17:18:31 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Andrew Or <andrew@databricks.com>,"+1


w happens
s
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 16 Dec 2015 17:24:56 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Michael Armbrust <michael@databricks.com>,1
Reynold Xin <rxin@databricks.com>,"Wed, 16 Dec 2015 17:26:14 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Mark Hamstra <mark@clearstorydata.com>,1
Joseph Bradley <joseph@databricks.com>,"Wed, 16 Dec 2015 17:39:39 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>",1
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 17 Dec 2015 09:55:35 +0800 (CST)",Re:Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""Joseph Bradley"" <joseph@databricks.com>","plus 1






‘⁄ 2015-12-17 09:39:39£¨""Joseph Bradley"" <joseph@databricks.com> –¥µ¿£∫

+1


On Wed, Dec 16, 2015 at 5:26 PM, Reynold Xin <rxin@databricks.com> wrote:

+1




On Wed, Dec 16, 2015 at 5:24 PM, Mark Hamstra <mark@clearstorydata.com> wrote:

+1


On Wed, Dec 16, 2015 at 1:32 PM, Michael Armbrust <michael@databricks.com> wrote:

Please vote on releasing the following candidate as Apache Spark version 1.6.0!

The vote is open until Saturday, December 19, 2015 at 18:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast.



[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...


To learn more about Apache Spark, please see http://spark.apache.org/


The tag to be voted on is v1.6.0-rc3 (168c89e07c51fa24b0bb88582c739cec0acb44d7)


The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-bin/


Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc


The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1174/


The test repository (versioned as v1.6.0-rc3) for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1173/


The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-docs/


=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.


================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes should only occur for significant regressions from 1.5. Bugs already present in 1.5, minor regressions, or bugs related to new features will not block this release.


===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into branch-1.6, since documentations will be published separately from the release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target version.




==================================================
== Major changes to help you focus your testing ==
==================================================


Notable changes since 1.6 RC2

- SPARK_VERSION has been set correctly
- SPARK-12199 ML Docs are publishing correctly
- SPARK-12345 Mesos cluster mode has been fixed


Notable changes since 1.6 RC1

Spark Streaming
SPARK-2629  trackStateByKey has been renamed to mapWithState
Spark SQL
SPARK-12165 SPARK-12189 Fix bugs in eviction of storage memory by execution.
SPARK-12258 correct passing null into ScalaUDF
Notable Features Since 1.5
Spark SQL
SPARK-11787 Parquet Performance - Improve Parquet scan performance when using flat schemas.
SPARK-10810 Session Management - Isolated devault database (i.e USE mydb) even on shared clusters.
SPARK-9999  Dataset API - A type-safe API (similar to RDDs) that performs many operations on serialized binary data and code generation (i.e. Project Tungsten).
SPARK-10000 Unified Memory Management - Shared memory for execution and caching instead of exclusive division of the regions.
SPARK-11197 SQL Queries on Files - Concise syntax for running SQL queries over files of any supported format without registering a table.
SPARK-11745 Reading non-standard JSON files - Added options to read non-standard JSON files (e.g. single-quotes, unquoted attributes)
SPARK-10412 Per-operator Metrics for SQL Execution - Display statistics on a peroperator basis for memory usage and spilled data size.
SPARK-11329 Star (*) expansion for StructTypes - Makes it easier to nest and unest arbitrary numbers of columns
SPARK-10917, SPARK-11149 In-memory Columnar Cache Performance - Significant (up to 14x) speed up when caching data that contains complex types in DataFrames or SQL.
SPARK-11111 Fast null-safe joins - Joins using null-safe equality (<=>) will now execute using SortMergeJoin instead of computing a cartisian product.
SPARK-11389 SQL Execution Using Off-Heap Memory - Support for configuring query execution to occur using off-heap memory to avoid GC overhead
SPARK-10978 Datasource API Avoid Double Filter - When implemeting a datasource with filter pushdown, developers can now tell Spark SQL to avoid double evaluating a pushed-down filter.
SPARK-4849  Advanced Layout of Cached Data - storing partitioning and ordering schemes in In-memory table scan, and adding distributeBy and localSort to DF API
SPARK-9858  Adaptive query execution - Intial support for automatically selecting the number of reducers for joins and aggregations.
SPARK-9241  Improved query planner for queries having distinct aggregations - Query plans of distinct aggregations are more robust when distinct columns have high cardinality.
Spark Streaming
API Updates
SPARK-2629  New improved state management - mapWithState - a DStream transformation for stateful stream processing, supercedes updateStateByKey in functionality and performance.
SPARK-11198 Kinesis record deaggregation - Kinesis streams have been upgraded to use KCL 1.4.0 and supports transparent deaggregation of KPL-aggregated records.
SPARK-10891 Kinesis message handler function - Allows arbitraray function to be applied to a Kinesis record in the Kinesis receiver before to customize what data is to be stored in memory.
SPARK-6328  Python Streamng Listener API - Get streaming statistics (scheduling delays, batch processing times, etc.) in streaming.
UI Improvements
Made failures visible in the streaming tab, in the timelines, batch list, and batch details page.
Made output operations visible in the streaming tab as progress bars.
MLlib
New algorithms/models
SPARK-8518  Survival analysis - Log-linear model for survival analysis
SPARK-9834  Normal equation for least squares - Normal equation solver, providing R-like model summary statistics
SPARK-3147  Online hypothesis testing - A/B testing in the Spark Streaming framework
SPARK-9930  New feature transformers - ChiSqSelector, QuantileDiscretizer, SQL transformer
SPARK-6517  Bisecting K-Means clustering - Fast top-down clustering variant of K-Means
API improvements
ML Pipelines
SPARK-6725  Pipeline persistence - Save/load for ML Pipelines, with partial coverage of spark.mlalgorithms
SPARK-5565  LDA in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
R API
SPARK-9836  R-like statistics for GLMs - (Partial) R-like stats for ordinary least squares via summary(model)
SPARK-9681  Feature interactions in R formula - Interaction operator "":"" in R formula
Python API - Many improvements to Python API to approach feature parity
Misc improvements
SPARK-7685 , SPARK-9642  Instance weights for GLMs - Logistic and Linear Regression can take instance weights
SPARK-10384, SPARK-10385 Univariate and bivariate statistics in DataFrames - Variance, stddev, correlations, etc.
SPARK-10117 LIBSVM data source - LIBSVM as a SQL data source
Documentation improvements
SPARK-7751  @since versions - Documentation includes initial version when classes and methods were added
SPARK-11337 Testable example code - Automated testing for code in user guide examples
Deprecations
In spark.mllib.clustering.KMeans, the ""runs"" parameter has been deprecated.
In spark.ml.classification.LogisticRegressionModel and spark.ml.regression.LinearRegressionModel, the ""weights"" field has been deprecated, in favor of the new name ""coefficients."" This helps disambiguate from instance (row) weights given to algorithms.
Changes of behavior
spark.mllib.tree.GradientBoostedTrees validationTol has changed semantics in 1.6. Previously, it was a threshold for absolute change in error. Now, it resembles the behavior of GradientDescent convergenceTol: For large errors, it uses relative error (relative to the previous error); for small errors (< 0.01), it uses absolute error.
spark.ml.feature.RegexTokenizer: Previously, it did not convert strings to lowercase before tokenizing. Now, it converts to lowercase by default, with an option not to. This matches the behavior of the simpler Tokenizer transformer.
Spark SQL's partition discovery has been changed to only discover partition directories that are children of the given path. (i.e. if path=""/my/data/x=1"" then x=1 will no longer be considered a partition but only children of x=1.) This behavior can be overridden by manually specifying the basePath that partitioning discovery should start with (SPARK-11678).
When casting a value of an integral type to timestamp (e.g. casting a long value to timestamp), the value is treated as being in seconds instead of milliseconds (SPARK-11724).
With the improved query planner for queries having distinct aggregations (SPARK-9241), the plan of a query having a single distinct aggregation has been changed to a more robust version. To switch back to the plan generated by Spark 1.5's planner, please set spark.sql.specializeSingleDistinctAggPlanning to true (SPARK-12077).





"
sara mustafa <eng.sara.mustafa@gmail.com>,"Wed, 16 Dec 2015 18:57:52 -0700 (MST)",Spark basicOperators,dev@spark.apache.org,"Hi,

The class org.apache.spark.sql.execution.basicOperators.scala contains the
implementation of multiple operators, how could I measure the execution time
of any operator?

thanks,



--

---------------------------------------------------------------------


"
Saisai Shao <sai.sai.shao@gmail.com>,"Thu, 17 Dec 2015 10:00:03 +0800",Re: Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Allen Zhang <allenzhang010@126.com>,"+1 (non-binding) after SPARK-12345 is merged.


.com> ÂÜôÈÅìÔºö
:
/
/
/
================
================
n
=========================
=========================
eady
l not
========================================
=
========================================
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 16 Dec 2015 18:15:00 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Andrew Or <andrew@databricks.com>,"Ran test suite (minus docker-integration-tests)
All passed

+1

[INFO] Spark Project External ZeroMQ ...................... SUCCESS [
13.647 s]
[INFO] Spark Project External Kafka ....................... SUCCESS [
45.424 s]
[INFO] Spark Project Examples ............................. SUCCESS [02:06
min]
[INFO] Spark Project External Kafka Assembly .............. SUCCESS [
11.280 s]
[INFO]
------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 01:49 h
[INFO] Finished at: 2015-12-16T17:06:58-08:00


w happens
s
"
Joseph Bradley <joseph@databricks.com>,"Wed, 16 Dec 2015 18:26:25 -0800","Re: java.lang.NoSuchMethodError while saving a random forest model
 Spark version 1.5",Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"This method is tested in the Spark 1.5 unit tests, so I'd guess it's a
problem with the Parquet dependency.  What version of Parquet are you
building Spark 1.5 off of?  (I'm not that familiar with Parquet issues
myself, but hopefully a SQL person can chime in.)


"
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 17 Dec 2015 10:31:20 +0800 (CST)",Re:Re: does spark really support label expr like && or || ?,"""Ted Yu"" <yuzhihong@gmail.com>","Hi Ted,


I have 4 vms(spark-dev, sut-1, sut-2, sut-3):


With these commands: 
1. yarn rmadmin -addToClusterNodeLabels foo,bar,bye
2. yarn rmadmin -replaceLabelsOnNode spark-dev:54321,foo;yarn rmadmin -replaceLabelsOnNode sut-1:54321,foo, same to sut-2 and sut-3
3. yarn rmadmin -refreshQueues


I am using Spark 1.6.0 snapshot and Apache Hadoop 2.6.0.


the command I submit to spark cluster is as below, which works as expected:
spark-submit --conf spark.yarn.executor.nodeLabelExpression=foo --conf spark.yarn.am.nodeLabelExpression=bar --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.6.0-SNAPSHOT-hadoop2.6.0.jar


NOTE: I compiled the spark tar.gz.


but, a little changed to spark.yarn.executor.nodeLabelExpression=""foo|bye"", the command does not work and the appplication will be waiting there in my console
 

spark-submit --conf spark.yarn.executor.nodeLabelExpression=""foo|bye"" --conf spark.yarn.am.nodeLabelExpression=bar --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.6.0-SNAPSHOT-hadoop2.6.0.jar


so , my question is does the spark.yarn.executor.nodeLabelExpression and  spark.yarn.am.nodeLabelExpression really support ""EXPRESSION"" like and &&, or ||, or even ! and so on.


NOTE:
I didn't change the capacity-scheduler.xml at all, I just wang to try specifing label in the point view of Application.
any feedback from community? 
Thanks,
Allen



At 2015-12-16 18:24:08, ""Ted Yu"" <yuzhihong@gmail.com> wrote:

Allen:
Since you mentioned scheduling, I assume you were talking about node label support in YARN. 
If that is the case, can you give us some more information:
How node labels are setup in YARN cluster
How you specified node labels in application
Hadoop and Spark releases you are using


Cheers

On Dec 16, 2015, at 1:00 AM, Chang Ya-Hsuan <sumtiogo@gmail.com> wrote:


are you trying to do dataframe boolean expression?
please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.



example:


>>> df = sqlContext.range(10)
>>> df.where( (df.id==1) | ~(df.id==1))
DataFrame[id: bigint]




On Wed, Dec 16, 2015 at 4:32 PM, Allen Zhang <allenzhang010@126.com> wrote:

Hi All,


does spark label expression really support ""&&"" or ""||"" or even ""!"" for label based schedulering?
I tried that but it does NOT work.


Best Regards,
Allen







--

-- èà—≈‹é"
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 17 Dec 2015 10:40:58 +0800 (CST)",Re: does spark really support label expr like && or || ?,"""Allen Zhang"" <allenzhang010@126.com>","more details commands:


2. yarn rmadmin -replaceLabelsOnNode spark-dev:54321,foo;
    yarn rmadmin -replaceLabelsOnNode sut-1:54321,bar;
    yarn rmadmin -replaceLabelsOnNode sut-2:54321,bye;
    yarn rmadmin -replaceLabelsOnNode sut-3:54321,foo;





At 2015-12-17 10:31:20, ""Allen Zhang"" <allenzhang010@126.com> wrote:

Hi Ted,


I have 4 vms(spark-dev, sut-1, sut-2, sut-3):


With these commands: 
1. yarn rmadmin -addToClusterNodeLabels foo,bar,bye
2. yarn rmadmin -replaceLabelsOnNode spark-dev:54321,foo;yarn rmadmin -replaceLabelsOnNode sut-1:54321,foo, same to sut-2 and sut-3
3. yarn rmadmin -refreshQueues


I am using Spark 1.6.0 snapshot and Apache Hadoop 2.6.0.


the command I submit to spark cluster is as below, which works as expected:
spark-submit --conf spark.yarn.executor.nodeLabelExpression=foo --conf spark.yarn.am.nodeLabelExpression=bar --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.6.0-SNAPSHOT-hadoop2.6.0.jar


NOTE: I compiled the spark tar.gz.


but, a little changed to spark.yarn.executor.nodeLabelExpression=""foo|bye"", the command does not work and the appplication will be waiting there in my console
 

spark-submit --conf spark.yarn.executor.nodeLabelExpression=""foo|bye"" --conf spark.yarn.am.nodeLabelExpression=bar --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.6.0-SNAPSHOT-hadoop2.6.0.jar


so , my question is does the spark.yarn.executor.nodeLabelExpression and  spark.yarn.am.nodeLabelExpression really support ""EXPRESSION"" like and &&, or ||, or even ! and so on.


NOTE:
I didn't change the capacity-scheduler.xml at all, I just wang to try specifing label in the point view of Application.
any feedback from community? 
Thanks,
Allen



At 2015-12-16 18:24:08, ""Ted Yu"" <yuzhihong@gmail.com> wrote:

Allen:
Since you mentioned scheduling, I assume you were talking about node label support in YARN. 
If that is the case, can you give us some more information:
How node labels are setup in YARN cluster
How you specified node labels in application
Hadoop and Spark releases you are using


Cheers

On Dec 16, 2015, at 1:00 AM, Chang Ya-Hsuan <sumtiogo@gmail.com> wrote:


are you trying to do dataframe boolean expression?
please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.



example:


>>> df = sqlContext.range(10)
>>> df.where( (df.id==1) | ~(df.id==1))
DataFrame[id: bigint]




On Wed, Dec 16, 2015 at 4:32 PM, Allen Zhang <allenzhang010@126.com> wrote:

Hi All,


does spark label expression really support ""&&"" or ""||"" or even ""!"" for label based schedulering?
I tried that but it does NOT work.


Best Regards,
Allen







--

-- èà—≈‹é"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 16 Dec 2015 18:56:09 -0800",Re: Re: does spark really support label expr like && or || ?,Allen Zhang <allenzhang010@126.com>,"
Spark doesn't do anything with those values except pass them to YARN.
So if you don't see any errors / warnings from the Spark side saying
the feature is not supported by your client libraries, you're probably
running into some issue with YARN. Try checking the YARN RM logs.

-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 16 Dec 2015 19:19:38 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Ted Yu <yuzhihong@gmail.com>,"+1


6
ow happens
e
m>
Is
"
Yin Huai <yhuai@databricks.com>,"Wed, 16 Dec 2015 21:14:26 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1

:

:
now happens
r
he
"
,"Thu, 17 Dec 2015 07:26:43 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),dev@spark.apache.org,"+1 (non binding)

Tested in standalone and yarn with different samples.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 17 Dec 2015 00:35:28 -0700 (MST)",How do we convert a Dataset includes timestamp columns to RDD?,dev@spark.apache.org,"Hi all,

When I tried to convert a Dataset which includes a TimestampType column to a
RDD under master branch on spark-shell, I got an error about
`org.apache.spark.SparkException: Task not serializable`.
How do we convert Dataset includes timestamp to RDD?

Here is the example code and the error:

```
import sqlContext.implicits._
import java.sql.Timestamp
import java.text.SimpleDateFormat

case class TimestampExample(dt: java.sql.Timestamp)
def parse(s: String): Timestamp = {
  val dateFormat = new SimpleDateFormat(""yyyy-MM-dd"")
   new Timestamp(dateFormat.parse(s).getTime())
}
val rdd = sc.parallelize(Seq(""2015-01-01"", ""2015-02-01""))
val df = rdd.map(x => TimestampExample(parse(x))).toDF()
val ds = df.as[TimestampExample]
ds.rdd

org.apache.spark.SparkException: Task not serializable
        at
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)
        at
org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
        at
org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
        at org.apache.spark.SparkContext.clean(SparkContext.scala:2061)
        at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:707)
        at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:706)
        at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:706)
        at org.apache.spark.sql.Dataset.rdd(Dataset.scala:166)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:60)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:69)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:73)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:75)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:77)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:79)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:81)
        at
$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:83)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:85)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:87)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:89)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:91)
        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:93)
        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:95)
        at $iwC$$iwC$$iwC.<init>(<console>:97)
        at $iwC$$iwC.<init>(<console>:99)
        at $iwC.<init>(<console>:101)
        at <init>(<console>:103)
        at .<init>(<console>:107)
        at .<clinit>(<console>)
        at .<init>(<console>:7)
        at .<clinit>(<console>)
        at $print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1045)
        at
org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1326)
        at
org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:821)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:852)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:800)
        at
org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
        at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
        at
org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
        at
org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
        at
org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
        at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
        at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at
scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at
org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1064)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.NotSerializableException:
org.apache.spark.sql.catalyst.util.DateTimeUtils$
Serialization stack:
        - object not serializable (class:
org.apache.spark.sql.catalyst.util.DateTimeUtils$, value:
org.apache.spark.sql.catalyst.util.DateTimeUtils$@216c782f)
        - field (class:
org.apache.spark.sql.catalyst.expressions.StaticInvoke, name: staticObject,
type: class java.lang.Object)
        - object (class
org.apache.spark.sql.catalyst.expressions.StaticInvoke,
staticinvoke(org.apache.spark.sql.catalyst.util.DateTimeUtils$@216c782f,ObjectType(class
java.sql.Timestamp),toJavaTimestamp,input[0, TimestampType],true))
        - writeObject data (class: scala.collection.immutable.$colon$colon)
        - object (class scala.collection.immutable.$colon$colon,
List(staticinvoke(org.apache.spark.sql.catalyst.util.DateTimeUtils$@216c782f,ObjectType(class
java.sql.Timestamp),toJavaTimestamp,input[0, TimestampType],true)))
        - field (class:
org.apache.spark.sql.catalyst.expressions.NewInstance, name: arguments,
type: interface scala.collection.Seq)
        - object (class
org.apache.spark.sql.catalyst.expressions.NewInstance, newinstance(class
$iwC$$iwC$TimestampExample,staticinvoke(org.apache.spark.sql.catalyst.util.DateTimeUtils$@216c782f,ObjectType(class
java.sql.Timestamp),toJavaTimestamp,input[0,
TimestampType],true),false,ObjectType(class
$iwC$$iwC$TimestampExample),Some($iwC$$iwC@23b27380)))
        - field (class:
org.apache.spark.sql.catalyst.encoders.ExpressionEncoder, name:
fromRowExpression, type: class
org.apache.spark.sql.catalyst.expressions.Expression)
        - object (class
org.apache.spark.sql.catalyst.encoders.ExpressionEncoder, class[dt[0]:
timestamp])
        - field (class: org.apache.spark.sql.Dataset, name: boundTEncoder,
type: class org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)
        - object (class org.apache.spark.sql.Dataset, [dt: timestamp])
        - field (class: org.apache.spark.sql.Dataset$$anonfun$rdd$1, name:
$outer, type: class org.apache.spark.sql.Dataset)
        - object (class org.apache.spark.sql.Dataset$$anonfun$rdd$1,
<function1>)
        at
org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
        at
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
        at
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
        at
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
        ... 68 more
```

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Yanbo Liang <ybliang8@gmail.com>,"Thu, 17 Dec 2015 16:19:43 +0800","Re: java.lang.NoSuchMethodError while saving a random forest model
 Spark version 1.5",Joseph Bradley <joseph@databricks.com>,"Spark 1.5 officially use Parquet 1.7.0, but Spark 1.3 use Parquet 1.6.0.
It's better to check which version of Parquet is used in your environment.

2015-12-17 10:26 GMT+08:00 Joseph Bradley <joseph@databricks.com>:

"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Thu, 17 Dec 2015 19:11:14 +0900",Re: How do we convert a Dataset includes timestamp columns to RDD?,"Yu Ishikawa <yuu.ishikawa+spark@gmail.com>, dev@spark.apache.org","Hi Yu,

I found it's because DateTimeUtils passed to StaticInvoke is not 
serializable.
I think it's potential bug that StaticInvoke can  receives 
non-Serializable objects.

I opened a PR about this issue.
https://github.com/apache/spark/pull/10357

- Kousuke



---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 17 Dec 2015 11:34:41 +0100","Re: Update to Spar Mesos docs possibly? LIBPROCESS_IP needs to be set
 for client mode",Aaron <aarongmldt@gmail.com>,"

Absolutely. Have a look at
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark if
you haven't done so already, it should answer most questions about starting
to contribute to Spark.

thanks,
iulian


.com>
8qf9pEJgNWOmasM2DQchYaxPcAovnFKFgGgxXpZJ2JOAfQ@mail.gmail.com%3E
he
--



-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 17 Dec 2015 14:06:28 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Andrew Or <andrew@databricks.com>,"-0 (non-binding)

Unfortunately the Mesos cluster regression is still there (see my comment
<https://github.com/apache/spark/pull/10332/files#r47902198> for
explanations). I'm not voting to delay the release any longer though.

We tested (and passed) Meso"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Thu, 17 Dec 2015 22:21:07 +0900",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Michael Armbrust <michael@databricks.com>,1
Hao Ren <invkrh@gmail.com>,"Thu, 17 Dec 2015 14:49:24 +0100",implict ClassTag in KafkaUtils,dev <dev@spark.apache.org>,"Hi,

I am reading spark streaming Kafka code.

In org.apache.spark.streaming.kafka.KafkaUtils file,
the function ""createDirectStream"" takes key class, value class, etc to
create classTag.
However, they are all implicit. I don't understand why they are implicit.

In fact, I can not find any other overloaded ""createDirectStream"" take
implicit parameters.

So what are these implicit ClassTags are used for ? Thank you.

def createDirectStream[K, V, KD <: Decoder[K], VD <: Decoder[V], R](
    jssc: JavaStreamingContext,
    keyClass: Class[K],
    valueClass: Class[V],
    keyDecoderClass: Class[KD],
    valueDecoderClass: Class[VD],
    recordClass: Class[R],
    kafkaParams: JMap[String, String],
    fromOffsets: JMap[TopicAndPartition, JLong],
    messageHandler: JFunction[MessageAndMetadata[K, V], R]
  ): JavaInputDStream[R] = {
  implicit val keyCmt: ClassTag[K] = ClassTag(keyClass)
  implicit val valueCmt: ClassTag[V] = ClassTag(valueClass)
  implicit val keyDecoderCmt: ClassTag[KD] = ClassTag(keyDecoderClass)
  implicit val valueDecoderCmt: ClassTag[VD] = ClassTag(valueDecoderClass)
  implicit val recordCmt: ClassTag[R] = ClassTag(recordClass)
  val cleanedHandler = jssc.sparkContext.clean(messageHandler.call _)
  createDirectStream[K, V, KD, VD, R](
    jssc.ssc,
    Map(kafkaParams.toSeq: _*),
    Map(fromOffsets.mapValues { _.longValue() }.toSeq: _*),
    cleanedHandler
  )
}


-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 17 Dec 2015 07:03:58 -0700 (MST)",Re: How do we convert a Dataset includes timestamp columns to RDD?,dev@spark.apache.org,"Hi Kosuke, 

Thank you for the PR.
I think we should fix this bug before releasing Spark 1.6 ASAP.
I'm looking forward to merging it.

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Saisai Shao <sai.sai.shao@gmail.com>,"Thu, 17 Dec 2015 22:11:19 +0800",Re: implict ClassTag in KafkaUtils,Hao Ren <invkrh@gmail.com>,"Actually this is a Scala problem. createDirectStream actually requires
implicit values, which is implied as context bound, Java does not have the
equivalence, so here change the java class to the ClassTag, and make it as
implicit value, it will be used by createDirectStream.


Thanks
Saisai



"
Timothy O <todell@yahoo-inc.com.INVALID>,"Thu, 17 Dec 2015 15:39:47 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""sarutak@oss.nttdata.co.jp"" <sarutak@oss.nttdata.co.jp>,
        Michael Armbrust <michael@databricks.com>","+1 


 

  +1
 
  
  Please vote on releasing the following candidate as Apache Spark version 1.6.0! 
 The vote is open until Saturday, December 19, 2015¬†at 18:00 UTC and passes if a majority¬†of at least 3 +1 PMC votes are cast.
  
  [ ] +1 Release this"
syepes <syepes@gmail.com>,"Thu, 17 Dec 2015 08:44:29 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),dev@spark.apache.org,"-1 (YARN Cluster deployment mode not working)

I have just tested 1.6 (d509194b) on our HDP 2.3 platform and the cluster
mode does not seem work. It looks like some parameter are not being passed
correctly.
This example works correctly with 1.5.

# spark-"
Hao Ren <invkrh@gmail.com>,"Thu, 17 Dec 2015 17:50:09 +0100",Re: implict ClassTag in KafkaUtils,Saisai Shao <sai.sai.shao@gmail.com>,"Thank you for your quick answer.
It helped me to find an implicit conversion for JavaInputDStream which
takes implicit ClassTag.

Cheers.




-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 17 Dec 2015 11:38:46 -0800",Re: does spark really support label expr like && or || ?,Allen Zhang <allenzhang010@126.com>,"I consulted with YARN developer, the notion presented in Allen's email is
not supported yet.

Cheers


d:
spark.yarn.am.nodeLabelExpression=bar --class org.apache.spark.examples.SparkPi --master yarn-cluster
ye"",
y
xamples.SparkPi --master yarn-cluster
nd
l
"
Andrew Or <andrew@databricks.com>,"Thu, 17 Dec 2015 12:14:03 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),syepes <syepes@gmail.com>,"@syepes

I just run Spark 1.6 (881f254) on YARN with Hadoop 2.4.0. I was able to run
a simple application in cluster mode successfully.

Can you verify whether the org.apache.spark.yarn.ApplicationMaster class
exists in your assembly jar?

jar -tf assembl"
Sebastian YEPES FERNANDEZ <syepes@gmail.com>,"Thu, 17 Dec 2015 21:36:30 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Andrew Or <andrew@databricks.com>,"@Andrew
Thanks for the reply, did you run this in a Hortonworks or Cloudera cluster?
I suspect the issue is coming from the ‚ÄãextraJavaOptions as these are
necessary in HDP, the strange thing is that with exactly the same settings
1.5 works.

# jar -tf s"
Andrew Or <andrew@databricks.com>,"Thu, 17 Dec 2015 14:07:28 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Sebastian YEPES FERNANDEZ <syepes@gmail.com>,"That seems like an HDP-specific issue. I did a quick search on ""spark bad
substitution"" and all the results have to do with people failing to run
YARN cluster in HDP. Here is a workaround
<https://mail-archives.apache.org/mod_mbox/spark-user/201509.mbox/%3CCABoKCLHrRLj6m3w+4Z2OQcOBr-aMZetut8AceNZgQLCs_OG_aw@mail.gmail.com%3E>
that seems to have worked for multiple people.

I would not block the release on this particular issue. First, this doesn't
seem like a Spark issue and second, even if it is, this only affects a
small number of users and there is a workaround for it. In my own testing
the `extraJavaOptions` are propagated correctly in both YARN client and
cluster modes.

2015-12-17 12:36 GMT-08:00 Sebastian YEPES FERNANDEZ <syepes@gmail.com>:

 are
s
on_1445706872927_1593/container_e44_1445706872927_1593_02_000001/launch_container.sh:
:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:
er
t/lib/native/Linux-amd64-64
50
t/lib/native/Linux-amd64-64
Apache-Spark-1-6-0-RC3-tp15660p15692.html
"
Michael Gummelt <mgummelt@mesosphere.io>,"Thu, 17 Dec 2015 14:40:33 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Andrew Or <andrew@databricks.com>,"The fix for the Mesos cluster regression has introduced another Mesos
cluster bug.  Namely, the MesosClusterDispatcher crashes when trying to
write to ZK: https://issues.apache.org/jira/browse/SPARK-12413

I have a tentative fix here: https://github.com/apache/spark/pull/10366


KCLHrRLj6m3w+4Z2OQcOBr-aMZetut8AceNZgQLCs_OG_aw@mail.gmail.com%3E>
wn
nt
e are
gs
ion_1445706872927_1593/container_e44_1445706872927_1593_02_000001/launch_container.sh:
D:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:
:
s
nt/lib/native/Linux-amd64-64
950
nt/lib/native/Linux-amd64-64
-Apache-Spark-1-6-0-RC3-tp15660p15692.html


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Vinay Shukla <vinayshukla@gmail.com>,"Thu, 17 Dec 2015 15:54:13 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),dev@spark.apache.org,"Agree with Andrew, we shouldn't block the release for this.

This issue won't be there in Spark distribution from Hortonworks since we
set the HDP version.

If you want to use the Apache Spark with HDP  you can modify mapred-site.xml
to replace the hdp.version property with the right value for your cluster.
You can find the right value by invoking the hdp-select script on a node
		hdp-select status hadoop-client
hadoop-client - 2.2.5.0-2644
Here is a one line script to get the version:
export HDP_VER=`hdp-select status hadoop-client | sed 's/hadoop-client -
\(.*\)/\1/'`

CAUTION - if you modify mapred-site.xml on a node on the cluster, this will
break rolling upgrades in certain scenarios where a program like oozie
submitting a job from that node will use the hardcoded version instead of
the version specified by the client.

So what does the Hortonworks distribution do under the covers to support
hdp.version? 
create a file called java-opts with the following config value in it 
-Dhdp.version=2.2.5.0-2644. You can also specify the same value using
SPARK_JAVA_OPTS, i.e. export SPARK_JAVA_OPTS=""-Dhdp.version=2.2.5.0-2644""
add the following options to spark-defaults.conf:
spark.driver.extraJavaOptions     	-Dhdp.version=2.2.5.0-2644
spark.yarn.am.extraJavaOptions 	-Dhdp.version=2.2.5.0-2644



--

---------------------------------------------------------------------


"
Lars Francke <lars.francke@gmail.com>,"Fri, 18 Dec 2015 00:13:34 +0100",Re: JIRA: Wrong dates from imported JIRAs,Josh Rosen <joshrosen@databricks.com>,"Okay thanks guys, that's two -1s and that's fair enough. I'll leave it at
that.


"
Vinay Shukla <vinayshukla@gmail.com>,"Thu, 17 Dec 2015 16:26:53 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),dev@spark.apache.org,".../spark/conf with the following config value in it  -Dhdp.version=<version
of HDP>.

your HDP cluster.

hdp-select status hadoop-client | sed 's/hadoop-client - \(.*\)/\1/'


You can also specify the same value using SPARK_JAVA_OPTS, i.e. export
SPARK_JAVA_OPTS=""-Dhdp.version=2.2.5.0-2644"" 
add the following options to spark-defaults.conf: 
spark.driver.extraJavaOptions     -Dhdp.version=2.2.5.0-2644 
spark.yarn.am.extraJavaOptions -Dhdp.version=2.2.5.0-2644




--

---------------------------------------------------------------------


"
"""Allen Zhang"" <allenzhang010@126.com>","Fri, 18 Dec 2015 09:51:07 +0800 (CST)",Re: does spark really support label expr like && or || ?,"""Ted Yu"" <yuzhihong@gmail.com>","

^_^ , Thanks Ted. 






At 2015-12-18 03:38:46, ""Ted Yu"" <yuzhihong@gmail.com> wrote:

I consulted with YARN developer, the notion presented in Allen's email is not supported yet.
Only single node label should be specified.


Cheers


On Wed, Dec 16, 2015 at 6:40 PM, Allen Zhang <allenzhang010@126.com> wrote:

more details commands:


2. yarn rmadmin -replaceLabelsOnNode spark-dev:54321,foo;
    yarn rmadmin -replaceLabelsOnNode sut-1:54321,bar;
    yarn rmadmin -replaceLabelsOnNode sut-2:54321,bye;
    yarn rmadmin -replaceLabelsOnNode sut-3:54321,foo;





At 2015-12-17 10:31:20, ""Allen Zhang"" <allenzhang010@126.com> wrote:

Hi Ted,


I have 4 vms(spark-dev, sut-1, sut-2, sut-3):


With these commands: 
1. yarn rmadmin -addToClusterNodeLabels foo,bar,bye
2. yarn rmadmin -replaceLabelsOnNode spark-dev:54321,foo;yarn rmadmin -replaceLabelsOnNode sut-1:54321,foo, same to sut-2 and sut-3
3. yarn rmadmin -refreshQueues


I am using Spark 1.6.0 snapshot and Apache Hadoop 2.6.0.


the command I submit to spark cluster is as below, which works as expected:
spark-submit --conf spark.yarn.executor.nodeLabelExpression=foo --conf spark.yarn.am.nodeLabelExpression=bar --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.6.0-SNAPSHOT-hadoop2.6.0.jar


NOTE: I compiled the spark tar.gz.


but, a little changed to spark.yarn.executor.nodeLabelExpression=""foo|bye"", the command does not work and the appplication will be waiting there in my console
 

spark-submit --conf spark.yarn.executor.nodeLabelExpression=""foo|bye"" --conf spark.yarn.am.nodeLabelExpression=bar --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.6.0-SNAPSHOT-hadoop2.6.0.jar


so , my question is does the spark.yarn.executor.nodeLabelExpression and  spark.yarn.am.nodeLabelExpression really support ""EXPRESSION"" like and &&, or ||, or even ! and so on.


NOTE:
I didn't change the capacity-scheduler.xml at all, I just wang to try specifing label in the point view of Application.
any feedback from community? 
Thanks,
Allen



At 2015-12-16 18:24:08, ""Ted Yu"" <yuzhihong@gmail.com> wrote:

Allen:
Since you mentioned scheduling, I assume you were talking about node label support in YARN. 
If that is the case, can you give us some more information:
How node labels are setup in YARN cluster
How you specified node labels in application
Hadoop and Spark releases you are using


Cheers

On Dec 16, 2015, at 1:00 AM, Chang Ya-Hsuan <sumtiogo@gmail.com> wrote:


are you trying to do dataframe boolean expression?
please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.



example:


>>> df = sqlContext.range(10)
>>> df.where( (df.id==1) | ~(df.id==1))
DataFrame[id: bigint]




On Wed, Dec 16, 2015 at 4:32 PM, Allen Zhang <allenzhang010@126.com> wrote:

Hi All,


does spark label expression really support ""&&"" or ""||"" or even ""!"" for label based schedulering?
I tried that but it does NOT work.


Best Regards,
Allen







--

-- èà—≈‹é

"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 17 Dec 2015 21:45:15 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Michael Armbrust <michael@databricks.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 29:32 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib (iPython 4.0)
2.0 Spark version is 1.6.0
2.1. statistics (min,max,mean,Pearson,Spearman)"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 18 Dec 2015 14:53:44 +0530",Re: security testing on spark ?,Judy Nash <judynash@exchange.microsoft.com>,"If the port 7077 is open for public on your cluster, that's all you need to
take over the cluster. You can read a bit about it here
https://www.sigmoid.com/securing-apache-spark-cluster/

You can also look at this small exploit I wrote
https://www.exploit-db.com/exploits/36562/

Thanks
Best Regards


"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 18 Dec 2015 15:11:24 +0530",Re: Spark basicOperators,sara mustafa <eng.sara.mustafa@gmail.com>,"You can pretty much measure it from the Event timeline listed in the driver
ui, You can click on jobs/tasks and get the time that it took for each of
it from there.

Thanks
Best Regards


"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Fri, 18 Dec 2015 16:34:00 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Krishna Sankar <ksankar42@gmail.com>,"+1 (non-binding)

It passes our tests after we registered 6 new classes with Kryo:


kryo.register(classOf[org.apache.spark.sql.catalyst.expressions.UnsafeRow])
    kryo.register(classOf[Array[org.apache.spark.mllib.tree.model.Split]])

    kryo.register("
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 18 Dec 2015 17:01:46 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"Michael Armbrust <michael@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1. ¬†Ran some regression tests on Spark on Yarn (hadoop 2.6 and 2.7).
Tom 


 

 Please vote on releasing the following candidate as Apache Spark version 1.6.0!
The vote is open until Saturday, December 19, 2015¬†at 18:00 UTC and passes if a majority¬†of"
Sean Owen <sowen@cloudera.com>,"Fri, 18 Dec 2015 17:22:11 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Michael Armbrust <michael@databricks.com>,"For me, mostly the same as before: tests are mostly passing, but I can
never get the docker tests to pass. If anyone knows a special profile
or package that needs to be enabled, I can try that and/or
fix/document it. Just wondering if it's me.

I'm on Java 7 + Ubuntu 15.10, with -Pyarn -Phive -Phive-thriftserver
-Phadoop-2.6


---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Fri, 18 Dec 2015 09:30:16 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Sean Owen <sowen@cloudera.com>,"Sean,
Are you referring to docker integration tests? If so, they were disabled
for majority of the release and I recently worked on it (SPARK-11796) and
once it got committed, the tests were re-enabled in Spark builds. I am not
sure what OSs the test builds use, but it should be passing there too.

During my work, I tested on Ubuntu Precise and they worked. If you could
share the logs with me offline, I could take a look. Alternatively, I can
try to see if I can get Ubuntu 15 instance. However, given the history of
these tests, I personally don't think it makes sense to block the release
based on them not running on Ubuntu 15.


"
Sean Owen <sowen@cloudera.com>,"Fri, 18 Dec 2015 17:32:18 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Mark Grover <mark@apache.org>,"Yes that's what I mean. If they're not quite working, let's disable
them, but first, we have to rule out that I'm not just missing some
requirement.

Functionally, it's not worth blocking the release. It seems like bad
form to release with tests that always fail for a non-trivial number
of users, but we have to establish that. If it's something with an
easy fix (or needs disabling) and another RC needs to be baked, might
be worth including.

Logs coming offline


---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Fri, 18 Dec 2015 10:27:25 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Sean Owen <sowen@cloudera.com>,"Thanks Sean for sending me the logs offline.

Turns out the tests are failing again, for reasons unrelated to Spark. I
have filed https://issues.apache.org/jira/browse/SPARK-12426 for that with
some details. In the meanwhile, I agree with Sean, these tests should be
disabled. And, again, I don't think this failures warrants blocking the
release.

Mark


"
Denny Lee <denny.g.lee@gmail.com>,"Fri, 18 Dec 2015 20:52:23 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"Michael Armbrust <michael@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Tested a number of tests surrounding DataFrames, Datasets, and ML.



"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Fri, 18 Dec 2015 17:48:06 -0700 (MST)","Is there any way to select columns of Dataset in addition to the
 combination of `expr` and `as`?",dev@spark.apache.org,"Hi all, 

I have two questions about selecting columns of Dataset.
First, could you tell me know if there is any way to select TypedColumn
columns in addition to the combination of `expr` and `as`?
Second, how can we alias such a `expr(""name"").as[String]` Column?

I tried to select a column of Dataset like DataFrame.
However, I couldn't do that.

```
case class Person(id: Int, name: String)
val df = sc.parallelize(Seq((1, ""Bob""), (2, ""Tom""))).toDF(""id"", ""name"")
val ds = df.as[Person]

ds.select(expr(""name"").as[String]).show
+-----+
|value|
+-----+
|  Bob|
|  Tom|
+-----+

ds.select('id).show
<console>:34: error: type mismatch;
found   : Symbol
required: org.apache.spark.sql.TypedColumn[Person,?]
              ds.select('id).show')

ds.select($""id"").show
<console>:34: error: type mismatch;
found   : org.apache.spark.sql.ColumnName
required: org.apache.spark.sql.TypedColumn[Person,?]
              ds.select($""id"").show

ds.select(ds(""id"")).show
<console>:34: error: org.apache.spark.sql.Dataset[Person] does not take
parameters
              ds.select(ds(""id"")).show

ds.select(""id"").show
<console>:34: error: type mismatch;
found   : String(""id"")
required: org.apache.spark.sql.TypedColumn[Person,?]
              ds.select(""id"").show
```

Best,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 18 Dec 2015 17:09:23 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Tests the without-hadoop binaries (so didn't run Hive-related tests)
with a test batch including standalone / client, yarn / client and
cluster, including core, mllib and streaming (flume and kafka).




-- 
Marcelo

--------------------"
=?UTF-8?Q?Zsolt_T=C3=B3th?= <toth.zsolt.bme@gmail.com>,"Sun, 20 Dec 2015 01:00:29 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Testing environment:
-CDH5.5 single node docker
-Prebuilt spark-1.6.0-hadoop2.6.tgz
-Yarn-cluster mode

Comparing outputs of Spark 1.5.x and 1.6.0-RC3:

Pyspark
OK?: K-Means (ml) - Note: our tests show a numerical diff here compared to
t"
Luciano Resende <luckbr1975@gmail.com>,"Sat, 19 Dec 2015 18:43:12 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Michael Armbrust <michael@databricks.com>,"+1 (non-binding)

Tested Standalone mode, SparkR and couple Stream Apps, all seem ok.




-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Jeff Zhang <zjffdu@gmail.com>,"Sun, 20 Dec 2015 15:44:52 +0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),Luciano Resende <luckbr1975@gmail.com>,"+1 (non-binding)

All the test passed, and run it on HDP 2.3.2 sandbox successfully.





-- 
Best Regards

Jeff Zhang
"
"""=?gb18030?B?Umlja3k=?="" <494165115@qq.com>","Sun, 20 Dec 2015 20:32:39 +0800","=?gb18030?B?u9i4tKO6IFtWT1RFXSBSZWxlYXNlIEFwYWNoZSBT?=
 =?gb18030?B?cGFyayAxLjYuMCAoUkMzKQ==?=","""=?gb18030?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?="" <dev@spark.apache.org>","SizeBasedRollingPolicy print too many               log  when spark.executor.logs.rolling.strategy is size , shouldRollover use logInfo method:
          
  def shouldRollover(bytesToBeWritten: Long): Boolean = {
    logInfo(s""$bytesToBeWritten + $bytesWrittenSinceRollover > $rolloverSizeBytes"")
    bytesToBeWritten + bytesWrittenSinceRollover > rolloverSizeBytes
  }



 logs.rolling:
  spark.executor.logs.rolling.strategy size
  spark.executor.logs.rolling.maxSize 134217728
  spark.executor.logs.rolling.maxRetainedFiles 8


 Can use logdebug instead of loginfo ?


   
          
          
                               
                       
      
 
                            ------------------         
                                                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                            Best Regards                                                                                                                                                                                                                                                                                                                                                                                                            
                                                 
                                                                                                                                                                           Ricky Yang                                                                                                                                                               
                                                                                                                                                                                         
                                                                                                                                                             
                                  
                                                                            
                                                                            
             
         
      
        
                            
         
                      
         
                      ------------------ ‘≠ º” º˛ ------------------         
                                                            ∑¢º˛»À:                                   ""Jeff Zhang"";<zjffdu@gmail.com>;             
                                                   ∑¢ÀÕ ±º‰:                                   2015ƒÍ12‘¬20»’(–«∆⁄ÃÏ) œ¬ŒÁ3:44             
                                                    ’º˛»À:                                   ""Luciano Resende""<luckbr1975@gmail.com>;                                  
                                                               ≥≠ÀÕ:                                           ""Michael Armbrust""<michael@databricks.com>; ""dev@spark.apache.org""<dev@spark.apache.org>;                                          
                                                                           ÷˜Ã‚:                                                   Re: [VOTE] Release Apache Spark 1.6.0 (RC3)                     
                 
                                      
                 
                                      +1 (non-binding)                                              
                     
                                              All the test passed, and run it on HDP 2.3.2 sandbox successfully.                     
                 
                                      
                                              On Sun, Dec 20, 2015 at 10:43 AM, Luciano Resende                                                      <                                                              luckbr1975@gmail.com                                                          >                                                  wrote:                         
                                                                                                                            +1 (non-binding)                                     
                                 
                                                                      
                                 
                                 Tested Standalone mode, SparkR and couple Stream Apps, all seem ok.                                 
                             
                                                              
                                                                                                               On Wed, Dec 16, 2015 at 1:32 PM, Michael Armbrust                                                                                      <                                                                                              michael@databricks.com                                                                                          >                                                                                  wrote:                                         
                                                                                                                                                                                                                                                                                                                                                                                            Please vote on releasing the following candidate as Apache Spark version 1.6.0!                                                                                                              
                                                                                                                                                                           
                                                             The vote is open until Saturday, December 19, 2015 at 18                                                                                                                              :00 UTC and passes if a majority of at least 3 +1 PMC votes are cast.                                                                                                                          
                                                                                                              
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           [ ] +1 Release this package as Apache Spark 1.6.0                                                                                                              
                                                                                                                                                                           [ ] -1 Do not release this package because ...                                                                                                              
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           To learn more about Apache Spark, please see                                                                                                                               http://spark.apache.org/                                                                                                                                                                           
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           The tag to be voted on is                                                                                                                                                                                                    v1.6.0-rc3 (168c89e07c51fa24b0bb88582c739cec0acb44d7)                                                                                                                                                                                                                                            
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           The release files, including signatures, digests, etc. can be found at:                                                                                                              
                                                                                                                                                                                                                                            http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-bin/                                                                                                                                                                           
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           Release artifacts are signed with the following key:                                                                                                              
                                                                                                                                                                                                                                            https://people.apache.org/keys/committer/pwendell.asc                                                                                                                                                                           
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                                                                                            The staging repository for this release can be found at:                                                                                                                      
                                                                                                                                                                                                                                                            https://repository.apache.org/content/repositories/orgapachespark-1174/                                                                                                                                                                                       
                                                     
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           The test repository (versioned as v1.6.0-rc3) for this release can be found at:                                                                                                              
                                                                                                                                                                                                                                            https://repository.apache.org/content/repositories/orgapachespark-1173/                                                                                                                                                                           
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           The documentation corresponding to this release can be found at:                                                                                                              
                                                                                                                                                                                                                                            http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-docs/                                                                                                                                                                           
                                                                                                              
                                                     
                                                                                                                                                                           =======================================                                                                                                              
                                                                                                                                                                           == How can I help test this release? ==                                                                                                              
                                                                                                                                                                           =======================================                                                                                                              
                                                                                                                                                                           If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.                                                                                                              
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           ================================================                                                                                                              
                                                                                                                                                                           == What justifies a -1 vote for this release? ==                                                                                                              
                                                                                                                                                                           ================================================                                                                                                              
                                                                                                                                                                           This vote is happening towards the end of the 1.6 QA period, so -1 votes should only occur for significant regressions from 1.5. Bugs already present in 1.5, minor regressions, or bugs related to new features will not block this release.                                                                                                              
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           ===============================================================                                                                                                              
                                                                                                                                                                           == What should happen to JIRA tickets still targeting 1.6.0? ==                                                                                                              
                                                                                                                                                                           ===============================================================                                                                                                              
                                                                                                                                                                           1. It is OK for documentation patches to target 1.6.0 and still go into branch-1.6, since documentations will be published separately from the release.                                                                                                              
                                                                                                                                                                           2. New features for non-alpha-modules should target 1.7+.                                                                                                              
                                                                                                                                                                           3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target version.                                                                                                              
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           
                                                                                                              
                                                                                                                                                                           ==================================================                                                                                                              
                                                                                                                                                                           == Major changes to help you focus your testing ==                                                                                                              
                                                                                                                                                                           ==================================================                                                                                                              
                                                                                                              
                                                     
                                                                                                              
                                                                                                                              Notable changes since 1.6 RC2                                                                                                                      
                                                                                                                      
                                                             - SPARK_VERSION has been set correctly                                                             
                                                             - SPARK-12199 ML Docs are publishing correctly                                                             
                                                             - SPARK-12345 Mesos cluster mode has been fixed                                                                                                                                                                                                                                                
                                                                                                                      
                                                         
                                                                                                                              Notable changes since 1.6 RC1                                                                                                                          
                                                         
                                                         
                                                                                                                              Spark Streaming                                                                                                                      
                                                                                                                      
                                                                                                                                                                                                               SPARK-2629                                                                                                                                                                                                                                                                                          trackStateByKey                                                                                                                                           has been renamed to                                                                                                                                               mapWithState                                                                                                                                                                                                   
                                                                                                                  
                                                                                                                              Spark SQL                                                                                                                      
                                                                                                                      
                                                                                                                                                                                                               SPARK-12165                                                                                                                                                                                                                                                                                         SPARK-12189                                                                                                                                           Fix bugs in eviction of storage memory by execution.                                                                                                                              
                                                             
                                                                                                                                                                                                               SPARK-12258                                                                                                                                           correct passing null into ScalaUDF                                                                                                                              
                                                                                                                  
                                                                                                                              Notable Features Since 1.5                                                                                                                      
                                                         
                                                                                                                              Spark SQL                                                                                                                      
                                                                                                                      
                                                                                                                                                                                                               SPARK-11787                                                                                                                                                                                                                                                                                         Parquet Performance                                                                                                                                           - Improve Parquet scan performance when using flat schemas.                                                                                                                              
                                                             
                                                                                                                                                                                                               SPARK-10810                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Session                                                                                                                                                                                                                                                                                                 Management                                                                                                                                                                                                                                - Isolated devault database (i.e                                                                                                                                                                                                                                                                                             USE mydb                                                                                                                                                                                                                                                                                                ) even on shared clusters.                                                                                                                                                                                                                                                                            
                                                             
                                                                                                                                                                                                               SPARK-9999                                                                                                                                                                                                                                                                                          Dataset API                                                                                                                                           - A type-safe API (similar to RDDs) that performs many opera"
                                                                                                                                                                                                               SPARK-10384                                                                                                                                          ,                                                                                                                                               SPARK-10385                                                                                                                                                                                                                                                                                         Univariate and bivariate statistics in DataFrames                                                                                                                                           - Variance, stddev, correlations, etc.                                                                                                                              
                                                                                                                                      spark.mllib.tree.GradientBoostedTrees validationTol has changed semantics in 1.6. Previously, it was a threshold for absolute change in error. Now, it resembles the behavior of GradientDescent convergenceTol: For large errors, it uses relative error (relative to the previous error); for small errors (< 0.01), it uses absolute error.                                                                                                                              
Alexander Pivovarov <apivovarov@gmail.com>,"Sun, 20 Dec 2015 10:42:57 -0800",Spark fails after 6000s because of akka,dev@spark.apache.org,"I run Spark 1.5.2 on YARN (EMR)

I noticed that my long running jobs always failed after 1h 40 min  (6000s)
with the exceptions below.

Then I found that Spark has spark.akka.heartbeat.pauses=6000s by default

I changed the settings to the following and it solve my issue.

""spark.akka.heartbeat.pauses"": ""60000s"",
""spark.akka.heartbeat.interval"": ""10000s""



RROR ErrorMonitor - Uncaught fatal error from thread
[sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down
ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
	at com.google.protobuf.ByteString.copyFrom(ByteString.java:192)
	at com.google.protobuf.ByteString.copyFrom(ByteString.java:204)
	at akka.remote.MessageSerializer$.serialize(MessageSerializer.scala:36)
	at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:843)
	at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:843)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at akka.remote.EndpointWriter.serializeMessage(Endpoint.scala:842)
	at akka.remote.EndpointWriter.writeSend(Endpoint.scala:743)
	at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:718)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:411)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
ERROR ActorSystemImpl - Uncaught fatal error from thread
[sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down
ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
	at com.google.protobuf.ByteString.copyFrom(ByteString.java:192)
	at com.google.protobuf.ByteString.copyFrom(ByteString.java:204)
	at akka.remote.MessageSerializer$.serialize(MessageSerializer.scala:36)
	at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:843)
	at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:843)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at akka.remote.EndpointWriter.serializeMessage(Endpoint.scala:842)
	at akka.remote.EndpointWriter.writeSend(Endpoint.scala:743)
	at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:718)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:411)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
ERROR ActorSystemImpl - Uncaught fatal error from thread
[sparkDriver-akka.remote.default-remote-dispatcher-5] shutting down
ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2271)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1876)
	at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply$mcV$sp(Serializer.scala:129)
	at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply(Serializer.scala:129)
	at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply(Serializer.scala:129)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at akka.serialization.JavaSerializer.toBinary(Serializer.scala:129)
	at akka.remote.MessageSerializer$.serialize(MessageSerializer.scala:36)
	at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:843)
	at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:843)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at akka.remote.EndpointWriter.serializeMessage(Endpoint.scala:842)
	at akka.remote.EndpointWriter.writeSend(Endpoint.scala:743)
	at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:718)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:411)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
ERROR ActorSystemImpl - Uncaught fatal error from thread
[sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down
ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
	at com.google.protobuf.AbstractMessageLite.toByteArray(AbstractMessageLite.java:62)
	at akka.remote.transport.AkkaPduProtobufCodec$.constructMessage(AkkaPduCodec.scala:138)
	at akka.remote.EndpointWriter.writeSend(Endpoint.scala:740)
	at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:718)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:411)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"
Alexander Pivovarov <apivovarov@gmail.com>,"Sun, 20 Dec 2015 11:28:44 -0800",Re: Spark fails after 6000s because of akka,dev@spark.apache.org,"it can also fail with the following message

Exception in thread ""main"" org.apache.spark.SparkException: Job
aborted due to stage failure: Task 133 in stage 33.1 failed 4 times,
most recent failure: Lost task 133.3 in stage 33.1 (TID 172737,
ip-10-0-25-2.ec2.internal): java.io.IOException: Failed to connect to
ip-10-0-25-2.ec2.internal/10.0.25.2:48048
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused:
ip-10-0-25-2.ec2.internal/10.0.25.2:48048
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1124)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1430)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1409)
	at com.radius.distiller.components.CondenseRecords.saveValidationQa(CondenseRecords.scala:65)
	at com.radius.distiller.Distiller.runCondenseRecords(Distiller.scala:49)
	at com.radius.distiller.Execute$.run(Execute.scala:56)
	at com.radius.distiller.Execute$.main(Execute.scala:33)
	at com.radius.distiller.Execute.main(Execute.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Failed to connect to
ip-10-0-25-2.ec2.internal/10.0.25.2:48048
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused:
ip-10-0-25-2.ec2.internal/10.0.25.2:48048
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more



"
Alexander Pivovarov <apivovarov@gmail.com>,"Sun, 20 Dec 2015 11:29:27 -0800",Re: Spark fails after 6000s because of akka,dev@spark.apache.org,"Or this message

Exception in thread ""main"" org.apache.spark.SparkException: Job
cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:703)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:702)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1514)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1438)
	at org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1724)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1723)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:146)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1124)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1430)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1409)
	at com.radius.distiller.components.CondenseRecords.saveValidationQa(CondenseRecords.scala:65)
	at com.radius.distiller.Distiller.runCondenseRecords(Distiller.scala:49)
	at com.radius.distiller.Execute$.run(Execute.scala:56)
	at com.radius.distiller.Execute$.main(Execute.scala:33)
	at com.radius.distiller.Execute.main(Execute.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



ue to stage failure: Task 133 in stage 33.1 failed 4 times, most recent failure: Lost task 133.3 in stage 33.1 (TID 172737, ip-10-0-25-2.ec2.internal): java.io.IOException: Failed to connect to ip-10-0-25-2.ec2.internal/10.0.25.2:48048
ransportClientFactory.java:193)
ransportClientFactory.java:156)
teAndStart(NettyBlockTransferService.scala:88)
ding(RetryingBlockFetcher.java:140)
yingBlockFetcher.java:43)
lockFetcher.java:170)
1)
java:1145)
.java:615)
2.internal/10.0.25.2:48048
)
tChannel.java:224)
ct(AbstractNioChannel.java:289)
a:528)
ntLoop.java:468)
va:382)
EventExecutor.java:111)
GScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
AGScheduler.scala:1271)
AGScheduler.scala:1270)
scala:59)
:1270)
1.apply(DAGScheduler.scala:697)
1.apply(DAGScheduler.scala:697)
ler.scala:697)
AGScheduler.scala:1496)
Scheduler.scala:1458)
Scheduler.scala:1447)
)
apply$mcV$sp(PairRDDFunctions.scala:1124)
apply(PairRDDFunctions.scala:1065)
apply(PairRDDFunctions.scala:1065)
cala:147)
cala:108)
tions.scala:1065)
ly$mcV$sp(PairRDDFunctions.scala:989)
ly(PairRDDFunctions.scala:965)
ly(PairRDDFunctions.scala:965)
cala:147)
cala:108)
ns.scala:965)
ly$mcV$sp(PairRDDFunctions.scala:897)
ly(PairRDDFunctions.scala:897)
ly(PairRDDFunctions.scala:897)
cala:147)
cala:108)
ns.scala:896)
cala:1430)
09)
09)
cala:147)
cala:108)
enseRecords.scala:65)
java:57)
sorImpl.java:43)
mit$$runMain(SparkSubmit.scala:674)
0)
ernal/10.0.25.2:48048
ransportClientFactory.java:193)
ransportClientFactory.java:156)
teAndStart(NettyBlockTransferService.scala:88)
ding(RetryingBlockFetcher.java:140)
yingBlockFetcher.java:43)
lockFetcher.java:170)
1)
java:1145)
.java:615)
2.internal/10.0.25.2:48048
)
tChannel.java:224)
ct(AbstractNioChannel.java:289)
a:528)
ntLoop.java:468)
va:382)
EventExecutor.java:111)
lt
emote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
t.scala:843)
t.scala:843)
)
tractDispatcher.scala:397)
l.java:1339)
979)
ead.java:107)
ka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
t.scala:843)
t.scala:843)
)
tractDispatcher.scala:397)
l.java:1339)
979)
ead.java:107)
ka.remote.default-remote-dispatcher-5] shutting down ActorSystem [sparkDriver]
ava:93)
tream.java:1876)
jectOutputStream.java:1785)
)
erializer.scala:129)
er.scala:129)
er.scala:129)
t.scala:843)
t.scala:843)
)
tractDispatcher.scala:397)
l.java:1339)
979)
ead.java:107)
ka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
ite.java:62)
Codec.scala:138)
)
tractDispatcher.scala:397)
l.java:1339)
979)
java:107)
"
Alexander Pivovarov <apivovarov@gmail.com>,"Sun, 20 Dec 2015 11:34:26 -0800",Re: Spark fails after 6000s because of akka,dev@spark.apache.org,"Usually Spark EMR job fails with the following exception in 1 hour 40 min - Job
cancelled because SparkContext was shut down

java.util.concurrent.RejectedExecutionException: Task
scala.concurrent.impl.CallbackRunnable@2d602a14 rejected from
java.util.concurrent.ThreadPoolExecutor@46a9e52[Terminated, pool size
= 0, active threads = 0, queued tasks = 0, completed tasks = 6294]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334)
	at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
	at java.lang.Thread.run(Thread.java:745)
Exception in thread ""main"" org.apache.spark.SparkException: Job
cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:703)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:702)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1514)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1438)
	at org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1724)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1723)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:146)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1063)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1057)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply$mcD$sp(DoubleRDDFunctions.scala:34)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:34)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:34)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.DoubleRDDFunctions.sum(DoubleRDDFunctions.scala:33)
	at com.radius.core.util.SparkUtils$.estimateNewPartitionsNum(SparkUtils.scala:41)
	at com.radius.core.util.SparkUtils$.coalesceRdd(SparkUtils.scala:35)
	at com.radius.distiller.Distiller.saveExtract(Distiller.scala:75)
	at com.radius.distiller.Execute$.run(Execute.scala:55)
	at com.radius.distiller.Execute$.main(Execute.scala:29)
	at com.radius.distiller.Execute.main(Execute.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Command exiting with ret '1'



 because SparkContext was shut down
rStop$1.apply(DAGScheduler.scala:703)
rStop$1.apply(DAGScheduler.scala:702)
Scheduler.scala:702)
eduler.scala:1514)
xt.scala:1724)
Thread.run(YarnClientSchedulerBackend.scala:146)
)
apply$mcV$sp(PairRDDFunctions.scala:1124)
apply(PairRDDFunctions.scala:1065)
apply(PairRDDFunctions.scala:1065)
cala:147)
cala:108)
tions.scala:1065)
ly$mcV$sp(PairRDDFunctions.scala:989)
ly(PairRDDFunctions.scala:965)
ly(PairRDDFunctions.scala:965)
cala:147)
cala:108)
ns.scala:965)
ly$mcV$sp(PairRDDFunctions.scala:897)
ly(PairRDDFunctions.scala:897)
ly(PairRDDFunctions.scala:897)
cala:147)
cala:108)
ns.scala:896)
cala:1430)
09)
09)
cala:147)
cala:108)
enseRecords.scala:65)
java:57)
sorImpl.java:43)
mit$$runMain(SparkSubmit.scala:674)
0)
due to stage failure: Task 133 in stage 33.1 failed 4 times, most recent failure: Lost task 133.3 in stage 33.1 (TID 172737, ip-10-0-25-2.ec2.internal): java.io.IOException: Failed to connect to ip-10-0-25-2.ec2.internal/10.0.25.2:48048
TransportClientFactory.java:193)
TransportClientFactory.java:156)
ateAndStart(NettyBlockTransferService.scala:88)
nding(RetryingBlockFetcher.java:140)
ryingBlockFetcher.java:43)
BlockFetcher.java:170)
71)
.java:1145)
r.java:615)
c2.internal/10.0.25.2:48048
4)
etChannel.java:224)
ect(AbstractNioChannel.java:289)
va:528)
entLoop.java:468)
ava:382)
dEventExecutor.java:111)
AGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
DAGScheduler.scala:1271)
DAGScheduler.scala:1270)
.scala:59)
a:1270)
$1.apply(DAGScheduler.scala:697)
$1.apply(DAGScheduler.scala:697)
uler.scala:697)
DAGScheduler.scala:1496)
GScheduler.scala:1458)
GScheduler.scala:1447)
7)
.apply$mcV$sp(PairRDDFunctions.scala:1124)
.apply(PairRDDFunctions.scala:1065)
.apply(PairRDDFunctions.scala:1065)
scala:147)
scala:108)
ctions.scala:1065)
ply$mcV$sp(PairRDDFunctions.scala:989)
ply(PairRDDFunctions.scala:965)
ply(PairRDDFunctions.scala:965)
scala:147)
scala:108)
ons.scala:965)
ply$mcV$sp(PairRDDFunctions.scala:897)
ply(PairRDDFunctions.scala:897)
ply(PairRDDFunctions.scala:897)
scala:147)
scala:108)
ons.scala:896)
scala:1430)
409)
409)
scala:147)
scala:108)
denseRecords.scala:65)
)
.java:57)
ssorImpl.java:43)
bmit$$runMain(SparkSubmit.scala:674)
80)
ternal/10.0.25.2:48048
TransportClientFactory.java:193)
TransportClientFactory.java:156)
ateAndStart(NettyBlockTransferService.scala:88)
nding(RetryingBlockFetcher.java:140)
ryingBlockFetcher.java:43)
BlockFetcher.java:170)
71)
.java:1145)
r.java:615)
c2.internal/10.0.25.2:48048
4)
etChannel.java:224)
ect(AbstractNioChannel.java:289)
va:528)
entLoop.java:468)
ava:382)
dEventExecutor.java:111)
ult
remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
)
nt.scala:843)
nt.scala:843)
8)
stractDispatcher.scala:397)
)
ol.java:1339)
1979)
read.java:107)
kka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
)
nt.scala:843)
nt.scala:843)
8)
stractDispatcher.scala:397)
)
ol.java:1339)
1979)
read.java:107)
kka.remote.default-remote-dispatcher-5] shutting down ActorSystem [sparkDriver]
java:93)
Stream.java:1876)
bjectOutputStream.java:1785)
8)
Serializer.scala:129)
zer.scala:129)
zer.scala:129)
)
nt.scala:843)
nt.scala:843)
8)
stractDispatcher.scala:397)
)
ol.java:1339)
1979)
read.java:107)
kka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
Lite.java:62)
uCodec.scala:138)
8)
stractDispatcher.scala:397)
)
ol.java:1339)
1979)
.java:107)
"
Jerry Lam <chilinglam@gmail.com>,"Sun, 20 Dec 2015 17:59:48 -0500",[Spark SQL] SQLContext getOrCreate incorrect behaviour,Spark dev list <dev@spark.apache.org>,"Hi Spark developers,

I found that SQLContext.getOrCreate(sc: SparkContext) does not behave
correctly when a different spark context is provided.

```
val sc = new SparkContext
val sqlContext =SQLContext.getOrCreate(sc)
sc.stop
...

val sc2 = new SparkContext
val sqlContext2 = SQLContext.getOrCreate(sc2)
sc2.stop
```

The sqlContext2 will reference sc instead of sc2 and therefore, the program
will not work because sc has been stopped.

Best Regards,

Jerry
"
Yin Huai <yhuai@databricks.com>,"Sun, 20 Dec 2015 20:25:50 -0800",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,Jerry Lam <chilinglam@gmail.com>,"Hi Jerry,

Looks like https://issues.apache.org/jira/browse/SPARK-11739 is for the
issue you described. It has been fixed in 1.6. With this change, when you
call SQLContext.getOrCreate(sc2), we will first check if sc has been
stopped. If so, we will create a new SQLContext using sc2.

Thanks,

Yin


"
Josh Rosen <joshrosen@databricks.com>,"Mon, 21 Dec 2015 05:40:07 +0000",Re: Spark fails after 6000s because of akka,"Alexander Pivovarov <apivovarov@gmail.com>, dev@spark.apache.org","Would you mind copying this information into a JIRA ticket to make it
easier to discover / track? Thanks!


pl.CallbackRunnable@2d602a14 rejected from java.util.concurrent.ThreadPoolExecutor@46a9e52[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 6294]
(ThreadPoolExecutor.java:2048)
a:821)
va:1372)
ontextImpl.scala:133)
:40)
a:248)
53)
torService.execute(MoreExecutors.java:293)
ontextImpl.scala:133)
:40)
a:248)
53)
.processBatch$1(Future.scala:643)
.apply$mcV$sp(Future.scala:658)
.apply(Future.scala:635)
.apply(Future.scala:635)
)
la:634)
ture$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
a:685)
:40)
a:248)
53)
torService.execute(MoreExecutors.java:293)
ontextImpl.scala:133)
:40)
a:248)
:334)
ture$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
a:691)
er.scala:467)
ler.scala:419)
la:423)
5)
 because SparkContext was shut down
rStop$1.apply(DAGScheduler.scala:703)
rStop$1.apply(DAGScheduler.scala:702)
Scheduler.scala:702)
eduler.scala:1514)
xt.scala:1724)
Thread.run(YarnClientSchedulerBackend.scala:146)
)
cala:147)
cala:108)
oubleRDDFunctions.scala:34)
DFunctions.scala:34)
DFunctions.scala:34)
cala:147)
cala:108)
33)
scala:41)
java:57)
sorImpl.java:43)
mit$$runMain(SparkSubmit.scala:674)
0)
d because SparkContext was shut down
erStop$1.apply(DAGScheduler.scala:703)
erStop$1.apply(DAGScheduler.scala:702)
GScheduler.scala:702)
heduler.scala:1514)
)
ext.scala:1724)
rThread.run(YarnClientSchedulerBackend.scala:146)
7)
.apply$mcV$sp(PairRDDFunctions.scala:1124)
.apply(PairRDDFunctions.scala:1065)
.apply(PairRDDFunctions.scala:1065)
scala:147)
scala:108)
ctions.scala:1065)
ply$mcV$sp(PairRDDFunctions.scala:989)
ply(PairRDDFunctions.scala:965)
ply(PairRDDFunctions.scala:965)
scala:147)
scala:108)
ons.scala:965)
ply$mcV$sp(PairRDDFunctions.scala:897)
ply(PairRDDFunctions.scala:897)
ply(PairRDDFunctions.scala:897)
scala:147)
scala:108)
ons.scala:896)
scala:1430)
409)
409)
scala:147)
scala:108)
denseRecords.scala:65)
)
.java:57)
ssorImpl.java:43)
bmit$$runMain(SparkSubmit.scala:674)
80)
 due to stage failure: Task 133 in stage 33.1 failed 4 times, most recent failure: Lost task 133.3 in stage 33.1 (TID 172737, ip-10-0-25-2.ec2.internal): java.io.IOException: Failed to connect to ip-10-0-25-2.ec2.internal/10.0.25.2:48048
(TransportClientFactory.java:193)
(TransportClientFactory.java:156)
eateAndStart(NettyBlockTransferService.scala:88)
anding(RetryingBlockFetcher.java:140)
tryingBlockFetcher.java:43)
gBlockFetcher.java:170)
471)
r.java:1145)
or.java:615)
ec2.internal/10.0.25.2:48048
44)
ketChannel.java:224)
nect(AbstractNioChannel.java:289)
ava:528)
ventLoop.java:468)
java:382)
adEventExecutor.java:111)
DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
(DAGScheduler.scala:1271)
(DAGScheduler.scala:1270)
y.scala:59)
la:1270)
d$1.apply(DAGScheduler.scala:697)
d$1.apply(DAGScheduler.scala:697)
duler.scala:697)
(DAGScheduler.scala:1496)
AGScheduler.scala:1458)
AGScheduler.scala:1447)
67)
1.apply$mcV$sp(PairRDDFunctions.scala:1124)
1.apply(PairRDDFunctions.scala:1065)
1.apply(PairRDDFunctions.scala:1065)
.scala:147)
.scala:108)
nctions.scala:1065)
pply$mcV$sp(PairRDDFunctions.scala:989)
pply(PairRDDFunctions.scala:965)
pply(PairRDDFunctions.scala:965)
.scala:147)
.scala:108)
ions.scala:965)
pply$mcV$sp(PairRDDFunctions.scala:897)
pply(PairRDDFunctions.scala:897)
pply(PairRDDFunctions.scala:897)
.scala:147)
.scala:108)
ions.scala:896)
.scala:1430)
1409)
1409)
.scala:147)
.scala:108)
ndenseRecords.scala:65)
9)
l.java:57)
essorImpl.java:43)
ubmit$$runMain(SparkSubmit.scala:674)
180)
nternal/10.0.25.2:48048
(TransportClientFactory.java:193)
(TransportClientFactory.java:156)
eateAndStart(NettyBlockTransferService.scala:88)
anding(RetryingBlockFetcher.java:140)
tryingBlockFetcher.java:43)
gBlockFetcher.java:170)
471)
r.java:1145)
or.java:615)
ec2.internal/10.0.25.2:48048
44)
ketChannel.java:224)
nect(AbstractNioChannel.java:289)
ava:528)
ventLoop.java:468)
java:382)
adEventExecutor.java:111)
ault
.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
6)
int.scala:843)
int.scala:843)
18)
bstractDispatcher.scala:397)
0)
ool.java:1339)
:1979)
hread.java:107)
akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
6)
int.scala:843)
int.scala:843)
18)
bstractDispatcher.scala:397)
0)
ool.java:1339)
:1979)
hread.java:107)
akka.remote.default-remote-dispatcher-5] shutting down ActorSystem [sparkDriver]
.java:93)
)
tStream.java:1876)
ObjectOutputStream.java:1785)
88)
)
(Serializer.scala:129)
izer.scala:129)
izer.scala:129)
6)
int.scala:843)
int.scala:843)
18)
bstractDispatcher.scala:397)
0)
ool.java:1339)
:1979)
hread.java:107)
akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
eLite.java:62)
duCodec.scala:138)
18)
bstractDispatcher.scala:397)
0)
ool.java:1339)
:1979)
d.java:107)
"
Alexander Pivovarov <apivovarov@gmail.com>,"Sun, 20 Dec 2015 22:40:03 -0800",Re: Spark fails after 6000s because of akka,Josh Rosen <joshrosen@databricks.com>,"Documentation says that this setting is used to disable Akka transport
failure detector.
Why magic number 6000s is used then?
It should be maximum possible number instead of 6000s to disable heartbeat

Using magic numbers like 1 hour and 40 min creates issues which are
difficult to debug. Most probably all Spark integration tests run faster
that 6000s and this area is basically not tested.

Anyone know why spark.akka.heartbeat.pauses=6000s  ???



m>
n
mpl.CallbackRunnable@2d602a14 rejected from java.util.concurrent.ThreadPoolExecutor@46a9e52[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 6294]
n(ThreadPoolExecutor.java:2048)
va:821)
ava:1372)
ContextImpl.scala:133)
a:40)
la:248)
153)
utorService.execute(MoreExecutors.java:293)
ContextImpl.scala:133)
a:40)
la:248)
153)
1.processBatch$1(Future.scala:643)
1.apply$mcV$sp(Future.scala:658)
1.apply(Future.scala:635)
1.apply(Future.scala:635)
2)
ala:634)
uture$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
la:685)
a:40)
la:248)
153)
utorService.execute(MoreExecutors.java:293)
ContextImpl.scala:133)
a:40)
la:248)
a:334)
uture$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
la:691)
ler.scala:467)
uler.scala:419)
ala:423)
75)
d because SparkContext was shut down
erStop$1.apply(DAGScheduler.scala:703)
erStop$1.apply(DAGScheduler.scala:702)
GScheduler.scala:702)
heduler.scala:1514)
)
ext.scala:1724)
rThread.run(YarnClientSchedulerBackend.scala:146)
7)
scala:147)
scala:108)
DoubleRDDFunctions.scala:34)
DDFunctions.scala:34)
DDFunctions.scala:34)
scala:147)
scala:108)
:33)
.scala:41)
.java:57)
ssorImpl.java:43)
bmit$$runMain(SparkSubmit.scala:674)
80)
ed because SparkContext was shut down
lerStop$1.apply(DAGScheduler.scala:703)
lerStop$1.apply(DAGScheduler.scala:702)
AGScheduler.scala:702)
cheduler.scala:1514)
8)
text.scala:1724)
orThread.run(YarnClientSchedulerBackend.scala:146)
67)
1.apply$mcV$sp(PairRDDFunctions.scala:1124)
1.apply(PairRDDFunctions.scala:1065)
1.apply(PairRDDFunctions.scala:1065)
.scala:147)
.scala:108)
nctions.scala:1065)
pply$mcV$sp(PairRDDFunctions.scala:989)
pply(PairRDDFunctions.scala:965)
pply(PairRDDFunctions.scala:965)
.scala:147)
.scala:108)
ions.scala:965)
pply$mcV$sp(PairRDDFunctions.scala:897)
pply(PairRDDFunctions.scala:897)
pply(PairRDDFunctions.scala:897)
.scala:147)
.scala:108)
ions.scala:896)
.scala:1430)
1409)
1409)
.scala:147)
.scala:108)
ndenseRecords.scala:65)
9)
l.java:57)
essorImpl.java:43)
ubmit$$runMain(SparkSubmit.scala:674)
180)
d due to stage failure: Task 133 in stage 33.1 failed 4 times, most recent failure: Lost task 133.3 in stage 33.1 (TID 172737, ip-10-0-25-2.ec2.internal): java.io.IOException: Failed to connect to ip-10-0-25-2.ec2.internal/10.0.25.2:48048
t(TransportClientFactory.java:193)
t(TransportClientFactory.java:156)
reateAndStart(NettyBlockTransferService.scala:88)
tanding(RetryingBlockFetcher.java:140)
etryingBlockFetcher.java:43)
ngBlockFetcher.java:170)
:471)
or.java:1145)
tor.java:615)
.ec2.internal/10.0.25.2:48048
744)
cketChannel.java:224)
nnect(AbstractNioChannel.java:289)
java:528)
EventLoop.java:468)
.java:382)
eadEventExecutor.java:111)
$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
y(DAGScheduler.scala:1271)
y(DAGScheduler.scala:1270)
ay.scala:59)
ala:1270)
ed$1.apply(DAGScheduler.scala:697)
ed$1.apply(DAGScheduler.scala:697)
eduler.scala:697)
e(DAGScheduler.scala:1496)
DAGScheduler.scala:1458)
DAGScheduler.scala:1447)
567)
$1.apply$mcV$sp(PairRDDFunctions.scala:1124)
$1.apply(PairRDDFunctions.scala:1065)
$1.apply(PairRDDFunctions.scala:1065)
e.scala:147)
e.scala:108)
unctions.scala:1065)
apply$mcV$sp(PairRDDFunctions.scala:989)
apply(PairRDDFunctions.scala:965)
apply(PairRDDFunctions.scala:965)
e.scala:147)
e.scala:108)
tions.scala:965)
apply$mcV$sp(PairRDDFunctions.scala:897)
apply(PairRDDFunctions.scala:897)
apply(PairRDDFunctions.scala:897)
e.scala:147)
e.scala:108)
tions.scala:896)
D.scala:1430)
:1409)
:1409)
e.scala:147)
e.scala:108)
ondenseRecords.scala:65)
49)
pl.java:57)
cessorImpl.java:43)
Submit$$runMain(SparkSubmit.scala:674)
:180)
internal/10.0.25.2:48048
t(TransportClientFactory.java:193)
t(TransportClientFactory.java:156)
reateAndStart(NettyBlockTransferService.scala:88)
tanding(RetryingBlockFetcher.java:140)
etryingBlockFetcher.java:43)
ngBlockFetcher.java:170)
:471)
or.java:1145)
tor.java:615)
.ec2.internal/10.0.25.2:48048
744)
cketChannel.java:224)
nnect(AbstractNioChannel.java:289)
java:528)
EventLoop.java:468)
.java:382)
eadEventExecutor.java:111)
a.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
36)
oint.scala:843)
oint.scala:843)
718)
AbstractDispatcher.scala:397)
60)
Pool.java:1339)
a:1979)
Thread.java:107)
-akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
36)
oint.scala:843)
oint.scala:843)
718)
AbstractDispatcher.scala:397)
60)
Pool.java:1339)
a:1979)
Thread.java:107)
-akka.remote.default-remote-dispatcher-5] shutting down ActorSystem [sparkDriver]
)
m.java:93)
3)
utStream.java:1876)
(ObjectOutputStream.java:1785)
188)
7)
p(Serializer.scala:129)
lizer.scala:129)
lizer.scala:129)
36)
oint.scala:843)
oint.scala:843)
718)
AbstractDispatcher.scala:397)
60)
Pool.java:1339)
a:1979)
Thread.java:107)
-akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
geLite.java:62)
PduCodec.scala:138)
718)
AbstractDispatcher.scala:397)
60)
Pool.java:1339)
a:1979)
ad.java:107)
"
Roland Reumerman <Roland.Reumerman@mendix.com>,"Mon, 21 Dec 2015 10:26:01 +0000",Expression/LogicalPlan dichotomy in Spark SQL Catalyst,"""dev@spark.apache.org"" <dev@spark.apache.org>","[Note: this question has been moved from the Conversation in

[SPARK-4226][SQL]Add subquery (not) in/exists support #9055

to the dev mailing list.]


We've added our own In/Exists - plus Subquery in Select - support to a partial fork of Spark SQL Catalyst (which we use in transformations from our own query language to SQL for relational databases). But since In, Exists and Select projections are Expressions which will then contain LogicalPlans (Subquery/Select with nested LogicalPlans with potential nested Expressions) this makes whole-tree transformations really cumbersome since we've got to deal with 'pivot points' for these 2 types of TreeNodes, where a recursive transformation can only be done on 1 specific type of children, and then has to be dealt with again within the same PartialFunction for the other type in which the matching case(s) can be nested. Why was the choice made in Catalyst to make LogicalPlan/QueryPlan and Expression separate subclasses of TreeNode, instead of e.g. also make QueryPlan inherit from Expression? The code also contains duplicate functionality, like LeafNode/LeafExpression, UnaryNode/UnaryExpression and BinaryNode/BinaryExpression.


Much obliged,

Roland Reumerman

mendix.com

"
"""Chester @work"" <chester@alpinenow.com>","Mon, 21 Dec 2015 08:23:19 -0800",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,Jerry Lam <chilinglam@gmail.com>,"Jerry
    I thought you should not create more than one SparkContext within one Jvm, ...
Chester

Sent from my iPhone

ectly when a different spark context is provided.
m will not work because sc has been stopped. 

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 21 Dec 2015 08:32:38 -0800",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,"""Chester @work"" <chester@alpinenow.com>","In Jerry's example, the first SparkContext, sc, has been stopped.

So there would be only one SparkContext running at any given moment.

Cheers


"
Renyi Xiong <renyixiong0@gmail.com>,"Mon, 21 Dec 2015 09:52:50 -0800",pyspark streaming 1.6 mapWithState?,"Tathagata Das <tdas@databricks.com>, dev@spark.apache.org","Hi TD,

I noticed mapWithState was available in spark 1.6. Is there any plan to
enable it in pyspark as well?

thanks,
Renyi.
"
PierreB <pierre.borckmans@realimpactanalytics.com>,"Mon, 21 Dec 2015 13:45:43 -0700 (MST)","Re: Tungsten gives unexpected results when selecting null elements
 in array",dev@spark.apache.org,"For info, this is the generated code:
<pre>
GeneratedExpressionCode(
      cursor8 = 16;
      convertedStruct6.pointTo(buffer7, Platform.BYTE_ARRAY_OFFSET, 1,
cursor8);
      
        /* input[0, ArrayType(StringType,true)][0] */

      /* input[0, ArrayType(StringType,true)] */

      boolean isNull2 = i.isNullAt(0);
      ArrayData primitive3 = isNull2 ? null : (i.getArray(0));
    
      boolean isNull0 = isNull2;
      UTF8String primitive1 = null;
      if (!isNull0) {
        /* 0 */

        if (!false) {
          
        final int index = (int) 0;
        if (index >= primitive3.numElements() || index < 0) {
          isNull0 = true;
        } else {
          primitive1 = primitive3.getUTF8String(index);
        }
      
        } else {
          isNull0 = true;
        }
      }
    
        
          int numBytes10 = cursor8 + (isNull0 ? 0 :
org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters$UTF8StringWriter.getSize(primitive1));
          if (buffer7.length < numBytes10) {
            // This will not happen frequently, because the buffer is
re-used.
            byte[] tmpBuffer9 = new byte[numBytes10 * 2];
            Platform.copyMemory(buffer7, Platform.BYTE_ARRAY_OFFSET,
              tmpBuffer9, Platform.BYTE_ARRAY_OFFSET, buffer7.length);
            buffer7 = tmpBuffer9;
          }
          convertedStruct6.pointTo(buffer7, Platform.BYTE_ARRAY_OFFSET, 1,
numBytes10);
         
        
          if (isNull0) {
            convertedStruct6.setNullAt(0);
          } else {
            cursor8 +=
org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters$UTF8StringWriter.write(convertedStruct6,
0, cursor8, primitive1);
          }
        
      
      ,false,convertedStruct6)
</pre>

The culprit line is the following:
<pre>
          int numBytes10 = cursor8 + (isNull0 ? 0 :
org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters$UTF8StringWriter.getSize(primitive1));
</pre>




--

---------------------------------------------------------------------


"
PierreB <pierre.borckmans@realimpactanalytics.com>,"Mon, 21 Dec 2015 14:12:45 -0700 (MST)","Re: Tungsten gives unexpected results when selecting null elements
 in array",dev@spark.apache.org,"I believe the problem is that the generated code does not check if the
selected item in the array is null.Na√Øvely, I think changing this line would
solve this:
https://github.com/apache/spark/blob/4af647c77ded6a0d3087ceafb2e30e01d97e7a06/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala#L219Replace:
if (index >= $eval1.numElements() || index < 0) {
By:
if (index >= $eval1.numElements() || index < 0 || ctx.getValue(eval1,
dataType, ""index"") == null ) { 
Does that make sense?



--
3.nabble.com/Tungsten-gives-unexpected-results-when-selecting-null-elements-in-array-tp15717p15719.html
om."
Michael Armbrust <michael@databricks.com>,"Mon, 21 Dec 2015 15:52:00 -0800",Re: Expression/LogicalPlan dichotomy in Spark SQL Catalyst,Roland Reumerman <Roland.Reumerman@mendix.com>,"I think this is a pretty common way to model things (glancing at postgres
it looks similar).  Expression and plans are pretty different concepts.  An
expression can be evaluated on a single input row and returns a single
value.  In contrast a query plan operates on a relation and has a schema
with many different atomic values.




These traits actually have different semantics for expressions vs. plans
(i.e. a UnaryExpression nullability is based on its child's nullability,
whereas this would not make sense for a UnaryNode which does not have a
concept of nullability).




It is not clear to me that you actually want these transformations to
happen seamlessly.  For example, the resolution rules for subqueries are
different than normal plans because you have to reason about correlation.
That said, it seems like you should be able to do some magic in
RuleExecutor to make sure that things like the optimizer descend seamlessly
into nested query plans.
"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 22 Dec 2015 00:20:53 +0000",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,Jerry Lam <chilinglam@gmail.com>,"This looks to me is a very unusual use case. You stop the SparkContext, and start another one. I donít think it is well supported. As the SparkContext is stopped, all the resources are supposed to be released. 

Is there any mandatory reason you have stop and restart another SparkContext.

Thanks.

Zhan Zhang

Note that when sc is stopped, all resources are released (for example in yarn 

rectly when a different spark context is provided.
am will not work because sc has been stopped. 


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 21 Dec 2015 17:48:37 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","It's come to my attention that there have been several bug fixes merged
since RC3:

  - SPARK-12404 - Fix serialization error for Datasets with
Timestamps/Arrays/Decimal
  - SPARK-12218 - Fix incorrect pushdown of filters to parquet
  - SPARK-12395 - Fix join columns of outer join for DataFrame using
  - SPARK-12413 - Fix mesos HA

Normally, these would probably not be sufficient to hold the release,
however with the holidays going on in the US this week, we don't have the
resources to finalize 1.6 until next Monday.  Given this delay anyway, I
propose that we cut one final RC with the above fixes and plan for the
actual release first thing next week.

I'll post RC4 shortly and cancel this vote if there are no objections.
Since this vote nearly passed with no major issues, I don't anticipate any
problems with RC4.

Michael


"
Jerry Lam <chilinglam@gmail.com>,"Tue, 22 Dec 2015 00:13:02 -0500",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,Zhan Zhang <zzhang@hortonworks.com>,"Hi Zhan,

I'm illustrating the issue via a simple example. However it is not
difficult to imagine use cases that need this behaviour. For example, you
want to release all resources of spark when it does not use for longer than
an hour in  a job server like web services. Unless you can prevent people
from stopping spark context, then it is reasonable to assume that people
can stop it and start it again in  later time.

Best Regards,

Jerry



"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Dec 2015 21:18:58 -0800",Re: Tungsten gives unexpected results when selecting null elements in array,PierreB <pierre.borckmans@realimpactanalytics.com>,"Thanks for the email. Do you mind creating a JIRA ticket and reply with a
link to the ticket?


ine
7a06/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala#L219
ataType, ""index"") == null ) {
-unexpected-results-when-selecting-null-elements-in-array-tp15717p15719.html>
"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Dec 2015 22:12:50 -0800",Re: A proposal for Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI I updated the master branch's Spark version to 2.0.0-SNAPSHOT.


 feature
res in
e
 a
n
m
is
1.X line.
r
s
ependency
In
ic
c
ld
g
 enormous
"
"""Allen Zhang"" <allenzhang010@126.com>","Tue, 22 Dec 2015 15:18:22 +0800 (CST)",Re: A proposal for Spark 2.0,"""Allen Zhang"" <allenzhang010@126.com>, ""Reynold Xin"" <rxin@databricks.com>","plus dev






‘⁄ 2015-12-22 15:15:59£¨""Allen Zhang"" <allenzhang010@126.com> –¥µ¿£∫

Hi Reynold,


Any new API support for GPU computing in our 2.0 new version ?


-Allen





‘⁄ 2015-12-22 14:12:50£¨""Reynold Xin"" <rxin@databricks.com> –¥µ¿£∫

FYI I updated the master branch's Spark version to 2.0.0-SNAPSHOT. 


On Tue, Nov 10, 2015 at 3:10 PM, Reynold Xin <rxin@databricks.com> wrote:

I°Øm starting a new thread since the other one got intermixed with feature requests. Please refrain from making feature request in this thread. Not that we shouldn°Øt be adding features, but we can always add features in 1.7, 2.1, 2.2, ...


First - I want to propose a premise for how to think about Spark 2.0 and major releases in Spark, based on discussion with several members of the community: a major release should be low overhead and minimally disruptive to the Spark community. A major release should not be very different from a minor release and should not be gated based on new features. The main purpose of a major release is an opportunity to fix things that are broken in the current API and remove certain deprecated APIs (examples follow).


For this reason, I would *not* propose doing major releases to break substantial API's or perform large re-architecting that prevent users from upgrading. Spark has always had a culture of evolving architecture incrementally and making changes - and I don't think we want to change this model. In fact, we°Øve released many architectural changes on the 1.X line.


If the community likes the above model, then to me it seems reasonable to do Spark 2.0 either after Spark 1.6 (in lieu of Spark 1.7) or immediately after Spark 1.7. It will be 18 or 21 months since Spark 1.0. A cadence of major releases every 2 years seems doable within the above model.


Under this model, here is a list of example things I would propose doing in Spark 2.0, separated into APIs and Operation/Deployment:




APIs


1. Remove interfaces, configs, and modules (e.g. Bagel) deprecated in Spark 1.x.


2. Remove Akka from Spark°Øs API dependency (in streaming), so user applications can use Akka (SPARK-5293). We have gotten a lot of complaints about user applications being unable to use Akka due to Spark°Øs dependency on Akka.


3. Remove Guava from Spark°Øs public API (JavaRDD Optional).


4. Better class package structure for low level developer API°Øs. In particular, we have some DeveloperApi (mostly various listener-related classes) added over the years. Some packages include only one or two public classes but a lot of private classes. A better structure is to have public classes isolated to a few public packages, and these public packages should have minimal private classes for low level developer APIs.


5. Consolidate task metric and accumulator API. Although having some subtle differences, these two are very similar but have completely different code path.


6. Possibly making Catalyst, Dataset, and DataFrame more general by moving them to other package(s). They are already used beyond SQL, e.g. in ML pipelines, and will be used by streaming also.




Operation/Deployment


1. Scala 2.11 as the default build. We should still support Scala 2.10, but it has been end-of-life.


2. Remove Hadoop 1 support. 


3. Assembly-free distribution of Spark: don°Øt require building an enormous assembly jar in order to run Spark.








 "
Reynold Xin <rxin@databricks.com>,"Mon, 21 Dec 2015 23:19:49 -0800",Re: A proposal for Spark 2.0,Allen Zhang <allenzhang010@126.com>,"I'm not sure if we need special API support for GPUs. You can already use
GPUs on individual executor nodes to build your own applications. If we
want to leverage GPUs out of the box, I don't think the solution is to
provide GPU specific APIs. Rather, we should just switch the underlying
execution to GPUs when it is more optimal.

Anyway, I don't want to distract this topic, If you want to discuss more
about GPUs, please start a new thread.


:

m> ÂÜôÈÅìÔºö
 ÂÜôÈÅìÔºö
h feature
ures in
ve
m a
en
om
his
 1.X line.
o
y
f
er
ts
dependency
 In
lic
ic
uld
in
n
"
"""Allen Zhang"" <allenzhang010@126.com>","Tue, 22 Dec 2015 15:22:36 +0800 (CST)",Re: A proposal for Spark 2.0,"""Reynold Xin"" <rxin@databricks.com>","

Thanks your quick respose, ok, I will start a new thread with my thoughts


Thanks,
Allen





At 2015-12-22 15:19:49, ""Reynold Xin"" <rxin@databricks.com> wrote:

I'm not sure if we need special API support for GPUs. You can already use GPUs on individual executor nodes to build your own applications. If we want to leverage GPUs out of the box, I don't think the solution is to provide GPU specific APIs. Rather, we should just switch the underlying execution to GPUs when it is more optimal.


Anyway, I don't want to distract this topic, If you want to discuss more about GPUs, please start a new thread.




On Mon, Dec 21, 2015 at 11:18 PM, Allen Zhang <allenzhang010@126.com> wrote:

plus dev







‘⁄ 2015-12-22 15:15:59£¨""Allen Zhang"" <allenzhang010@126.com> –¥µ¿£∫

Hi Reynold,


Any new API support for GPU computing in our 2.0 new version ?


-Allen





‘⁄ 2015-12-22 14:12:50£¨""Reynold Xin"" <rxin@databricks.com> –¥µ¿£∫

FYI I updated the master branch's Spark version to 2.0.0-SNAPSHOT. 


On Tue, Nov 10, 2015 at 3:10 PM, Reynold Xin <rxin@databricks.com> wrote:

I°Øm starting a new thread since the other one got intermixed with feature requests. Please refrain from making feature request in this thread. Not that we shouldn°Øt be adding features, but we can always add features in 1.7, 2.1, 2.2, ...


First - I want to propose a premise for how to think about Spark 2.0 and major releases in Spark, based on discussion with several members of the community: a major release should be low overhead and minimally disruptive to the Spark community. A major release should not be very different from a minor release and should not be gated based on new features. The main purpose of a major release is an opportunity to fix things that are broken in the current API and remove certain deprecated APIs (examples follow).


For this reason, I would *not* propose doing major releases to break substantial API's or perform large re-architecting that prevent users from upgrading. Spark has always had a culture of evolving architecture incrementally and making changes - and I don't think we want to change this model. In fact, we°Øve released many architectural changes on the 1.X line.


If the community likes the above model, then to me it seems reasonable to do Spark 2.0 either after Spark 1.6 (in lieu of Spark 1.7) or immediately after Spark 1.7. It will be 18 or 21 months since Spark 1.0. A cadence of major releases every 2 years seems doable within the above model.


Under this model, here is a list of example things I would propose doing in Spark 2.0, separated into APIs and Operation/Deployment:




APIs


1. Remove interfaces, configs, and modules (e.g. Bagel) deprecated in Spark 1.x.


2. Remove Akka from Spark°Øs API dependency (in streaming), so user applications can use Akka (SPARK-5293). We have gotten a lot of complaints about user applications being unable to use Akka due to Spark°Øs dependency on Akka.


3. Remove Guava from Spark°Øs public API (JavaRDD Optional).


4. Better class package structure for low level developer API°Øs. In particular, we have some DeveloperApi (mostly various listener-related classes) added over the years. Some packages include only one or two public classes but a lot of private classes. A better structure is to have public classes isolated to a few public packages, and these public packages should have minimal private classes for low level developer APIs.


5. Consolidate task metric and accumulator API. Although having some subtle differences, these two are very similar but have completely different code path.


6. Possibly making Catalyst, Dataset, and DataFrame more general by moving them to other package(s). They are already used beyond SQL, e.g. in ML pipelines, and will be used by streaming also.




Operation/Deployment


1. Scala 2.11 as the default build. We should still support Scala 2.10, but it has been end-of-life.


2. Remove Hadoop 1 support. 


3. Assembly-free distribution of Spark: don°Øt require building an enormous assembly jar in order to run Spark.








 





 


"
PierreB <pierre.borckmans@realimpactanalytics.com>,"Tue, 22 Dec 2015 01:53:31 -0700 (MST)","Re: Tungsten gives unexpected results when selecting null elements
 in array",dev@spark.apache.org,"FYI, I filed a JIRA and submitted a PR for this issue:

https://issues.apache.org/jira/browse/SPARK-12477
<https://issues.apache.org/jira/browse/SPARK-12477>  
https://github.com/apache/spark/pull/10429
<https://github.com/apache/spark/pull/10429>  



--

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 22 Dec 2015 14:27:51 +0000 (UTC)",Re: A proposal for Spark 2.0,"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Do we have a summary of all the discussions and what is planned for 2.0 then? ¬†Perhaps we should put on the wiki for reference.
Tom 

 

 FYI I updated the master branch's Spark version to 2.0.0-SNAPSHOT.¬†

I‚Äôm starting a new thread since the other one got intermixed with feature requests. Please refrain from making feature request in this thread. Not that we shouldn‚Äôt be adding features, but we can always add features in 1.7, 2.1, 2.2, ...
First - I want to propose a premise for how to think about Spark 2.0 and major releases in Spark, based on discussion with several members of the community: a major release should be low overhead and minimally disruptive to the Spark community. A major release should not be very different from a minor release and should not be gated based on new features. The main purpose of a major release is an opportunity to fix things that are broken in the current API and remove certain deprecated APIs (examples follow).
For this reason, I would *not* propose doing major releases to break substantial API's or perform large re-architecting that prevent users from upgrading. Spark has always had a culture of evolving architecture incrementally and making changes - and I don't think we want to change this model. In fact, we‚Äôve released many architectural changes on the 1.X line.
If the community likes the above model, then to me it seems reasonable to do Spark 2.0 either after Spark 1.6 (in lieu of Spark 1.7) or immediately after Spark 1.7. It will be 18 or 21 months since Spark 1.0. A cadence of major releases every 2 years seems doable within the above model.
Under this model, here is a list of example things I would propose doing in Spark 2.0, separated into APIs and Operation/Deployment:

APIs
1. Remove interfaces, configs, and modules (e.g. Bagel) deprecated in Spark 1.x.
2. Remove Akka from Spark‚Äôs API dependency (in streaming), so user applications can use Akka (SPARK-5293). We have gotten a lot of complaints about user applications being unable to use Akka due to Spark‚Äôs dependency on Akka.
3. Remove Guava from Spark‚Äôs public API (JavaRDD Optional).
4. Better class package structure for low level developer API‚Äôs. In particular, we have some DeveloperApi (mostly various listener-related classes) added over the years. Some packages include only one or two public classes but a lot of private classes. A better structure is to have public classes isolated to a few public packages, and these public packages should have minimal private classes for low level developer APIs.
5. Consolidate task metric and accumulator API. Although having some subtle differences, these two are very similar but have completely different code path.
6. Possibly making Catalyst, Dataset, and DataFrame more general by moving them to other package(s). They are already used beyond SQL, e.g. in ML pipelines, and will be used by streaming also.

Operation/Deployment
1. Scala 2.11 as the default build. We should still support Scala 2.10, but it has been end-of-life.
2. Remove Hadoop 1 support.¬†
3. Assembly-free distribution of Spark: don‚Äôt require building an enormous assembly jar in order to run Spark.




  "
Jerry Lam <chilinglam@gmail.com>,"Tue, 22 Dec 2015 12:57:13 -0500",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,Sean Owen <sowen@cloudera.com>,"Hi Sean,

What if the spark context stops for involuntary reasons (misbehavior of some connections) then we need to programmatically handle the failures by recreating spark context. Is there something I don't understand/know about the assumptions on how to use spark context? I tend to think of it as a resource manager/scheduler for spark jobs. Are you guys planning to deprecate the stop method from spark? 

Best Regards,

Jerry 

Sent from my iPhone

ult
ur
top
te:


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 22 Dec 2015 19:00:21 +0000",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,Jerry Lam <chilinglam@gmail.com>,"I think the original idea is that the life of the driver is the life
of the SparkContext: the context is stopped when the driver finishes.
Or: if for some reason the ""context"" dies or there's an unrecoverable
error, that's it for the driver.

(There's nothing wrong with stop(), right? you have to call that when
the driver ends to shut down Spark cleanly. It's the re-starting
another context that's at issue.)

This makes most sense in the context of a resource manager, which can
conceivably restart a driver if you like, but can't reach into your
program.

That's probably still the best way to think of it. Still it would be
nice if SparkContext were friendlier to a restart just as a matter of
design. AFAIK it is; not sure about SQLContext though. If it's not a
priority it's just because this isn't a usual usage pattern, which
doesn't mean it's crazy, just not the primary pattern.

ome connections) then we need to programmatically handle the failures by recreating spark context. Is there something I don't understand/know about the assumptions on how to use spark context? I tend to think of it as a resource manager/scheduler for spark jobs. Are you guys planning to deprecate the stop method from spark?
:
icult
hour
 stop
rote:
,
the
.
in

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 22 Dec 2015 12:10:46 -0800",[VOTE] Release Apache Spark 1.6.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.0!

The vote is open until Friday, December 25, 2015 at 18:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is *v1.6.0-rc4
(4062cda3087ae42c6c3cb24508fc1d3a931accdf)
<https://github.com/apache/spark/tree/v1.6.0-rc4>*

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1176/

The test repository (versioned as v1.6.0-rc4) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1175/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-docs/

=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.

===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.


==================================================
== Major changes to help you focus your testing ==
==================================================

Notable changes since 1.6 RC3

  - SPARK-12404 - Fix serialization error for Datasets with
Timestamps/Arrays/Decimal
  - SPARK-12218 - Fix incorrect pushdown of filters to parquet
  - SPARK-12395 - Fix join columns of outer join for DataFrame using
  - SPARK-12413 - Fix mesos HA

Notable changes since 1.6 RC2
- SPARK_VERSION has been set correctly
- SPARK-12199 ML Docs are publishing correctly
- SPARK-12345 Mesos cluster mode has been fixed

Notable changes since 1.6 RC1
Spark Streaming

   - SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629>
   trackStateByKey has been renamed to mapWithState

Spark SQL

   - SPARK-12165 <https://issues.apache.org/jira/browse/SPARK-12165>
   SPARK-12189 <https://issues.apache.org/jira/browse/SPARK-12189> Fix bugs
   in eviction of storage memory by execution.
   - SPARK-12258 <https://issues.apache.org/jira/browse/SPARK-12258> correct
   passing null into ScalaUDF

Notable Features Since 1.5Spark SQL

   - SPARK-11787 <https://issues.apache.org/jira/browse/SPARK-11787> Parquet
   Performance - Improve Parquet scan performance when using flat schemas.
   - SPARK-10810 <https://issues.apache.org/jira/browse/SPARK-10810>
   Session Management - Isolated devault database (i.e USE mydb) even on
   shared clusters.
   - SPARK-9999  <https://issues.apache.org/jira/browse/SPARK-9999> Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000 <https://issues.apache.org/jira/browse/SPARK-10000> Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197 <https://issues.apache.org/jira/browse/SPARK-11197> SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745 <https://issues.apache.org/jira/browse/SPARK-11745> Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 <https://issues.apache.org/jira/browse/SPARK-10412>
Per-operator
   Metrics for SQL Execution - Display statistics on a peroperator basis
   for memory usage and spilled data size.
   - SPARK-11329 <https://issues.apache.org/jira/browse/SPARK-11329> Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 <https://issues.apache.org/jira/browse/SPARK-10917>,
   SPARK-11149 <https://issues.apache.org/jira/browse/SPARK-11149> In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111 <https://issues.apache.org/jira/browse/SPARK-11111> Fast
   null-safe joins - Joins using null-safe equality (<=>) will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389 <https://issues.apache.org/jira/browse/SPARK-11389> SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978 <https://issues.apache.org/jira/browse/SPARK-10978> Datasource
   API Avoid Double Filter - When implemeting a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849  <https://issues.apache.org/jira/browse/SPARK-4849> Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858  <https://issues.apache.org/jira/browse/SPARK-9858> Adaptive
   query execution - Intial support for automatically selecting the number
   of reducers for joins and aggregations.
   - SPARK-9241  <https://issues.apache.org/jira/browse/SPARK-9241> Improved
   query planner for queries having distinct aggregations - Query plans of
   distinct aggregations are more robust when distinct columns have high
   cardinality.

Spark Streaming

   - API Updates
      - SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629> New
      improved state management - mapWithState - a DStream transformation
      for stateful stream processing, supercedes updateStateByKey in
      functionality and performance.
      - SPARK-11198 <https://issues.apache.org/jira/browse/SPARK-11198> Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891 <https://issues.apache.org/jira/browse/SPARK-10891> Kinesis
      message handler function - Allows arbitraray function to be applied
      to a Kinesis record in the Kinesis receiver before to customize what data
      is to be stored in memory.
      - SPARK-6328  <https://issues.apache.org/jira/browse/SPARK-6328> Python
      Streamng Listener API - Get streaming statistics (scheduling delays,
      batch processing times, etc.) in streaming.


   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress
      bars.

MLlibNew algorithms/models

   - SPARK-8518  <https://issues.apache.org/jira/browse/SPARK-8518> Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834  <https://issues.apache.org/jira/browse/SPARK-9834> Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930  <https://issues.apache.org/jira/browse/SPARK-9930> New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517  <https://issues.apache.org/jira/browse/SPARK-6517> Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means

API improvements

   - ML Pipelines
      - SPARK-6725  <https://issues.apache.org/jira/browse/SPARK-6725> Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.mlalgorithms
      - SPARK-5565  <https://issues.apache.org/jira/browse/SPARK-5565> LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836  <https://issues.apache.org/jira/browse/SPARK-9836> R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681  <https://issues.apache.org/jira/browse/SPARK-9681> Feature
      interactions in R formula - Interaction operator "":"" in R formula
   - Python API - Many improvements to Python API to approach feature parity

Misc improvements

   - SPARK-7685  <https://issues.apache.org/jira/browse/SPARK-7685>,
   SPARK-9642  <https://issues.apache.org/jira/browse/SPARK-9642> Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 <https://issues.apache.org/jira/browse/SPARK-10384>,
   SPARK-10385 <https://issues.apache.org/jira/browse/SPARK-10385> Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117 <https://issues.apache.org/jira/browse/SPARK-10117> LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751  <https://issues.apache.org/jira/browse/SPARK-7751> @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337 <https://issues.apache.org/jira/browse/SPARK-11337> Testable
   example code - Automated testing for code in user guide examples

Deprecations

   - In spark.mllib.clustering.KMeans, the ""runs"" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the ""weights"" field has been
   deprecated, in favor of the new name ""coefficients."" This helps
   disambiguate from instance (row) weights given to algorithms.

Changes of behavior

   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
   - Spark SQL's partition discovery has been changed to only discover
   partition directories that are children of the given path. (i.e. if
   path=""/my/data/x=1"" then x=1 will no longer be considered a partition
   but only children of x=1.) This behavior can be overridden by manually
   specifying the basePath that partitioning discovery should start with (
   SPARK-11678 <https://issues.apache.org/jira/browse/SPARK-11678>).
   - When casting a value of an integral type to timestamp (e.g. casting a
   long value to timestamp), the value is treated as being in seconds instead
   of milliseconds (SPARK-11724
   <https://issues.apache.org/jira/browse/SPARK-11724>).
   - With the improved query planner for queries having distinct
   aggregations (SPARK-9241
   <https://issues.apache.org/jira/browse/SPARK-9241>), the plan of a query
   having a single distinct aggregation has been changed to a more robust
   version. To switch back to the plan generated by Spark 1.5's planner,
   please set spark.sql.specializeSingleDistinctAggPlanning to true (
   SPARK-12077 <https://issues.apache.org/jira/browse/SPARK-12077>).
"
Michael Armbrust <michael@databricks.com>,"Tue, 22 Dec 2015 12:29:37 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'll kick the voting off with a +1.


"
Reynold Xin <rxin@databricks.com>,"Tue, 22 Dec 2015 12:43:25 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Michael Armbrust <michael@databricks.com>,1
Andrew Or <andrew@databricks.com>,"Tue, 22 Dec 2015 12:54:44 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Reynold Xin <rxin@databricks.com>,"+1

2015-12-22 12:43 GMT-08:00 Reynold Xin <rxin@databricks.com>:

"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Tue, 22 Dec 2015 23:28:42 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Andrew Or <andrew@databricks.com>,1
Ted Yu <yuzhihong@gmail.com>,"Tue, 22 Dec 2015 15:01:14 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Benjamin Fradet <benjamin.fradet@gmail.com>,"Running test suite, there was timeout in hive-thriftserver module.

This has been fixed by SPARK-11823. So I assume this is test issue.

lgtm


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 22 Dec 2015 15:36:50 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Michael Armbrust <michael@databricks.com>,1
Reynold Xin <rxin@databricks.com>,"Tue, 22 Dec 2015 15:52:23 -0800",Re: A proposal for Spark 2.0,Tom Graves <tgraves_cs@yahoo.com>,"I started a wiki page:
https://cwiki.apache.org/confluence/display/SPARK/Development+Discussions



 feature
res in
e
 a
n
m
is
1.X line.
r
s
ependency
In
ic
c
ld
g
 enormous
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 22 Dec 2015 19:01:31 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Jeff Zhang <zjffdu@gmail.com>,1
Aaron Davidson <ilikerps@gmail.com>,"Tue, 22 Dec 2015 19:05:14 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Josh Rosen <joshrosen@databricks.com>,1
Denny Lee <denny.g.lee@gmail.com>,"Wed, 23 Dec 2015 04:10:10 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),"Aaron Davidson <ilikerps@gmail.com>, Josh Rosen <joshrosen@databricks.com>",1
Yin Huai <yhuai@databricks.com>,"Tue, 22 Dec 2015 20:39:53 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Denny Lee <denny.g.lee@gmail.com>,1
,"Wed, 23 Dec 2015 08:14:28 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),dev@spark.apache.org,"+1 (non binding)

Tested with samples on standalone and yarn.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
kostas papageorgopoylos <p02096@gmail.com>,"Wed, 23 Dec 2015 09:32:23 +0200",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,"Sean Owen <sowen@cloudera.com>, Jerry Lam <chilinglam@gmail.com>","Hi

Fyi
The following 2 tickets are blocking currently (for releases up to 1.5.2)
the pattern of Starting and Stopping a sparkContext inside the same driver
program

https://issues.apache.org/jira/browse/SPARK-11700 ->memory leak in
SqlContext
https://issues.apache.org/jira/browse/SPARK-11739

In an application we have built we initially wanted to use the same pattern
(start-stop-start.etc)
in order to have a better usage of the spark cluster resources.

I believe that the fixes in the above tickets will allow to safely stop and
restart the sparkContext in the driver program in release 1.6.0

Kind Regards



2015-12-22 21:00 GMT+02:00 Sean Owen <sowen@cloudera.com>:

t
to
n
m
s the
e
:
ve
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Wed, 23 Dec 2015 17:33:41 +0900",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),,"+1



---------------------------------------------------------------------


"
=?UTF-8?Q?Zsolt_T=C3=B3th?= <toth.zsolt.bme@gmail.com>,"Wed, 23 Dec 2015 09:49:30 +0100",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non binding)

(Pyspark K-Means still shows the numeric diff, of course.)

2015-12-23 9:33 GMT+01:00 Kousuke Saruta <sarutak@oss.nttdata.co.jp>:

n
s
================
================
=========================
=========================
s
=============="
Roland Reumerman <Roland.Reumerman@mendix.com>,"Wed, 23 Dec 2015 09:58:41 +0000",Re: Expression/LogicalPlan dichotomy in Spark SQL Catalyst,Michael Armbrust <michael@databricks.com>,"
Thanks for the informative reply Michael. The things I'm trying to accomplish with Catalyst are certain external domain model resolving and security-related constraint-handling transformations that depend more on the syntactic (nested TreeNode) structure of the query than on the actual semantics of the nodes in the query tree, but I understand this is kind of an atypical use case compared to the existing Analyzer and Optimizer phases in Catalyst. I also hadn't made very explicit in my post that a difference between the official Catalyst code base and the fork I created is that I don't evaluate expressions in Spark, but execute them embedded as-is in the finally produced SQL query 'physical plan' on a relational database (in this context a concept like expr nullability no longer has semantic meaning). I'll look into RuleExecutor then to see if I can make magic happen.


P.S. I've been informed by someone on the mailing list that some other Spark developers are working on a similar concept, called the push-down of everything:

https://issues.apache.org/jira/browse/SPARK-12449


Roland


________________________________
From: Michael Armbrust <michael@databricks.com>
Sent: Tuesday, December 22, 2015 12:52 AM
To: Roland Reumerman
Cc: dev@spark.apache.org
Subject: Re: Expression/LogicalPlan dichotomy in Spark SQL Catalyst


Why was the choice made in Catalyst to make LogicalPlan/QueryPlan and Expression separate subclasses of TreeNode, instead of e.g. also make QueryPlan inherit from Expression?

I think this is a pretty common way to model things (glancing at postgres it looks similar).  Expression and plans are pretty different concepts.  An expression can be evaluated on a single input row and returns a single value.  In contrast a query plan operates on a relation and has a schema with many different atomic values.


The code also contains duplicate functionality, like LeafNode/LeafExpression, UnaryNode/UnaryExpression and BinaryNode/BinaryExpression.

These traits actually have different semantics for expressions vs. plans (i.e. a UnaryExpression nullability is based on its child's nullability, whereas this would not make sense for a UnaryNode which does not have a concept of nullability).

this makes whole-tree transformations really cumbersome since we've got to deal with 'pivot points' for these 2 types of TreeNodes, where a recursive transformation can only be done on 1 specific type of children, and then has to be dealt with again within the same PartialFunction for the other type in which the matching case(s) can be nested.

It is not clear to me that you actually want these transformations to happen seamlessly.  For example, the resolution rules for subqueries are different than normal plans because you have to reason about correlation.  That said, it seems like you should be able to do some magic in RuleExecutor to make sure that things like the optimizer descend seamlessly into nested query plans.
"
Sean Owen <sowen@cloudera.com>,"Wed, 23 Dec 2015 10:06:29 +0000",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"I think this will be hard to maintain; we already have JIRA as the de
facto central place to store discussions and prioritize work, and the
2.x stuff is already a JIRA. The wiki doesn't really hurt, just
probably will never be looked at again. Let's point people in all
cases to JIRA.

:
h feature
ures in 1.7,
ve
m a
en
om
his
 1.X line.
o
y
f
er
ts
dependency
 In
lic
ic
uld
rent
ng
n enormous

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 23 Dec 2015 12:35:14 +0000",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Michael Armbrust <michael@databricks.com>,"Docker integration tests still fail for Mark and I, and should
probably be disabled:
https://issues.apache.org/jira/browse/SPARK-12426

... but if anyone else successfully runs these (and I assume Jenkins
does) then not a blocker.

I'm having intermittent trouble with other tests passing, but nothing unusual.
Sigs and hashes are OK.

We have 30 issues fixed for 1.6.1. All but those resolved in the last
24 hours or so should be fixed for 1.6.0 right? I can touch that up.






---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Wed, 23 Dec 2015 15:44:08 +0200",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Sean Owen <sowen@cloudera.com>,"+1 (non-binding)

Tested Mesos deployments (client and cluster-mode, fine-grained and
coarse-grained). Things look good
<https://ci.typesafe.com/view/Spark/job/mit-docker-test-ref/8/console>.

iulian




-- 

--
Iulian Dragos

------
Reactive Apps on the "
"""Allen Zhang"" <allenzhang010@126.com>","Wed, 23 Dec 2015 22:14:22 +0800 (CST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"

+1 (non-binding)


I have just tarball a new binary and tested am.nodelabelexpression and executor.nodelabelexpression manully, result is expected.





At 2015-12-23 21:44:08, ""Iulian Drago»ô"" <iulian.dragos@typesafe.com> wrote:

+1 (non-binding)


Tested Mesos deployments (client and cluster-mode, fine-grained and coarse-grained). Things look good.


iulian


On Wed, Dec 23, 2015 at 2:35 PM, Sean Owen <sowen@cloudera.com> wrote:
Docker integration tests still fail for Mark and I, and should
probably be disabled:
https://issues.apache.org/jira/browse/SPARK-12426

... but if anyone else successfully runs these (and I assume Jenkins
does) then not a blocker.

I'm having intermittent trouble with other tests passing, but nothing unusual.
Sigs and hashes are OK.

We have 30 issues fixed for 1.6.1. All but those resolved in the last
24 hours or so should be fixed for 1.6.0 right? I can touch that up.






On Tue, Dec 22, 2015 at 8:10 PM, Michael Armbrust
<michael@databricks.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version
> 1.6.0!
>
> The vote is open until Friday, December 25, 2015 at 18:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.6.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see http://spark.apache.org/
>
> The tag to be voted on is v1.6.0-rc4
> (4062cda3087ae42c6c3cb24508fc1d3a931accdf)
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-bin/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1176/
>
> The test repository (versioned as v1.6.0-rc4) for this release can be found
> at:
> https://repository.apache.org/content/repositories/orgapachespark-1175/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-docs/
>
> =======================================
> == How can I help test this release? ==
> =======================================
> If you are a Spark user, you can help us test this release by taking an
> existing Spark workload and running on this release candidate, then
> reporting any regressions.
>
> ================================================
> == What justifies a -1 vote for this release? ==
> ================================================
> This vote is happening towards the end of the 1.6 QA period, so -1 votes
> should only occur for significant regressions from 1.5. Bugs already present
> in 1.5, minor regressions, or bugs related to new features will not block
> this release.
>
> ===============================================================
> == What should happen to JIRA tickets still targeting 1.6.0? ==
> ===============================================================
> 1. It is OK for documentation patches to target 1.6.0 and still go into
> branch-1.6, since documentations will be published separately from the
> release.
> 2. New features for non-alpha-modules should target 1.7+.
> 3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
> version.
>
>
> ==================================================
> == Major changes to help you focus your testing ==
> ==================================================
>
> Notable changes since 1.6 RC3
>
>
>   - SPARK-12404 - Fix serialization error for Datasets with
> Timestamps/Arrays/Decimal
>   - SPARK-12218 - Fix incorrect pushdown of filters to parquet
>   - SPARK-12395 - Fix join columns of outer join for DataFrame using
>   - SPARK-12413 - Fix mesos HA
>
>
> Notable changes since 1.6 RC2
>
>
> - SPARK_VERSION has been set correctly
> - SPARK-12199 ML Docs are publishing correctly
> - SPARK-12345 Mesos cluster mode has been fixed
>
> Notable changes since 1.6 RC1
>
> Spark Streaming
>
> SPARK-2629  trackStateByKey has been renamed to mapWithState
>
> Spark SQL
>
> SPARK-12165 SPARK-12189 Fix bugs in eviction of storage memory by execution.
> SPARK-12258 correct passing null into ScalaUDF
>
> Notable Features Since 1.5
>
> Spark SQL
>
> SPARK-11787 Parquet Performance - Improve Parquet scan performance when
> using flat schemas.
> SPARK-10810 Session Management - Isolated devault database (i.e USE mydb)
> even on shared clusters.
> SPARK-9999  Dataset API - A type-safe API (similar to RDDs) that performs
> many operations on serialized binary data and code generation (i.e. Project
> Tungsten).
> SPARK-10000 Unified Memory Management - Shared memory for execution and
> caching instead of exclusive division of the regions.
> SPARK-11197 SQL Queries on Files - Concise syntax for running SQL queries
> over files of any supported format without registering a table.
> SPARK-11745 Reading non-standard JSON files - Added options to read
> non-standard JSON files (e.g. single-quotes, unquoted attributes)
> SPARK-10412 Per-operator Metrics for SQL Execution - Display statistics on a
> peroperator basis for memory usage and spilled data size.
> SPARK-11329 Star (*) expansion for StructTypes - Makes it easier to nest and
> unest arbitrary numbers of columns
> SPARK-10917, SPARK-11149 In-memory Columnar Cache Performance - Significant
> (up to 14x) speed up when caching data that contains complex types in
> DataFrames or SQL.
> SPARK-11111 Fast null-safe joins - Joins using null-safe equality (<=>) will
> now execute using SortMergeJoin instead of computing a cartisian product.
> SPARK-11389 SQL Execution Using Off-Heap Memory - Support for configuring
> query execution to occur using off-heap memory to avoid GC overhead
> SPARK-10978 Datasource API Avoid Double Filter - When implemeting a
> datasource with filter pushdown, developers can now tell Spark SQL to avoid
> double evaluating a pushed-down filter.
> SPARK-4849  Advanced Layout of Cached Data - storing partitioning and
> ordering schemes in In-memory table scan, and adding distributeBy and
> localSort to DF API
> SPARK-9858  Adaptive query execution - Intial support for automatically
> selecting the number of reducers for joins and aggregations.
> SPARK-9241  Improved query planner for queries having distinct aggregations
> - Query plans of distinct aggregations are more robust when distinct columns
> have high cardinality.
>
> Spark Streaming
>
> API Updates
>
> SPARK-2629  New improved state management - mapWithState - a DStream
> transformation for stateful stream processing, supercedes updateStateByKey
> in functionality and performance.
> SPARK-11198 Kinesis record deaggregation - Kinesis streams have been
> upgraded to use KCL 1.4.0 and supports transparent deaggregation of
> KPL-aggregated records.
> SPARK-10891 Kinesis message handler function - Allows arbitraray function to
> be applied to a Kinesis record in the Kinesis receiver before to customize
> what data is to be stored in memory.
> SPARK-6328  Python Streamng Listener API - Get streaming statistics
> (scheduling delays, batch processing times, etc.) in streaming.
>
> UI Improvements
>
> Made failures visible in the streaming tab, in the timelines, batch list,
> and batch details page.
> Made output operations visible in the streaming tab as progress bars.
>
> MLlib
>
> New algorithms/models
>
> SPARK-8518  Survival analysis - Log-linear model for survival analysis
> SPARK-9834  Normal equation for least squares - Normal equation solver,
> providing R-like model summary statistics
> SPARK-3147  Online hypothesis testing - A/B testing in the Spark Streaming
> framework
> SPARK-9930  New feature transformers - ChiSqSelector, QuantileDiscretizer,
> SQL transformer
> SPARK-6517  Bisecting K-Means clustering - Fast top-down clustering variant
> of K-Means
>
> API improvements
>
> ML Pipelines
>
> SPARK-6725  Pipeline persistence - Save/load for ML Pipelines, with partial
> coverage of spark.mlalgorithms
> SPARK-5565  LDA in ML Pipelines - API for Latent Dirichlet Allocation in ML
> Pipelines
>
> R API
>
> SPARK-9836  R-like statistics for GLMs - (Partial) R-like stats for ordinary
> least squares via summary(model)
> SPARK-9681  Feature interactions in R formula - Interaction operator "":"" in
> R formula
>
> Python API - Many improvements to Python API to approach feature parity
>
> Misc improvements
>
> SPARK-7685 , SPARK-9642  Instance weights for GLMs - Logistic and Linear
> Regression can take instance weights
> SPARK-10384, SPARK-10385 Univariate and bivariate statistics in DataFrames -
> Variance, stddev, correlations, etc.
> SPARK-10117 LIBSVM data source - LIBSVM as a SQL data source
>
> Documentation improvements
>
> SPARK-7751  @since versions - Documentation includes initial version when
> classes and methods were added
> SPARK-11337 Testable example code - Automated testing for code in user guide
> examples
>
> Deprecations
>
> In spark.mllib.clustering.KMeans, the ""runs"" parameter has been deprecated.
> In spark.ml.classification.LogisticRegressionModel and
> spark.ml.regression.LinearRegressionModel, the ""weights"" field has been
> deprecated, in favor of the new name ""coefficients."" This helps disambiguate
> from instance (row) weights given to algorithms.
>
> Changes of behavior
>
> spark.mllib.tree.GradientBoostedTrees validationTol has changed semantics in
> 1.6. Previously, it was a threshold for absolute change in error. Now, it
> resembles the behavior of GradientDescent convergenceTol: For large errors,
> it uses relative error (relative to the previous error); for small errors (<
> 0.01), it uses absolute error.
> spark.ml.feature.RegexTokenizer: Previously, it did not convert strings to
> lowercase before tokenizing. Now, it converts to lowercase by default, with
> an option not to. This matches the behavior of the simpler Tokenizer
> transformer.
> Spark SQL's partition discovery has been changed to only discover partition
> directories that are children of the given path. (i.e. if
> path=""/my/data/x=1"" then x=1 will no longer be considered a partition but
> only children of x=1.) This behavior can be overridden by manually
> specifying the basePath that partitioning discovery should start with
> (SPARK-11678).
> When casting a value of an integral type to timestamp (e.g. casting a long
> value to timestamp), the value is treated as being in seconds instead of
> milliseconds (SPARK-11724).
> With the improved query planner for queries having distinct aggregations
> (SPARK-9241), the plan of a query having a single distinct aggregation has
> been changed to a more robust version. To switch back to the plan generated
> by Spark 1.5's planner, please set
> spark.sql.specializeSingleDistinctAggPlanning to true (SPARK-12077).


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org







--


--
Iulian Dragos



------
Reactive Apps on the JVM
www.typesafe.com

"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Thu, 24 Dec 2015 00:09:38 +0800",value of sc.defaultParallelism,dev <dev@spark.apache.org>,"python version: 2.7.9
os: ubuntu 14.04
spark: 1.5.2

I run a standalone spark on localhost, and use the following code to access
sc.defaultParallism

# a.py
import pyspark
sc = pyspark.SparkContext()
print(sc.defaultParallelism)

and use the following command to submit

$  spark-submit --master spark://yahsuan-vm:7077 a.py

it prints 2, however, my spark web page shows I got 4 cores


‚Äã
according to http://spark.apache.org/docs/latest/configuration.html

spark.default.parallelismFor distributed shuffle operations likereduceByKey
 and join, the largest number of partitions in a parent RDD. For operations
likeparallelize with no parent RDDs, it depends on the cluster manager:

   - Local mode: number of cores on the local machine
   - Mesos fine grained mode: 8
   - Others: total number of cores on all executor nodes or 2, whichever is
   larger

Default number of partitions in RDDs returned by transformations like join,
reduceByKey, andparallelize when not set by user.
It seems I should get 4 rather than 2.
Am I misunderstood the document?

-- 
-- ÂºµÈõÖËªí
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 23 Dec 2015 16:52:28 +0000",Re: A proposal for Spark 2.0,"Sean Owen <sowen@cloudera.com>, Reynold Xin <rxin@databricks.com>","Yeah, I'd also favor maintaining docs with strictly temporary relevance on
JIRA when possible. The wiki is like this weird backwater I only rarely
visit.

Don't we typically do this kind of stuff with an umbrella issue on JIRA?
Tom, wouldn't that work well for you?

Nick


:
0
ith
ot
atures in
nd
he
).
he 1.X
ng
user
s
s. In
,
 an
"
Jerry Lam <chilinglam@gmail.com>,"Wed, 23 Dec 2015 12:18:15 -0500",Re: [Spark SQL] SQLContext getOrCreate incorrect behaviour,kostas papageorgopoylos <p02096@gmail.com>,"Hi Kostas,

Thank you for the references of the 2 tickets. It helps me to understand
why I got some weird experiences lately.

Best Regards,

Jerry


r
nd
f
y
ut
om
As the
e
"
eugene miretsky <eugene.miretsky@gmail.com>,"Wed, 23 Dec 2015 16:27:03 -0500",Kafka consumer: Upgrading to use the the new Java Consumer,dev@spark.apache.org,"Hi,

The Kafka connector currently uses the older Kafka Scala consumer. Kafka
0.9 came out with a new Java Kafka consumer.

a Decoder( kafka.serializer.decoder) trait to decode keys/values while
the Java consumer uses the  Deserializer interface
(org.apache.kafka.common.serialization.deserializer).

The main difference between Decoder and Deserializer is that
Deserializer.deserialize accepts a topic and a payload while Decoder.decode
accepts only a payload. Topics in Kafka are pretty useful, as one example:
Confluent Schema Registry uses topic names to find the schema for each
key/value - while Confluent does provide a Decoder implementation, it is
mostly a hack that is incompatible  with the new Kafka Java Producer.

Any thoughts about changing the Kafka connector to work with the new Kafka
Java Consumer?

Cheers,
Eugene
"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Thu, 24 Dec 2015 12:19:27 +0800","confused behavior about pyspark.sql, Row, schema, and createDataFrame",dev <dev@spark.apache.org>,"python version: 2.7.9
os: ubuntu 14.04
spark: 1.5.2

```
import pyspark
from pyspark.sql import Row
from pyspark.sql.types import StructType, IntegerType
sc = pyspark.SparkContext()
sqlc = pyspark.SQLContext(sc)
schema1 = StructType() \
    .add('a', IntegerType()) \
    .add('b', IntegerType())

schema2 = StructType() \
    .add('b', IntegerType()) \
    .add('a', IntegerType())
print(schema1 == schema2)

r1 = Row(a=1, b=2)
r2 = Row(b=2, a=1)
print(r1 == r2)

data = [r1, r2]

df1 = sqlc.createDataFrame(data, schema1)
df1.show()

df2 = sqlc.createDataFrame(data, schema2)
df2.show()
```

intuitively, I thought df1 and df2 should contain the same data, however,
the output is

```
False
True
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  1|  2|
+---+---+
‚Äã
+---+---+
|  b|  a|
+---+---+
|  1|  2|
|  1|  2|
+---+---+
```

after trace the source code, I found

1. schema (StructType) use list store fields, so it is order-sensitive
https://github.com/apache/spark/blob/master/python/pyspark/sql/types.py#L459
2. Row will sort according to field name when new
https://github.com/apache/spark/blob/master/python/pyspark/sql/types.py#L1204
3. It seems (not 100% sure) that when createDataFrame, it access the filed
of Row by order not by filed name
https://github.com/apache/spark/blob/master/python/pyspark/sql/context.py#L422

This behavior is a little bit tricky. Maybe this behavior could be mention
at document?

Thanks.

-- 
-- ÂºµÈõÖËªí
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 24 Dec 2015 05:59:35 +0000",Re: Downloading Hadoop from s3://spark-related-packages/,"Luciano Resende <luckbr1975@gmail.com>, shivaram@eecs.berkeley.edu","FYI: I opened an INFRA ticket with questions about how best to use the
Apache mirror network.

https://issues.apache.org/jira/browse/INFRA-10999

Nick

:

.
tar.gz
loser.lua
.1/hadoop-2.7.1.tar.gz&action=download
.7.1.tar.gz?asjson=1
ng
 in the
e
ous EC2
set
y
o
t
at
it
e
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 24 Dec 2015 10:43:02 +0000",Re: Downloading Hadoop from s3://spark-related-packages/,Spark dev list <dev@spark.apache.org>,"

FYI: I opened an INFRA ticket with questions about how best to use the Apache mirror network.

https://issues.apache.org/jira/browse/INFRA-10999

Nick


not that likely to get an answer as it's really a support call, not a bug/task. You never know though.

There's another way to get at binaries, which is check them out direct from SVN

https://dist.apache.org/repos/dist/release/

This is a direct view into how you release things in the ASF (you just create a new dir under your project, copy the files and then do an svn commit; I believe the replicated servers may just do svn update on their local cache.

there's no mirroring, if you install to lots of machines your download time will be slow. You could automate it though, do something like D/L, upload to your own bucket, do an s3 GET.
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 24 Dec 2015 14:19:13 +0100","[DAGScheduler] resubmitFailedStages, failedStages.clear() and submitStage",dev <dev@spark.apache.org>,"Hi,

While reviewing DAGScheduler, and where failedStages internal
collection of failed staged ready for resubmission is used, I came
across a question for which I'm looking an answer to. Any hints would
be greatly appreciated.

When resubmitFailedStages [1] is executed, and there are any failed
stages, they are resubmitted using submitStage [2], but before it
happens, failedStages is cleared [3] so when submitStage is called
that will ultimately call submitMissingTasks for the stage, it checks
whether the stage is in failedStages (among the other sets for waiting
and running stages) [4].

My naive understanding is that the call to submitStage is a no-op in
this case, i.e. nothing really happens and the if expression will
silently finish without doing anything useful until some other event
happens that changes the status of the failed stages into waiting
ones.

Is my understanding incorrect? Where? Could the call to submitStage be
superfluous? Please guide in the right direction. Thanks.

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L734
[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L743
[3] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L741
[4] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L919

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 24 Dec 2015 07:27:15 -0800","Re: [DAGScheduler] resubmitFailedStages, failedStages.clear() and submitStage",Jacek Laskowski <jacek@japila.pl>,"getMissingParentStages(stage) would be called for the stage (being
re-submitted)

If there is no missing parents, submitMissingTasks() would be called.
If there is missing parent(s), the parent would go through the same flow.

I don't see issue in this part of the code.

Cheers


"
Vinay Shukla <vinayshukla@gmail.com>,"Thu, 24 Dec 2015 08:30:59 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Allen Zhang <allenzhang010@126.com>,"+1
Tested on HDP 2.3, YARN cluster mode, spark-shell


on
:
/
/
/
================
================
n
=========================
=========================
es
========================================
=
========================================
o
============"
gsvic <victorasgs@gmail.com>,"Thu, 24 Dec 2015 09:53:49 -0700 (MST)",Shuffle Write Size,dev@spark.apache.org,"Is there any formula with which I could determine Shuffle Write before
execution?

For example, in Sort Merge join in the stage in which the first table is
being loaded the shuffle write is 429.2 MB. The table is 5.5G in the HDFS
with block size 128 MB. Consequently is being loaded in 45 tasks/partitions.
How this 5.5 GB results in 429 MB? Could I determine it before execution? 

Environment:
#Workers = 2
#Cores/Worker = 4
#Assigned Memory / Worker = 512M

spark.shuffle.partitions=200
spark.shuffle.compress=false
spark.shuffle.memoryFraction=0.1
spark.shuffle.spill=true



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 24 Dec 2015 17:18:50 +0000",Re: Downloading Hadoop from s3://spark-related-packages/,"Steve Loughran <stevel@hortonworks.com>, Spark dev list <dev@spark.apache.org>","not that likely to get an answer as it‚Äôs really a support call, not a
bug/task.

The first question is about proper documentation of all the stuff we‚Äôve
been discussing in this thread, so one would think that‚Äôs a valid task. It
doesn‚Äôt seem right that closer.lua, for example, is undocumented. Either
it‚Äôs not meant for public use (and I am not an intended user), or there
should be something out there that explains how to use it.

I‚Äôm not looking for much; just some basic info that covers the various
things I‚Äôve had to piece together from mailing lists and Google.

there‚Äôs no mirroring, if you install to lots of machines your download time
will be slow. You could automate it though, do something like D/L, upload
to your own bucket, do an s3 GET.

Yeah, this is what I‚Äôm probably going to do eventually‚Äîjust use my own S3
bucket.

It‚Äôs disappointing that, at least as far as I can tell, the Apache
foundation doesn‚Äôt have a fast CDN or something like that to serve its
files. So users like me are left needing to come up with their own solution
if they regularly download Apache software to many machines in an automated
fashion.

Now, perhaps Apache mirrors are not meant to be used in this way. Perhaps
they‚Äôre just meant for people to do the one-off download to their personal
machines and that‚Äôs it. That‚Äôs totally fine! But that goes back to my first
question from the ticket‚Äîthere should be a simple doc that spells this out
for us if that‚Äôs the case: ‚ÄúDon‚Äôt use the mirror network for automated
provisioning/deployments.‚Äù That would suffice. But as things stand now, I
have to guess and wonder at this stuff.

Nick
‚Äã


"
Xingchi Wang <regrecall@gmail.com>,"Fri, 25 Dec 2015 13:29:52 +0800",Re: Shuffle Write Size,gsvic <victorasgs@gmail.com>,"I think shuffle write size not dependency on the your data, but on the join
operation, maybe your join action, don't need to shuffle more data, because
the table data has already on its partition, so it not need shuffle write,
is it possible?

2015-12-25 0:53 GMT+08:00 gsvic <victorasgs@gmail.com>:

"
salexln <salexln@gmail.com>,"Thu, 24 Dec 2015 23:51:41 -0700 (MST)",latest Spark build error,dev@spark.apache.org," Hi all,

I'm getting build error when trying to build a clean version of latest
Spark. I did the following

1) git clone https://github.com/apache/spark.git
2) build/mvn -DskipTests clean package

But I get the following error:

Spark Project Parent POM .......................... FAILURE [2.338s]
...
BUILD FAILURE
...
[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-enforcer-plugin:1.4:enforce
(enforce-versions) on project spark-parent_2.10: Some Enforcer rules have
failed. Look above for specific messages explaining why the rule failed. ->
[Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please
read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException



I'm running Lubuntu 14.04 with the following:

java version ""1.7.0_91""
OpenJDK Runtime Environment (IcedTea 2.6.3) (7u91-2.6.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.91-b01, mixed mode)
Apache Maven 3.0.5 



--

---------------------------------------------------------------------


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Fri, 25 Dec 2015 16:18:08 +0900",Re: latest Spark build error,"salexln <salexln@gmail.com>, dev@spark.apache.org","This is because to build Spark requires maven 3.3.3 or later.
http://spark.apache.org/docs/latest/building-spark.html

Regards,
Kazuaki Ishizaki



From:   salexln <salexln@gmail.com>
To:     dev@spark.apache.org
Date:   2015/12/25 15:52
Subject:        latest Spark build error



 Hi all,

I'm getting build error when trying to build a clean version of latest
Spark. I did the following

1) git clone https://github.com/apache/spark.git
2) build/mvn -DskipTests clean package

But I get the following error:

Spark Project Parent POM .......................... FAILURE [2.338s]
...
BUILD FAILURE
...
[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-enforcer-plugin:1.4:enforce
(enforce-versions) on project spark-parent_2.10: Some Enforcer rules have
failed. Look above for specific messages explaining why the rule failed. 
->
[Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the 
-e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, 
please
read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException



I'm running Lubuntu 14.04 with the following:

java version ""1.7.0_91""
OpenJDK Runtime Environment (IcedTea 2.6.3) (7u91-2.6.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.91-b01, mixed mode)
Apache Maven 3.0.5 



--
View this message in context: 
http://apache-spark-developers-list.1001551.n3.nabble.com/latest-Spark-build-error-tp15782.html

Nabble.com.

---------------------------------------------------------------------




"
=?gb2312?B?em1s1cXD98Da?= <mingleizhang@Ctrip.com>,"Fri, 25 Dec 2015 07:35:11 +0000","How can I get the column data based on specific column name and
 then stored these data in array or list ?","""dev@spark.apache.org"" <dev@spark.apache.org>","
Hi,

       I am a new to Scala and Spark and trying to find relative API in DataFrame to solve my problem as title described. However, I just only find this API DataFrame.col(colName : String) : Column which returns an object of Column. Not the content. If only DataFrame support such API which like Column.toArray : Type is enough for me. But now, it doesn°Øt. How can I do can achieve this function ?

Thanks,
Minglei.
"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 25 Dec 2015 15:39:03 +0800","Re: How can I get the column data based on specific column name and
 then stored these data in array or list ?",=?UTF-8?B?em1s5byg5piO56OK?= <mingleizhang@ctrip.com>,"Not sure what you mean. Do you want to choose some columns of a Row and
convert it to an Arrray ?


ataFrame
I *DataFrame.col(colName
 function



-- 
Best Regards

Jeff Zhang
"
salexln <salexln@gmail.com>,"Fri, 25 Dec 2015 00:40:32 -0700 (MST)",Re: latest Spark build error,dev@spark.apache.org,"Updating Maven version to 3.3.9 solved the issue

Thanks everyone!




--

---------------------------------------------------------------------


"
=?utf-8?B?em1s5byg5piO56OK?= <mingleizhang@Ctrip.com>,"Fri, 25 Dec 2015 07:44:04 +0000","=?utf-8?B?562U5aSNOiBIb3cgY2FuIEkgZ2V0IHRoZSBjb2x1bW4gZGF0YSBiYXNlZCBv?=
 =?utf-8?B?biBzcGVjaWZpYyBjb2x1bW4gbmFtZSBhbmQgdGhlbiBzdG9yZWQgdGhlc2Ug?=
 =?utf-8?Q?data_in_array_or_list_=3F?=",Jeff Zhang <zjffdu@gmail.com>,"Thanks, Jeff. It‚Äôs not choose some columns of a Row. It‚Äôs just choose all data in a column and convert it to an Array. Do you understand my mean ?

In Chinese
ÊàëÊòØÊÉ≥Âü∫‰∫éËøô‰∏™ÂàóÂêçÊääËøô‰∏ÄÂàó‰∏≠ÁöÑÊâÄÊúâÊï∞ÊçÆÈÉΩÈÄâÂá∫Êù•ÔºåÁÑ∂ÂêéÊîæÂà∞Êï∞ÁªÑÈáåÈù¢Âéª„ÄÇ


Âèë‰ª∂‰∫∫: π¥12Êúà25Êó• 15:39
Êî∂‰ª∂‰∫∫: zmlÂº†ÊòéÁ£ä
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: How can I get the column data based on specific column name and then stored these data in array or list ?

Not sure what you mean. Do you want to choose some columns of a Row and convert it to an Arrray ?

On Fri, Dec 25, 2015 at 3:35 PM, zmlÂº†ÊòéÁ£ä <mingleizhang@ctrip.com<mailto:mingleizhang@ctrip.com>> wrote:

Hi,

       I am a new to Scala and Spark and trying to find relative API in DataFrame to solve my problem as title described. However, I just only find this API DataFrame.col(colName : String) : Column which returns an object of Column. Not the content. If only DataFrame support such API which like Column.toArray : Type is enough for me. But now, it doesn‚Äôt. How can I do can achieve this function ?

Thanks,
Minglei.



--
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 25 Dec 2015 16:00:03 +0800","=?UTF-8?Q?Re=3A_=E7=AD=94=E5=A4=8D=3A_How_can_I_get_the_column_data_based_on_s?=
	=?UTF-8?Q?pecific_column_name_and_then_stored_these_data_in_array_or_l?=
	=?UTF-8?Q?ist_=3F?=",=?UTF-8?B?em1s5byg5piO56OK?= <mingleizhang@ctrip.com>,"You can use udf to convert one column for array type. Here's one sample

val conf = new SparkConf().setMaster(""local[4]"").setAppName(""test"")
val sc = new SparkContext(conf)
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._
import sqlContext._
sqlContext.udf.register(""f"", (a:String) => Array(a,a))
val df1 = Seq(
  (1, ""jeff"", 12),
  (2, ""andy"", 34),
  (3, ""pony"", 23),
  (4, ""jeff"", 14)
).toDF(""id"", ""name"", ""age"")

val df2=df1.withColumn(""name"", expr(""f(name)""))
df2.printSchema()
df2.show()



 just choose all
ÂêçÊääËøô‰∏ÄÂàó‰∏≠ÁöÑÊâÄÊúâÊï∞ÊçÆÈÉΩÈÄâÂá∫Êù•ÔºåÁÑ∂ÂêéÊîæÂà∞Êï∞ÁªÑÈáåÈù¢Âéª„ÄÇ
• 15:39
 column name and
ataFrame
I *DataFrame.col(colName
 function



-- 
Best Regards

Jeff Zhang
"
Yanbo Liang <ybliang8@gmail.com>,"Fri, 25 Dec 2015 16:06:36 +0800","=?UTF-8?Q?Re=3A_=E7=AD=94=E5=A4=8D=3A_How_can_I_get_the_column_data_based_on_s?=
	=?UTF-8?Q?pecific_column_name_and_then_stored_these_data_in_array_or_l?=
	=?UTF-8?Q?ist_=3F?=",Jeff Zhang <zjffdu@gmail.com>,"Actually you can call df.collect_list(""a"").

2015-12-25 16:00 GMT+08:00 Jeff Zhang <zjffdu@gmail.com>:

s just choose all
ÂêçÊääËøô‰∏ÄÂàó‰∏≠ÁöÑÊâÄÊúâÊï∞ÊçÆÈÉΩÈÄâÂá∫Êù•ÔºåÁÑ∂ÂêéÊîæÂà∞Êï∞ÁªÑÈáåÈù¢Âéª„ÄÇ
• 15:39
c column name
DataFrame
e *Column.toArray
achieve
"
=?utf-8?B?em1s5byg5piO56OK?= <mingleizhang@Ctrip.com>,"Fri, 25 Dec 2015 08:07:40 +0000","=?utf-8?B?562U5aSNOiDnrZTlpI06IEhvdyBjYW4gSSBnZXQgdGhlIGNvbHVtbiBkYXRh?=
 =?utf-8?B?IGJhc2VkIG9uIHNwZWNpZmljIGNvbHVtbiBuYW1lIGFuZCB0aGVuIHN0b3Jl?=
 =?utf-8?Q?d_these_data_in_array_or_list_=3F?=",Jeff Zhang <zjffdu@gmail.com>,"Yes. It‚Äôs a good method . But UDF ? What is UDF ?  U‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..D‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶F ?  OK, I can learn from it.

Thanks,
Minglei.

Âèë‰ª∂‰∫∫: Jeff Zhang [mailto:zjffdu@gmail.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà25Êó• 16:00
Êî∂‰ª∂‰∫∫: zmlÂº†ÊòéÁ£ä
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: Á≠îÂ§ç: How can I get the column data based on specific column name and then stored these data in array or list ?

You can use udf to convert one column for array type. Here's one sample


val conf = new SparkConf().setMaster(""local[4]"").setAppName(""test"")
val sc = new SparkContext(conf)
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._
import sqlContext._
sqlContext.udf.register(""f"", (a:String) => Array(a,a))
val df1 = Seq(
  (1, ""jeff"", 12),
  (2, ""andy"", 34),
  (3, ""pony"", 23),
  (4, ""jeff"", 14)
).toDF(""id"", ""name"", ""age"")

val df2=df1.withColumn(""name"", expr(""f(name)""))
df2.printSchema()
df2.show()

On Fri, Dec 25, 2015 at 3:44 PM, zmlÂº†ÊòéÁ£ä <mingleizhang@ctrip.com<mailto:mingleizhang@ctrip.com>> wrote:
Thanks, Jeff. It‚Äôs not choose some columns of a Row. It‚Äôs just choose all data in a column and convert it to an Array. Do you understand my mean ?

In Chinese
ÊàëÊòØÊÉ≥Âü∫‰∫éËøô‰∏™ÂàóÂêçÊääËøô‰∏ÄÂàó‰∏≠ÁöÑÊâÄÊúâÊï∞ÊçÆÈÉΩÈÄâÂá∫Êù•ÔºåÁÑ∂ÂêéÊîæÂà∞Êï∞ÁªÑÈáåÈù¢Âéª„ÄÇ


Âèë‰ª∂‰∫∫: Jeff Zhang [mailto:zjffdu@gmail.com<mailto:zjffdu@gmail.com>]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà25Êó• 15:39
Êî∂‰ª∂‰∫∫: zmlÂº†ÊòéÁ£ä
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spark.apache.org>
‰∏ªÈ¢ò: Re: How can I get the column data based on specific column name and then stored these data in array or list ?

Not sure what you mean. Do you want to choose some columns of a Row and convert it to an Arrray ?

On Fri, Dec 25, 2015 at 3:35 PM, zmlÂº†ÊòéÁ£ä <mingleizhang@ctrip.com<mailto:mingleizhang@ctrip.com>> wrote:

Hi,

       I am a new to Scala and Spark and trying to find relative API in DataFrame to solve my problem as title described. However, I just only find this API DataFrame.col(colName : String) : Column which returns an object of Column. Not the content. If only DataFrame support such API which like Column.toArray : Type is enough for me. But now, it doesn‚Äôt. How can I do can achieve this function ?

Thanks,
Minglei.



--
Best Regards

Jeff Zhang



--
Best Regards

Jeff Zhang
"
=?utf-8?B?em1s5byg5piO56OK?= <mingleizhang@Ctrip.com>,"Fri, 25 Dec 2015 08:09:08 +0000","=?utf-8?B?562U5aSNOiDnrZTlpI06IEhvdyBjYW4gSSBnZXQgdGhlIGNvbHVtbiBkYXRh?=
 =?utf-8?B?IGJhc2VkIG9uIHNwZWNpZmljIGNvbHVtbiBuYW1lIGFuZCB0aGVuIHN0b3Jl?=
 =?utf-8?Q?d_these_data_in_array_or_list_=3F?=","Yanbo Liang <ybliang8@gmail.com>, Jeff Zhang <zjffdu@gmail.com>","Âí¶ ÔºüÔºüÔºü I will have a try.
Thanks,
Minglei.

Âèë‰ª∂‰∫∫: Yanbo Liang [mailto:ybliang8@gmail.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà25Êó• 16:07
Êî∂‰ª∂‰∫∫: Jeff Zhang
ÊäÑÈÄÅ: zmlÂº†ÊòéÁ£ä; dev@spark.apache.org
‰∏ªÈ¢ò: Re: Á≠îÂ§ç: How can I get the column data based on specific column name and then stored these data in array or list ?

Actually you can call df.collect_list(""a"").

2015-12-25 16:00 GMT+08:00 Jeff Zhang <zjffdu@gmail.com<mailto:zjffdu@gmail.com>>:
You can use udf to convert one column for array type. Here's one sample


val conf = new SparkConf().setMaster(""local[4]"").setAppName(""test"")
val sc = new SparkContext(conf)
val sqlContext = new SQLContext(sc)
import sqlContext.implicits._
import sqlContext._
sqlContext.udf.register(""f"", (a:String) => Array(a,a))
val df1 = Seq(
  (1, ""jeff"", 12),
  (2, ""andy"", 34),
  (3, ""pony"", 23),
  (4, ""jeff"", 14)
).toDF(""id"", ""name"", ""age"")

val df2=df1.withColumn(""name"", expr(""f(name)""))
df2.printSchema()
df2.show()

On Fri, Dec 25, 2015 at 3:44 PM, zmlÂº†ÊòéÁ£ä <mingleizhang@ctrip.com<mailto:mingleizhang@ctrip.com>> wrote:
Thanks, Jeff. It‚Äôs not choose some columns of a Row. It‚Äôs just choose all data in a column and convert it to an Array. Do you understand my mean ?

In Chinese
ÊàëÊòØÊÉ≥Âü∫‰∫éËøô‰∏™ÂàóÂêçÊääËøô‰∏ÄÂàó‰∏≠ÁöÑÊâÄÊúâÊï∞ÊçÆÈÉΩÈÄâÂá∫Êù•ÔºåÁÑ∂ÂêéÊîæÂà∞Êï∞ÁªÑÈáåÈù¢Âéª„ÄÇ


Âèë‰ª∂‰∫∫: Jeff Zhang [mailto:zjffdu@gmail.com<mailto:zjffdu@gmail.com>]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥12Êúà25Êó• 15:39
Êî∂‰ª∂‰∫∫: zmlÂº†ÊòéÁ£ä
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spark.apache.org>
‰∏ªÈ¢ò: Re: How can I get the column data based on specific column name and then stored these data in array or list ?

Not sure what you mean. Do you want to choose some columns of a Row and convert it to an Arrray ?

On Fri, Dec 25, 2015 at 3:35 PM, zmlÂº†ÊòéÁ£ä <mingleizhang@ctrip.com<mailto:mingleizhang@ctrip.com>> wrote:

Hi,

       I am a new to Scala and Spark and trying to find relative API in DataFrame to solve my problem as title described. However, I just only find this API DataFrame.col(colName : String) : Column which returns an object of Column. Not the content. If only DataFrame support such API which like Column.toArray : Type is enough for me. But now, it doesn‚Äôt. How can I do can achieve this function ?

Thanks,
Minglei.



--
Best Regards

Jeff Zhang



--
Best Regards

Jeff Zhang

"
Tao Wang <wangtaothetonic@163.com>,"Fri, 25 Dec 2015 02:05:03 -0700 (MST)",Re: A proposal for Spark 2.0,dev@spark.apache.org,"How about the Hive dependency? We use ThriftServer, serdes and even the
parser/execute logic in Hive. Where will we direct about this part?



--

---------------------------------------------------------------------


"
salexln <salexln@gmail.com>,"Fri, 25 Dec 2015 02:57:41 -0700 (MST)",Re: latest Spark build error,dev@spark.apache.org,"Is there a way only to build the MLlib using command line?




--

---------------------------------------------------------------------


"
"""Allen Zhang"" <allenzhang010@126.com>","Fri, 25 Dec 2015 22:12:21 +0800 (CST)",Re: latest Spark build error,salexln <salexln@gmail.com>,"
Try -pl option in mvn command, and append -am or amd for more choice.


for instance:
mvn clean install -pl :spark-mllib_2.10 -DskipTests





At 2015-12-25 17:57:41, ""salexln"" <salexln@gmail.com> wrote:
>One more question:
>Is there a way only to build the MLlib using command line?
>
>
>
>
>--
>View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/latest-Spark-build-error-tp15782p15794.html
>Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>
"
vaquar khan <vaquar.khan@gmail.com>,"Fri, 25 Dec 2015 20:31:41 +0530",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Vinay Shukla <vinayshukla@gmail.com>,"+1

t:
/
================
================
an
=========================
=========================
========================================
=
========================================
to
e
===========================
===========================
en
nd
ly
s
r"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Fri, 25 Dec 2015 20:59:48 +0530",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),vaquar khan <vaquar.khan@gmail.com>,"+1


m>
/
/
e
/
/
=================
=================
==========================
==========================
=========================================
=
=========================================
he
============================
============================
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 25 Dec 2015 13:48:26 -0800",recurring test failures against hadoop-2.4 profile,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
You may have noticed the following test failures:
org.apache.spark.sql.hive.execution.HiveUDFSuite.UDFIntegerToString
org.apache.spark.sql.hive.execution.SQLQuerySuite.udf_java_method

Tracing backwards, they started failing since this build:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/4521/

There were several commits in the above build. So it is not immediately
obvious which commit caused the failures.

FYI
"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 25 Dec 2015 14:47:45 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Michael Armbrust <michael@databricks.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 29:25 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib (iPython 4.0)
2.0 Spark version is 1.6.0
2.1. statistics (min,max,mean,Pearson,Spearman)"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Fri, 25 Dec 2015 17:11:19 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),dev@spark.apache.org,"+1 (non binding)
Tested Python API, Spark Core, Spark SQL, Spark MLlib  on a standalone
cluster 



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 25 Dec 2015 22:26:42 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),Michael Armbrust <michael@databricks.com>,"I found that SBT build for Scala 2.11 has been failing (
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-branch-1.6-COMPILE-SBT-SCALA-2.11/3/consoleFull
)

I logged SPARK-12527 and sent a PR.

FYI


"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 26 Dec 2015 22:11:04 +0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),"Yin Huai <yhuai@databricks.com>, Denny Lee <denny.g.lee@gmail.com>",1
Disha Shrivastava <dishu.905@gmail.com>,"Sat, 26 Dec 2015 23:38:33 +0530",Akka with Spark,dev@spark.apache.org,"Hi,

I wanted to know how to use Akka framework with Spark starting from basics.
I saw online that Spark uses Akka framework but I am not really sure if I
can define Actors and use it in Spark.

Also, how to integrate Akka with Spark as in how will I know how many Akka
actors are running on each of my worker machines? Can I control that?

Please help. The only useful resource which I could find online was Akka
with Spark Streaming which was also not very clear.

Thanks,

Disha
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 26 Dec 2015 10:54:21 -0800",Re: Akka with Spark,Disha Shrivastava <dishu.905@gmail.com>,"Do you mind sharing your use case ?

It may be possible to use a different approach than Akka.

Cheers


"
Dean Wampler <deanwampler@gmail.com>,"Sat, 26 Dec 2015 21:01:21 -0600",Re: Akka with Spark,Ted Yu <yuzhihong@gmail.com>,"Note that Akka is being removed from Spark. Even if it weren't, I would
consider keeping Akka processes separate from Spark processes, so you can
monitor, debug, and scale them independently. So consider streaming data
from Akka to Spark Streaming or go the other way, from Spark to Akka
Streams.

dean

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Dasun Hegoda <dasunhegoda@gmail.com>,"Sun, 27 Dec 2015 10:40:10 +0530",ERROR server.TThreadPoolServer: Error occurred during processing of message,dev@spark.apache.org,"I'm running apache-hive-1.2.1-bin and spark-1.5.1-bin-hadoop2.6. spark as
the hive engine. When I try to connect through JasperStudio using thrift
port I get below error. I'm running ubuntu 14.04.

    15/12/26 23:36:20 ERROR server.TThreadPoolServer: Error occurred during
processing of message.
    java.lang.RuntimeException:
org.apache.thrift.transport.TSaslTransportException: No data or no sasl
data in the stream
    at
org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)
    at
org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
    Caused by: org.apache.thrift.transport.TSaslTransportException: No data
or no sasl data in the stream
    at
org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:328)
    at
org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
    at
org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
    ... 4 more
    15/12/26 23:36:20 INFO thrift.ThriftCLIService: Client protocol
version: HIVE_CLI_SERVICE_PROTOCOL_V5
    15/12/26 23:36:20 INFO session.SessionState: Created local directory:
/tmp/c670ff55-01bb-4f6f-a375-d22a13c44eaf_resources
    15/12/26 23:36:20 INFO session.SessionState: Created HDFS directory:
/tmp/hive/anonymous/c670ff55-01bb-4f6f-a375-d22a13c44eaf
    15/12/26 23:36:20 INFO session.SessionState: Created local directory:
/tmp/hduser/c670ff55-01bb-4f6f-a375-d22a13c44eaf
    15/12/26 23:36:20 INFO session.SessionState: Created HDFS directory:
/tmp/hive/anonymous/c670ff55-01bb-4f6f-a375-d22a13c44eaf/_tmp_space.db
    15/12/26 23:36:20 INFO thriftserver.SparkExecuteStatementOperation:
Running query 'use default' with d842cd88-2fda-42b2-b943-468017e95f37
    15/12/26 23:36:20 INFO parse.ParseDriver: Parsing command: use default
    15/12/26 23:36:20 INFO parse.ParseDriver: Parse Completed
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=Driver.run
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=compile
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=parse
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO parse.ParseDriver: Parsing command: use default
    15/12/26 23:36:20 INFO parse.ParseDriver: Parse Completed
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=parse
start=1451190980590 end=1451190980591 duration=1
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO metastore.HiveMetaStore: 2: get_database: default
    15/12/26 23:36:20 INFO HiveMetaStore.audit: ugi=hduser
ip=unknown-ip-addr cmd=get_database: default
    15/12/26 23:36:20 INFO metastore.HiveMetaStore: 2: Opening raw store
with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
    15/12/26 23:36:20 INFO metastore.ObjectStore: ObjectStore, initialize
called
    15/12/26 23:36:20 INFO DataNucleus.Query: Reading in results for query
""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is
closing
    15/12/26 23:36:20 INFO metastore.MetaStoreDirectSql: Using direct SQL,
underlying DB is DERBY
    15/12/26 23:36:20 INFO metastore.ObjectStore: Initialized ObjectStore
    15/12/26 23:36:20 INFO ql.Driver: Semantic Analysis Completed
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze
start=1451190980592 end=1451190980620 duration=28
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO ql.Driver: Returning Hive schema:
Schema(fieldSchemas:null, properties:null)
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=compile
start=1451190980588 end=1451190980621 duration=33
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO ql.Driver: Concurrency mode is disabled, not
creating a lock manager
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=Driver.execute
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO ql.Driver: Starting
command(queryId=hduser_20151226233620_6bc633ef-5c6f-49e4-9300-f79fdf0c357b):
use default
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit
start=1451190980588 end=1451190980622 duration=34
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=runTasks
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=task.DDL.Stage-0
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO ql.Driver: Starting task [Stage-0:DDL] in serial
mode
    15/12/26 23:36:20 INFO metastore.HiveMetaStore: 2: get_database: default
    15/12/26 23:36:20 INFO HiveMetaStore.audit: ugi=hduser
ip=unknown-ip-addr cmd=get_database: default
    15/12/26 23:36:20 INFO metastore.HiveMetaStore: 2: get_database: default
    15/12/26 23:36:20 INFO HiveMetaStore.audit: ugi=hduser
ip=unknown-ip-addr cmd=get_database: default
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=runTasks
start=1451190980622 end=1451190980637 duration=15
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=Driver.execute
start=1451190980621 end=1451190980637 duration=16
from=org.apache.hadoop.hive.ql.Driver>
    OK
    15/12/26 23:36:20 INFO ql.Driver: OK
    15/12/26 23:36:20 INFO log.PerfLogger: <PERFLOG method=releaseLocks
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=releaseLocks
start=1451190980639 end=1451190980639 duration=0
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO log.PerfLogger: </PERFLOG method=Driver.run
start=1451190980587 end=1451190980639 duration=52
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:36:20 INFO thriftserver.SparkExecuteStatementOperation:
Result Schema: List(result#28)
    15/12/26 23:36:20 INFO thriftserver.SparkExecuteStatementOperation:
Running query 'SELECT * FROM service' with
37916038-9856-43eb-8b73-920f9faf738f
    15/12/26 23:36:20 INFO parse.ParseDriver: Parsing command: SELECT *
FROM service
    15/12/26 23:36:20 INFO parse.ParseDriver: Parse Completed
    15/12/26 23:36:20 INFO metastore.HiveMetaStore: 2: get_table :
db=default tbl=service
    15/12/26 23:36:20 INFO HiveMetaStore.audit: ugi=hduser
ip=unknown-ip-addr cmd=get_table : db=default tbl=service
    15/12/26 23:36:20 ERROR thriftserver.SparkExecuteStatementOperation:
Error executing query, currentState RUNNING,
    org.apache.spark.sql.AnalysisException: no such table service; line 1
pos 14
    at
org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:260)
    at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:268)
    at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:264)
    at
org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)
    at
org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)
    at
org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
    at
org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)
    at
org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)
    at
org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)
    at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
    at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
    at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
    at scala.collection.AbstractIterator.to(Iterator.scala:1157)
    at
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
    at
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
    at
org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)
    at
org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)
    at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:264)
    at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:254)
    at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)
    at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)
    at
scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
    at scala.collection.immutable.List.foldLeft(List.scala:84)
    at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)
    at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at
org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)
    at
org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:916)
    at
org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:916)
    at
org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:914)
    at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:132)
    at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
    at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:725)
    at
org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.runInternal(SparkExecuteStatementOperation.scala:224)
    at
org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(SparkExecuteStatementOperation.scala:144)
    at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:388)
    at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:369)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at
org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
    at
org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
    at
org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
    at
org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
    at com.sun.proxy.$Proxy47.executeStatement(Unknown Source)
    at
org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:261)
    at
org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:486)
    at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
    at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
    at
org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
    at
org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
    15/12/26 23:36:20 WARN thrift.ThriftCLIService: Error executing
statement:
    org.apache.hive.service.cli.HiveSQLException:
org.apache.spark.sql.AnalysisException: no such table service; line 1 pos 14
    at
org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.runInternal(SparkExecuteStatementOperation.scala:259)
    at
org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(SparkExecuteStatementOperation.scala:144)
    at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:388)
    at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:369)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at
org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
    at
org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
    at
org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
    at
org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
    at com.sun.proxy.$Proxy47.executeStatement(Unknown Source)
    at
org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:261)
    at
org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:486)
    at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
    at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
    at
org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
    at
org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
    15/12/26 23:41:08 ERROR server.TThreadPoolServer: Error occurred during
processing of message.
    java.lang.RuntimeException:
org.apache.thrift.transport.TSaslTransportException: No data or no sasl
data in the stream
    at
org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)
    at
org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
    Caused by: org.apache.thrift.transport.TSaslTransportException: No data
or no sasl data in the stream
    at
org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:328)
    at
org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
    at
org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
    ... 4 more
    15/12/26 23:41:08 INFO thrift.ThriftCLIService: Client protocol
version: HIVE_CLI_SERVICE_PROTOCOL_V5
    15/12/26 23:41:08 INFO session.SessionState: Created local directory:
/tmp/aa7ce472-0284-4469-823b-748ef786ab73_resources
    15/12/26 23:41:08 INFO session.SessionState: Created HDFS directory:
/tmp/hive/anonymous/aa7ce472-0284-4469-823b-748ef786ab73
    15/12/26 23:41:08 INFO session.SessionState: Created local directory:
/tmp/hduser/aa7ce472-0284-4469-823b-748ef786ab73
    15/12/26 23:41:08 INFO session.SessionState: Created HDFS directory:
/tmp/hive/anonymous/aa7ce472-0284-4469-823b-748ef786ab73/_tmp_space.db
    15/12/26 23:41:08 INFO thriftserver.SparkExecuteStatementOperation:
Running query 'use default' with 6a274f01-2a83-44b9-b970-2154792af7a2
    15/12/26 23:41:08 INFO parse.ParseDriver: Parsing command: use default
    15/12/26 23:41:08 INFO parse.ParseDriver: Parse Completed
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=Driver.run
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=compile
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=parse
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO parse.ParseDriver: Parsing command: use default
    15/12/26 23:41:08 INFO parse.ParseDriver: Parse Completed
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=parse
start=1451191268389 end=1451191268390 duration=1
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO metastore.HiveMetaStore: 2: get_database: default
    15/12/26 23:41:08 INFO HiveMetaStore.audit: ugi=hduser
ip=unknown-ip-addr cmd=get_database: default
    15/12/26 23:41:08 INFO ql.Driver: Semantic Analysis Completed
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze
start=1451191268390 end=1451191268397 duration=7
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO ql.Driver: Returning Hive schema:
Schema(fieldSchemas:null, properties:null)
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=compile
start=1451191268387 end=1451191268398 duration=11
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO ql.Driver: Concurrency mode is disabled, not
creating a lock manager
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=Driver.execute
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO ql.Driver: Starting
command(queryId=hduser_20151226234108_27b4ad3d-0f88-4a81-83f6-eaf0ef49cd22):
use default
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit
start=1451191268387 end=1451191268399 duration=12
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=runTasks
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=task.DDL.Stage-0
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO ql.Driver: Starting task [Stage-0:DDL] in serial
mode
    15/12/26 23:41:08 INFO metastore.HiveMetaStore: 2: get_database: default
    15/12/26 23:41:08 INFO HiveMetaStore.audit: ugi=hduser
ip=unknown-ip-addr cmd=get_database: default
    15/12/26 23:41:08 INFO metastore.HiveMetaStore: 2: get_database: default
    15/12/26 23:41:08 INFO HiveMetaStore.audit: ugi=hduser
ip=unknown-ip-addr cmd=get_database: default
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=runTasks
start=1451191268399 end=1451191268412 duration=13
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=Driver.execute
start=1451191268398 end=1451191268412 duration=14
from=org.apache.hadoop.hive.ql.Driver>
    OK
    15/12/26 23:41:08 INFO ql.Driver: OK
    15/12/26 23:41:08 INFO log.PerfLogger: <PERFLOG method=releaseLocks
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=releaseLocks
start=1451191268413 end=1451191268413 duration=0
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO log.PerfLogger: </PERFLOG method=Driver.run
start=1451191268387 end=1451191268413 duration=26
from=org.apache.hadoop.hive.ql.Driver>
    15/12/26 23:41:08 INFO thriftserver.SparkExecuteStatementOperation:
Result Schema: List(result#43)


Below is the apache-hive-1.2.1-bin/conf/hive-site.xml


        <description>
               The cluster manager to connect to
        </description>
      </property>

       <property>
        <name>spark.serializer</name>
        <value>org.apache.spark.serializer.KryoSerializer</value>
        <description>
               Class to use for serializing objects that will be sent over
the network
        </description>
      </property>



    <property>
      <name>hive.server2.authentication</name>
      <value>NONE</value>
      <description>
        Client authentication types.
           NONE: no authentication check
           LDAP: LDAP/AD based authentication
           KERBEROS: Kerberos/GSSAPI authentication
           CUSTOM: Custom authentication provider
                   (Use with property
hive.server2.custom.authentication.class)
      </description>
    </property>

    <property>
      <name>hive.metastore.sasl.enabled</name>
      <value>false</value>
      <description>If true, the metastore thrift interface will be secured
with SASL. Clients must authenticate with Kerberos.</description>
    </property>

    <!--Hive server -->
    <property>
      <name>hive.server2.thrift.port</name>
      <value>10000</value>
      <description>Port number of HiveServer2 Thrift interface.
      Can be overridden by setting $HIVE_SERVER2_THRIFT_PORT</description>
    </property>

    <property>
      <name>hive.server2.thrift.bind.host</name>
      <value>192.168.7.87</value>
      <description>Bind host on which to run the HiveServer2 Thrift
interface.
      Can be overridden by setting
$HIVE_SERVER2_THRIFT_BIND_HOST</description>
    </property>

How can I fix this?

-- 
Regards,
Dasun Hegoda, Software Engineer
www.dasunhegoda.com | dasunhegoda@gmail.com
"
Soumya Simanta <soumya.simanta@gmail.com>,"Sun, 27 Dec 2015 11:01:42 +0530",Re: Akka with Spark,Dean Wampler <deanwampler@gmail.com>,"

Any rationale for removing Akka from Spark ? Also, what is the replacement ? 

Thanks 

nsider keeping Akka processes separate from Spark processes, so you can monitor, debug, and scale them independently. So consider streaming data from Akka to Spark Streaming or go the other way, from Spark to Akka Streams.
cs. I saw online that Spark uses Akka framework but I am not really sure if I can define Actors and use it in Spark.
ka actors are running on each of my worker machines? Can I control that?
 with Spark Streaming which was also not very clear.
"
Reynold Xin <rxin@databricks.com>,"Sat, 26 Dec 2015 23:34:51 -0800",Re: Akka with Spark,Soumya Simanta <soumya.simanta@gmail.com>,"We are just removing Spark's dependency on Akka. It has nothing to do with
whether user applications can use Akka or not. As a matter of fact, by
removing the Akka dependency from Spark, it becomes easier for user
applications to use Akka, because there is no more dependency conflict.

For more information, see https://issues.apache.org/jira/browse/SPARK-5293


"
salexln <salexln@gmail.com>,"Sun, 27 Dec 2015 01:20:43 -0700 (MST)",what is the best way to debug spark / mllib?,dev@spark.apache.org,"Hi guys,

I'm debugging my code in mllib/clustering but i'm not sure i'm doing it the
best way:
I build my changes in mllib using ""build/mvn -DskipTests package"" and then
running invoking my code using 
""./bin/spark-shell""

My two main issues:
1) After each change the build (build/mvn -DskipTests package) takes ~15
mins
2) I cannot put breakpoints
3) If I add println of logInfo, I do not see it in the console.

What us the best way to debug it?




--

---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Sun, 27 Dec 2015 00:32:32 -0800",Re: what is the best way to debug spark / mllib?,salexln <salexln@gmail.com>,"1) you should run zinc incremental compiler
2) if you want breakpoints that should likely be done in local mode
3) adjust the log4j.properties settings and you can start to see the logInfo

2015-12-27 0:20 GMT-08:00 salexln <salexln@gmail.com>:

"
Disha Shrivastava <dishu.905@gmail.com>,"Sun, 27 Dec 2015 15:36:39 +0530",Re: Akka with Spark,Reynold Xin <rxin@databricks.com>,"Hi All,

I need an Akka like framework to implement model parallelism in neural
networks, an architecture similar to that given in the link
http://alexminnaar.com/implementing-the-distbelief-deep-neural-network-training-framework-with-akka.html.
I need to divide a big neural network ( which can't fit into the memory of
one machine) layer by layer and do message passing across actors which are
distributed across different worker machines. I found Akka to be most
suitable for the job.

Please suggest if it can be done by any other suitable frameworks.

Regards,
Disha


"
Yanbo Liang <ybliang8@gmail.com>,"Sun, 27 Dec 2015 18:23:23 +0800",Re: SparkML algos limitations question.,Joseph Bradley <joseph@databricks.com>,"Hi Eugene,

AFAIK, the current implementation of MultilayerPerceptronClassifier have
some scalability problems if the model is very huge (such as >10M),
although I think the limitation can cover many use cases already.

Yanbo

2015-12-16 6:00 GMT+08:00 Joseph Bradley <joseph@databricks.com>:

"
Dean Wampler <deanwampler@gmail.com>,"Sun, 27 Dec 2015 07:15:06 -0600",Re: Akka with Spark,Disha Shrivastava <dishu.905@gmail.com>,"As Reynold said, you can still use Akka with Spark, but now it's more like
using any third-party library that isn't already a Spark dependency (at
least once the current Akka dependency is fully removed).

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 27 Dec 2015 08:13:17 -0800",Re: Akka with Spark,Dean Wampler <deanwampler@gmail.com>,"Disha:
Please consider these resources:

https://groups.google.com/forum/#!forum/akka-user
https://groups.google.com/forum/#!forum/akka-dev


"
salexln <salexln@gmail.com>,"Sun, 27 Dec 2015 09:45:39 -0700 (MST)",Re: what is the best way to debug spark / mllib?,dev@spark.apache.org,"Thanks for the response, I have several more questions:

*1) you should run zinc incremental compiler*
I run ""./build/zinc-0.3.9/bin/zinc -scala-home $SCALA_HOME -nailed -start""
but the compilation time of 
""build/mvn -DskipTests package' is still about 9 mins. Is this normal?

*2) if you want breakpoints that should likely be done in local mode*
What do you mean by local mode? I've downloaded the latest version from
github, and then made my changes on it.

*3) adjust the log4j.properties settings and you can start to see the
logInfo*
I've copied log4j.properties from log4j.properties.template and is it set:
log4j.rootCategory=INFO, console

But I still do not see the logInfo in the console.





--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 27 Dec 2015 08:52:50 -0800",Re: what is the best way to debug spark / mllib?,salexln <salexln@gmail.com>,"For #1, 9 minutes seem to be normal. Here was duration for recent build on
master branch:

[INFO]
------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 10:44 min
[INFO] Finished at: 2015-12-25T09:53:28-08:00

For #2, please take a look at
https://spark.apache.org/docs/latest/submitting-applications.html
Look for 'Run application locally;'

Cheers


"
"""Fathi Salmi, Meisam"" <meisam.fathi@gmail.com>","Sun, 27 Dec 2015 12:18:29 -0500",Re: what is the best way to debug spark / mllib?,"dev@spark.apache.org, salexln@gmail.com","If you are modifying only mlib, you can use the ""-am"" and ""-pl"" options 
with mvn to cut the build time even more.

Thanks,
Meisam


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Sun, 27 Dec 2015 11:59:09 -0600",Re: Kafka consumer: Upgrading to use the the new Java Consumer,eugene miretsky <eugene.miretsky@gmail.com>,"Have you seen
SPARK-12177


"
Cody Koeninger <cody@koeninger.org>,"Sun, 27 Dec 2015 12:00:49 -0600","Re: Spark Streaming Kafka - DirectKafkaInputDStream: Using the new
 Kafka Consumer API",Mario Ds Briggs <mario.briggs@in.ibm.com>,"Should probably get everyone on the same page at

https://issues.apache.org/jira/browse/SPARK-12177


"
Michael Armbrust <michael@databricks.com>,"Sun, 27 Dec 2015 16:00:44 -0800",Re: [VOTE] Release Apache Spark 1.6.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for testing and voting everyone.  The vote passes unanimously with
21 +1 votes and no -1 votes.  I start finalizing the release now.

+1
Michael Armbrust*
Reynold Xin*
Andrew Or*
Benjamin Fradet
Mark Hamstra*
Jeff Zhang
Josh Rosen*
Aaron Davidson*
Denny Lee
Yin Huai
Kousuke Saruta
Zsolt T√≥th
Iulian Drago»ô
Allen Zhang
Vinay Shukla
Vaquar Khan
Bhupendra Mishra
Krishna Sankar
Ricardo Almeida
Cheng Lian

-1:
none


4-bin/>
in/
1176/>
176/
1175/>
175/
4-docs/>
ocs/
=================
=================
g
hen
==========================
==========================
already
will not
=========================================
==
=========================================
from the
============================
============================
g
 Fix
 Dataset
roject
)
ames or SQL.
isian
ead
QL to avoid
 Advanced
alSort to
 Adaptive
 Improved
umns have
ggregated
 customize
 Survival
 Normal
k
 New
 Bisecting
 Instance
 @since
as been
olute
nt
ve to the
or.
 to
havior of
th. (i.e.
ed a
g in
 a more
5's
g
"
Li Li <fancyerii@gmail.com>,"Mon, 28 Dec 2015 11:26:43 +0800",running lda in spark throws exception,dev@spark.apache.org,"I ran my lda example in a yarn 2.6.2 cluster with spark 1.5.2.
it throws exception in line:   Matrix topics = ldaModel.topicsMatrix();
But in yarn job history ui, it's successful. What's wrong with it?
I submit job with
.bin/spark-submit --class Myclass \
    --master yarn-client \
    --num-executors 2 \
    --driver-memory 4g \
    --executor-memory 4g \
    --executor-cores 1 \


My codes:

   corpus.cache();


    // Cluster the documents into three topics using LDA

    DistributedLDAModel ldaModel = (DistributedLDAModel) new
LDA().setOptimizer(""em"").setMaxIterations(iterNumber).setK(topicNumber).run(corpus);


    // Output topics. Each is a distribution over words (matching word
count vectors)

    System.out.println(""Learned topics (as distributions over vocab of
"" + ldaModel.vocabSize()

        + "" words):"");

   //Line81, exception here:    Matrix topics = ldaModel.topicsMatrix();

    for (int topic = 0; topic < topicNumber; topic++) {

      System.out.print(""Topic "" + topic + "":"");

      for (int word = 0; word < ldaModel.vocabSize(); word++) {

        System.out.print("" "" + topics.apply(word, topic));

      }

      System.out.println();

    }


    ldaModel.save(sc.sc(), modelPath);


Exception in thread ""main"" java.lang.IndexOutOfBoundsException:
(1025,0) not in [-58,58) x [-100,100)

        at breeze.linalg.DenseMatrix$mcD$sp.update$mcD$sp(DenseMatrix.scala:112)

        at org.apache.spark.mllib.clustering.DistributedLDAModel$$anonfun$topicsMatrix$1.apply(LDAModel.scala:534)

        at org.apache.spark.mllib.clustering.DistributedLDAModel$$anonfun$topicsMatrix$1.apply(LDAModel.scala:531)

        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)

        at org.apache.spark.mllib.clustering.DistributedLDAModel.topicsMatrix$lzycompute(LDAModel.scala:531)

        at org.apache.spark.mllib.clustering.DistributedLDAModel.topicsMatrix(LDAModel.scala:523)

        at com.mobvoi.knowledgegraph.textmining.lda.ReviewLDA.main(ReviewLDA.java:81)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:606)

        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)

        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)

        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)

        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)

        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

15/12/23 00:01:16 INFO spark.SparkContext: Invoking stop() from shutdown hook

---------------------------------------------------------------------


"
sara mustafa <eng.sara.mustafa@gmail.com>,"Mon, 28 Dec 2015 13:04:16 -0700 (MST)",Catalyst Class Cast Exception,dev@spark.apache.org,"Hi,

I am trying to run a sql query from TPC-H query samples using DataFrame, but
it throw this exception

java.lang.ClassCastException:
org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast
to org.apache.spark.sql.catalyst.expressions.UnsafeRow
	at
org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.writeValue(UnsafeRowSerializer.scala:61)
	at
org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:121)
	at
org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:74)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:42)
	at org.apache.spark.scheduler.Task.run(Task.scala:90)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Can anyone help me, please?



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 28 Dec 2015 12:15:50 -0800",Re: Catalyst Class Cast Exception,sara mustafa <eng.sara.mustafa@gmail.com>,"Which version of spark?


"
sara mustafa <eng.sara.mustafa@gmail.com>,"Mon, 28 Dec 2015 13:17:31 -0700 (MST)",Re: Catalyst Class Cast Exception,dev@spark.apache.org,"spark 1.5.2



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 28 Dec 2015 12:21:35 -0800",Re: Catalyst Class Cast Exception,sara mustafa <eng.sara.mustafa@gmail.com>,"Looks like a bug.  Have you tried Spark 1.6 RC4?

http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-bin/


"
sara mustafa <eng.sara.mustafa@gmail.com>,"Mon, 28 Dec 2015 13:29:23 -0700 (MST)",Re: Catalyst Class Cast Exception,dev@spark.apache.org,"Not yet, but I will try it. 
Thank you. 



--

---------------------------------------------------------------------


"
salexln <salexln@gmail.com>,"Mon, 28 Dec 2015 13:36:33 -0700 (MST)",RDD[Vector] Immutability issue,dev@spark.apache.org,"Hi guys,
I know the RDDs are immutable and therefore their value cannot be changed
but I see the following behaviour:
I wrote an implementation for FuzzyCMeans algorithm and now I'm testing it,
so i run the following example:

import org.apache.spark.mllib.clustering.FuzzyCMeans
import org.apache.spark.mllib.linalg.Vectors

val data =
sc.textFile(""/home/development/myPrjects/R/butterfly/butterfly.txt"")
val parsedData = data.map(s => Vectors.dense(s.split('
').map(_.toDouble))).cache()

val numClusters = 2
val numIterations = 20

parsedData.foreach{ point => println(point) } 
[-3.0,-2.0]
[-3.0,0.0]
[-3.0,2.0]
[-2.0,-1.0]
[-2.0,0.0]
[-2.0,1.0]
[-1.0,0.0]
[0.0,0.0]
[1.0,0.0]
[2.0,-1.0]
[2.0,0.0]
[2.0,1.0]
[3.0,-2.0]
[3.0,0.0]
[3.0,2.0]
[0.0,8.0]

val clusters = FuzzyCMeans.train(parsedData, numClusters, numIteration
parsedData.foreach{ point => println(point) } 
[0.0,-0.4803333185624595]
[-0.1811743096972924,-0.12078287313152826]
[-0.06638890786148487,0.0]
[-0.04005925925925929,0.02670617283950619]
[-0.12193263222069807,-0.060966316110349035]
[-0.0512,0.0]
[NaN,NaN]
[-0.049382716049382706,0.0]
[NaN,NaN]
[0.006830134553650707,0.0]
[0.05120000000000002,-0.02560000000000001]
[0.04755220304297078,0.0]
[0.06581619798335057,0.03290809899167529]
[0.12010867103812725,-0.0800724473587515]
[0.10946638900458144,0.0]
[0.14814814814814817,0.09876543209876545]
[0.0,0.49119985188436205]



But how can this be that my method changes the Immutable RDD?

BTW, the signature of the train method, is the following:

train( data: RDD[Vector], clusters: Int, maxIterations: Int)




--

---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Mon, 28 Dec 2015 17:51:33 -0700 (MST)",Re: RDD[Vector] Immutability issue,dev@spark.apache.org,"Hi salexln,

Did you reproduce the same issue under any different condition?
I can't reproduce this issue, since I don't know the details of the
algorithm.
Please let me know more detailed condition or the repository?

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
salexln <salexln@gmail.com>,"Tue, 29 Dec 2015 00:42:47 -0700 (MST)",Re: RDD[Vector] Immutability issue,dev@spark.apache.org,"Hi Yu,
thanks for the reply.

I only run it in my machine (in order to debug the algorithm) . What other
conditions did you have it mind?
My whole code is here:  https://github.com/salexln/FinalProject_FCM

Best,
Alex



--

---------------------------------------------------------------------


"
Jan Uyttenhove <jan@insidin.com>,"Tue, 29 Dec 2015 12:42:57 +0100",Spark streaming 1.6.0-RC4 NullPointerException using mapWithState,dev@spark.apache.org,"Hi guys,

I upgraded to the RC4 of Spark (streaming) 1.6.0 to (re)test the new
mapWithState API, after previously reporting issue SPARK-11932 (
https://issues.apache.org/jira/browse/SPARK-11932).

My Spark streaming job involves reading data from a Kafka topic (using
KafkaUtils.createDirectStream), stateful processing (using checkpointing &
mapWithState) & publishing the results back to Kafka.

I'm now facing the NullPointerException below when restoring from a
checkpoint in the following scenario:
1/ run job (with local[2]), process data from Kafka while creating &
keeping state
2/ stop the job
3/ generate some extra message on the input Kafka topic
4/ start the job again (and restore offsets & state from the checkpoints)

The problem is caused by (or at least related to) step 3, i.e. publishing
data to the input topic while the job is stopped.
The above scenario has been tested successfully when:
- step 3 is excluded, so restoring state from a checkpoint is successful
when no messages are added when the job is stopped
- after step 2, the checkpoints are deleted

Any clues? Am I doing something wrong here, or is there still a problem
with the mapWithState impl?

Thanx,

Jan



15/12/29 11:56:12 ERROR executor.Executor: Exception in task 0.0 in stage
3.0 (TID 24)
java.lang.NullPointerException
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at
org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
15/12/29 11:56:12 INFO storage.BlockManagerInfo: Added rdd_25_1 in memory
on localhost:10003 (size: 1024.0 B, free: 511.1 MB)
15/12/29 11:56:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0
non-empty blocks out of 8 blocks
15/12/29 11:56:12 INFO storage.ShuffleBlockFetcherIterator: Started 0
remote fetches in 0 ms
15/12/29 11:56:12 INFO storage.MemoryStore: Block rdd_29_1 stored as values
in memory (estimated size 1824.0 B, free 488.0 KB)
15/12/29 11:56:12 INFO storage.BlockManagerInfo: Added rdd_29_1 in memory
on localhost:10003 (size: 1824.0 B, free: 511.1 MB)
15/12/29 11:56:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0
non-empty blocks out of 8 blocks
15/12/29 11:56:12 INFO storage.ShuffleBlockFetcherIterator: Started 0
remote fetches in 0 ms
15/12/29 11:56:12 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 3.0
(TID 24, localhost): java.lang.NullPointerException
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at
org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

15/12/29 11:56:12 INFO storage.MemoryStore: Block rdd_33_1 stored as values
in memory (estimated size 2.6 KB, free 490.6 KB)
15/12/29 11:56:12 INFO storage.BlockManagerInfo: Added rdd_33_1 in memory
on localhost:10003 (size: 2.6 KB, free: 511.1 MB)
15/12/29 11:56:12 ERROR scheduler.TaskSetManager: Task 0 in stage 3.0
failed 1 times; aborting job
15/12/29 11:56:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 3
15/12/29 11:56:12 INFO executor.Executor: Executor is trying to kill task
1.0 in stage 3.0 (TID 25)
15/12/29 11:56:12 INFO scheduler.TaskSchedulerImpl: Stage 3 was cancelled
15/12/29 11:56:12 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (map at
Visitize.scala:91) failed in 0.126 s
15/12/29 11:56:12 INFO scheduler.DAGScheduler: Job 0 failed:
foreachPartition at Visitize.scala:96, took 2.222262 s
15/12/29 11:56:12 INFO scheduler.JobScheduler: Finished job streaming job
1451386550000 ms.0 from job set of time 1451386550000 ms
15/12/29 11:56:12 INFO scheduler.JobScheduler: Total delay: 22.738 s for
time 1451386550000 ms (execution: 2.308 s)
15/12/29 11:56:12 INFO spark.SparkContext: Starting job: foreachPartition
at Visitize.scala:96
15/12/29 11:56:12 INFO scheduler.JobScheduler: Starting job streaming job
1451386560000 ms.0 from job set of time 1451386560000 ms
15/12/29 11:56:12 ERROR scheduler.JobScheduler: Error running job streaming
job 1451386550000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0
in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage
3.0 (TID 24, localhost): java.lang.NullPointerException
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at
org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
at
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:920)
at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:918)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:918)
at tts.job.Visitize$$anonfun$createContext$8.apply(Visitize.scala:96)
at tts.job.Visitize$$anonfun$createContext$8.apply(Visitize.scala:94)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
at scala.util.Try$.apply(Try.scala:161)
at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at
org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
... 3 more
15/12/29 11:56:12 INFO scheduler.JobGenerator: Checkpointing graph for time
1451386550000 ms
15/12/29 11:56:12 INFO spark.MapOutputTrackerMaster: Size of output
statuses for shuffle 3 is 158 bytes
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure:
Lost task 0.0 in stage 3.0 (TID 24, localhost):
java.lang.NullPointerException
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at
org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
at
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:920)
at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:918)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:918)
at tts.job.Visitize$$anonfun$createContext$8.apply(Visitize.scala:96)
at tts.job.Visitize$$anonfun$createContext$8.apply(Visitize.scala:94)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
at scala.util.Try$.apply(Try.scala:161)
at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
at
org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at
org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
at
org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
... 3 more
15/12/29 11:56:12 INFO streaming.DStreamGraph: Updating checkpoint data for
time 1451386550000 ms
15/12/29 11:56:12 INFO executor.Executor: Executor killed task 1.0 in stage
3.0 (TID 25)
15/12/29 11:56:12 INFO spark.MapOutputTrackerMaster: Size of output
statuses for shuffle 2 is 153 bytes
15/12/29 11:56:12 INFO spark.MapOutputTrackerMaster: Size of output
statuses for shuffle 1 is 153 bytes



-- 
Jan Uyttenhove
Streaming data & digital solutions architect @ Insidin bvba

jan@insidin.com

https://twitter.com/xorto
https://www.linkedin.com/in/januyttenhove

This e-mail and any files transmitted with it are intended solely for the
use of the individual or entity to whom they are addressed. It may contain
privileged and confidential information. If you are not the intended
recipient please notify the sender immediately and destroy this e-mail. Any
form of reproduction, dissemination, copying, disclosure, modification,
distribution and/or publication of this e-mail message is strictly
prohibited. Whilst all efforts are made to safeguard e-mails, the sender
cannot guarantee that attachments are virus free or compatible with your
systems and does not accept liability in respect of viruses or computer
problems experienced.
"
Disha Shrivastava <dishu.905@gmail.com>,"Tue, 29 Dec 2015 17:27:58 +0530",Partitioning of RDD across worker machines,dev@spark.apache.org,"Hi,

Suppose I have a file locally on my master machine and the same file is
also present in the same path on all the worker machines , say
/home/user_name/Desktop. I wanted to know that when we partition the data
using sc.parallelize , Spark actually broadcasts parts of the RDD to all
the worker machines or it reads the corresponding segment locally from the
memory of the worker machine?

How to I avoid movement of this data? Will it help if I store the file in
HDFS?

Thanks and Regards,
Disha
"
Disha Shrivastava <dishu.905@gmail.com>,"Tue, 29 Dec 2015 17:40:07 +0530",Re: Data and Model Parallelism in MLPC,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi Alexander,

Thanks a lot for your response.Yes, I am considering the use case when the
weight matrix is too large to fit into the main memory of a single machine.

Can you tell me ways of dividing the weight matrix? According to my
investigations so far, we can do this by two ways:

1. By parallelizing the weight matrix RDD using sc.parallelize and then
using suitable map functions in the forward and backward pass.
2. By using RowMatrix / BlockMatrix to represent the weight matrix and do
calculations on it.

Which of these methods will be efficient to use ? Also, I came across an
implementation using Akka where layer-by-layer partitioning of the network
has been done (
http://alexminnaar.com/implementing-the-distbelief-deep-neural-network-training-framework-with-akka.html)
which I believe is model parallelism in the true sense.

Please suggest any other ways/implementation that can help. I would love to
hear your remarks on the above.

Thanks and Regards,
Disha


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Tue, 29 Dec 2015 18:56:06 +0100",Is there any way to stop a jenkins build,dev@spark.apache.org,"My AMPLAB jenkins build has been stuck for a few hours now:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/48414/consoleFull

Is there a way for me to stop the build?

Kind regards,

Herman van H√∂vell
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 29 Dec 2015 09:59:29 -0800",Re: Is there any way to stop a jenkins build,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"HiveThriftBinaryServerSuite got stuck.

I thought Josh has fixed this issue:

[SPARK-11823][SQL] Fix flaky JDBC cancellation test in
HiveThriftBinaryServerSuite


consoleFull
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Tue, 29 Dec 2015 19:04:00 +0100",Re: Is there any way to stop a jenkins build,Ted Yu <yuzhihong@gmail.com>,"Thanks. I'll merge the most recent master...

Still curious if we can stop a build.

Kind regards,

Herman van H√∂vell tot Westerflier

2015-12-29 18:59 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:

/consoleFull
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 29 Dec 2015 10:19:02 -0800",Re: Is there any way to stop a jenkins build,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Yeah, I thought that my quick fix might address the
HiveThriftBinaryServerSuite hanging issue, but it looks like it didn't work
so I'll now have to do the more principled fix of using a UDF which sleeps
for some amount of time.

In order to stop builds, you need to have a Jenkins account with the proper
permissions. I believe that it's generally only Spark committers and AMPLab
members who have accounts + Jenkins SSH access.

I've gone ahead killed the build for you. It looks like someone had
configured the pull request builder timeout to be 300 minutes (5 hours),
but I think we should consider decreasing that to match the timeout used by
the Spark full test jobs.


<
4/consoleFull
"
Shixiong Zhu <zsxwing@gmail.com>,"Tue, 29 Dec 2015 10:22:53 -0800",Re: Spark streaming 1.6.0-RC4 NullPointerException using mapWithState,jan@insidin.com,"Could you create a JIRA? We can continue the discussion there. Thanks!

Best Regards,
Shixiong Zhu

2015-12-29 3:42 GMT-08:00 Jan Uyttenhove <jan@insidin.com>:

"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Tue, 29 Dec 2015 19:30:33 +0100",Re: Is there any way to stop a jenkins build,Josh Rosen <joshrosen@databricks.com>,"Hi Josh,

Your HiveThriftBinaryServerSuite fix wasn't in the build I was running (I
forgot to merge the latest master). So it might actually work.

As for stopping the build, it is understandable that you cannot do that
without the proper permissions. It would still be cool to be able to issue
a 'stop build' command from github though.

Kind regards,

Herman

2015-12-29 19:19 GMT+01:00 Josh Rosen <joshrosen@databricks.com>:

rk
s
nd
by
<
 <
14/consoleFull
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 29 Dec 2015 10:40:38 -0800",Re: Is there any way to stop a jenkins build,Josh Rosen <joshrosen@databricks.com>,"bq. had configured the pull request builder timeout to be 300 minutes (5
hours)

From
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=spark-test/4592/
:

   - 1 hr 55 min building on an executor;
   - 1 hr 55 min total from scheduled to completion.

Looks like 2.5 hours should be good enough.

Cheers


rk
s
nd
by
<
 <
14/consoleFull
"
ai he <heai0727@gmail.com>,"Tue, 29 Dec 2015 10:48:51 -0800",Re: RDD[Vector] Immutability issue,salexln <salexln@gmail.com>,"Hi salexln,

RDD's immutability depends on the underlying structure. I have the
following example.

------------------------------------------------------------------------------------------------------------------
scala> val m = Array.fill(2, 2)(0)
m: Array[Array[Int]] = Array(Array(0, 0), Array(0, 0))


scala> val rdd = sc.parallelize(m)
rdd: org.apache.spark.rdd.RDD[Array[Int]] = ParallelCollectionRDD[1]
at parallelize at <console>:23


scala> rdd.collect()
res6: Array[Array[Int]] = Array(Array(0, 0), Array(0, 0))


scala> m(0)(1) = 2


scala> rdd.collect()
res8: Array[Array[Int]] = Array(Array(0, 2), Array(0, 0))
------------------------------------------------------------------------------------------------------------------

You see that variable rdd actually changes when its underlying array
changes. Hopefully this helps you.

Best,
Ai




-- 
Best
Ai

---------------------------------------------------------------------


"
salexln <salexln@gmail.com>,"Tue, 29 Dec 2015 12:19:34 -0700 (MST)",Re: RDD[Vector] Immutability issue,dev@spark.apache.org,"I see, so in order the RDD to be completely immutable, its content should be
immutable as well.

And if the content is not immutable, we can change its content, but cannot
add / remove data?




--

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Tue, 29 Dec 2015 11:22:27 -0800",Re: running lda in spark throws exception,Li Li <fancyerii@gmail.com>,"Hi Li,

I'm wondering if you're running into the same bug reported here:
https://issues.apache.org/jira/browse/SPARK-12488

I haven't figured out yet what is causing it.  Do you have a small corpus
which reproduces this error, and which you can share on the JIRA?  If so,
that would help a lot in debugging this failure.

Thanks!
Joseph


"
ai he <heai0727@gmail.com>,"Tue, 29 Dec 2015 11:24:49 -0800",Re: RDD[Vector] Immutability issue,salexln <salexln@gmail.com>,"Same thing.

Say, your underlying structure is like Array(ArrayBuffer(1, 2),
ArrayBuffer(3, 4)).

Then you can add/remove data in ArrayBuffers and then the change will
be reflected in the rdd.






-- 
Best
Ai

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 29 Dec 2015 11:50:59 -0800",Re: RDD[Vector] Immutability issue,ai he <heai0727@gmail.com>,"You can, but you shouldn't.  Using backdoors to mutate the data in an RDD
is a good way to produce confusing and inconsistent results when, e.g., an
RDD's lineage needs to be recomputed or a Task is resubmitted on fetch
failure.


"
Vivekananda Venkateswaran <svvenkat@gmail.com>,"Tue, 29 Dec 2015 15:09:14 -0500",Re: RDD[Vector] Immutability issue,Mark Hamstra <mark@clearstorydata.com>,"RDD is collection of object And if these objects are mutable and changed
then the same will reflect in RDD.
For immutable objects it will not. Changing the mutable objects that are in
the RDD is not right practise.

The RDD is immutable in the sense that any transformation on the RDD  will
result in new RDD object.



"
Reynold Xin <rxin@databricks.com>,"Tue, 29 Dec 2015 13:20:42 -0800",Re: Partitioning of RDD across worker machines,Disha Shrivastava <dishu.905@gmail.com>,"If you use hadoopFile (or textFile) and have the same file on the same path
in every node, I suspect it might just work.


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 29 Dec 2015 13:40:13 -0800",Re: Spark streaming 1.6.0-RC4 NullPointerException using mapWithState,jan@insidin.com," Hi Jan, could you post your codes? I could not reproduce this issue in my
environment.

Best Regards,
Shixiong Zhu

2015-12-29 10:22 GMT-08:00 Shixiong Zhu <zsxwing@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Tue, 29 Dec 2015 21:36:05 -0800",IndentationCheck of checkstyle,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I noticed that there are a lot of checkstyle warnings in the following form:

<error line=""1784"" severity=""error"" message=""&apos;block&apos; child have
incorrect indentation level 8, expected level should be 10.""
source=""com.puppycrawl.tools.checkstyle.
checks.indentation.IndentationCheck""/>

To my knowledge, we use two spaces for each tab. Not sure why all of a
sudden we have so many IndentationCheck warnings:

grep 'hild have incorrect indentati' trunkCheckstyle.xml | wc
    3133   52645  678294

If there is no objection, I would create a JIRA and relax IndentationCheck
warning.

Cheers
"
"""Allen Zhang"" <allenzhang010@126.com>","Wed, 30 Dec 2015 13:46:52 +0800 (CST)",Re:IndentationCheck of checkstyle,"""Ted Yu"" <yuzhihong@gmail.com>","

format issue I think, go ahead




At 2015-12-30 13:36:05, ""Ted Yu"" <yuzhihong@gmail.com> wrote:

Hi,
I noticed that there are a lot of checkstyle warnings in the following form:


<error line=""1784"" severity=""error"" message=""&apos;block&apos; child have incorrect indentation level 8, expected level should be 10."" source=""com.puppycrawl.tools.checkstyle.   checks.indentation.IndentationCheck""/>



To my knowledge, we use two spaces for each tab. Not sure why all of a sudden we have so many IndentationCheck warnings:


grep 'hild have incorrect indentati' trunkCheckstyle.xml | wc
    3133   52645  678294


If there is no objection, I would create a JIRA and relax IndentationCheck warning.


Cheers"
Ted Yu <yuzhihong@gmail.com>,"Tue, 29 Dec 2015 21:55:16 -0800",Re: IndentationCheck of checkstyle,Reynold Xin <rxin@databricks.com>,"Oops, wrong list :-)

e:
rm:
d have incorrect indentation level 8, expected level should be 10."" source=""com.puppycrawl.tools.checkstyle.   checks.indentation.IndentationCheck""/>
dden we have so many IndentationCheck warnings:
k warning.
"
Reynold Xin <rxin@databricks.com>,"Tue, 29 Dec 2015 21:58:02 -0800",Re: IndentationCheck of checkstyle,Ted Yu <yuzhihong@gmail.com>,"OK to close the loop - this thread has nothing to do with Spark?



"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Wed, 30 Dec 2015 14:57:28 +0800",problem with reading source code-pull out nondeterministic expresssions,dev@spark.apache.org,"Hi fellas,
I am new to spark and I have a newbie question. I am currently reading the source code in spark sql catalyst analyzer. I not quite understand the partial function in PullOutNondeterministric. What does it mean by ""pull out°±? Why do we have to do the ""pulling out°±?
I would really appreciate it if somebody explain it to me. 
Thanks. "
Li Li <fancyerii@gmail.com>,"Wed, 30 Dec 2015 15:34:28 +0800",Re: running lda in spark throws exception,Joseph Bradley <joseph@databricks.com>,"I will use a portion of data and try. will the hdfs block affect
spark?(if so, it's hard to reproduce)


---------------------------------------------------------------------


"
Disha Shrivastava <dishu.905@gmail.com>,"Wed, 30 Dec 2015 17:33:10 +0530",Re: Data and Model Parallelism in MLPC,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi,

I went through the code for implementation of MLPC and couldn't understand
why stacking/unstacking of the input data has been done. The description
says "" Block size for stacking input data in matrices to speed up the
computation. Data is stacked within partitions. If block size is more than
remaining data in a partition then it is adjusted to the size of this
data. Recommended
size is between 10 and 1000. Default: 128"". I am not pretty sure what this
means and how does this attain speed in computation?

Also, I couldn't find exactly how data parallelism as depicted in
http://static.googleusercontent.com/media/research.google.com/hi//archive/large_deep_networks_nips2012.pdf
is incorporated in the existing code. There seems to be no notion of
parameter server and optimization routine is also normal LBFGS not
Sandblaster LBFGS. The only parallelism seems to be coming from the way
input data is read and stored.

Please correct me if I am wrong and clarify my doubt.

Thanks and Regards,
Disha


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 30 Dec 2015 04:07:48 -0800",Re: IndentationCheck of checkstyle,Reynold Xin <rxin@databricks.com>,"Right. 

Pardon my carelessness. 

ote:
orm:
ild have incorrect indentation level 8, expected level should be 10."" source=""com.puppycrawl.tools.checkstyle.   checks.indentation.IndentationCheck""/udden we have so many IndentationCheck warnings:
eck warning.
"
Li Li <fancyerii@gmail.com>,"Wed, 30 Dec 2015 20:59:33 +0800",Re: running lda in spark throws exception,Joseph Bradley <joseph@databricks.com>,"I use a small data and reproduce the problem.
But I don't know my codes are correct or not because I am not familiar
with spark.
So I first post my codes here. If it's correct, then I will post the data.
one line of my data like:

{ ""time"":""08-09-17"",""cmtUrl"":""2094361""
,""rvId"":""rev_10000020"",""webpageUrl"":""http://www.dianping.com/shop/2094361"",""word_vec"":[0,1,2,3,4,5,6,2,7,8,9
    ,10,11,12,13,14,15,16,8,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,32,35,36,37,38,15,39,40,41,42,5,43,44,17,45,46,42,47,26,48,49]}

it's a json file which contains webpageUrl and word_vec which is the
encoded words.
The first step is to prase the input rdd to a rdd of VectorUrl.
BTW, if public VectorUrl call(String s) return null, is it ok?
Then follow the example Index documents with unique IDs
Then I create a rdd to map id to url so after lda training, I can find
the url of the document. Then save this rdd to hdfs.
Then create corpus rdd and train

The exception stack is

15/12/30 20:45:42 ERROR yarn.ApplicationMaster: User class threw
exception: java.lang.IndexOutOfBoundsException: (454,0) not in
[-58,58) x [-100,100)
java.lang.IndexOutOfBoundsException: (454,0) not in [-58,58) x [-100,100)
at breeze.linalg.DenseMatrix$mcD$sp.update$mcD$sp(DenseMatrix.scala:112)
at org.apache.spark.mllib.clustering.DistributedLDAModel$$anonfun$topicsMatrix$1.apply(LDAModel.scala:534)
at org.apache.spark.mllib.clustering.DistributedLDAModel$$anonfun$topicsMatrix$1.apply(LDAModel.scala:531)
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
at org.apache.spark.mllib.clustering.DistributedLDAModel.topicsMatrix$lzycompute(LDAModel.scala:531)
at org.apache.spark.mllib.clustering.DistributedLDAModel.topicsMatrix(LDAModel.scala:523)
at com.mobvoi.knowledgegraph.textmining.lda.ReviewLDA.main(ReviewLDA.java:89)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:525)


==========here is my codes==============

SparkConf conf = new SparkConf().setAppName(ReviewLDA.class.getName());

    JavaSparkContext sc = new JavaSparkContext(conf);


    // Load and parse the data

    JavaRDD<String> data = sc.textFile(inputDir + ""/*"");

    JavaRDD<VectorUrl> parsedData = data.map(new Function<String, VectorUrl>() {

      public VectorUrl call(String s) {

        JsonParser parser = new JsonParser();

        JsonObject jo = parser.parse(s).getAsJsonObject();

        if (!jo.has(""word_vec"") || !jo.has(""webpageUrl"")) {

          return null;

        }

        JsonArray word_vec = jo.get(""word_vec"").getAsJsonArray();

        String url = jo.get(""webpageUrl"").getAsString();

        double[] values = new double[word_vec.size()];

        for (int i = 0; i < values.length; i++)

          values[i] = word_vec.get(i).getAsInt();

        return new VectorUrl(Vectors.dense(values), url);

      }

    });



    // Index documents with unique IDs

    JavaPairRDD<Long, VectorUrl> id2doc =
JavaPairRDD.fromJavaRDD(parsedData.zipWithIndex().map(

        new Function<Tuple2<VectorUrl, Long>, Tuple2<Long, VectorUrl>>() {

          public Tuple2<Long, VectorUrl> call(Tuple2<VectorUrl, Long> doc_id) {

            return doc_id.swap();

          }

        }));

    JavaPairRDD<Long, String> id2Url = JavaPairRDD.fromJavaRDD(id2doc

        .map(new Function<Tuple2<Long, VectorUrl>, Tuple2<Long, String>>() {

          @Override

          public Tuple2<Long, String> call(Tuple2<Long, VectorUrl>
id2doc) throws Exception {

            return new Tuple2(id2doc._1, id2doc._2.url);

          }

        }));

    id2Url.saveAsTextFile(id2UrlPath);

    JavaPairRDD<Long, Vector> corpus = JavaPairRDD.fromJavaRDD(id2doc

        .map(new Function<Tuple2<Long, VectorUrl>, Tuple2<Long, Vector>>() {

          @Override

          public Tuple2<Long, Vector> call(Tuple2<Long, VectorUrl>
id2doc) throws Exception {

            return new Tuple2(id2doc._1, id2doc._2.vec);

          }

        }));

    corpus.cache();


    // Cluster the documents into three topics using LDA

    DistributedLDAModel ldaModel = (DistributedLDAModel) new
LDA().setMaxIterations(iterNumber)

        .setK(topicNumber).run(corpus);


---------------------------------------------------------------------


"
Li Li <fancyerii@gmail.com>,"Wed, 30 Dec 2015 21:03:10 +0800",Re: running lda in spark throws exception,Joseph Bradley <joseph@databricks.com>,"the data is in the attachment


---------------------------------------------------------------------"
Michael Armbrust <michael@databricks.com>,"Wed, 30 Dec 2015 11:00:45 -0800",Re: problem with reading source code-pull out nondeterministic expresssions,=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>,"The goal here is to ensure that the non-deterministic value is evaluated
only once, so the result won't change for a given row (i.e. when sorting).


e
"
Josh Rosen <joshrosen@databricks.com>,"Wed, 30 Dec 2015 12:52:29 -0800",New processes / tools for changing dependencies in Spark,Dev <dev@spark.apache.org>,"I just merged https://github.com/apache/spark/pull/10461, a PR that adds
new automated tooling to help us reason about dependency changes in Spark.
Here's a summary of the changes:

   - The dev/run-tests script (used in the SBT Jenkins builds and for
   testing Spark pull requests) now generates a file which contains Spark's
   resolved runtime classpath for each Hadoop profile, then compares that file
   to a copy which is checked into the repository. These dependency lists are
   found at https://github.com/apache/spark/tree/master/dev/deps; there is
   a separate list for each Hadoop profile.

   - If a pull request changes dependencies without updating these manifest
   files, our test script will fail the build
   <https://github.com/apache/spark/pull/10461#issuecomment-168066328> and
   the build console output will list the dependency diff
   <https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/48505/consoleFull>
   .

   - If you are intentionally changing dependencies, run
./dev/test-dependencies.sh
   --replace-manifest to re-generate these dependency manifests then commit
   the changed files and include them with your pull request.

The goal of this change is to make it simpler to reason about build
changes: it should now be much easier to verify whether dependency
exclusions worked properly or determine whether transitive dependencies
changed in a way that affects the final classpath.

Let me know if you have any questions about this change and, as always,
feel free to submit pull requests if you would like to make any
enhancements to this script.

Thanks,
Josh
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 30 Dec 2015 17:32:31 -0800",Re: Spark streaming 1.6.0-RC4 NullPointerException using mapWithState,jan@insidin.com,"I went through StateMap.scala a few times but didn't find any logic error
yet.

According to the call stack, the following was executed in get(key):

    } else {
      parentStateMap.get(key)
    }
This implies that parentStateMap was null.
But it seems parentStateMap is properly assigned in readObject().

Jan:
Which serializer did you use ?

Thanks


"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 30 Dec 2015 19:00:54 -0800",Automated close of PR's ?,dev@spark.apache.org,"Is there a script running to close ""old"" PR's ? I was not aware of any
discussion about this in dev list.

- Mridul

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 30 Dec 2015 19:12:39 -0800",Re: Automated close of PR's ?,Mridul Muralidharan <mridul@gmail.com>,"No there is not. I actually manually closed them to cut down the number of
open pull requests. Feel free to reopen individual ones.


"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 30 Dec 2015 19:25:57 -0800",Re: Automated close of PR's ?,Reynold Xin <rxin@databricks.com>,"I am not sure of others, but I had a PR close from under me where
ongoing discussion was as late as 2 weeks back.
Given this, I assumed it was automated close and not manual !

When the change was opened is not a good metric about viability of the
change (particularly when it touches code which is rarely modified;
and so will merge after multiple releases).

Regards,
Mridul


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 31 Dec 2015 08:02:57 +0000",Re: Automated close of PR's ?,Mridul Muralidharan <mridul@gmail.com>,"There's a script that can be run manually which closes PRs that have
been 'requested' to be closed. I'm not sure of the exact words it
looks for but ""Do you mind closing this PR?"" seems to work. However it
does seem to mean that PRs will occasionally get closed as a false
positive, so maybe that happened here.

You can use your judgment about whether to reopen, but I tend to think
PRs are not meant to be long-lived. They don't go away even when
closed, so can always stand as a record of a proposed change or be
reopened. But there shouldn't be such a thing as a PR open for months.
(In practice, you can see a huge number of dead, stale PRs are left
open by people out there anyway)


---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 31 Dec 2015 00:59:33 -0800",Re: Automated close of PR's ?,Sean Owen <sowen@cloudera.com>,"reference of discussion about review and changes.
Particularly so for spark since JIRA's and review board are not used
for code review.
Note - they are not used only in spark, but by other organization's to
track contributions (like in our case).

If you look at Reynold's response, he has clarified they were closed
by him and not via user request - he probably missed out on activity
on some of the PR's when closing in bulk.
I would have preferred pinging the PR contributors to close, and
subsequently doing so if inactive after ""some"" time (and definitely
not when folks are off on vacations).

Regards,
Mridul




---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 31 Dec 2015 01:13:42 -0800",Re: Automated close of PR's ?,Mridul Muralidharan <mridul@gmail.com>,"Hi Mridul,

Thanks for the message. All you said made sense to me.

It can definitely be frustrating when one of your pull requests got
accidentally closed, and that's why we often ping the original authors to
close them. However, this doesn't always work because the original authors
might be inactive for some old pull requests, and sorting these out of 400+
open pull requests can be pretty tough.

Regardless of whether pull requests should be long-lived or short-lived,
closing a pull request does not wipe any of its content. All the changes
and their associated reviews are kept there, and it is trivial to re-open
(one click of a button).

I also agree with your point that the ""date of open"" is not the best metric
to look at here. Inactivity for a certain period is a much better metric to
use in the future.



"
Sean Owen <sowen@cloudera.com>,"Thu, 31 Dec 2015 09:15:09 +0000",Re: Automated close of PR's ?,Mridul Muralidharan <mridul@gmail.com>,"Yes, I mean ""open"" for a long time, but I do mean PRs aren't intended
to be open for long periods. Of course, they actually stick around
forever on github.

I think Reynold did manually close yours, but I was noting for the
record that there's also an automated process that does this in
response to a request. That has also surprised people in the past.

Generally, we have way more problem with people abandoning or failing
to follow through on PRs, or simply proposing things that aren't going
to be merged. I agree in general with reflecting the reality by
closing lots more JIRAs and PRs -- mostly because these are not
permanent operations at all, and the intent is that in the occasional
case where the owner disagrees, it can simply be reopened. This serves
as a reminder that we need to drive all of these things to a
conclusion.


---------------------------------------------------------------------


"
Jan Uyttenhove <jan@insidin.com>,"Thu, 31 Dec 2015 12:43:04 +0100",Re: Spark streaming 1.6.0-RC4 NullPointerException using mapWithState,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for your answers, and I'm sorry for the sight delay.

I was trying to narrow it down first since I noticed very unpredictable
behaviour in reproducing it. Finally the unpredictability seemed related to
the message format of the messages on Kafka, so I also came to suspect it
had something to do with serialization.

I was using the KryoSerializer, and I can confirm that it is in fact
related to it (no exception when using the JavaSerializer). And it's
unrelated to Kafka.

Apparently the exception occurs when restoring state from previous
checkpoints and using KryoSerialization. When the job has checkpoints from
a previous run (containing state) and is started with new messages already
available on e.g. a Kafka topic (or via nc), the NPE occurs. And this is -
of course - a typical use case of using Kafka with Spark streaming and
checkpointing.

As requested I created issue SPARK-12591 (
https://issues.apache.org/jira/browse/SPARK-12591). It contains the
procedure to reproduce the error with the testcase which is again a
modified version of the StatefulNetworkWordCount Spark streaming example (
https://gist.github.com/juyttenh/9b4a4103699a7d5f698f).

Best regards,
Jan




-- 
Jan Uyttenhove
Streaming data & digital solutions architect @ Insidin bvba

jan@insidin.com

https://twitter.com/xorto
https://www.linkedin.com/in/januyttenhove

This e-mail and any files transmitted with it are intended solely for the
use of the individual or entity to whom they are addressed. It may contain
privileged and confidential information. If you are not the intended
recipient please notify the sender immediately and destroy this e-mail. Any
form of reproduction, dissemination, copying, disclosure, modification,
distribution and/or publication of this e-mail message is strictly
prohibited. Whilst all efforts are made to safeguard e-mails, the sender
cannot guarantee that attachments are virus free or compatible with your
systems and does not accept liability in respect of viruses or computer
problems experienced.
"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Thu, 31 Dec 2015 23:06:07 +0800","Re: problem with reading source code-pull out nondeterministic
 expresssions",Michael Armbrust <michael@databricks.com>,"I get it, thanks!

Armbrust <michael@databricks.com> –¥µ¿£∫
evaluated only once, so the result won't change for a given row (i.e. when sorting).
the source code in spark sql catalyst analyzer. I not quite understand the partial function in PullOutNondeterministric. What does it mean by ""pull out°±? Why do we have to do the ""pulling out°±?

"
