Muhammed Uluyol <uluyol@umich.edu>,"Thu, 01 Oct 2015 02:49:24 +0000",Speculatively using spare capacity,dev@spark.apache.org,"Hello,

How feasible would it be to have spark speculatively increase the number of
partitions when there is spare capacity in the system? We want to do this
to increase to decrease application runtime. Initially, we will assume that
function calls of the same type will have the same runtime (e.g. all maps
take equal time) and that the runtime will scale linearly with the number
of workers. If a numPartitions value is specified, we may increase beyond
this, but if a Partitioner is specified, we would not change the number of
partitions.

Some initial questions we had:
 * Does spark already do this?
 * Is there interest in supporting this functionality?
 * Are there any potential issues that we should be aware of?
 * What changes would need to be made for such a project?

Thanks,
Muhammed
"
Sean Owen <sowen@cloudera.com>,"Thu, 1 Oct 2015 00:53:55 -0400",Re: Speculatively using spare capacity,Muhammed Uluyol <uluyol@umich.edu>,"Why change the number of partitions of RDDs? especially since you
can't generally do that without a shuffle. If you just mean to ramp up
and down resource usage, dynamic allocation (of executors) already
does that.


---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Thu, 1 Oct 2015 11:50:41 +0530",Spark 1.6 Release window is not updated in Spark-wiki,dev@spark.apache.org,"Hi,
In the https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage the
current release window has not been changed from 1.5. Can anybody give an
idea of the expected dates for 1.6 version?

Regards,

Meethu Mathew
Senior Engineer
Flytxt
"
Sean Owen <sowen@cloudera.com>,"Thu, 1 Oct 2015 06:55:29 -0400",Re: Spark 1.6 Release window is not updated in Spark-wiki,Meethu Mathew <meethu.mathew@flytxt.com>,"My guess is that the 1.6 merge window should close at the end of
November (2 months from now)? I can update it but wanted to check if
anyone else has a preferred tentative plan.


---------------------------------------------------------------------


"
Ewan Leith <ewan.leith@realitymine.com>,"Thu, 1 Oct 2015 14:33:11 +0000",Dataframe nested schema inference from Json without type conflicts,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

We really like the ability to infer a schema from JSON contained in an RDD, but when we're using Spark Streaming on small batches of data, we sometimes find that Spark infers a more specific type than it should use, for example if the json in that small batch only contains integer values for a String field, it'll class the field as an Integer type on one Streaming batch, then a String on the next one.

Instead, we'd rather match every value as a String type, then handle any casting to a desired type later in the process.

I don't think there's currently any simple way to avoid this that I can see, but we could add the functionality in the JacksonParser.scala file, probably in convertField.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JacksonParser.scala

Does anyone know an easier and cleaner way to do this?

Thanks,
Ewan
"
Rishitesh Mishra <rishi80.mishra@gmail.com>,"Thu, 1 Oct 2015 23:10:15 +0530",Re: Task Execution,gsvic <victorasgs@gmail.com>,"Depending upon the configured cores assigned to the executor, scheduler
will assign that many tasks. So yes they execute in parallel.

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 1 Oct 2015 11:30:06 -0700",Re: Spark 1.6 Release window is not updated in Spark-wiki,Sean Owen <sowen@cloudera.com>,"Ah - I can update it. Usually i do it after the release is cut. It's
just a standard 3 month cadence.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 1 Oct 2015 11:31:55 -0700",Re: Spark 1.6 Release window is not updated in Spark-wiki,Sean Owen <sowen@cloudera.com>,"BTW - the merge window for 1.6 is September+October. The QA window is
November and we'll expect to ship probably early december. We are on a
3 month release cadence, with the caveat that there is some
pipelining... as we finish release X we are already starting on
release X+1.

- Patrick


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 1 Oct 2015 17:12:09 -0400",Re: Dataframe nested schema inference from Json without type conflicts,Ewan Leith <ewan.leith@realitymine.com>,"You can pass the schema into json directly, can't you?


a, we
ne
dle any
his that I can
pache/spark/sql/execution/datasources/json/JacksonParser.scala
"
Ewan Leith <ewan.leith@realitymine.com>,"Thu, 1 Oct 2015 21:27:04 +0000","Re: Dataframe nested schema inference from Json without type
 conflicts","""rxin@databricks.com"" <rxin@databricks.com>","We could, but if a client sends some unexpected records in the schema (which happens more than I'd like, our schema seems to constantly evolve), its fantastic how Spark picks up on that data and includes it.


Passing in a fixed schema loses that nice additional ability, though it's what we'll probably have to adopt if we can't come up with a way to keep the inference working.


Thanks,

Ewan


------ Original message------

From: Reynold Xin

Date: Thu, 1 Oct 2015 22:12

To: Ewan Leith;

Cc: dev@spark.apache.org;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


You can pass the schema into json directly, can't you?

Hi all,

We really like the ability to infer a schema from JSON contained in an RDD, but when we're using Spark Streaming on small batches of data, we sometimes find that Spark infers a more specific type than it should use, for example if the json in that small batch only contains integer values for a String field, it'll class the field as an Integer type on one Streaming batch, then a String on the next one.

Instead, we'd rather match every value as a String type, then handle any casting to a desired type later in the process.

I don't think there's currently any simple way to avoid this that I can see, but we could add the functionality in the JacksonParser.scala file, probably in convertField.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JacksonParser.scala

Does anyone know an easier and cleaner way to do this?

Thanks,
Ewan

"
Yin Huai <yhuai@databricks.com>,"Thu, 1 Oct 2015 15:54:16 -0700",Re: Dataframe nested schema inference from Json without type conflicts,Ewan Leith <ewan.leith@realitymine.com>,"Hi Ewan,

For your use case, you only need the schema inference to pick up the
structure of your data (basically you want spark sql to infer the type of
complex values like arrays and structs but keep the type of primitive
values as strings), right?

Thanks,

Yin


,
ta, we
,
one
ndle any
this that I can
apache/spark/sql/execution/datasources/json/JacksonParser.scala
"
Paul Wais <paulwais@gmail.com>,"Thu, 1 Oct 2015 16:53:09 -0700 (MST)",Re: Tungsten off heap memory access for C++ libraries,dev@spark.apache.org,"Update for those who are still interested: djinni is a nice tool for
generating Java/C++ bindings.  Before today djinni's Java support was only
aimed at Android, but now djinni works with (at least) Debian, Ubuntu, and
CentOS.

djinni will help you run C++ code in-process with the caveat that djinni
only supports deep-copies of on-JVM-heap data (and no special off-heap
features yet).  However, you can in theory use Unsafe to get pointers to
off-heap memory and pass those (as ints) to native code.  

So if you need a solution *today*,  try checking out a small demo:
https://github.com/dropbox/djinni/tree/master/example/localhost

For the long deets, see:
 https://github.com/dropbox/djinni/pull/140



--

---------------------------------------------------------------------


"
Ewan Leith <ewan.leith@realitymine.com>,"Fri, 2 Oct 2015 00:57:29 +0000","Re: Dataframe nested schema inference from Json without type
 conflicts","""yhuai@databricks.com"" <yhuai@databricks.com>","Exactly, that's a much better way to put it.


Thanks,

Ewan


------ Original message------

From: Yin Huai

Date: Thu, 1 Oct 2015 23:54

To: Ewan Leith;

Cc: rxin@databricks.com;dev@spark.apache.org;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


Hi Ewan,

For your use case, you only need the schema inference to pick up the structure of your data (basically you want spark sql to infer the type of complex values like arrays and structs but keep the type of primitive values as strings), right?

Thanks,

Yin


We could, but if a client sends some unexpected records in the schema (which happens more than I'd like, our schema seems to constantly evolve), its fantastic how Spark picks up on that data and includes it.


Passing in a fixed schema loses that nice additional ability, though it's what we'll probably have to adopt if we can't come up with a way to keep the inference working.


Thanks,

Ewan


------ Original message------

From: Reynold Xin

Date: Thu, 1 Oct 2015 22:12

To: Ewan Leith;

Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


You can pass the schema into json directly, can't you?

Hi all,

We really like the ability to infer a schema from JSON contained in an RDD, but when we're using Spark Streaming on small batches of data, we sometimes find that Spark infers a more specific type than it should use, for example if the json in that small batch only contains integer values for a String field, it'll class the field as an Integer type on one Streaming batch, then a String on the next one.

Instead, we'd rather match every value as a String type, then handle any casting to a desired type later in the process.

I don't think there's currently any simple way to avoid this that I can see, but we could add the functionality in the JacksonParser.scala file, probably in convertField.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JacksonParser.scala

Does anyone know an easier and cleaner way to do this?

Thanks,
Ewan


"
Reynold Xin <rxin@databricks.com>,"Thu, 1 Oct 2015 22:42:31 -0400",[ANNOUNCE] Announcing Spark 1.5.1,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi All,

Spark 1.5.1 is a maintenance release containing stability fixes. This
release is based on the branch-1.5 maintenance branch of Spark. We
*strongly recommend* all 1.5.0 users to upgrade to this release.

The full list of bug fixes is here: http://s.apache.org/spark-1.5.1

http://spark.apache.org/releases/spark-release-1-5-1.html


(note: it can take a few hours for everything to be propagated, so you
might get 404 on some download links, but everything should be in maven
central already)
"
andy petrella <andy.petrella@gmail.com>,"Fri, 02 Oct 2015 17:49:34 +0000","[Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)","""dev@spark.apache.org"" <dev@spark.apache.org>","Yup folks,

I've been reported by someone building the Spark-Notebook that repo1 is
apparently broken for scala 2.10 and spark 1.5.0.

Check this
https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.10/1.5.0/spark-streaming_2.10-1.5.0.pom

The URL is correct since
https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.10/1.5.0/
is ok...

scala 2.11 is fine btw
https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.11/1.5.0/spark-streaming_2.11-1.5.0.pom
<https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.10/1.5.0/spark-streaming_2.10-1.5.0.pom>

Any idea?

ps: this happens for streaming too at least

-- 
andy
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 2 Oct 2015 10:52:03 -0700","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)",andy petrella <andy.petrella@gmail.com>,"What is broken? Looks fine to me.




-- 
Marcelo

---------------------------------------------------------------------


"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 2 Oct 2015 10:57:39 -0700",SparkR dataframe UDF,"shivaram@eecs.berkeley.edu, dev@spark.apache.org","Hi Shiva,

Is Dataframe UDF implemented in SparkR yet? - I could not find it in below
URL

https://github.com/hlin09/spark/tree/SparkR-streaming/R/pkg/R

Thanks,
Renyi.
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 2 Oct 2015 11:05:17 -0700","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)",andy petrella <andy.petrella@gmail.com>,"I tried to access
https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.10/1.5.0/spark-streaming_2.10-1.5.0.pom
on
Chrome and Firefox (on Mac)
I got 404

FYI


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 2 Oct 2015 11:06:51 -0700","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)",Ted Yu <yuzhihong@gmail.com>,"Hmm, now I get that too (did not get it before). Maybe the servers are
having issues.




-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 2 Oct 2015 11:08:43 -0700","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)",Marcelo Vanzin <vanzin@cloudera.com>,"Andy:
1.5.1 has been released.

Maybe you can use this:
https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.10/1.5.1/spark-streaming_2.10-1.5.1.pom

I can access the above.


"
Reynold Xin <rxin@databricks.com>,"Fri, 2 Oct 2015 11:11:15 -0700","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)",Ted Yu <yuzhihong@gmail.com>,"Both work for me. It's possible maven.org is having problems with some
servers.



"
andy petrella <andy.petrella@gmail.com>,"Fri, 02 Oct 2015 18:18:38 +0000","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)","Reynold Xin <rxin@databricks.com>, Ted Yu <yuzhihong@gmail.com>","yup looks like
it's funky

they may have scalability issues ^^

Le ven. 2 oct. 2015 20:11, Reynold Xin <rxin@databricks.com> a écrit :

.1/spark-streaming_2.10-1.5.1.pom
5.0/spark-streaming_2.10-1.5.0.pom
5.0/spark-streaming_2.10-1.5.0.pom
5.0/
5.0/spark-streaming_2.11-1.5.0.pom
andy
"
andy petrella <andy.petrella@gmail.com>,"Fri, 02 Oct 2015 18:19:22 +0000","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)","Ted Yu <yuzhihong@gmail.com>, Marcelo Vanzin <vanzin@cloudera.com>","it's an option but not a solution, indeed

Le ven. 2 oct. 2015 20:08, Ted Yu <yuzhihong@gmail.com> a écrit :

1/spark-streaming_2.10-1.5.1.pom
.0/spark-streaming_2.10-1.5.0.pom
m
is
.0/spark-streaming_2.10-1.5.0.pom
.0/
.0/spark-streaming_2.11-1.5.0.pom
andy
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 2 Oct 2015 11:26:22 -0700","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)",andy petrella <andy.petrella@gmail.com>,"Andy:
1.5.1 has many critical bug fixes on top of 1.5.0

http://search-hadoop.com/m/q3RTtGrXP31BVt4l1

Please consider using 1.5.1

Cheers


.1/spark-streaming_2.10-1.5.1.pom
5.0/spark-streaming_2.10-1.5.0.pom
5.0/spark-streaming_2.10-1.5.0.pom
5.0/
5.0/spark-streaming_2.11-1.5.0.pom
"
andy petrella <andy.petrella@gmail.com>,"Fri, 02 Oct 2015 18:31:07 +0000","Re: [Build] repo1.maven.org: spark libs 1.5.0 for scala 2.10 poms are
 broken (404)",Ted Yu <yuzhihong@gmail.com>,"sure thing, I'll do it today
I was just talking about the page
thanks btw

Le ven. 2 oct. 2015 20:26, Ted Yu <yuzhihong@gmail.com> a écrit :

5.1/spark-streaming_2.10-1.5.1.pom
.5.0/spark-streaming_2.10-1.5.0.pom
1
.5.0/spark-streaming_2.10-1.5.0.pom
.5.0/
.5.0/spark-streaming_2.11-1.5.0.pom
andy
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 2 Oct 2015 18:39:17 +0000",RE: GraphX PageRank keeps 3 copies of graph in memory,Robin East <robin.east@xense.co.uk>,"Hi Robin,

Sounds interesting. I am running 1.5.0. Could you copy-paste your Storage tab?

I’ve just double checked on another cluster with 1 master and 5 workers. It still has 3 pairs of VertexRDD and EdgeRDD at the end of benchmark’s execution:

RDD Name          Storage Level     Cached Partitions             Fraction Cached                Size in Memory Size in ExternalBlockStore          Size on Disk
VertexRDD         Memory Deserialized 1x Replicated         3              150%     6.9 MB  0.0 B      0.0 B
EdgeRDD             Memory Deserialized 1x Replicated         2              100%     155.5 MB             0.0 B      0.0 B
EdgeRDD             Memory Deserialized 1x Replicated         2              100%     154.7 MB             0.0 B      0.0 B
VertexRDD, VertexRDD Memory Deserialized 1x Replicated         3              150%     8.4 MB  0.0 B      0.0 B
EdgeRDD             Memory Deserialized 1x Replicated         2              100%     202.9 MB             0.0 B      0.0 B
VertexRDD         Memory Deserialized 1x Replicated         2              100%     5.6 MB  0.0 B      0.0 B

During the execution I observe that one pair is added and removed from the list. This should correspond to the unpersist statements in the code.

Also, according to the code, you one should end up with 1 set of RDDs, because of unpersist statements in the end of the loop. Does it make sense to you?

Best regards, Alexander

From: Robin East [mailto:robin.east@xense.co.uk]
Sent: Friday, October 02, 2015 12:27 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: GraphX PageRank keeps 3 copies of graph in memory

Alexander,

I’ve just run the benchmark and only end up with 2 sets of RDDs in the Storage tab. This is on 1.5.0, what version are you using?

Robin
-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action




On 30 Sep 2015, at 23:55, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:

Dear Spark developers,

I would like to understand GraphX caching behavior with regards to PageRank in Spark, in particular, the following implementation of PageRank:
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/PageRank.scala

On each iteration the new graph is created and cached, and the old graph is un-cached:
1) Create new graph and cache it:
rankGraph = rankGraph.joinVertices(rankUpdates) {
        (id, oldRank, msgSum) => rPrb(src, id) + (1.0 - resetProb) * msgSum
      }.cache()
2) Unpersist the old one:
      prevRankGraph.vertices.unpersist(false)
      prevRankGraph.edges.unpersist(false)

According to the code, at the end of each iteration only one graph should be in memory, i.e. one EdgeRDD and one VertexRDD. During the iteration, exactly between the mentioned lines of code, there will be two graphs: old and new. It is two pairs of Edge and Vertex RDDs. However, when I run the example provided in Spark examples folder, I observe the different behavior.

Run the example (I checked that it runs the mentioned code):
$SPARK_HOME/bin/spark-submit --class ""org.apache.spark.examples.graphx.SynthBenchmark""  --master spark://mynode.net:7077 $SPARK_HOME/examples/target/spark-examples.jar

According to “Storage” and RDD DAG in Spark UI, 3 VertexRDDs and 3 EdgeRDDs are cached, even when all iterations are finished, given that the mentioned code suggests caching at most 2 (and only in particular stage of the iteration):
https://drive.google.com/file/d/0BzYMzvDiCep5WFpnQjFzNy0zYlU/view?usp=sharing
Edges (the green ones are cached):
https://drive.google.com/file/d/0BzYMzvDiCep5S2JtYnhVTlV1Sms/view?usp=sharing
Vertices (the green ones are cached):
https://drive.google.com/file/d/0BzYMzvDiCep5S1k4N2NFb05RZDA/view?usp=sharing

Could you explain, why 3 VertexRDDs and 3 EdgeRDDs are cached?

Is it OK that there is a double caching in code, given that joinVertices implicitly caches vertices and then the graph is cached in the PageRank code?

Best regards, Alexander

"
Joseph Bradley <joseph@databricks.com>,"Fri, 2 Oct 2015 12:07:28 -0700","Re: RowMatrix tallSkinnyQR - ERROR: Second call to constructor of
 static parser",Saif.A.Ellafi@wellsfargo.com,"Is this specific to tallSkinnyQR, or can you trigger it with other
operations?  It would be helpful to see enough of the trace to know where
the parser is being used.


"
Justin Uang <justin.uang@gmail.com>,"Fri, 02 Oct 2015 19:20:10 +0000",Python UDAFs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Is there a Python API for UDAFs?

Thanks!

Justin
"
Reynold Xin <rxin@databricks.com>,"Fri, 2 Oct 2015 12:21:34 -0700",Re: Python UDAFs,Justin Uang <justin.uang@gmail.com>,"No, not yet.



"
Justin Uang <justin.uang@gmail.com>,"Fri, 02 Oct 2015 19:25:36 +0000",Re: Python UDAFs,Reynold Xin <rxin@databricks.com>,"Cool, filed here: https://issues.apache.org/jira/browse/SPARK-10915


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 05 Oct 2015 00:17:09 +0000",Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Spark dev list <dev@spark.apache.org>,"I’m looking here:

https://s3.amazonaws.com/spark-related-packages/

I believe this is where one set of official packages is published. Please
correct me if this is not the case.

It appears that almost every version of Spark up to and including 1.5.0 has
included a --bin-hadoop1.tgz release (e.g. spark-1.5.0-bin-hadoop1.tgz).

However, 1.5.1 has no such package. There is a
spark-1.5.1-bin-hadoop1-scala2.11.tgz package, but this is a separate
thing. (1.5.0 also has a hadoop1-scala2.11 package.)

Was this intentional?

More importantly, is there some rough specification for what packages we
should be able to expect in this S3 bucket with every release?

This is important for those of us who depend on this publishing venue (e.g.
spark-ec2 and related tools).

Nick
​
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 4 Oct 2015 20:20:35 -0700",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Nicholas Chammas <nicholas.chammas@gmail.com>,"hadoop1 package for Scala 2.10 wasn't in RC1 either:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/

m

z
"
=?UTF-8?B?Qmxhxb4gxaBudWRlcmw=?= <snuderl@gmail.com>,"Mon, 5 Oct 2015 09:27:10 +0200",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Ted Yu <yuzhihong@gmail.com>,"Also missing is
http://s3.amazonaws.com/spark-related-packages/spark-1.5.1-bin-hadoop1.tgz
which breaks spark-ec2 script.


e
"
Guna Prasaad <gunaprsd@gmail.com>,"Mon, 5 Oct 2015 16:22:21 +0530",Difference between a task and a job,dev@spark.apache.org,"What is the difference between a task and a job in spark and
spark-streaming?

Regards,
Guna
"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Mon, 5 Oct 2015 13:23:34 +0200",Re: Difference between a task and a job,Guna Prasaad <gunaprsd@gmail.com>,"Actions trigger jobs. A job is made up of stages. A stage is made up of
tasks. Executor threads execute tasks.

Does that answer your question?


"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Mon, 5 Oct 2015 14:28:13 +0300","StructType has more rows, than corresponding Row has objects.","dev@spark.apache.org, user@spark.apache.org","Hi,

We're building our own framework on top of spark and we give users pretty
complex schema to work with. That requires from us to build dataframes by
ourselves: we transform business objects to rows and struct types and uses
these two to create dataframe.

Everything was fine until I started to upgrade to spark 1.5.0 (from 1.3.1).
Seems to be catalyst engine has been changed and now using almost the same
code to produce rows and struct types I have the following:
http://ibin.co/2HzUsoe9O96l, some of rows in the end result have different
number of values and corresponding struct types.

I'm almost sure it's my own fault, but there is always a small chance, that
something is wrong in spark codebase. If you've seen something similar or
if there is a jira for smth similar, I'd be glad to know. Thanks.
--
Be well!
Jean Morozov
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 05 Oct 2015 15:46:21 +0000",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,"=?UTF-8?B?Qmxhxb4gxaBudWRlcmw=?= <snuderl@gmail.com>, 
	Ted Yu <yuzhihong@gmail.com>","Blaž said:

Also missing is
http://s3.amazonaws.com/spark-related-packages/spark-1.5.1-bin-hadoop1.tgz
which breaks spark-ec2 script.

This is the package I am referring to in my original email.

Nick said:

It appears that almost every version of Spark up to and including 1.5.0 has
included a —bin-hadoop1.tgz release (e.g. spark-1.5.0-bin-hadoop1.tgz).
However, 1.5.1 has no such package.

Nick
​

rote:

1-bin-hadoop1.tgz
e
"
Ewan Leith <ewan.leith@realitymine.com>,"Mon, 5 Oct 2015 16:04:33 +0000","RE: Dataframe nested schema inference from Json without type
 conflicts","""dev@spark.apache.org"" <dev@spark.apache.org>","I've done some digging today and, as a quick and ugly fix, altering the case statement of the JSON inferField function in InferSchema.scala

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/InferSchema.scala

to have

case VALUE_STRING | VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT | VALUE_TRUE | VALUE_FALSE => StringType

rather than the rules for each type works as we'd want.

If we were to wrap this up in a configuration setting in JSONRelation like the samplingRatio setting, with the default being to behave as it currently works, does anyone think a pull request would plausibly get into the Spark main codebase?

Thanks,
Ewan



From: Ewan Leith [mailto:ewan.leith@realitymine.com]
Sent: 02 October 2015 01:57
To: yhuai@databricks.com
Cc: rxin@databricks.com; dev@spark.apache.org
Subject: Re: Dataframe nested schema inference from Json without type conflicts


Exactly, that's a much better way to put it.



Thanks,

Ewan



------ Original message------

From: Yin Huai

Date: Thu, 1 Oct 2015 23:54

To: Ewan Leith;

Cc: rxin@databricks.com;dev@spark.apache.org<mailto:rxin@databricks.com;dev@spark.apache.org>;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


Hi Ewan,

For your use case, you only need the schema inference to pick up the structure of your data (basically you want spark sql to infer the type of complex values like arrays and structs but keep the type of primitive values as strings), right?

Thanks,

Yin


We could, but if a client sends some unexpected records in the schema (which happens more than I'd like, our schema seems to constantly evolve), its fantastic how Spark picks up on that data and includes it.



Passing in a fixed schema loses that nice additional ability, though it's what we'll probably have to adopt if we can't come up with a way to keep the inference working.



Thanks,

Ewan



------ Original message------

From: Reynold Xin

Date: Thu, 1 Oct 2015 22:12

To: Ewan Leith;

Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


You can pass the schema into json directly, can't you?

Hi all,

We really like the ability to infer a schema from JSON contained in an RDD, but when we're using Spark Streaming on small batches of data, we sometimes find that Spark infers a more specific type than it should use, for example if the json in that small batch only contains integer values for a String field, it'll class the field as an Integer type on one Streaming batch, then a String on the next one.

Instead, we'd rather match every value as a String type, then handle any casting to a desired type later in the process.

I don't think there's currently any simple way to avoid this that I can see, but we could add the functionality in the JacksonParser.scala file, probably in convertField.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JacksonParser.scala

Does anyone know an easier and cleaner way to do this?

Thanks,
Ewan


"
Yin Huai <yhuai@databricks.com>,"Mon, 5 Oct 2015 09:39:53 -0700",Re: Dataframe nested schema inference from Json without type conflicts,Ewan Leith <ewan.leith@realitymine.com>,"Hello Ewan,

Adding a JSON-specific option makes sense. Can you open a JIRA for this?
Also, sending out a PR will be great. For JSONRelation, I think we can pass
all user-specific options to it (see
org.apache.spark.sql.execution.datasources.json.DefaultSource's
createRelation) just like what we do for ParquetRelation. Then, inside
JSONRelation, we figure out what kind of options that have been specified.

Thanks,

Yin


ng the
pache/spark/sql/execution/datasources/json/InferSchema.scala
e
ly
k
,
a, we
ne
dle any
his that I can
pache/spark/sql/execution/datasources/json/JacksonParser.scala
"
Ewan Leith <ewan.leith@realitymine.com>,"Mon, 5 Oct 2015 17:12:48 +0000","Re: Dataframe nested schema inference from Json without type
 conflicts","""yhuai@databricks.com"" <yhuai@databricks.com>","Thanks Yin, I'll put together a JIRA and a PR tomorrow.


Ewan


------ Original message------

From: Yin Huai

Date: Mon, 5 Oct 2015 17:39

To: Ewan Leith;

Cc: dev@spark.apache.org;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


Hello Ewan,

Adding a JSON-specific option makes sense. Can you open a JIRA for this? Also, sending out a PR will be great. For JSONRelation, I think we can pass all user-specific options to it (see org.apache.spark.sql.execution.datasources.json.DefaultSource's createRelation) just like what we do for ParquetRelation. Then, inside JSONRelation, we figure out what kind of options that have been specified.

Thanks,

Yin

I've done some digging today and, as a quick and ugly fix, altering the case statement of the JSON inferField function in InferSchema.scala

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/InferSchema.scala

to have

case VALUE_STRING | VALUE_NUMBER_INT | VALUE_NUMBER_FLOAT | VALUE_TRUE | VALUE_FALSE => StringType

rather than the rules for each type works as we'd want.

If we were to wrap this up in a configuration setting in JSONRelation like the samplingRatio setting, with the default being to behave as it currently works, does anyone think a pull request would plausibly get into the Spark main codebase?

Thanks,
Ewan



From: Ewan Leith [mailto:ewan.leith@realitymine.com<mailto:ewan.leith@realitymine.com>]
Sent: 02 October 2015 01:57
To: yhuai@databricks.com<mailto:yhuai@databricks.com>

Cc: rxin@databricks.com<mailto:rxin@databricks.com>; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Dataframe nested schema inference from Json without type conflicts


Exactly, that's a much better way to put it.



Thanks,

Ewan



------ Original message------

From: Yin Huai

Date: Thu, 1 Oct 2015 23:54

To: Ewan Leith;

Cc: rxin@databricks.com;dev@spark.apache.org<mailto:rxin@databricks.com;dev@spark.apache.org>;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


Hi Ewan,

For your use case, you only need the schema inference to pick up the structure of your data (basically you want spark sql to infer the type of complex values like arrays and structs but keep the type of primitive values as strings), right?

Thanks,

Yin


We could, but if a client sends some unexpected records in the schema (which happens more than I'd like, our schema seems to constantly evolve), its fantastic how Spark picks up on that data and includes it.



Passing in a fixed schema loses that nice additional ability, though it's what we'll probably have to adopt if we can't come up with a way to keep the inference working.



Thanks,

Ewan



------ Original message------

From: Reynold Xin

Date: Thu, 1 Oct 2015 22:12

To: Ewan Leith;

Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


You can pass the schema into json directly, can't you?

Hi all,

We really like the ability to infer a schema from JSON contained in an RDD, but when we're using Spark Streaming on small batches of data, we sometimes find that Spark infers a more specific type than it should use, for example if the json in that small batch only contains integer values for a String field, it'll class the field as an Integer type on one Streaming batch, then a String on the next one.

Instead, we'd rather match every value as a String type, then handle any casting to a desired type later in the process.

I don't think there's currently any simple way to avoid this that I can see, but we could add the functionality in the JacksonParser.scala file, probably in convertField.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JacksonParser.scala

Does anyone know an easier and cleaner way to do this?

Thanks,
Ewan



"
weoccc <weoccc@gmail.com>,"Mon, 5 Oct 2015 11:03:48 -0700",spark hive branch location,dev@spark.apache.org,"Hi,

I would like to know where is the spark hive github location where spark
build depend on ? I was told it used to be here
https://github.com/pwendell/hive but it seems it is no longer there.

Thanks a lot,

Weide
"
Michael Armbrust <michael@databricks.com>,"Mon, 5 Oct 2015 14:06:07 -0500",Re: spark hive branch location,weoccc <weoccc@gmail.com>,"I think this is the most up to date branch (used in Spark 1.5):
https://github.com/pwendell/hive/tree/release-1.2.1-spark


"
<Saif.A.Ellafi@wellsfargo.com>,"Mon, 5 Oct 2015 19:57:07 +0000",HiveContext in standalone mode: shuffle hang ups,<dev@spark.apache.org>,"Hi all,

I have a process where local mode takes only 40 seconds. While the same on stand-alone mode, being the same node used for local mode the only available node, is taking up for ever. rdd actions hang up.

I could only ""sort this out"" by turning speculation on, so the same task hanging is retried and then it works.

Sadly, this does not help on huge tasks.

How could I diagnose this fruther?

Thanks,
Saif

"
Josh Rosen <joshrosen@databricks.com>,"Mon, 5 Oct 2015 14:39:46 -0700",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Nicholas Chammas <nicholas.chammas@gmail.com>,"I'm working on a fix for this right now. I'm planning to re-run a modified
copy of the release packaging scripts which will emit only the missing
artifacts (so we won't upload new artifacts with different SHAs for the
builds which *did* succeed).

I expect to have this finished in the next day or so; I'm currently blocked
by some infra downtime but expect that to be resolved soon.

- Josh

m

z
oop1.tgz).
0
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 05 Oct 2015 21:41:51 +0000",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Josh Rosen <joshrosen@databricks.com>,"Thanks for looking into this Josh.


d
gz
doop1.tgz).
"
Renyi Xiong <renyixiong0@gmail.com>,"Mon, 5 Oct 2015 15:03:41 -0700",Re: failure notice,dev@spark.apache.org,"if RDDs from same DStream not guaranteed to run on same worker, then the
question becomes:

is it possible to specify an unlimited duration in ssc to have a continuous
stream (as opposed to discretized).

say, we have a per node streaming engine (built-in checkpoint and recovery)
we'd like to integrate with spark streaming. can we have a never-ending
batch (or RDD) this way?


"
Tathagata Das <tdas@databricks.com>,"Mon, 5 Oct 2015 15:11:44 -0700",Re: failure notice,Renyi Xiong <renyixiong0@gmail.com>,"What happens when a whole node running  your "" per node streaming engine
(built-in checkpoint and recovery)"" fails? Can its checkpoint and recovery
mechanism handle whole node failure? Can you recover from the checkpoint on
a different node?

Spark and Spark Streaming were designed with the idea that executors are
disposable, and there should not be any node-specific long term state that
you rely on unless you can recover that state on a different node.


"
Jegan <jegansp@gmail.com>,"Mon, 5 Oct 2015 15:31:52 -0700",IllegalArgumentException: Size exceeds Integer.MAX_VALUE,dev@spark.apache.org,"Hi All,

I am facing the below exception when the size of the file being read in a
partition is above 2GB. This is apparently because Java's limitation on
memory mapped files. It supports mapping only 2GB files.

Caused by: java.lang.IllegalArgumentException: Size exceeds
Integer.MAX_VALUE
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836)
    at
org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125)
    at
org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1207)
    at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127)
    at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134)
    at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:102)
    at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:791)
    at
org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
    at
org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:153)
    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:262)
    at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
    at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

My use case is to read the files from S3 and do some processing. I am
caching the data like below in order to avoid SocketTimeoutExceptions from
another library I am using for the processing.

val rdd1 = sc.textFile(""*******"").coalesce(1000)
rdd1.persist(DISK_ONLY_2) // replication factor 2
rdd1.foreachPartition { iter => } // one pass over the data to download

The 3rd line fails with the above error when a partition contains a file of
size more than 2GB file.

wrapper class (something called BigByteBuffer) which keeps an array of
ByteBuffers and keeps the index of the current buffer being read etc. Below
is the modified DiskStore.scala.

private def getBytes(file: File, offset: Long, length: Long):
Option[ByteBuffer] = {
  val channel = new RandomAccessFile(file, ""r"").getChannel
  Utils.tryWithSafeFinally {
    // For small files, directly read rather than memory map
    if (length < minMemoryMapBytes) {
      // Map small file in Memory
    } else {
      // TODO Create a BigByteBuffer

    }
  } {
    channel.close()
  }
}

class BigByteBuffer extends ByteBuffer {
  val buffers: Array[ByteBuffer]
  var currentIndex = 0

  ... // Other methods
}

Please let me know if there is any other work-around for the same.
Thanks for your time.

Regards,
Jegan
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 5 Oct 2015 15:53:07 -0700",Re: IllegalArgumentException: Size exceeds Integer.MAX_VALUE,Jegan <jegansp@gmail.com>,"As a workaround, can you set the number of partitions higher in the
sc.textFile method ?

Cheers


"
Davies Liu <davies@databricks.com>,"Mon, 5 Oct 2015 15:58:42 -0700","Re: StructType has more rows, than corresponding Row has objects.",Eugene Morozov <evgeny.a.morozov@gmail.com>,"Could you tell us a way to reproduce this failure? Reading from JSON or Parquet?


---------------------------------------------------------------------


"
Russell Spitzer <russell.spitzer@gmail.com>,"Mon, 05 Oct 2015 23:31:46 +0000",Re: Dataframes: PrunedFilteredScan without Spark Side Filtering,Michael Armbrust <michael@databricks.com>,"That sounds fine to me, we already do the filtering so populating that
field would be pretty simple.


"
Jegan <jegansp@gmail.com>,"Mon, 5 Oct 2015 16:37:10 -0700",Re: IllegalArgumentException: Size exceeds Integer.MAX_VALUE,Ted Yu <yuzhihong@gmail.com>,"Thanks for your suggestion Ted.

Unfortunately at this point of time I cannot go beyond 1000 partitions. I
am writing this data to BigQuery and it has a limit of 1000 jobs per day
for a table(they have some limits on this)  I currently create 1 load job
per partition. Is there any other work-around?

Thanks again.

Regards,
Jegan


"
Reynold Xin <rxin@databricks.com>,"Mon, 5 Oct 2015 16:42:39 -0700",Re: IllegalArgumentException: Size exceeds Integer.MAX_VALUE,Jegan <jegansp@gmail.com>,"You can write the data to local hdfs (or local disk) and just load it from
there.



"
weoccc <weoccc@gmail.com>,"Mon, 5 Oct 2015 16:52:22 -0700",Re: spark hive branch location,Michael Armbrust <michael@databricks.com>,"Hi Michael,

Thanks for pointing me the branch. What's the build instructions to build
the hive 1.2.1 release branch for spark 1.5 ?

Weide


"
Jegan <jegansp@gmail.com>,"Mon, 5 Oct 2015 16:52:28 -0700",Re: IllegalArgumentException: Size exceeds Integer.MAX_VALUE,Reynold Xin <rxin@databricks.com>,"I am sorry, I didn't understand it completely. Are you suggesting to copy
the files from S3 to HDFS? Actually, that is what I am doing. I am reading
the files using Spark and persisting it locally.

Or did you actually mean to ask the producer to write the files directly to
HDFS instead of S3? I am not sure I can do this now either.

Please clarify me if I misunderstood what you meant.

Thanks,
Jegan


"
Reynold Xin <rxin@databricks.com>,"Mon, 5 Oct 2015 16:54:41 -0700",Re: IllegalArgumentException: Size exceeds Integer.MAX_VALUE,Jegan <jegansp@gmail.com>,"I meant to say just copy everything to a local hdfs, and then don't use
caching ...



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 5 Oct 2015 22:13:28 -0700",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Nicholas Chammas <nicholas.chammas@gmail.com>,"The missing artifacts are uploaded now. Things should propagate in the next
24 hours. If there are still issues past then ping this thread. Thanks!

- Patrick

m

for
tgz
adoop1.tgz).
/
s
e
"
Ratika Prasad <rprasad@couponsinc.com>,"Tue, 6 Oct 2015 06:01:32 +0000",FW: Spark error while running in spark mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","

From: Ratika Prasad
Sent: Monday, October 05, 2015 2:39 PM
To: user@spark.apache.org
Cc: Ameeta Jayarajan <AJayarajan@couponsinc.com>
Subject: Spark error while running in spark mode

Hi,

When we run our spark component in cluster mode as below we get the following error

./bin/spark-submit --class com.coupons.stream.processing.SparkStreamEventProcessingEngine --master spark://172.28.161.138:7077 EventProcessingEngine-0.0.1-SNAPSHOT-jar-with-dependencies.jar

ERROR ErrorMonitor: dropping message [class akka.actor.ActorSelectionMessage] for non-local recipient [Actor[akka.tcp://sparkMaster@172.28.161.138:7077/]] arriving at [akka.tcp://sparkMaster@172.28.161.138:7077] inbound addresses are [akka.tcp://sparkDriver@172.28.161.138:7077]
akka.event.Logging$Error$NoCause$


Kindly help





"
jatinganhotra <jatin.ganhotra@gmail.com>,"Mon, 5 Oct 2015 23:58:23 -0700 (MST)",How can I access data on RDDs?,dev@spark.apache.org,"Consider the following 2 scenarios:

*Scenario #1*
val pagecounts = sc.textFile(""data/pagecounts"")
pagecounts.checkpoint
pagecounts.count

*Scenario #2*
val pagecounts = sc.textFile(""data/pagecounts"")
pagecounts.count

The total time show in the Spark shell Application UI was different for both
scenarios. /Scenario #1 took 0.5 seconds, while scenario #2 took only 0.2
s/.

*Questions:*
1. I understand that scenario #1 is taking more time, because the RDD is
check-pointed (written to disk). Is there a way I can know the time taken
for checkpoint, from the total time?  

The Spark shell Application UI shows the following - Scheduler delay, Task
Deserialization time, GC time, Result serialization time, getting result
time. But, doesn't show the breakdown for checkpointing.  

2. Is there a way to access the above metrics e.g. scheduler delay, GC time
and save them programmatically? I want to log some of the above metrics for
every action invoked on an RDD.  

3. How can I programmatically access the following information:  
- Size of an RDD, when persisted to disk on checkpointing?  
- How much percentage of an RDD is in memory currently?  
- Overall time taken for computing an RDD?  

Please let me know if you need more information.



--

---------------------------------------------------------------------


"
=?UTF-8?B?Qmxhxb4gxaBudWRlcmw=?= <snuderl@gmail.com>,"Tue, 6 Oct 2015 09:02:41 +0200",Pyspark dataframe read,Spark dev list <dev@spark.apache.org>,"Hello everyone.

It seems pyspark dataframe read is broken for reading multiple files.

sql.read.json( ""file1,file2"") fails with java.io.IOException: No input
paths specified in job.

This used to work in spark 1.4 and also still work with sc.textFile

Blaž
"
Koert Kuipers <koert@tresata.com>,"Tue, 6 Oct 2015 04:21:00 -0400",Re: Pyspark dataframe read,=?UTF-8?B?Qmxhxb4gxaBudWRlcmw=?= <snuderl@gmail.com>,"i ran into the same thing in scala api. we depend heavily on comma
separated paths, and it no longer works.



"
Josh Rosen <rosenville@gmail.com>,"Tue, 6 Oct 2015 01:30:39 -0700",Re: Pyspark dataframe read,Koert Kuipers <koert@tresata.com>,"Could someone please file a JIRA to track this?
https://issues.apache.org/jira/browse/SPARK


"
Reynold Xin <rxin@databricks.com>,"Tue, 6 Oct 2015 01:31:55 -0700",Re: Pyspark dataframe read,Josh Rosen <rosenville@gmail.com>,"I think the problem is that comma is actually a legitimate character for
file name, and as a result ...


m
"
Koert Kuipers <koert@tresata.com>,"Tue, 6 Oct 2015 04:55:33 -0400",Re: Pyspark dataframe read,Reynold Xin <rxin@databricks.com>,"i personally find the comma separated paths feature much more important
than commas in paths (which one could argue you should avoid).

but assuming people want to keep commas as legitimate characters in paths:
https://issues.apache.org/jira/browse/SPARK-10185
https://github.com/apache/spark/pull/8416




"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Tue, 6 Oct 2015 11:58:44 +0300","Re: StructType has more rows, than corresponding Row has objects.",Davies Liu <davies@databricks.com>,"Davies,

that seemed to be my issue, my colleague helped me to resolved it. The
problem was that we build RDD<Row> and corresponding StructType by
ourselves (no json, parquet, cassandra, etc - we take a list of business
objects and convert them to Rows, then infer struct type) and I missed one
thing.
--
Be well!
Jean Morozov


"
Renyi Xiong <renyixiong0@gmail.com>,"Tue, 6 Oct 2015 08:27:35 -0700",Re: failure notice,Tathagata Das <tdas@databricks.com>,"yes, it can recover on a different node. it uses write-ahead-log,
checkpoints offsets of both ingress and egress (e.g. using zookeeper and/or
kafka), replies on the streaming engine's deterministic operations.

by replaying back a certain range of data based on checkpointed
ingress offset (at least once semantic), state can be recovered, and
filters out duplicate events based on checkpointed egress offset (at most
once semantic)

hope it makes sense.


"
Hossein <falaki@gmail.com>,"Tue, 6 Oct 2015 09:38:39 -0700",Re: SparkR dataframe UDF,Renyi Xiong <renyixiong0@gmail.com>,"User defined functions written in R are not supposed yet. You can implement
your UDF in Scala, register it in sqlContext and use it in SparkR, provided
that you share your context between R and Scala.

--Hossein




-- 
--Hossein
"
Yogesh Mahajan <mahajan.yogesh@gmail.com>,"Tue, 6 Oct 2015 22:29:40 +0530",Re: CQs on WindowedStream created on running StreamingContext,"""dev@spark.apache.org"" <dev@spark.apache.org>","Anyone knows about this ? TD ?

-yogesh

. The queries could be registered/deregistered dynamically or can be submitted through command line. Currently Spark streaming doesn’t allow adding any new inputs, transformations, and output operations after starting a StreamingContext. But doing following code changes in DStream.scala allows me to create an window on DStream even after StreamingContext has started (in StreamingContextState.ACTIVE). 
arting a streaming context
tarted
RDDsInWindow.
m, and evaluate queries on those DataFrames. 
 Runtime and memory management?
ontextState.ACTIVE state?
his case?

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 6 Oct 2015 15:12:00 -0700",Adding Spark Testing functionality,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark Devs,

So this has been brought up a few times before, and generally on the user
list people get directed to use spark-testing-base. I'd like to start
moving some of spark-testing-base's functionality into Spark so that people
don't need a library to do what is (hopefully :p) a very common requirement
across all Spark projects.

To that end I was wondering what peoples thoughts are on where this should
live inside of Spark. I was thinking it could either be a separate testing
project (like sql or similar), or just put the bits to enable testing
inside of each relevant project.

I was also thinking it probably makes sense to only move the unit testing
parts at the start and leave things like integration testing in a testing
project since that could vary depending on the users environment.

What are peoples thoughts?

Cheers,

Holden :)
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 6 Oct 2015 15:44:12 -0700",Re: Adding Spark Testing functionality,Holden Karau <holden@pigscanfly.ca>,"Hey Holden,

It would be helpful if you could outline the set of features you'd imagine
being part of Spark in a short doc. I didn't see a README on the existing
repo, so it's hard to know exactly what is being proposed.

As a general point of process, we've typically avoided merging modules into
Spark that can exist outside of the project. A testing utility package that
is based on Spark's public API's seems like a really useful thing for the
community, but it does seem like a good fit for a package library. At
least, this is my first question after taking a look at the project.

In any case, getting some high level view of the functionality you imagine
would be helpful to give more detailed feedback.

- Patrick


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 6 Oct 2015 15:49:11 -0700",Re: Adding Spark Testing functionality,Patrick Wendell <pwendell@gmail.com>,"I'll put together a google doc and send that out (in the meantime a quick
guide of sort of how the current package can be used is in the blog post I
did at
http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/
)  If people think its better to keep as a package I am of course happy to
keep doing that. It feels a little strange to have something as core as
being able to test your code live outside.




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Tue, 6 Oct 2015 17:51:17 -0700",multiple count distinct in SQL/DataFrame?,"""dev@spark.apache.org"" <dev@spark.apache.org>","The current implementation of multiple count distinct in a single query is
very inferior in terms of performance and robustness, and it is also hard
to guarantee correctness of the implementation in some of the refactorings
for Tungsten. Supporting a better version of it is possible in the future,
but will take a lot of engineering efforts. Most other Hadoop-based SQL
systems (e.g. Hive, Impala) don't support this feature.

As a result, we are considering removing support for multiple count
distinct in a single query in the next Spark release (1.6). If you use this
feature, please reply to this email. Thanks.

Note that if you don't care about null values, it is relatively easy to
reconstruct a query using joins to support multiple distincts.
"
Reynold Xin <rxin@databricks.com>,"Tue, 6 Oct 2015 17:54:01 -0700",Re: multiple count distinct in SQL/DataFrame?,"""dev@spark.apache.org"" <dev@spark.apache.org>","To provide more context, if we do remove this feature, the following SQL
query would throw an AnalysisException:

select count(distinct colA), count(distinct colB) from foo;

The following should still work:

select count(distinct colA) from foo;

The following should also work:

select count(distinct colA, colB) from foo;



"
Tathagata Das <tdas@databricks.com>,"Tue, 6 Oct 2015 19:38:11 -0700",Re: failure notice,Renyi Xiong <renyixiong0@gmail.com>,"Unfortunately, there is not an obvious way to do this. I am guessing that
you want to partition your stream such that the same keys always go to the
same executor, right?

You could do it by writing a custom RDD. See ShuffledRDD
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala>.
That is what is used to do a lot of shuffling. See how it is used from
RDD.partitionByKey() or RDD.reduceByKey(). You could subclass it specify a
set of preferred locations, and the system will try to respect those
locations. These locations should be among the currently active executors.
You could either get the current list of executors from
SparkContext.getExecutorMemoryStatus(),

Hope this helps.


"
YiZhi Liu <javelinjs@gmail.com>,"Wed, 7 Oct 2015 14:47:57 +0800","What is the difference between ml.classification.LogisticRegression
 and mllib.classification.LogisticRegressionWithLBFGS","dev@spark.apache.org, user <user@spark.apache.org>","Hi everyone,

I'm curious about the difference between
ml.classification.LogisticRegression and
mllib.classification.LogisticRegressionWithLBFGS. Both of them are
optimized using LBFGS, the only difference I see is LogisticRegression
takes DataFrame while LogisticRegressionWithLBFGS takes RDD.

So I wonder,
1. Why not simply add a DataFrame training interface to
LogisticRegressionWithLBFGS?
2. Whats the difference between ml.classification and
mllib.classification package?
3. Why doesn't ml.classification.LogisticRegression call
mllib.optimization.LBFGS / mllib.optimization.OWLQN directly? Instead,
it uses breeze.optimize.LBFGS and re-implements most of the procedures
in mllib.optimization.{LBFGS,OWLQN}.

Thank you.

Best,

-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
Russell Spitzer <russell.spitzer@gmail.com>,"Wed, 07 Oct 2015 16:49:09 +0000",Re: Dataframes: PrunedFilteredScan without Spark Side Filtering,Michael Armbrust <michael@databricks.com>,"Should I make up a new ticket for this? Or is there something already
underway?


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 07 Oct 2015 16:59:33 +0000",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Patrick Wendell <pwendell@gmail.com>,"Thanks guys.

Regarding this earlier question:

More importantly, is there some rough specification for what packages we
should be able to expect in this S3 bucket with every release?

Is the implied answer that we should continue to expect the same set of
artifacts for every release for the foreseeable future?

Nick
​


ks!
 for
.tgz
0
hadoop1.tgz).
n/
elease?
"
Michael Armbrust <michael@databricks.com>,"Wed, 7 Oct 2015 10:03:47 -0700",Re: Dataframes: PrunedFilteredScan without Spark Side Filtering,Russell Spitzer <russell.spitzer@gmail.com>,"Please do.


"
Joseph Bradley <joseph@databricks.com>,"Wed, 7 Oct 2015 10:15:32 -0700","Re: What is the difference between ml.classification.LogisticRegression
 and mllib.classification.LogisticRegressionWithLBFGS",YiZhi Liu <javelinjs@gmail.com>,"Hi YiZhi Liu,

The spark.ml classes are part of the higher-level ""Pipelines"" API, which
works with DataFrames.  When creating this API, we decided to separate it
from the old API to avoid confusion.  You can read more about it here:
http://spark.apache.org/docs/latest/ml-guide.html

For (3): We use Breeze, but we have to modify it in order to do distributed
optimization based on Spark.

Joseph


"
<Saif.A.Ellafi@wellsfargo.com>,"Wed, 7 Oct 2015 17:23:20 +0000",Spark standalone hangup during shuffle flatMap or explode in cluster,"<dev@spark.apache.org>, <user@spark.apache.org>","When running stand-alone cluster mode job, the process hangs up randomly during a DataFrame flatMap or explode operation, in HiveContext:

-->> df.flatMap(r => for (n <- 1 to r.getInt(ind)) yield r)

This does not happen either with SQLContext in cluster, or Hive/SQL in local mode, where it works fine.

A couple minutes after the hangup, executors start dropping. I am attching the logs

Saif




15/10/07 12:15:19 INFO TaskSetManager: Finished task 50.0 in stage 17.0 (TID 166) in 2511 ms on 162.101.194.47 (180/200)
15/10/07 12:15:19 INFO TaskSetManager: Finished task 66.0 in stage 17.0 (TID 182) in 2510 ms on 162.101.194.47 (181/200)
15/10/07 12:15:19 INFO TaskSetManager: Finished task 110.0 in stage 17.0 (TID 226) in 2505 ms on 162.101.194.47 (182/200)
15/10/07 12:15:19 INFO TaskSetManager: Finished task 74.0 in stage 17.0 (TID 190) in 2530 ms on 162.101.194.47 (183/200)
15/10/07 12:15:19 INFO TaskSetManager: Finished task 106.0 in stage 17.0 (TID 222) in 2530 ms on 162.101.194.47 (184/200)
15/10/07 12:20:01 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 141447 ms exceeds timeout 120000 ms
15/10/07 12:20:01 ERROR TaskSchedulerImpl: Lost executor 2 on 162.101.194.44: Executor heartbeat timed out after 141447 ms
15/10/07 12:20:01 INFO TaskSetManager: Re-queueing tasks for 2 from TaskSet 17.0
15/10/07 12:20:01 WARN TaskSetManager: Lost task 113.0 in stage 17.0 (TID 229, 162.101.194.44): ExecutorLostFailure (executor 2 lost)
15/10/07 12:20:01 WARN TaskSetManager: Lost task 73.0 in stage 17.0 (TID 189, 162.101.194.44): ExecutorLostFailure (executor 2 lost)
15/10/07 12:20:01 WARN TaskSetManager: Lost task 81.0 in stage 17.0 (TID 197, 162.101.194.44): ExecutorLostFailure (executor 2 lost)
15/10/07 12:20:01 INFO TaskSetManager: Starting task 81.1 in stage 17.0 (TID 316, 162.101.194.45, PROCESS_LOCAL, 2045 bytes)
15/10/07 12:20:01 INFO TaskSetManager: Starting task 73.1 in stage 17.0 (TID 317, 162.101.194.44, PROCESS_LOCAL, 2045 bytes)
15/10/07 12:20:01 INFO TaskSetManager: Starting task 113.1 in stage 17.0 (TID 318, 162.101.194.48, PROCESS_LOCAL, 2045 bytes)
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Requesting to kill executor(s) 2
15/10/07 12:20:01 INFO DAGScheduler: Executor lost: 2 (epoch 4)
15/10/07 12:20:01 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
15/10/07 12:20:01 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 162.101.194.44, 42537)
15/10/07 12:20:01 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
15/10/07 12:20:01 INFO ShuffleMapStage: ShuffleMapStage 15 is now unavailable on executor 2 (1/2, false)
15/10/07 12:20:01 INFO ShuffleMapStage: ShuffleMapStage 16 is now unavailable on executor 2 (8/16, false)
15/10/07 12:20:01 INFO DAGScheduler: Host added was in lost list earlier: 162.101.194.44
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/69 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/69 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/69 is now RUNNING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/69 is now LOADING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/69 is now EXITED (Command exited with code 1)
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/69 removed: Command exited with code 1
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 69
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/70 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/70 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/70 is now RUNNING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/70 is now LOADING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/70 is now EXITED (Command exited with code 1)
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/70 removed: Command exited with code 1
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 70
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/71 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/71 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/71 is now LOADING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/71 is now RUNNING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/71 is now EXITED (Command exited with code 1)
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/71 removed: Command exited with code 1
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 71
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/72 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/72 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/72 is now LOADING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/72 is now RUNNING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/72 is now EXITED (Command exited with code 1)
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/72 removed: Command exited with code 1
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 72
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/73 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/73 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/73 is now LOADING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/73 is now RUNNING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/73 is now EXITED (Command exited with code 1)
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/73 removed: Command exited with code 1
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 73
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/74 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:01 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/74 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/74 is now LOADING
15/10/07 12:20:01 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/74 is now RUNNING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/74 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/74 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 74
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/75 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/75 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/75 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/75 is now RUNNING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/75 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/75 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 75
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/76 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/76 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/76 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/76 is now RUNNING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/76 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/76 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 76
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/77 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/77 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/77 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/77 is now RUNNING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/77 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/77 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 77
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/78 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/78 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/78 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/78 is now RUNNING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/78 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/78 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 78
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/79 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/79 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/79 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/79 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/79 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 79
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/80 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/80 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/80 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/80 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/80 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 80
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/81 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/81 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/81 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/81 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/81 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 81
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/82 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/82 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/82 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/82 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/82 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 82
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/83 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/83 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/83 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/83 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/83 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 83
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/84 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/84 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/84 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/84 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/84 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 84
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/85 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/85 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/85 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/85 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/85 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 85
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/86 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/86 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/86 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/86 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/86 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 86
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor added: app-20151007121501-0022/87 on worker-20151007063932-162.101.194.44-57091 (162.101.194.44:57091) with 32 cores
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20151007121501-0022/87 on hostPort 162.101.194.44:57091 with 32 cores, 100.0 GB RAM
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/87 is now LOADING
15/10/07 12:20:02 INFO AppClient$ClientEndpoint: Executor updated: app-20151007121501-0022/87 is now EXITED (Command exited with code 1)
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Executor app-20151007121501-0022/87 removed: Command exited with code 1
15/10/07 12:20:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 87
15/10/07 12:20:17 INFO BlockManagerMasterEndpoint: Registering block manager 162.101.194.44:42537 with 51.8 GB RAM, BlockManagerId(2, 162.101.194.44, 42537)
15/10/07 12:20:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 162.101.194.44:42537 (size: 10.0 KB, free: 51.8 GB)
15/10/07 12:20:18 ERROR TaskSchedulerImpl: Lost executor 2 on 162.101.194.44: remote Rpc client disassociated
15/10/07 12:20:18 INFO TaskSetManager: Re-queueing tasks for 2 from TaskSet 17.0
15/10/07 12:20:18 WARN TaskSetManager: Lost task 73.1 in stage 17.0 (TID 317, 162.101.194.44): ExecutorLostFailure (executor 2 lost)
15/10/07 12:20:18 INFO DAGScheduler: Executor lost: 2 (epoch 6)
15/10/07 12:20:18 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
15/10/07 12:20:18 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 162.101.194.44, 42537)
15/10/07 12:20:18 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
15/10/07 12:20:18 INFO TaskSetManager: Starting task 73.2 in stage 17.0 (TID 319, 162.101.194.48, PROCESS_LOCAL, 2045 bytes)
15/10/07 12:20:18 WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkExecutor@162.101.194.44:43010] has failed, address is now gated for [5000] ms. Reason: [Disassociated]
15/10/07 12:20:34 INFO CoarseGrainedExecutorBackend: Got assigned task 316
15/10/07 12:20:34 INFO Executor: Running task 81.1 in stage 17.0 (TID 316)
15/10/07 12:20:34 INFO ShuffleBlockFetcherIterator: Getting 16 non-empty blocks out of 16 blocks
15/10/07 12:20:34 INFO TransportClientFactory: Found inactive connection to /162.101.194.44:42537, creating a new one.
15/10/07 12:20:34 ERROR RetryingBlockFetcher: Exception while beginning fetch of 8 outstanding blocks
java.io.IOException: Failed to connect to /162.101.194.44:42537
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
        at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
        at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:97)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:152)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:265)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:112)
        at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:43)
        at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:71)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:88)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: /162.101.194.44:42537
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        ... 1 more
15/10/07 12:20:34 INFO RetryingBlockFetcher: Retrying fetch (1/3) for 8 outstanding blocks after 5000 ms
15/10/07 12:20:34 INFO ShuffleBlockFetcherIterator: Started 1 remote fetches in 15 ms

---------------------------------------------------------------------"
Patrick Wendell <pwendell@gmail.com>,"Wed, 7 Oct 2015 10:57:06 -0700",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Nicholas Chammas <nicholas.chammas@gmail.com>,"I don't think we have a firm contract around that. So far we've never
removed old artifacts, but the ASF has asked us at time to decrease the
size of binaries we post. In the future at some point we may drop older
ones since we keep adding new ones.

If downstream projects are depending on our artifacts, I'd say just hold
tight for now until something changes. If it changes, then those projects
might need to build Spark on their own and host older hadoop versions, etc.

m

:
nks!
e
s for
1.tgz
-
in/
release?
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 07 Oct 2015 18:10:30 +0000",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Patrick Wendell <pwendell@gmail.com>,"Sounds good to me.

For my purposes, I'm less concerned about old Spark artifacts and more
concerned about the consistency of the set of artifacts that get generated
with new releases. (e.g. Each new release will always include one artifact
each for Hadoop 1, Hadoop 1 + Scala 2.11, etc...)

It sounds like we can expect that set to stay the same with new releases
for now, but it's not a hard guarantee. I think that's fine for now.

Nick


c.
anks!
he
As for
p1.tgz
.com>
op1.tgz
bin/
.
 release?
"
Sean Owen <sowen@cloudera.com>,"Wed, 7 Oct 2015 19:13:11 +0100",Re: Spark 1.5.1 - Scala 2.10 - Hadoop 1 package is missing from S3,Patrick Wendell <pwendell@gmail.com>,"This is about the s3.amazonaws.com files, not dist.apache.org right?
or does it affect both?

(BTW you can keep as many old release artifacts around on the
apache.org archives as you like; I think the suggestion is to remove
all but the most recent releases from the set that's replicated to all
the Apache mirrors.)

ize
c.
e:
anks!
he
As for
p1.tgz
op1.tgz
bin/
.
rate thing.
 release?

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 7 Oct 2015 13:43:35 -0700",Fwd: multiple count distinct in SQL/DataFrame?,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Adding user list too.



---------- Forwarded message ----------
From: Reynold Xin <rxin@databricks.com>
Date: Tue, Oct 6, 2015 at 5:54 PM
Subject: Re: multiple count distinct in SQL/DataFrame?
To: ""dev@spark.apache.org"" <dev@spark.apache.org>


To provide more context, if we do remove this feature, the following SQL
query would throw an AnalysisException:

select count(distinct colA), count(distinct colB) from foo;

The following should still work:

select count(distinct colA) from foo;

The following should also work:

select count(distinct colA, colB) from foo;



"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Wed, 7 Oct 2015 22:56:10 +0200",Re: multiple count distinct in SQL/DataFrame?,Reynold Xin <rxin@databricks.com>,"We could also fallback to approximate count distincts when the user
requests multiple count distincts. This is less invasive than throwing an
AnalysisException, but it could violate the principle of least surprise.



Met vriendelijke groet/Kind regards,

Herman van Hövell tot Westerflier

QuestTec B.V.
Torenwacht 98
2353 DC Leiderdorp
hvanhovell@questtec.nl
+599 9 521 4402


2015-10-07 22:43 GMT+02:00 Reynold Xin <rxin@databricks.com>:

 in
his
"
Mayank Pradhan <mayank@platfora.com>,"Wed, 7 Oct 2015 14:19:18 -0700",Re: multiple count distinct in SQL/DataFrame?,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Is this limited only to grand multiple count distincts or does it extends
to all kinds of multiple count distincts? More precisely would the
following multiple count distinct query also be affected?
select a, b, count(distinct x), count(distinct y) from foo group by a,b;

It would be unfortunate to loose that too.

-Mayank


e in
.
this
"
Arijit <arijitt@live.com>,"Wed, 7 Oct 2015 17:47:14 -0600","=?windows-1256?Q?Understand?= =?windows-1256?Q?ing_code/c?=
 =?windows-1256?Q?losure_shi?= =?windows-1256?Q?pment_to_S?=
 =?windows-1256?Q?park_worke?= =?windows-1256?Q?rs=FE?=","""user@spark.apache.org"" <dev@spark.apache.org>"," Hi,
 
I want to understand the code flow starting from the Spark jar that I submit through spark-submit, how does Spark identify and extract the closures, clean and serialize them and ship them to workers to execute as tasks. Can someone point me to any documentation or a pointer to the source code path to help me understand this.
 
Thanks, Arijit 		 	   		  "
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 8 Oct 2015 00:20:04 +0000",RE: GraphX PageRank keeps 3 copies of graph in memory,Ankur Dave <ankurdave@gmail.com>,"Hi Ankur,

Could you help with explanation of the problem below?

Best regards, Alexander

From: Ulanov, Alexander
Sent: Friday, October 02, 2015 11:39 AM
To: 'Robin East'
Cc: dev@spark.apache.org
Subject: RE: GraphX PageRank keeps 3 copies of graph in memory

Hi Robin,

Sounds interesting. I am running 1.5.0. Could you copy-paste your Storage tab?

I’ve just double checked on another cluster with 1 master and 5 workers. It still has 3 pairs of VertexRDD and EdgeRDD at the end of benchmark’s execution:

RDD Name          Storage Level     Cached Partitions             Fraction Cached                Size in Memory Size in ExternalBlockStore          Size on Disk
VertexRDD         Memory Deserialized 1x Replicated         3              150%     6.9 MB  0.0 B      0.0 B
EdgeRDD             Memory Deserialized 1x Replicated         2              100%     155.5 MB             0.0 B      0.0 B
EdgeRDD             Memory Deserialized 1x Replicated         2              100%     154.7 MB             0.0 B      0.0 B
VertexRDD, VertexRDD Memory Deserialized 1x Replicated         3              150%     8.4 MB  0.0 B      0.0 B
EdgeRDD             Memory Deserialized 1x Replicated         2              100%     202.9 MB             0.0 B      0.0 B
VertexRDD         Memory Deserialized 1x Replicated         2              100%     5.6 MB  0.0 B      0.0 B

During the execution I observe that one pair is added and removed from the list. This should correspond to the unpersist statements in the code.

Also, according to the code, you one should end up with 1 set of RDDs, because of unpersist statements in the end of the loop. Does it make sense to you?

Best regards, Alexander

From: Robin East [mailto:robin.east@xense.co.uk]
Sent: Friday, October 02, 2015 12:27 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: GraphX PageRank keeps 3 copies of graph in memory

Alexander,

I’ve just run the benchmark and only end up with 2 sets of RDDs in the Storage tab. This is on 1.5.0, what version are you using?

Robin
-------------------------------------------------------------------------------
Robin East
Spark GraphX in Action Michael Malak and Robin East
Manning Publications Co.
http://www.manning.com/books/spark-graphx-in-action




On 30 Sep 2015, at 23:55, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:

Dear Spark developers,

I would like to understand GraphX caching behavior with regards to PageRank in Spark, in particular, the following implementation of PageRank:
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/PageRank.scala

On each iteration the new graph is created and cached, and the old graph is un-cached:
1) Create new graph and cache it:
rankGraph = rankGraph.joinVertices(rankUpdates) {
        (id, oldRank, msgSum) => rPrb(src, id) + (1.0 - resetProb) * msgSum
      }.cache()
2) Unpersist the old one:
      prevRankGraph.vertices.unpersist(false)
      prevRankGraph.edges.unpersist(false)

According to the code, at the end of each iteration only one graph should be in memory, i.e. one EdgeRDD and one VertexRDD. During the iteration, exactly between the mentioned lines of code, there will be two graphs: old and new. It is two pairs of Edge and Vertex RDDs. However, when I run the example provided in Spark examples folder, I observe the different behavior.

Run the example (I checked that it runs the mentioned code):
$SPARK_HOME/bin/spark-submit --class ""org.apache.spark.examples.graphx.SynthBenchmark""  --master spark://mynode.net:7077 $SPARK_HOME/examples/target/spark-examples.jar

According to “Storage” and RDD DAG in Spark UI, 3 VertexRDDs and 3 EdgeRDDs are cached, even when all iterations are finished, given that the mentioned code suggests caching at most 2 (and only in particular stage of the iteration):
https://drive.google.com/file/d/0BzYMzvDiCep5WFpnQjFzNy0zYlU/view?usp=sharing
Edges (the green ones are cached):
https://drive.google.com/file/d/0BzYMzvDiCep5S2JtYnhVTlV1Sms/view?usp=sharing
Vertices (the green ones are cached):
https://drive.google.com/file/d/0BzYMzvDiCep5S1k4N2NFb05RZDA/view?usp=sharing

Could you explain, why 3 VertexRDDs and 3 EdgeRDDs are cached?

Is it OK that there is a double caching in code, given that joinVertices implicitly caches vertices and then the graph is cached in the PageRank code?

Best regards, Alexander

"
Lloyd Haris <lloydharis@gmail.com>,"Thu, 8 Oct 2015 12:16:43 +1100",SparkSQL: First query execution is always slower than subsequent queries,Spark dev list <dev@spark.apache.org>,"Hi Spark Devs,

I am doing a performance evaluation of Spark using pyspark. I am using
Spark 1.5 with a Hadoop 2.6 cluster of 4 nodes and ran these tests on local
mode.

After a few dozen test executions, it turned out that the very first
SparkSQL query execution is always slower than the subsequent executions of
the same query.

First run:

15/10/08 11:15:35 INFO ParseDriver: Parsing command: SELECT CATAID, RA, DEC
FROM InputCatA
15/10/08 11:15:36 INFO ParseDriver: Parse Completed
15/10/08 11:15:36 INFO MemoryStore: ensureFreeSpace(484576) called with
curMem=0, maxMem=556038881
15/10/08 11:15:36 INFO MemoryStore: Block broadcast_0 stored as values in
memory (estimated size 473.2 KB, free 529.8 MB)
15/10/08 11:15:37 INFO MemoryStore: ensureFreeSpace(45559) called with
curMem=484576, maxMem=556038881
15/10/08 11:15:37 INFO MemoryStore: Block broadcast_0_piece0 stored as
bytes in memory (estimated size 44.5 KB, free 529.8 MB)
15/10/08 11:15:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory
on localhost:59594 (size: 44.5 KB, free: 530.2 MB)
15/10/08 11:15:37 INFO SparkContext: Created broadcast 0 from collect at
<stdin>:3
15/10/08 11:15:37 INFO FileInputFormat: Total input paths to process : 4
15/10/08 11:15:37 INFO SparkContext: Starting job: collect at <stdin>:3
15/10/08 11:15:37 INFO DAGScheduler: Got job 0 (collect at <stdin>:3) with
4 output partitions
15/10/08 11:15:37 INFO DAGScheduler: Final stage: ResultStage 0(collect at
<stdin>:3)
15/10/08 11:15:37 INFO DAGScheduler: Parents of final stage: List()
15/10/08 11:15:37 INFO DAGScheduler: Missing parents: List()
15/10/08 11:15:37 INFO DAGScheduler: Submitting ResultStage 0
(MapPartitionsRDD[4] at collect at <stdin>:3), which has no missing parents
15/10/08 11:15:37 INFO MemoryStore: ensureFreeSpace(8896) called with
curMem=530135, maxMem=556038881
15/10/08 11:15:37 INFO MemoryStore: Block broadcast_1 stored as values in
memory (estimated size 8.7 KB, free 529.8 MB)
15/10/08 11:15:37 INFO MemoryStore: ensureFreeSpace(4679) called with
curMem=539031, maxMem=556038881
15/10/08 11:15:37 INFO MemoryStore: Block broadcast_1_piece0 stored as
bytes in memory (estimated size 4.6 KB, free 529.8 MB)
15/10/08 11:15:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory
on localhost:59594 (size: 4.6 KB, free: 530.2 MB)
15/10/08 11:15:37 INFO SparkContext: Created broadcast 1 from broadcast at
DAGScheduler.scala:861
15/10/08 11:15:37 INFO DAGScheduler: Submitting 4 missing tasks from
ResultStage 0 (MapPartitionsRDD[4] at collect at <stdin>:3)
15/10/08 11:15:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
15/10/08 11:15:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID
0, localhost, ANY, 2184 bytes)
15/10/08 11:15:37 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID
1, localhost, ANY, 2184 bytes)
15/10/08 11:15:37 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID
2, localhost, ANY, 2184 bytes)
15/10/08 11:15:37 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID
3, localhost, ANY, 2184 bytes)
15/10/08 11:15:37 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/10/08 11:15:37 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/10/08 11:15:37 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/10/08 11:15:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/08 11:15:37 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00003:0+55353265
15/10/08 11:15:37 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00000:0+55149172
15/10/08 11:15:37 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00002:0+55418752
15/10/08 11:15:37 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00001:0+55083905
15/10/08 11:15:37 INFO deprecation: mapred.tip.id is deprecated. Instead,
use mapreduce.task.id
15/10/08 11:15:37 INFO deprecation: mapred.task.id is deprecated. Instead,
use mapreduce.task.attempt.id
15/10/08 11:15:37 INFO deprecation: mapred.task.is.map is deprecated.
Instead, use mapreduce.task.ismap
15/10/08 11:15:37 INFO deprecation: mapred.task.partition is deprecated.
Instead, use mapreduce.task.partition
15/10/08 11:15:37 INFO deprecation: mapred.job.id is deprecated. Instead,
use mapreduce.job.id
15/10/08 11:15:39 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1).
8919569 bytes result sent to driver
15/10/08 11:15:39 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3).
8919548 bytes result sent to driver
15/10/08 11:15:39 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2).
8926124 bytes result sent to driver
15/10/08 11:15:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID
1) in 2186 ms on localhost (1/4)
15/10/08 11:15:39 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID
3) in 2203 ms on localhost (2/4)
15/10/08 11:15:39 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID
2) in 2208 ms on localhost (3/4)
15/10/08 11:15:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0).
8794038 bytes result sent to driver
15/10/08 11:15:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID
0) in 2395 ms on localhost (4/4)
15/10/08 11:15:39 INFO DAGScheduler: ResultStage 0 (collect at <stdin>:3)
finished in 2.403 s
15/10/08 11:15:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks
have all completed, from pool
15/10/08 11:15:40 INFO DAGScheduler: Job 0 finished: collect at <stdin>:3,
took 2.468262 s
query executed
8.088552951812744

Second run:

15/10/08 11:16:39 INFO ParseDriver: Parsing command: SELECT CATAID, RA, DEC
FROM InputCatA
15/10/08 11:16:39 INFO ParseDriver: Parse Completed
15/10/08 11:16:40 INFO MemoryStore: ensureFreeSpace(484576) called with
curMem=543710, maxMem=556038881
15/10/08 11:16:40 INFO MemoryStore: Block broadcast_2 stored as values in
memory (estimated size 473.2 KB, free 529.3 MB)
15/10/08 11:16:40 INFO MemoryStore: ensureFreeSpace(45559) called with
curMem=1028286, maxMem=556038881
15/10/08 11:16:40 INFO MemoryStore: Block broadcast_2_piece0 stored as
bytes in memory (estimated size 44.5 KB, free 529.3 MB)
15/10/08 11:16:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory
on localhost:59594 (size: 44.5 KB, free: 530.2 MB)
15/10/08 11:16:40 INFO SparkContext: Created broadcast 2 from collect at
<stdin>:3
15/10/08 11:16:40 INFO FileInputFormat: Total input paths to process : 4
15/10/08 11:16:40 INFO SparkContext: Starting job: collect at <stdin>:3
15/10/08 11:16:40 INFO DAGScheduler: Got job 1 (collect at <stdin>:3) with
4 output partitions
15/10/08 11:16:40 INFO DAGScheduler: Final stage: ResultStage 1(collect at
<stdin>:3)
15/10/08 11:16:40 INFO DAGScheduler: Parents of final stage: List()
15/10/08 11:16:40 INFO DAGScheduler: Missing parents: List()
15/10/08 11:16:40 INFO DAGScheduler: Submitting ResultStage 1
(MapPartitionsRDD[9] at collect at <stdin>:3), which has no missing parents
15/10/08 11:16:40 INFO MemoryStore: ensureFreeSpace(8896) called with
curMem=1073845, maxMem=556038881
15/10/08 11:16:40 INFO MemoryStore: Block broadcast_3 stored as values in
memory (estimated size 8.7 KB, free 529.2 MB)
15/10/08 11:16:40 INFO MemoryStore: ensureFreeSpace(4688) called with
curMem=1082741, maxMem=556038881
15/10/08 11:16:40 INFO MemoryStore: Block broadcast_3_piece0 stored as
bytes in memory (estimated size 4.6 KB, free 529.2 MB)
15/10/08 11:16:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory
on localhost:59594 (size: 4.6 KB, free: 530.2 MB)
15/10/08 11:16:40 INFO SparkContext: Created broadcast 3 from broadcast at
DAGScheduler.scala:861
15/10/08 11:16:40 INFO DAGScheduler: Submitting 4 missing tasks from
ResultStage 1 (MapPartitionsRDD[9] at collect at <stdin>:3)
15/10/08 11:16:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks
15/10/08 11:16:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID
4, localhost, ANY, 2184 bytes)
15/10/08 11:16:40 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID
5, localhost, ANY, 2184 bytes)
15/10/08 11:16:40 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID
6, localhost, ANY, 2184 bytes)
15/10/08 11:16:40 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID
7, localhost, ANY, 2184 bytes)
15/10/08 11:16:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)
15/10/08 11:16:40 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)
15/10/08 11:16:40 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)
15/10/08 11:16:40 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00001:0+55083905
15/10/08 11:16:40 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00000:0+55149172
15/10/08 11:16:40 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)
15/10/08 11:16:40 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00003:0+55353265
15/10/08 11:16:40 INFO HadoopRDD: Input split:
hdfs://<url>:8020/user/hive/warehouse/inputcata/part-m-00002:0+55418752
15/10/08 11:16:41 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5).
8925439 bytes result sent to driver
15/10/08 11:16:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4).
8793850 bytes result sent to driver
15/10/08 11:16:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID
4) in 1238 ms on localhost (1/4)
15/10/08 11:16:41 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID
5) in 1265 ms on localhost (2/4)
15/10/08 11:16:41 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7).
8925361 bytes result sent to driver
15/10/08 11:16:41 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID
7) in 1283 ms on localhost (3/4)
15/10/08 11:16:41 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6).
8926039 bytes result sent to driver
15/10/08 11:16:41 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID
6) in 1419 ms on localhost (4/4)
15/10/08 11:16:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks
have all completed, from pool
15/10/08 11:16:41 INFO DAGScheduler: ResultStage 1 (collect at <stdin>:3)
finished in 1.421 s
15/10/08 11:16:41 INFO DAGScheduler: Job 1 finished: collect at <stdin>:3,
took 1.435883 s
query executed
5.115309000015259

As you can see from the log messages, the major difference I can see is the
highlighted part. Seems to me like(Yes I am newbie to Spark) it is trying
to create some Map/Reduce jobs and it's trying to use MR1 as Opposed to
MR2. Those messages are not printed on any subsequent query execution. If
you ran a different query which involved a completely different table/s
several times, there wouldn't be any significant difference in execution
times among those runs. So the delay is only with the very first run.

So my questions are:

1). Is that the reason why it's always slow in the first run? Or are there
any other reasons? Apparently it loads data to memory every time so it
shouldn't be something to do with disk read should it?

2). Does Spark use the Hadoop's Map Reduce engine under the hood? If so can
we configure it to use MR2 instead of MR1.

I might be completely wrong so if anyone can tell my why this is happening
that will be highly appreciated.

Cheers
Lloyd
"
Michael Armbrust <michael@databricks.com>,"Wed, 7 Oct 2015 18:47:50 -0700",Re: SparkSQL: First query execution is always slower than subsequent queries,Lloyd Haris <lloydharis@gmail.com>,"-dev +user

1). Is that the reason why it's always slow in the first run? Or are there

You are probably seeing the effect of the JVMs JIT.  The first run is
it will compile it to native code.  This applies both to Spark / Spark SQL
itself and (as of Spar"
<Saif.A.Ellafi@wellsfargo.com>,"Thu, 8 Oct 2015 13:22:56 +0000","RowNumber in HiveContext returns null, negative numbers or huge",<dev@spark.apache.org>,"Hi all, would this be a bug??

        val ws = Window.
            partitionBy(""clrty_id"").
            orderBy(""filemonth_dtt"")

        val nm = ""repeatMe""
        df.select(df.col(""*""), rowNumber().over(ws).cast(""int"").as(nm))

        stacked_data.filter(stacked_data(""repeatMe"").isNotNull).orderBy(""repeatMe"").take(50).foreach(println(_))

--->

Long, DateType, Int
[200000000003,2006-06-01,-1863462909]
[200000000003,2006-09-01,-1863462909]
[200000000003,2007-01-01,-1863462909]
[200000000003,2007-08-01,-1863462909]
[200000000003,2007-07-01,-1863462909]
[200000000138,2007-07-01,-1863462774]
[200000000138,2007-02-01,-1863462774]
[200000000138,2006-11-01,-1863462774]
[200000000138,2006-08-01,-1863462774]
[200000000138,2007-08-01,-1863462774]
[200000000138,2006-09-01,-1863462774]
[200000000138,2007-03-01,-1863462774]
[200000000138,2006-10-01,-1863462774]
[200000000138,2007-05-01,-1863462774]
[200000000138,2006-06-01,-1863462774]
[200000000138,2006-12-01,-1863462774]


Thanks,
Saif

"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 8 Oct 2015 15:40:03 +0200",Scala 2.11 builds broken/ Can the PR build run also 2.11?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Since Oct. 4 the build fails on 2.11 with the dreaded

[error] /home/ubuntu/workspace/Apache Spark (master) on
2.11/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala:310:
no valid targets for annotation on value conf - it is discarded
unused. You may specify targets with meta-annotations, e.g.
@(transient @param)
[error] private[netty] class NettyRpcEndpointRef(@transient conf: SparkConf)

Can we have the pull request builder at least build with 2.11? This makes
#8433 <https://github.com/apache/spark/pull/8433> pretty much useless,
since people will continue to add useless @transient annotations.
​
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 8 Oct 2015 06:50:34 -0700",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Interesting

https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/job/Spark-Master-Scala211-Compile/
shows green builds.


com>

n/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala:310: no valid targets for annotation on value conf - it is discarded unused. You may specify targets with meta-annotations, e.g. @(transient @param)
nf)
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 8 Oct 2015 07:55:39 -0700",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"I tried building with Scala 2.11 on Linux with latest master branch :

[INFO] Spark Project External MQTT ........................ SUCCESS [
19.188 s]
[INFO] Spark Project External MQTT Assembly ............... SUCCESS [
 7.081 s]
[INFO] Spark Project External ZeroMQ ...................... SUCCESS [
 8.790 s]
[INFO] Spark Project External Kafka ....................... SUCCESS [
14.764 s]
[INFO] Spark Project Examples ............................. SUCCESS [02:22
min]
[INFO] Spark Project External Kafka Assembly .............. SUCCESS [
10.286 s]
[INFO]
------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 17:49 min

FYI


ster-Scala211-Compile/
e.com>
in/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala:310: no valid targets for annotation on value conf - it is discarded unused. You may specify targets with meta-annotations, e.g. @(transient @param)
onf)
s
"
<Saif.A.Ellafi@wellsfargo.com>,"Thu, 8 Oct 2015 16:03:37 +0000","RE: RowNumber in HiveContext returns null, negative numbers or huge","<Saif.A.Ellafi@wellsfargo.com>, <dev@spark.apache.org>","Hi,

I have figured this only happens in cluster mode. working properly in local[32]

From: Saif.A.Ellafi@wellsfargo.com [mailto:Saif.A.Ellafi@wellsfargo.com]
Sent: Thursday, October 08, 2015 10:23 AM
To: dev@spark.apache.org
Subject: RowNumber in HiveContext returns null, negative numbers or huge

Hi all, would this be a bug??

        val ws = Window.
            partitionBy(""clrty_id"").
            orderBy(""filemonth_dtt"")

        val nm = ""repeatMe""
        df.select(df.col(""*""), rowNumber().over(ws).cast(""int"").as(nm))

        stacked_data.filter(stacked_data(""repeatMe"").isNotNull).orderBy(""repeatMe"").take(50).foreach(println(_))

--->

Long, DateType, Int
[200000000003,2006-06-01,-1863462909]
[200000000003,2006-09-01,-1863462909]
[200000000003,2007-01-01,-1863462909]
[200000000003,2007-08-01,-1863462909]
[200000000003,2007-07-01,-1863462909]
[200000000138,2007-07-01,-1863462774]
[200000000138,2007-02-01,-1863462774]
[200000000138,2006-11-01,-1863462774]
[200000000138,2006-08-01,-1863462774]
[200000000138,2007-08-01,-1863462774]
[200000000138,2006-09-01,-1863462774]
[200000000138,2007-03-01,-1863462774]
[200000000138,2006-10-01,-1863462774]
[200000000138,2007-05-01,-1863462774]
[200000000138,2006-06-01,-1863462774]
[200000000138,2006-12-01,-1863462774]


Thanks,
Saif

"
Reynold Xin <rxin@databricks.com>,"Thu, 8 Oct 2015 10:24:18 -0700",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Ted Yu <yuzhihong@gmail.com>,"The problem only applies to the sbt build because it treats warnings as
errors.

@Iulian - how about we disable warnings -> errors for 2.11? That would seem
better until we switch 2.11 to be the default build.



2
aster-Scala211-Compile/
fe.com
ain/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala:310: no valid targets for annotation on value conf - it is discarded unused. You may specify targets with meta-annotations, e.g. @(transient @param)
Conf)
ns.
"
Xiao Li <gatorsmile@gmail.com>,"Thu, 8 Oct 2015 10:26:55 -0700","=?UTF-8?Q?Re=3A_Understanding_code=2Fclosure_shipment_to_Spark_wor?=
	=?UTF-8?Q?kers=E2=80=8F?=",Arijit <arijitt@live.com>,"Hi, Arijit,

The code flow of spark-submit is simple.

Enter the main function of SparkSubmit.scala
    --> case SparkSubmitAction.SUBMIT => submit(appArgs)
    --> doRunMain() in function submit() in the same file
    --> runMain(childArgs,...) in the same file
    --> mainMethod.invoke(null, childArgs.toArray)  in the same file

Function Invoke() is provided by JAVA Reflection for invoking the main
function of your JAR.

Hopefully, it can help you understand the problem.

Thanks,

Xiao Li


2015-10-07 16:47 GMT-07:00 Arijit <arijitt@live.com>:

"
Chester Chen <chester@alpinenow.com>,"Thu, 8 Oct 2015 10:35:17 -0700",Build spark 1.5.1 branch fails,"""dev@spark.apache.org"" <dev@spark.apache.org>","Question regarding branch-1.5  build.

Noticed that the spark project no longer publish the spark-assembly. We
have to build ourselves ( until we find way to not depends on assembly
jar).


I check out the tag v.1.5.1 release version and using the sbt to build it,
I get the following error

build/sbt -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive
-Phive-thriftserver -DskipTests clean package assembly


[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] ::          UNRESOLVED DEPENDENCIES         ::
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] :: org.apache.spark#spark-network-common_2.10;1.5.1: configuration
not public in org.apache.spark#spark-network-common_2.10;1.5.1: 'test'. It
was required from org.apache.spark#spark-network-shuffle_2.10;1.5.1 test
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn]
[warn] Note: Unresolved dependencies path:
[warn] org.apache.spark:spark-network-common_2.10:1.5.1
((com.typesafe.sbt.pom.MavenHelper) MavenHelper.scala#L76)
[warn]  +- org.apache.spark:spark-network-shuffle_2.10:1.5.1
[info] Packaging
/Users/chester/projects/alpine/apache/spark/launcher/target/scala-2.10/spark-launcher_2.10-1.5.1.jar
...
[info] Done packaging.
[warn] four warnings found
[warn] Note: Some input files use unchecked or unsafe operations.
[warn] Note: Recompile with -Xlint:unchecked for details.
[warn] No main class detected
[info] Packaging
/Users/chester/projects/alpine/apache/spark/external/flume-sink/target/scala-2.10/spark-streaming-flume-sink_2.10-1.5.1.jar
...
[info] Done packaging.
sbt.ResolveException: unresolved dependency:
org.apache.spark#spark-network-common_2.10;1.5.1: configuration not public
in org.apache.spark#spark-network-common_2.10;1.5.1: 'test'. It was
required from org.apache.spark#spark-network-shuffle_2.10;1.5.1 test


Somehow the network-shuffle can't find the test jar needed ( not sure why
test still needed, even the  -DskipTests is already specified)

tried the maven command, the build failed as well ( without assembly)

mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver
-DskipTests clean package

[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-enforcer-plugin:1.4:enforce
(enforce-versions) on project spark-parent_2.10: Some Enforcer rules have
failed. Look above for specific messages explaining why the rule failed. ->
[Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions,
please read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException



I checkout the branch-1.5 and replaced ""1.5.2-SNAPSHOT"" with ""1.5.1"" and
build/sbt will still fail ( same error as above for sbt)

But if I keep the version string as ""1.5.2-SNAPSHOT"", the build/sbt works
fine.


Any ideas ?

Chester
"
sbiookag <sbiookag@asu.edu>,"Thu, 8 Oct 2015 11:22:57 -0700 (MST)",Compiling Spark with a local hadoop profile,dev@spark.apache.org,"I'm modifying hdfs module inside hadoop, and would like the see the
reflection while i'm running spark on top of it, but I still see the native
hadoop behaviour. I've checked and saw Spark is building a really fat jar
file, which contains all hadoop classes (using hadoop profile defined in
maven), and deploy it over all workers. I also tried bigtop-dist, to exclude
hadoop classes but see no effect.

Is it possible to do such a thing easily, for example by small modifications
inside the maven file?



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 8 Oct 2015 11:26:36 -0700",Re: Compiling Spark with a local hadoop profile,sbiookag <sbiookag@asu.edu>,"In root pom.xml :
    <hadoop.version>2.2.0</hadoop.version>

You can override the version of hadoop with command similar to:
-Phadoop-2.4 -Dhadoop.version=2.7.0

Cheers


"
sbiookag <sbiookag@asu.edu>,"Thu, 8 Oct 2015 11:31:40 -0700 (MST)",Re: Compiling Spark with a local hadoop profile,dev@spark.apache.org,"Thanks Ted for reply.

But this is not what I want. This would tell spark to read hadoop dependency
from maven repository, which is the original version of hadoop. I myslef is
modifying the hadoop code, and wanted to include them inside the spark fat
jar. ""Spark-Class"" would run slaves with the fat jar created in the assembly
folder, and that jar does not contain my modified classes. 

Something that confuses me is, what spark includes the hadoop classes in
it's built jar output? Isn't it supposed to go and read from the hadoop
folder in each worker node?



--

---------------------------------------------------------------------


"
Pranay Tonpay <ptonpay@gmail.com>,"Fri, 9 Oct 2015 00:21:41 +0530",spark over drill,dev@spark.apache.org,"hi ,,
Is spark-drill integration already done ? if yes, which spark version
supports it ... it was in the ""upcming list for 2015"" is what i had read
somewhere
"
Reynold Xin <rxin@databricks.com>,"Thu, 8 Oct 2015 11:58:04 -0700",Re: spark over drill,Pranay Tonpay <ptonpay@gmail.com>,"You probably saw that in a presentation given by the drill team. You should
check with them on that.


"
Niranda Perera <niranda.perera@gmail.com>,"Fri, 9 Oct 2015 12:55:23 +0530",passing a AbstractFunction1 to sparkContext().runJob instead of a Closure,"""dev@spark.apache.org"" <dev@spark.apache.org>","hi all,

I want to run a job in the spark context and since I am running the system
in the java environment, I can not use a closure in
the sparkContext().runJob. Instead, I am passing an AbstractFunction1
extension.

while I get the jobs run without an issue, I constantly get the following
WARN message

TID: [-1234] [] [2015-10-06 04:39:43,387]  WARN
{org.apache.spark.util.ClosureCleaner} -  Expected a closure; got
org.wso2.carbon.analytics.spark.core.sources.AnalyticsWritingFunction
{org.apache.spark.util.ClosureCleaner}


I want to know what are the implications of this approach?
could this WARN cause issues in the functionality later on?

rgds
-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 9 Oct 2015 09:08:22 +0000",Re: Compiling Spark with a local hadoop profile,sbiookag <sbiookag@asu.edu>,"
ncy
is
t
bly

it should if you have built a local hadoop version and done the -Phadoop-2.6 -Dhadoop.version=2.8.0-SNAPSHOT

if you are rebuilding hadoop with an existing version number (e.g. 2.6.0, 2.7.1) then maven may not actually be picking up your new code




There's a hadoop-provided profile which you can build with; this should leave the hadoop artifacts (and other stuff expected to be in the far-end's classpath) out of the assembly

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Fri, 9 Oct 2015 16:18:30 +0200",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Reynold Xin <rxin@databricks.com>,"Sorry for not being clear, yes, that's about the Sbt build and treating
warnings as errors.

Warnings in 2.11 are useful, though, it'd be a pity to keep introducing
potential issues. As a stop-gap measure I can disable them in the Sbt
build, is it hard to run the CI test with 2.11/sbt?

iulian



Master-Scala211-Compile/
main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala:310: no valid targets for annotation on value conf - it is discarded unused. You may specify targets with meta-annotations, e.g. @(transient @param)
kConf)
ons.


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Sean Owen <sowen@cloudera.com>,"Fri, 9 Oct 2015 15:27:36 +0100",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"How about just fixing the warning? I get it; it doesn't stop this from
happening again, but still seems less drastic than tossing out the
whole mechanism.

ld,

---------------------------------------------------------------------


"
Robert Dodier <robert.dodier@gmail.com>,"Fri, 9 Oct 2015 09:41:42 -0700","sbt test error -- ""Could not reserve enough space""","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am trying to  build and test the current master. My system is Ubuntu
14.04 with 4 G physical memory with Oracle Java 8.

I have been running into various out-of-memory errors. I tried
building with Maven but couldn't get all the way through compile and
package. I'm having better luck with sbt. At this point build/sbt
package runs to completion, so that's great.

When I try to run build/sbt test, I get a lot of errors saying: ""Could
not reserve enough space for 3145728KB object heap"". Unfortunately 3.1
G is somewhat larger than the available memory, as reported by 'free'.
Is there any way to convince sbt that it needs to allocate less
memory?

I tried build/sbt ""test-only
org.apache.spark.mllib.random.RandomDataGeneratorSuite"" (I'm not
particularly interested in that test, it's just one that I thought
would be relatively simple) but it seems to do a lot more work than
just running that one test, and I still get the out-of-memory errors.

Aside from getting a machine with more memory (which is not out of the
question), are there any stretegies for coping with out-of-memory
errors in Maven and/or sbt?

Thanks in advance for any light you can shed on this problem.

Robert Dodier

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 9 Oct 2015 11:47:23 -0700",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Sean Owen <sowen@cloudera.com>,"
+1

It also does not seem that expensive to test only compilation for Scala
2.11 on PR builds.
"
Hari Shreedharan <hshreedharan@cloudera.com>,"Fri, 9 Oct 2015 12:31:13 -0700",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Michael Armbrust <michael@databricks.com>,"+1, much better than having a new PR each time to fix something for scala-2.11 every time a patch breaks it.

Thanks,
Hari Shreedharan




Scala 2.11 on PR builds. 

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 9 Oct 2015 16:34:23 -0400",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Hari Shreedharan <hshreedharan@cloudera.com>,"I would push back slightly. The reason we have the PR builds taking so long
is death by a million small things that we add. Doing a full 2.11 compile
is order minutes... it's a nontrivial increase to the build times.

It doesn't seem that bad to me to go back post-hoc once in a while and fix
2.11 bugs when they come up. It's on the order of once or twice per release
and the typesafe guys keep a close eye on it (thanks!). Compare that to
literally thousands of PR runs and a few minutes every time, IMO it's not
worth it.


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 9 Oct 2015 23:35:37 +0000",Operations with cached RDD,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I am trying to understand how Spark UI displays operation with the cached RDD.

For example, the following code caches an rdd:
The Jobs tab shows me that the RDD is evaluated:
: 1 count at <console>:24              2015/10/09 16:15:43        0.4 s       1/1
: 0 zipWithIndex at <console> :21             2015/10/09 16:15:38        0.6 s       1/1
An I can observe this rdd in the Storage tab of Spark UI:
: ZippedWithIndexRDD  Memory Deserialized 1x Replicated

Then I want to make an operation over the cached RDD. I run the following code:
The Jobs tab shows me a new Job:
: 2 count at <console>:26
Inside this Job there are two stages:
: 3 count at <console>:26 +details 2015/10/09 16:16:18   0.2 s       5/5
: 2 zipWithIndex at <console>:21
It shows that zipWithIndex is executed again. It does not seem to be reasonable, because the rdd is cached, and zipWithIndex is already executed previously.

Could you explain why if I perform an operation followed by an action on a cached RDD, then the last operation in the lineage of the cached RDD is shown to be executed in the Spark UI?


Best regards, Alexander
"
Prashant Sharma <scrapcodes@gmail.com>,"Sat, 10 Oct 2015 10:34:59 +0530",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Patrick Wendell <pwendell@gmail.com>,"That is correct !, I have thought about this a lot of times. The only
solution is to implement a ""real"" cross build for both version. I am going
to think more in this. :)

Prashant Sharma




"
Akhil Das <akhil@sigmoidanalytics.com>,"Sun, 11 Oct 2015 14:08:37 +0530",Re: Too many executors are created,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","For some reason the executors are getting killed,

15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated:
app-20150929120924-0000/24463 is now EXITED (Command exited with code 1)

Can you paste your spark-submit command? You can also look in the executor
logs and see whats going on.

Thanks
Best Regards


"
Disha Shrivastava <dishu.905@gmail.com>,"Sun, 11 Oct 2015 17:57:44 +0530","No speedup in MultiLayerPerceptronClassifier with increase in number
 of cores","dev@spark.apache.org, alexander.ulanov@hpe.com","Dear Spark developers,

I am trying to study the effect of increasing number of cores ( CPU's) on
speedup and accuracy ( scalability with spark ANN ) performance for the
MNIST dataset using ANN implementation provided in the latest spark release.

I have formed a cluster of 5 machines with 88 cores in total.The thing
which is troubling me is that even if I have more than 2 workers in my
spark cluster the job gets divided only to 2 workers.( executors) which
Spark takes by default and hence it takes the same time . I know we can set
the number of partitions manually using sc.parallelize(train_data,10)
suppose which then divides the data in 10 partitions and all the workers
are involved in the computation.I am using the below code:


import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.sql.Row

// Load training data
val data = MLUtils.loadLibSVMFile(sc, ""data/10000_libsvm"").toDF()
// Split the data into train and test
val splits = data.randomSplit(Array(0.7, 0.3), seed = 1234L)
val train = splits(0)
val test = splits(1)
//val tr=sc.parallelize(train,10);
// specify layers for the neural network:
// input layer of size 4 (features), two intermediate of size 5 and 4 and
output of size 3 (classes)
val layers = Array[Int](784,160,10)
// create the trainer and set its parameters
val trainer = new
MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)
// train the model
val model = trainer.fit(train)
// compute precision on the test set
val result = model.transform(test)
val predictionAndLabels = result.select(""prediction"", ""label"")
val evaluator = new
MulticlassClassificationEvaluator().setMetricName(""precision"")
println(""Precision:"" + evaluator.evaluate(predictionAndLabels))

Can you please suggest me how can I ensure that the data/task is divided
equally to all the worker machines?

Thanks and Regards,
Disha Shrivastava
Masters student, IIT Delhi
"
Nitin Goyal <nitin2goyal@gmail.com>,"Sun, 11 Oct 2015 18:27:17 +0530",Re: Operations with cached RDD,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","The problem is not that zipWithIndex is executed again. ""groupBy"" triggered
hash partitioning on your keys and a shuffle happened due to that and
that's why you are seeing 2 stages. You can confirm this by clicking on
latter ""zipWithIndex"" stage and input data has ""(memory)"" written which
means input data has been fetched from memory (your cached RDD).

As far as lineage/call site is concerned, I think there was a change in
spark 1.3 which excluded some classes from appearing in call site (I know
that some Spark SQL related were removed for sure).

Thanks
-Nitin






-- 
Regards
Nitin Goyal
"
"""Daniel Gruno""<humbedooh@apache.org>","Sun, 11 Oct 2015 14:26:16 -0000",Re: [ANNOUNCE] Announcing Spark 1.5.1,<dev@spark.apache.org>,"Out of curiosity: How can you vote on a release that contains 34 binary files? Surely a source code release should only contain source code and not binaries, as you cannot verify the content of these.

Looking forward to a response.

With regards,
Daniel.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 11 Oct 2015 16:12:45 +0100",Re: [ANNOUNCE] Announcing Spark 1.5.1,Daniel Gruno <humbedooh@apache.org>,"The Spark releases include a source distribution and several binary
distributions. This is pretty normal for Apache projects. What are you
referring to here?


---------------------------------------------------------------------


"
Daniel Gruno <humbedooh@apache.org>,"Sun, 11 Oct 2015 17:23:43 +0200",Re: [ANNOUNCE] Announcing Spark 1.5.1,"Sean Owen <sowen@cloudera.com>, Daniel Gruno <humbedooh@apache.org>","
Surely the _source_ distribution does not contain binaries? How else can
you vote on a release if you don't know what it contains?

You can produce convenience downloads that contain binary files, yes,
but surely you need a source-only package which is the one you vote on,
that does not contain any binaries. Do you have such a thing? And where
may I find it?

With regards,
Daniel.



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 11 Oct 2015 16:29:10 +0100",Re: [ANNOUNCE] Announcing Spark 1.5.1,Daniel Gruno <humbedooh@apache.org>,"Of course, but what's making you think this was a binary-only
distribution? The downloads page points you directly to the source
distro: http://spark.apache.org/downloads.html

Look for the last vote, and you'll find it was of course a vote on
source (and binary) artifacts:
http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-5-1-RC1-tt14310.html#none
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 11 Oct 2015 15:30:06 +0000",Re: [ANNOUNCE] Announcing Spark 1.5.1,"Daniel Gruno <humbedooh@apache.org>, Sean Owen <sowen@cloudera.com>","You can find the source tagged for release on GitHub
<https://github.com/apache/spark/releases/tag/v1.5.1>, as was clearly
linked to in the thread to vote on the release (titled ""[VOTE] Release
Apache Spark 1.5.1 (RC1)"").

Is there something about that thread that was unclear?

Nick



"
Daniel Gruno <humbedooh@apache.org>,"Sun, 11 Oct 2015 17:33:17 +0200",Re: [ANNOUNCE] Announcing Spark 1.5.1,"Sean Owen <sowen@cloudera.com>, Daniel Gruno <humbedooh@apache.org>","
I'm not saying binary-only, I am saying your source release contains
binary programs, which would invalidate a release vote. Is there a
release candidate package, that is voted on (saying you have a git tag
does not satisfy this criteria, you need to vote on an actual archive of
files, otherwise there is no cogent proof of the release being from that
specific git tag).

Here's what I found in your source release:

Binary application (application/jar; charset=binary) found in
spark-1.5.1/sql/hive/src/test/resources/data/files/TestSerDe.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/sql/hive/src/test/resources/regression-test-SPARK-8489/test.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/sql/hive/src/test/resources/TestUDTF.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/R/pkg/inst/test_support/sparktestjar_2.10-1.0.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/zinc-0.3.5.3/lib/scala-reflect.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/zinc-0.3.5.3/lib/sbt-interface.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/zinc-0.3.5.3/lib/compiler-interface-sources.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/zinc-0.3.5.3/lib/incremental-compiler.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/zinc-0.3.5.3/lib/scala-compiler.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/zinc-0.3.5.3/lib/zinc.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/zinc-0.3.5.3/lib/scala-library.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/misc/scala-devel/plugins/continuations.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/scala-reflect.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/akka-actors.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/typesafe-config.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/scala-actors-migration.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/scala-actors.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/scalap.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/scala-swing.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/scala-compiler.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/lib/scala-library.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/scala-reflect-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/scala-swing-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/scalap-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/scala-actors-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/scala-partest-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/scala-library-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/fjbg-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/scala-compiler-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/scala-2.10.4/src/msil-src.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/apache-maven-3.3.3/boot/plexus-classworlds-2.5.2.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/apache-maven-3.3.3/lib/guava-18.0.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/apache-maven-3.3.3/lib/wagon-http-2.9-shaded.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/apache-maven-3.3.3/lib/jsr250-api-1.0.jar

Binary application (application/jar; charset=binary) found in
spark-1.5.1/build/apache-maven-3.3.3/lib/javax.inject-1.jar



The downloads page points you directly to the source


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 11 Oct 2015 16:42:36 +0100",Re: [ANNOUNCE] Announcing Spark 1.5.1,Daniel Gruno <humbedooh@apache.org>,"Still confused. Why are you saying we didn't vote on an archive? refer
to the email I linked, which includes both the git tag and a link to
all generated artifacts (also in my email).

So, there are two things at play here:

First, I am not sure what you mean that a source distro can't have
binary files. It's supposed to have the source code of Spark, and
shouldn't contain binary Spark. Nothing you listed are Spark binaries.
However, a distribution might have a lot of things in it that support
the source build, like copies of tools, test files, etc.  That
explains I think the first couple lines that you identified.

Still, I am curious why you are saying that would invalidate a source
release? I have never heard anything like that.

Second, I do think there are some binaries in here that aren't
supposed to be there, like the build/ directory stuff. IIRC these were
included accidentally and won't be in the next release. At least, I
don't see why they need to be bundled. These are just local copies of
third party tools though, and don't really matter. As it happens, the
licenses that get distributed with the source distro even cover all of
this stuff. I think that's not supposed to be there, but, also don't
see it's 'invalid' as a result.



---------------------------------------------------------------------


"
Daniel Gruno <humbedooh@apache.org>,"Sun, 11 Oct 2015 17:49:27 +0200",Re: [ANNOUNCE] Announcing Spark 1.5.1,"Sean Owen <sowen@cloudera.com>, Daniel Gruno <humbedooh@apache.org>","Here's my issue:

How am I to audit that the dependencies you bundle are in fact what you
claim they are?  How do I know they don't contain malware or - in light
of recent events - emissions test rigging? ;)

I am not interested in a git tag - that means nothing in the ASF voting
process, you cannot vote on a tag, only on a release candidate. The VCS
in use is irrelevant in this issue. If you can point me to a release
candidate archive that was voted upon and does not contain binary
applications, all is well.

If there is no such thing, and we cannot come to an understanding, I
will exercise my ASF Members' rights and bring this to the attention of
the board of directors and ask for a clarification of the legality of this.

I find it highly irregular. Perhaps it is something some projects do in
the Java community, but that doesn't make it permissible in my view.

With regards,
Daniel.




---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 11 Oct 2015 16:53:18 +0100",Re: [ANNOUNCE] Announcing Spark 1.5.1,Daniel Gruno <humbedooh@apache.org>,"Daniel: we did not vote on a tag. Please again read the VOTE email I
linked to you:

http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-5-1-RC1-tt14310.html#none

among other things, it contains a link to the concrete source (and
binary) distribution under vote:

http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/

You can still examine it, sure.

Dependencies are *not* bundled in the source release. You're again
misunderstanding what you are seeing. Read my email again.

I am still pretty confused about what the problem is. This is entirely
business as usual for ASF projects. I'll follow up with you offline if
you have any more doubts.


---------------------------------------------------------------------


"
Mike Hynes <91mbbh@gmail.com>,"Sun, 11 Oct 2015 12:19:39 -0400","Re: No speedup in MultiLayerPerceptronClassifier with increase in
 number of cores",Disha Shrivastava <dishu.905@gmail.com>,"Having only 2 workers for 5 machines would be your problem: you
probably want 1 worker per physical machine, which entails running the
spark-daemon.sh script to start a worker on those machines.
The partitioning is agnositic to how many executors are available for
running the tasks, so you can't do scalability tests in the manner
you're thinking by changing the partitioning.



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Disha Shrivastava <dishu.905@gmail.com>,"Sun, 11 Oct 2015 21:58:47 +0530","Re: No speedup in MultiLayerPerceptronClassifier with increase in
 number of cores",Mike Hynes <91mbbh@gmail.com>,"Actually I have 5 workers running ( 1 per physical machine) as displayed by
the spark UI on spark://IP_of_the_master:7077. I have entered all the
physical machines IP in a file named slaves in spark/conf directory and
using the script start-all.sh to start the cluster.

My question is that is there a way to control how the tasks are distributed
among different workers? To my knowledge it is done by Spark automatically
and is not in our control.


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Mon, 12 Oct 2015 03:49:52 +0000",yarn-cluster mode throwing NullPointerException,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I am trying to submit a job using yarn-cluster mode using spark-submit command.  My code works fine when I use yarn-client mode.

Cloudera Version:
CDH-5.4.7-1.cdh5.4.7.p0.3

Command Submitted:
spark-submit --class ""com.markmonitor.antifraud.ce.KafkaURLStreaming""  \
--driver-java-options ""-Dlog4j.configuration=file:///etc/spark/myconf/log4j.sample.properties"" \
--conf ""spark.driver.extraJavaOptions=-Dlog4j.configuration=file:///etc/spark/myconf/log4j.sample.properties"" \
--conf ""spark.executor.extraJavaOptions=-Dlog4j.configuration=file:///etc/spark/myconf/log4j.sample.properties"" \
--num-executors 2 \
--executor-cores 2 \
../target/mm-XXX-ce-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
yarn-cluster 10 ""XXX:2181"" ""XXX:9092"" groups kafkaurl 5 \
""hdfs://ip-10-0-0-XXX.us-west-2.compute.internal:8020/user/ec2-user/urlFeature.properties"" \
""hdfs://ip-10-0-0-XXX.us-west-2.compute.internal:8020/user/ec2-user/urlFeatureContent.properties"" \
""hdfs://ip-10-0-0-XXX.us-west-2.compute.internal:8020/user/ec2-user/hdfsOutputNEWScript/OUTPUTYarn2""  false


Log Details:
INFO : org.apache.spark.SparkContext - Running Spark version 1.3.0
INFO : org.apache.spark.SecurityManager - Changing view acls to: ec2-user
INFO : org.apache.spark.SecurityManager - Changing modify acls to: ec2-user
INFO : org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ec2-user); users with modify permissions: Set(ec2-user)
INFO : akka.event.slf4j.Slf4jLogger - Slf4jLogger started
INFO : Remoting - Starting remoting
INFO : Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@ip-10-0-0-XXX.us-west-2.compute.internal:49579]
INFO : Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriver@ip-10-0-0-XXX.us-west-2.compute.internal:49579]
INFO : org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49579.
INFO : org.apache.spark.SparkEnv - Registering MapOutputTracker
INFO : org.apache.spark.SparkEnv - Registering BlockManagerMaster
INFO : org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/spark-1c805495-c7c4-471d-973f-b1ae0e2c8ff9/blockmgr-fff1946f-a716-40fc-a62d-bacba5b17638
INFO : org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 265.4 MB
INFO : org.apache.spark.HttpFileServer - HTTP File server directory is /tmp/spark-8ed6f513-854f-4ee4-95ea-87185364eeaf/httpd-75cee1e7-af7a-4c82-a9ff-a124ce7ca7ae
INFO : org.apache.spark.HttpServer - Starting HTTP Server
INFO : org.spark-project.jetty.server.Server - jetty-8.y.z-SNAPSHOT
INFO : org.spark-project.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:46671
INFO : org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 46671.
INFO : org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
INFO : org.spark-project.jetty.server.Server - jetty-8.y.z-SNAPSHOT
INFO : org.spark-project.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
INFO : org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
INFO : org.apache.spark.ui.SparkUI - Started SparkUI at http://ip-10-0-0-XXX.us-west-2.compute.internal:4040
INFO : org.apache.spark.SparkContext - Added JAR file:/home/ec2-user/CE/correlationengine/scripts/../target/mm-anti-fraud-ce-0.0.1-SNAPSHOT-jar-with-dependencies.jar at http://10.0.0.XXX:46671/jars/mm-anti-fraud-ce-0.0.1-SNAPSHOT-jar-with-dependencies.jar with timestamp 1444620509463
INFO : org.apache.spark.scheduler.cluster.YarnClusterScheduler - Created YarnClusterScheduler
ERROR: org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend - Application ID is not set.
INFO : org.apache.spark.network.netty.NettyBlockTransferService - Server created on 33880
INFO : org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
INFO : org.apache.spark.storage.BlockManagerMasterActor - Registering block manager ip-10-0-0-XXX.us-west-2.compute.internal:33880 with 265.4 MB RAM, BlockManagerId(<driver>, ip-10-0-0-XXX.us-west-2.compute.internal, 33880)
INFO : org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
INFO : org.apache.spark.scheduler.EventLoggingListener - Logging events to hdfs://ip-10-0-0-XXX.us-west-2.compute.internal:8020/user/spark/applicationHistory/spark-application-1444620509497
Exception in thread ""main"" java.lang.NullPointerException
    at org.apache.spark.deploy.yarn.ApplicationMaster$.sparkContextInitialized(ApplicationMaster.scala:580)
    at org.apache.spark.scheduler.cluster.YarnClusterScheduler.postStartHook(YarnClusterScheduler.scala:32)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:541)
    at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
    at com.markmonitor.antifraud.ce.KafkaURLStreaming.main(KafkaURLStreaming.java:91)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
WARN : org.apache.hadoop.hdfs.DFSClient - Unable to persist blocks in hflush for /user/spark/applicationHistory/spark-application-1444620509497.inprogress
java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.0.0.XXX:43929 remote=ip-10-0-0-XXX.us-west-2.compute.internal/10.0.0.XXX:8020]. 59998 millis timeout left.; Host Details : local host is: ""ip-10-0-0-XXX.us-west-2.compute.internal/10.0.0.XXX""; destination host is: ""ip-10-0-0-XXX.us-west-2.compute.internal"":8020;
    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
    at org.apache.hadoop.ipc.Client.call(Client.java:1472)
    at org.apache.hadoop.ipc.Client.call(Client.java:1399)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
    at com.sun.proxy.$Proxy18.fsync(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.fsync(ClientNamenodeProtocolTranslatorPB.java:814)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
    at com.sun.proxy.$Proxy19.fsync(Unknown Source)
    at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:2067)
    at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959)
    at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener.onBlockManagerAdded(EventLoggingListener.scala:171)
    at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:46)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)
    at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)
Caused by: java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.0.0.XXX:43929 remote=ip-10-0-0-XXX.us-west-2.compute.internal/10.0.0.XXX:8020]. 59998 millis timeout left.
    at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
    at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
    at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
    at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
    at java.io.FilterInputStream.read(FilterInputStream.java:133)
    at java.io.FilterInputStream.read(FilterInputStream.java:133)
    at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:513)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
    at java.io.DataInputStream.readInt(DataInputStream.java:387)
    at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
    at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
WARN : org.apache.hadoop.hdfs.DFSClient - Error while syncing
java.nio.channels.ClosedChannelException
    at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1635)
    at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:2074)
    at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959)
    at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener.onBlockManagerAdded(EventLoggingListener.scala:171)
    at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:46)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)
    at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)
ERROR: org.apache.spark.scheduler.LiveListenerBus - Listener EventLoggingListener threw an exception
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener.onBlockManagerAdded(EventLoggingListener.scala:171)
    at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:46)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)
    at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)
Caused by: java.nio.channels.ClosedChannelException
    at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1635)
    at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:2074)
    at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959)
    at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)
    ... 19 more
ERROR: org.apache.spark.scheduler.LiveListenerBus - Listener EventLoggingListener threw an exception
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144)
    at org.apache.spark.scheduler.EventLoggingListener.onApplicationStart(EventLoggingListener.scala:177)
    at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:52)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
    at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)
    at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
    at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)
Caused by: java.io.IOException: Filesystem closed
    at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:794)
    at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1998)
    at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959)
    at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)
    ... 19 more
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/jars/avro-tools-1.7.6-cdh5.4.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Thanks,

Rachana
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 11 Oct 2015 21:35:30 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.1,Sean Owen <sowen@cloudera.com>,"I think Daniel is correct here. The source artifact incorrectly includes
jars. It is inadvertent and not part of our intended release process. This
was something I noticed in Spark 1.5.0 and filed a JIRA and was fixed by
updating our build scripts to fix it. However, our build environment was
not using the most current version of the build scripts. See related links:

https://issues.apache.org/jira/browse/SPARK-10511
https://github.com/apache/spark/pull/8774/files

I can update our build environment and we can repackage the Spark 1.5.1
source tarball. To not include sources.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 11 Oct 2015 21:35:41 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.1,Sean Owen <sowen@cloudera.com>,"*to not include binaries.


"
Sean Owen <sowen@cloudera.com>,"Mon, 12 Oct 2015 04:57:23 +0000",Re: [ANNOUNCE] Announcing Spark 1.5.1,Patrick Wendell <pwendell@gmail.com>,"Agree, but we are talking about the build/ bit right?

I don't agree that it invalidates the release, which is probably the more
important idea. As a point of process, you would not want to modify and
republish the artifact that was already released after being voted on -
unless it was invalid in which case we spin up 1.5.1.1 or something.

But that build/ directory should go in future releases.

I think he is talking about more than this though and the other jars look
like they are part of tests, and still nothing to do with Spark binaries.
Those can and should stay.


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 11 Oct 2015 22:03:27 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.1,Sean Owen <sowen@cloudera.com>,"Oh I see - yes it's the build/. I always thought release votes related to a
source tag rather than specific binaries. But maybe we can just fix it in
1.5.2 if there is concern about mutating binaries. It seems reasonable to
me.

For tests... in the past we've tried to avoid having jars inside of the
source tree, including some effort to generate jars on the fly which a lot
of our tests use. I am not sure whether it's a firm policy that you can't
have jars in test folders, though. If it is, we could probably do some
magic to get rid of these few ones that have crept in.

- Patrick


"
Sean Owen <sowen@cloudera.com>,"Mon, 12 Oct 2015 05:12:42 +0000",Re: [ANNOUNCE] Announcing Spark 1.5.1,Patrick Wendell <pwendell@gmail.com>,"No we are voting on the artifacts being released (too) in principle.
Although of course the artifacts should be a deterministic function of the
source at a certain point in time.

I think the concern is about putting Spark binaries or its dependencies
into a source release. That should not happen, but it is not what has
happened here.


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 11 Oct 2015 22:18:48 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.1,Sean Owen <sowen@cloudera.com>,"Yeah I mean I definitely think we're not violating the *spirit* of the ""no
binaries"" policy, in that we do not include any binary code that is used at
runtime. This is because the binaries we distribute relate only to build
and testing.

Whether we are violating the *letter* of the policy, I'm not so sure. In
the very strictest interpretation of ""there cannot be any binary files in
your downloaded tarball"" - we aren't honoring that. We got a lot of people
complaining about the sbt jar for instance when we were in the incubator. I
found those complaints a little pedantic, but we ended up removing it from
our source tree and adding things to download it for the user.

- Patrick


"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 12 Oct 2015 11:15:55 +0530",taking the heap dump when an executor goes OOM,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

is there a way for me to get the heap-dump hprof of an executor jvm, when
it goes out of memory?

is this currently supported or do I have to change some configurations?

cheers

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Venkatakrishnan Sowrirajan <vsowrira@asu.edu>,"Sun, 11 Oct 2015 23:06:51 -0700",Re: yarn-cluster mode throwing NullPointerException,Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Hi Rachana,


Are you by any chance saying something like this in your code
​?
​

""sparkConf.setMaster(""yarn-cluster"");""

​SparkContext is not supported with yarn-cluster mode.​


I think you are hitting this bug -- >
https://issues.apache.org/jira/browse/SPARK-7504. This got fixed in
Spark-1.4.0, so you can try in 1.4.0

Regards
Venkata krishnan


"" \
k/myconf/log4j.sample.properties""
ark/myconf/log4j.sample.properties""
ature.properties""
atureContent.properties""
utputNEWScript/OUTPUTYarn2""
er
y
fc-a62d-bacba5b17638
a9ff-a124ce7ca7ae
ce-0.0.1-SNAPSHOT-jar-with-dependencies.jar
endencies.jar
B
o
onHistory/spark-application-1444620509497
pplicationMaster.scala:580)*
nClusterScheduler.scala:32)*
61)
a:91)
:57)
mpl.java:43)
$runMain(SparkSubmit.scala:569)
l
s:
java:232)
c(ClientNamenodeProtocolTranslatorPB.java:814)
:57)
mpl.java:43)
ationHandler.java:187)
andler.java:102)
067)
0)
:57)
mpl.java:43)
(EventLoggingListener.scala:144)
(EventLoggingListener.scala:144)
ener.scala:144)
LoggingListener.scala:171)
erBus.scala:46)
ala:31)
ala:31)
nerBus.scala:36)
y$mcV$sp(AsynchronousListenerBus.scala:76)
y(AsynchronousListenerBus.scala:61)
y(AsynchronousListenerBus.scala:61)
7)
tenerBus.scala:60)
O
hTimeout.java:352)
57)
513)
71)
635)
074)
0)
:57)
mpl.java:43)
(EventLoggingListener.scala:144)
(EventLoggingListener.scala:144)
ener.scala:144)
LoggingListener.scala:171)
erBus.scala:46)
ala:31)
ala:31)
nerBus.scala:36)
y$mcV$sp(AsynchronousListenerBus.scala:76)
y(AsynchronousListenerBus.scala:61)
y(AsynchronousListenerBus.scala:61)
7)
tenerBus.scala:60)
:57)
mpl.java:43)
(EventLoggingListener.scala:144)
(EventLoggingListener.scala:144)
ener.scala:144)
LoggingListener.scala:171)
erBus.scala:46)
ala:31)
ala:31)
nerBus.scala:36)
y$mcV$sp(AsynchronousListenerBus.scala:76)
y(AsynchronousListenerBus.scala:61)
y(AsynchronousListenerBus.scala:61)
7)
tenerBus.scala:60)
635)
074)
0)
:57)
mpl.java:43)
(EventLoggingListener.scala:144)
(EventLoggingListener.scala:144)
ener.scala:144)
oggingListener.scala:177)
erBus.scala:52)
ala:31)
ala:31)
nerBus.scala:36)
y$mcV$sp(AsynchronousListenerBus.scala:76)
y(AsynchronousListenerBus.scala:61)
y(AsynchronousListenerBus.scala:61)
7)
tenerBus.scala:60)
998)
0)
j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
-1.7.6-cdh5.4.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 12 Oct 2015 00:04:52 -0700",Re: taking the heap dump when an executor goes OOM,Niranda Perera <niranda.perera@gmail.com>,"http://stackoverflow.com/questions/542979/using-heapdumponoutofmemoryerror-parameter-for-heap-dump-for-jboss

ote:
t goes out of memory? 

"
Xiao Li <gatorsmile@gmail.com>,"Mon, 12 Oct 2015 00:05:12 -0700",Re: Build spark 1.5.1 branch fails,Chester Chen <chester@alpinenow.com>,"Hi, Chester,

Please check your pom.xml. Your java.version and maven.version might not
match your build environment.

Or using -Denforcer.skip=true from the command line to skip it.

Good luck,

Xiao Li

2015-10-08 10:35 GMT-07:00 Chester Chen <chester@alpinenow.com>:

"
Xiao Li <gatorsmile@gmail.com>,"Mon, 12 Oct 2015 00:10:15 -0700","Re: sbt test error -- ""Could not reserve enough space""",Robert Dodier <robert.dodier@gmail.com>,"Hi, Robert,

Please check the following link. It might help you.

http://stackoverflow.com/questions/18155325/scala-error-occurred-during-initialization-of-vm-on-ubuntu-12-04

Good luck,

Xiao Li


2015-10-09 9:41 GMT-07:00 Robert Dodier <robert.dodier@gmail.com>:

"
YiZhi Liu <javelinjs@gmail.com>,"Mon, 12 Oct 2015 16:24:09 +0800","Re: What is the difference between ml.classification.LogisticRegression
 and mllib.classification.LogisticRegressionWithLBFGS",Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

Thank you for clarifying the motivation that you setup a different API
for ml pipelines, it sounds great. But I still think we could extract
some common parts of the training & inference procedures for ml and
mllib. In ml.classification.LogisticRegression, you simply transform
the DataFrame into RDD and follow the same procedures in
mllib.optimization.{LBFGS,OWLQN}, right?

My suggestion is, if I may, ml package should focus on the public API,
and leave the underlying implementations, e.g. numerical optimization,
to mllib package.

Please let me know if my understanding has any problem. Thank you!

2015-10-08 1:15 GMT+08:00 Joseph Bradley <joseph@databricks.com>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 12 Oct 2015 15:55:37 +0200",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Patrick Wendell <pwendell@gmail.com>,"

We can host the build if there's a way to post back a comment when the
build is broken.



Anything that can be done by a machine should be done by a machine. I am
not sure we have enough data to say it's only once or twice per release,
and even if we were to issue a PR for each breakage, it's additional load
on committers and reviewers, not to mention our own work. I personally
don't see how 2-3 minutes of compute time per PR can justify hours of work
plus reviews.

iulian




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Mon, 12 Oct 2015 13:56:11 +0000 (UTC)",Re: [ANNOUNCE] Announcing Spark 1.5.1,"Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","I know there are multiple things being talked about here, but  I agree with Patrick here, we vote on the source distribution - src tarball (and of course the tag should match).  Perhaps in principle we vote on all the other specific binary distributions since they are generated from source tarball but that isn't the main thing and I surely don't test and verify each one of those.
Tom 


rote:
   

 No we are voting on the artifacts being released (too) in principle. Although of course the artifacts should be a deterministic function of the source at a certain point in time. I think the concern is about putting Spark binaries or its dependencies into a source release. That should not happen, but it is not what has happened here.

te:

Oh I see - yes it's the build/. I always thought release votes related to a source tag rather than specific binaries. But maybe we can just fix it in 1.5.2 if there is concern about mutating binaries. It seems reasonable to me.
For tests... in the past we've tried to avoid having jars inside of the source tree, including some effort to generate jars on the fly which a lot of our tests use. I am not sure whether it's a firm policy that you can't have jars in test folders, though. If it is, we could probably do some magic to get rid of these few ones that have crept in.
- Patrick

Agree, but we are talking about the build/ bit right?I don't agree that it invalidates the release, which is probably the more important idea. As a point of process, you would not want to modify and republish the artifact that was already released after being voted on - unless it was invalid in which case we spin up 1.5.1.1 or something. But that build/ directory should go in future releases. I think he is talking about more than this though and the other jars look like they are part of tests, and still nothing to do with Spark binaries. Those can and should stay.

te:

I think Daniel is correct here. The source artifact incorrectly includes jars. It is inadvertent and not part of our intended release process. This was something I noticed in Spark 1.5.0 and filed a JIRA and was fixed by updating our build scripts to fix it. However, our build environment was not using the most current version of the build scripts. See related links:
https://issues.apache.org/jira/browse/SPARK-10511https://github.com/apache/spark/pull/8774/files
I can update our build environment and we can repackage the Spark 1.5.1 source tarball. To not include sources.

- Patrick

Daniel: we did not vote on a tag. Please again read the VOTE email I
linked to you:

http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-5-1-RC1-tt14310.html#none

among other things, it contains a link to the concrete source (and
binary) distribution under vote:

http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/

You can still examine it, sure.

Dependencies are *not* bundled in the source release. You're again
misunderstanding what you are seeing. Read my email again.

I am still pretty confused about what the problem is. This is entirely
business as usual for ASF projects. I'll follow up with you offline if
you have any more doubts.

ght
s.
te:
f
t
.jar
ar
-Apache-Spark-1-5-1-RC1-tt14310.html#none
rote:
ou
can
n,
re
nary files? Surely a source code release should only contain source code and not binaries, as you cannot verify the content of these.
his
1
 you
maven
--

---------------------------------------------------------------------









  "
Hao Ren <invkrh@gmail.com>,"Mon, 12 Oct 2015 16:54:35 +0200",SparkSQL can not extract values from UDT (like VectorUDT),dev <dev@spark.apache.org>,"Hi,

Consider the following code using spark.ml to get the probability column on
a data set:

model.transform(dataSet)
.selectExpr(""probability.values"")
.printSchema()

 Note that ""probability"" is `vector` type which is a UDT with the following
implementation.

class VectorUDT extends UserDefinedType[Vector] {

  override def sqlType: StructType = {
    // type: 0 = sparse, 1 = dense
    // We only use ""values"" for dense vectors, and ""size"", ""indices"",
and ""values"" for sparse
    // vectors. The ""values"" field is nullable because we might want
to add binary vectors later,
    // which uses ""size"" and ""indices"", but not ""values"".
    StructType(Seq(
      StructField(""type"", ByteType, nullable = false),
      StructField(""size"", IntegerType, nullable = true),
      StructField(""indices"", ArrayType(IntegerType, containsNull =
false), nullable = true),
      StructField(""values"", ArrayType(DoubleType, containsNull =
false), nullable = true)))
  }

  //...

}


`values` is one of its attribute. However, it can not be extracted.

The first code snippet results in an exception of  complexTypeExtractors:

org.apache.spark.sql.AnalysisException: Can't extract value from
probability#743;
      at ...
      at ...
      at ...
...

Here is the code:
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala#L49

It seems that the pattern matching does not take UDT into consideration.

Is this an intended feature? If not, I would like to create a PR to fix it.

-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
Sean Owen <sowen@cloudera.com>,"Mon, 12 Oct 2015 16:24:47 +0100",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"There are many Jenkins jobs besides the pull request builder that
build against various Hadoop combinations, for example, in the
background. Is there an obstacle to building vs 2.11 on both Maven and
SBT this way?

not

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 12 Oct 2015 08:34:47 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.1,Tom Graves <tgraves_cs@yahoo.com>,"I think the key is to vote a specific set of source tarballs without any
binary artifacts. The specific binaries are useful but shouldn't be part of
the voting process. Makes sense, we really cannot prove (and no need to)
that the  binaries do not contain malware, but the source can be proven to
be clean by inspection, I assume.
Cheers
<k/>


"
"""Jagadeesan A.S."" <lijugan92@gmail.com>","Mon, 12 Oct 2015 21:26:48 +0530",Regarding SPARK JIRA ID-10286,dev@spark.apache.org,"Hi,

I'm newbie to SPARK community. Last three months i started to working on
spark and it's various modules. I tried to test spark with spark-perf
workbench.

Now one step forward, started to contribute in JIRA ID.

i took SPARK JIRA ID - 10286 and sent pull request.

Add @since annotation to pyspark.ml.param and pyspark.ml.*
https://github.com/Jetsonpaul/spark/commit/be1c2769c178746ff094abfdbdefe4869f75ba0d
https://github.com/Jetsonpaul/spark/commit/a1a1c62e9f5b520bef7ce50a09fb72ffde955167

Kindly check and give reviews. As well if i'm wrong please direct me in
correct way, so that i can correct my mistakes in further.


with regards
Jagadeesan A S
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 12 Oct 2015 09:16:54 -0700",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Sean Owen <sowen@cloudera.com>,"We already do automated compile testing for Scala 2.11 similar to Hadoop
versions:

https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/
https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/job/Spark-master-Scala211-Compile/buildTimeTrend


If you look, this build takes 7-10 minutes, so it's a nontrivial increase
to add it to all new PR's. Also, it's only broken once in the last few
months  (despite many patches going in) - a pretty low failure rate. For
scenarios like this it's better to test it asynchronously. We can even just
revert a patch immediately if it's found to break 2.11.

Put another way - we typically have 1000 patches or more per release. Even
at one jenkins run per patch: 7 minutes * 1000 = 7 days of developer
productivity loss. Compare that to having a few times where we have to
revert a patch and ask someone to resubmit (which maybe takes at most one
hour)... it's not worth it.

- Patrick


m
d
n
't
us
"
Sean Owen <sowen@cloudera.com>,"Mon, 12 Oct 2015 17:21:10 +0100",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Patrick Wendell <pwendell@gmail.com>,"Yeah, was the issue that it had to be built vs Maven to show the error
and this uses SBT -- or vice versa? that's why the existing test
didn't detect it. Was just thinking of adding one more of these non-PR
builds, but I forget if there was a reason this is hard. Certainly not
worth building for each PR.

:
ster-Scala211-Compile/buildTimeTrend
 to
s
s
a
n
am
nd
on

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 12 Oct 2015 09:25:49 -0700",Re: Scala 2.11 builds broken/ Can the PR build run also 2.11?,Sean Owen <sowen@cloudera.com>,"It's really easy to create and modify those builds. If the issue is that we
need to add SBT or Maven to the existing one, it's a short change. We can
just have it build both of them. I wasn't aware of things breaking before
in one build but not another.

- Patrick


p
ster-Scala211-Compile/buildTimeTrend
t
ne
I
d
"
Sean Owen <sowen@cloudera.com>,"Mon, 12 Oct 2015 17:56:52 +0100",Re: Regarding SPARK JIRA ID-10286,"""Jagadeesan A.S."" <lijugan92@gmail.com>","I don't see that you ever opened a pull request. You just linked to
commits in your branch. Please have a look at
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 12 Oct 2015 18:15:00 +0000","RE: No speedup in MultiLayerPerceptronClassifier with increase in
 number of cores","Disha Shrivastava <dishu.905@gmail.com>, Mike Hynes <91mbbh@gmail.com>","Hi Disha,

The problem might be as follows. The data that you have might physically reside only on two nodes and Spark launches data-local tasks. As a result, only two workers are used. You might want to force Spark to distribute the data across all nodes, however it does not seem to be worthwhile for this rather small dataset.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com]
Sent: Sunday, October 11, 2015 9:29 AM
To: Mike Hynes
Cc: dev@spark.apache.org; Ulanov, Alexander
Subject: Re: No speedup in MultiLayerPerceptronClassifier with increase in number of cores

Actually I have 5 workers running ( 1 per physical machine) as displayed by the spark UI on spark://IP_of_the_master:7077. I have entered all the physical machines IP in a file named slaves in spark/conf directory and using the script start-all.sh to start the cluster.
My question is that is there a way to control how the tasks are distributed among different workers? To my knowledge it is done by Spark automatically and is not in our control.

On Sun, Oct 11, 2015 at 9:49 PM, Mike Hynes <91mbbh@gmail.com<mailto:91mbbh@gmail.com>> wrote:
Having only 2 workers for 5 machines would be your problem: you
probably want 1 worker per physical machine, which entails running the
spark-daemon.sh script to start a worker on those machines.
The partitioning is agnositic to how many executors are available for
running the tasks, so you can't do scalability tests in the manner
you're thinking by changing the partitioning.

On 10/11/15, Disha Shrivastava <dishu.905@gmail.com<mailto:dishu.905@gmail.com>> wrote:
> Dear Spark developers,
>
> I am trying to study the effect of increasing number of cores ( CPU's) on
> speedup and accuracy ( scalability with spark ANN ) performance for the
> MNIST dataset using ANN implementation provided in the latest spark
> release.
>
> I have formed a cluster of 5 machines with 88 cores in total.The thing
> which is troubling me is that even if I have more than 2 workers in my
> spark cluster the job gets divided only to 2 workers.( executors) which
> Spark takes by default and hence it takes the same time . I know we can set
> the number of partitions manually using sc.parallelize(train_data,10)
> suppose which then divides the data in 10 partitions and all the workers
> are involved in the computation.I am using the below code:
>
>
> import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
> import org.apache.spark.mllib.util.MLUtils
> import org.apache.spark.sql.Row
>
> // Load training data
> val data = MLUtils.loadLibSVMFile(sc, ""data/10000_libsvm"").toDF()
> // Split the data into train and test
> val splits = data.randomSplit(Array(0.7, 0.3), seed = 1234L)
> val train = splits(0)
> val test = splits(1)
> //val tr=sc.parallelize(train,10);
> // specify layers for the neural network:
> // input layer of size 4 (features), two intermediate of size 5 and 4 and
> output of size 3 (classes)
> val layers = Array[Int](784,160,10)
> // create the trainer and set its parameters
> val trainer = new
> MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)
> // train the model
> val model = trainer.fit(train)
> // compute precision on the test set
> val result = model.transform(test)
> val predictionAndLabels = result.select(""prediction"", ""label"")
> val evaluator = new
> MulticlassClassificationEvaluator().setMetricName(""precision"")
> println(""Precision:"" + evaluator.evaluate(predictionAndLabels))
>
> Can you please suggest me how can I ensure that the data/task is divided
> equally to all the worker machines?
>
> Thanks and Regards,
> Disha Shrivastava
> Masters student, IIT Delhi
>

--
Thanks,
Mike

"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 12 Oct 2015 18:20:59 +0000",RE: Operations with cached RDD,Nitin Goyal <nitin2goyal@gmail.com>,"Thank you, Nitin. This does explain the problem. It seems that UI should make this more clear to the user, otherwise it is simply misleading if you read it as it.

From: Nitin Goyal [mailto:nitin2goyal@gmail.com]
Sent: Sunday, October 11, 2015 5:57 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Operations with cached RDD

The problem is not that zipWithIndex is executed again. ""groupBy"" triggered hash partitioning on your keys and a shuffle happened due to that and that's why you are seeing 2 stages. You can confirm this by clicking on latter ""zipWithIndex"" stage and input data has ""(memory)"" written which means input data has been fetched from memory (your cached RDD).

As far as lineage/call site is concerned, I think there was a change in spark 1.3 which excluded some classes from appearing in call site (I know that some Spark SQL related were removed for sure).

Thanks
-Nitin


On Sat, Oct 10, 2015 at 5:05 AM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I am trying to understand how Spark UI displays operation with the cached RDD.

For example, the following code caches an rdd:
>> val rdd = sc.parallelize(1 to 5, 5).zipWithIndex.cache
>> rdd.count
The Jobs tab shows me that the RDD is evaluated:
: 1 count at <console>:24              2015/10/09 16:15:43        0.4 s       1/1
: 0 zipWithIndex at <console> :21             2015/10/09 16:15:38        0.6 s       1/1
An I can observe this rdd in the Storage tab of Spark UI:
: ZippedWithIndexRDD  Memory Deserialized 1x Replicated

Then I want to make an operation over the cached RDD. I run the following code:
>> val g = rdd.groupByKey()
>> g.count
The Jobs tab shows me a new Job:
: 2 count at <console>:26
Inside this Job there are two stages:
: 3 count at <console>:26 +details 2015/10/09 16:16:18   0.2 s       5/5
: 2 zipWithIndex at <console>:21
It shows that zipWithIndex is executed again. It does not seem to be reasonable, because the rdd is cached, and zipWithIndex is already executed previously.

Could you explain why if I perform an operation followed by an action on a cached RDD, then the last operation in the lineage of the cached RDD is shown to be executed in the Spark UI?


Best regards, Alexander



--
Regards
Nitin Goyal
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 12 Oct 2015 11:37:06 -0700",Re: Adding Spark Testing functionality,Patrick Wendell <pwendell@gmail.com>,"So here is a quick description of the current testing bits (I can expand on
it if people are interested) http://bit.ly/pandaPandaPanda .





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
"
Meihua Wu <rotationsymmetry14@gmail.com>,"Mon, 12 Oct 2015 13:24:34 -0700",Flaky Jenkins tests?,dev <dev@spark.apache.org>,"Hi Spark Devs,

I recently encountered several cases that the Jenkin failed tests that
are supposed to be unrelated to my patch. For example, I made a patch
to Spark ML Scala API but some Scala RDD tests failed due to timeout,
or the java_gateway in PySpark fails. Just wondering if these are
isolated cases?

Thanks,

---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 12 Oct 2015 13:32:16 -0700","Re: What is the difference between ml.classification.LogisticRegression
 and mllib.classification.LogisticRegressionWithLBFGS",YiZhi Liu <javelinjs@gmail.com>,"Hi Liu,

In ML, even after extracting the data into RDD, the versions between MLib
and ML are quite different. Due to legacy design, in MLlib, we use Updater
for handling regularization, and this layer of abstraction also does
adaptive step size which is only for SGD. In order to get it working with
LBFGS, some hacks were being done here and there, and in Updater, all the
components including intercept are regularized which is not desirable in
many cases. Also, in the legacy design, it's hard for us to do in-place
standardization to improve the convergency rate. As a result, at some
point, we decide to ditch those abstractions, and customize them for each
algorithms. (Even LiR and LoR use different tricks to have better
performance for numerical optimization, so it's hard to share code at that
time. But I can see the point that we have working code now, so it's time
to try to refactor those code to share more.)


Sincerely,

DB Tsai
----------------------------------------------------------
Blog: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D
<https://pgp.mit.edu/pks/lookup?search=0x59DF55B8AF08DF8D>


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 12 Oct 2015 13:36:27 -0700",Re: Flaky Jenkins tests?,Meihua Wu <rotationsymmetry14@gmail.com>,"You can go to:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN

and see if the test failure(s) you encountered appeared there.

FYI


"
Meihua Wu <rotationsymmetry14@gmail.com>,"Mon, 12 Oct 2015 14:07:26 -0700",Re: Flaky Jenkins tests?,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

Thanks for the info. I have checked but I did not find the failures though.

In my cases, I have seen

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/43531/console]

2) pySpark failure
[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/43553/console]

Traceback (most recent call last):
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"",
line 316, in _get_connection
IndexError: pop from an empty deque




---------------------------------------------------------------------


"
Jakob Odersky <jodersky@gmail.com>,"Mon, 12 Oct 2015 14:36:16 -0700",Live UI,dev@spark.apache.org,"Hi everyone,
I am just getting started working on spark and was thinking of a first way
to contribute whilst still trying to wrap my head around the codebase.

Exploring the web UI, I noticed it is a classic request-response website,
requiring manual refresh to get the latest data.
I think it would be great to have a ""live"" website where data would be
displayed real-time without the need to hit the refresh button. I would be
very interested in contributing this feature if it is acceptable.

Specifically, I was thinking of using websockets with a ScalaJS front-end.
Please let me know if this design would be welcome or if it introduces
unwanted dependencies, I'll be happy to discuss this further in detail.

thanks for your feedback,
--Jakob
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 12 Oct 2015 14:40:07 -0700",Re: Live UI,Jakob Odersky <jodersky@gmail.com>,"I don't think there has been much work done with ScalaJS and Spark (outside
of the April fools press release), but there is a live Web UI project out
of hammerlab with Ryan Williams https://github.com/hammerlab/spree which
you may want to take a look at.





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 12 Oct 2015 14:44:58 -0700",Re: Flaky Jenkins tests?,Meihua Wu <rotationsymmetry14@gmail.com>,"Can you re-submit your PR to trigger a new build - assuming the tests are
flaky ?

If any test fails again, consider contacting the owner of the module for
expert opinion.

Cheers


"
Personal <rosenville@gmail.com>,"Mon, 12 Oct 2015 14:47:55 -0700",Re: Flaky Jenkins tests?,"Meihua Wu <rotationsymmetry14@gmail.com>, Ted Yu
 <yuzhihong@gmail.com>","Just ask Jenkins to retest; no need to open a new PR just to re-trigger the build.



Can you re-submit your PR to trigger a new build - assuming the tests are flaky ?

If any test fails again, consider contacting the owner of the module for expert opinion.

Cheers

Hi Ted,

Thanks for the info. I have checked but I did not find the failures though.

In my cases, I have seen

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/43531/console]

2) pySpark failure
[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/43553/console]

Traceback (most recent call last):
  File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"",
line 316, in _get_connection
IndexError: pop from an empty deque




com>

h

"
Ryan Williams <ryan.blake.williams@gmail.com>,"Mon, 12 Oct 2015 21:49:02 +0000",Re: Live UI,"Jakob Odersky <jodersky@gmail.com>, dev@spark.apache.org","Yea, definitely check out Spree <https://github.com/hammerlab/spree>! It
functions as ""live"" UI, history server, and archival storage of event log
data.

There are pros and cons to building something like it in Spark trunk (and
running it in the Spark driver, presumably) that I've spent a lot of time
thinking about and am happy to talk through (here, offline, or in the Spree
gitter room <https://gitter.im/hammerlab/spree>) if you want to go that
route.



"
Ted Yu <yuzhihong@gmail.com>,"Mon, 12 Oct 2015 14:54:08 -0700",Re: Flaky Jenkins tests?,Personal <rosenville@gmail.com>,"Josh:
We're on the same page.

I used the term 're-submit your PR' which was different from opening new PR.


"
Reynold Xin <rxin@databricks.com>,"Mon, 12 Oct 2015 15:28:38 -0700",a few major changes / improvements for Spark 1.6,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,

It is hard to track everything going on in Spark with so many pull requests
and JIRA tickets. Below are 4 major improvements that will likely be in
Spark 1.6. We have already done prototyping for all of them, and want
feedback on their design.


1. SPARK-9850 Adaptive query execution in Spark
https://issues.apache.org/jira/browse/SPARK-9850

Historically, query planning is done using statistics before the execution
begins. However, the query engine doesn't always have perfect statistics
before execution, especially on fresh data with blackbox UDFs. SPARK-9850
proposes adaptively picking executions plans based on runtime statistics.


2. SPARK-9999 Type-safe API on top of Catalyst/DataFrame
https://issues.apache.org/jira/browse/SPARK-9999

A high level, typed API built on top of Catalyst/DataFrames. This API can
leverage all the work in Project Tungsten to have more robust and efficient
execution (including memory management, code generation, and query
optimization). This API is tentatively named Dataset (i.e. the last D in
RDD).


3. SPARK-10000 Unified memory management (by consolidating cache and
execution memory)
https://issues.apache.org/jira/browse/SPARK-10000

Spark statically divides memory into multiple fractions. The two biggest
ones are cache (aka storage) memory and execution memory. Out of the box,
only 16% of the memory is used for execution. That is to say, if an
application is not using caching, it is wasting majority of the memory
resource with the default configuration. SPARK-10000 proposes a solution to
dynamically allocate memory for these two fractions, and should improve
performance for large workloads without configuration tuning.


4. SPARK-10810 Improved session management in Spark SQL and DataFrames
https://issues.apache.org/jira/browse/SPARK-10810

Session isolation & management is important in SQL query engines. In Spark,
this is slightly more complicated since users can also use DataFrames
interactively beyond SQL. SPARK-10810 implements session management for
both SQL's JDBC/ODBC servers, as well as the DataFrame API.

Most of this work has been merged already in this pull request:
https://github.com/apache/spark/pull/8909
"
Alex Rovner <alex.rovner@magnetic.com>,"Mon, 12 Oct 2015 22:21:09 -0400",SPARK-10617,dev@spark.apache.org,"Would someone mind reviewing?

https://github.com/apache/spark/pull/9004
<https://github.com/apache/spark/pull/9004#issuecomment-146031856>



-- 
*Alex Rovner*
*Director, Data Engineering *
*o:* 646.759.0052

* <http://www.magnetic.com/>*
"
YiZhi Liu <javelinjs@gmail.com>,"Tue, 13 Oct 2015 12:19:36 +0800","Re: What is the difference between ml.classification.LogisticRegression
 and mllib.classification.LogisticRegressionWithLBFGS",DB Tsai <dbtsai@dbtsai.com>,"Hi Tsai,

Thank you for pointing out the implementation details which I missed.
Yes I saw several jira issues with the intercept, regularization and
standardization, I just didn't realize it made such a big impact.
Thanks again.

2015-10-13 4:32 GMT+08:00 DB Tsai <dbtsai@dbtsai.com>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Tue, 13 Oct 2015 16:16:30 +0800",How to split one RDD to small ones according to its key's value,<dev@spark.apache.org>,"Hi everyone,

 

I am facing a requirement that I want to split one RDD into some small ones:

 

but I want to split it according to its Key element value , e.g: for those
its key is X, they gonna be in RDD1; for those its key is Y, they gonna be
in RDD2 , and so on.

 

I know it has a routine call randomSplit but I don't think it meets my need.

 

thanks for your feedback,

-Allen Zhang

"
_abhishek <abhishek.2014@iitg.ernet.in>,"Tue, 13 Oct 2015 05:49:27 -0700 (MST)",Getting started,dev@spark.apache.org,"Hello
I am interested in contributing to apache spark.I am new to open source.Can
someone please help me with how to get started,beginner level bugs etc.
Thanks
Abhishek Kumar 



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 13 Oct 2015 07:02:30 -0700",Re: Getting started,_abhishek <abhishek.2014@iitg.ernet.in>,"Please see
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


"
"""PK Gnanam"" <pk.gnanam@bridgepearl.com>","Tue, 13 Oct 2015 13:35:51 -0400",RE: How to split one RDD to small ones according to its key's value,"=?gb2312?B?J9XF1r7HvyjN+tD5KSc=?= <zzq98736@alibaba-inc.com>,
	<dev@spark.apache.org>","I think you will need to use the partitionBy method 

.partitionBy(no of partitions, lambda that returns a partitioner)

 

Thanks,

PK

 

From: ־ǿ() [mailto:zzq98736@alibaba-inc.com] 
Sent: Tuesday, October 13, 2015 4:17 AM
To: dev@spark.apache.org
Subject: How to split one RDD to small ones according to its key's value

 

Hi everyone,

 

I am facing a requirement that I want to split one RDD into some small ones:

 

but I want to split it according to its Key element value , e.g: for those
its key is X, they gonna be in RDD1; for those its key is Y, they gonna be
in RDD2 , and so on.

 

I know it has a routine call randomSplit but I dont think it meets my
need.

 

thanks for your feedback,

-Allen Zhang

"
Jakob Odersky <jodersky@gmail.com>,"Tue, 13 Oct 2015 16:29:05 -0700",Spark Event Listener,dev@spark.apache.org,"Hi,
I came across the spark listener API while checking out possible UI
extensions recently. I noticed that all events inherit from a sealed trait
`SparkListenerEvent` and that a SparkListener has a corresponding
`onEventXXX(event)` method for every possible event.

Considering that events inherit from a sealed trait and thus all events are
known during compile-time, what is the rationale of using specific methods
for every event rather than a single method that would let a client pattern
match on the type of event?

I don't know the internals of the pattern matcher, but again, considering
events are sealed, I reckon that matching performance should not be an
issue.

thanks,
--Jakob
"
Jakob Odersky <jodersky@gmail.com>,"Tue, 13 Oct 2015 16:36:31 -0700",Re: Spark Event Listener,dev@spark.apache.org,"the path of the source file defining the event API is
`core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala`


"
Josh Rosen <rosenville@gmail.com>,"Tue, 13 Oct 2015 16:37:53 -0700",Re: Spark Event Listener,Jakob Odersky <jodersky@gmail.com>,"Check out SparkFirehoseListener, an adapter which forwards all events to a
single `onEvent` method in order to let you do pattern-matching as you have
described:
https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/SparkFirehoseListener.java


"
Daniel Li <danielli90@gmail.com>,"Tue, 13 Oct 2015 17:14:24 -0700",[Streaming] join events in last 10 minutes,dev@spark.apache.org,"We have a scenario that events from three kafka topics sharing the same
other two topics arrive within 10 minutes of master event arrival. Wrote
pseudo code below. I'd love to hear your thoughts whether I am on the right
track.

    // Scenario
    //     (1) Merging events from Kafka topic1, topic2 and topic 3 sharing
the same keys
    //     (2) Events in topic1 are master events
Topic3 sharing the same key
    //     (4) Most events in topic2 and topic3 will arrive within 10
minutes of the master event arrival
    //
    // Pseudo code
    //     Use 1-minute window of events in topic1, to left-outer-join with
next 10-minute of events from
    //     topic2 and topic3


    // parse the event to form key-value pair
    def parse(v:String) = {
        (v.split("","")(0), v)
    }

    // Create context with 1 minute batch interval
    val sparkConf = new SparkConf().setAppName(""MergeLogs"")
    val ssc = new StreamingContext(sparkConf, Minutes(1))
    ssc.checkpoint(checkpointDirectory)

    // Create direct kafka stream with brokers and topics
    val kafkaParams = Map[String, String](""metadata.broker.list"" -> brokers)

    val stream1 = KafkaUtils.createDirectStream[String, String,
StringDecoder, StringDecoder](
      ssc, kafkaParams, Set(“topic1”)
    stream1.checkpoint(Minutes(5)
    val pairStream1 = stream1.map(_._2).map(s => parse(s))

    val stream2 = KafkaUtils.createDirectStream[String, String,
StringDecoder, StringDecoder](
      ssc, kafkaParams, Set(“topic2”)
    stream2.checkpoint(Minutes(5)
    val pairStream2 = stream2.map(_._2).map(s => parse(s))

    val stream3 = KafkaUtils.createDirectStream[String, String,
StringDecoder, StringDecoder](
      ssc, kafkaParams, Set(“topic3”)
    stream3.checkpoint(Minutes(5)
    val pairStream3 = stream3.map(_._2).map(s => parse(s))

    // load 1 minute of master events from topic 1
    val windowedStream1 = pairStream1.window(Minutes(1))

    // load 10 minutes of topic1 and topic2
    val windowedStream2 = pairStream2.window(Minutes(10), Minutes(1))
    val windowedStream3 = pairStream3.window(Minutes(10), Minutes(1))

    // lefter join topic1 with topic2 and topic3
    *val joinedStream =
windowedStream1.leftOuterJoin(windowedStream2).leftOuterJoin(windowedStream3)*

    // dump merged events
    joinedStream.foreachRDD { rdd =>
        val connection = createNewConnection()  // executed at the driver
        rdd.foreach { record =>
            connection.send(record) // executed at the worker
    }

    // Start the computation
    val ssc = StreamingContext.getOrCreate(checkpointDirectory,
      () => {
        createContext(ip, port, outputPath, checkpointDirectory)
      })
    ssc.start()
    ssc.awaitTermination()

thx
Daniel
"
canan chen <ccnfdu@gmail.com>,"Wed, 14 Oct 2015 10:50:47 +0800",When does python program started in pyspark,"spark users <user@spark.apache.org>, dev@spark.apache.org","I look at the source code of spark, but didn't find where python program is
started in python.

It seems spark-submit will call PythonGatewayServer, but where is python
program started ?

Thanks
"
skaarthik oss <skaarthik.oss@gmail.com>,"Tue, 13 Oct 2015 21:46:13 -0700",Re: When does python program started in pyspark,canan chen <ccnfdu@gmail.com>,"See PythonRunner @
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala


"
canan chen <ccnfdu@gmail.com>,"Wed, 14 Oct 2015 12:58:52 +0800",Re: When does python program started in pyspark,skaarthik oss <skaarthik.oss@gmail.com>,"I think PythonRunner is launched when executing python script.
PythonGatewayServer is entry point for python spark shell


if (args.isPython && deployMode == CLIENT) {
  if (args.primaryResource == PYSPARK_SHELL) {
    args.mainClass = ""org.apache.spark.api.python.PythonGatewayServer""
  } else {
    // If a python file is provided, add it to the child arguments and
list of files to deploy.
    // Usage: PythonAppRunner <main python file> <extra python files>
[app arguments]
    args.mainClass = ""org.apache.spark.deploy.PythonRunner""
    args.childArgs = ArrayBuffer(args.primaryResource, args.pyFiles)
++ args.childArgs
    if (clusterManager != YARN) {
      // The YARN backend distributes the primary file differently, so
don't merge it.
      args.files = mergeFileLists(args.files, args.primaryResource)
    }
  }



"
Sean Owen <sowen@cloudera.com>,"Wed, 14 Oct 2015 10:13:15 +0100","Is ""mllib"" no longer Experimental?",dev <dev@spark.apache.org>,"Someone asked, is ""ML pipelines"" stable? I said, no, most of the key
classes are still marked @Experimental, which matches my expression that
things may still be subject to change.

But then, I see that MLlib classes, which are de facto not seeing much
further work and no API change, are also mostly marked @Experimental. If,
generally, no more significant work is going into MLlib classes, is it time
to remove most or all of those labels, to keep it meaningful?

Sean
"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Wed, 14 Oct 2015 15:46:08 +0530","Contributing Receiver based Low Level Kafka Consumer from
 Spark-Packages to Apache Spark Project","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have raised a JIRA ( https://issues.apache.org/jira/browse/SPARK-11045)
to track the discussion but also mailing dev group for your opinion. There
are some discussions already happened in Jira and love to hear what others
think. You can directly comment against the Jira if you wish.

This kafka consumer is around for a while in spark-packages (
http://spark-packages.org/package/dibbhatt/kafka-spark-consumer ) and I see
many people started using it , I am now thinking of contributing back to
Apache Spark core project so that it can get better support ,visibility and
adoption.

Few Point about this consumer

*Why this is needed , and how I position this Consumer : *

This Consumer is NOT the replacement for existing DirectStream API.
Ordering"" of messages . But to achieve this DirectStream comes with an
overhead. The overhead of maintaining the offset externally
, limited parallelism while processing the RDD ( as the RDD partition is
same as Kafka Partition ), and higher latency while processing RDD ( as
messages are fetched when RDD is processed) . There are many who does not
managed in external store ( say HBase),  and want more parallelism and
lower latency in their Streaming channel . At this point Spark does not
have a better fallback option available in terms of Receiver Based API.
Present Receiver Based API use Kafka High Level API which is low
performance and has serious issue. [For this reason Kafka is coming up with
new High Level Consumer API in 0.9]

The Consumer which I implemented is using the Kafka Low Level API which
gives more performance.  This consumer has built in fault tolerant features
for all failures recovery. This Consumer extended the code from Storm Kafka
Spout which is being around for some time and has matured over the years
and has all built in Kafka fault tolerant capabilities. This same Kafka
consumer for spark is being running in various production scenarios
presently and already being adopted by many in the spark community.

*Why Can't we fix existing Receiver based API in Spark* :

This is not possible unless you move to Kafka Low Level API . Or let wait
for Kafka 0.9 where they are re-writing the HighLevel Consumer API and
built another consumer for Kafka 0.9 customers .
This approach seems to be not good in my opinion. The Kafka Low Level API
which I used in my consumer ( and even DirectStream uses ) will not going
to be deprecated in near future. So if Kafka Consumer for Spark is using
Low Level API for Receiver based mode, that will make sure all Kafka
Customers who are presently in 0.8.x or who will use 0.9 , benefited form
this same API. This will give easier maintenance to manage single API for
any Kafka versions. Also this will make sure both Direct Stream and
Receiver mode utilize same Kafka API.

*Concerns around Low Level API Complexity*

Yes, implementing a reliable consumer using Kafka Low Level consumer API is
complex. But same has been done for Strom -Kafka Spout and has been stable
for quite some time. This consumer for Spark is battle tested in various
production loads and gives much better performance than existing Kafka
Consumers for Spark and has better fault tolerant approach than existing
Receiver based mode. I do not think having a complex code should be a major
concern to deny a stable and high performance consumer for community. I am
okay if anyone interested to benchmark against other Kafka Consumers for
Spark and do various fault testing to make sure what I am saying is correct.

*Why can't this consumer continue to be in Spark-Package ?*

This can be possible. But what I see , many customer who want to fallback
""Global Ordering"" , seems to little tentative using a spark-package library
for their critical streaming pipeline. And they are forced to use faulty
and buggy Kafka High Level API based mode. This consumer being part of
Spark project will give much higher adoption and support from community.

*Some Major features around this consumer :*

This consumer is controlling the rate limit by maintaining the constant
Block size where as default rate limiting in other Spark consumers are done
by number of messages. This is an issue when Kafka has messages of
different sizes and there is no deterministic way to know the actual block
sizes and memory utilization if rate control done by number of messages.

This consumer has in-built PID controller which controls the Rate of
consumption again by modifying the block size and consume only that much
amount of messages needed from Kafka . In default Spark consumer , it
fetches chunk of messages and then apply throttle to control the rate.
Which can lead to excess I/O while consuming from Kafka.

There are other features in this Consumer which we can discuss at length
once we are convinced that Kafka Low Level API is way to go.

Regards,
Dibyendu
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 14 Oct 2015 09:39:44 -0700","Re: Is ""mllib"" no longer Experimental?",Sean Owen <sowen@cloudera.com>,"I would tend to agree with this approach. We should audit all
@Experimenetal labels before the 1.6 release and clear them out when
appropriate.

- Patrick


"
Reynold Xin <rxin@databricks.com>,"Wed, 14 Oct 2015 11:39:53 -0700",Re: [SQL] Memory leak with spark streaming and spark sql in spark 1.5.1,"Terry Hoo <hujie.eagle@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>",#NAME?
Reynold Xin <rxin@databricks.com>,"Wed, 14 Oct 2015 12:00:37 -0700",If you use Spark 1.5 and disabled Tungsten mode ...,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Can you reply to this email and provide us with reasons why you disable it?

Thanks.
"
Jakob Odersky <jodersky@gmail.com>,"Wed, 14 Oct 2015 12:13:01 -0700",Status of SBT Build,dev@spark.apache.org,"Hi everyone,

I've been having trouble building Spark with SBT recently. Scala 2.11
doesn't work and in all cases I get large amounts of warnings and even
errors on tests.

I was therefore wondering what the official status of spark with sbt is? Is
it very new and still buggy or unmaintained and ""falling to pieces""?

In any case, I would be glad to help with any issues on setting up a clean
and working build with sbt.

thanks,
--Jakob
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 14 Oct 2015 12:20:02 -0700",Re: Status of SBT Build,Jakob Odersky <jodersky@gmail.com>,"Hi Jakob,

There is a temporary issue with the Scala 2.11 build in SBT. The problem is
this wasn't previously covered by our automated tests so it broke without
us knowing - this has been actively discussed on the dev list in the last
24 hours. I am trying to get it working in our test harness today.

In terms of fixing the underlying issues, I am not sure whether there is a
JIRA for it yet, but we should make one if not. Does anyone know?

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 14 Oct 2015 12:59:55 -0700",Re: Status of SBT Build,Jakob Odersky <jodersky@gmail.com>,"Jakob this is now being tested by our harness. I've created a JIRA for the
issue, if you want to take a stab at fixing these, that would be great:

https://issues.apache.org/jira/browse/SPARK-11110

- Patrick


"
zhaoxia <zhaoxiahust@gmail.com>,"Wed, 14 Oct 2015 13:05:40 -0700 (MST)",Strange spark problems among different versions,dev@spark.apache.org,"Hi. I try to run the Spark Pi on the cluster, some strange errors happen and
I do not know what cause the error. Although I have posted this error to the
user@spark, I think it may be not a simple configuration error and the
developers may know it well.
  
  When I am using the hadoop2.6 and spark-1.5.1-bin-hadoop2.6 the error log
is below: 


118 10/01/01 11:59:14 ERROR yarn.ApplicationMaster: User class threw
exception: java.lang.reflect.InvocationTargetException 

119 java.lang.reflect.InvocationTargetException 

Caused by: java.lang.IllegalArgumentException:
java.lang.UnsatisfiedLinkError:
/opt/hadoop/tmp/nm-local-dir/usercache/root/appcache/application_1444839345484_0006/container_1444839345484_0006_01_000001/tmp/snappy-1.0.4.1-8378427e-4d5c-42b1-ae49-c9600
c204bd7-libsnappyjava.so:
/opt/hadoop/tmp/nm-local-dir/usercache/root/appcache/application_1444839345484_0006/container_14448
39345484_0006_01_000001/tmp/*snappy-1.0.4.1-8378427e-4d5c-42b1-ae49-c9600c204bd7-libsnappyjava.so:
cannot open shared object file: No such file or directory*



  When I am using the hadoop2.6 and spark-1.3.0-bin-hadoop2.4 the error
seems different and the log is below: 


*   108 org.apache.spark.SparkException: Job aborted due to stage failure:
Task serialization failed: java.lang.reflect.InvocationTargetException*
    109 sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method) 
    110
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) 
    111
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 
    112 java.lang.reflect.Constructor.newInstance(Constructor.java:408) 
    113
org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:68) 
    114
org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:60) 
    115
org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$setConf(TorrentBroadcast.scala:73) 
    116
org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:79) 
    117
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34) 
    118
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29) 
    119
org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62) 
    120 org.apache.spark.SparkContext.broadcast(SparkContext.scala:1051) 
    121
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:839) 
    122
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:778) 
    123
org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:762) 
    124
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362) 
    125
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) 
    126 org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) 


What causes the error? the java compatibility or the hadoop compatibility? 
Thank you for your help 



--

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Wed, 14 Oct 2015 20:18:42 +0000",Gradient Descent with large model size,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I have noticed that Gradient Descent is Spark MLlib takes long time if the model is large. It is implemented with TreeAggregate. I've extracted the code from GradientDescent.scala to perform the benchmark. It allocates the Array of a given size and the aggregates it:

val dataSize = 12000000
val n = 5
val maxIterations = 3
val rdd = sc.parallelize(0 until n, n).cache()
rdd.count()
var avgTime = 0.0
for (i <- 1 to maxIterations) {
  val start = System.nanoTime()
  val result = rdd.treeAggregate((new Array[Double](dataSize), 0.0, 0L))(
        seqOp = (c, v) => {
          // c: (grad, loss, count)
          val l = 0.0
          (c._1, c._2 + l, c._3 + 1)
        },
        combOp = (c1, c2) => {
          // c: (grad, loss, count)
          (c1._1, c1._2 + c2._2, c1._3 + c2._3)
        })
  avgTime += (System.nanoTime() - start) / 1e9
  assert(result._1.length == dataSize)
}
println(""Avg time: "" + avgTime / maxIterations)

If I run on my cluster of 1 master and 5 workers, I get the following results (given the array size = 12M):
n = 1: Avg time: 4.555709667333333
n = 2 Avg time: 7.059724584666667
n = 3 Avg time: 9.937117377666667
n = 4 Avg time: 12.687526233
n = 5 Avg time: 12.939526129666667

Could you explain why the time becomes so big? The data transfer of 12M array of double should take ~ 1 second in 1Gbit network. There might be other overheads, however not that big as I observe.
Best regards, Alexander
"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 14 Oct 2015 15:06:21 -0700",Re: [Streaming] join events in last 10 minutes,"Daniel Li <danielli90@gmail.com>, Tathagata Das <tdas@databricks.com>","Hi TD,

The scenario here is to let events from topic1 wait a fixed 10 minutes for
events with same key from topic2 to come and left outer join them by the key

does the query do what is expected? if not, what is the right way to
achieve this?

thanks,
Renyi.


ht
r
am3)*
er
"
Arijit <arijitt@live.com>,"Wed, 14 Oct 2015 16:23:03 -0600","=?windows-1256?Q?RE:_Unders?= =?windows-1256?Q?tanding_co?=
 =?windows-1256?Q?de/closure?= =?windows-1256?Q?_shipment_?=
 =?windows-1256?Q?to_Spark_w?= =?windows-1256?Q?orkers=FE?=",Xiao Li <gatorsmile@gmail.com>,"Hi Xiao,
 
Thank you very much for the pointers. I looked into the part of the code. I now understand how the main method is invoked. Still not clear how is the code distributed to the executors. Is it the whole jar or some serialized object. I was expecting to see the part of the code where the closures are serialized and shipped. Maybe I am missing something.
 
Thanks again, Arijit
 
Date: Thu, 8 Oct 2015 10:26:55 -0700
Subject: Re: Understanding code/closure shipment to Spark workers
From: gatorsmile@gmail.com
To: arijitt@live.com
CC: dev@spark.apache.org

Hi, Arijit, 
The code flow of spark-submit is simple. 
Enter the main function of SparkSubmit.scala     --> case SparkSubmitAction.SUBMIT => submit(appArgs)    --> doRunMain() in function submit() in the same file 
    --> runMain(childArgs,...) in the same file    --> mainMethod.invoke(null, childArgs.toArray)  in the same file 
Function Invoke() is provided by JAVA Reflection for invoking the main function of your JAR. 
Hopefully, it can help you understand the problem. 
Thanks, 
Xiao Li

2015-10-07 16:47 GMT-07:00 Arijit <arijitt@live.com>:



 Hi,
 
I want to understand the code flow starting from the Spark jar that I submit through spark-submit, how does Spark identify and extract the closures, clean and serialize them and ship them to workers to execute as tasks. Can someone point me to any documentation or a pointer to the source code path to help me understand this.
 
Thanks, Arijit 		 	   		  



 		 	   		  "
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 15 Oct 2015 02:10:41 +0000","SPARK_MASTER_IP actually expects a DNS name, not IP address",Spark dev list <dev@spark.apache.org>,"I’m setting the Spark master address via the SPARK_MASTER_IP environment
variable in spark-env.sh, like spark-ec2 does
<https://github.com/amplab/spark-ec2/blob/a990752575cd8b0ab25731d7820a55c714798ec3/templates/root/spark/conf/spark-env.sh#L13>
.

The funny thing is that Spark seems to accept this only if the value of
SPARK_MASTER_IP is a DNS name and not an IP address.

When I provide an IP address, I get errors in the log when starting the
master:

15/10/15 01:47:31 ERROR NettyTransport: failed to bind to
/54.210.XX.XX:7077, shutting down Netty transport

(XX is my redaction of the full IP address.)

Am I misunderstanding something about how to use this environment variable?

The spark-env.sh template indicates that either an IP address or a hostname
should work
<https://github.com/apache/spark/blob/4ace4f8a9c91beb21a0077e12b75637a4560a542/conf/spark-env.sh.template#L49>,
but my testing shows that only hostnames work.

Nick
​
"
Jeff Zhang <zjffdu@gmail.com>,"Thu, 15 Oct 2015 10:26:30 +0800",Should enforce the uniqueness of field name in DataFrame ?,dev@spark.apache.org,"Currently seems DataFrame doesn't enforce the uniqueness of field name. So
it is possible to have same fields in DataFrame. It usually happens after
join especially self-join. Although user can rename the column names before
join, or rename the column names after join (DataFrame#withColunmRenamed is
not sufficient for now).  In hive, the ambiguous name can be resolved by
using the table name as prefix, but seems DataFrame don't support it ( I
mean DataFrame API rather than SparkSQL). I think we have 2 options here
1. Enforce the uniqueness of field name in DataFrame, so that the following
operations would not cause ambiguous column reference
2. Provide DataFrame#withColunmsRenamed(oldColumns:Seq[String],
newColumns:Seq[String]) to allow change schema names

For now, I would prefer option 2 which is more easier to implement and keep
compatibility.


val df = ...        // schema (name, age)
val df2 = df.join(df, ""name"")   // schema (name, age, age)
df2.select(""age"")   // ambiguous column reference.

-- 
Best Regards

Jeff Zhang
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 14 Oct 2015 21:37:03 -0700","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",Nicholas Chammas <nicholas.chammas@gmail.com>,"Some old bits:

http://stackoverflow.com/questions/28162991/cant-run-spark-1-2-in-standalone-mode-on-mac
http://stackoverflow.com/questions/29412157/passing-hostname-to-netty

FYI


ronment
714798ec3/templates/root/spark/conf/spark-env.sh#L13>
077, shutting down Netty transport
e?
0a542/conf/spark-env.sh.template#L49>,
"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 15 Oct 2015 14:31:04 +0800",Re: [SQL] Memory leak with spark streaming and spark sql in spark 1.5.1,Terry Hoo <hujie.eagle@gmail.com>,"Thanks for reporting it Terry. I submitted a PR to fix it:
https://github.com/apache/spark/pull/9132

Best Regards,
Shixiong Zhu

2015-10-15 2:39 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
Joseph Bradley <joseph@databricks.com>,"Wed, 14 Oct 2015 23:34:59 -0700",Re: Gradient Descent with large model size,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","For those numbers of partitions, I don't think you'll actually use tree
aggregation.  The number of partitions needs to be over a certain threshold
(>= 7) before treeAggregate really operates on a tree structure:
https://github.com/apache/spark/blob/9808052b5adfed7dafd6c1b3971b998e45b2799a/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1100

Do you see a slower increase in running time with more partitions?  For 5
partitions, do you find things improve if you tell treeAggregate to use
depth > 2?

Joseph

m

e
ted the
e
)(
"
Reynold Xin <rxin@databricks.com>,"Wed, 14 Oct 2015 23:58:50 -0700",Re: Should enforce the uniqueness of field name in DataFrame ?,Jeff Zhang <zjffdu@gmail.com>,"That could break a lot of applications. In particular, a lot of input data
sources (csv, json) don't have clean schema, and can have duplicate column
names.

For the case of join, maybe a better solution is to ask the left/right
prefix/suffix in the user code, similar to what Pandas does.


"
Xiao Li <gatorsmile@gmail.com>,"Thu, 15 Oct 2015 00:05:29 -0700",Re: Should enforce the uniqueness of field name in DataFrame ?,Reynold Xin <rxin@databricks.com>,"True. As long as we can ensure the correct message are printed out, users
can correct their app easily. For example, Reference 'name' is ambiguous,
could be: name#1, name#5.;

Thanks,

Xiao Li

2015-10-14 23:58 GMT-07:00 Reynold Xin <rxin@databricks.com>:

"
Koert Kuipers <koert@tresata.com>,"Thu, 15 Oct 2015 03:28:11 -0400",Re: Should enforce the uniqueness of field name in DataFrame ?,Xiao Li <gatorsmile@gmail.com>,"if DataFrame aspires to be more than a vehicle for SQL then i think it
would be mistake to allow multiple column names. it is very confusing.
pandas indeed allows this and it has led to many bugs. R does not allow it
for data.frame (it renames the name dupes).

i would consider a csv with duplicate column names invalid and it should
not be loaded, or if it is loaded dupes should be renamed (e.g. append a
""1"" to the name).

DataFrame did check for duplicate column names until Sep 2014, but then the
check got pushed into the SQL planner making DataFrame standalone (so
without SQL) less useful as an API.

i filed a jira about this a while ago here:
https://issues.apache.org/jira/browse/SPARK-8817




"
Lars Francke <lars.francke@gmail.com>,"Thu, 15 Oct 2015 08:46:28 +0100","=?UTF-8?Q?Re=3A_Understanding_code=2Fclosure_shipment_to_Spark_wor?=
	=?UTF-8?Q?kers=E2=80=8F?=",Arijit <arijitt@live.com>,"Hi Arijit,

my understanding is the following:

RDD actions will at some point call the runJob method of a SparkContext
That runJob method calls the clean method which in turn calls
ClosureCleaner.clean which removes unneeded stuff from closures and also
checks whether they are serializable.
The next step is to submit it to the DAGScheduler which in turn has a
closureSerializer which is just an instance of a
normal org.apache.spark.serializer.Serializer:

val closureSerializer =
instantiateClassFromConf[Serializer](""spark.closure.serializer"",
""org.apache.spark.serializer.JavaSerializer"")

This serializes the closure to a byte array which can then be shipped.

I might be wrong here but I hope it helps :)

Cheers,
Lars




ce
"
Fazlan Nazeem <fazlann@wso2.com>,"Thu, 15 Oct 2015 14:55:43 +0530",PMML export for LinearRegressionModel,dev@spark.apache.org,"Hi

I am trying to export a LinearRegressionModel in PMML format. According to
the following resource[1] PMML export is supported for
LinearRegressionModel.

[1] https://spark.apache.org/docs/latest/mllib-pmml-model-export.html

But there is *no* *toPMML* method in *LinearRegressionModel* class although
LogisticRegressionModel, ReidgeRegressionModel,SVMModel etc has toPMML
method.

Can someone explain what is the issue here?

-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
canan chen <ccnfdu@gmail.com>,"Thu, 15 Oct 2015 17:41:07 +0800",Re: PMML export for LinearRegressionModel,Fazlan Nazeem <fazlann@wso2.com>,"The method toPMML is in trait PMMLExportable

*LinearRegressionModel has this trait, you should be able to call *
*LinearRegressionModel#toPMML*


"
Fazlan Nazeem <fazlann@wso2.com>,"Thu, 15 Oct 2015 15:23:38 +0530",Re: PMML export for LinearRegressionModel,canan chen <ccnfdu@gmail.com>,"This is the API doc for LinearRegressionModel. It does not implement
PMMLExportable

https://spark.apache.org/docs/latest/api/java/index.html




-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
Fazlan Nazeem <fazlann@wso2.com>,"Thu, 15 Oct 2015 15:28:22 +0530",Re: PMML export for LinearRegressionModel,canan chen <ccnfdu@gmail.com>,"Ok It turns out I was using the wrong LinearRegressionModel which was
in  package
org.apache.spark.ml.regression;.







-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
Kybe67 <beck.gael@gmail.com>,"Thu, 15 Oct 2015 07:58:54 -0700 (MST)",MLlib Contribution,dev@spark.apache.org,"Hi, i made a clustering algorithm in Scala/Spark during my internship, i
would like to contribute to MLlib, but i don't know how, i do my best to
follow this instructions :

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-MLlib-specificContributionGuidelines
https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide

The algorithm is the Mean Shift. It works fine on multivariate
muldimensional datasets, especially on image. I think some works should be
done but i don't know what i should do.

Thank you for your support and for the amazing Spark project.



--

---------------------------------------------------------------------


"
mkhaitman <mark.khaitman@chango.com>,"Thu, 15 Oct 2015 09:33:23 -0700 (MST)",Re: If you use Spark 1.5 and disabled Tungsten mode ...,dev@spark.apache.org,"Are you referring to spark.shuffle.manager=tungsten-sort? If so, we saw the
default value as still being as the regular sort, and since it was only
first introduced in 1.5, were actually waiting a bit to see if anyone
ENABLED it as opposed to DISABLING it since - it's disabled by default! :)

I recall enabling it during testing within our dev environment, but didn't
have a comparable workload and environment to our production cluster, so we
were going to play it safe and wait until 1.6 in case there were any major
changes / regressions that weren't seen during 1.5 testing!

Mark.



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Thu, 15 Oct 2015 09:39:10 -0700",Re: If you use Spark 1.5 and disabled Tungsten mode ...,mkhaitman <mark.khaitman@chango.com>,"To clarify, we're asking about the *spark.sql.tungsten.enabled* flag, which
was introduced in Spark 1.5 and enables Project Tungsten optimizations in
Spark SQL. This option is set to *true* by default in Spark 1.5+ and exists
primarily to allow users to disable the new code paths if they encounter
bugs or performance regressions.

If anyone sets spark.sql.tungsten.enabled=*false *in their SparkConf in
order to *disable* these optimizations, we'd like to hear from you in order
to figure out why you disabled them and to see whether we can make
improvements to allow your workload to run with Tungsten enabled.

Thanks,
Josh


"
mkhaitman <mark.khaitman@chango.com>,"Thu, 15 Oct 2015 09:41:17 -0700 (MST)",Re: If you use Spark 1.5 and disabled Tungsten mode ...,dev@spark.apache.org,"My apologies for mixing up what was being referred to in that case! :)

Mark.





--

---------------------------------------------------------------------


"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 15 Oct 2015 09:47:22 -0700",Network-related environemental problem when running JDBCSuite,Dev <dev@spark.apache.org>,"

I am seeing what look like environmental errors when I try to run a test on
a clean local branch which has been sync'd to the head of the development
trunk. I would appreciate advice about how to debug or hack around this
problem. For the record, the test ran cleanly last week. This is the
experiment I am running:

# build
mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive -Phive-thriftserver clean
package

# run one suite
mvn -Dhadoop.version=2.4.0 -DwildcardSuites=JDBCSuite

The test bombs out before getting to JDBCSuite. I see this summary at the
end...

[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS
[  2.023 s]
[INFO] Spark Project Test Tags ............................ SUCCESS
[  1.924 s]
[INFO] Spark Project Launcher ............................. SUCCESS
[  5.837 s]
[INFO] Spark Project Networking ........................... SUCCESS
[ 12.498 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [01:28
min]
[INFO] Spark Project Unsafe ............................... SUCCESS [01:09
min]
[INFO] Spark Project Core ................................. SUCCESS [02:45
min]
[INFO] Spark Project Bagel ................................ SUCCESS
[ 30.182 s]
[INFO] Spark Project GraphX ............................... SUCCESS
[ 59.002 s]
[INFO] Spark Project Streaming ............................ FAILURE [06:21
min]
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Spark Project External Twitter ..................... SKIPPED
[INFO] Spark Project External Flume Sink .................. SKIPPED
[INFO] Spark Project External Flume ....................... SKIPPED
[INFO] Spark Project External Flume Assembly .............. SKIPPED
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External MQTT Assembly ............... SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project External Kafka Assembly .............. SKIPPED
[INFO]
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 13:37 min
[INFO] Finished at: 2015-10-15T09:03:06-07:00
[INFO] Final Memory: 69M/793M
[INFO]
------------------------------------------------------------------------
[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test)
on project spark-streaming_2.10: There are test failures.
[ERROR]
[ERROR] Please refer
to /Users/rhillegas/spark/spark/streaming/target/surefire-reports for the
individual test results.
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions,
please read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the
command
[ERROR]   mvn <goals> -rf :spark-streaming_2.10




following tests failed...

org.apache.spark.streaming.JavaAPISuite.txt
org.apache.spark.streaming.JavaReceiverAPISuite.txt

...with this error:

java.net.BindException: Failed to bind to: /9.52.158.156:0: Service
'sparkDriver' failed after 100 retries!
	at org.jboss.netty.bootstrap.ServerBootstrap.bind
(ServerBootstrap.java:272)
	at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply
(NettyTransport.scala:393)
	at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply
(NettyTransport.scala:389)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Success.map(Try.scala:206)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch
(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext
(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run
(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec
(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec
(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask
(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker
(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run
(ForkJoinWorkerThread.java:107)


It is suggested that there might be a problem with my /etc/hosts, according
to
http://stackoverflow.com/questions/29906686/failed-to-bind-to-spark-master-using-a-remote-cluster-with-two-workers
. But /etc/hosts looks fine to me:

bash-3.2$ cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1	localhost
255.255.255.255	broadcasthost
::1             localhost

Is there some environmental variable, config file setting, or JVM system
property which will hack around this problem? Any advice would be
appreciated.


Thanks,
-Rick"
Disha Shrivastava <dishu.905@gmail.com>,"Thu, 15 Oct 2015 22:42:57 +0530","Re: No speedup in MultiLayerPerceptronClassifier with increase in
 number of cores","""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi Alexander,

Thanks for your reply.Actually I am working with a modified version of the
actual MNIST dataset ( maximum samples = 8.2 M)
https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html. I
have been running different sized versions*( 10000,100000,500000,1M,8M
samples)* on different number of workers(*1,2,3,4,5*) and obtaining
results. I have observed that when I specify partitions manually, the
cluster actually shows scalability performance with decrease in time taken
with increase in number of cores. With default settings, Spark
automatically divides the data into partitions ( I guess based on data
size,etc) and this number is fixed irrespective of the actual number of
workers present in the cluster.

As per the data residing on two machines is concerned, I am reading the
data from HDFS ( multi-node hadoop cluster setup done for all worker
machines). With default number of partitions, Spark gives better results (
less time and better accuracy) as compared to when I manually set the
number of partitions; but the problem here is that I can't observe the
effect of scalability.

My question is that if I have to obtain both scalability and optimality how
should I go about it in Spark? Because clearly in my case, scalable
implementation is not necessarily optimal. Here, by scalability I mean that
if I increase he number of worker machines , I should get a better
performance ( less time taken).

Thanks and Regards
Disha


"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 15 Oct 2015 10:50:55 -0700",Re: Network-related environemental problem when running JDBCSuite,Richard Hillegas <rhilleg@us.ibm.com>,"
For the record, I get the same error when I simply try to boot the spark
shell:

bash-3.2$ bin/spark-shell
log4j:WARN No appenders could be found for logger
(org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for
more info.
Using Spark's repl log4j profile:
org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel(""INFO"")
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java
1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port
0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0,
shutting down Netty transport
15/10/15 10:49:09 ERROR SparkContext: Error initializing SparkContext.
java.net.BindException: Failed to bind to: /9.52.158.156:0: Service
'sparkDriver' failed after 16 retries!
	at org.jboss.netty.bootstrap.ServerBootstrap.bind
(ServerBootstrap.java:272)
	at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply
(NettyTransport.scala:393)
	at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply
(NettyTransport.scala:389)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Success.map(Try.scala:206)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch
(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext
(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run
(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec
(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec
(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask
(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker
(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run
(ForkJoinWorkerThread.java:107)
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated
abrubtly. Attempting to shut down transports
java.net.BindException: Failed to bind to: /9.52.158.156:0: Service
'sparkDriver' failed after 16 retries!
	at org.jboss.netty.bootstrap.ServerBootstrap.bind
(ServerBootstrap.java:272)
	at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply
(NettyTransport.scala:393)
	at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply
(NettyTransport.scala:389)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Success.map(Try.scala:206)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch
(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply
(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext
(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run
(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec
(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec
(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask
(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker
(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run
(ForkJoinWorkerThread.java:107)

java.lang.NullPointerException
	at org.apache.spark.sql.SQLContext$.createListenerAndUI
(SQLContext.scala:1323)
	at org.apache.spark.sql.hive.HiveContext.<init>
(HiveContext.scala:100)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance
(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance
(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.spark.repl.SparkILoop.createSQLContext
(SparkILoop.scala:1028)
	at $iwC$$iwC.<init>(<console>:9)
	at $iwC.<init>(<console>:18)
	at <init>(<console>:20)
	at .<init>(<console>:24)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke
(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke
(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call
(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun
(SparkIMain.scala:1340)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1
(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1
(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith
(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark
$1.apply(SparkILoopInit.scala:132)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark
$1.apply(SparkILoopInit.scala:124)
	at org.apache.spark.repl.SparkIMain.beQuietDuring
(SparkIMain.scala:324)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark
(SparkILoopInit.scala:124)
	at org.apache.spark.repl.SparkILoop.initializeSpark
(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl
$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp
(SparkILoop.scala:974)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks
(SparkILoopInit.scala:159)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization
(SparkILoopInit.scala:108)
	at org.apache.spark.repl.SparkILoop.postInitialization
(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl
$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl
$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl
$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader
(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$
$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke
(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke
(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy
$SparkSubmit$$runMain(SparkSubmit.scala:680)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1
(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

<console>:10: error: not found: value sqlContext
       import sqlContext.implicits._
              ^
<console>:10: error: not found: value sqlContext
       import sqlContext.sql

Thanks,
Rick Hillegas



Richard Hillegas/San Francisco/IBM@IBMUS wrote on 10/15/2015 09:47:22 AM:

e
package
---
---
---
---
.
(ServerBootstrap.java:272)
:72)
0)
(ForkJoinPool.java:1979)
"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 15 Oct 2015 11:15:29 -0700",Re: Network-related environemental problem when running JDBCSuite,Dev <dev@spark.apache.org>,"
Continuing this lively conversation with myself (hopefully this archived
thread may be useful to someone else in the future):

I set the following environment variable as recommended by this page:
http://stackoverflow.com/questions/29906686/failed-to-bind-to-spark-master-using-a-remote-cluster-with-two-workers

export SPARK_LOCAL_IP=127.0.0.1

Then I got errors related to booting the metastore_db. So I deleted that
directory. After that I was able to run spark-shell again.

Now let's see if this hack fixes the tests...


Thanks,
Rick Hillegas



Richard Hillegas/San Francisco/IBM@IBMUS wrote on 10/15/2015 10:50:55 AM:

JDBCSuite

1.8.0_60)
.
(ServerBootstrap.java:272)
:72)
0)
(ForkJoinPool.java:1979)
(ServerBootstrap.java:272)
:72)
0)
(ForkJoinPool.java:1979)
(SQLContext.scala:1323)
)
od)
(SparkILoop.scala:1028)
(SparkIMain.scala:1065)
(SparkIMain.scala:1340)
840)
(SparkILoop.scala:857)
4)
64)
(SparkILoop.scala:64)
(SparkSubmit.scala:180)

 AM:
ite


n
package
------------------------------------------------------------------------



























------------------------------------------------------------------------
------------------------------------------------------------------------
------------------------------------------------------------------------
h
ng.
h


(ServerBootstrap.java:272)
(BlockContext.scala:72)

260)
(ForkJoinPool.java:1979)


"
<sgoodwin@cfl.rr.com>,"Thu, 15 Oct 2015 14:21:09 -0400",Re: Network-related environemental problem when running JDBCSuite,"""Richard Hillegas"" <rhilleg@us.ibm.com>","Rick,

Try setting the environment variable SPARK_LOCAL_IP=127.0.0.1 in your spark-env.conf (if not done yet) ...

Regards,

- Steve

From: Richard Hillegas 
Sent: Thursday, October 15, 2015 1:50 PM
To: Richard Hillegas 
Cc: Dev 
Subject: Re: Network-related environemental problem when running JDBCSuite

For the record, I get the same error when I simply try to boot the spark shell:

bash-3.2$ bin/spark-shell
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel(""INFO"")
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
Type in expressions to have them evaluated.
Type :help for more information.
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 WARN Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
15/10/15 10:49:09 ERROR NettyTransport: failed to bind to /9.52.158.156:0, shutting down Netty transport
15/10/15 10:49:09 ERROR SparkContext: Error initializing SparkContext.
java.net.BindException: Failed to bind to: /9.52.158.156:0: Service 'sparkDriver' failed after 16 retries!
at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393)
at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389)
at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
at scala.util.Try$.apply(Try.scala:161)
at scala.util.Success.map(Try.scala:206)
at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
15/10/15 10:49:09 ERROR Remoting: Remoting system has been terminated abrubtly. Attempting to shut down transports
java.net.BindException: Failed to bind to: /9.52.158.156:0: Service 'sparkDriver' failed after 16 retries!
at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393)
at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389)
at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)
at scala.util.Try$.apply(Try.scala:161)
at scala.util.Success.map(Try.scala:206)
at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

java.lang.NullPointerException
at org.apache.spark.sql.SQLContext$.createListenerAndUI(SQLContext.scala:1323)
at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:100)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
at org.apache.spark.repl.SparkILoop.createSQLContext(SparkILoop.scala:1028)
at $iwC$$iwC.<init>(<console>:9)
at $iwC.<init>(<console>:18)
at <init>(<console>:20)
at .<init>(<console>:24)
at .<clinit>(<console>)
at .<init>(<console>:7)
at .<clinit>(<console>)
at $print(<console>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)
at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:132)
at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:124)
at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:124)
at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:64)
at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:159)
at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:108)
at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:64)
at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
at org.apache.spark.repl.Main$.main(Main.scala:31)
at org.apache.spark.repl.Main.main(Main.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:680)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

<console>:10: error: not found: value sqlContext
       import sqlContext.implicits._
              ^
<console>:10: error: not found: value sqlContext
       import sqlContext.sql

Thanks,
Rick Hillegas



Richard Hillegas/San Francisco/IBM@IBMUS wrote on 10/15/2015 09:47:22 AM:

package
------------------------------------------------------------------------
------------------------------------------------------------------------
------------------------------------------------------------------------
------------------------------------------------------------------------
org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
"
Michael Armbrust <michael@databricks.com>,"Thu, 15 Oct 2015 11:40:40 -0700",Re: Should enforce the uniqueness of field name in DataFrame ?,Koert Kuipers <koert@tresata.com>,"r


You can do the same using pure DataFrames.

Seq((1,2)).toDF(""a"", ""b"").registerTempTable(""y"")
Seq((1,4)).toDF(""a"", ""b"").registerTempTable(""x"")
​
table(""x"").join(table(""y""), $""x.a"" === $""y.a"").select(""y.b"", ""x.b"").show()
+-+-+
|b|b|
+-+-+
|2|4|
+-+-+

DataFrame did check for duplicate column names until Sep 2014, but then the


The check in question was removed because it made it impossible to even
reason about a schema that had duplicate column names.  In general, it
seems restrictive to throw an error if duplicate column names exist in an
intermediate schema even when they aren't referenced ambiguously.  We could
consider adding an option to throw an error during analysis for this case,
but it certainly shouldn't be in the constructor of StructType.  My guess
is an option to rename as Reynold suggests would be more popular (though
this could probably not be the default without breaking things).

Anther option that seems nice to me is to always add default qualifiers of
left/right when doing a join.  So you could always do:

df.join(df).where(""left.a = right.a"")

Even when you didn't manually specify left/right.  This could be done only
when there is not a qualifier already called left or right.
"
Annabel Melongo <melongo_annabel@yahoo.com.INVALID>,"Thu, 15 Oct 2015 22:45:28 +0000 (UTC)",Building Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","I was trying to build a cloned version of Spark on my local machine using the command:        mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean packageHowever I got the error:       [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.4.1:shade (default) on project spark-network-common_2.10: Error creating shaded jar: C:\Users\Annabel\git\spark\network\common\dependency-reduced-pom.xml (Access is denied) -> [Help 1]
Any idea, I'm running a 64-bit Windows 8 machine
Thanks
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 15 Oct 2015 15:46:48 -0700",Re: Building Spark,Annabel Melongo <melongo_annabel@yahoo.com>,"bq. Access is denied

Please check permission of the path mentioned.


"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 15 Oct 2015 17:04:14 -0700",Re: Network-related environemental problem when running JDBCSuite,Dev <dev@spark.apache.org>,"
Thanks for everyone's patience with this email thread. I have fixed my
environmental problem and my tests run cleanly now. This seems to be a
problem which afflicts modern JVMs on Mac OSX (and maybe other unix
variants). The following can happen on these platforms:

  InetAddress.getLocalHost().isReachable( 2000 ) == false

If this happens to you, the fix is to add the following line to /etc/hosts:

127.0.0.1	localhost $yourMachineName

where $yourMachineName is the result of the hostname command. For more
information, see
http://stackoverflow.com/questions/1881546/inetaddress-getlocalhost-throws-unknownhostexception

Thanks,
-Rick




Richard Hillegas/San Francisco/IBM@IBMUS wrote on 10/15/2015 11:15:29 AM:

JDBCSuite


 AM:
JDBCSuite
g
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
xt.

(ServerBootstrap.java:272)
(BlockContext.scala:72)

260)
(ForkJoinPool.java:1979)

(ServerBootstrap.java:272)
(BlockContext.scala:72)

260)
(ForkJoinPool.java:1979)
00)
Method)
(SparkILoop.scala:1028)
(SparkIMain.scala:840)


(SparkILoop.scala:857)
324)
(SparkILoop.scala:64)
(SparkILoop.scala:64)
p
(SparkSubmit.scala:180)
5)

22
AM:
JDBCSuite
 a

ug

y
------------------------------------------------------------------------
SS
SS
SS
SS
SS
SS
SS
SS
SS
RE
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
ED
------------------------------------------------------------------------
------------------------------------------------------------------------
------------------------------------------------------------------------
ith
logging.
/
ith
at
ce


(BlockContext.scala:72)
ec
(ForkJoinTask.java:260)

o-
 /

"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 16 Oct 2015 02:01:07 +0000",RE: Gradient Descent with large model size,Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

There seems to be no improvement if I run it with more partitions or bigger depth:
N = 6 Avg time: 13.491579108666668
N = 7 Avg time: 8.929480508
N = 8 Avg time: 14.507123471999998
N= 9 Avg time: 13.854871645333333

Depth = 3
N=2 Avg time: 8.853895346333333
N=5 Avg time: 15.991574924666667

I also measured the bandwidth of my network with iperf. It shows 247Mbit/s. So the transfer of 12M array of double message should take 64 * 12M/247M~3.1s. Does this mean that for 5 nodes with treeaggreate of depth 1 it will take 5*3.1~15.5 seconds?

Best regards, Alexander
From: Joseph Bradley [mailto:joseph@databricks.com]
Sent: Wednesday, October 14, 2015 11:35 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Gradient Descent with large model size

For those numbers of partitions, I don't think you'll actually use tree aggregation.  The number of partitions needs to be over a certain threshold (>= 7) before treeAggregate really operates on a tree structure:
https://github.com/apache/spark/blob/9808052b5adfed7dafd6c1b3971b998e45b2799a/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1100

Do you see a slower increase in running time with more partitions?  For 5 partitions, do you find things improve if you tell treeAggregate to use depth > 2?

Joseph

On Wed, Oct 14, 2015 at 1:18 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I have noticed that Gradient Descent is Spark MLlib takes long time if the model is large. It is implemented with TreeAggregate. I’ve extracted the code from GradientDescent.scala to perform the benchmark. It allocates the Array of a given size and the aggregates it:

val dataSize = 12000000
val n = 5
val maxIterations = 3
val rdd = sc.parallelize(0 until n, n).cache()
rdd.count()
var avgTime = 0.0
for (i <- 1 to maxIterations) {
  val start = System.nanoTime()
  val result = rdd.treeAggregate((new Array[Double](dataSize), 0.0, 0L))(
        seqOp = (c, v) => {
          // c: (grad, loss, count)
          val l = 0.0
          (c._1, c._2 + l, c._3 + 1)
        },
        combOp = (c1, c2) => {
          // c: (grad, loss, count)
          (c1._1, c1._2 + c2._2, c1._3 + c2._3)
        })
  avgTime += (System.nanoTime() - start) / 1e9
  assert(result._1.length == dataSize)
}
println(""Avg time: "" + avgTime / maxIterations)

If I run on my cluster of 1 master and 5 workers, I get the following results (given the array size = 12M):
n = 1: Avg time: 4.555709667333333
n = 2 Avg time: 7.059724584666667
n = 3 Avg time: 9.937117377666667
n = 4 Avg time: 12.687526233
n = 5 Avg time: 12.939526129666667

Could you explain why the time becomes so big? The data transfer of 12M array of double should take ~ 1 second in 1Gbit network. There might be other overheads, however not that big as I observe.
Best regards, Alexander

"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 16 Oct 2015 02:10:25 +0000","RE: No speedup in MultiLayerPerceptronClassifier with increase in
 number of cores",Disha Shrivastava <dishu.905@gmail.com>,"Hi Disha,

This is a good question. We plan to elaborate on it in our talk on the upcoming Spark Summit. Less workers means less compute power, more workers means more communication overhead. So, there exist an optimal number of workers for solving optimization problem with batch gradient given the size of the data and the model. Also, you have to make sure that all workers own local data, that is a separate thing to the number of partitions.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com]
Sent: Thursday, October 15, 2015 10:13 AM
To: Ulanov, Alexander
Cc: Mike Hynes; dev@spark.apache.org
Subject: Re: No speedup in MultiLayerPerceptronClassifier with increase in number of cores

Hi Alexander,
Thanks for your reply.Actually I am working with a modified version of the actual MNIST dataset ( maximum samples = 8.2 M) https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html. I have been running different sized versions( 10000,100000,500000,1M,8M samples) on different number of workers(1,2,3,4,5) and obtaining results. I have observed that when I specify partitions manually, the cluster actually shows scalability performance with decrease in time taken with increase in number of cores. With default settings, Spark automatically divides the data into partitions ( I guess based on data size,etc) and this number is fixed irrespective of the actual number of workers present in the cluster.
As per the data residing on two machines is concerned, I am reading the data from HDFS ( multi-node hadoop cluster setup done for all worker machines). With default number of partitions, Spark gives better results ( less time and better accuracy) as compared to when I manually set the number of partitions; but the problem here is that I can't observe the effect of scalability.
My question is that if I have to obtain both scalability and optimality how should I go about it in Spark? Because clearly in my case, scalable implementation is not necessarily optimal. Here, by scalability I mean that if I increase he number of worker machines , I should get a better performance ( less time taken).
Thanks and Regards
Disha

On Mon, Oct 12, 2015 at 11:45 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Hi Disha,

The problem might be as follows. The data that you have might physically reside only on two nodes and Spark launches data-local tasks. As a result, only two workers are used. You might want to force Spark to distribute the data across all nodes, however it does not seem to be worthwhile for this rather small dataset.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com<mailto:dishu.905@gmail.com>]
Sent: Sunday, October 11, 2015 9:29 AM
To: Mike Hynes
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>; Ulanov, Alexander
Subject: Re: No speedup in MultiLayerPerceptronClassifier with increase in number of cores

Actually I have 5 workers running ( 1 per physical machine) as displayed by the spark UI on spark://IP_of_the_master:7077. I have entered all the physical machines IP in a file named slaves in spark/conf directory and using the script start-all.sh to start the cluster.
My question is that is there a way to control how the tasks are distributed among different workers? To my knowledge it is done by Spark automatically and is not in our control.

On Sun, Oct 11, 2015 at 9:49 PM, Mike Hynes <91mbbh@gmail.com<mailto:91mbbh@gmail.com>> wrote:
Having only 2 workers for 5 machines would be your problem: you
probably want 1 worker per physical machine, which entails running the
spark-daemon.sh script to start a worker on those machines.
The partitioning is agnositic to how many executors are available for
running the tasks, so you can't do scalability tests in the manner
you're thinking by changing the partitioning.

On 10/11/15, Disha Shrivastava <dishu.905@gmail.com<mailto:dishu.905@gmail.com>> wrote:
> Dear Spark developers,
>
> I am trying to study the effect of increasing number of cores ( CPU's) on
> speedup and accuracy ( scalability with spark ANN ) performance for the
> MNIST dataset using ANN implementation provided in the latest spark
> release.
>
> I have formed a cluster of 5 machines with 88 cores in total.The thing
> which is troubling me is that even if I have more than 2 workers in my
> spark cluster the job gets divided only to 2 workers.( executors) which
> Spark takes by default and hence it takes the same time . I know we can set
> the number of partitions manually using sc.parallelize(train_data,10)
> suppose which then divides the data in 10 partitions and all the workers
> are involved in the computation.I am using the below code:
>
>
> import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
> import org.apache.spark.mllib.util.MLUtils
> import org.apache.spark.sql.Row
>
> // Load training data
> val data = MLUtils.loadLibSVMFile(sc, ""data/10000_libsvm"").toDF()
> // Split the data into train and test
> val splits = data.randomSplit(Array(0.7, 0.3), seed = 1234L)
> val train = splits(0)
> val test = splits(1)
> //val tr=sc.parallelize(train,10);
> // specify layers for the neural network:
> // input layer of size 4 (features), two intermediate of size 5 and 4 and
> output of size 3 (classes)
> val layers = Array[Int](784,160,10)
> // create the trainer and set its parameters
> val trainer = new
> MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)
> // train the model
> val model = trainer.fit(train)
> // compute precision on the test set
> val result = model.transform(test)
> val predictionAndLabels = result.select(""prediction"", ""label"")
> val evaluator = new
> MulticlassClassificationEvaluator().setMetricName(""precision"")
> println(""Precision:"" + evaluator.evaluate(predictionAndLabels))
>
> Can you please suggest me how can I ensure that the data/task is divided
> equally to all the worker machines?
>
> Thanks and Regards,
> Disha Shrivastava
> Masters student, IIT Delhi
>
--
Thanks,
Mike


"
Annabel Melongo <melongo_annabel@yahoo.com.INVALID>,"Fri, 16 Oct 2015 15:19:44 +0000 (UTC)",Re: Building Spark,Spark Dev List <dev@spark.apache.org>,"Can someone please provide insight why I get an access denied when I do the build according to the documentation?
Ted said I have to provide the credentials but there's nothing mention about that in the build documentation. 


   

  Ted,
How do I check the permission? I just ran the command as prescribed.


Sent from my Verizon Wireless 4G LTE smartphone
<br><br>-------- Original message --------<br>From: Ted Yu &lt;yuzhihong@gmail.com&gt; <br>Date: 10/15/2015  18:46  (GMT-05:00) <br>To: Annabel Melongo &lt;melongo_annabel@yahoo.com&gt; <br>Cc: dev@spark.apache.org <br>Subject: Re: Building Spark <br><br>
bq. Access is denied
Please check permission of the path mentioned.

I was trying to build a cloned version of Spark on my local machine using the command:        mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean packageHowever I got the error:       [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.4.1:shade (default) on project spark-network-common_2.10: Error creating shaded jar: C:\Users\Annabel\git\spark\network\common\dependency-reduced-pom.xml (Access is denied) -> [Help 1]
Any idea, I'm running a 64-bit Windows 8 machine
Thanks



bq. Access is denied
Please check permission of the path mentioned.
I was trying to build a cloned version of Spark on my local machine using the command:        mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean packageHowever I got the error:       [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.4.1:shade (default) on project spark-network-common_2.10: Error creating shaded jar
: C:\Users\Annabel\git\spark\network\common\dependency-reduced-pom.xml (Access i
s denied) -> [Help 1]
Any idea, I'm running a 64-bit Windows 8 machine
Thanks




  "
,"Fri, 16 Oct 2015 17:22:08 +0200",Re: Building Spark,dev@spark.apache.org,"Hi Annabel,

with the user that you use to lunch mvn, do you have write permission in 
C:\Users\Annabel\git\spark\network\common folder ?

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
jeff saremi <jeffsaremi@hotmail.com>,"Fri, 16 Oct 2015 11:43:47 -0400",Insight into Spark Packages,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm looking for any form of documentation on Spark Packages
Specifically, what happens when one issues a command like the following:


$SPARK_HOME/bin/spark-shell --packages RedisLabs:spark-redis:0.1.0


Something like an architecture diagram. What happens when this package gets submitted? Does this need to be done each time? Is that package downloaded each time? Is there a persistent cache on the server (master i guess)?
Can these packages be installed offline with no Internet connectivity?
How does a package get created? 

and so on and so forth 		 	   		  "
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 16 Oct 2015 16:01:33 +0000","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",Ted Yu <yuzhihong@gmail.com>,"I'd look into tracing a possible bug here, but I'm not sure where to look.
Searching the codebase for `SPARK_MASTER_IP`, amazingly, does not show it
being used in any place directly by Spark
<https://github.com/apache/spark/search?utf8=%E2%9C%93&q=SPARK_MASTER_IP>.

Clearly, Spark is using this environment variable (otherwise I wouldn't see
the behavior described in my first email), but I can't see where.

Can someone give me a pointer?

Nick


one-mode-on-mac
ironment
c714798ec3/templates/root/spark/conf/spark-env.sh#L13>
7077, shutting down Netty transport
60a542/conf/spark-env.sh.template#L49>,
"
,"Fri, 16 Oct 2015 18:05:18 +0200","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",dev@spark.apache.org,"Hi Nick,

there's the Spark master defined in conf/spark-defaults.conf and the -h 
option that you can provide to sbin/start-master.sh script.

Did you try:

sbin/start-master.sh -h xxx.xxx.xxx.xxx

and then use the IP when you start the slaves:

sbin/start-slave.sh spark://xxx.xxx.xxx.xxx.7077

?

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 16 Oct 2015 09:05:44 -0700","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",Nicholas Chammas <nicholas.chammas@gmail.com>,"if [ ""$SPARK_MASTER_IP"" = """" ]; then
  SPARK_MASTER_IP=`hostname`
  --ip $SPARK_MASTER_IP --port $SPARK_MASTER_PORT --webui-port
$SPARK_MASTER_WEBUI_PORT \
  ""$sbin""/../tachyon/bin/tachyon bootstrap-conf $SPARK_MASTER_IP
./sbin/start-master.sh

if [ ""$SPARK_MASTER_IP"" = """" ]; then
  SPARK_MASTER_IP=""`hostname`""
  ""$sbin/slaves.sh"" cd ""$SPARK_HOME"" \; ""$sbin""/../tachyon/bin/tachyon
bootstrap-conf ""$SPARK_MASTER_IP""
""$sbin/slaves.sh"" cd ""$SPARK_HOME"" \; ""$sbin/start-slave.sh""
""spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT""
./sbin/start-slaves.sh


.
_IP>.
lone-mode-on-mac
5c714798ec3/templates/root/spark/conf/spark-env.sh#L13>
:7077, shutting down Netty transport
560a542/conf/spark-env.sh.template#L49>,
"
Sean Owen <sowen@cloudera.com>,"Fri, 16 Oct 2015 17:05:29 +0100","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",Nicholas Chammas <nicholas.chammas@gmail.com>,"It's used in scripts like sbin/start-master.sh


.
_IP>.
lone-mode-on-mac
5c714798ec3/templates/root/spark/conf/spark-env.sh#L13>
:7077, shutting down Netty transport
560a542/conf/spark-env.sh.template#L49>,
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 16 Oct 2015 16:17:27 +0000","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",Sean Owen <sowen@cloudera.com>,"Ah, my bad, I missed it
<https://github.com/apache/spark/blob/08698ee1d6f29b2c999416f18a074d5193cdacd5/sbin/start-master.sh#L58-L60>
since the GitHub search results preview only showed
<https://github.com/apache/spark/search?utf8=%E2%9C%93&q=SPARK_MASTER_IP>
the first hit from start-master.sh and not this part:

""$sbin""/spark-daemon.sh start org.apache.spark.deploy.master.Master 1 \
  --ip $SPARK_MASTER_IP --port $SPARK_MASTER_PORT --webui-port
$SPARK_MASTER_WEBUI_PORT \
  $ORIGINAL_ARGS

Same goes for some of the other sbin scripts.

Anyway, let’s take a closer look…

Nick
​


R_IP>
alone-mode-on-mac
55c714798ec3/templates/root/spark/conf/spark-env.sh#L13>
f
e
X:7077, shutting down Netty transport
4560a542/conf/spark-env.sh.template#L49>,
"
Josh Rosen <rosenville@gmail.com>,"Fri, 16 Oct 2015 10:17:02 -0700",Re: Spark Event Listener,Jakob Odersky <jodersky@gmail.com>,"The reason for having two separate interfaces is developer API
backwards-compatibility, as far as I know. SparkFirehoseListener came later.


"
Jakob Odersky <jodersky@gmail.com>,"Fri, 16 Oct 2015 10:36:42 -0700",Re: Insight into Spark Packages,jeff saremi <jeffsaremi@hotmail.com>,"[repost to mailing list]

I don't know much about packages, but have you heard about the
sbt-spark-package plugin?
Looking at the code, specifically
https://github.com/databricks/sbt-spark-package/blob/master/src/main/scala/sbtsparkpackage/SparkPackagePlugin.scala,
might give you insight on the details about package creation. Package
submission is implemented in
https://github.com/databricks/sbt-spark-package/blob/master/src/main/scala/sbtsparkpackage/SparkPackageHttp.scala

At a quick first overview, it seems packages are bundled as maven artifacts
and then posted to ""http://spark-packages.org/api/submit-release"".

Hope this helps for your last question


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 16 Oct 2015 18:16:58 +0000","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address
	dev@spark.apache.org",,"JB,

I am using spark-env.sh to define the master address instead of using
spark-defaults.conf.

I understand that that should work, and indeed it does, but only if
SPARK_MASTER_IP is set to a DNS name and not an IP address.

Perhaps I'm misunderstanding these configuration methods...

Nick



ER_IP
one-mode-on-mac
TER_IP|
14798ec3/templates/root/spark/conf/spark-env.sh#L13
r
a542/conf/spark-env.sh.template#L49
"
Bill Bejeck <bbejeck@gmail.com>,"Fri, 16 Oct 2015 17:06:05 -0400",Spark Implicit Functions,dev <dev@spark.apache.org>,"All,

I just did a post on adding groupByKeyToList and groupByKeyUnique using
implicit classes.  I thought it might be useful to someone.

http://codingjunkie.net/learning-scala-implicits-with-spark/

Thanks,
Bill
"
Reynold Xin <rxin@databricks.com>,"Fri, 16 Oct 2015 14:55:26 -0700",Re: Spark Implicit Functions,Bill Bejeck <bbejeck@gmail.com>,"Thanks for sharing, Bill.



"
YiZhi Liu <javelinjs@gmail.com>,"Sun, 18 Oct 2015 00:18:22 +0800",Re: Spark Implicit Functions,Bill Bejeck <bbejeck@gmail.com>,"Cool! Help to improve, there exists a typo 'avaragingFunction' :)

Thanks for sharing.

2015-10-17 5:55 GMT+08:00 Reynold Xin <rxin@databricks.com>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Sat, 17 Oct 2015 12:47:40 -0700",Re: Gradient Descent with large model size,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","The decrease in running time from N=6 to N=7 makes some sense to me; that
should be when tree aggregation kicks in.  I'd call it an improvement to
run in the same ~13sec increasing from N=6 to N=9.

""Does this mean that for 5 nodes with treeaggreate of depth 1 it will take
5*3.1~15.5 seconds?""
--> I would guess so since all of that will be aggregated on the driver,
but I don't know enough about Spark's shuffling/networking to say for
sure.  Others may be able to help more.

Your numbers do make me wonder if we should examine the structure of the
tree aggregation more carefully and see if we can improve it.
https://issues.apache.org/jira/browse/SPARK-11168

Joseph

m

*
 1
ld
799a/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1100
e
ted the
e
)(
"
Reynold Xin <rxin@databricks.com>,"Sat, 17 Oct 2015 13:42:05 -0700","flaky test ""map stage submission with multiple shared stages and failures""","""dev@spark.apache.org"" <dev@spark.apache.org>","I just saw this happening:

[info] - map stage submission with multiple shared stages and failures ***
FAILED *** (566 milliseconds)
[info]   java.lang.IndexOutOfBoundsException: 2
[info]   at
scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)
[info]   at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)
[info]   at
org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$49.apply$mcV$sp(DAGSchedulerSuite.scala:1667)
[info]   at
org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$49.apply(DAGSchedulerSuite.scala:1634)
[info]   at
org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$49.apply(DAGSchedulerSuite.scala:1634)
[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at
org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)


Can somebody take a look at it?

Thanks.
"
Evan Sparks <evan.sparks@gmail.com>,"Sat, 17 Oct 2015 14:23:49 -0700",Re: Gradient Descent with large model size,Joseph Bradley <joseph@databricks.com>,"Yes, remember that your bandwidth is the maximum number of bytes per second that can be shipped to the driver. So if you've got 5 blocks that size, then it looks like you're basically saturating the network. 

Aggregation trees help for many partitions/nodes and butterfly mixing can help use all of the network resources. I have seen implementations of butterfly mixing in spark but don't know if we've got one in mainline. Zhao and Canny's work [1] details this approach in the context of model fitting. 

At any rate, for this type of ANN work with huge models in *any* distributed setting, you're going to need to get faster networking (most production deployments I know of either have 10 gigabit Ethernet or 40 gigabit infiniband links), or figure out a way to decrease frequency or density of updates. Both would be even better. 

[1] http://www.cs.berkeley.edu/~jfc/papers/13/butterflymixing.pdf

:
hat should be when tree aggregation kicks in.  I'd call it an improvement to run in the same ~13sec increasing from N=6 to N=9.
 5*3.1~15.5 seconds?""
ut I don't know enough about Spark's shuffling/networking to say for sure.  Others may be able to help more.
ree aggregation more carefully and see if we can improve it.  https://issues.apache.org/jira/browse/SPARK-11168
er depth:
s. So the transfer of 12M array of double message should take 64 * 12M/247M~3.1s. Does this mean that for 5 nodes with treeaggreate of depth 1 it will take 5*3.1~15.5 seconds?
ggregation.  The number of partitions needs to be over a certain threshold (799a/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1100
 partitions, do you find things improve if you tell treeAggregate to use depth > 2?
e model is large. It is implemented with TreeAggregate. I’ve extracted the code from GradientDescent.scala to perform the benchmark. It allocates the Array of a given size and the aggregates it:
)(
ults (given the array size = 12M):
rray of double should take ~ 1 second in 1Gbit network. There might be other overheads, however not that big as I observe.
"
Joseph Bradley <joseph@databricks.com>,"Sat, 17 Oct 2015 14:25:27 -0700",Re: PMML export for LinearRegressionModel,Fazlan Nazeem <fazlann@wso2.com>,"Thanks for bringing this up!  We need to add PMML export methods to the
spark.ml API.  I just made a JIRA for tracking that:
https://issues.apache.org/jira/browse/SPARK-11171

Joseph


"
Chester Chen <chester@alpinenow.com>,"Sat, 17 Oct 2015 14:35:36 -0700",Re: Build spark 1.5.1 branch fails,Xiao Li <gatorsmile@gmail.com>,"I was using jdk 1.7 and maven version is the same as pom file.

᚛ |(v1.5.1)|$ java -version
java version ""1.7.0_51""
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

Using build/sbt still fail the same with -Denforcer.skip, with mvn build,
it fails with


[ERROR] PermGen space -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging

I am giving up on this. Just using 1.5.2-SNAPSHOT for now.

Chester



d
park-launcher_2.10-1.5.1.jar
cala-2.10/spark-streaming-flume-sink_2.10-1.5.1.jar
ic
y
ver
e
 ->
s
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 17 Oct 2015 14:44:51 -0700",Re: Build spark 1.5.1 branch fails,Chester Chen <chester@alpinenow.com>,"Have you set MAVEN_OPTS with the following ?
-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m

Cheers

:

ed
spark-launcher_2.10-1.5.1.jar
scala-2.10/spark-streaming-flume-sink_2.10-1.5.1.jar
lic
ve
. ->
e
d
"
Joseph Bradley <joseph@databricks.com>,"Sat, 17 Oct 2015 14:48:31 -0700",Re: MLlib Contribution,Kybe67 <beck.gael@gmail.com>,"Hi, it'd be great to share your implementation with the community.  I'd
recommend:

(1) Share it immediately by creating a Spark package:
http://spark-packages.org/

You can use this helper package to create your own:
http://spark-packages.org/package/databricks/sbt-spark-package
After you create and test it, I'd recommend emailing the user list to
announce it and see if others have feedback.

(2) If you'd like to get it into MLlib itself, I'd recommend creating a
JIRA first to discuss the algorithm, its design, and a timeline/priority
for getting it into Spark.  (I'd say it looks useful, but it's hard to
guess the use cases & priority within the whole community.  We'd want to
collect user feedback on JIRA, the mailing list, and/or your Spark Package
in order to gauge use cases and priority.)

Thanks!
Joseph


"
Chester Chen <chester@alpinenow.com>,"Sat, 17 Oct 2015 14:58:12 -0700",Re: Build spark 1.5.1 branch fails,Ted Yu <yuzhihong@gmail.com>,"Yes, I have tried MAVEN_OPTS with

-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m

-Xmx4g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m

-Xmx2g -XX:MaxPermSize=1g -XX:ReservedCodeCacheSize=512m

None of them works. All failed with the same error.

thanks







,
t
e
red
/spark-launcher_2.10-1.5.1.jar
/scala-2.10/spark-streaming-flume-sink_2.10-1.5.1.jar
blic
ave
d. ->
n
"
vonnagy <ivan@vadio.com>,"Sat, 17 Oct 2015 19:23:06 -0700 (MST)",Streaming and storing to Google Cloud Storage or S3,dev@spark.apache.org,"I have a streaming application that reads from Kakfa (direct stream) and then
write parquet files. It is a pretty simple app that gets a Kafka direct
stream (8 partitions) and then calls `stream.foreachRdd` and then stores to
parquet using a Dataframe. Batch intervals are set to 10 seconds. During the
storage I use `partitionBy` so the data can be order by time and client.

When running the app and storing the data to a local FS the performance is
somewhat acceptable (~1800 events in 3 seconds). If I point the destination
to Google Cloud storage using the GCS connector the same number of records
takes about 4 minutes. All other things are equal except for the file
destination.

Has anyone tried to go from streaming directly to GCS or S3 and overcome the
unacceptable performance. It can never keep up.

Thanks,

Ivan 



--

---------------------------------------------------------------------


"
Robert Dodier <robert.dodier@gmail.com>,"Sat, 17 Oct 2015 20:21:27 -0700 (MST)","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",dev@spark.apache.org,"Nicholas Chammas wrote

A couple of things. (1) That log message appears to originate at line 434 of
NettyTransport.scala.
(https://github.com/akka/akka/blob/master/akka-remote/src/main/scala/akka/remote/transport/netty/NettyTransport.scala)
It appears the exception is rethrown; is it caught somewhere else so we can
see what the actual error was that triggered the log message? I don't see
anything obvious in the code.

(2) sbin/start-master.sh executes something.Master with --ip
SPARK_MASTER_IP, which calls something.MasterArguments to handle its
arguments, which says:

      case (""--ip"" | ""-i"") :: value :: tail =>
        Utils.checkHost(value, ""ip no longer supported, please use hostname
"" + value)
        host = value
        parse(tail)

      case (""--host"" | ""-h"") :: value :: tail =>
        Utils.checkHost(value, ""Please use hostname "" + value)
        host = value
        parse(tail)

So it would appear that the intent is that numerical IP addresses are
disallowed, however, Utils.checkHost says:

    def checkHost(host: String, message: String = """") {
      assert(host.indexOf(':') == -1, message)
    }

which accepts numerical IP addresses just fine. Is there some other test
that should be applied in MasterArguments? or maybe checkHost should be
looking for some other pattern? Is it possible that MasterArguments was
changed to disallow --ip without propagating that backwards into any scripts
that call it?

Hope this helps in some way.

Robert Dodier



--

---------------------------------------------------------------------


"
jatinganhotra <jatin.ganhotra@gmail.com>,"Sat, 17 Oct 2015 20:40:43 -0700 (MST)",Checkpointing RDD calls the job twice?,dev@spark.apache.org,"Hi,

I noticed that when you checkpoint a given RDD, it results in performing the
action twice as I can see 2 jobs being executed in the Spark UI.

Example:
val logFile = ""/data/pagecounts""
sc.setCheckpointDir(""/checkpoints"")
val logData = sc.textFile(logFile, 2)
val as = logData.filter(line => line.contains(""a""))

Scenario #1:

But, if I change the above code to below:

Scenario #2:
as.cache()
as.checkpoint()
as.count()

Here, there are 2 jobs being executed as shown in the Spark UI, with
duration 0.9s and 0.4s

Why are there 2 jobs in scenario #2? In Spark source code, the comment for
RDD.checkpoint() says the following - 
""This function must be called before any job has been executed on this RDD.
It is strongly recommended that this RDD is persisted in memory, otherwise
saving it on a file will require recompilation.""

In my example above, I am calling cache() before checkpoint(), so RDD will
be persisted in memory. Also, both of the above calls are before the count()
action, so checkpoint() is called before any job execution.

What am I missing here? I have looked further into source code to understand
what could be wrong, but found nothing.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 18 Oct 2015 10:09:45 +0000",Re: Build spark 1.5.1 branch fails,"Chester Chen <chester@alpinenow.com>, Ted Yu <yuzhihong@gmail.com>","These are still too low I think. Try 4g heap and 1g permgen. That's what
the error tells you right?


e
mbly
d
ired
0/spark-launcher_2.10-1.5.1.jar
t/scala-2.10/spark-streaming-flume-sink_2.10-1.5.1.jar
ublic
have
ed. ->
.
on
"
Steve Loughran <stevel@hortonworks.com>,"Sun, 18 Oct 2015 10:31:59 +0000",Re: Build spark 1.5.1 branch fails,Sean Owen <sowen@cloudera.com>,"


These are still too low I think. Try 4g heap and 1g permgen. That's what the error tells you right?

Yes, I have tried MAVEN_OPTS with

-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m

-Xmx4g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m

-Xmx2g -XX:MaxPermSize=1g -XX:ReservedCodeCacheSize=512m

None of them works. All failed with the same error.

thanks



fwiw, here's min. The headless one is a relic of apple jdk6 that I could probably cut now.

$ echo $MAVEN_OPTS
-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m -Xms256m -Djava.awt.headless=true

"
Steve Loughran <stevel@hortonworks.com>,"Sun, 18 Oct 2015 10:33:30 +0000",Re: Streaming and storing to Google Cloud Storage or S3,vonnagy <ivan@vadio.com>,"
the

the problem here is that they aren't really filesystems (certainly s3 via the s3n & s3a clients), flush() is a no-op, and its's only on the close() that there's a bulk upload. For bonus fun, anything that does a rename() usually forces a download/re-upload of the source files.

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 18 Oct 2015 07:54:06 -0700",test failed due to OOME,"""dev@spark.apache.org"" <dev@spark.apache.org>","From
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=spark-test/3846/console
:

SparkListenerSuite:- basic creation and shutdown of LiveListenerBus-
bus.stop() waits for the event queue to completely drain- basic
creation of StageInfo- basic creation of StageInfo with shuffle-
StageInfo with fewer tasks than partitions- local metrics-
onTaskGettingResult() called when result fetched remotely *** FAILED
***  org.apache.spark.SparkException: Job aborted due to stage
failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost
task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError:
Java heap space	at java.util.Arrays.copyOf(Arrays.java:2271)	at
java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)	at
java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)	at
java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)	at
java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1852)	at
java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)	at
org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:182)	at
org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:52)	at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)	at
org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:49)	at
java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1458)	at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429)	at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)	at
java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)	at
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)	at
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)	at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:256)	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at
java.lang.Thread.run(Thread.java:745)


Should more heap be given to test suite ?


Cheers
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 18 Oct 2015 09:56:34 -0700",streaming test failure,"""dev@spark.apache.org"" <dev@spark.apache.org>","When I ran the following command on Linux with latest master branch:
~/apache-maven-3.3.3/bin/mvn clean -Phive -Phive-thriftserver -Pyarn
-Phadoop-2.4 -Dhadoop.version=2.7.0 package

I saw some test failures:
http://pastebin.com/1VYZYy5K

Has anyone seen similar test failure before ?

Thanks
"
gsvic <victorasgs@gmail.com>,"Sun, 18 Oct 2015 12:55:04 -0700 (MST)",ShuffledHashJoin Possible Issue,dev@spark.apache.org,"I am doing some experiments with join algorithms in SparkSQL and I am facing
the following issue:

I have costructed two ""dummy"" json tables, t1.json and t2.json. Each of them
has two columns, ID and Value. The ID is an incremental integer(unique) and
the Value a random value. I am running an equi-join query on ID attribute.
In case of SortMerge and BroadcastHashJoin algorithms, the return result is
correct but in case of ShuffledHashJoin the count aggregate returns always
zero. The correct result is t2, as t2.ID is a subset of t1.ID.

The query is *t1.join(t2).where(t1(""ID"").equalTo(t2(""ID"")))*





--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 18 Oct 2015 20:21:37 +0000","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address","Robert Dodier <robert.dodier@gmail.com>, dev@spark.apache.org","Good catches, Robert.

I had actually typed up a draft email a couple of days ago citing those
same two blocks of code. I deleted it when I realized like you that the
snippets did not explain why IP addresses weren’t working.

Something seems wrong here, but I’m not sure what exactly. Maybe this is a
documentation bug or missing deprecation warning.

For example, this line
<https://github.com/apache/spark/blob/a337c235a12d4ea6a7d6db457acc6b32f1915241/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala#L93>
seems to back up your finding that SPARK_MASTER_IP is deprecated (since the
--ip it maps to is deprecated), but no warnings are displayed and the docs
make no mention of this being deprecated.

  private def printUsageAndExit(exitCode: Int) {
    // scalastyle:off println
    System.err.println(
      ""Usage: Master [options]\n"" +
      ""\n"" +
      ""Options:\n"" +
      ""  -i HOST, --ip HOST     Hostname to listen on (deprecated,
please use --host or -h) \n"" +
      ""  -h HOST, --host HOST   Hostname to listen on\n"" +
      ""  -p PORT, --port PORT   Port to listen on (default: 7077)\n"" +
      ""  --webui-port PORT      Port for web UI (default: 8080)\n"" +
      ""  --properties-file FILE Path to a custom Spark properties file.\n"" +
      ""                         Default is conf/spark-defaults.conf."")
    // scalastyle:on println
    System.exit(exitCode)
  }

Hopefully someone with better knowledge of this code can explain what’s
going on.

I’m beginning to think SPARK_MASTER_IP is straight up deprecated in favor
of SPARK_MASTER_HOST, but a warning needs to be actually thrown here in the
code
<https://github.com/apache/spark/blob/a337c235a12d4ea6a7d6db457acc6b32f1915241/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala#L52-L56>
and the template
<https://github.com/apache/spark/blob/a337c235a12d4ea6a7d6db457acc6b32f1915241/conf/spark-env.sh.template#L49>
and docs need to be updated.

This code hasn’t been touched much since Spark’s genesis, so I’m not
expecting anyone to know off the top of their head whether this is wrong or
right. Perhaps I should just open a PR and take it from there.

Nick
​


remote/transport/netty/NettyTransport.scala
an
me
-actually-expects-a-DNS-name-not-IP-address-tp14613p14665.html
"
Robert Dodier <robert.dodier@gmail.com>,"Sun, 18 Oct 2015 14:19:07 -0700","Re: SPARK_MASTER_IP actually expects a DNS name, not IP address",Nicholas Chammas <nicholas.chammas@gmail.com>,"Nicholas,

FWIW the --ip option seems to have been deprecated in commit d90d2af1,
but that was a pretty big commit, lots of other stuff changed, and there
isn't any hint in the log message as to the reason for changing --ip.

best,

Robert Dodier

---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Mon, 19 Oct 2015 02:23:11 +0000",RE: ShuffledHashJoin Possible Issue,"gsvic <victorasgs@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Gsvic, Can you please provide detail code / steps to reproduce that?

Hao

g the following issue:

I have costructed two ""dummy"" json tables, t1.json and t2.json. Each of them has two columns, ID and Value. The ID is an incremental integer(unique) and the Value a random value. I am running an equi-join query on ID attribute.
In case of SortMerge and BroadcastHashJoin algorithms, the return result is correct but in case of ShuffledHashJoin the count aggregate returns always zero. The correct result is t2, as t2.ID is a subset of t1.ID.

The query is *t1.join(t2).where(t1(""ID"").equalTo(t2(""ID"")))*





--
3.nabble.com/ShuffledHashJoin-Possible-Issue-tp14672.html
om.

---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Fazlan Nazeem <fazlann@wso2.com>,"Mon, 19 Oct 2015 09:38:19 +0530",Re: PMML export for LinearRegressionModel,Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

That's great. Also It would be great if spark extends the PMML support to
models which are not PMML supported right now.
e.g

   - Decision Tree
   - Random Forest
   - Naive Bayes





-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
Xiao Li <gatorsmile@gmail.com>,"Sun, 18 Oct 2015 23:38:57 -0700",Spark SQL: what does an exclamation mark mean in the plan?,"""user@spark.apache.org"" <dev@spark.apache.org>","Hi, all,

After turning on the trace, I saw a strange exclamation mark in
the intermediate plans. This happened in catalyst analyzer.

Join Inner, Some((col1#0 = col1#6))
 Project [col1#0,col2#1,col3#2,col2_alias#24,col3#2 AS col3_alias#13]
  Project [col1#0,col2#1,col3#2,col2#1 AS col2_alias#24]
   LogicalRDD [col1#0,col2#1,col3#2], MapPartitionsRDD[1] at
createDataFrame at SimpleApp.scala:32
 Aggregate [col1#6], [col1#6,count(col1#6) AS count(col1)#5L]
  *!Project [col1#6,col2#7,col3#8,col2_alias#24,col3#8 AS col3_alias#4]*
   Project [col1#6,col2#7,col3#8,col2#7 AS col2_alias#3]
    LogicalRDD [col1#6,col2#7,col3#8], MapPartitionsRDD[1] at
createDataFrame at SimpleApp.scala:32

Could anybody give me a hint why there exists a !(exclamation mark) before
the node name (Project)? This ! mark does not disappear in the subsequent
query plan.

Thank you!

Xiao Li
"
Renjie Liu <liurenjie2008@gmail.com>,"Mon, 19 Oct 2015 14:58:10 +0800",Guaranteed processing orders of each batch in Spark Streaming,dev@spark.apache.org,"Hi, all:
I've read source code and it seems that there is no guarantee that the
order of processing of each RDD is guaranteed since jobs are just submitted
to a thread pool. I  believe that this is quite important in streaming
since updates should be ordered.
"
prakhar jauhari <prak840@gmail.com>,"Mon, 19 Oct 2015 00:51:21 -0700 (MST)","Spark driver reducing total executors count even when Dynamic
 Allocation is disabled.",dev@spark.apache.org,"Hey all,

Thanks in advance. I ran into a situation where spark driver reduced the
total executors count for my job even with dynamic allocation disabled, and
caused the job to hang for ever. 

Setup: 
Spark-1.3.1 on hadoop-yarn-2.4.0 cluster. 
All servers in cluster running Linux version 2.6.32. 
Job in yarn-client mode.

Scenario:
1. Application running with required number of executors.
2. Spark issues a killExecutor for the executor on the DN which was timed
out. 
3. Even with dynamic allocation off, spark's driver reduces the
""targetNumExecutors"".


When my DN goes unreachable: 
	
Spark core's HeartbeatReceiver invokes expireDeadHosts(): which checks if
Dynamic Allocation is supported and then invokes ""sc.killExecutor()""

	/if (sc.supportDynamicAllocation) {
          	sc.killExecutor(executorId)
        }/ 

Surprisingly supportDynamicAllocation in sparkContext.scala is defined as,
resulting ""True"" if dynamicAllocationTesting flag is enabled or spark is
running over ""yarn"".

/private[spark] def supportDynamicAllocation = 
    		    master.contains(""yarn"") || dynamicAllocationTesting	/

""sc.killExecutor()"" matches it to configured ""schedulerBackend""
(CoarseGrainedSchedulerBackend in this case) and invokes
""killExecutors(executorIds)""

CoarseGrainedSchedulerBackend calculates a ""newTotal"" for the total number
of executors required, and sends a update to application master by invoking
""doRequestTotalExecutors(newTotal)""
 
CoarseGrainedSchedulerBackend then invokes a
""doKillExecutors(filteredExecutorIds)"" for the lost executors. 

Thus reducing the total number of executors in a host intermittently
unreachable scenario.


I noticed that this change to ""CoarseGrainedSchedulerBackend"" was introduced
while fixing :  https://issues.apache.org/jira/browse/SPARK-6325
<https://issues.apache.org/jira/browse/SPARK-6325>  



I am new to this code, If any of you could comment on why do we need
""doRequestTotalExecutors"" in ""killExecutors"" would be a great help. Also why
do we have ""supportDynamicAllocation"" = True even if i have not enabled
dynamic allocation. 

Regards,
Prakhar.




--

---------------------------------------------------------------------


"
weymouth <weymouth@umich.edu>,"Mon, 19 Oct 2015 01:34:41 -0700 (MST)",Re: Haskell language Spark support,dev@spark.apache.org,"Wojciech,
  I am a programmer with over 30 years of programming experience, most
recently in Java, and with lots of experience in languages like LISP
(functional), and R (array/list). I'm currently learning Haskell, and
working in an environment where I need to apply Spark to ""large data"". I'd
be very interested in helping with a Haskell binding for Spark. 
Terry Weymouth.



--

---------------------------------------------------------------------


"
Saisai Shao <sai.sai.shao@gmail.com>,"Mon, 19 Oct 2015 17:17:04 +0800","Re: Spark driver reducing total executors count even when Dynamic
 Allocation is disabled.",prakhar jauhari <prak840@gmail.com>,"This is a deliberate killing request by heartbeat mechanism, have nothing
to do with dynamic allocation. Here because you're running on yarn mode, so
""supportDynamicAllocation"" will be true, but actually there's no relation
to dynamic allocation.

total executor number with AM, AM will try to cancel some pending container
requests when current expected executor number is less. The actual
container killing command is issued by ""doRequestTotalExecutors"".

Not sure what is your actual problem? is it unexpected?

Thanks
Saisai



"
gsvic <victorasgs@gmail.com>,"Mon, 19 Oct 2015 02:59:51 -0700 (MST)",RE: ShuffledHashJoin Possible Issue,dev@spark.apache.org,"Hi Hao,

Each table is created with the following python code snippet:

data = [{'id': 'A%d'%i, 'value':ceil(random()*10)} for i in range(0,50)]
with open('A.json', 'w+') as output:
    json.dump(data, output)

The tables A and B containing 10 and 50 tuples respectively. 

In spark shell I type

sqlContext.setConf(""spark.sql.planner.sortMergeJoin"", ""false"") to disable
sortMergeJoin and
sqlContext.setConf(""spark.sql.autoBroadcastJoinThreshold"", ""0"") to disable
BroadcastHashJoin, cause the tables are too small and this join will be
selected.

Finally I run the following query:
t1.join(t2).where(t1(""id"").equalTo(t2(""id""))).count

and the result I get equals to zero, while ShuffledHashJoin and
SortMergeJoin returns the right result (10).



--

---------------------------------------------------------------------


"
Rohith Parameshwara <rparameshwara@couponsinc.com>,"Mon, 19 Oct 2015 12:05:51 +0000",Unable to run applications on spark in standalone cluster  mode ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
                I am doing some experiments on spark standalone cluster setup and I am facing the following issue:
I have a 4 node cluster setup. As per http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually I tried to start the cluster with the scripts but, the slaves did not start and gave permission denied error....My conf/slaves had a list of IP addresses of the slaves... But Then I was able to start the worker nodes by going to the respective slave machine and running the start-slave.sh script with the master IP as parameter.... The webui showed the all the 3 worker nodes running as :
[cid:image001.png@01D10A93.B59D7300]

The application when submitted to the cluster will show in all the work nodes in the webui.... But in the command line where is spark-submit command was run, it gives this error periodically and continuously....
ERROR NettyTransport: failed to bind to /172.28.161.138:7077, shutting down Netty transport
15/10/19 11:59:38 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.
15/10/19 11:59:38 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@localhost:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@localhost:7077....

My conf/spark-env.sh has:

SPARK_MASTER_IP=172.28.161.138
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_WORKER_WEBUI_PORT=8081
SPARK_WORKER_INSTANCES=1
And I have also put this in all the slave nodes too....

The applications are running fine in -master local mode but in -master spark://masterip:7077, it is not working....
Any type of help would be appreciated..... Thanks in advance
"
,"Mon, 19 Oct 2015 14:16:45 +0200",Re: Unable to run applications on spark in standalone cluster mode,dev@spark.apache.org,"Hi Rohith,

Do you have multiple interfaces on the machine hosting the master ?

If so, can you try to force to the public interface using:

sbin/start-master.sh --ip xxx.xxx.xxx.xxx

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Adrian Bridgett <adrian@opensignal.com>,"Mon, 19 Oct 2015 13:41:56 +0100",failed mesos task loses executor,dev@spark.apache.org,"Just testing spark v1.5.0 (on mesos v0.23) and we saw something 
unexpected (according to the event timeline) - when a spark task failed 
(intermittent S3 connection failure), the whole executor was removed and 
was never recovered so the job proceeded slower than normal.

Looking at the code I saw something that seemed a little odd in 
core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala:

   override def statusUpdate(d: SchedulerDriver, status: TaskStatus) {
...
         if (TaskState.isFailed(TaskState.fromMesos(status.getState))
           && taskIdToSlaveId.contains(tid)) {
           // We lost the executor on this slave, so remember that it's gone
           removeExecutor(taskIdToSlaveId(tid), ""Lost executor"")
         }
         if (TaskState.isFinished(state)) {
           taskIdToSlaveId.remove(tid)
         }
       }

I don't know either codebase at all, however it seems odd to kill the 
executor for a failed task rather than just a lost task.  I did a quick 
test (with v1.5.1) where I replaced this line with:

    if ((TaskState.fromMesos(status.getState) == TaskState.LOST)

and all seemed well - I faked the problem (using iptables to briefly 
block access to the S3 endpoint), the task failed but was retried (on 
the same executor), succeeded and continued on its merry way.

Adrian
-- 
*Adrian Bridgett* |  Sysadmin Engineer, OpenSignal 
<http://www.opensignal.com>
_____________________________________________________
Office: First Floor, Scriptor Court, 155-157 Farringdon Road, 
Clerkenwell, London, EC1R 3AD
Phone #: +44 777-377-8251
Skype: abridgett  |@adrianbridgett <http://twitter.com/adrianbridgett>| 
LinkedIn link <https://uk.linkedin.com/in/abridgett>
_____________________________________________________
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 19 Oct 2015 15:19:12 +0200",Building Spark w/ 1.8 and binary incompatibilities,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

tl;dr; I built Spark with Java 1.8 even though my JAVA_HOME pointed to 1.7.
Then it failed with binary incompatibilities.

I couldn’t find any mention of this in the docs, so It might be a known
thing, but it’s definitely too easy to do the wrong thing.

The problem is that Maven is using the Zinc incremental compiler, which is
a long-running server. If the first build (that spawns the zinc server) is
started with Java 8 on the path, Maven will compile against Java 8 even
after changing JAVA_HOME and rebuilding.

I filed scala-maven-plugin#173
<https://github.com/davidB/scala-maven-plugin/issues/173> but so far no
comment.

Steps to reproduce:

   - make sure zinc is not running yet
   - build with JAVA_HOME pointing to 1.8
   - point JAVA_HOME to 1.7
   - clean build
   - run Spark, watch it fail with NoSuchMethodError in ConcurrentHashMap.
   More details here
   <https://gist.github.com/AlainODea/1375759b8720a3f9f094>

Workaround:

   - build/zinc/bin/zinc -shutdown
   - rebuild

iulian
​
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
shane knapp <sknapp@berkeley.edu>,"Mon, 19 Oct 2015 09:39:09 -0700",BUILD SYSTEM: amp-jenkins-worker-05 offline,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","i'll have to head down to the colo and see what's up with it...  it
seems to be wedged (pings ok, can't ssh in) and i'll update the list
when i figure out what's wrong.

i don't think it caught fire (#toosoon?), because everything else is
up and running.  :)

shane

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Mon, 19 Oct 2015 10:27:59 -0700",Re: ShuffledHashJoin Possible Issue,gsvic <victorasgs@gmail.com>,"Can you reproduce it on master?

I can't reproduce it with the following code:

ShuffledHashJoin [id#21], [id#19], BuildRight
 TungstenExchange hashpartitioning(id#21,200)
  TungstenProject [concat(A,cast(id#20L as string)) AS id#21]
   Scan PhysicalRDD[id#20L]
 TungstenExchange hashpartitioning(id#19,200)
  TungstenProject [concat(A,cast(id#18L as string)) AS id#19]
   Scan PhysicalRDD[id#18L]

10



---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 19 Oct 2015 17:32:49 +0000",RE: Gradient Descent with large model size,"Evan Sparks <evan.sparks@gmail.com>, Joseph Bradley
	<joseph@databricks.com>","Evan, Joseph

Thank you for valuable suggestions. It would be great to improve TreeAggregate (if possible).

Making less updates would certainly make sense, though that will mean using batch gradient such as LBFGS. It seems as today it is the only viable option in Spark.

I will also take a look into how to zip the data sent as update. Do you know any options except going from double to single precision (or less) ?

Best regards, Alexander

From: Evan Sparks [mailto:evan.sparks@gmail.com]
Sent: Saturday, October 17, 2015 2:24 PM
To: Joseph Bradley
Cc: Ulanov, Alexander; dev@spark.apache.org
Subject: Re: Gradient Descent with large model size

Yes, remember that your bandwidth is the maximum number of bytes per second that can be shipped to the driver. So if you've got 5 blocks that size, then it looks like you're basically saturating the network.

Aggregation trees help for many partitions/nodes and butterfly mixing can help use all of the network resources. I have seen implementations of butterfly mixing in spark but don't know if we've got one in mainline. Zhao and Canny's work [1] details this approach in the context of model fitting.

At any rate, for this type of ANN work with huge models in *any* distributed setting, you're going to need to get faster networking (most production deployments I know of either have 10 gigabit Ethernet or 40 gigabit infiniband links), or figure out a way to decrease frequency or density of updates. Both would be even better.

[1] http://www.cs.berkeley.edu/~jfc/papers/13/butterflymixing.pdf

On Oct 17, 2015, at 12:47 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
The decrease in running time from N=6 to N=7 makes some sense to me; that should be when tree aggregation kicks in.  I'd call it an improvement to run in the same ~13sec increasing from N=6 to N=9.

""Does this mean that for 5 nodes with treeaggreate of depth 1 it will take 5*3.1~15.5 seconds?""
--> I would guess so since all of that will be aggregated on the driver, but I don't know enough about Spark's shuffling/networking to say for sure.  Others may be able to help more.

Your numbers do make me wonder if we should examine the structure of the tree aggregation more carefully and see if we can improve it.  https://issues.apache.org/jira/browse/SPARK-11168

Joseph

On Thu, Oct 15, 2015 at 7:01 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Hi Joseph,

There seems to be no improvement if I run it with more partitions or bigger depth:
N = 6 Avg time: 13.491579108666668
N = 7 Avg time: 8.929480508
N = 8 Avg time: 14.507123471999998
N= 9 Avg time: 13.854871645333333

Depth = 3
N=2 Avg time: 8.853895346333333
N=5 Avg time: 15.991574924666667

I also measured the bandwidth of my network with iperf. It shows 247Mbit/s. So the transfer of 12M array of double message should take 64 * 12M/247M~3.1s. Does this mean that for 5 nodes with treeaggreate of depth 1 it will take 5*3.1~15.5 seconds?

Best regards, Alexander
From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com>]
Sent: Wednesday, October 14, 2015 11:35 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Gradient Descent with large model size

For those numbers of partitions, I don't think you'll actually use tree aggregation.  The number of partitions needs to be over a certain threshold (>= 7) before treeAggregate really operates on a tree structure:
https://github.com/apache/spark/blob/9808052b5adfed7dafd6c1b3971b998e45b2799a/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1100

Do you see a slower increase in running time with more partitions?  For 5 partitions, do you find things improve if you tell treeAggregate to use depth > 2?

Joseph

On Wed, Oct 14, 2015 at 1:18 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I have noticed that Gradient Descent is Spark MLlib takes long time if the model is large. It is implemented with TreeAggregate. I’ve extracted the code from GradientDescent.scala to perform the benchmark. It allocates the Array of a given size and the aggregates it:

val dataSize = 12000000
val n = 5
val maxIterations = 3
val rdd = sc.parallelize(0 until n, n).cache()
rdd.count()
var avgTime = 0.0
for (i <- 1 to maxIterations) {
  val start = System.nanoTime()
  val result = rdd.treeAggregate((new Array[Double](dataSize), 0.0, 0L))(
        seqOp = (c, v) => {
          // c: (grad, loss, count)
          val l = 0.0
          (c._1, c._2 + l, c._3 + 1)
        },
        combOp = (c1, c2) => {
          // c: (grad, loss, count)
          (c1._1, c1._2 + c2._2, c1._3 + c2._3)
        })
  avgTime += (System.nanoTime() - start) / 1e9
  assert(result._1.length == dataSize)
}
println(""Avg time: "" + avgTime / maxIterations)

If I run on my cluster of 1 master and 5 workers, I get the following results (given the array size = 12M):
n = 1: Avg time: 4.555709667333333
n = 2 Avg time: 7.059724584666667
n = 3 Avg time: 9.937117377666667
n = 4 Avg time: 12.687526233
n = 5 Avg time: 12.939526129666667

Could you explain why the time becomes so big? The data transfer of 12M array of double should take ~ 1 second in 1Gbit network. There might be other overheads, however not that big as I observe.
Best regards, Alexander


"
shane knapp <sknapp@berkeley.edu>,"Mon, 19 Oct 2015 11:13:52 -0700",Re: BUILD SYSTEM: amp-jenkins-worker-05 offline,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","worker 05 is back up now...  looks like the machine OOMed and needed
to be kicked.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 19 Oct 2015 11:16:25 -0700",Re: Spark SQL: what does an exclamation mark mean in the plan?,Xiao Li <gatorsmile@gmail.com>,"It means that there is an invalid attribute reference (i.e. a #n where the
attribute is missing from the child operator).


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 Oct 2015 12:27:19 -0700",Re: BUILD SYSTEM: amp-jenkins-worker-05 offline,amp-infra@googlegroups.com,"Hey Shane,

It also appears that every Spark build is failing right now. Could it be
related to your changes?

- Patrick


"
shane knapp <sknapp@berkeley.edu>,"Mon, 19 Oct 2015 12:58:54 -0700",Re: BUILD SYSTEM: amp-jenkins-worker-05 offline,amp-infra <amp-infra@googlegroups.com>,"all we did was reboot -05 and -03...  i'm seeing a bunch of green
builds.  could you provide me w/some specific failures so i can look
in to them more closely?


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 Oct 2015 13:44:48 -0700",Re: BUILD SYSTEM: amp-jenkins-worker-05 offline,amp-infra@googlegroups.com,"This is what I'm looking at:

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/




"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 Oct 2015 13:49:33 -0700",Re: BUILD SYSTEM: amp-jenkins-worker-05 offline,amp-infra@googlegroups.com,"I think many of them are coming form the Spark 1.4 builds:

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/Spark-1.4-Maven-pre-YARN/3900/console


"
shane knapp <sknapp@berkeley.edu>,"Mon, 19 Oct 2015 13:57:30 -0700",Re: BUILD SYSTEM: amp-jenkins-worker-05 offline,"amp-infra <amp-infra@googlegroups.com>, Josh Rosen <joshrosen@databricks.com>","++joshrosen

some of those 1.4 builds were incorrectly configured and launching on
a reserved executor...  josh fixed them and we're looking a lot better
(meaning that we're building and not failing at launch).

shane


-----------------------------------"
Xiao Li <gatorsmile@gmail.com>,"Mon, 19 Oct 2015 16:25:31 -0700",Re: Spark SQL: what does an exclamation mark mean in the plan?,Michael Armbrust <michael@databricks.com>,"Hi, Michael,

Thank you again! Just found the functions that generate the ! mark

  /**
   * A prefix string used when printing the plan.
   *
   * We use ""!"" to indicate an invalid plan, and ""'"" to indicate an
unresolved plan.
   */
  protected def statePrefix = if (missingInput.nonEmpty &&
children.nonEmpty) ""!"" else """"

  override def simpleString: String = statePrefix + super.simpleString


Xiao Li

2015-10-19 11:16 GMT-07:00 Michael Armbrust <michael@databricks.com>:

"
Vladimir Vladimirov <smartkiwi@gmail.com>,"Mon, 19 Oct 2015 19:38:07 -0400","Problem using User Defined Predicate pushdown with core RDD and
 parquet - UDP class not found",dev@spark.apache.org,"Hi all

I feel like this questions is more Spark dev related that Spark user
related. Please correct me if I'm wrong.

My project's data flow involves sampling records from the data stored as
Parquet dataset.
I've checked DataFrames API and it doesn't support user defined predicates
projection pushdown - only simple filter expressions.
I want to use custom filter function predicate pushdown feature of parquet
while loading data with newAPIHadoopFile.
Simple filters constructed with org.apache.parquet.filter2 API works fine.
But User Defined Predicate works only with `--master local` mode.

When I try to run in yarn-client mode my test program that uses UDP class
to be used by parquet-mr I'm getting class not found exception.

I suspect that the issue could be related to the way how class loader works
from parquet or maybe it could be related to the fact that Spark executor
processes has my jar loaded from HTTP server and there is some security
policies (classpath shows that the jar URI is actually HTTP URL and not
local file).

I've tried to create uber jar with all dependencies and shipt it with the
spark app - no success.

PS I'm using spark 1.5.1.

Here is my command line I'm using to submit the application:

SPARK_CLASSPATH=./lib/my-jar-with-dependencies.jar spark-submit \
    --master yarn-client
    --num-executors 3 --driver-memory 3G --executor-memory 2G \
    --executor-cores 1 \
    --jars
./lib/my-jar-with-dependencies.jar,./lib/snappy-java-1.1.2.jar,./lib/parquet-hadoop-1.7.0.jar,./lib/parquet-avro-1.7.0.jar,./lib/parquet-column-1.7.0.jar,/opt/cloudera/parcels/CDH/jars/avro-1.7.6-cdh5.4.0.jar,/opt/cloudera/parcels/CDH/jars/avro-mapred-1.7.6-cdh5.4.0-hadoop2.jar,
\
    --class my.app.parquet.filters.tools.TestSparkApp \
    ./lib/my-jar-with-dependencies.jar \
    yarn-client \
    ""/user/vvlad/2015/*/*/*/EVENTS""

Here is the code of my UDP class:

package my.app.parquet.filters.udp

import org.apache.parquet.filter2.predicate.Statistics
import org.apache.parquet.filter2.predicate.UserDefinedPredicate


import java.lang.{Integer => JInt}

import scala.util.Random

class SampleIntColumn(threshold: Double) extends UserDefinedPredicate[JInt]
with Serializable {
  lazy val random = { new Random() }
  val myThreshold = threshold
  override def keep(value: JInt): Boolean = {
    random.nextFloat() < myThreshold
  }

  override def canDrop(statistics: Statistics[JInt]): Boolean = false

  override def inverseCanDrop(statistics: Statistics[JInt]): Boolean = false

  override def toString: String = {
    ""%s(%f)"".format(getClass.getName, myThreshold)
  }
}

Spark app:

package my.app.parquet.filters.tools

import my.app.parquet.filters.udp.SampleIntColumn
import org.apache.avro.generic.GenericRecord
import org.apache.hadoop.mapreduce.Job
import org.apache.parquet.avro.AvroReadSupport
import org.apache.parquet.filter2.dsl.Dsl.IntColumn
import org.apache.parquet.hadoop.ParquetInputFormat
import org.apache.spark.{SparkContext, SparkConf}

import org.apache.parquet.filter2.dsl.Dsl._
import org.apache.parquet.filter2.predicate.FilterPredicate


object TestSparkApp {
  def main (args: Array[String]) {
    val conf = new SparkConf()
      //""local[2]"" or yarn-client etc
      .setMaster(args(0))
      .setAppName(""Spark Scala App"")
      .set(""spark.executor.memory"", ""1g"")
      .set(""spark.rdd.compress"", ""true"")
      .set(""spark.storage.memoryFraction"", ""1"")

    val sc = new SparkContext(conf)

    val job = new Job(sc.hadoopConfiguration)
    ParquetInputFormat.setReadSupportClass(job,
classOf[AvroReadSupport[GenericRecord]])

    val sampler = new SampleIntColumn(0.05)
    val impField = IntColumn(""impression"")

    val pred: FilterPredicate = impField.filterBy(sampler)

    ParquetInputFormat.setFilterPredicate(job.getConfiguration, pred)


println(job.getConfiguration.get(""parquet.private.read.filter.predicate""))

println(job.getConfiguration.get(""parquet.private.read.filter.predicate.human.readable""))

    val records1 = sc.newAPIHadoopFile(
    //<path to parquet>
      args(1),
      classOf[ParquetInputFormat[GenericRecord]],
      classOf[Void],
      classOf[GenericRecord],
      job.getConfiguration
    ).map(_._2).cache()

    println(""result count "" + records1.count().toString)

    sc.stop()
  }
}



Here are logs with exception I'm getting:


15/10/19 11:14:43 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID
0, hdp010........, NODE_LOCAL, 2815 bytes)
15/10/19 11:14:43 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID
1, hdp042........, NODE_LOCAL, 2816 bytes)
15/10/19 11:14:43 INFO YarnClientSchedulerBackend: Registered executor:
AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hdp027........:43593/user/Executor#-832887318])
with ID 3
15/10/19 11:14:43 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID
2, hdp027........, NODE_LOCAL, 2814 bytes)
15/10/19 11:14:44 INFO BlockManagerMasterEndpoint: Registering block
manager hdp027........:64266 with 883.8 MB RAM, BlockManagerId(3,
hdp027........, 64266)
15/10/19 11:14:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory
on hdp010........:23967 (size: 1516.0 B, free: 883.8 MB)
15/10/19 11:14:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory
on hdp042........:63034 (size: 1516.0 B, free: 883.8 MB)
15/10/19 11:14:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory
on hdp010........:23967 (size: 25.1 KB, free: 883.8 MB)
15/10/19 11:14:45 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID
3, hdp010........, NODE_LOCAL, 2816 bytes)
15/10/19 11:14:45 WARN TaskSetManager: Lost task 21.0 in stage 0.0 (TID 0,
hdp010........): java.lang.RuntimeException: java.io.IOException: Could not
read object from config with key parquet.private.read.filter.predicate
at
org.apache.parquet.hadoop.ParquetInputFormat.getFilterPredicate(ParquetInputFormat.java:196)
at
org.apache.parquet.hadoop.ParquetInputFormat.getFilter(ParquetInputFormat.java:205)
at
org.apache.parquet.hadoop.ParquetInputFormat.createRecordReader(ParquetInputFormat.java:241)
at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:151)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:124)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:65)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:262)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
at org.apache.spark.scheduler.Task.run(Task.scala:88)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Could not read object from config with key
parquet.private.read.filter.predicate
at
org.apache.parquet.hadoop.util.SerializationUtil.readObjectFromConfAsBase64(SerializationUtil.java:102)
at
org.apache.parquet.hadoop.ParquetInputFormat.getFilterPredicate(ParquetInputFormat.java:194)
... 17 more
Caused by: java.lang.ClassNotFoundException:
my.app.parquet.filters.udp.SampleIntColumn
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:274)
at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:625)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
at
org.apache.parquet.hadoop.util.SerializationUtil.readObjectFromConfAsBase64(SerializationUtil.java:100)
... 18 more


Best Regards
Vladimir Vladimirov
"
shane knapp <sknapp@berkeley.edu>,"Mon, 19 Oct 2015 16:50:06 -0700",Re: BUILD SYSTEM: amp-jenkins-worker-05 offline,"amp-infra <amp-infra@googlegroups.com>, Josh Rosen <joshrosen@databricks.com>","things are green, nice catch on the job config, josh.


---------------------------------------------------------------------


"
Annabel Melongo <melongo_annabel@yahoo.com.INVALID>,"Tue, 20 Oct 2015 01:59:00 +0000 (UTC)",Problem building Spark,Spark Dev List <dev@spark.apache.org>,"I tried to build Spark according to the build directions and the it failed due to the following error: 
|   |
|   |   |   |   |   |
| Building Spark - Spark 1.5.1 DocumentationBuilding Spark Building with build/mvn Building a Runnable Distribution Setting up Maven’s Memory Usage Specifying the Hadoop Version Building With Hive and JDBC Support Building for Scala 2.11  |
|  |
| View on spark.apache.org | Preview by Yahoo |
|  |
|   |

    [ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.5.5:single (test-jar-with-           dependencies) on project spark-streaming-mqtt_2.10: Failed to create assembly: Error creating assembly archive test-        jar-with-dependencies: Problem creating jar: Execution exception (and the archive is probably corrupt but I could not         delete it): Java heap space -> [Help 1]
Any help?  I have a 64-bit windows 8 machine
"
Tathagata Das <tdas@databricks.com>,"Mon, 19 Oct 2015 19:06:51 -0700",Re: Problem building Spark,Annabel Melongo <melongo_annabel@yahoo.com>,"Seems to be a heap space issue for Maven. Have you configured Maven's
memory according the instruction on the web page?

export MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m""



lding With
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 19 Oct 2015 19:26:33 -0700",Re: Problem building Spark,Annabel Melongo <melongo_annabel@yahoo.com>,"See this thread
http://search-hadoop.com/m/q3RTtV3VFNdgNri2&subj=Re+Build+spark+1+5+1+branch+fails

 due to the following error: 
tting up Maven’s Memory Usage Specifying the Hadoop Version Building With Hive and JDBC Support Building for Scala 2.11
-plugin:2.5.5:single (test-jar-with- 
eate assembly: Error creating assembly archive test-
and the archive is probably corrupt but I could not 
"
Mike Hynes <91mbbh@gmail.com>,"Tue, 20 Oct 2015 01:40:24 -0400",Re: Gradient Descent with large model size,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi Alexander, Joseph, Evan,

I just wanted to weigh in an empirical result that we've had on a
standalone cluster with 16 nodes and 256 cores.

Typically we run optimization tasks with 256 partitions for 1
partition per core, and find that performance worsens with more
partitions than physical cores in communication operations, which
makes sense; the computational work has to be very high to justify so
many partitions.

However, with this setup, the default of numLevels = 2 in MLlib
methods using treeAggregate is generally a poor choice for large
datasets; it has been empirically far better in our tests to use
numLevels = log_2(16). The difference in clock time per iteration for
iterative optimization jobs can be huge; it takes 1.5--1.6x *less*
time to use more levels in the tree. I never see a difference running
running test suites on a single node for building, but on a large job
across the cluster it's very noticeable.

If there's to be any modifications of treeAggregate, I would recommend
some heuristics that uses numLevels = log_2(numNodes) or something
similar, or have the numLevels be specifiable in the MLlib APIs
instead of defaulting to 2.

Mike


ng
ion
now
nd
hen
ao
g.
ted
f
that
run
e
but
er
s.
 1
ld
799a/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1100
e
ted the
e
)(


-- 
Thanks,
Mike

---------------------------------------------------------------------


"
yaoqin <yaoqin@huawei.com>,"Tue, 20 Oct 2015 06:59:17 +0000",MapStatus too large for drvier,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
    When I run a spark job contains quite a lot of tasks(in my case is 200,000*200,000), the driver occured OOM mainly caused by the object MapStatus,
As is shown in the pic bellow, RoaringBitmap that used to mark which block is empty seems to use too many memories.
    Are there any data structue can replace RoaringBitmap to fix my problem?

    Thank you!
    Qin.
[cid:image001.png@01D10B46.7C248740]
"
YiZhi Liu <javelinjs@gmail.com>,"Tue, 20 Oct 2015 15:34:48 +0800",Ability to offer initial coefficients in ml.LogisticRegression,dev <dev@spark.apache.org>,"Hi all,

I noticed that in ml.classification.LogisticRegression, users are not
allowed to set initial coefficients, while it is supported in
mllib.classification.LogisticRegressionWithSGD.

Sometimes we know specific coefficients are close to the final optima.
e.g., we usually pick yesterday's output model as init coefficients
since the data distribution between two days' training sample
shouldn't change much.

Is there any concern for not supporting this feature?

-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 20 Oct 2015 01:37:55 -0700",Re: MapStatus too large for drvier,yaoqin <yaoqin@huawei.com>,"How big is your driver heap size? And any reason why you'd need 200k map
and 200k reduce tasks?



"
yaoqin <yaoqin@huawei.com>,"Tue, 20 Oct 2015 02:10:46 -0700 (MST)",Re: MapStatus too large for drvier,dev@spark.apache.org,"In our case, we are dealing with 20TB text data which is separated to about
200k map tasks and 200k reduce tasks, and our driver's memory is 15G,.



--

---------------------------------------------------------------------


"
prakhar jauhari <prak840@gmail.com>,"Tue, 20 Oct 2015 18:59:42 +0530","Re: Spark driver reducing total executors count even when Dynamic
 Allocation is disabled.",Saisai Shao <sai.sai.shao@gmail.com>,"Thanks sai for the input,

So the problem is : i start my job with some fixed number of executors, but
when a host running my executors goes unreachable, driver reduces the total
number of executors. And never increases it.

I have a repro for the issue, attaching logs:
!!!! Running spark job is configured for 2 executors, dynamic allocation
not enabled !!!!!!!

AM starts requesting the 2 executors:
15/10/19 12:25:58 INFO yarn.YarnRMClient: Registering the ApplicationMaster
15/10/19 12:25:59 INFO yarn.YarnAllocator: Will request 2 executor
containers, each with 1 cores and 1408 MB memory including 384 MB overhead
15/10/19 12:25:59 INFO yarn.YarnAllocator: Container request (host: Any,
capability: <memory:1408, vCores:1>)
15/10/19 12:25:59 INFO yarn.YarnAllocator: Container request (host: Any,
capability: <memory:1408, vCores:1>)
15/10/19 12:25:59 INFO yarn.ApplicationMaster: Started progress reporter
thread - sleep time : 5000

Executors launched:
15/10/19 12:26:04 INFO impl.AMRMClientImpl: Received new token for :
DN-2:58739
15/10/19 12:26:04 INFO impl.AMRMClientImpl: Received new token for :
DN-1:44591
15/10/19 12:26:04 INFO yarn.YarnAllocator: Launching container
container_1444841612643_0014_01_000002 for on host DN-2
15/10/19 12:26:04 INFO yarn.YarnAllocator: Launching ExecutorRunnable.
driverUrl: akka.tcp://sparkDriver@NN-1:35115/user/CoarseGrainedScheduler,
executorHostname: DN-2
15/10/19 12:26:04 INFO yarn.YarnAllocator: Launching container
container_1444841612643_0014_01_000003 for on host DN-1
15/10/19 12:26:04 INFO yarn.YarnAllocator: Launching ExecutorRunnable.
driverUrl: akka.tcp://sparkDriver@NN-1:35115/user/CoarseGrainedScheduler,
executorHostname: DN-1

Now my AM and executor 1 are running on DN-2, DN-1 has executor 2 running
on it. To reproduce this issue I removed IP from DN-1, until it was timed
out by spark.
15/10/19 13:03:30 INFO yarn.YarnAllocator: Driver requested a total number
of 1 executor(s).
15/10/19 13:03:30 INFO yarn.ApplicationMaster: Driver requested to kill
executor(s) 2.


So the driver has reduced the total number of executor to : 1
And now even when the DN comes up and rejoins the cluster, this count is
not increased.
If I had executor 1 running on a separate DN (not the same as AM's DN), and
that DN went unreachable, driver would reduce total number of executor to :
0 and the job hangs forever. And this is when i have not enabled Dynamic
allocation. My cluster has other DN's available, AM should request the
killed executors from yarn, and get it on some other DN's.

Regards,
Prakhar



"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 20 Oct 2015 22:48:08 +0800","Re: Spark driver reducing total executors count even when Dynamic
 Allocation is disabled.",prakhar jauhari <prak840@gmail.com>,"Hi Prakhar,

I start to know your problem, you expected that the killed exexcutor by
heartbeat mechanism should be launched again but seems not. This problem I
think is fixed in the version 1.5 of Spark, you could check this jira
https://issues.apache.org/jira/browse/SPARK-8119

Thanks
Saisai

2015年10月20日星期二，prakhar jauhari <prak840@gmail.com> 写道：

er
d
r
r
t
g
 so
ner
e
ed
if
s
 /
o
ed
reducing-total-executors-count-even-when-Dynamic-Allocation-is-disabled-tp14679.html
"
charmee <charmeep@gmail.com>,"Tue, 20 Oct 2015 13:48:33 -0700 (MST)",Re: If you use Spark 1.5 and disabled Tungsten mode ...,dev@spark.apache.org,"We had disabled tungsten after we found few performance issues, but had to
enable it back because we found that when we had large number of group by
fields, if tungsten is disabled the shuffle keeps failing. 

Here is an excerpt from one of our engineers with his analysis. 

With Tungsten Enabled (default in spark 1.5): 
~90 files of 0.5G each: 

Ingest (after applying broadcast lookups) : 54 min 
Aggregation (~30 fields in group by and another 40 in aggregation) : 18 min 

With Tungsten Disabled: 

Ingest : 30 min 
Aggregation : Erroring out 

GROUP BY, disabling tungsten is not working in the first place. 

Hope this helps. 

-Charmee



--

---------------------------------------------------------------------


"
Jerry Lam <chilinglam@gmail.com>,"Tue, 20 Oct 2015 17:11:37 -0400",Re: If you use Spark 1.5 and disabled Tungsten mode ...,charmee <charmeep@gmail.com>,"I disabled it because of the ""Could not acquire 65536 bytes of memory"". It
happens to fail the job. So for now, I'm not touching it.


"
Reynold Xin <rxin@databricks.com>,"Tue, 20 Oct 2015 14:27:33 -0700",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Jerry Lam <chilinglam@gmail.com>,"Jerry - I think that's been fixed in 1.5.1. Do you still see it?


"
Jerry Lam <chilinglam@gmail.com>,"Tue, 20 Oct 2015 18:20:04 -0400",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Yes, I'm using 1.5.1. I see them quite often. Sometimes it recovers but
sometimes it does not. For one particular job, it failed all the time with
the acquire-memory issue. I'm using spark on mesos with fine grained mode.
Does it make a difference?

Best Regards,

Jerry


"
shane knapp <sknapp@berkeley.edu>,"Tue, 20 Oct 2015 15:24:07 -0700","BUILD SYSTEM: builds are OOMing the jenkins workers, investigating.
 also need to reboot amp-jenkins-worker-06","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Patrick Wendell <patrick@databricks.com>, Josh Rosen <joshrosen@databricks.com>","starting this saturday (oct 17) we started getting alerts on the
jenkins workers that various processes were dying (specifically ssh).

since then, we've had half of our workers OOM due to java processes
and have had now to reboot two of them (-05 and -06).

if we look at the current machine that's wedged (amp-jenkins-worker-06), we see:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=spark-test/3814/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/HADOOP_VERSION=2.0.0-mr1-cdh4.1.2,label=spark-test/4508/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/HADOOP_VERSION=1.2.1,label=spark-test/4508/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/3868/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Compile-Master-Maven-with-YARN/4510/

have there been any changes to any of these builds that might have
caused this?  anyone have any ideas?

sadly, even though i saw that -06 was about to OOM and got a shell
opened before SSH died, my command prompt is completely unresponsive.
:(

shane

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 20 Oct 2015 15:35:41 -0700","Re: BUILD SYSTEM: builds are OOMing the jenkins workers,
 investigating. also need to reboot amp-jenkins-worker-06","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Patrick Wendell <patrick@databricks.com>, Josh Rosen <joshrosen@databricks.com>","-06 just kinda came back...

[root@amp-jenkins-worker-06 ~]# uptime
 15:29:07 up 26 days,  7:34,  2 users,  load average: 1137.91, 1485.69, 1635.89

the builds that, from looking at the process table, seem to be at
fault are the Spark-Master-Maven-pre-yar"
shane knapp <sknapp@berkeley.edu>,"Tue, 20 Oct 2015 15:39:41 -0700","Re: BUILD SYSTEM: builds are OOMing the jenkins workers,
 investigating. also need to reboot amp-jenkins-worker-06","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Patrick Wendell <patrick@databricks.com>, Josh Rosen <joshrosen@databricks.com>","here's the related stack trace from dmesg...  UID 500 is jenkins.

Out of memory: Kill process 142764 (java) score 40 or sacrifice child
Killed process 142764, UID 500, (java) total-vm:24685036kB,
anon-rss:5730824kB, file-rss:64kB
Uhhuh. NMI received for unknown reason 21 on CPU 0.
Do you have a strange power saving mode enabled?
Dazed and confused, but trying to continue
java: page allocation failure. order:2, mode:0xd0
Pid: 142764, comm: java Not tainted 2.6.32-573.3.1.el6.x86_64 #1
Call Trace:
 [<ffffffff8113770c>] ? __alloc_pages_nodemask+0x7dc/0x950
 [<ffffffff81074fa8>] ? copy_process+0x168/0x1530
 [<ffffffff810764c6>] ? do_fork+0x96/0x4c0
 [<ffffffff810b828b>] ? sys_futex+0x7b/0x170
 [<ffffffff81009598>] ? sys_clone+0x28/0x30
 [<ffffffff8100b3f3>] ? stub_clone+0x13/0x20
 [<ffffffff8100b0d2>] ? system_call_fastpath+0x16/0x1b


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 20 Oct 2015 15:46:30 -0700","Re: BUILD SYSTEM: builds are OOMing the jenkins workers,
 investigating. also need to reboot amp-jenkins-worker-06","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Patrick Wendell <patrick@databricks.com>, Josh Rosen <joshrosen@databricks.com>","amp-jenkins-worker-06 is back up.

my next bets are on -07 and -08...  :\

https://amplab.cs.berkeley.edu/jenkins/computer/


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 20 Oct 2015 16:04:54 -0700","Re: BUILD SYSTEM: builds are OOMing the jenkins workers,
 investigating. also need to reboot amp-jenkins-worker-06","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Patrick Wendell <patrick@databricks.com>, Josh Rosen <joshrosen@databricks.com>","ok, based on the timing, i *think* this might be the culprit:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=spark-test/3814/console


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 20 Oct 2015 18:10:05 -0700",Fwd: If you use Spark 1.5 and disabled Tungsten mode ...,"""dev@spark.apache.org"" <dev@spark.apache.org>","With Jerry's permission, sending this back to the dev list to close the
loop.


---------- Forwarded message ----------
From: Jerry Lam <chilinglam@gmail.com>
Date: Tue, Oct 20, 2015 at 3:54 PM
Subject: Re: If you use Spark 1.5 and disabled Tungsten mode ...
To: Reynold Xin <rxin@databricks.com>


Yup, coarse grained mode works just fine. :)
The difference is that by default, coarse grained mode uses 1 core per
task. If I constraint 20 cores in total, there can be only 20 tasks running
at the same time. However, with fine grained, I cannot set the total number
of cores and therefore, it could be +200 tasks running at the same time (It
is dynamic). So it might be the calculation of how much memory to acquire
fail when the number of cores cannot be known ahead of time because you
cannot make the assumption that X tasks running in an executor? Just my
guess...



"
"""qinggangwang7@gmail.com"" <qinggangwang7@gmail.com>","Wed, 21 Oct 2015 10:10:44 +0800",Set numExecutors by sparklaunch,"""dev@spark.apache.org"" <dev@spark.apache.org>","





Hi all,I want to launch spark job on yarn by java, but it seemes that there is no way to set numExecutors int the class SparkLauncher. Is there any way to set numExecutors ?Thanks


qinggangwang7@gmail.com

"
shane knapp <sknapp@berkeley.edu>,"Tue, 20 Oct 2015 19:33:22 -0700","Re: BUILD SYSTEM: builds are OOMing the jenkins workers,
 investigating. also need to reboot amp-jenkins-worker-06","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Patrick Wendell <patrick@databricks.com>, Josh Rosen <joshrosen@databricks.com>","well, it was -08, and ssh stopped working (according to the alerts)
just as i was logging in to kill off any errant processes.  i've taken
that worker offline in jenkins and will be rebooting it asap.

on a positive note, i was able to clear out -07 before anything
horrible happened to that one.


---------------------------------------------------------------------


"
yaoqin <yaoqin@huawei.com>,"Tue, 20 Oct 2015 20:02:42 -0700 (MST)",Re: MapStatus too large for drvier,dev@spark.apache.org,"I try to use org.apache.spark.util.collection.BitSet instead of
RoaringBitMap, and it can save about 20% memories but runs much slower.

For the 200K tasks job, 
RoaringBitMap uses 3 Long[1024] and 1 Short[3392]
=3*64*1024+16*3392=250880(bit)
BitSet uses 1 Long[3125] = 3125*64=200000(bit)

Memory saved = (250880-200000) / 250880 ≈20%



--
3.nabble.com/MapStatus-too-large-for-drvier-tp14704p14723.html
om.

---------------------------------------------------------------------


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 21 Oct 2015 15:00:47 +0530",Exception when using cosh,dev@spark.apache.org,"Hi! I was trying out different arithmetic functions in SparkSql. I noticed
a weird thing. While *sinh* and *tanh* functions are working, using
*cosh* results
in an error saying:

*Exception in thread ""main"" org.apache.spark.sql.AnalysisException:
undefined function cosh;*

The documentation says *cosh* is implemented since 1.4 and I also find it
weird that since *tanh* and *sinh* are implemented (and working) why would
*cosh* fail. I looked for it on jira but could not find any related bug.
Could someone confirm if this is an actual issue or something wrong on my
part.

Query I am using: SELECT cosh(`age`) as `data` FROM `table`
Spark Version: 10.4
SparkSql Version: 1.5.1

I am using the standard example of (name, age) schema (though I am setting
age as Double and not Int as I am trying out maths functions).

The entire error stack can be found here <http://pastebin.com/Tc7BQkCm>.

Thanks!

Shagun
"
tyronecai <tyronecai@163.com>,"Wed, 21 Oct 2015 17:46:17 +0800",SPARK_DRIVER_MEMORY doc wrong,dev@spark.apache.org,"In conf/spark-env.sh.template
https://github.com/apache/spark/blob/master/conf/spark-env.sh.template#L42
# - SPARK_DRIVER_MEMORY, Memory for Master (e.g. 1000M, 2G) (Default: 1G)


SPARK_DRIVER_MEMORY is memory config for driver, not master.

Thanks!


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 21 Oct 2015 10:24:38 +0000",Re: SPARK_DRIVER_MEMORY doc wrong,"tyronecai <tyronecai@163.com>, dev@spark.apache.org","You're welcome to open a little pull request to fix that.


"
Adrian Tanase <atanase@adobe.com>,"Wed, 21 Oct 2015 11:45:14 +0000",FW: Spark Streaming scheduler delay VS driver.cores,"""dev@spark.apache.org"" <dev@spark.apache.org>","Apologies for reposting this to the dev list but I’ve had no luck in getting information about spark.driver.cores on the user list.

Happy to create a PR with documentation improvements for the spark.driver.cores config setting after I get some more details.

Thanks!
-adrian

From: Adrian Tanase
Date: Monday, October 19, 2015 at 1che.org>""
Subject: Re: Spark Streaming scheduler delay VS driver.cores

Bump on this question – does anyone know what is the effect of spark.driver.cores on the driver's ability to manage larger clusters?

Any tips on setting a correct value? I’m running Spark streaming on Yarn / Hadoop 2.6 / Spark 1.5.1.

Thanks,
-adrian

From: Adrian Tanase
Date: Saturday, October 17, 2015 at 10:58 PM
To: ""user@spark.apache.org<mailto:user@spark.apache.org>""
Subject: Spark Streaming scheduler delay VS driver.cores

Hi,

I’ve recently bumped up the resources for a spark streaming job – and the performance started to degrade over time.
it was running fine on 7 nodes with 14 executor cores each (via Yarn) until I bumped executor.cores to 22 cores/node (out of 32 on AWS c3.xlarge, 24 for yarn)

The driver has 2 cores and 2 GB ram (usage is at zero).

For really low data volume it goes from 1-2 seconds per batch to 4-5 s/batch after about 6 hours, doing almost nothing. I’ve noticed that the scheduler delay is 3-4s, even 5-6 seconds for some tasks. Should be in the low tens of milliseconds. What’s weirder is that under moderate load (thousands of events per second) - the delay is not as obvious anymore.

After this I reduced the executor.cores to 20 and bumped driver.cores to 4 and it seems to be ok now.
However, this is totally empirical, I have not found any documentation, code samples or email discussion on how to properly set driver.cores.

Does anyone know:

  *   If I assign more cores to the driver/application manager, will it use them?
     *   I was looking at the process list with htop and only one of the jvm’s on the driver was really taking up CPU time
  *   What is a decent parallelism factor for a streaming app with 10-20 secs batch time? I found it odd that at  7 x 22 = 154 the driver is becoming a bottleneck
     *   I’ve seen people recommend 3-4 taks/core or ~1000 parallelism for clusters in the tens of nodes

Thanks in advance,
-adrian
"
Reynold Xin <rxin@databricks.com>,"Wed, 21 Oct 2015 10:29:43 -0700",Re: Exception when using cosh,Shagun Sodhani <sshagunsodhani@gmail.com>,"I think we made a mistake and forgot to register the function in the
registry:
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala

Do you mind submitting a pull request to fix this? Should be an one line
change. I filed a ticket to track this:
https://issues.apache.org/jira/browse/SPARK-11233





"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 21 Oct 2015 23:01:18 +0530",Re: Exception when using cosh,Reynold Xin <rxin@databricks.com>,"Sure! Would do that.

Thanks a lot


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 21 Oct 2015 23:14:57 +0530",Re: Exception when using cosh,Reynold Xin <rxin@databricks.com>,"@Reynold submitted the PR: https://github.com/apache/spark/pull/9199


"
Jerry Lam <chilinglam@gmail.com>,"Wed, 21 Oct 2015 16:16:15 -0400",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Reynold Xin <rxin@databricks.com>,"Hi guys,

There is another memory issue. Not sure if this is related to Tungsten this
time because I have it disable (spark.sql.tungsten.enabled=false). It
happens more there are too many tasks running (300). I need to limit the
number of task to avoid this. The executor has 6G. Spark 1.5.1 is been used.

Best Regards,

Jerry

org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:393)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Unable to acquire 67108864 bytes of memory
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPage(UnsafeExternalSorter.java:351)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:138)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:106)
	at org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:74)
	at org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:56)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:339)



"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 21 Oct 2015 13:16:33 -0700",Bringing up JDBC Tests to trunk,dev <dev@spark.apache.org>,"I have started looking into PR-8101 [1] and what is required to merge it
into trunk which will also unblock me around SPARK-10521 [2].

So here is the minimal plan I was thinking about :

- make the docker image version fixed so we make sure we are using the same
image all the time
- pull the required images on the Jenkins executors so tests are not
delayed/timedout because it is waiting for docker images to download
- create a profile to run the JDBC tests
- create daily jobs for running the JDBC tests


In parallel, I learned that Alan Chin from my team is working with the
AmpLab team to expand the build capacity for Spark, so I will use some of
the nodes he is preparing to test/run these builds for now.

Please let me know if there is anything else needed around this.


[1] https://github.com/apache/spark/pull/8101
[2] https://issues.apache.org/jira/browse/SPARK-10521

-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Josh Rosen <rosenville@gmail.com>,"Wed, 21 Oct 2015 13:26:10 -0700",Re: Bringing up JDBC Tests to trunk,Luciano Resende <luckbr1975@gmail.com>,"Hey Luciano,

some Dockerized MySQL testing utilities, so I'll take a peek at those to
see if there are any specifics of their solution that we should adapt for
Spark.


"
Reynold Xin <rxin@databricks.com>,"Wed, 21 Oct 2015 14:13:08 -0700",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Jerry Lam <chilinglam@gmail.com>,"Is this still Mesos fine grained mode?



"
Chester Chen <chester@alpinenow.com>,"Wed, 21 Oct 2015 14:33:12 -0700",Possible bug on Spark Yarn Client (1.5.1) during kerberos mode ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","All,

    just to see if this happens to other as well.

  This is tested against the

   spark 1.5.1 ( branch 1.5  with label 1.5.2-SNAPSHOT with commit on Tue
Oct 6, 84f510c4fa06e43bd35e2dc8e1008d0590cbe266)

   Spark deployment mode : Spark-Cluster

   Notice that if we enable Kerberos mode, the spark yarn client fails with
the following:

*Could not initialize class org.apache.hadoop.hive.ql.metadata.Hive*
*java.lang.NoClassDefFoundError: Could not initialize class
org.apache.hadoop.hive.ql.metadata.Hive*
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$obtainTokenForHiveMetastore(Client.scala:1252)
        at
org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:271)
        at
org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:629)
        at
org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:119)
        at org.apache.spark.deploy.yarn.Client.run(Client.scala:907)


Diving in Yarn Client.scala code and tested against different dependencies
and notice the followings:  if  the kerberos mode is enabled,
Client.obtainTokenForHiveMetastore()
will try to use scala reflection to get Hive and HiveConf and method on
these method.


      val hiveClass =
mirror.classLoader.loadClass(""org.apache.hadoop.hive.ql.metadata.Hive"")
      val hive = hiveClass.getMethod(""get"").invoke(null)

      val hiveConf = hiveClass.getMethod(""getConf"").invoke(hive)
      val hiveConfClass =
mirror.classLoader.loadClass(""org.apache.hadoop.hive.conf.HiveConf"")

      val hiveConfGet = (param: String) => Option(hiveConfClass
        .getMethod(""get"", classOf[java.lang.String])
        .invoke(hiveConf, param))


   If the ""org.spark-project.hive"" % ""hive-exec"" % ""1.2.1.spark"" is used,
then you will get above exception. But if we use the

       ""org.apache.hive"" % ""hive-exec"" ""0.13.1-cdh5.2.0""

 The above method will not throw exception.


  Here some questions and comments

0) is this a bug ?

1) Why spark-hive hive-exec behave differently ? I understand
spark-hive hive-exec has less dependencies

   but I would expect it functionally the same

2) Where I can find the source code for spark-hive hive-exec ?

3) regarding the method obtainTokenForHiveMetastore(),

   I would assume that the method will first check if the
hive-metastore uri is present before

   trying to get the hive metastore tokens, it seems to invoke the
reflection regardless the hive service in the cluster is enabled or
not.

4) Noticed the obtainTokenForHBase() in the same Class (Client.java) catches

   case e: java.lang.NoClassDefFoundError => logDebug(""HBase Class not
found: "" + e)

   and just ignore the exception ( log debug),

   but obtainTokenForHiveMetastore() does not catch
NoClassDefFoundError exception, I guess this is the problem.

private def *obtainTokenForHiveMetastore*(conf: Configuration,
credentials: Credentials) {

    // rest of code

 } catch {
    case e: java.lang.NoSuchMethodException => { logInfo(""Hive Method
not found "" + e); return }
    case e: java.lang.ClassNotFoundException => { logInfo(""Hive Class
not found "" + e); return }
    case e: Exception => { logError(""Unexpected Exception "" + e)
      throw new RuntimeException(""Unexpected exception"", e)
    }
  }
}


thanks


Chester
"
Jerry Lam <chilinglam@gmail.com>,"Wed, 21 Oct 2015 18:20:28 -0400",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Reynold Xin <rxin@databricks.com>,"Yes. The crazy thing about mesos running in fine grained mode is that there
is no way (correct me if I'm wrong) to set the number of cores per
executor. If one of my slaves on mesos has 32 cores, the fine grained mode
can allocate 32 cores on this executor for the job and if there are 32
tasks running on this executor at the same time, that is when the acquire
memory issue appears. Of course the 32 cores are dynamically allocated. So
mesos can take them back or put them in again depending on the cluster
utilization.


"
Chester Chen <chester@alpinenow.com>,"Wed, 21 Oct 2015 17:45:10 -0700",Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode ?,Doug Balog <doug@balog.net>,"Doug
  thanks for responding.
 >>I think Spark just needs to be compiled against 1.2.1

   Can you elaborate on this, or specific command you are referring ?

   In our build.scala, I was including the following

""org.spark-project.hive"" % ""hive-exec"" % ""1.2.1.spark"" intransitive()

   I am not sure how the Spark compilation is directly related to this,
please explain.

   When we submit the spark job, the we call Spark Yarn Client.scala
directly ( not using spark-submit).
   The client side is not depending on spark-assembly jar ( which is in the
hadoop cluster).  The job submission actually failed in the client side.

   Currently we get around this by replace the spark's hive-exec with
apache hive-exec.


Chester






:
:57)
mpl.java:43)
$obtainTokenForHiveMetastore(Client.scala:1252)
1)
cala:629)
,
e
e
.
t
r
"
Doug Balog <doug.sparkdev@dugos.com>,"Thu, 22 Oct 2015 03:05:07 -0400",Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode ?,Chester Chen <chester@alpinenow.com>,"

this, please explain.   

I was referring to this comment
https://issues.apache.org/jira/browse/SPARK-6906?focusedCommentId=14712336&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14712336
And the updated documentation,
http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore

Perhaps I misunderstood your question and why you are trying to compile against a different version of Hive.

directly ( not using spark-submit). 
in the hadoop cluster).  The job submission actually failed in the client side. 
apache hive-exec. 

Why are you using the Spark Yarn Client.scala directly and not using the SparkLauncher that was introduced in 1.4.0 ?


Doug

on Tue Oct 6, 84f510c4fa06e43bd35e2dc8e1008d0590cbe266)
fails with the following:
org.apache.hadoop.hive.ql.metadata.Hive
Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$obtainTokenForHiveMetastore(Client.scala:1252)
org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:271)
org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:629)
org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:119)
dependencies and notice the followings:  if  the kerberos mode is enabled, Client.obtainTokenForHiveMetastore() will try to use scala reflection to get Hive and HiveConf and method on these method.
mirror.classLoader.loadClass(""org.apache.hadoop.hive.ql.metadata.Hive"")
mirror.classLoader.loadClass(""org.apache.hadoop.hive.conf.HiveConf"")
used, then you will get above exception. But if we use the
bug.
https://issues.apache.org/jira/browse/SPARK-6906
spark-hive hive-exec has less dependencies
hive-metastore uri is present before
reflection regardless the hive service in the cluster is enabled or not.
get a delegation token would be an improvement.
too.
catches
not found: "" + e)
NoClassDefFoundError exception, I guess this is the problem.
credentials: Credentials) {
Method not found "" + e); return }
Class not found "" + e); return }
missed the case where the class was not found.


---------------------------------------------------------------------


"
chester@alpinenow.com,"Thu, 22 Oct 2015 00:25:47 -0700",Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode ?,Doug Balog <doug.sparkdev@dugos.com>,"Doug
    We are not trying to compiling against different version of hive. The 1.2.1.spark hive-exec is specified on spark 1.5.2 Pom file. We are moving from spark 1.3.1 to 1.5.1. Simply trying to supply the needed dependency. The rest of application (besides spark) simply uses hive 0.13.1.

    Yes we are using yarn client directly, there are many functions we need and modified are not provided in yarn client. The spark launcher in the current form does not satisfy our requirements (at least last time I see it) there is a discussion thread about several month ago.

     From spark 1.x  to 1.3.1, we fork the yarn client to achieve these goals ( yarn listener call backs, killApplications, yarn capacities call back etc). In current integration for 1.5.1, to avoid forking the spark, we simply subclass the yarn client overwrites a few methods. But we lost resource capacity call back and estimation by doing this.

    This is bit off the original topic. 

     I still think there is a bug related to the spark yarn client in case of Kerberos + spark hive-exec dependency. 

Chester

  

Sent from my iPad

ease explain.   
36&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14712336
-with-different-versions-of-hive-metastore
ainst a different version of Hive.
tly ( not using spark-submit). 
e hadoop cluster).  The job submission actually failed in the client side. 
he hive-exec. 
parkLauncher that was introduced in 1.4.0 ?

 Oct 6, 84f510c4fa06e43bd35e2dc8e1008d0590cbe266)
th the following:
doop.hive.ql.metadata.Hive
orImpl.java:57)
odAccessorImpl.java:43)
arn$Client$$obtainTokenForHiveMetastore(Client.scala:1252)
nt.scala:271)
xt(Client.scala:629)
cala:119)
es and notice the followings:  if  the kerberos mode is enabled, Client.obtainTokenForHiveMetastore() will try to use scala reflection to get Hive and HiveConf and method on these method.
ive.ql.metadata.Hive"")
op.hive.conf.HiveConf"")
 then you will get above exception. But if we use the
jira/browse/SPARK-6906
 hive-exec has less dependencies
ri is present before
tion regardless the hive service in the cluster is enabled or not.
 a delegation token would be an improvement.
.
ches
ound: "" + e)
xception, I guess this is the problem.
: Credentials) {
ot found "" + e); return }
ot found "" + e); return }
he case where the class was not found.

---------------------------------------------------------------------


"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Thu, 22 Oct 2015 09:02:10 +0000",repartitionAndSortWithinPartitions task shuffle phase is very slow,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, spark community
      I have an application which I try to migrate from MR to Spark.
      It will do some calculations from Hive and output to hfile which will
be bulk load to HBase Table, details as follow:

     Rdd<Element> input = getSourceInputFromHive()
     Rdd<Tuple2<byte[], byte[]>> mapSideResult =
input.glom().mapPartitions(/*some calculation, equivalent to MR mapper*/)
     // PS: the result in each partition has already been sorted according
to the lexicographical order during the calculation
     mapSideResult.repartitionAndSortWithPartitions(/*partition with
byte[][] which is HTable split key, equivalent to MR shuffle
*/).map(/*transform
Tuple2<byte[], byte[]> to Tuple2<ImmutableBytesWritable, KeyValue>/*equivalent
to MR reducer without output*/).saveAsNewAPIHadoopFile(/*write to hfile*/)

      This all works fine on a small dataset, and spark outruns MR by about
10%. However when I apply it on a dataset of 150 million records, MR is
about 100% faster than spark.(*MR 25min spark 50min*)
       After exploring into the application UI, it shows that in the
repartitionAndSortWithinPartitions stage is very slow, and in the shuffle
phase a 6GB size shuffle cost about 18min which is quite unreasonable
       *Can anyone help with this issue and give me some advice on this? **It’s
not iterative processing, however I believe Spark could be the same fast at
minimal.*

      Here are the cluster info:
          vm: 8 nodes * (128G mem + 64 core)
          hadoop cluster: hdp 2.2.6
          spark running mode: yarn-client
          spark version: 1.5.1
"
Luc Bourlier <luc.bourlier@typesafe.com>,"Thu, 22 Oct 2015 12:14:33 +0200",Re: Set numExecutors by sparklaunch,"""qinggangwang7@gmail.com"" <qinggangwang7@gmail.com>","Hi,

I don't know much about you particular use case, but most (if not all) of
the Spark command line parameters can also be specified as properties.
You should try to use

SparkLauncher.setConf(""spark.executor.instances"", ""3"")

HTH,
Luc

Luc Bourlier
*Spark Team  - Typesafe, Inc.*
luc.bourlier@typesafe.com

<http://www.typesafe.com>


"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 22 Oct 2015 17:35:17 +0530",Re: Guaranteed processing orders of each batch in Spark Streaming,Renjie Liu <liurenjie2008@gmail.com>,"I guess the order is guaranteed unless you set
the spark.streaming.concurrentJobs to a higher number than 1.

Thanks
Best Regards


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 22 Oct 2015 15:33:21 +0000","Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode
 ?",Chester Chen <chester@alpinenow.com>,"

Doug
   We are not trying to compiling against different version of hive. The 1.2.1.spark hive-exec is specified on spark 1.5.2 Pom file. We are moving from spark 1.3.1 to 1.5.1. Simply trying to supply the needed dependency. The rest of application (besides spark) simply uses hive 0.13.1.

   Yes we are using yarn client directly, there are many functions we need and modified are not provided in yarn client. The spark launcher in the current form does not satisfy our requirements (at least last time I see it) there is a discussion thread about several month ago.

    From spark 1.x  to 1.3.1, we fork the yarn client to achieve these goals ( yarn listener call backs, killApplications, yarn capacities call back etc). In current integration for 1.5.1, to avoid forking the spark, we simply subclass the yarn client overwrites a few methods. But we lost resource capacity call back and estimation by doing this.

   This is bit off the original topic.

    I still think there is a bug related to the spark yarn client in case of Kerberos + spark hive-exec dependency.

Chester


I think I understand what's being implied here.


  1.  In a secure cluster, a spark app needs a hive delegation token  to talk to hive
  2.  Spark yarn Client (org.apache.spark.deploy.yarn.Client) uses reflection to get the delegation token
  3.  The reflection doesn't work, a CFNE exception is logged
  4.  The app should still launch, but it'll be without a hive token , so attempting to work with Hive will fail.

I haven't seen this, because while I do test runs against a kerberos cluster, I wasn't talking to hive from the deployed app.


It sounds like this workaround works because the hive RPC protocol is compatible enough with 0.13 that a 0.13 client can ask hive for the token, though then your remote CP is stuck on 0.13

Looking at the hive class, the metastore has now made the hive constructor private and gone to a factory method (public static Hive get(HiveConf c) throws HiveException) to get an instance. The reflection code would need to be updated.

I'll file a bug with my name next to it



"
YiZhi Liu <javelinjs@gmail.com>,"Fri, 23 Oct 2015 00:06:52 +0800",Re: Ability to offer initial coefficients in ml.LogisticRegression,dev <dev@spark.apache.org>,"Would someone mind giving some hint?

2015-10-20 15:34 GMT+08:00 YiZhi Liu <javelinjs@gmail.com>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
Richard Marscher <rmarscher@localytics.com>,"Thu, 22 Oct 2015 12:38:35 -0400",Trouble creating JIRA issue,Dev <dev@spark.apache.org>,"Hi,

I'm working on following the guidelines for contributing code to Spark and
am trying to create a related JIRA issue. I'm logged into my user on
issues.apache.org, but I don't seem to have an option to create an issue,
just browse/search existing.

Any help would be appreciated!

Thanks

-- 
*Richard Marscher*
Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 22 Oct 2015 09:40:42 -0700",Re: Trouble creating JIRA issue,Richard Marscher <rmarscher@localytics.com>,"You can use the following link:
https://issues.apache.org/jira/secure/CreateIssue!default.jspa

Remember to select Spark as the project.


"
Charmee Patel <charmeep@gmail.com>,"Thu, 22 Oct 2015 17:18:06 +0000",Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode ?,"Steve Loughran <stevel@hortonworks.com>, Chester Chen <chester@alpinenow.com>","A similar issue occurs when interacting with Hive secured by Sentry.
https://issues.apache.org/jira/browse/SPARK-9042

By changing how Hive Context instance is created, this issue might also be
resolved.


"
DB Tsai <dbtsai@dbtsai.com>,"Thu, 22 Oct 2015 10:36:24 -0700",Re: Ability to offer initial coefficients in ml.LogisticRegression,YiZhi Liu <javelinjs@gmail.com>,"There is a JIRA for this. I know Holden is interested in this.



-- 
- DB

Sent from my iPhone
"
Chester Chen <chester@alpinenow.com>,"Thu, 22 Oct 2015 11:32:50 -0700",Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode ?,Charmee Patel <charmeep@gmail.com>,"Steven
      You summarized mostly correct. But there is a couple points I want to
emphasize.

     Not every cluster have the Hive Service enabled. So The Yarn Client
shouldn't try to get the hive delegation token just because security mode
is enabled.

     The Yarn Client code can check if the service is enabled or not
(possible by check hive metastore URI is present or other hive-site.xml
elements). If hive service is not enabled, then we don't need to get hive
delegation token. Hence we don't have the exception.

     If we still try to get hive delegation regardless hive service is
enabled or not ( like the current code is doing now), then code should
still launch the yarn container and spark job, as the user could simply run
a job against HDFS, not accessing Hive.  Of course, access Hive will fail.

     The 3rd point is that not sure why org.spark-project.hive's hive-exec
and orga.apache.hadoop.hive hive-exec behave differently for the same
method.

Chester










"
Reynold Xin <rxin@databricks.com>,"Thu, 22 Oct 2015 11:42:45 -0700",Re: repartitionAndSortWithinPartitions task shuffle phase is very slow,=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Why do you do a glom? It seems unnecessarily expensive to materialize each
partition in memory.



g
ransform
alent
)
R
**It’s
at
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 22 Oct 2015 20:15:55 +0000","Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode
 ?",Chester Chen <chester@alpinenow.com>,"

Steven
      You summarized mostly correct. But there is a couple points I want to emphasize.

     Not every cluster have the Hive Service enabled. So The Yarn Client shouldn't try to get the hive delegation token just because security mode is enabled.

I agree, but it shouldn't be failing with a stack trace. Log -yes, fail no.


     The Yarn Client code can check if the service is enabled or not (possible by check hive metastore URI is present or other hive-site.xml elements). If hive service is not enabled, then we don't need to get hive delegation token. Hence we don't have the exception.

     If we still try to get hive delegation regardless hive service is enabled or not ( like the current code is doing now), then code should still launch the yarn container and spark job, as the user could simply run a job against HDFS, not accessing Hive.  Of course, access Hive will fail.


That's exactly what should be happening: the token is only needed if the code tries to talk to hive. The problem is the YARN client doesn't know whether that's the case, so it tries every time. It shouldn't be failing though.

Created an issue to cover this; I'll see what reflection it takes. I'll also pull the code out into a method that can be tested standalone: we shoudn't have to wait until a run on UGI.isSecure() mode.

https://issues.apache.org/jira/browse/SPARK-11265


Meanwhile, for the curious, these slides include an animation of what goes on when a YARN app is launched in a secure cluster, to help explain why things seem a bit complicated

http://people.apache.org/~stevel/kerberos/2015-09-kerberos-the-madness.pptx

     The 3rd point is that not sure why org.spark-project.hive's hive-exec and orga.apache.hadoop.hive hive-exec behave differently for the same method.

Chester









A similar issue occurs when interacting with Hive secured by Sentry. https://issues.apache.org/jira/browse/SPARK-9042

By changing how Hive Context instance is created, this issue might also be resolved.


Doug
   We are not trying to compiling against different version of hive. The 1.2.1.spark hive-exec is specified on spark 1.5.2 Pom file. We are moving from spark 1.3.1 to 1.5.1. Simply trying to supply the needed dependency. The rest of application (besides spark) simply uses hive 0.13.1.

   Yes we are using yarn client directly, there are many functions we need and modified are not provided in yarn client. The spark launcher in the current form does not satisfy our requirements (at least last time I see it) there is a discussion thread about several month ago.

    From spark 1.x  to 1.3.1, we fork the yarn client to achieve these goals ( yarn listener call backs, killApplications, yarn capacities call back etc). In current integration for 1.5.1, to avoid forking the spark, we simply subclass the yarn client overwrites a few methods. But we lost resource capacity call back and estimation by doing this.

   This is bit off the original topic.

    I still think there is a bug related to the spark yarn client in case of Kerberos + spark hive-exec dependency.

Chester


I think I understand what's being implied here.


  1.  In a secure cluster, a spark app needs a hive delegation token  to talk to hive
  2.  Spark yarn Client (org.apache.spark.deploy.yarn.Client) uses reflection to get the delegation token
  3.  The reflection doesn't work, a CFNE exception is logged
  4.  The app should still launch, but it'll be without a hive token , so attempting to work with Hive will fail.

I haven't seen this, because while I do test runs against a kerberos cluster, I wasn't talking to hive from the deployed app.


It sounds like this workaround works because the hive RPC protocol is compatible enough with 0.13 that a 0.13 client can ask hive for the token, though then your remote CP is stuck on 0.13

Looking at the hive class, the metastore has now made the hive constructor private and gone to a factory method (public static Hive get(HiveConf c) throws HiveException) to get an instance. The reflection code would need to be updated.

I'll file a bug with my name next to it





"
Chester Chen <chester@alpinenow.com>,"Thu, 22 Oct 2015 13:54:41 -0700",Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode ?,Steve Loughran <stevel@hortonworks.com>,"Thanks Steve
       Likes the slides on kerberos, I have enough scars from Kerberos
while trying to integrated it with (Pig, MapRed, Hive JDBC, and HCatalog
and Spark) etc.  I am still having trouble making Impersonating to work for
HCatalog.  I might send you an offline email to ask some pointers

      Thanks for the ticket.

Chester






"
Steve Loughran <stevel@hortonworks.com>,"Thu, 22 Oct 2015 21:21:04 +0000","Re: Possible bug on Spark Yarn Client (1.5.1) during kerberos mode
 ?",Chester Chen <chester@alpinenow.com>,"

Thanks Steve
       Likes the slides on kerberos, I have enough scars from Kerberos while trying to integrated it with (Pig, MapRed, Hive JDBC, and HCatalog and Spark) etc.  I am still having trouble making Impersonating to work for HCatalog.  I might send you an offline email to ask some pointers


pointer? Run

      Thanks for the ticket.


Looking at the code, it seems to me you could bypass the hive loading by setting spark.yarn.security.tokens.hive.enabled=false

I've started on a patch, but so far just cleaned up the code ready for writing a test against it. Expect more tomorrow

https://github.com/apache/spark/pull/9232



"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Fri, 23 Oct 2015 02:20:56 +0000",Re: repartitionAndSortWithinPartitions task shuffle phase is very slow,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, Reynold
      Using glom() is because it is easy to adapt to calculation logic
already implemented in MR. And o be clear, we are still in POC.
      Since the results shows there is almost no difference between this
glom stage and the MR mapper, using glom here might not be the issue.
      I was trying to monitor the network traffic when repartition happens,
and it showed that the traffic peek is about 200 - 300MB/s while it stayed
at speed of about 3-4MB/s for a long time. Have you guys got any idea about
it?

Reynold Xin <rxin@databricks.com>于2015年10月23日周五 上午2:43写道：

h
)
transform
valent
MR
e
ould be
"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Fri, 23 Oct 2015 02:24:23 +0000",Re: repartitionAndSortWithinPartitions task shuffle phase is very slow,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>, 
	""dev@kylin.incubator.apache.org"" <dev@kylin.incubator.apache.org>","+kylin dev list

周千昊 <qhzhou@apache.org>于2015年10月23日周五 上午10:20写道：

y
周五 上午2:43写道：
*transform
ivalent
 MR
le
could be
"
YiZhi Liu <javelinjs@gmail.com>,"Fri, 23 Oct 2015 10:40:59 +0800",Re: Ability to offer initial coefficients in ml.LogisticRegression,"DB Tsai <dbtsai@dbtsai.com>, holden@pigscanfly.ca","Thank you Tsai.

Holden, would you mind posting the JIRA issue id here? I searched but
found nothing. Thanks.

2015-10-23 1:36 GMT+08:00 DB Tsai <dbtsai@dbtsai.com>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
Li Yang <liyang@apache.org>,"Fri, 23 Oct 2015 16:17:13 +0800",Re: repartitionAndSortWithinPartitions task shuffle phase is very slow,dev@kylin.incubator.apache.org,"Any advise on how to tune the repartitionAndSortWithinPartitions stage?
Any particular metrics or parameter to look into? Basically Spark and MR
shuffles the same amount of data, cause we kinda copied MR implementation
into Spark.

Let us know if more info is needed.


月23日周五 上午10:20写道：
s
le
周五 上午2:43写道：
h
r
k could be
"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Fri, 23 Oct 2015 09:50:43 +0000",Re: Re: repartitionAndSortWithinPartitions task shuffle phase is very slow,dev@kylin.incubator.apache.org,"We have not tried that yet, however both implementations on MR and spark
are tested on the same amount of partition and same cluster

250635732@qq.com <250635732@qq.com>于2015年10月23日周五 下午5:21写道：

y
月23日周五 上午10:20写道：
c
t
日周五 上午2:43写道：
e
.
h
by
e
le
ark could
"
Ewan Leith <ewan.leith@realitymine.com>,"Fri, 23 Oct 2015 10:31:49 +0000","RE: Dataframe nested schema inference from Json without type
 conflicts",Yin Huai <yhuai@databricks.com>,"Hi all,

It’s taken us a while, but one of my colleagues has made the pull request on github for our proposed solution to this,

https://issues.apache.org/jira/browse/SPARK-10947
https://github.com/apache/spark/pull/9249

It adds a parameter to the Json read otpions to force all primitives as a String type:

val jsonDf = sqlContext.read.option(""primitivesAsString"", ""true"").json(sampleJsonFile)

scala> jsonDf.printSchema()
root
|-- bigInteger: string (nullable = true)
|-- boolean: string (nullable = true)
|-- double: string (nullable = true)
|-- integer: string (nullable = true)
|-- long: string (nullable = true)
|-- null: string (nullable = true)
|-- string: string (nullable = true)

Thanks,
Ewan

From: Yin Huai [mailto:yhuai@databricks.com]
Sent: 01 October 2015 23:54
To: Ewan Leith <ewan.leith@realitymine.com>
Cc: rxin@databricks.com; dev@spark.apache.org
Subject: Re: Dataframe nested schema inference from Json without type conflicts

Hi Ewan,

For your use case, you only need the schema inference to pick up the structure of your data (basically you want spark sql to infer the type of complex values like arrays and structs but keep the type of primitive values as strings), right?

Thanks,

Yin

On Thu, Oct 1, 2015 at 2:27 PM, Ewan Leith <ewan.leith@realitymine.com<mailto:ewan.leith@realitymine.com>> wrote:

We could, but if a client sends some unexpected records in the schema (which happens more than I'd like, our schema seems to constantly evolve), its fantastic how Spark picks up on that data and includes it.



Passing in a fixed schema loses that nice additional ability, though it's what we'll probably have to adopt if we can't come up with a way to keep the inference working.



Thanks,

Ewan



------ Original message------

From: Reynold Xin

Date: Thu, 1 Oct 2015 22:12

To: Ewan Leith;

Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>;

Subject:Re: Dataframe nested schema inference from Json without type conflicts


You can pass the schema into json directly, can't you?

On Thu, Oct 1, 2015 at 10:33 AM, Ewan Leith <ewan.leith@realitymine.com<mailto:ewan.leith@realitymine.com>> wrote:
Hi all,

We really like the ability to infer a schema from JSON contained in an RDD, but when we’re using Spark Streaming on small batches of data, we sometimes find that Spark infers a more specific type than it should use, for example if the json in that small batch only contains integer values for a String field, it’ll class the field as an Integer type on one Streaming batch, then a String on the next one.

Instead, we’d rather match every value as a String type, then handle any casting to a desired type later in the process.

I don’t think there’s currently any simple way to avoid this that I can see, but we could add the functionality in the JacksonParser.scala file, probably in convertField.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JacksonParser.scala

Does anyone know an easier and cleaner way to do this?

Thanks,
Ewan


"
mkhaitman <mark.khaitman@chango.com>,"Fri, 23 Oct 2015 12:05:55 -0700 (MST)",Spark.Executor.Cores question,dev@spark.apache.org,"Regarding the 'spark.executor.cores' config option in a Standalone spark
environment, I'm curious about whether there's a way to enforce the
following logic:

*- Max cores per executor = 4*
** Max executors PER application PER worker = 1*

In order to force better balance across all workers, I want to ensure that a
single spark job can only ever use a specific upper limit on the number of
cores for each executor it holds, however, do not want a situation where it
can spawn 3 executors on a worker and only 1/2 on the others. Some spark
jobs end up using much more memory during aggregation tasks (joins /
groupBy's) which is more heavily impacted by the number of cores per
executor for that job. 

If this kind of setup/configuration doesn't already exist for Spark, and
others see the benefit of what I mean by this, where would be the best
location to insert this logic?

Mark.



--

---------------------------------------------------------------------


"
Robert Dodier <robert.dodier@gmail.com>,"Fri, 23 Oct 2015 12:43:42 -0700",slightly more informative error message in MLUtils.loadLibSVMFile,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

MLUtils.loadLibSVMFile verifies that indices are 1-based and
increasing, and otherwise triggers an error. I'd like to suggest that
the error message be a little more informative. I ran into this when
loading a malformed file. Exactly what gets printed isn't too crucial,
maybe you would want to print something else, all that matters is to
give some context so that the user can find the problem more quickly.

Hope this helps in some way.

Robert Dodier

PS.

diff --git a/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala
b/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala
index 81c2f0c..6f5f680 100644
--- a/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala
+++ b/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala
@@ -91,7 +91,7 @@ object MLUtils {
         val indicesLength = indices.length
         while (i < indicesLength) {
           val current = indices(i)
-          require(current > previous, ""indices should be one-based
and in ascending order"" )
+          require(current > previous, ""indices should be one-based
and in ascending order; found current="" + current + "", previous="" +
previous + ""; line=\"""" + line + ""\"""" )
           previous = current
           i += 1
         }

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 25 Oct 2015 08:07:07 +0100",[VOTE] Release Apache Spark 1.5.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark
version 1.5.2. The vote is open until Wed Oct 28, 2015 at 08:00 UTC and
passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5.2
[ ] -1 Do not release this package because ...


The release fixes 51 known issues in Spark 1.5.1, listed here:
http://s.apache.org/spark-1.5.2

The tag to be voted on is v1.5.2-rc1:
https://github.com/apache/spark/releases/tag/v1.5.2-rc1

The release files, including signatures, digests, etc. can be found at:
*http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc1-bin/
<http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc1-bin/>*

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
- as version 1.5.2-rc1:
https://repository.apache.org/content/repositories/orgapachespark-1151
- as version 1.5.2:
https://repository.apache.org/content/repositories/orgapachespark-1150

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-v1.5.2-rc1-docs/


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.1. Bugs already present
in 1.5.1 will not block this release.

===============================================================
What should happen to JIRA tickets still targeting 1.5.2?
===============================================================
Please target 1.5.3 or 1.6.0.
"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Sun, 25 Oct 2015 21:31:31 +0530",Adding support for truncate operator,dev@spark.apache.org,"Hi! I noticed that SparkSQL does not support truncate operator as of now.
Can we add it? I am willing to send over a PR for it
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 25 Oct 2015 09:06:23 -0700",Re: Adding support for truncate operator,Shagun Sodhani <sshagunsodhani@gmail.com>,"Have you seen the following ?
[SPARK-3907][SQL] Add truncate table support

Cheers


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 25 Oct 2015 09:28:43 -0700",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Reynold Xin <rxin@databricks.com>,"When I ran the following command:
~/apache-maven-3.3.3/bin/mvn -Phive -Phive-thriftserver -Pyarn -Phadoop-2.4
-Dhadoop.version=2.6.0 package

I got:

testChildProcLauncher(org.apache.spark.launcher.SparkLauncherSuite)  Time
elapsed: 0.031 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<1>
        at org.junit.Assert.fail(Assert.java:93)
        at org.junit.Assert.failNotEquals(Assert.java:647)
        at org.junit.Assert.assertEquals(Assert.java:128)
        at org.junit.Assert.assertEquals(Assert.java:472)
        at org.junit.Assert.assertEquals(Assert.java:456)
        at
org.apache.spark.launcher.SparkLauncherSuite.testChildProcLauncher(SparkLauncherSuite.java:105)

java version ""1.7.0_67""
Java(TM) SE Runtime Environment (build 1.7.0_67-b01)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)

I
checked ./launcher/target/surefire-reports/TEST-org.apache.spark.launcher.SparkLauncherSuite.xml
but didn't get much clue.

FYI


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Sun, 25 Oct 2015 22:00:43 +0530",Re: Adding support for truncate operator,Ted Yu <yuzhihong@gmail.com>,"My bad. I did not specify that I meant truncate operator on a column
similar to how other maths operators work.


"
Sean Owen <sowen@cloudera.com>,"Sun, 25 Oct 2015 16:30:45 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Ted Yu <yuzhihong@gmail.com>,"I believe you still need to ""clean package"" and then ""test""
separately. Or did the change to make that unnecessary go in to 1.5?

FWIW I do not see this (my results coming soon)


---------------------------------------------------------------------


"
Pranay Tonpay <ptonpay@gmail.com>,"Sun, 25 Oct 2015 22:35:15 +0530",spark-sql / apache-drill / jboss-tiied,dev@spark.apache.org,"Hi,
In terms of federated query, has anyone done any evaluation between
spark-sql and drill and jboss-tiied.
I have a very urgent requirement for creating a virtualized layer (sitting
atop several databases) and am evaluating these 3 as an option.. Any help
would be appreciated.
I know Spark-SQL has the benefit that i can invoke MLLib algorithms on the
data fetched, but apart from that, any other considerations ?
Drill does not seem to have support for many data sources..

Any inputs ?

thx
pranay
"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 25 Oct 2015 11:23:20 -0700",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Sean Owen <sowen@cloudera.com>,"You're correct, Sean: That build change isn't in branch-1.5, so the
two-phase build is still needed there.


"
Sean Owen <sowen@cloudera.com>,"Sun, 25 Oct 2015 21:25:30 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Reynold Xin <rxin@databricks.com>,"The signatures and licenses are fine. I continue to get failures in
these tests though, with ""-Pyarn -Phadoop-2.6 -Phive
-Phive-thriftserver"" on Ubuntu 15 / Java 7.

- Unpersisting HttpBroadcast on executors and driver in distributed
mode *** FAILED ***
  java.util.concurrent.TimeoutException: Can't find 2 executors before
10000 milliseconds elapsed
  at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561)
  at org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:313)
  at org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistHttpBroadcast(BroadcastSuite.scala:238)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$12.apply$mcV$sp(BroadcastSuite.scala:149)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$12.apply(BroadcastSuite.scala:149)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$12.apply(BroadcastSuite.scala:149)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  ...
- Unpersisting TorrentBroadcast on executors only in local mode
- Unpersisting TorrentBroadcast on executors and driver in local mode
- Unpersisting TorrentBroadcast on executors only in distributed mode
- Unpersisting TorrentBroadcast on executors and driver in distributed
mode *** FAILED ***
  java.util.concurrent.TimeoutException: Can't find 2 executors before
10000 milliseconds elapsed
  at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561)
  at org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:313)
  at org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:287)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply$mcV$sp(BroadcastSuite.scala:165)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  ...


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 25 Oct 2015 15:50:28 -0700",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Should 1.5.2 wait for Josh's fix of SPARK-11293?


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Sun, 25 Oct 2015 16:40:58 -0700",Duplicate (?) code paths to handle Executor failures,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I noticed that when the JVM for an executor fails, in Standalone mode, we
have two duplicate code paths that handle the failure, one via Akka, and
the second via the Worker/ExecutorRunner:

via Akka:
(1) CoarseGrainedSchedulerBackend is notified that the remote Akka endpoint
is disconnected:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L189
and it calls CoarseGrainedSchedulerBackend.removeExecutor
(2) removeExecutor() tells the task scheduler to reschedule all of the
tasks that were running on that executor

via the Worker/ExecutorRunner:
(1) The ExecutorRunner notes that the Executor process has failed and
notifies the Spark Master
(2) The Master notifies the AppClient (for the particular application),
which then notifies the SparkDeploySchedulerBackend (which subclasses
SparkDeploySchedulerBackend)
(3) SparkDeploySchedulerBackend calls
CoarseGrainedSchedulerBackend.removeExecutor, which eventually tells the
task scheduler that the executor was lost and all tasks running on it
should be re-scheduled (as above)

For YARN, my understanding is that there is a 3rd code path where the
YarnAllocator's processCompletedContainers() gets information about the
process's exit from the master, and translates it into an ""ExecutorExited""
message that gets passed to the scheduler, similar to in the
Worker/ExecutorRunner case (YARN folks, is this correct?).

It's confusing and error prone to have these multiple different ways of
handling failures (I ran into this problem because I was fixing a bug where
one of the code paths can lead to a hang, but the other one doesn't).  Can
we eliminate all but one of these code paths?  Is there a reason for the
duplicate error handling?

Do all of the cluster managers (Standalone, YARN, Mesos) communicate in
some way when an Executor has failed, so we can ignore the Akka code path?
The Akka code path is most tempting to eliminate because it has less
information about the failure (the other code path typically has an exit
code for the process, at a minimum).

I'm also curious if others have seen this issue; for example, Marcelo, I'm
wondering if this came up in your attempts to treat YARN pre-emption
differently (did you run into issues where, when YARN pre-empts an
executor, Spark gets the ""Rpc disassociated"" failure from AKKA before the
more useful error from Yarn saying that the executor was pre-empted?).

-Kay

----------------------------------

To reproduce this issue, you can run one of these jobs:

sc.parallelize(1 to 10, 2).foreach { x => if (x == 1) throw new
OutOfMemoryError(""test OOM"") }

or

sc.parallelize(1 to 10, 2).foreach { x => if (x == 1) System.exit(42) }
"
Josh Rosen <rosenville@gmail.com>,"Sun, 25 Oct 2015 16:55:01 -0700",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Mark Hamstra <mark@clearstorydata.com>,"Hi Mark,

The shuffle memory leaks that I identified in SPARK-11239 have been around
for multiple releases and it's not clear whether they have caused
performance problems in real workloads, so I would say that it's fine to
move the release forward without including my patch. If we have to cut
another release candidate for some other reason, though, then it might be
nice to include it then, but I don't think it qualifies as a
release-blocker by itself.


"
Rohith P <rparameshwara@couponsinc.com>,"Sun, 25 Oct 2015 22:18:16 -0700 (MST)",Re: Unable to run applications on spark in standalone cluster mode,dev@spark.apache.org,"No.. the ./sbin/start-master.sh --ip option did not work... It is still the
same error




--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 25 Oct 2015 23:50:23 -0700",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Reynold Xin <rxin@databricks.com>,"I verified that the issue with build binaries being present in the source
release is fixed. Haven't done enough vetting for a full vote, but did
verify that.


"
<prajod.vettiyattil@wipro.com>,"Mon, 26 Oct 2015 07:12:29 +0000",RE: spark-sql / apache-drill / jboss-tiied,<ptonpay@gmail.com>,"Hi,

Though not the comparison you wanted, I have implemented a SparkSQL vs Hive performance comparison with one master and two worker instances. Data was stored in HDFS. SparkSQL showed promise. I used Spark version 1.4 and Hadoop version 2.6.

https://hivevssparksql.wordpress.com/

The table data size used for the performance comparison ranged 100,000 to 100 million rows. The master and slaves ran on EC2 m3.xlarge(4core/15GB RAM).

In the graph you can observe the consistent response behavior of SparkSQL.

Regards,
Prajod

From: Pranay Tonpay [mailto:ptonpay@gmail.com]
Sent: 25 October 2015 22:35
To: dev@spark.apache.org
Subject: spark-sql / apache-drill / jboss-tiied

Hi,
In terms of federated query, has anyone done any evaluation between spark-sql and drill and jboss-tiied.
I have a very urgent requirement for creating a virtualized layer (sitting atop several databases) and am evaluating these 3 as an option.. Any help would be appreciated.
I know Spark-SQL has the benefit that i can invoke MLLib algorithms on the data fetched, but apart from that, any other considerations ?
Drill does not seem to have support for many data sources..

Any inputs ?
thx
pranay
The information contained in this electronic message and any attachments to this message are intended for the exclusive use of the addressee(s) and may contain proprietary, confidential or privileged information. If you are not the intended recipient, you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately and destroy all copies of this message and any attachments. WARNING: Computer viruses can be transmitted via email. The recipient should check this email and any attachments for the presence of viruses. The company accepts no liability for any damage caused by any virus transmitted by this email. www.wipro.com
"
Jinfeng Li <lijinf8@gmail.com>,"Mon, 26 Oct 2015 08:57:20 +0000",Loading Files from HDFS Incurs Network Communication,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, I find that loading files from HDFS can incur huge amount of network
traffic. Input size is 90G and network traffic is about 80G. By my
understanding, local files should be read and thus no network communication
is needed.

I use Spark 1.5.1, and the following is my code:

val textRDD = sc.textFile(""hdfs://master:9000/inputDir"")
textRDD.count

Jeffrey
"
Sean Owen <sowen@cloudera.com>,"Mon, 26 Oct 2015 09:00:08 +0000",Re: Loading Files from HDFS Incurs Network Communication,Jinfeng Li <lijinf8@gmail.com>,"-dev +user
How are you measuring network traffic?
It's not in general true that there will be zero network traffic, since not
all executors are local to all data. That can be the situation in many
cases but not always.


"
Meihua Wu <rotationsymmetry14@gmail.com>,"Mon, 26 Oct 2015 11:42:53 -0700",Spark Implementation of XGBoost,"dev <dev@spark.apache.org>, user <user@spark.apache.org>","Hi Spark User/Dev,

Inspired by the success of XGBoost, I have created a Spark package for
gradient boosting tree with 2nd order approximation of arbitrary
user-defined loss functions.

https://github.com/rotationsymmetry/SparkXGBoost

Currently linear (normal) regression, binary classification, Poisson
regression are supported. You can extend with other loss function as
well.

L1, L2, bagging, feature sub-sampling are also employed to avoid overfitting.

Thank you for testing. I am looking forward to your comments and
suggestions. Bugs or improvements can be reported through GitHub.

Many thanks!

Meihua

---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 26 Oct 2015 16:06:14 -0700",Re: Spark Implementation of XGBoost,Meihua Wu <rotationsymmetry14@gmail.com>,"Interesting. For feature sub-sampling, is it per-node or per-tree? Do
you think you can implement generic GBM and have it merged as part of
Spark codebase?

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D



---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 26 Oct 2015 16:07:10 -0700",Re: Spark Implementation of XGBoost,Meihua Wu <rotationsymmetry14@gmail.com>,"Also, does it support categorical feature?

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D



---------------------------------------------------------------------


"
YiZhi Liu <javelinjs@gmail.com>,"Tue, 27 Oct 2015 09:16:48 +0800",Re: Spark Implementation of XGBoost,"DB Tsai <dbtsai@dbtsai.com>, Meihua Wu <rotationsymmetry14@gmail.com>","There's an xgboost exploration jira SPARK-8547. Can it be a good start?

2015-10-27 7:07 GMT+08:00 DB Tsai <dbtsai@dbtsai.com>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
=?UTF-8?B?5ZGo5Y2D5piK?= <z.qianhao@gmail.com>,"Tue, 27 Oct 2015 02:33:07 +0000",Re: Re: repartitionAndSortWithinPartitions task shuffle phase is very slow,dev@kylin.incubator.apache.org,"I have replace default java serialization with Kyro.
It indeed reduce the shuffle size and the performance has been improved,
however the shuffle speed remains unchanged.
I am quite newbie to Spark, does anyone have idea about towards which
direction I should go to find the root cause?

周千昊 <qhzhou@apache.org>于2015年10月23日周五 下午5:50写道：

周五 下午5:21写道：
n
0月23日周五 上午10:20写道：
ic
.
ot
日周五 上午2:43写道：
ze
k.
th
he
park
Best Regard
ZhouQianhao
"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 26 Oct 2015 20:08:20 -0700",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Reynold Xin <rxin@databricks.com>,"Guys,
   The sc.version returns 1.5.1 in python and scala. Is anyone getting the
same results ? Probably I am doing something wrong.
Cheers
<k/>


"
Meihua Wu <rotationsymmetry14@gmail.com>,"Mon, 26 Oct 2015 20:37:17 -0700",Re: Spark Implementation of XGBoost,DB Tsai <dbtsai@dbtsai.com>,"Hi DB Tsai,

Thank you very much for your interest and comment.

1) feature sub-sample is per-node, like random forest.

2) The current code heavily exploits the tree structure to speed up
the learning (such as processing multiple learning node in one pass of
the training data). So a generic GBM is likely to be a different
codebase. Do you have any nice reference of efficient GBM? I am more
than happy to look into that.

3) The algorithm accept training data as a DataFrame with the
featureCol indexed by VectorIndexer. You can specify which variable is
categorical in the VectorIndexer. Please note that currently all
categorical variables are treated as ordered. If you want some
categorical variables as unordered, you can pass the data through
unordered categorical variable using the approach in RF in Spark ML
(Please see roadmap in the README.md)

Thanks,

Meihua




---------------------------------------------------------------------


"
Meihua Wu <rotationsymmetry14@gmail.com>,"Mon, 26 Oct 2015 20:46:29 -0700",Re: Spark Implementation of XGBoost,YiZhi Liu <javelinjs@gmail.com>,"Hi YiZhi,

Thank you for mentioning the jira. I will add a note to the jira.

Meihua


---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Tue, 27 Oct 2015 01:02:34 -0700",Re: Spark Implementation of XGBoost,Meihua Wu <rotationsymmetry14@gmail.com>,"Hi Meihua,

For categorical features, the ordinal issue can be solved by trying
all kind of different partitions 2^(q-1) -1 for q values into two
groups. However, it's computational expensive. In Hastie's book, in
9.2.4, the trees can be trained by sorting the residuals and being
learnt as if they are ordered. It can be proven that it will give the
optimal solution. I have a proof that this works for learning
regression trees through variance reduction.

I'm also interested in understanding how the L1 and L2 regularization
within the boosting works (and if it helps with overfitting more than
shrinkage).

Thanks.

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 27 Oct 2015 09:43:39 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Krishna Sankar <ksankar42@gmail.com>,"Ah, good point. I also see it still reads 1.5.1. I imagine we just need
another sweep to update all the version strings.


"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Oct 2015 11:03:10 +0100",Re: [VOTE] Release Apache Spark 1.5.2 (RC1),Sean Owen <sowen@cloudera.com>,"Yup looks like I missed that. I will build a new one.


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Tue, 27 Oct 2015 15:49:44 +0530",Exception when using some aggregate operators,dev@spark.apache.org,"Hi! I was trying out some aggregate  functions in SparkSql and I noticed
that certain aggregate operators are not working. This includes:

approxCountDistinct
countDistinct
mean
sumDistinct

For example using countDistinct results in an error saying
*Exception in thread ""main"" org.apache.spark.sql.AnalysisException:
undefined function cosh;*

I had a similar issue with cosh operator
<http://apache-spark-developers-list.1001551.n3.nabble.com/Exception-when-using-cosh-td14724.html>
as well some time back and it turned out that it was not registered in the
registry:
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala


*I* *think it is the same issue again and would be glad to send over a PR
if someone can confirm if this is an actual bug and not some mistake on my
part.*


Query I am using: SELECT countDistinct(`age`) as `data` FROM `table`
Spark Version: 10.4
SparkSql Version: 1.5.1

I am using the standard example of (name, age) schema (though I am setting
age as Double and not Int as I am trying out maths functions).

The entire error stack can be found here <http://pastebin.com/G6YzQXnn>.

Thanks!
"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Tue, 27 Oct 2015 19:53:38 +0530",Re: Exception when using some aggregate operators,dev@spark.apache.org,"Oops seems I made a mistake. The error message is : Exception in thread
""main"" org.apache.spark.sql.AnalysisException: undefined function
countDistinct

"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Oct 2015 15:28:20 +0100",Re: Exception when using some aggregate operators,Shagun Sodhani <sshagunsodhani@gmail.com>,"Try

count(distinct columnane)

In SQL distinct is not part of the function name.


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Tue, 27 Oct 2015 20:02:09 +0530",Re: Exception when using some aggregate operators,Reynold Xin <rxin@databricks.com>,"Will try in a while when I get back. I assume this applies to all functions
other than mean. Also countDistinct is defined along with all other SQL
functions. So I don't get ""distinct is not part of function name"" part.

"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Tue, 27 Oct 2015 22:42:34 +0530",Re: Exception when using some aggregate operators,Reynold Xin <rxin@databricks.com>,"So I tried @Reynold's suggestion. I could get countDistinct and sumDistinct
running but  mean and approxCountDistinct do not work. (I guess I am using
the wrong syntax for approxCountDistinct) For mean, I think the
registry entry is missing. Can someone clarify that as well?


"
Sjoerd Mulder <sjoerdmulder@gmail.com>,"Tue, 27 Oct 2015 20:56:09 +0100",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Reynold Xin <rxin@databricks.com>,"I have disabled it because of it started generating ERROR's when upgrading
from Spark 1.4 to 1.5.1

2015-10-27T20:50:11.574+0100 ERROR TungstenSort.newOrdering() - Failed to
generate ordering, fallback to interpreted
java.util.concurrent.ExecutionException: java.lang.Exception: failed to
compile: org.codehaus.commons.compiler.CompileException: Line 15, Column 9:
Invalid character input ""@"" (character code 64)

public SpecificOrdering
generate(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
  return new SpecificOrdering(expr);
}

class SpecificOrdering extends
org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {

  private org.apache.spark.sql.catalyst.expressions.Expression[]
expressions;



  public
SpecificOrdering(org.apache.spark.sql.catalyst.expressions.Expression[]
expr) {
    expressions = expr;

  }

  @Override
  public int compare(InternalRow a, InternalRow b) {
    InternalRow i = null;  // Holds current row being evaluated.

    i = a;
    boolean isNullA2;
    long primitiveA3;
    {
      /* input[2, LongType] */

      boolean isNull0 = i.isNullAt(2);
      long primitive1 = isNull0 ? -1L : (i.getLong(2));

      isNullA2 = isNull0;
      primitiveA3 = primitive1;
    }
    i = b;
    boolean isNullB4;
    long primitiveB5;
    {
      /* input[2, LongType] */

      boolean isNull0 = i.isNullAt(2);
      long primitive1 = isNull0 ? -1L : (i.getLong(2));

      isNullB4 = isNull0;
      primitiveB5 = primitive1;
    }
    if (isNullA2 && isNullB4) {
      // Nothing
    } else if (isNullA2) {
      return 1;
    } else if (isNullB4) {
      return -1;
    } else {
      int comp = (primitiveA3 > primitiveB5 ? 1 : primitiveA3 < primitiveB5
? -1 : 0);
      if (comp != 0) {
        return -comp;
      }
    }

    return 0;
  }
}

at
org.spark-project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
at
org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
at
org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
at
org.spark-project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
at
org.spark-project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
at
org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
at
org.spark-project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
at
org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
at
org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:362)
at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering$.create(GenerateOrdering.scala:139)
at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering$.create(GenerateOrdering.scala:37)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:425)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:422)
at org.apache.spark.sql.execution.SparkPlan.newOrdering(SparkPlan.scala:294)
at org.apache.spark.sql.execution.TungstenSort.org
$apache$spark$sql$execution$TungstenSort$$preparePartition$1(sort.scala:131)
at
org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:169)
at
org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:169)
at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:59)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
at org.apache.spark.scheduler.Task.run(Task.scala:88)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)


2015-10-14 21:00 GMT+02:00 Reynold Xin <rxin@databricks.com>:

"
Josh Rosen <joshrosen@databricks.com>,"Tue, 27 Oct 2015 19:59:59 +0000",Re: If you use Spark 1.5 and disabled Tungsten mode ...,"Sjoerd Mulder <sjoerdmulder@gmail.com>, Reynold Xin <rxin@databricks.com>","Hi Sjoerd,

Did your job actually *fail* or did it just generate many spurious
exceptions? While the stacktrace that you posted does indicate a bug, I
don't think that it should have stopped query execution because Spark
should have fallen back to an interpreted code path (note the ""Failed to
generate ordering, fallback to interpreted"" in the error message).


"
Sjoerd Mulder <sjoerdmulder@gmail.com>,"Tue, 27 Oct 2015 21:12:12 +0100",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Josh Rosen <joshrosen@databricks.com>,"No the job actually doesn't fail, but since our tests is generating all
these stacktraces i have disabled the tungsten mode just to be sure (and
don't have gazilion stacktraces in production).

2015-10-27 20:59 GMT+01:00 Josh Rosen <joshrosen@databricks.com>:

"
agg212 <agg@cs.brown.edu>,"Tue, 27 Oct 2015 13:47:08 -0700 (MST)",Pickle Spark DataFrame,dev@spark.apache.org,"Hi, I'd like to ""pickle"" a Spark DataFrame object and have tried the
following:

        import pickle
        data = sparkContext.jsonFile(data_file) #load file
        with open('out.pickle', 'wb') as handle:
            pickle.dump(data, handle)

If I convert ""data"" to a Pandas DataFrame (e.g., using data.toPandas()), the
above code works.  Does anybody have any idea how to do this?



--

---------------------------------------------------------------------


"
Richard Marscher <rmarscher@localytics.com>,"Tue, 27 Oct 2015 16:51:41 -0400",Re: Spark.Executor.Cores question,mkhaitman <mark.khaitman@chango.com>,"Hi Mark,

if you know your cluster's number of workers and cores per worker you can
set this up when you create a SparkContext and shouldn't need to tinker
with the 'spark.executor.cores' setting. That setting is for running
multiple executors per application per worker, which you are saying you
don't want.

How to do what I'm describing? In standalone mode, you will be assigned
cores in round robin order through the cluster's available workers (someone
correct me if that has changed since 1.3). So if you have 4 workers and set
`spark.cores.max` to `16` on your SparkContext then you will have 4
executors on each worker that are using 4 cores each. If you set
`spark.cores.max` to `6` then two executors would have 2 cores and two
executors would have 1 core.

Hope that helps




-- 
*Richard Marscher*
Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
mkhaitman <mark.khaitman@chango.com>,"Tue, 27 Oct 2015 13:57:18 -0700 (MST)",Re: Spark.Executor.Cores question,dev@spark.apache.org,"Hi Richard,

Thanks for the response. 

I should have added that the specific case where this becomes a problem is
when one of the executors for that application is lost/killed prematurely,
and the application attempts to spawn up a new executor without
consideration as to whether an executor already exists on the other node. 

In your example, if one of the executors dies for some reason (memory
exhaustion, or something crashed it), if there are still free cores on the
other nodes, it will spawn an extra executor, which can lead to further
memory problems on the other node that it just spawned on. 

Hopefully that clears up what I mean :) 

Mark.



--

---------------------------------------------------------------------


"
Richard Marscher <rmarscher@localytics.com>,"Tue, 27 Oct 2015 17:13:00 -0400",Re: Spark.Executor.Cores question,mkhaitman <mark.khaitman@chango.com>,"Ah I see, that's a bit more complicated =). If it's possible, would using
`spark.executor.memory` to set the available worker memory used by
executors help alleviate the problem of running on a node that already has
an executor on it? I would assume that would have a constant worst case
overhead per worker and shouldn't matter if two executors on the same node
are from one application or two applications. Maybe you have data that
states otherwise? I suppose it depends on what resources are causing
problems when the executors are imbalanced across the cluster. That could
be an indication of possibly not leaving enough free RAM outside the worker
and executors heap allocations on worker nodes.




-- 
*Richard Marscher*
Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 27 Oct 2015 14:50:57 -0700",Re: Exception when using some aggregate operators,Shagun Sodhani <sshagunsodhani@gmail.com>,"Have you tried using avg in place of mean ?

(1 to 5).foreach { i => val df = (1 to 1000).map(j => (j,
s""str$j"")).toDF(""a"", ""b"").save(s""/tmp/partitioned/i=$i"") }
    sqlContext.sql(""""""
    CREATE TEMPORARY TABLE partitionedParquet
    USING org.apache.spark.sql.parquet
    OPTIONS (
      path '/tmp/partitioned'
    )"""""")
sqlContext.sql(""""""select avg(a) from partitionedParquet"""""").show()

Cheers


"
Meihua Wu <rotationsymmetry14@gmail.com>,"Tue, 27 Oct 2015 17:04:47 -0700",Re: Spark Implementation of XGBoost,DB Tsai <dbtsai@dbtsai.com>,"Hi DB Tsai,

Thank you again for your insightful comments!

1) I agree the sorting method you suggested is a very efficient way to
handle the unordered categorical variables in binary classification
and regression. I propose we have a Spark ML Transformer to do the
sorting and encoding, bringing the benefits to many tree based
methods. How about I open a jira for this?

2) For L2/L1 regularization vs Learning rate (I use this name instead
shrinkage to avoid confusion), I have the following observations:

Suppose G and H are the sum (over the data assigned to a leaf node) of
the 1st and 2nd derivative of the loss evaluated at f_m, respectively.
Then for this leaf node,

* With a learning rate eta, f_{m+1} = f_m - G/H*eta

* With a L2 regularization coefficient lambda, f_{m+1} =f_m - G/(H+lambda)

If H>0 (convex loss), both approach lead to ""shrinkage"":

* For the learning rate approach, the percentage of shrinkage is
uniform for any leaf node.

* For L2 regularization, the percentage of shrinkage would adapt to
the number of instances assigned to a leaf node: more instances =>
larger G and H => less shrinkage. This behavior is intuitive to me. If
the value estimated from this node is based on a large amount of data,
the value should be reliable and less shrinkage is needed.

I suppose we could have something similar for L1.

I am not aware of theoretical results to conclude which method is
better. Likely to be dependent on the data at hand. Implementing
learning rate is on my radar for version 0.2. I should be able to add
it in a week or so. I will send you a note once it is done.

Thanks,

Meihua


---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 28 Oct 2015 11:11:29 +0900",Filter applied on merged Parquet shemsa with new column fails.,"user@spark.apache.org, dev@spark.apache.org","When enabling mergedSchema and predicate filter, this fails since Parquet
filters are pushed down regardless of each schema of the splits (or rather
files).

Dominic Ricard reported this issue (
https://issues.apache.org/jira/browse/SPARK-11103)

Even though this would work okay by setting spark.sql.parquet.filterPushdown
to false, the default value of this is true. So this looks an issue.

My questions are,
is this clearly an issue?
and if so, which way would this be handled?


I thought this is an issue and I made three rough patches for this and
tested them and this looks fine though.

The first approach looks simpler and appropriate as I presume from the
previous approaches such as
https://issues.apache.org/jira/browse/SPARK-11153

However, in terms of safety and performances, I also want to ensure which
one would be a proper approach before trying to open a PR.

1. Simply set false to spark.sql.parquet.filterPushdown when using
mergeSchema

2. If spark.sql.parquet.filterPushdown is true, retrieve all the schema of
every part-files (and also merged one) and check if each can accept the
given schema and then, apply the filter only when they all can accept,
which I think it's a bit over-implemented.

3. If spark.sql.parquet.filterPushdown is true, retrieve all the schema of
every part-files (and also merged one) and apply the filter to each split
(rather file) that can accept the filter which (I think it's hacky) ends up
different configurations for each task in a job.
"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 28 Oct 2015 09:03:51 +0530",Re: Exception when using some aggregate operators,Ted Yu <yuzhihong@gmail.com>,"Yup avg works good. So we have alternate functions to use in place on the
functions pointed out earlier. But my point is that are those original
aggregate functions not supposed to be used or I am using them in the wrong
way or is it a bug as I asked in my first mail.


"
Rohith Parameshwara <rparameshwara@couponsinc.com>,"Wed, 28 Oct 2015 03:41:23 +0000",Task not serializable exception,"""dev@spark.apache.org"" <dev@spark.apache.org>","I am getting this spark not serializable exception when running spark submit in standalone mode. I am trying to use spark streaming which gets its stream from kafka queues.. but it is not able to process the mapping actions on the RDDs from the stream ..the code where the serialization exception occurs as follows.
I have a separate class to manage the contexts... which has the respective getteres and setters:
public class Contexts {
public RedisContext rc=null;
public SparkContext sc=null;
public Gson serializer = new Gson();
public  SparkConf sparkConf = null;//new SparkConf().setAppName(""SparkStreamEventProcessingEngine"");
public  JavaStreamingContext jssc=null;//new JavaStreamingContext(sparkConf, new Duration(2000));
public  Producer<String, String> kafkaProducer=null;
public  Tuple2<String,Object> hostTup=null;



The class with the main process logic of spark streaming is as follows:
public final class SparkStreamEventProcessingEngine {
     public Contexts contexts= new Contexts();
     public SparkStreamEventProcessingEngine() {
       }

       public static void main(String[] args) {
             SparkStreamEventProcessingEngine temp=new SparkStreamEventProcessingEngine();
             temp.tempfunc();
       }
       private void tempfunc(){

           System.out.println(contexts.getJssc().toString() +""\n""+ contexts.getRc().toString()+""\n""+contexts.getSc().toString() +""\n"");
         createRewardProducer();
         Properties props = new Properties();
         try {
             props.load(SparkStreamEventProcessingEngine.class.getResourceAsStream(""/application.properties""));
         } catch (IOException e) {
             System.out.println(""Error loading application.properties file"");
             return ;
         }

         Map<String, Integer> topicMap = new HashMap<String, Integer>();
         topicMap.put(props.getProperty(""kafa.inbound.queue""),1);
         JavaPairReceiverInputDStream<String, String> messages =
                 KafkaUtils.createStream(contexts.getJssc(), props.getProperty(""kafka.zookeeper.quorum""), props.getProperty(""kafka.consumer.group""), topicMap);

      //The exception occurs at this line..
         JavaDStream<String> lines = messages.map(new Function<Tuple2<String, String>, String>() {
               //      private static final long serialVersionUID = 1L;

                     public String call(Tuple2<String, String> tuple2) {
                         return tuple2._2();
                     }
         });

         lines.foreachRDD(new Function<JavaRDD<String>,Void>() {
                public Void call(JavaRDD<String> rdd) throws Exception {
                 rdd.foreach(new VoidFunction<String>(){
                           public void call(String stringData) throws Exception {
                                Gson serializer = new Gson();
                         OfferRedeemed event = serializer.fromJson(stringData, OfferRedeemed.class);
                         System.out.println(""Incoming Event:"" + event.toString());
                         processTactic(event,""51367"");
                         processTactic(event,""53740"");
                     }
                 });
                 return null;
             }
         });

         contexts.getJssc().start();
         contexts.getJssc().awaitTermination();
       }

       private void processTactic(OfferRedeemed event, String tacticId){
             System.out.println(contexts.getRc().toString()+""hi4"");

           TacticDefinition tactic = readTacticDefinition(tacticId);
           boolean conditionMet = false;
           if(tactic != null){
               System.out.println(""Evaluating event of type :"" + event.getEventType() + "" for Tactic : "" + tactic.toString());.... And so on.. for respective functionalities...

The exception thrown is as follows:


Exception in thread ""main"" org.apache.spark.SparkException: Task not serializable
        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:315)
        at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)
        at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)
        at org.apache.spark.SparkContext.clean(SparkContext.scala:1893)
        at org.apache.spark.streaming.dstream.DStream$$anonfun$map$1.apply(DStream.scala:528)
        at org.apache.spark.streaming.dstream.DStream$$anonfun$map$1.apply(DStream.scala:528)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
        at org.apache.spark.SparkContext.withScope(SparkContext.scala:681)
        at org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:258)
        at org.apache.spark.streaming.dstream.DStream.map(DStream.scala:527)
        at org.apache.spark.streaming.api.java.JavaDStreamLike$class.map(JavaDStreamLike.scala:157)
        at org.apache.spark.streaming.api.java.AbstractJavaDStreamLike.map(JavaDStreamLike.scala:43)
        at com.coupons.stream.processing.SparkStreamEventProcessingEngine.tempfunc(SparkStreamEventProcessingEngine.java:366)
        at com.coupons.stream.processing.SparkStreamEventProcessingEngine.main(SparkStreamEventProcessingEngine.java:346)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.NotSerializableException: com.coupons.stream.processing.SparkStreamEventProcessingEngine
Serialization stack:
        - object not serializable (class: com.coupons.stream.processing.SparkStreamEventProcessingEngine, value: com.coupons.stream.processing.SparkStreamEventProcessingEngine@6a48a7f3)
        - field (class: com.coupons.stream.processing.SparkStreamEventProcessingEngine$1, name: this$0, type: class com.coupons.stream.processing.SparkStreamEventProcessingEngine)
        - object (class com.coupons.stream.processing.SparkStreamEventProcessingEngine$1, com.coupons.stream.processing.SparkStreamEventProcessingEngine$1@1c6c6f24)
        - field (class: org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, name: fun$1, type: interface org.apache.spark.api.java.function.Function)
        - object (class org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, <function1>)
        at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
        at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:81)
        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:312)
        ... 23 more




Any type of help on the topic is appreciated...


"
Rohith P <rparameshwara@couponsinc.com>,"Tue, 27 Oct 2015 21:07:25 -0700 (MST)",Re: using JavaRDD in spark-redis connector,dev@spark.apache.org,"got it ..thank u...




--

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 28 Oct 2015 15:52:27 +0800",Re: Filter applied on merged Parquet shemsa with new column fails.,"Hyukjin Kwon <gurwls223@gmail.com>, user@spark.apache.org,
 dev@spark.apache.org","Hey Hyukjin,

Sorry that I missed the JIRA ticket. Thanks for bring this issue up 
here, your detailed investigation.

 From my side, I think this is a bug of Parquet. Parquet was designed to 
support schema evolution. When scanning a Parquet, if a column exists in 
the requested schema but missing in the file schema, that column is 
filled with null. This should also hold for pushed-down predicate 
filters. For example, if filter ""a = 1"" is pushed down but column ""a"" 
doesn't exist in the Parquet file being scanned, it's safe to assume ""a"" 
NULL"" is pushed down, all records should be preserved.

Apparently, before this issue is properly fixed on Parquet side, we need 
to workaround this issue from Spark side. Please see my comments of all 
3 of your solutions inlined below. In short, I'd like to have approach 1 
for branch-1.5 and approach 2 for master.

Cheng

This one is pretty simple and safe, I'd like to have this for 1.5.2, or 
1.5.3 if we can't make it for 1.5.2.
Actually we only need to calculate the intersection of all file 
schemata. We can make ParquetRelation.mergeSchemaInParallel return two 
StructTypes, the first one is the original merged schema, the other is 
the intersection of all file schemata, which only contains fields that 
exist in all file schemata. Then we decide which filter to pushed down 
according to the second StructType.
The idea I came up with at first was similar to this one. Instead of 
pulling all file schemata to driver side, we can push filter push-down 
to executor side. Namely, passing candidate filters to executor side, 
and compute the Parquet predicate filter according to each file schema. 
I haven't looked into this direction in depth, but we can probably put 
this part into CatalystReadSupport, which is now initialized on executor 
side.

However, correctness of this approach can only guaranteed by the 
defensive filtering we do in Spark SQL (i.e. apply all the filters no 
matter they are pushed down or not), but we are considering to remove it 
because it imposes unnecessary performance cost. This makes me hesitant 
to go along this way.
"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Oct 2015 10:05:12 +0100",Re: Pickle Spark DataFrame,agg212 <agg@cs.brown.edu>,"What are you trying to accomplish to pickle a Spark DataFrame? If your
dataset is large, it doesn't make much sense to pickle it. If your dataset
is small, maybe it's best to just pickle a Pandas dataframe.



"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 28 Oct 2015 14:37:17 +0530",Re: Exception when using some aggregate operators,Ted Yu <yuzhihong@gmail.com>,"I tried adding the aggregate functions in the registry and they work, other
than mean, for which Ted has forwarded some code changes. I will try out
those changes and update the status here.


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 28 Oct 2015 03:36:11 -0700",Re: Exception when using some aggregate operators,Shagun Sodhani <sshagunsodhani@gmail.com>,"Since there is already Average, the simplest change is the following:

$ git diff
sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala
diff --git
a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala
b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Functi
index 3dce6c1..920f95b 100644
---
a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala
+++
b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala
@@ -184,6 +184,7 @@ object FunctionRegistry {
     expression[Last](""last""),
     expression[Last](""last_value""),
     expression[Max](""max""),
+    expression[Average](""mean""),
     expression[Min](""min""),
     expression[Stddev](""stddev""),
     expression[StddevPop](""stddev_pop""),

FYI


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 28 Oct 2015 16:08:22 +0530",Re: Exception when using some aggregate operators,Ted Yu <yuzhihong@gmail.com>,"Wouldnt it be:

+    expression[Max](""avg""),


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 28 Oct 2015 16:08:45 +0530",Re: Exception when using some aggregate operators,Ted Yu <yuzhihong@gmail.com>,"Also are the other aggregate functions to be treated as bugs or not?


"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Oct 2015 11:42:34 +0100",Re: Exception when using some aggregate operators,Shagun Sodhani <sshagunsodhani@gmail.com>,"I don't think these are bugs. The SQL standard for average is ""avg"", not
""mean"". Similarly, a distinct count is supposed to be written as
""count(distinct col)"", not ""countDistinct(col)"".

We can, however, make ""mean"" an alias for ""avg"" to improve compatibility
between DataFrame and SQL.



"
Ted Yu <yuzhihong@gmail.com>,"Wed, 28 Oct 2015 03:48:01 -0700",Re: Exception when using some aggregate operators,Reynold Xin <rxin@databricks.com>,"Created SPARK-11371 with a patch.

Will create PR soon.


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 28 Oct 2015 16:19:17 +0530",Re: Exception when using some aggregate operators,Reynold Xin <rxin@databricks.com>,"@Reynold I seem to be missing something. Aren't the functions listed here
<http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$>
to
be treated as sql operators as well? I do see that these are mentioned
as Functions
ava"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Oct 2015 11:51:27 +0100",Re: Exception when using some aggregate operators,Shagun Sodhani <sshagunsodhani@gmail.com>,"No those are just functions for the DataFrame programming API.


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Wed, 28 Oct 2015 16:24:17 +0530",Re: Exception when using some aggregate operators,Reynold Xin <rxin@databricks.com>,"Ohh great! Thanks for the clarification.


"
agg212 <agg@cs.brown.edu>,"Wed, 28 Oct 2015 08:27:21 -0700 (MST)",Re: Pickle Spark DataFrame,dev@spark.apache.org,"I would just like to be able to put a Spark DataFrame in a manager.dict() and
be able to get it out (manager.dict() calls pickle on the object being
stored).  Ideally, I would just like to store a pointer to the DataFrame
object so that it remains distributed within Spark (i.e., not materialize
and then store).  Here is an example:

data = sparkContext.jsonFile(data_file) #load file
cache = Manager.dict() #thread-safe container
cache['id'] = data #store reference to data, not materialized result
new_data = cache['id'] #get reference to distributed spark dataframe
new_data.show()




--

---------------------------------------------------------------------


"
mkhaitman <mark.khaitman@chango.com>,"Wed, 28 Oct 2015 08:39:31 -0700 (MST)",Re: Spark.Executor.Cores question,dev@spark.apache.org,"Unfortunately setting the executor memory to prevent multiple executors from
the same framework would inherently mean that we'd need to set just over
half the available worker memory for each node. So if each node had 32GB of
worker memory, then the application would need to set 17GB to absolutely
ensure that it never gets 2 executors on the same host (but that's way too
much memory to request per node in our case haha).

I'll look into adding this type of config option myself when I get some
time, since I think it would still be valuable to prevent a memory-intensive
application from taking down a bunch of spark workers just because their
executors are going bananas :P



--

---------------------------------------------------------------------


"
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Thu, 29 Oct 2015 13:15:21 +0800",sample or takeSample or ??,<dev@spark.apache.org>,"How do I to get a NEW RDD that has a number of elements that I specified?
Sample()? It has no the number parameter, takeSample() it returns as a list?

 

Help, please.

"
Aaska Shah <aaskashah1996@gmail.com>,"Thu, 29 Oct 2015 15:53:50 +0530",Guidance to get started,dev@spark.apache.org,"Hello,my name is Aaska Shah and I am a second year undergrad student at
DAIICT,Gandhinagar,India.

I have quite lately been interested in contributing towards the open source
organization and I find your organization the most appropriate one.

I request you to please guide me through how to install your codebase and
how to get started to your organization.

Thanking You,
Aaska Shah
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 29 Oct 2015 10:42:20 +0000","Fwd: [jira] [Created] (HADOOP-12527) Upgrade Avro dependency to
 1.7.7","""dev@spark.apache.org"" <dev@spark.apache.org>","Coming from AWS dev team.

he's probably got a point: even if you cut avro from the hadoop POM transitives, if you run in a Hadoop cluster with the hadoop classpaths getting in, there could be conflict.

1. has anyone seen this?
2. when spark when up to avro 1.7.7 —what broke? It's that risk of breaking downstream things that leads to hadoop JARs being frozen


Begin forwarded message:

From: ""Jonathan Kelly (JIRA)"" <jira@apache.org<mailto:jira@apache.org>>
Date: 29 October 2015 at 00:52:27 GMT
To: <common-dev@hadoop.apache.org<mailto:common-dev@hadoop.apache.org>>
Subject: [jira] [Created] (HADOOP-12527) Upgrade Avro dependency to 1.7.7

Jonathan Kelly created HADOOP-12527:
---------------------------------------

            Summary: Upgrade Avro dependency to 1.7.7
                Key: HADOOP-12527
                URL: https://issues.apache.org/jira/browse/HADOOP-12527
            Project: Hadoop Common
         Issue Type: Improvement
   Affects Versions: 2.7.1
           Reporter: Jonathan Kelly


Hadoop has depended upon Avro 1.7.4 for a couple of years now (see HADOOP-9672), but Apache Spark depends upon what is currently the latest version of Avro (1.7.7).

This can cause issues if Spark is configured to include the full Hadoop classpath, as the classpath would then contain both Avro 1.7.4 and 1.7.7, with the 1.7.4 classes possibly winning depending on ordering. Here is an example of this issue: http://stackoverflow.com/questions/33159254/avro-error-on-aws-emr/33403111#33403111

Would it be possible to upgrade Hadoop's Avro dependency to 1.7.7 now?



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)


"
Aadi Thakar <thakkar.aadi2@gmail.com>,"Thu, 29 Oct 2015 16:13:23 +0530",want to contribute,dev@spark.apache.org,"Hello, my name is Aaditya Thakkar and I am a second year undergraduate ICT
student at DA-IICT, Gandhinagar, India. I have quite lately been interested
in contributing towards the open source organization and I find your
organization the most appropriate one.

I request you to please guide me through how to install your code-base and
how to get started to your organization.
Thanking you,
Aaditya Thakkar.
"
Nitin Goyal <nitin2goyal@gmail.com>,"Thu, 29 Oct 2015 16:20:02 +0530",Re: want to contribute,"Aadi Thakar <thakkar.aadi2@gmail.com>, aaskashah1996@gmail.com, dev@spark.apache.org","You both can check out following links :-

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

http://spark.apache.org/docs/latest/building-spark.html

Thanks
-Nitin





-- 
Regards
Nitin Goyal
"
Adrian Tanase <atanase@adobe.com>,"Thu, 29 Oct 2015 11:04:04 +0000",Spark streaming - failed recovery from checkpoint,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi guys,

I’ve encountered some problems with a crashed Spark Streaming job, when restoring from checkpoint.
I’m runnning spark 1.5.1 on Yarn (hadoop 2.6) in cluster mode, reading from Kafka with the direct consumer and a few updateStateByKey stateful transformations.

After investigating, I think the following happened:

  *   Active ResourceManager crashed (aws machine crashed)
  *   10 minutes later — default Yarn settings :( — Standby took over and redeployed the job, sending a SIGTERM to the running driver
  *   Recovery from checkpoint failed because of missing RDD in checkpoint folder

One complication - UNCONFIRMED because of missing logs – I believe that the new driver was started ~5 minutes before the old one stopped.

With your help, I’m trying to zero in on a root cause or a combination of:

  *   bad Yarn/Spark configuration (10 minutes to react to missing node, already fixed through more aggressive liveliness settings)
  *   YARN fact of life – why is running job redeployed when standby RM takes over?
  *   Bug/race condition in spark checkpoint cleanup/recovery? (why is RDD cleaned up by the old app and then recovery fails when it looks for it?)
  *   Bugs in the Yarn-Spark integration (missing heartbeats? Why is the new app started 5 minutes before the old one dies?)
  *   Application code – should we add graceful shutdown? Should I add a Zookeeper lock that prevents 2 instances of the driver starting at the same time?

Sorry if the questions are a little all over the place, getting to the root cause of this was a pain and I can’t even log an issue in Jira without your help.

Attaching some logs that showcase the checkpoint recovery failure (I’ve grepped for “checkpoint” to highlight the core issue):

  *   Driver logs prior to shutdown: http://pastebin.com/eKqw27nT
  *   Driver logs, failed recovery: http://pastebin.com/pqACKK7W
  *
Other info:
     *   spark.streaming.unpersist = true
     *   spark.cleaner.ttl = 259200 (3 days)

Last question – in the checkpoint recovery process I notice that it’s going back ~6 minutes on the persisted RDDs and ~10 minutes to replay from kafka.
I’m running with 20 second batches and 100 seconds checkpoint interval (small issue - one of the RDDs was using the default interval of 20 secs). Shouldn’t the lineage be a lot smaller?
Based on the documentation I would have expected that the recovery goes back at most 100 seconds, as I’m not doing any windowed operations…

Thanks in advance!
-adrian
"
Saurabh Shah <shahsaurabh0103@gmail.com>,"Fri, 30 Oct 2015 16:55:16 +0530",Getting Started,dev@spark.apache.org,"Hello, my name is Saurabh Shah and I am a second year undergraduate 
student at DA-IICT, Gandhinagar, India. I have quite lately been 
contributing towards the open source organizations and I find your 
organization the most appropriate one to work on.

I request you to please guide me through the installation of your 
codebase and how to get started to your organization.


Thanking You,

Saurabh Shah.

"
Adrian Tanase <atanase@adobe.com>,"Thu, 29 Oct 2015 11:38:59 +0000",Spark streaming - failed recovery from checkpoint,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi guys,

 (apologize for the huge font, reposting),

I’ve encountered some problems with a crashed Spark Streaming job, when restoring from checkpoint.
I’m runnning spark 1.5.1 on Yarn (hadoop 2.6) in cluster mode, reading from Kafka with the direct consumer and a few updateStateByKey stateful transformations.

After investigating, I think the following happened:

  *   Active ResourceManager crashed (aws machine crashed)
  *   10 minutes later — default Yarn settings :( — Standby took over and redeployed the job, sending a SIGTERM to the running driver
  *   Recovery from checkpoint failed because of missing RDD in checkpoint folder

One complication - UNCONFIRMED because of missing logs – I believe that the new driver was started ~5 minutes before the old one stopped.

With your help, I’m trying to zero in on a root cause or a combination of:

  *   bad Yarn/Spark configuration (10 minutes to react to missing node, already fixed through more aggressive liveliness settings)
  *   YARN fact of life – why is running job redeployed when standby RM takes over?
  *   Bug/race condition in spark checkpoint cleanup/recovery? (why is RDD cleaned up by the old app and then recovery fails when it looks for it?)
  *   Bugs in the Yarn-Spark integration (missing heartbeats? Why is the new app started 5 minutes before the old one dies?)
  *   Application code – should we add graceful shutdown? Should I add a Zookeeper lock that prevents 2 instances of the driver starting at the same time?

Sorry if the questions are a little all over the place, getting to the root cause of this was a pain and I can’t even log an issue in Jira without your help.

Attaching some logs that showcase the checkpoint recovery failure (I’ve grepped for “checkpoint” to highlight the core issue):

  *   Driver logs prior to shutdown: http://pastebin.com/eKqw27nT
  *   Driver logs, failed recovery: http://pastebin.com/pqACKK7W
  *
Other info:
     *   spark.streaming.unpersist = true
     *   spark.cleaner.ttl = 259200 (3 days)

Last question – in the checkpoint recovery process I notice that it’s going back ~6 minutes on the persisted RDDs and ~10 minutes to replay from kafka.
I’m running with 20 second batches and 100 seconds checkpoint interval (small issue - one of the RDDs was using the default interval of 20 secs). Shouldn’t the lineage be a lot smaller?
Based on the documentation I would have expected that the recovery goes back at most 100 seconds, as I’m not doing any windowed operations…

Thanks in advance!
-adrian
"
Stavros Kontopoulos <stavros.kontopoulos@typesafe.com>,"Thu, 29 Oct 2015 13:26:19 +0100",[VOTE] Release Apache Spark 1.5.2 (RC1),dev@spark.apache.org,"+1  (non binding)

I tested several of the examples on mesos latest version (fine and
coarse-grained modes) and they work fine. Hope not too late...though..

-- 

Stavros Kontopoulos

<http://www.typesafe.com>


<http://www.typesafe.com>
"
Sandeep Giri <sandeep@knowbigdata.com>,"Fri, 30 Oct 2015 03:38:42 +0530",Maintaining overall cumulative data in Spark Streaming,"user <user@spark.apache.org>, dev <dev@spark.apache.org>","Dear All,

If a continuous stream of text is coming in and you have to keep publishing
the overall word count so far since 0:00 today, what would you do?

Publishing the results for a window is easy but if we have to keep
aggregating the results, how to go about it?

I have tried to keep an StreamRDD with aggregated count and keep doing a
fullouterjoin but didn't work. Seems like the StreamRDD gets reset.

Kindly help.

Regards,
Sandeep Giri
"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Thu, 29 Oct 2015 23:20:07 +0100",Re: Maintaining overall cumulative data in Spark Streaming,,"-dev +user

Hi Sandeep,

Perhaps (flat)mapping values and using an accumulator?


ó:
g the overall word count so far since 0:00 today, what would you do?
ting the results, how to go about it?
ullouterjoin but didn't work. Seems like the StreamRDD gets re"
"""skaarthik oss"" <skaarthik.oss@gmail.com>","Thu, 29 Oct 2015 19:57:20 -0700",RE: Maintaining overall cumulative data in Spark Streaming,"""'Sandeep Giri'"" <sandeep@knowbigdata.com>,
	""'user'"" <user@spark.apache.org>,
	""'dev'"" <dev@spark.apache.org>","Did you consider UpdateStateByKey operation?

 

From: Sandeep Giri [mailto:sandeep@knowbigdata.com] 
Sent: Thursday, October 29, 2015 3:09 PM
To: user <user@spark.apache.org>; dev <dev@spark.apache.org>
Subject: Maintaining overall cumulative data in Spark Streaming

 

Dear All,

 

If a continuous stream of text is coming in and you have to keep publishing the overall word count so far since 0:00 today, what would you do?

 

Publishing the results for a window is easy but if we have to keep aggregating the results, how to go about it?

 

I have tried to keep an StreamRDD with aggregated count and keep doing a fullouterjoin but didn't work. Seems like the StreamRDD gets reset.

 

Kindly help.

 

Regards,

Sandeep Giri

 

"
Sandeep Giri <sandeep@knowbigdata.com>,"Fri, 30 Oct 2015 09:49:03 +0530",RE: Maintaining overall cumulative data in Spark Streaming,skaarthik oss <skaarthik.oss@gmail.com>,"Yes, update state by key worked.

Though there are some more complications.

"
Sandeep Giri <sandeep@knowbigdata.com>,"Fri, 30 Oct 2015 18:59:12 +0530",Re: Maintaining overall cumulative data in Spark Streaming,skaarthik oss <skaarthik.oss@gmail.com>,"How to we reset the aggregated statistics to null?

Regards,
Sandeep Giri,
+1 347 781 4573 (US)
+91-953-899-8962 (IN)

www.KnowBigData.com. <http://KnowBigData.com.>
Phone: +1-253-397-1945 (Office)

[image: linkedin icon] <https://linkedin.com/company/knowbigdata> [image:
other site icon] <http://knowbigdata.com>  [image: facebook icon]
<https://facebook.com/knowbigdata> [image: twitter icon]
<https://twitter.com/IKnowBigData> <https://twitter.com/IKnowBigData>



"
Luke Han <luke.hq@gmail.com>,"Fri, 30 Oct 2015 23:34:42 +0800",Re: Re: repartitionAndSortWithinPartitions task shuffle phase is very slow,"""dev@kylin.incubator.apache.org"" <dev@kylin.incubator.apache.org>","Would love to have any suggestion or comments about our implementation.

Is there anyone who has such experience?

Thanks.


Best Regards!
---------------------

Luke Han


月23日周五 下午5:50写道：
k
日周五 下午5:21写道：
?
?
MR
10月23日周五 上午10:20写道：
n
s
23日周五 上午2:43写道：
o
MR
e
on
 Spark
"
Justin Uang <justin.uang@gmail.com>,"Fri, 30 Oct 2015 16:13:44 +0000",Off-heap storage and dynamic allocation,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey guys,

According to the docs for 1.5.1, when an executor is removed for dynamic
allocation, the cached data is gone. If I use off-heap storage like
tachyon, conceptually there isn't this issue anymore, but is the cached
data still available in practice? This would be great because then we would
be able to set spark.dynamicAllocation.cachedExecutorIdleTimeout to be
quite small.

==================
In addition to writing shuffle files, executors also cache data either on
disk or in memory. When an executor is removed, however, all cached data
will no longer be accessible. There is currently not yet a solution for
this in Spark 1.2. In future releases, the cached data may be preserved
through an off-heap storage similar in spirit to how shuffle files are
preserved through the external shuffle service.
==================
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 30 Oct 2015 09:47:39 -0700",Re: test failed due to OOME,"""dev@spark.apache.org"" <dev@spark.apache.org>","This happened recently on Jenkins:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.3,label=spark-test/3964/console


ADOOP_PROFILE=hadoop-2.4,label=spark-test/3846/console
stop() waits for the event queue to completely drain- basic creation of StageInfo- basic creation of StageInfo with shuffle- StageInfo with fewer tasks than partitions- local metrics- onTaskGettingResult() called when result fetched remotely *** FAILED ***  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space	at java.util.Arrays.copyOf(Arrays.java:2271)	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1852)	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:182)	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:52)	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:49)	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1458)	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429)	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:256)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745)
"
shane knapp <sknapp@berkeley.edu>,"Fri, 30 Oct 2015 11:28:31 -0700",Re: test failed due to OOME,Ted Yu <yuzhihong@gmail.com>,"here's the current heap settings on our workers:
InitialHeapSize == 2.1G
MaxHeapSize == 32G

system ram:  128G

we can bump it pretty easily...  it's just a matter of deciding if we
want to do this globally (super easy, but will affect ALL maven builds
on our system -- not just spark) or on a per-job basis (this doesn't
scale that well).

thoughts?


---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 30 Oct 2015 12:04:33 -0700",Re: test failed due to OOME,shane knapp <sknapp@berkeley.edu>,"It is giving OOM at 32GB ? Something looks wrong with that ... that is
already on the higher side.

Regards,
Mridul


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 30 Oct 2015 13:19:39 -0700",Re: test failed due to OOME,Mridul Muralidharan <mridul@gmail.com>,"I noticed that the SparkContext created in each sub-test is not stopped
upon finishing sub-test.

Would stopping each SparkContext make a difference in terms of heap memory
consumption ?

Cheers


"
Michael Armbrust <michael@databricks.com>,"Sat, 31 Oct 2015 12:25:50 +0100",Spark 1.6 Release Schedule,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just a friendly reminder that today (October 31st) is the scheduled code
freeze for Spark 1.6.  Since a lot of developers were busy with the Spark
Summit last week I'm going to delay cutting the branch until Monday,
November 2nd.  After that point, we'll package a release for testing and
then go into the normal triage process where bugs are prioritized and some
smaller features are allowed in on a case by case basis (if they are very
low risk/additive/feature flagged/etc).

As a reminder, release window dates are always maintained on the wiki and
are updated after each release according to our 3 month release cadence:

https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage

Thanks!

Michael
"
gus <gustavo.arocena@gmail.com>,"Sat, 31 Oct 2015 05:20:31 -0700 (MST)",SparkLauncher#setJavaHome does not set JAVA_HOME in child process,dev@spark.apache.org,"Hi
the SparkLauncher#setJavaHome method uses its argument to find the java
executable, but it does not reset the JAVA_HOME env var in the child
process. As a result, spark scripts such as bin/spark-class that rely on
JAVA_HOME use the wrong value. Adding this to setJavaHome fixes the problem:

    builder.childEnv.put(""JAVA_HOME"", javaHome);

Should I submit a PR for this or if someone is already working on this code
maybe they can fix it?




--

---------------------------------------------------------------------


"
,"Sat, 31 Oct 2015 13:46:04 +0100",Re: Spark 1.6 Release Schedule,dev@spark.apache.org,"Hi Michael,

thanks for the reminder and update. I have some PR in progress, but no 
rush to be included in 1.6.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 31 Oct 2015 07:49:25 -0700",Re: SparkLauncher#setJavaHome does not set JAVA_HOME in child process,gus <gustavo.arocena@gmail.com>,"change):

testChildProcLauncher(org.apache.spark.launcher.SparkLauncherSuite)  Time
elapsed: 0.036 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<1>
at org.junit.Assert.fail(Assert.java:88)
at org.junit.Assert.failNotEquals(Assert.java:743)
at org.junit.Assert.assertEquals(Assert.java:118)
at org.junit.Assert.assertEquals(Assert.java:555)
at org.junit.Assert.assertEquals(Assert.java:542)
at
org.apache.spark.launcher.SparkLauncherSuite.testChildProcLauncher(SparkLauncherSuite.java:113)

Has anyone seen the above before ?

Cheers


"
