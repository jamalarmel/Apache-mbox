Aaron Davidson <ilikerps@gmail.com>,"Mon, 30 Jun 2014 19:58:30 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"I don't know of any way to avoid Akka doing a copy, but I would like to
mention that it's on the priority list to piggy-back only the map statuses
relevant to a particular map task on the task itself, thus reducing the
total amount of data sent over the wire by a factor of N for N physical
machines in your cluster. Ideally we would also avoid Akka entirely when
sending the tasks, as these can get somewhat large and Akka doesn't work
well with large messages.

Do note that your solution of using broadcast to send the map tasks is very
similar to how the executor returns the result of a task when it's too big
for akka. We were thinking of refactoring this too, as using the block
manager has much higher latency than a direct TCP send.



"
Bert Greevenbosch <Bert.Greevenbosch@huawei.com>,"Tue, 1 Jul 2014 03:13:42 +0000",RE: Artificial Neural Network in Spark?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Debasish, Alexander, all,

Indeed I found the OpenDL project through the Powered by Spark page. I'll need some time to look into the code, but on the first sight it looks quite well-developed. I'll contact the author about this too.

My own implementation (in Scala) works for multiple inputs and multiple outputs. It implements a single hidden layer, the number of nodes in it can be specified.

The implementation is a general ANN implementation. As such, it should be useable for an autoencoder too, since that is just an ANN with some special input/output constraints.

As said before, the implementation is built upon the linear regression model and gradient descent implementation. However it did require some tweaks:

- The linear regression model only supports a single output ""label"" (as Double). Since the ANN can have multiple outputs, it ignores the ""label"" attribute, but for training divides the input vector into two parts, the first part being the genuine input vector, the second the target output vector.

- The concatenation of input and target output vectors is only internally, the training function takes as input an RDD with tuples of two Vectors, one for each input and output.

- The GradientDescend optimizer is re-used without modification.

- I have made an even simpler updater than the SimpleUpdater, leaving out the division by the square root of the number of iterations. The SimpleUpdater can also be used, but I created this simpler one because I like to plot the result every now and then, and then continue the calculations. For this, I also wrote a training function with as input the weights from the previous training session.

- I created a ParallelANNModel similar to the LinearRegressionModel.

- I created a new GeneralizedSteepestDescendAlgorithm class similar to the GeneralizedLinearAlgorithm class.

- Created some example code to test with 2D (1 input 1 output), 3D (2 inputs 1 output) and 4D (1 input 3 outputs) functions.

If there is interest, I would be happy to release the code. What would be the best way to do this? Is there some kind of review process?

Best regards,
Bert


> -----Original Message-----
> From: Debasish Das [mailto:debasish.das83@gmail.com]
> Sent: 27 June 2014 14:02
> To: dev@spark.apache.org
> Subject: Re: Artificial Neural Network in Spark?
> 
> Look into Powered by Spark page...I found a project there which used
> autoencoder functions...It's not updated for a long time now !
> 
> On Thu, Jun 26, 2014 at 10:51 PM, Ulanov, Alexander
> <alexander.ulanov@hp.com
> > wrote:
> 
> > Hi Bert,
> >
> > It would be extremely interesting. Do you plan to implement
> autoencoder as
> > well? It would be great to have deep learning in Spark.
> >
> > Best regards, Alexander
> >
> > 27.06.2014, в 4:47, ""Bert Greevenbosch"" <Bert.Greevenbosch@huawei.com>
> > написал(а):
> >
> > > Hello all,
> > >
> > > I was wondering whether Spark/mllib supports Artificial Neural
> Networks
> > (ANNs)?
> > >
> > > If not, I am currently working on an implementation of it. I re-use
> the
> > code for linear regression and gradient descent as much as possible.
> > >
> > > Would the community be interested in such implementation? Or maybe
> > somebody is already working on it?
> > >
> > > Best regards,
> > > Bert
> >
"
Gang Bai <baigang@staff.sina.com.cn>,"Tue, 1 Jul 2014 11:17:42 +0800",Re: Contributing to MLlib on GLM,<dev@spark.apache.org>,"Thanks Xiaokai,

I’ve created a pull request to merge features in my PR to your repo. Please take a review here https://github.com/xwei-datageek/spark/pull/2 .

As for GLMs, here at Sina, we are solving the problem of predicting the num of visitors who read a particular news article or watch an online sports live stream in a particular period. I’m trying to improve the prediction results by tuning features and incorporating new models. So I’ll try Gamma regression later. Thanks for the implementation.

Cheers,
-Gang


comprehensive. I'd like to merge my branch with yours.
different GLMs, usually they only differ in gradient calculation but the ****regression.scala files are quite similar. For example, linearRegressionSGD, logisticRegressionSGD, RidgeRegressionSGD, poissonRegressionSGD all share quite a bit of common code in their class implementations. Since such redundancy is already there in the legacy code, simply merging Poisson and Gamma does not seem to help much. So I suggest we just leave them as separate classes for the time being. 
regression. The mails were buried in junk by the corp mail master. Also, thanks for considering my comments and advice in your PR. 
fields and prediction method. Shall we use one instead of two redundant classes? Say, a LogLinearModel. 
convergence than SGD. I implemented two GeneralizedLinearAlgorithm classes using LBFGS and SGD respectively. You may take a look into it. If it's OK to you, I'd be happy to send a PR to your branch. 
data for testing. In my implementation, I added the test data from https://onlinecourses.science.psu.edu/stat504/node/223. Please check my test suite. 
Gradient.scala and 
We made 
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7088.html
Nabble.com. 
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7107.html
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7117.html
Nabble.com.


"
Debasish Das <debasish.das83@gmail.com>,"Mon, 30 Jun 2014 21:20:42 -0700",Re: Artificial Neural Network in Spark?,dev@spark.apache.org,"I will let Xiangrui to comment on the PR process to add the code in mllib
but I would love to look into your initial version if you push it to
github...

As far as I remember Quoc got his best ANN results using back-propagation
algorithm and solved using CG...do you have those features or you are using
SGD style update....




te
an
al
,
ne
e
e
i.com>
"
Taka Shinagawa <taka.epsilon@gmail.com>,"Tue, 1 Jul 2014 01:00:10 -0700",Errors from Sbt Test,dev@spark.apache.org,"Since Spark 1.0.0, I've been seeing multiple errors when running sbt test.

I ran the following commands from Spark 1.0.1 RC1 on Mac OSX 10.9.2.

$ sbt/sbt clean
$ SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly
$ sbt/sbt test


I'm attaching the log file generated by the sbt test.

Here's the summary part of the test.

[info] Run completed in 30 minutes, 57 seconds.
[info] Total number of tests run: 605
[info] Suites: completed 83, aborted 0
[info] Tests: succeeded 600, failed 5, canceled 0, ignored 5, pending 0
[info] *** 5 TESTS FAILED ***
[error] Failed: Total 653, Failed 5, Errors 0, Passed 648, Ignored 5
[error] Failed tests:
[error] org.apache.spark.ShuffleNettySuite
[error] org.apache.spark.ShuffleSuite
[error] org.apache.spark.FileServerSuite
[error] org.apache.spark.DistributedSuite
[error] (core/test:test) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 2033 s, completed Jul 1, 2014 12:08:03 AM

Is anyone else seeing errors like this?


Thanks,
Taka
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 1 Jul 2014 01:04:22 -0700",Re: Errors from Sbt Test,"""dev@spark.apache.org"" <dev@spark.apache.org>","Do those also happen if you run other hadoop versions (e.g. try 1.0.4)?


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 1 Jul 2014 01:06:27 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah I created a JIRA a while back to piggy-back the map status info
on top of the task (I honestly think it will be a small change). There
isn't a good reason to broadcast the entire array and it can be an
issue during large shuffles.

- Patrick


"
Taka Shinagawa <taka.epsilon@gmail.com>,"Tue, 1 Jul 2014 01:10:54 -0700",Re: Errors from Sbt Test,dev@spark.apache.org,"If I remember correctly, similar/same errors happened with other hadoop
versions. I need to rebuild it with those and compare the logs.



"
qingyang li <liqingyang1985@gmail.com>,"Tue, 1 Jul 2014 16:25:05 +0800",task always lost,dev@spark.apache.org,"i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started, when I
using spark-shell to submit one job, the tasks always lost.  here is the
log:
----------
14/07/01 16:24:27 INFO DAGScheduler: Host gained which was in lost list
earlier: bigdata005
14/07/01 16:24:27 INFO TaskSetManager: Starting task 0.0:1 as TID 4042 on
executor 20140616-143932-1694607552-5050-4080-2: bigdata005 (PROCESS_LOCAL)
14/07/01 16:24:27 INFO TaskSetManager: Serialized task 0.0:1 as 1570 bytes
in 0 ms
14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
20140616-104524-1694607552-5050-26919-1 from TaskSet 0.0
14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4041 (task 0.0:0)
14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
20140616-104524-1694607552-5050-26919-1 (epoch 3427)
14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
14/07/01 16:24:28 INFO BlockManagerMaster: Removed
20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
20140616-143932-1694607552-5050-4080-2 from TaskSet 0.0
14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4042 (task 0.0:1)
14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
20140616-143932-1694607552-5050-4080-2 (epoch 3428)
14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
20140616-143932-1694607552-5050-4080-2 from BlockManagerMaster.
14/07/01 16:24:28 INFO BlockManagerMaster: Removed
20140616-143932-1694607552-5050-4080-2 successfully in removeExecutor
14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
earlier: bigdata005
14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
earlier: bigdata001
14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:1 as TID 4043 on
executor 20140616-143932-1694607552-5050-4080-2: bigdata005 (PROCESS_LOCAL)
14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:1 as 1570 bytes
in 0 ms
14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:0 as TID 4044 on
executor 20140616-104524-1694607552-5050-26919-1: bigdata001 (PROCESS_LOCAL)
14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:0 as 1570 bytes
in 0 ms


it seems other guy has also encountered such problem,
http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201305.mbox/%3C201305161047069952830@nfs.iscas.ac.cn%3E
"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 1 Jul 2014 15:21:24 +0530",Re: Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"We had considered both approaches (if I understood the suggestions right) :
a) Pulling only map output states for tasks which run on the reducer
by modifying the Actor. (Probably along lines of what Aaron described
?)
The performance implication of this was bad :
1) We cant cache serialized result anymore, (caching it makes no sense rather).
2) The number requests to master will go from num_executors to
num_reducers - the latter can be orders of magnitude higher than
former.

b) Instead of pulling this information, push it to executors as part
of task submission. (What Patrick mentioned ?)
(1) a.1 from above is still an issue for this.
(2) Serialized task size is also a concern : we have already seen
users hitting akka limits for task size - this will be an additional
vector which might exacerbate it.
Our jobs are not hitting this yet though !

I was hoping there might be something in akka itself to alleviate this
- but if not, we can solve it within context of spark.

Currently, we have worked around it by using broadcast variable when
serialized size is above some threshold - so that our immediate
concerns are unblocked :-)
But a better solution should be greatly welcomed !
Maybe we can unify it with large serialized task as well ...


Btw, I am not sure what the higher cost of BlockManager referred to is
Aaron - do you mean the cost of persisting the serialized map outputs
to disk ?




Regards,
Mridul



"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 1 Jul 2014 10:16:54 +0000",RE: Artificial Neural Network in Spark?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Bert,

There is a specific process of pull request if you wish to share the code https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

I would be glad to benchmark your ANN implementation by means of running some experiments that we run with the other ANN toolkits. I am also interested in Autoencoder and have plans to implement it for MLLib in the near future. 

Best regards, Alexander

-----Original Message-----
From: Bert Greevenbosch [mailto:Bert.Greevenbosch@huawei.com] 
Sent: Tuesday, July 01, 2014 7:14 AM
To: dev@spark.apache.org
Subject: RE: Artificial Neural Network in Spark?

Hi Debasish, Alexander, all,

Indeed I found the OpenDL project through the Powered by Spark page. I'll need some time to look into the code, but on the first sight it looks quite well-developed. I'll contact the author about this too.

My own implementation (in Scala) works for multiple inputs and multiple outputs. It implements a single hidden layer, the number of nodes in it can be specified.

The implementation is a general ANN implementation. As such, it should be useable for an autoencoder too, since that is just an ANN with some special input/output constraints.

As said before, the implementation is built upon the linear regression model and gradient descent implementation. However it did require some tweaks:

- The linear regression model only supports a single output ""label"" (as Double). Since the ANN can have multiple outputs, it ignores the ""label"" attribute, but for training divides the input vector into two parts, the first part being the genuine input vector, the second the target output vector.

- The concatenation of input and target output vectors is only internally, the training function takes as input an RDD with tuples of two Vectors, one for each input and output.

- The GradientDescend optimizer is re-used without modification.

- I have made an even simpler updater than the SimpleUpdater, leaving out the division by the square root of the number of iterations. The SimpleUpdater can also be used, but I created this simpler one because I like to plot the result every now and then, and then continue the calculations. For this, I also wrote a training function with as input the weights from the previous training session.

- I created a ParallelANNModel similar to the LinearRegressionModel.

- I created a new GeneralizedSteepestDescendAlgorithm class similar to the GeneralizedLinearAlgorithm class.

- Created some example code to test with 2D (1 input 1 output), 3D (2 inputs 1 output) and 4D (1 input 3 outputs) functions.

If there is interest, I would be happy to release the code. What would be the best way to do this? Is there some kind of review process?

Best regards,
Bert


> -----Original Message-----
> From: Debasish Das [mailto:debasish.das83@gmail.com]
> Sent: 27 June 2014 14:02
> To: dev@spark.apache.org
> Subject: Re: Artificial Neural Network in Spark?
> 
> Look into Powered by Spark page...I found a project there which used 
> autoencoder functions...It's not updated for a long time now !
> 
> On Thu, Jun 26, 2014 at 10:51 PM, Ulanov, Alexander 
> <alexander.ulanov@hp.com
> > wrote:
> 
> > Hi Bert,
> >
> > It would be extremely interesting. Do you plan to implement
> autoencoder as
> > well? It would be great to have deep learning in Spark.
> >
> > Best regards, Alexander
> >
> > 27.06.2014, в 4:47, ""Bert Greevenbosch"" 
> > <Bert.Greevenbosch@huawei.com>
> > написал(а):
> >
> > > Hello all,
> > >
> > > I was wondering whether Spark/mllib supports Artificial Neural
> Networks
> > (ANNs)?
> > >
> > > If not, I am currently working on an implementation of it. I 
> > > re-use
> the
> > code for linear regression and gradient descent as much as possible.
> > >
> > > Would the community be interested in such implementation? Or maybe
> > somebody is already working on it?
> > >
> > > Best regards,
> > > Bert
> >
"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 1 Jul 2014 10:45:43 -0700",Re: task always lost,dev@spark.apache.org,"Can you post the logs from any of the dying executors?



"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Tue, 1 Jul 2014 14:31:38 -0400",PySpark Driver from Jython,"""user@spark.incubator.apache.org"" <user@spark.incubator.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Has anyone tried running pyspark driver code in Jython, preferably by
calling python code within Java code?

I know CPython is the only interpreter tested because of the need to
support C extensions.

But in my case, C extensions would be called on the worker, not in the
driver.

And being able to execute the python driver from within my JVM is an
advantage in my current use case.

-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
qingyang li <liqingyang1985@gmail.com>,"Wed, 2 Jul 2014 11:46:33 +0800",Re: task always lost,dev@spark.apache.org,"Here is the log:

E0702 10:32:07.599364 14915 slave.cpp:2686] Failed to unmonitor container
for executor 20140616-104524-1694607552-5050-26919-1 of framework
20140702-102939-1694607552-5050-14846-0000: Not monitored


2014-07-02 1:45 GMT+08:00 Aaron Davidson <ilikerps@gmail.com>:

"
qingyang li <liqingyang1985@gmail.com>,"Wed, 2 Jul 2014 12:01:15 +0800",Re: task always lost,dev@spark.apache.org,"also this one in warning log:

E0702 11:35:08.869998 17840 slave.cpp:2310] Container
'af557235-2d5f-4062-aaf3-a747cb3cd0d1' for executor
'20140616-104524-1694607552-5050-26919-1' of framework
'20140702-113428-1694607552-5050-17766-0000' failed to start: Failed to
fetch URIs for container 'af557235-2d5f-4062-aaf3-a747cb3cd0d1': exit
status 32512


2014-07-02 11:46 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 1 Jul 2014 22:22:12 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","
I don't understand problem a.1 is. In this case, we don't need to do
caching, right?


This would add only a small, constant amount of data to the task. It's
strictly better than before. Before if the map output status array was
size M x R, we send a single akka message to every node of size M x
R... this basically scales quadratically with the size of the RDD. The
new approach is constant... it's much better. And the total amount of
data send over the wire is likely much less.

- Patrick

"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Jul 2014 22:27:46 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I was actually talking to tgraves today at the summit about this.

Based on my understanding, the sizes we track and send (which is
unfortunately O(M*R) regardless of how we change the implementation --
whether we send via task or send via MapOutputTracker) is only used to
compute maxBytesInFlight so we can throttle the fetching speed to not
result in oom. Perhaps for very large shuffles, we don't need to send the
bytes for each block, and we can send whether they are zero or not (which
can be tracked via a compressed bitmap that can be tiny).

The other thing we do need is the location of blocks. This is actually just
O(n) because we just need to know where the map was run.



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 1 Jul 2014 23:05:16 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hi Xiangrui,

Could you please point to the IPM solver that you have positive results
with ? I was planning to compare with CVX, KNITRO from Professor Nocedal
and MOSEK probably...I don't have CPLEX license so I won't be able to do
that comparison...

My experiments so far tells me that ADMM based solver is faster than IPM
for simpler constraints but then perhaps I did not choose the correct
IPM....

Proximal algorithm paper also shows very similar results compared to CVX:

http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf

Thanks.
Deb


"
Taka Shinagawa <taka.epsilon@gmail.com>,"Wed, 2 Jul 2014 00:09:30 -0700",Re: Errors from Sbt Test,dev@spark.apache.org,"With Hadoop 1.0.4, the sbt test completed with fewer errors than with
Hadoop 1.2.1. I'll run the test for other Hadoop versions and report back
later.

--------------------------------
sbt test errors with Hadoop 1.0.4

[info] FlumeStreamSuite:

2014-07-01 23:18:55.057 java[90699:5903] Unable to load realm info from
SCDynamicStore

[info] - flume input stream *** FAILED ***

[info]   java.io.IOException: Error connecting to localhost/127.0.0.1:9999

[info]   at
org.apache.avro.ipc.NettyTransceiver.getChannel(NettyTransceiver.java:261)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:203)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:152)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:120)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:107)

[info]   at
org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$1.apply$mcV$sp(FlumeStreamSuite.scala:54)

[info]   at
org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$1.apply(FlumeStreamSuite.scala:40)

[info]   at
org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$1.apply(FlumeStreamSuite.scala:40)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   ...

[info]   Cause: java.net.ConnectException: Connection refused: localhost/
127.0.0.1:9999

[info]   at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)

[info]   at
sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)

[info]   at
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)

[info]   at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

[info]   at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

[info]   at java.lang.Thread.run(Thread.java:744)

[info]   ...

[info] - cached post-shuffle

[ERROR] [07/01/2014 23:24:58.083] [test-akka.actor.default-dispatcher-3]
[akka://test/user/dagSupervisor/$a] error

org.apache.spark.SparkException: error

at
org.apache.spark.scheduler.BuggyDAGEventProcessActor$$anonfun$receive$1.applyOrElse(DAGSchedulerSuite.scala:36)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


[info] - DAGSchedulerActorSupervisor closes the SparkContext when
EventProcessActor crashes

[ERROR] [07/01/2014 23:24:58.115]
[DAGSchedulerSuite-akka.actor.default-dispatcher-6]
[akka://DAGSchedulerSuite/user/$$a] Job cancelled because SparkContext was
shut down

org.apache.spark.SparkException: Job cancelled because SparkContext was
shut down

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:639)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:638)

at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)

at
org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:638)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor.postStop(DAGScheduler.scala:1227)

at
akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:201)

at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:163)

at akka.actor.ActorCell.terminate(ActorCell.scala:338)

at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431)

at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)

at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)

at
akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:244)

at
akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:284)

at
akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:192)

at akka.actor.dungeon.Dispatch$class.stop(Dispatch.scala:106)

at akka.actor.ActorCell.stop(ActorCell.scala:338)

at akka.actor.LocalActorRef.stop(ActorRef.scala:340)

at akka.actor.dungeon.Children$class.stop(Children.scala:66)

at akka.actor.ActorCell.stop(ActorCell.scala:338)

at
akka.actor.dungeon.FaultHandling$$anonfun$terminate$1.apply(FaultHandling.scala:149)

at
akka.actor.dungeon.FaultHandling$$anonfun$terminate$1.apply(FaultHandling.scala:149)

at scala.collection.Iterator$class.foreach(Iterator.scala:727)

at
akka.util.Collections$PartialImmutableValuesIterable$$anon$1.foreach(Collections.scala:27)

at
akka.util.Collections$PartialImmutableValuesIterable.foreach(Collections.scala:52)

at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:149)

at akka.actor.ActorCell.terminate(ActorCell.scala:338)

at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431)

at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)

at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)

at akka.dispatch.Mailbox.run(Mailbox.scala:218)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)











"
Eustache DIEMERT <eustache@diemert.fr>,"Wed, 2 Jul 2014 10:01:38 +0200",process for contributing to mllib,dev@spark.apache.org,"Hi there,

I just created an issue [1] for MLlib on Jira. I also want to contribute a
fix, is it a good idea to submit a PR on github [2] ?

Should I also mention the issue on this list ?

Thanks

Eustache

[1] https://issues.apache.org/jira/browse/SPARK-2341
[2] https://github.com/apache/spark/pulls
"
Reynold Xin <rxin@databricks.com>,"Wed, 2 Jul 2014 01:10:12 -0700",Re: process for contributing to mllib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yes it would be great to mention the JIRA ticket number on the pull
request. Thanks!




"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 2 Jul 2014 01:52:49 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hi Deb,

KNITRO and MOSEK are both commercial. If you are looking for
open-source ones, you can take a look at PDCO from SOL:

http://web.stanford.edu/group/SOL/software/pdco/

Each subproblem is really just a small QP. ADMM is designed for the
cases when data is distributively stored or the objective function is
complex but splittable. Neither applies to this case.

Best,
Xiangrui


"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 2 Jul 2014 14:42:31 +0530",Re: Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"Hi Patrick,

  Please see inline.

Regards,
Mridul




To rephrase in this context, attempting to cache wont help since it is
reducer specific and benefits are minimal (other than for reexecution
for failures and speculative tasks).




It would be a function of the number of mappers - and an overhead for each task.


Regards,
Mridul


"
qingyang li <liqingyang1985@gmail.com>,"Wed, 2 Jul 2014 17:44:25 +0800",Re: task always lost,dev@spark.apache.org,"executor always been removed.

someone encountered same issue
https://groups.google.com/forum/#!topic/spark-users/-mYn6BF-Y5Y

-------------
14/07/02 17:41:16 INFO storage.BlockManagerMasterActor: Trying to remove
executor 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
14/07/02 17:41:16 INFO storage.BlockManagerMaster: Removed
20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
14/07/02 17:41:16 DEBUG spark.MapOutputTrackerMaster: Increasing epoch to 10
14/07/02 17:41:16 INFO scheduler.DAGScheduler: Host gained which was in
lost list earlier: bigdata001
14/07/02 17:41:16 DEBUG scheduler.TaskSchedulerImpl: parentName: , name:
TaskSet_0, runningTasks: 0
14/07/02 17:41:16 DEBUG scheduler.TaskSchedulerImpl: parentName: , name:
TaskSet_0, runningTasks: 0
14/07/02 17:41:16 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID
12 on executor 20140616-143932-1694607552-5050-4080-3: bigdata004
(NODE_LOCAL)
14/07/02 17:41:16 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as
10785 bytes in 1 ms
14/07/02 17:41:16 INFO scheduler.TaskSetManager: Starting task 0.0:1 as TID
13 on executor 20140616-104524-1694607552-5050-26919-3: bigdata002
(NODE_LOCAL


2014-07-02 12:01 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:

"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 2 Jul 2014 16:14:26 +0530",Re: Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"Hi Reynold,

  Please see inline.

Regards,
Mridul


You are right, currently for large blocks, we just need to know where
the block exists.
I was not sure if there was any possible future extension on that -
for this reason, in order to preserve functionality, we moved to using
Short from Byte for MapOutputTracker.compressedSize (to ensure large
sizes can be represented with 0.7% error).

Within a MapStatus, we moved to holding compressed data to save on
space within master/workers (particularly for large number of
reducers).

If we do not anticipate any other reason for ""size"", we can move back
to using Byte instead of Short to compress size (which will reduce
required space by some factor less than 2) : since error in computed
size for blocks larger than maxBytesInFlight does not really matter :
we will split them into different FetchRequest's.



For well partitioned data, wont this not involve a lot of unwanted
requests to nodes which are not hosting data for a reducer (and lack
of ability to throttle).


Regards,
Mridul


"
salexln <salexln@gmail.com>,"Wed, 2 Jul 2014 11:02:31 -0700 (PDT)",Re: Contributing to MLlib,dev@spark.incubator.apache.org,"guys??? anyone???



--

"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Wed, 2 Jul 2014 11:10:24 -0700",Re: Contributing to MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi there,

Generally we try to avoid duplicating logic if possible, particularly for
algorithms that share a great deal of algorithmic similarity. See, for
example, the way we implement Logistic regression vs. Linear regression vs.
Linear SVM with different gradient functions all on top of SGD or L-BFGS.

Based on my (brief) look at the FCM algorithm, it appears that the main
difference is the ability to assign a weight vector associating the degree
of relationship of a given point to some centroid. My guess is that you can
figure out a way to inherit much of the K-Means logic in an algorithm for
FCM.

Regardless, if you'd like to add an algorithm, please create a JIRA ticket
for it and then send a pull request which references that JIRA where we can
discuss the specifics of that implementation and whether it is of broad
enough interest to warrant inclusion in the library.

- Evan



"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Wed, 2 Jul 2014 11:10:24 -0700",Re: Contributing to MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi there,

Generally we try to avoid duplicating logic if possible, particularly for
algorithms that share a great deal of algorithmic similarity. See, for
example, the way we implement Logistic regression vs. Linear regression vs.
Linear SVM with different gradient functions all on top of SGD or L-BFGS.

Based on my (brief) look at the FCM algorithm, it appears that the main
difference is the ability to assign a weight vector associating the degree
of relationship of a given point to some centroid. My guess is that you can
figure out a way to inherit much of the K-Means logic in an algorithm for
FCM.

Regardless, if you'd like to add an algorithm, please create a JIRA ticket
for it and then send a pull request which references that JIRA where we can
discuss the specifics of that implementation and whether it is of broad
enough interest to warrant inclusion in the library.

- Evan



"
salexln <salexln@gmail.com>,"Wed, 2 Jul 2014 11:21:32 -0700 (PDT)",Re: Contributing to MLlib,dev@spark.incubator.apache.org,"thanks for the response !

that's is exactly the way i wanted to implement it :)

I will create JIRA ticket and a request.



--

"
salexln <salexln@gmail.com>,"Wed, 2 Jul 2014 12:07:09 -0700 (PDT)",Re: Contributing to MLlib,dev@spark.incubator.apache.org,"I opened a JIRA (https://issues.apache.org/jira/browse/SPARK-2344)

and a pull request for this (https://github.com/salexln/spark/pull/1)



--

"
RJ Nowling <rnowling@gmail.com>,"Wed, 2 Jul 2014 15:41:33 -0400",Re: Contributing to MLlib,dev@spark.apache.org,"Hey Alex,

I'm also a new contributor.  I created a pull request for the KMeans
MiniBatch implementation here:

https://github.com/apache/spark/pull/1248

I also created a JIRA here:

https://issues.apache.org/jira/browse/SPARK-2308

As part of my work, I started to refactor the common code to create a
KMeansCommons file containing traits for the KMeans classes and KMeans
objects.

We should probability coordinate a bit on this.

RJ
rnowling@gmail.com




-- 
em rnowling@gmail.com
c 954.496.2314

"
Reynold Xin <rxin@databricks.com>,"Wed, 2 Jul 2014 23:02:14 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","

Was that a question? (I'm guessing it is). What do you mean exactly?
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 3 Jul 2014 00:00:49 -0700",Re: Contributing to MLlib,dev@spark.apache.org,"Alex, please send the pull request to apache/spark instead of your own
repo, following the instructions in

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Thanks,
Xiangrui


"
Bert Greevenbosch <Bert.Greevenbosch@huawei.com>,"Thu, 3 Jul 2014 07:04:23 +0000",RE: Artificial Neural Network in Spark?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Debasish, all,

Thanks for your feedback. I have submitted the code to GitHub and created a Jira ticket (links below).

The ANN uses back-propagation with the Steepest Gradient Descent (SGD) method.

Best regards,
Bert

https://github.com/apache/spark/pull/1290
https://issues.apache.org/jira/browse/SPARK-2352


 
> -----Original Message-----
> From: Debasish Das [mailto:debasish.das83@gmail.com]
> Sent: 01 July 2014 12:21
> To: dev@spark.apache.org
> Subject: Re: Artificial Neural Network in Spark?
> 
> I will let Xiangrui to comment on the PR process to add the code in
> mllib
> but I would love to look into your initial version if you push it to
> github...
> 
> As far as I remember Quoc got his best ANN results using back-
> propagation
> algorithm and solved using CG...do you have those features or you are
> using
> SGD style update....
> 
> 
> 
> On Mon, Jun 30, 2014 at 8:13 PM, Bert Greevenbosch <
> Bert.Greevenbosch@huawei.com> wrote:
> 
> > Hi Debasish, Alexander, all,
> >
> > Indeed I found the OpenDL project through the Powered by Spark page.
> I'll
> > need some time to look into the code, but on the first sight it looks
> quite
> > well-developed. I'll contact the author about this too.
> >
> > My own implementation (in Scala) works for multiple inputs and
> multiple
> > outputs. It implements a single hidden layer, the number of nodes in
> it can
> > be specified.
> >
> > The implementation is a general ANN implementation. As such, it
> should be
> > useable for an autoencoder too, since that is just an ANN with some
> special
> > input/output constraints.
> >
> > As said before, the implementation is built upon the linear
> regression
> > model and gradient descent implementation. However it did require
> some
> > tweaks:
> >
> > - The linear regression model only supports a single output ""label""
> (as
> > Double). Since the ANN can have multiple outputs, it ignores the
> ""label""
> > attribute, but for training divides the input vector into two parts,
> the
> > first part being the genuine input vector, the second the target
> output
> > vector.
> >
> > - The concatenation of input and target output vectors is only
> internally,
> > the training function takes as input an RDD with tuples of two
> Vectors, one
> > for each input and output.
> >
> > - The GradientDescend optimizer is re-used without modification.
> >
> > - I have made an even simpler updater than the SimpleUpdater, leaving
> out
> > the division by the square root of the number of iterations. The
> > SimpleUpdater can also be used, but I created this simpler one
> because I
> > like to plot the result every now and then, and then continue the
> > calculations. For this, I also wrote a training function with as
> input the
> > weights from the previous training session.
> >
> > - I created a ParallelANNModel similar to the LinearRegressionModel.
> >
> > - I created a new GeneralizedSteepestDescendAlgorithm class similar
> to the
> > GeneralizedLinearAlgorithm class.
> >
> > - Created some example code to test with 2D (1 input 1 output), 3D (2
> > inputs 1 output) and 4D (1 input 3 outputs) functions.
> >
> > If there is interest, I would be happy to release the code. What
> would be
> > the best way to do this? Is there some kind of review process?
> >
> > Best regards,
> > Bert
> >
> >
> > > -----Original Message-----
> > > From: Debasish Das [mailto:debasish.das83@gmail.com]
> > > Sent: 27 June 2014 14:02
> > > To: dev@spark.apache.org
> > > Subject: Re: Artificial Neural Network in Spark?
> > >
> > > Look into Powered by Spark page...I found a project there which
> used
> > > autoencoder functions...It's not updated for a long time now !
> > >
> > > On Thu, Jun 26, 2014 at 10:51 PM, Ulanov, Alexander
> > > <alexander.ulanov@hp.com
> > > > wrote:
> > >
> > > > Hi Bert,
> > > >
> > > > It would be extremely interesting. Do you plan to implement
> > > autoencoder as
> > > > well? It would be great to have deep learning in Spark.
> > > >
> > > > Best regards, Alexander
> > > >
> > > > 27.06.2014, в 4:47, ""Bert Greevenbosch""
> <Bert.Greevenbosch@huawei.com>
> > > > написал(а):
> > > >
> > > > > Hello all,
> > > > >
> > > > > I was wondering whether Spark/mllib supports Artificial Neural
> > > Networks
> > > > (ANNs)?
> > > > >
> > > > > If not, I am currently working on an implementation of it. I
> re-use
> > > the
> > > > code for linear regression and gradient descent as much as
> possible.
> > > > >
> > > > > Would the community be interested in such implementation? Or
> maybe
> > > > somebody is already working on it?
> > > > >
> > > > > Best regards,
> > > > > Bert
> > > >
> >
"
Bert Greevenbosch <Bert.Greevenbosch@huawei.com>,"Thu, 3 Jul 2014 07:07:15 +0000",RE: Artificial Neural Network in Spark?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Alexander, all,

I now have uploaded the code (see links below), and look forward to learn about the outcome of your experiments!
 
Best regards,
Bert

---
https://github.com/apache/spark/pull/1290
https://issues.apache.org/jira/browse/SPARK-2352


> -----Original Message-----
> From: Ulanov, Alexander [mailto:alexander.ulanov@hp.com]
> Sent: 01 July 2014 18:17
> To: dev@spark.apache.org
> Subject: RE: Artificial Neural Network in Spark?
> 
> Hi Bert,
> 
> There is a specific process of pull request if you wish to share the
> code
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> 
> I would be glad to benchmark your ANN implementation by means of
> running some experiments that we run with the other ANN toolkits. I am
> also interested in Autoencoder and have plans to implement it for MLLib
> in the near future.
> 
> Best regards, Alexander
> 
> -----Original Message-----
> From: Bert Greevenbosch [mailto:Bert.Greevenbosch@huawei.com]
> Sent: Tuesday, July 01, 2014 7:14 AM
> To: dev@spark.apache.org
> Subject: RE: Artificial Neural Network in Spark?
> 
> Hi Debasish, Alexander, all,
> 
> Indeed I found the OpenDL project through the Powered by Spark page.
> I'll need some time to look into the code, but on the first sight it
> looks quite well-developed. I'll contact the author about this too.
> 
> My own implementation (in Scala) works for multiple inputs and multiple
> outputs. It implements a single hidden layer, the number of nodes in it
> can be specified.
> 
> The implementation is a general ANN implementation. As such, it should
> be useable for an autoencoder too, since that is just an ANN with some
> special input/output constraints.
> 
> As said before, the implementation is built upon the linear regression
> model and gradient descent implementation. However it did require some
> tweaks:
> 
> - The linear regression model only supports a single output ""label"" (as
> Double). Since the ANN can have multiple outputs, it ignores the
> ""label"" attribute, but for training divides the input vector into two
> parts, the first part being the genuine input vector, the second the
> target output vector.
> 
> - The concatenation of input and target output vectors is only
> internally, the training function takes as input an RDD with tuples of
> two Vectors, one for each input and output.
> 
> - The GradientDescend optimizer is re-used without modification.
> 
> - I have made an even simpler updater than the SimpleUpdater, leaving
> out the division by the square root of the number of iterations. The
> SimpleUpdater can also be used, but I created this simpler one because
> I like to plot the result every now and then, and then continue the
> calculations. For this, I also wrote a training function with as input
> the weights from the previous training session.
> 
> - I created a ParallelANNModel similar to the LinearRegressionModel.
> 
> - I created a new GeneralizedSteepestDescendAlgorithm class similar to
> the GeneralizedLinearAlgorithm class.
> 
> - Created some example code to test with 2D (1 input 1 output), 3D (2
> inputs 1 output) and 4D (1 input 3 outputs) functions.
> 
> If there is interest, I would be happy to release the code. What would
> be the best way to do this? Is there some kind of review process?
> 
> Best regards,
> Bert
> 
> 
> > -----Original Message-----
> > From: Debasish Das [mailto:debasish.das83@gmail.com]
> > Sent: 27 June 2014 14:02
> > To: dev@spark.apache.org
> > Subject: Re: Artificial Neural Network in Spark?
> >
> > Look into Powered by Spark page...I found a project there which used
> > autoencoder functions...It's not updated for a long time now !
> >
> > On Thu, Jun 26, 2014 at 10:51 PM, Ulanov, Alexander
> > <alexander.ulanov@hp.com
> > > wrote:
> >
> > > Hi Bert,
> > >
> > > It would be extremely interesting. Do you plan to implement
> > autoencoder as
> > > well? It would be great to have deep learning in Spark.
> > >
> > > Best regards, Alexander
> > >
> > > 27.06.2014, в 4:47, ""Bert Greevenbosch""
> > > <Bert.Greevenbosch@huawei.com>
> > > написал(а):
> > >
> > > > Hello all,
> > > >
> > > > I was wondering whether Spark/mllib supports Artificial Neural
> > Networks
> > > (ANNs)?
> > > >
> > > > If not, I am currently working on an implementation of it. I
> > > > re-use
> > the
> > > code for linear regression and gradient descent as much as possible.
> > > >
> > > > Would the community be interested in such implementation? Or
> maybe
> > > somebody is already working on it?
> > > >
> > > > Best regards,
> > > > Bert
> > >
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 3 Jul 2014 03:30:47 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hi Xiangrui,

I did some out-of-box comparisons with ECOS and PDCO from SOL.

Both of them seems to be running at par but I will do more detailed
analysis.

I used pdco's testQP randomized problem generation. pdcotestQP(m, n) means
m constraints and n variables

For ECOS runtime reference here is the paper
http://web.stanford.edu/~boyd/papers/pdf/ecos_ecc.pdf

It runs at par with gurobi but slower than MOSEK. Note that MOSEK is also a
SOCP solver.

K>> pdcotestQP(50, 100)

ECOSQP: Converting QP to SOCP...

ECOSQP: Time for Cholesky: 0.00 seconds

Conversion completed. Calling ECOS...

ECOS - (c) A. Domahidi, Automatic Control Laboratory, ETH Zurich, 2012-2014.

OPTIMAL (within feastol=1.0e-05, reltol=1.0e-06, abstol=1.0e-06).

Runtime: 0.010340 seconds.

--------------------------------------------------------

   pdco.m                            Version of 23 Nov 2013

   Primal-dual barrier method to minimize a convex function

   subject to linear constraints Ax + r = b,  bl <= x <= bu



   Michael Saunders       SOL and ICME, Stanford University

   Contributors:     Byunggyoo Kim (SOL), Chris Maes (ICME)

                     Santiago Akle (ICME), Matt Zahr (ICME)

   --------------------------------------------------------

m        =       50     n        =      100      nnz(A)  =      483

Method   =       21     (1=chol  2=QR  3=LSMR  4=MINRES  21=SQD(LU)
22=SQD(MA57))

Elapsed time is 0.050226 seconds.

2. With a larger problem with 50 equality and 1000 variables:

K>> pdcotestQP(50, 1000)

ECOSQP: Converting QP to SOCP...

ECOSQP: Time for Cholesky: 0.05 seconds

Conversion completed. Calling ECOS...

ECOS - (c) A. Domahidi, Automatic Control Laboratory, ETH Zurich, 2012-2014.

OPTIMAL (within feastol=1.0e-05, reltol=1.0e-06, abstol=1.0e-06).

Runtime: 6.333036 seconds.


   --------------------------------------------------------

   pdco.m                            Version of 23 Nov 2013

   Primal-dual barrier method to minimize a convex function

   subject to linear constraints Ax + r = b,  bl <= x <= bu



   Michael Saunders       SOL and ICME, Stanford University

   Contributors:     Byunggyoo Kim (SOL), Chris Maes (ICME)

                     Santiago Akle (ICME), Matt Zahr (ICME)

   --------------------------------------------------------


The objective is defined by a function handle:

   @(x)deal(0.5*(x'*H*x)+c'*x,H*x+c,H)

The matrix A is an explicit sparse matrix

m        =       50     n        =     1000      nnz(A)  =     4842

Method   =       21     (1=chol  2=QR  3=LSMR  4=MINRES  21=SQD(LU)
22=SQD(MA57))

Elapsed time is 7.531934 seconds.

I will change the Method = 21 (LU) to 1 (chol) and that should help PDCO.
If both the IPMs are at par it's still a good idea to choose ECOS as the
generic IPM since it can solve conic programs which are a superset of
quadratic programs (robust portfolio optimization from quantitative finance
is an example of standard conic program).

For the runtime comparisons with ADMM based decomposition for simpler
constraints, I am doing further profiling and see if the jnilib is causing
any performance issues for ECOS calls...

Please look at the proximal algorithm references
http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf. For many problems
like L1 constraint / positivity / bounds / huber / hyperplane projection
etc, the proximal operator is simple to evaluate and for these cases ADMM
decomposition has been shown to run faster than standard constraint solvers
like IPM. I am not very surprised that Sparse NMF runs in 4X runtime of
least squares using ADMM decomposition.

Distributed consensus is another ADMM decomposition which we are working
with right now. We will have some results on that soon. There the idea is
to use ADMM so that accumulator need not collect dense gradient vectors
from each worker. This development will further help the treeReduce work.

Should I open up Spark JIRA's so that we can document Quadratic
Minimization related runtime experiments/benchmarks and share the code for
review ?

Most likely the core solvers will go to breeze and in Spark mllib
optimization, I will add a QpSolver object which will call the underlying
breeze solvers based on the problem complexity....the ecos jnilib can be
part of breeze natives as it is GPL licensed (same as netlib-java
jniloader). I will push the jnilib as a PR to ecos repository
https://github.com/ifa-ethz/ecos

Thanks.

Deb

"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 3 Jul 2014 11:24:35 +0000",Pass parameters to RDD functions,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I wonder how I can pass parameters to RDD functions with closures. If I do it in a following way, Spark crashes with NotSerializableException:

class TextToWordVector(csvData:RDD[Array[String]]) {

  val n = 1
  lazy val x = csvData.map{ stringArr => stringArr(n)}.collect()
}

Exception:
Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.spark.mllib.util.TextToWordVector
org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.spark.mllib.util.TextToWordVector
                at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1038)


This message proposes a workaround, but it didn't work for me:
http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CCAA_qdLrxXzwXd5=6SXLOgSmTTorpOADHjnOXn=tMrOLEJM=Frw@mail.gmail.com%3E

What is the best practice?

Best regards, Alexander
"
Sean Owen <sowen@cloudera.com>,"Thu, 3 Jul 2014 12:30:38 +0100",Re: Pass parameters to RDD functions,"""dev@spark.apache.org"" <dev@spark.apache.org>","Declare this class with ""extends Serializable"", meaning java.io.Serializable?


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 3 Jul 2014 12:21:52 +0000",RE: Pass parameters to RDD functions,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks, this works both with Scala and Java Serializable. Which one should I use?

Related question: I would like only the particular val to be used instead of the whole class, what should I do?
As far as I understand, the whole class is serialized and transferred between nodes (am I right?)

Alexander

-----Original Message-----
Fry, July 03, 2014 3:31 PM
To: dev@spark.apache.org
Subject: Re: Pass parameters to RDD functions

Declare this class with ""extends Serializable"", meaning java.io.Serializable?

On Thu, Jul 3, 2014 at 12:24 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
> Hi,
>
> I wonder how I can pass parameters to RDD functions with closures. If I do it in a following way, Spark crashes with NotSerializableException:
>
> class TextToWordVector(csvData:RDD[Array[String]]) {
>
>   val n = 1
>   lazy val x = csvData.map{ stringArr => stringArr(n)}.collect() }
>
> Exception:
> Job aborted due to stage failure: Task not serializable: 
> java.io.NotSerializableException: 
> org.apache.spark.mllib.util.TextToWordVector
> org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.spark.mllib.util.TextToWordVector
>                 at 
> org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAG
> Scheduler$$failJobAndIndependentStages(DAGScheduler.scala:1038)
>
>
> This message proposes a workaround, but it didn't work for me:
> http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CCAA
> _qdLrxXzwXd5=6SXLOgSmTTorpOADHjnOXn=tMrOLEJM=Frw@mail.gmail.com%3E
>
> What is the best practice?
>
> Best regards, Alexander
"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 3 Jul 2014 20:31:50 +0530",Re: Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"

I was not sure if I understood the proposal correctly - hence the
query : if I understood it right - the number of wasted requests goes
up by num_reducers * avg_nodes_not_hosting data.

Ofcourse, if avg_nodes_not_hosting data == 0, then we are fine !

Regards,
Mridul

"
salexln <salexln@gmail.com>,"Thu, 3 Jul 2014 08:45:24 -0700 (PDT)",Re: Contributing to MLlib,dev@spark.incubator.apache.org,"thanks for the input.
at the moment , I don't have any code commits yet.

I wanted to discuss the algorithm implementation prior to the code
submission.

(never work with Git\ GutHub - so I hope this isn't very basic stuff....)








--

"
Denis Turdakov <turdakov@ispras.ru>,"Thu, 3 Jul 2014 08:49:44 -0700 (PDT)",PLSA,dev@spark.incubator.apache.org,"Hello guys,

We made pull request with PLSA and its modifications:
- https://github.com/apache/spark/pull/1269
- JIRA issue SPARK-2199
Could somebody look at the code and provide some feedback what we should
improve.

Best regards,
Denis Turdakov




--

"
Aaron Davidson <ilikerps@gmail.com>,"Thu, 3 Jul 2014 09:28:01 -0700",Re: Pass parameters to RDD functions,dev@spark.apache.org,"Either Serializable works, scala Serializable extends Java's (originally
intended a common interface for people who didn't want to run Scala on a
JVM).

Class fields require the class be serialized along with the object to
access. If you declared ""val n"" inside a method's scope instead, though, we
wouldn't need the class. E.g.:

class TextToWordVector(csvData:RDD[Array[String]]) {
  def computeX() = {
    val n = 1
    csvData.map{ stringArr => stringArr(n)}.collect()
  }
  lazy val x = computeX()
}

Note that if the class itself doesn't actually contain many (large) fields,
though, it may not be an issue to actually transfer it around.




"
Aaron Davidson <ilikerps@gmail.com>,"Thu, 3 Jul 2014 09:37:24 -0700",Re: task always lost,dev@spark.apache.org,"The issue you're seeing is not the same as the one you linked to -- your
serialized task sizes are very small, and Mesos fine-grained mode doesn't
use Akka anyway.

The error log you printed seems to be from some sort of Mesos logs, but do
you happen to have the logs from the actual executors themselves? These
should be Spark logs which hopefully show the actual Exception (or lack
thereof) before the executors die.

The tasks are dying very quickly, so this is probably either related to
your application logic throwing some sort of fatal JVM error or due to your
Mesos setup. I'm not sure if that ""Failed to fetch URIs for container"" is
fatal or not.



"
Debasish Das <debasish.das83@gmail.com>,"Thu, 3 Jul 2014 10:57:05 -0700",Re: PLSA,dev@spark.apache.org,"Hi Denis,

Are you using matrix factorization to generate the latent factors ?

Thanks.
Deb




"
Patrick Wendell <pwendell@gmail.com>,"Thu, 3 Jul 2014 13:37:42 -0700","Re: Assorted project updates (tests, build, etc)","""dev@spark.apache.org"" <dev@spark.apache.org>","Just a reminder here - we'll soon be merging a patch that changes the
SBT build internals significantly. We've tried to make this fully
backwards compatible, but there may be issues (which we'll resolve as
they arrive).

https://github.com/apache/spark/pull/77

- Patrick


"
Reynold Xin <rxin@databricks.com>,"Thu, 3 Jul 2014 15:12:50 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yes, that number is likely == 0 in any real workload ...



"
Reynold Xin <rxin@databricks.com>,"Thu, 3 Jul 2014 15:13:28 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Note that in my original proposal, I was suggesting we could track whether
block size = 0 using a compressed bitmap. That way we can still avoid
requests for zero-sized blocks.




"
Patrick Wendell <pwendell@gmail.com>,"Thu, 3 Jul 2014 15:48:46 -0700","Re: Assorted project updates (tests, build, etc)","""dev@spark.apache.org"" <dev@spark.apache.org>","Sorry all, I sent the wrong pull request to refer to Prashant's work:

https://github.com/apache/spark/pull/772


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 4 Jul 2014 14:58:47 +0530",Re: Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"In our clusters, number of containers we can get is high but memory
per container is low : which is why avg_nodes_not_hosting data is
rarely zero for ML tasks :-)

To update - to unblock our current implementation efforts, we went
with broadcast - since it is intutively easier and minimal change; and
compress the array as bytes in TaskResult.
This is then stored in disk backed maps - to remove memory pressure on
master and workers (else MapOutputTracker becomes a memory hog).

But I agree, compressed bitmap to represent 'large' blocks (anything
larger that maxBytesInFlight actually) and probably existing to track
non zero should be fine (we should not really track zero output for
reducer - just waste of space).


Regards,
Mridul


"
Denis Turdakov <turdakov@ispras.ru>,"Fri, 4 Jul 2014 04:27:02 -0700 (PDT)",Re: PLSA,dev@spark.incubator.apache.org,"Hi, Deb.

I don't quite understand the question. PLSA is an instance of matrix
factorization problem. 

If you are asking about inference algorithm, we use EM-algorithm.
Description of this approach is, for example, here:
http://www.machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf


Best, Denis.



--

"
Debasish Das <debasish.das83@gmail.com>,"Fri, 4 Jul 2014 08:48:20 -0700",Re: PLSA,dev@spark.apache.org,"Thanks for the pointer...

Looks like you are using EM algorithm for factorization which looks similar
to multiplicative update rules

Do you think using mllib ALS implicit feedback, you can scale the problem
further ?

We can handle L1, L2, equality and positivity constraints in ALS now...As
long as you can find the gradient and hessian from the KL divergence loss,
you can use that in place of gram matrix that is used in ALS right now

If you look in topic modeling work in Solr (Carrot is the package), they
use ALS to generate the topics...that algorithm looks like a simplified
version of what you are attempting here...

May be the EM algorithm for topic modeling is efficient than ALS but from
looking at it I don't see how...I see lot of broadcasts...while in implicit
feedback you need one broadcast of gram matrix...


"
Debasish Das <debasish.das83@gmail.com>,"Fri, 4 Jul 2014 10:19:06 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"I looked further and realized that ECOS used a mex file while PDCO is using
pure Matlab code. So the out-of-box runtime comparison is not fair.

I am trying to generate PDCO C port. Like ECOS, PDCO also makes use of
sparse support from Tim Davis.

Thanks.
Deb
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Fri, 4 Jul 2014 11:14:27 -0700",Invalid link for Spark 1.0.0 in Official Web Site,dev@spark.apache.org,"Hi,

I found there is a invalid link in <http://spark.apache.org/downloads.html> .
The link for release note of Spark 1.0.0 indicates http://spark.apache.org/releases/spark-release-1.0.0.html but this link is invalid.
I think that is mistake for <http://spark.apache.org/releases/spark-release-1-0-0.html>.

Thanks,
Kousuke


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 4 Jul 2014 12:39:14 -0700",[VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.0.1!

The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1021/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/

Please vote on releasing this package as Apache Spark 1.0.1!

The vote is open until Monday, July 07, at 20:45 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

=== Differences from RC1 ===
This release includes only one ""blocking"" patch from rc1:
https://github.com/apache/spark/pull/1255

There are also smaller fixes which came in over the last week.

=== About this release ===
This release fixes a few high-priority bugs in 1.0 and has a variety
of smaller fixes. The full list is here: http://s.apache.org/b45. Some
of the more visible patches are:

SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
SPARK-1790: Support r3 instance types on EC2.

This is the first maintenance release on the 1.0 line. We plan to make
additional maintenance releases as new fixes come in.

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 4 Jul 2014 12:39:53 -0700",[RESULT] [VOTE] Release Apache Spark 1.0.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of RC2. Thanks to everyone who voted.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 4 Jul 2014 12:40:38 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'll start the voting with a +1 - ran tests on the release candidate
and ran some basic programs. RC1 passed our performance regression
suite, and there are no major changes from that RC.


"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 4 Jul 2014 21:02:38 +0000","2nd Workshop on Sustainable Software for Science: Practice and
 Experiences (WSSSPE2)","""dev@oodt.apache.org"" <dev@oodt.apache.org>,
        ""dev@gora.apache.org""
	<dev@gora.apache.org>,
        ""dev@tika.apache.org"" <dev@tika.apache.org>","(apologies for Cross Posting)

2nd Workshop on Sustainable Software for Science: Practice and Experiences
(WSSSPE2)
http://wssspe.researchcomputing.org.uk/wssspe2/
(to be held in conjunction with SC14, Sunday, 16 November 2014, New
Orleans, LA, USA)

Progress in scientific research is dependent on the quality and
accessibility of software at all levels and it is critical to address
challenges related to the development, deployment, and maintenance of
reusable software as well as education around software practices. These
challenges can be technological, policy based, organizational, and
educational, and are of interest to developers (the software community),
users (science disciplines), and researchers studying the conduct of
science (science of team science, science of organizations, science of
science and innovation policy, and social science communities).

The WSSSPE1 workshop (http://wssspe.researchcomputing.org.uk/WSSSPE1)
engaged the broad scientific community to identify challenges and best
practices in areas of interest for sustainable scientific software. At
WSSSPE2, we invite the community to propose and discuss specific
mechanisms to move towards an imagined future practice of software
development and usage in science and engineering. The workshop will
include multiple mechanisms for participation, encourage team building
around solutions, and identify risky solutions with potentially
transformative outcomes. Participation by early career students and
postdoctoral researchers is strongly encouraged.

We invite short (4-page) actionable papers that will lead to improvements
for sustainable software science. These papers could be a call to action,
or could provide position or experience reports on sustainable software
activities. The papers will be used by the organizing committee to design
sessions that will be highly interactive and targeted towards facilitating
action. Submitted papers should be archived by a third-party service that
provides DOIs. We encourage submitters to license their papers under a
Creative Commons license that encourages sharing and remixing, as we will
combine ideas (with attribution) into the outcomes of the workshop.

The organizers will invite one or more submitters of provocative papers to
start the workshop by presenting highlights of their papers in a keynote
presentation to initiate active discussion that will continue throughout
the day.

Areas of interest for WSSSPE2, include, but are not limited to:

=80 defining software sustainability in the context of science and
engineering software
=80 how to evaluate software sustainability
=80 improving the development process that leads to new software
=80 methods to develop sustainable software from the outset
=80 effective approaches to reusable software created as a by-product of
research
=80 impact of computer science research on the development of scientific
software
=80 recommendations for the support and maintenance of existing software
=80 software engineering best practices
=80 governance, business, and sustainability models
=80 the role of community software repositories, their operation and
sustainability
=80 reproducibility, transparency needs that may be unique to science
=80 successful open source software implementations
=80 incentives for using and contributing to open source software
=80 transitioning users into contributing developers
=80 building large and engaged user communities
=80 developing strong advocates
=80 measurement of usage and impact
=80 encouraging industry=B9s role in sustainability
=80 engagement of industry with volunteer communities
=80 incentives for industry
=80 incentives for community to contribute to industry-driven projects
=80 recommending policy changes
=80 software credit, attribution, incentive, and reward
=80 issues related to multiple organizations and multiple countries, such
a=
s
intellectual property, licensing, etc.
=80 mechanisms and venues for publishing software, and the role of
publishe=
rs
=80 improving education and training
=80 best practices for providing graduate students and postdoctoral
researchers in domain communities with sufficient training in software
development
=80 novel uses of sustainable software in education (K-20)
=80 case studies from students on issues around software development in the
undergraduate or graduate curricula
=80 careers and profession
=80 successful examples of career paths for developers
=80 institutional changes to support sustainable software such as promotion
and tenure metrics, job categories, etc.

Submissions:

Submissions of up to four pages should be formatted to be easily readable
and submitted to an open access repository that provides unique
identifiers (e.g., DOIs) that can be cited, for example http://arXiv.org
<http://arxiv.org/>
or http://figshare.com <http://figshare.com/>.

repository, submit it to WSSSPE2 by creating a new submission at
https://www.easychair.org/conferences/?conf=3Dwssspe2, and entering:

=80 author information for all authors
=80 title
=80 abstract (with the identifier as the first line of the abstract, for
example, http://dx.doi.org/10.6084/m9.figshare.791606 or
http://arxiv.org/abs/1404.7414 or alternative)
=80 at least three keywords
=80 tick the abstract only box
Do not submit the paper itself through EasyChair; the identifier in the
abstract that points to the paper is sufficient.

Deadline for Submission:

14 July 2014 (any time of day, no extensions)

Travel Support

Funds are available to support participation in WSSSPE2 by 1) US-based
students, early-career researchers, and members of underrepresented
groups; and 2) participants who would not otherwise attend the SC14
conference. Priority will be given to those who have submitted papers and
can make a compelling case for how their participation will strengthen the
overall workshop and/or positively impact their future research or
educational activities.

Submissions for travel support will be accepted from September 1st to
September 15th 2014 following instructions posted on the workshop web site.

Financial support to enable this has been generously provided by 1) the
National Science Foundation and 2) the Gordon and Betty Moore Foundation.

Important Dates:

July 14, 2014 Paper submission deadline
September 1, 2014 Author notification
September 15, 2014  Funding request submission deadline
September 22, 2014  Funding decision notification
November 16, 2014 WSSSPE2 Workshop

Organizers:

=80 Daniel S. Katz, d.katz@ieee.org, National Science Foundation, USA
=80 Gabrielle Allen, gdallen@illinois.edu, University of Illinois
Urbana-Champaign, USA
=80 Neil Chue Hong, N.ChueHong@software.ac.uk, Software Sustainability
Institute, University of Edinburgh, UK
=80 Karen Cranston, karen.cranston@nescent.org, National Evolutionary
Synthesis Center (NESCent), USA
=80 Manish Parashar, parashar@rutgers.edu, Rutgers University, USA
=80 David Proctor, djproctor@gmail.com, National Science Foundation, USA
=80 Matthew Turk, matthewturk@gmail.com, Columbia University, USA
=80 Colin C. Venters, colin.venters@googlemail.com, University of
Huddersfield, UK
=80 Nancy Wilkins-Diehr, wilkinsn@sdsc.edu, San Diego Supercomputer Center,
University of California, San Diego, USA

Program Committee:

=80 Aron Ahmadia, U.S. Army Engineer Research and Development Center, USA
=80 Liz Allen, Wellcome Trust, UK
=80 Lorena A. Barba, The George Washington University, USA
=80 C. Titus Brown, Michigan State University, USA
=80 Coral Calero, Universidad Castilla La Mancha, Spain
=80 Jeffrey Carver, University of Alabama, USA
=80 Ewa Deelman, University of Southern California, USA
=80 Gabriel A. Devenyi, McMaster University, Canada
=80 Charlie E. Dibsdale, O-Sys, Rolls Royce PLC, UK
=80 Alberto Di Meglio, CERN, Switzerland
=80 Anshu Dubey, Lawrence Berkeley National Laboratory, USA
=80 David Gavaghan, University of Oxford, UK
=80 Paul Ginsparg, Cornell University, USA
=80 Josh Greenberg, Alfred P. Sloan Foundation, USA
=80 Sarah Harris, University of Leeds, UK
=80 James Herbsleb, Carnegie Mellon University, USA
=80 James Howison, University of Texas at Austin, USA
=80 Caroline Jay, University of Manchester, UK
=80 Matthew B. Jones, National Center for Ecological Analysis and Synthesis
(NCEAS), University of California, Santa Barbara, USA
=80 Jong-Suk Ruth Lee, National Institute of Supercomputing and Networking,
KISTI (Korea Institute of Science and Technology Information), Korea
=80 James Lin, Shanghai Jiao Tong University, China
=80 Frank L=F6ffler, Louisiana State University, USA
=80 Chris A. Mattmann, NASA JPL & University of Southern California, USA
=80 Robert H. McDonald, Indiana University, USA
=80 Lois Curfman McInnes, Argonne National Laboratory, USA
=80 Chris Mentzel, Gordon and Betty Moore Foundation, USA
=80 Kenneth M. Merz, Jr., Michigan State University, USA
=80 Marek T. Michalewicz, A*STAR Computational Resource Centre, Singapore
=80 Peter E. Murray, LYRASIS, USA
=80 Kenjo Nakajima, University of Tokyo, Japan
=80 Cameron Neylon, PLOS, UK
=80 Aleksandra Pawlik, Software Sustainability Institute, Manchester
University, UK
=80 Birgit Penzenstadler, University of California, Irvine, USA
=80 Marian Petre, The Open University, UK
=80 Mark D. Plumbley, Queen Mary University of London, UK
=80 Andreas Prlic, University of California, San Diego, USA
=80 Victoria Stodden, Columbia University, USA
=80 Kaitlin Thaney, Mozilla Science Lab, USA
=80 Greg Watson, IBM, USA
=80 Theresa Windus, Iowa State University and Ames Laboratory, USA





++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 168-519, Mailstop: 168-527
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++





"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 5 Jul 2014 09:30:43 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,1
Michael Armbrust <michael@databricks.com>,"Sat, 5 Jul 2014 13:38:13 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"+1

I tested sql/hive functionality.



"
DB Tsai <dbtsai@dbtsai.com>,"Sat, 5 Jul 2014 14:02:20 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,1
Krishna Sankar <ksankar42@gmail.com>,"Sat, 5 Jul 2014 19:41:21 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"+1

   - Compiled rc2 w/ CentOS 6.5, Yarn,Hadoop 2.2.0 - successful
   - Smoke Test (scala,python) (distributed cluster) - successful
   - We had ran Java/SparkSQL (count, distinct et al) ~250M records RDD
   over HBase 0.98.3 over last build (rc1) - succ"
Soren Macbeth <soren@yieldbot.com>,"Sat, 5 Jul 2014 20:01:34 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>",1
Andrew Or <andrew@databricks.com>,"Sat, 5 Jul 2014 22:54:22 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1, verified that the UI bug is in fact fixed in
https://github.com/apache/spark/pull/1255.


2014-07-05 20:01 GMT-07:00 Soren Macbeth <soren@yieldbot.com>:

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 6 Jul 2014 16:21:37 -0400",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"+1

Tested on Mac OS X.

Matei


RDD
Yarn
<pwendell@gmail.com>
candidate
<pwendell@gmail.com>
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
found
https://repository.apache.org/content/repositories/"
Denis Turdakov <turdakov@ispras.ru>,"Mon, 7 Jul 2014 08:34:02 -0700 (PDT)",Re: PLSA,dev@spark.incubator.apache.org,"Hi, Deb.

Thanks for your idea to use ALS for PLSA training. I discussed it with our
engineers and it seems it's better to use EM. We have the following points:

1. We have some doubts that ALS is applicable to the problem. By its
definition, PLSA is a matrix decomposition with respect to Kullback–Leibler
divergence. Unlike matrix decomposition with respect to Frobenius' distance,
this problem lacks convexity. Actually, PLSA has multiple solutions and
EM-algorithm is experimentally proven to obtain ""good"" local maxima. By the
way, Kullback–Leibler divergence is not symmetric, does not satisfy to
triangle inequality and the objects it's defined on do not form the linear
space -- would not it be a problem for ALS? For instance, does not ALS rely
on the fact that L_2 is defined on the object that form self-dual space?  

2. Using EM-algorithm is a common way for PLSA training described in most
known papers. ""Contributing to Spark"" page says, ""a new algorithm"" ""should
be used and accepted"" and ""widely known"". We have found no publications
describing PLSA training via ALS. So, we are not sure if it will provide
results of comparable quality to EM-algorithm. That could be an issue for
research.

3. PLSA objective function is non-differentiable in some points (e.g.
\phi_{wt} = 0 \forall t), EM-algorithm is theoretically proven to avoid such
points. We are afraid, ALS is likely to fall into this trap.

4. We also suppose that EM is faster for this problem. PLSA has D*T + W*T
parameters (where D stand for the number of documents, T - for the number of
topics, W - for the size of the alphabet). Thus, Hessian matrix has size
(D*T + W*T)^2 -- that's a lot. Note, EM-algorithm has O(D*T*W) complexity
for every iteration. Probably, it's possible to use a diagonalized Hessian,
but it will increase the number of iterations needed for convergence and we
think EM-algorithm will outperform it due to the fact that we have a very
simple analytical solution for E-step. (There was a story in 80's about
EM-algorithms and second-order methods. Second-order methods were believed
to outperform EM-algorithm unless someone has proven that it's not necessary
to conduct precise optimization during E-step -- it's enough to increase the
objective function. This idea allowed to speed up E-step and EM-algorithm
analytical solution for E-step -- it's very fast).

5. As far as we can understand, RALS handles only quadratic regularizers 
(maybe it's possible to substitute a quadratic approximation at every
iteration, but we've no idea why it must work). In PSLA we want to allow
user to define every regularizer she wants.

is  broadcasted (yes, we have to call broadcast(...) method in three
different places, but we broadcast only once in iteration). 



--
3.nabble.com/PLSA-tp7170p7194.html
om.

"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 7 Jul 2014 09:23:56 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hey Deb,

If your goal is to solve the subproblems in ALS, exploring sparsity
doesn't give you much benefit because the data is small and dense.
Porting either ECOS's or PDCO's implementation but using dense
representation should be sufficient. Feel free to open a JIRA and we
can move our discussion there.

Best,
Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 7 Jul 2014 09:32:16 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"+1

Ran mllib examples.


"
xwei <weixiaokai@gmail.com>,"Mon, 7 Jul 2014 12:00:57 -0700 (PDT)",Re: Contributing to MLlib on GLM,dev@spark.incubator.apache.org,"Hi Gang,

No admin is looking at our patch:( do you have some suggestions so that our
patch can get noticed by the admin?

Best regards,

Xiaokai



po.
the
’ll
e,
]
s
e
-MLlib-on-GLM-tp7033p7088.html
-MLlib-on-GLM-tp7033p7107.html
-MLlib-on-GLM-tp7033p7117.html
-MLlib-on-GLM-tp7033p7131.html
ervlet.jtp?macro=unsubscribe_by_code&node=7033&code=d2VpeGlhb2thaUBnbWFpbC5jb218NzAzM3w2NTc5NDUzMzA=>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7197.html
om."
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Mon, 7 Jul 2014 14:17:43 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with authentication on.

Tom


 


Please vote on releasing the following candidate as Apache Spark version 1.0.1!

The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
https://git-wip-us.apache.or"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Jul 2014 15:47:40 -0700",Re: Invalid link for Spark 1.0.0 in Official Web Site,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for reporting this. I just fixed it.




"
Gang Bai <baigang@staff.sina.com.cn>,"Tue, 8 Jul 2014 10:32:46 +0800",Re: Contributing to MLlib on GLM,<dev@spark.apache.org>,"Poisson and Gamma regressions for modeling count data are definitely important in spark.mllib.regression. So don’t worry. Let’s change the updater to SquaredL2Updater as we discussed in the PR. Then we can ask Jenkins to run the test.


that our
your repo.
https://github.com/xwei-datageek/spark/pull/2 .
the
improve the
So I’ll
different
class
code,
suggest
List]
Poisson
Also,
fields
classes?
classes
it's OK
real-world
my
gradients. We
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7088.html
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7107.html
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7117.html
discussion
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7131.html
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=7033&code=d2VpeGlhb2thaUBnbWFpbC5jb218NzAzM3w2NTc5NDUzMzA=>
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7197.html
Nabble.com.


"
"""Lizhengbing (bing, BIPA)"" <zhengbing.li@huawei.com>","Tue, 8 Jul 2014 06:29:55 +0000","Could the function MLUtils.loadLibSVMFile be modified to support
 zero-based-index data?","""dev@spark.apache.org"" <dev@spark.apache.org>","
1)  I download the imdb data from http://komarix.org/ac/ds/Blanc__Mel.txt.bz2 and use this data to test LBFGS
When I run examples referencing http://spark.apache.org/docs/latest/mllib-optimization.html,  an error occus.
4/07/07 08:37:27 ERROR Executor: Exception in task ID 2
java.lang.ArrayIndexOutOfBoundsException: -1
         at breeze.linalg.operators.DenseVector_SparseVector_Ops$$anon$129.apply(SparseVectorOps.scala:231)
         at breeze.linalg.operators.DenseVector_SparseVector_Ops$$anon$129.apply(SparseVectorOps.scala:216)
         at breeze.linalg.operators.BinaryRegistry$class.apply(BinaryOp.scala:60)
         at breeze.linalg.VectorOps$$anon$178.apply(Vector.scala:391)
         at breeze.linalg.NumericOps$class.dot(NumericOps.scala:83)
         at breeze.linalg.DenseVector.dot(DenseVector.scala:47)
..................

2)  I find the imdb data are zero-based-index data
0 0:1 3:1 6208:1 8936:1 8959:1 16434:1 29840:1 29843:1 30274:1 32092:1 63727:1 109302:1 114311:1 114336:1 119637:1 121867:1 143744:1 145106:1 186951:1 216401:1 228548:1 248919:1 251691:1 294713:1 302316:1 307685:1 316421:1 316556:1 317062:1 321771:1 327174:1 364381:1 384514:1 404531:1 414947:1 434235:1 434250:1 462625:1 471013:1 503923:1 511725:1 514582:1 514635:1 519251:1 524274:1 540734:1 556018:1 559036:1 559037:1 559039:1 559341:1 609032:1 644534:1 650763:1 659114:1 666864:1 669778:1 669783:1 669787:1 673083:1

3) If change code ""val index = indexAndValue(0).toInt - 1"" to ""val index = indexAndValue(0).toInt - offset"" (offset equals 0 or 1 based on user's selection), then MLUtils.loadLibSVMFile will support both zero-based-index data and one-based-index data.
  That also means the interface of MLUtils.loadLibSVMFile will be changed


"
Sean Owen <sowen@cloudera.com>,"Tue, 8 Jul 2014 08:22:20 +0100","Re: Could the function MLUtils.loadLibSVMFile be modified to support
 zero-based-index data?","""dev@spark.apache.org"" <dev@spark.apache.org>","

Since the method is for parsing the LIBSVM format, and its labels are
always 1-indexed IIUC, I don't think it would make sense to read 0-indexed
labels. It sounds like that input is not properly formatted, unless anyone
knows to the contrary?
"
qingyang li <liqingyang1985@gmail.com>,"Tue, 8 Jul 2014 16:18:51 +0800","on shark, is tachyon less efficient than memory_only cache strategy ?",dev@spark.apache.org,"hi, when i create a table, i can point the cache strategy using shark.cache,
i think ""shark.cache=memory_only""  means data are managed by spark, and
data are in the same jvm with excutor;   while  ""shark.cache=tachyon""
 means  data are managed by tachyon which is off heap, and data are not in
the same jvm with excutor,  so spark will load data from tachyon for each
query sql , so,  is  tachyon less efficient than memory_only cache strategy
 ?
if yes, can we let spark load all data once from tachyon  for all sql query
 if i want to use tachyon cache strategy since tachyon is more HA than
memory_only ?
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 8 Jul 2014 04:43:40 -0700",Re: (send this email to subscribe),"""dev@spark.apache.org"" <dev@spark.apache.org>","See http://spark.apache.org/news/spark-mailing-lists-moving-to-apache.html

Cheers


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 8 Jul 2014 04:49:55 -0700",Re: (send this email to subscribe),"""dev@spark.apache.org"" <dev@spark.apache.org>","This is the correct page: http://spark.apache.org/community.html

Cheers


"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 8 Jul 2014 08:18:54 -0700","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"Tachyon should only be marginally less performant than memory_only, because
we mmap the data from Tachyon's ramdisk. We do not have to, say, transfer
the data over a pipe from Tachyon; we can directly read from the buffers in
the same way that Shark reads from its in-memory columnar format.




"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 8 Jul 2014 08:25:28 -0700","Re: Could the function MLUtils.loadLibSVMFile be modified to support
 zero-based-index data?","""dev@spark.apache.org"" <dev@spark.apache.org>","As Sean mentions, if you can change the data to the standard format, that's
probably a good idea. If you'd rather read the data raw, then writing your
own version of loadLibSVMFile - then you could make your own loader
function which is very similar to the existing one with a few characters
removed:

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala#L81

You will also likely need to change the logic where it determines the
number of features (currently line 95)



"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 8 Jul 2014 22:20:38 +0530","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"You are ignoring serde costs :-)

- Mridul


"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 8 Jul 2014 09:58:16 -0700","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"Shark's in-memory format is already serialized (it's compressed and
column-based).



"
Haoyuan Li <haoyuan.li@gmail.com>,"Tue, 8 Jul 2014 10:19:06 -0700","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"Yes. For Shark, two modes, ""shark.cache=tachyon"" and ""shark.cache=memory"",
have the same ser/de overhead. Shark loads data from outsize of the process
in Tachyon mode with the following benefits:


   - In-memory data sharing across multiple Shark instances (i.e. stronger
   isolation)
   - Instant recovery of in-memory tables
   - Reduce heap size => faster GC in shark
   - If the table is larger than the memory size, only the hot columns will
   be cached in memory

from http://tachyon-project.org/master/Running-Shark-on-Tachyon.html and
https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon

Haoyuan






-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
RJ Nowling <rnowling@gmail.com>,"Tue, 8 Jul 2014 13:54:00 -0400",Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"Hi all,

MLlib currently has one clustering algorithm implementation, KMeans.
It would benefit from having implementations of other clustering
algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
Clustering, and Affinity Propagation.

I recently submitted a PR [1] for a MiniBatch KMeans implementation,
and I saw an email on this list about interest in implementing Fuzzy
C-Means.

Based on Sean Owen's review of my MiniBatch KMeans code, it became
apparent that before I implement more clustering algorithms, it would
be useful to hammer out a framework to reduce code duplication and
implement a consistent API.

I'd like to gauge the interest and goals of the MLlib community:

1. Are you interested in having more clustering algorithms available?

2. Is the community interested in specifying a common framework?

Thanks!
RJ

[1] - https://github.com/apache/spark/pull/1248


-- 
em rnowling@gmail.com
c 954.496.2314

"
"""anishsneh@yahoo.co.in"" <anishsneh@yahoo.co.in>","Wed, 9 Jul 2014 03:24:29 +0800",Cloudera's Hive on Spark vs AmpLab's Shark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All

I read somewhere that Cloudera announced Hive on Spark, since AmpLab already have Shark. I was trying to understand is it rebranding of Shark or they are planning something new altogether.

Please suggest.

--
Anish Sneh
""Experience is the best teacher.""
http://in.linkedin.com/in/anishsneh

"
"""anishsneh@yahoo.co.in"" <anishsneh@yahoo.co.in>","Wed, 9 Jul 2014 03:27:12 +0800",Data Locality In Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All

My apologies for very basic question, do we have full support of data locality in Spark MapReduce.

Please suggest.

--
Anish Sneh
""Experience is the best teacher.""
http://in.linkedin.com/in/anishsneh

"
Reynold Xin <rxin@databricks.com>,"Tue, 8 Jul 2014 12:29:37 -0700",Re: Cloudera's Hive on Spark vs AmpLab's Shark,"""dev@spark.apache.org"" <dev@spark.apache.org>","This blog post probably clarifies a lot of things:
http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html





"
Hector Yee <hector.yee@gmail.com>,"Tue, 8 Jul 2014 13:01:19 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"I would say for bigdata applications the most useful would be hierarchical
k-means with back tracking and the ability to support k nearest centroids.






-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 8 Jul 2014 13:13:47 -0700",Re: Data Locality In Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Anish,

Spark, like MapReduce, makes an effort to schedule tasks on the same nodes
and racks that the input blocks reside on.

-Sandy



"
RJ Nowling <rnowling@gmail.com>,"Tue, 8 Jul 2014 16:19:09 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks, Hector! Your feedback is useful.




-- 
em rnowling@gmail.com
c 954.496.2314
"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Tue, 8 Jul 2014 13:24:04 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"Hector, could you share the references for hierarchical K-means? thanks.



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 8 Jul 2014 13:24:37 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","Having a common framework for clustering makes sense to me.  While we
should be careful about what algorithms we include, having solid
implementations of minibatch clustering and hierarchical clustering seems
like a worthwhile goal, and we should reuse as much code and APIs as
reasonable.



"
Hector Yee <hector.yee@gmail.com>,"Tue, 8 Jul 2014 13:31:53 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev <dev@spark.apache.org>,"No idea, never looked it up. Always just implemented it as doing k-means
again on each cluster.

FWIW standard k-means with euclidean distance has problems too with some
dimensionality reduction methods. Swapping out the distance metric with
negative dot or cosine may help.

Other more useful clustering would be hierarchical SVD. The reason why I
like hierarchical clustering is it makes for faster inference especially
over billions of users.






-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Tue, 8 Jul 2014 13:50:00 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"sure. more interesting problem here is choosing k at each level. Kernel
methods seem to be most promising.



"
RJ Nowling <rnowling@gmail.com>,"Tue, 8 Jul 2014 16:59:59 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"The scikit-learn implementation may be of interest:

http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.html#sklearn.cluster.Ward

It's a bottom up approach.  The pair of clusters for merging are
chosen to minimize variance.

Their code is under a BSD license so it can be used as a template.

Is something like that you were thinking Hector?




-- 
em rnowling@gmail.com
c 954.496.2314

"
Hector Yee <hector.yee@gmail.com>,"Tue, 8 Jul 2014 14:00:40 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"K doesn't matter much I've tried anything from 2^10 to 10^3 and the
performance
doesn't change much as measured by precision @ K. (see table 1
http://machinelearning.wustl.edu/mlpapers/papers/weston13). Although 10^3
kmeans did outperform 2^10 hierarchical SVD slightly in terms of the
metrics, 2^10 SVD was much faster in terms of inference time.

I found the thing that affected performance most was adding in back
tracking to fix mistakes made at higher levels rather than how the K is
picked per level.







-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Hector Yee <hector.yee@gmail.com>,"Tue, 8 Jul 2014 14:06:17 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev <dev@spark.apache.org>,"No was thinking more top-down:

assuming a distributed kmeans system already existing, recursively apply
the kmeans algorithm on data already partitioned by the previous level of
kmeans.

I haven't been much of a fan of bottom up approaches like HAC mainly
because they assume there is already a distance metric for items to items.
This makes it hard to cluster new content. The distances between sibling
clusters is also hard to compute (if you have thrown away the similarity
matrix), do you count paths to same parent node if you are computing
distances between items in two adjacent nodes for example. It is also a bit
harder to distribute the computation for bottom up approaches as you have
to already find the nearest neighbor to an item to begin the process.






-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 8 Jul 2014 14:12:37 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","If you're thinking along these lines, have a look at the DecisionTree
implementation in MLlib. It uses the same idea and is optimized to prevent
multiple passes over the data by computing several splits at each level of
tree building. The tradeoff is increased model state and computation per
pass over the data, but fewer total passes and hopefully lower
communication overheads than, say, shuffling data around that belongs to
one cluster or another. Something like that could work here as well.

I'm not super-familiar with hierarchical K-Means so perhaps there's a more
efficient way to implement it, though.



"
Hector Yee <hector.yee@gmail.com>,"Tue, 8 Jul 2014 14:44:00 -0700",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev <dev@spark.apache.org>,"Yeah if one were to replace the objective function in decision tree with
minimizing the variance of the leaf nodes it would be a hierarchical
clusterer.






-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Will Benton <willb@redhat.com>,"Tue, 8 Jul 2014 22:58:11 -0400 (EDT)",odd test suite failures while adding functions to Catalyst,dev@spark.apache.org,"Hi all,

I was testing an addition to Catalyst today (reimplementing a Hive UDF) and ran into some odd failures in the test suite.  In particular, it seems that what most of these have in common is that an array is spuriously reversed somewhere.  For example, the stddev tests in the HiveCompatibilitySuite all failed this way (note reversed synonyms list; note also that stddev isn't the function I reimplemented):

   [info] - udf_std *** FAILED ***
   [info]   Results do not match for udf_std:
   [info]   DESCRIBE FUNCTION EXTENDED std
   [info]   == Logical Plan ==
   [info]   NativeCommand DESCRIBE FUNCTION EXTENDED std
   [info]   
   [info]   == Optimized Logical Plan ==
   [info]   NativeCommand DESCRIBE FUNCTION EXTENDED std
   [info]   
   [info]   == Physical Plan ==
   [info]   NativeCommand DESCRIBE FUNCTION EXTENDED std, [result#38637:0]
   [info]   result
   [info]   !== HIVE - 2 row(s) ==                                         == CATALYST - 2 row(s) ==
   [info]    std(x) - Returns the standard deviation of a set of numbers   std(x) - Returns the standard deviation of a set of numbers
   [info]   !Synonyms: stddev_pop, stddev                                  Synonyms: stddev, stddev_pop (HiveComparisonTest.scala:372)

I also saw a reversed array (relative to the expected array) of Hive settings in HiveQuerySuite.

I'll probably be able to track down where things are going wrong after getting away from my desk for a bit, but I thought I'd send it out to the dev list in case this looks familiar to anyone.  Has anyone seen this kind of failure before?



thanks,
wb

"
Usman Ghani <usman@platfora.com>,"Tue, 8 Jul 2014 21:57:23 -0700",Not starting the Web ui in the driver,dev@spark.apache.org,"Is there a way to run the spark driver program without starting the
monitoring web UI in-process? I didn't see any config setting around it.
"
Konstantin Boudnik <cos@apache.org>,"Tue, 8 Jul 2014 22:54:24 -0700",Meetup invitation: Consensus based replication in Hadoop,dev@spark.apache.org,"[cross-posted from hdfs-dev@hadoop, common-dev@hadoop]

We'd like to invite you to the 
    Consensus based replication in Hadoop: A deep dive
event that we are happy to hold in our San Ramon office on July 15th at noon.
We'd like to accommodate as many people as possible, but I think are physically
limited to 30 (+/- a few), so please RSVP to this Eventbrite invitation:

https://www.eventbrite.co.uk/e/consensus-based-replication-in-hadoop-a-deep-dive-tickets-12158236613

We'll provide pizza and beverages (feel free to express your special dietary
requirements if any).

See you soon!
With regards,
  Cos


"
MEETHU MATHEW <meethu2006@yahoo.co.in>,"Wed, 9 Jul 2014 19:05:22 +0800",Contribution to MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am interested in contributing a clustering algorithm towards MLlib of Spark.I am focusing on Gaussian Mixture Model.
But I saw a JIRA @ https://spark-project.atlassian.net/browse/SPARK-952 regrading the same.I would like to know whetherGaussian Mixture Model isalready implemented or not.



Thanks & Regards, 
Meethu M"
RJ Nowling <rnowling@gmail.com>,"Wed, 9 Jul 2014 07:56:22 -0400",Re: Contribution to MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>, MEETHU MATHEW <meethu2006@yahoo.co.in>","Hi Meethu,

There is no code for a Gaussian Mixture Model clustering algorithm in the
repository, but I don't know if anyone is working on it.

RJ





-- 
em rnowling@gmail.com
c 954.496.2314
"
RJ Nowling <rnowling@gmail.com>,"Wed, 9 Jul 2014 08:15:20 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks everyone for the input.

So it seems what people want is:

* Implement MiniBatch KMeans and Hierarchical KMeans (Divide and
conquer approach, look at DecisionTree implementation as a reference)
* Restructure 3 Kmeans clustering algorithm implementations to prevent
code duplication and conform to a consistent API where possible

If this is correct, I'll start work on that.  How would it be best to
structure it? Should I submit separate JIRAs / PRs for refactoring of
current code, MiniBatch KMeans, and Hierarchical or keep my current
JIRA and PR for MiniBatch KMeans open and submit a second JIRA and PR
for Hierarchical KMeans that builds on top of that?

Thanks!





-- 
em rnowling@gmail.com
c 954.496.2314

"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Wed, 09 Jul 2014 05:39:28 -0700 (PDT)",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"Cool seems like a god initiative. Adding a couple extra high quality clustering implantations will be great.


I'd say it would make most sense to submit a PR for the Standardised API first, agree that with everyone and then build on it for the specific implementations.
—
Sent from Mailbox


with
prevent
 of
per
to
more
apply
level of
sibling
similarity
 a
have

html#sklearn.cluster.Ward
com>
com>
doing
with
metric
reason
com
K-means?
com>
be
nearest
com
implementation,
implementing
it
algorithms,
duplication
algorithms"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 9 Jul 2014 22:47:22 +0530",Unresponsive to PR/jira changes,dev@spark.apache.org,"Hi,


  I noticed today that gmail has been marking most of the mails from
spark github/jira I was receiving to spam folder; and I was assuming
it was lull in activity due to spark summit for past few weeks !

In case I have commented on specific PR/JIRA issues and not followed
up, apologies for the same - please do reach out in case it is still
pending something from my end.



Regards,
Mridul

"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 9 Jul 2014 10:32:19 -0700",Re: Contribution to MLlib,dev@spark.apache.org,"I don't know if anyone is working on it either. If that JIRA is not
moved to Apache JIRA, feel free to create a new one and make a note
that you are working on it. Thanks! -Xiangrui


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Wed, 9 Jul 2014 11:43:26 -0700",15 new MLlib algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","At Spark Summit, Patrick Wendell indicated the number of MLlib algorithms would ""roughly double"" in 1.1 from the current approx. 15.
http://spark-summit.org/wp-content/uploads/2014/07/Future-of-Spark-Patrick-Wendell.pdf

What are the planned additional algorithms?

In Jira, I only see two when filtering on version 1.1, component MLlib: one on multi-label and another on high dimensionality.

https://issues.apache.org/jira/browse/SPARK-2329?jql=issuetype%20in%20(Brainstorming%2C%20Epic%2C%20%22New%20Feature%22%2C%20Story)%20AND%20fixVersion%20%3D%201.1.0%20AND%20component%20%3D%20MLlib

http://tinyurl.com/ku7sehu

"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Wed, 9 Jul 2014 12:23:17 -0700",CPU/Disk/network performance instrumentation,dev@spark.apache.org,"Hi all,

I've been doing a bunch of performance measurement of Spark and, as part of
doing this, added metrics that record the average CPU utilization, disk
throughput and utilization for each block device, and network throughput
while each task is running.  These metrics are collected by reading the
/proc filesystem so work only on Linux.  I'm happy to submit a pull request
with the appropriate changes but first wanted to see if sufficiently many
people think this would be useful.  I know the metrics reported by Spark
(and in the UI) are already overwhelming to some folks so don't want to add
more instrumentation if it's not widely useful.

These metrics are slightly more difficult to interpret for Spark than
similar metrics reported by Hadoop because, with Spark, multiple tasks run
in the same JVM and therefore as part of the same process.  This means
that, for example, the CPU utilization metrics reflect the CPU use across
all tasks in the JVM, rather than only the CPU time used by the particular
task.  This is a pro and a con -- it makes it harder to determine why
utilization is high (it may be from a different task) but it also makes the
metrics useful for diagnosing straggler problems.  Just wanted to clarify
this before asking folks to weigh in on whether the added metrics would be
useful.

-Kay

(if you're curious, the instrumentation code is on a very messy branch
here:
https://github.com/kayousterhout/spark-1/tree/proc_logging_perf_minimal_temp/core/src/main/scala/org/apache/spark/performance_logging
)
"
Reynold Xin <rxin@databricks.com>,"Wed, 9 Jul 2014 12:25:16 -0700",Re: CPU/Disk/network performance instrumentation,"""dev@spark.apache.org"" <dev@spark.apache.org>","Maybe it's time to create an advanced mode in the ui.



"
Burak Yavuz <byavuz@stanford.edu>,"Wed, 9 Jul 2014 12:31:29 -0700 (PDT)",Re: 15 new MLlib algorithms,"dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>","Hi,

The roadmap for the 1.1 release and MLLib includes algorithms such as:

Non-negative matrix factorization, Sparse SVD, Multiclass 
decision tree, Random Forests (?)

and optimizers such as:
ADMM, Accelerated gradient methods

also a statistical toolbox that includes:
descriptive statistics, sampling, hypothesis testing

and hopefully Parallel model training for autotuning.

Source:
https://databricks-training.s3.amazonaws.com/slides/Spark_Summit_MLlib_070214_v2.pdf

Best,
Burak



http://spark-summit.org/wp-content/uploads/2014/07/Future-of-Spark-Patrick-Wendell.pdf

What are the planned additional algorithms?

In Jira, I only see two when filtering on version 1.1, component MLlib: one on multi-label and another on high dimensionality.

https://issues.apache.org/jira/browse/SPARK-2329?jql=issuetype%20in%20(Brainstorming%2C%20Epic%2C%20%22New%20Feature%22%2C%20Story)%20AND%20fixVersion%20%3D%201.1.0%20AND%20component%20%3D%20MLlib

http://tinyurl.com/ku7sehu


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 9 Jul 2014 14:17:51 -0700",Re: CPU/Disk/network performance instrumentation,dev@spark.apache.org,"I think it would be very useful to have this. We could put the ui display
either behind a flag or a url parameter

Shivaram



"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 10 Jul 2014 02:50:10 +0530",Re: CPU/Disk/network performance instrumentation,dev@spark.apache.org,"+1 on advanced mode !

Regards.
Mridul


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 9 Jul 2014 14:36:43 -0700",ExecutorState.LOADING?,dev@spark.apache.org,"Doesn't look to me like this is used.  Does anybody recall what it was
intended for?
"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Wed, 9 Jul 2014 17:44:01 -0400",Re: CPU/Disk/network performance instrumentation,"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 on advanced tab.







-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.vel"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Wed, 9 Jul 2014 14:49:07 -0700",Re: ExecutorState.LOADING?,dev@spark.apache.org,"Git history to the rescue!  It seems to have been added by Matei way back
in July 2012:
https://github.com/apache/spark/commit/5d1a887bed8423bd6c25660910d18d91880e01fe

and then was removed a few months later (replaced by RUNNING) by the same
Mr. Zaharia:
https://github.com/apache/spark/commit/bb1bce79240da22c2677d9f8159683cdf73158c2#diff-776a630ac2b2ec5fe85c07ca20a58fc0

So I'd say it's safe to delete it.



"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 9 Jul 2014 15:08:05 -0700",Re: ExecutorState.LOADING?,dev@spark.apache.org,"Actually, I'm thinking about re-purposing it.  There's a nasty behavior
that I'll open a JIRA for soon, and that I'm thinking about addressing by
introducing/using another ExecutorState transition.  The basic problem is
that Master can be overly aggressive in calling removeApplication on
ExecutorStateChanged.  For example, say you have a working, long-running
Spark stand-alone-mode application and then try to add some more worker
nodes, but manage to misconfigure the new nodes so that on the new nodes
Executors never successfully start.  In that scenario, you will repeatedly
end up in the !normalExit branch of Master's receive ExecutorStateChanged,
quickly exceed ApplicationState.MAX_NUM_RETRY (a non-configurable 10, which
is another irritation), and end up having your application killed off even
though it is still running successfully on the old worker nodes.




"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 9 Jul 2014 15:50:19 -0700",Re: ExecutorState.LOADING?,dev@spark.apache.org,"Agreed that the behavior of the Master killing off an Application when
Executors from the same set of nodes repeatedly die is silly. This can also
strike if a single node enters a state where any Executor created on it
quickly dies (e.g., a block device becomes faulty). This prevents the
Application from launching despite only one node being bad.



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 9 Jul 2014 19:31:29 -0700",Testing period for better jenkins integration,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just a heads up - I've added some better Jenkins integration that
posts more useful messages on pull requests. We'll run this
side-by-side with the current Jenkins messages for a while to make
sure it's working well. Things may be a bit chatty while we are
testing this - we can migrate over as soon as we feel it's stable.

- Patrick

"
Taka Shinagawa <taka.epsilon@gmail.com>,"Wed, 9 Jul 2014 19:35:02 -0700",libgfortran Dependency,dev@spark.apache.org,"Hi,

After testing Spark 1.0.1-RC2 on EC2 instances from the standard Ubuntu and
Amazon Linux AMIs,
I've noticed the MLlib's dependancy on gfortran library (libgfortran.so.3).

""sbt assembly"" succeeds without this library installed, but ""sbt test""
fails as follows.

I'm wondering if documenting this dependency in README and online doc might
a good idea.

-------------
[info] ALSSuite:
-- org.jblas ERROR Couldn't load copied link file:
java.lang.UnsatisfiedLinkError:
/tmp/jblas8312335435391185287libjblas_arch_flavor.so: libgfortran.so.3:
cannot open shared object file: No such file or directory.

You need to install libgfortran3.

For example for debian or Ubuntu, type ""sudo apt-get install libgfortran3""

For more information, see
https://github.com/mikiobraun/jblas/wiki/Missing-Libraries
[info] Exception encountered when attempting to run a suite with class
name: org.apache.spark.mllib.recommendation.ALSSuite *** ABORTED ***
[info]   java.lang.UnsatisfiedLinkError:
org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V
[info]   at org.jblas.NativeBlas.dgemm(Native Method)
[info]   at org.jblas.SimpleBlas.gemm(SimpleBlas.java:251)
[info]   at org.jblas.DoubleMatrix.mmuli(DoubleMatrix.java:1697)
[info]   at org.jblas.DoubleMatrix.mmul(DoubleMatrix.java:3054)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$.generateRatings(ALSSuite.scala:67)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite.testALS(ALSSuite.scala:167)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply$mcV$sp(ALSSuite.scala:83)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
[info]   ...

-------------
"
qingyang li <liqingyang1985@gmail.com>,"Thu, 10 Jul 2014 10:40:40 +0800","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"could i set some cache policy to let spark load data from tachyon only one
time for all sql query?  for example by using CacheAllPolicy
FIFOCachePolicy LRUCachePolicy.  But I have tried that three policy, they
are not useful.
I think , if spark always load data for each sql query,  it will impact the
query speed , it will take more time than the case that data are managed by
spark itself.




2014-07-09 1:19 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:

"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 9 Jul 2014 19:45:26 -0700",Re: libgfortran Dependency,dev@spark.apache.org,"It is documented in the official doc:
http://spark.apache.org/docs/latest/mllib-guide.html


"
Taka Shinagawa <taka.epsilon@gmail.com>,"Wed, 9 Jul 2014 19:57:35 -0700",Re: libgfortran Dependency,dev@spark.apache.org,"Thanks for point me to the MLlib guide. I was looking at only README and
Spark docs.

Also found it's already filed in JIRA
https://spark-project.atlassian.net/browse/SPARK-797



"
"""Liu, Raymond"" <raymond.liu@intel.com>","Thu, 10 Jul 2014 07:21:00 +0000",RE: MIMA Compatiblity Checks,"""dev@spark.apache.org"" <dev@spark.apache.org>","so how to run the check locally?

 reported. Do we need to modify SparkBuilder.scala etc to run it locally? Could not figure out how Jekins run the check on its console outputs.


Best Regards,
Raymond Liu


Some people may have noticed PR failures due to binary compatibility checks. We've had these enabled in several of the sub-modules since the 0.9.0 release but we've turned them on in Spark core post 1.0.0 which has much higher churn.

The checks are based on the ""migration manager"" tool from Typesafe.
ses or methods. Prashant Sharma has built instrumentation that adds partial support for package-privacy (via a workaround) but since there isn't really native support for this in MIMA we are still finding cases in which we trigger false positives.

In the next week or two we'll make it a priority to handle more of these false-positive cases. In the mean time users can add manual excludes to:

project/MimaExcludes.scala

to avoid triggering warnings for certain issues.

This is definitely annoying - sorry about that. Unfortunately we are the first open source Scala project to ever do this, so we are dealing with uncharted territory.

Longer term I'd actually like to see us just write our own sbt-based tool to do this in a better way (we've had trouble trying to extend MIMA itself, it e.g. has copy-pasted code in it from an old version of the scala compiler). If someone in the community is a Scala fan and wants to take that on, I'm happy to give more details.

- Patrick

"
Reynold Xin <rxin@databricks.com>,"Thu, 10 Jul 2014 00:33:09 -0700",Re: MIMA Compatiblity Checks,"""dev@spark.apache.org"" <dev@spark.apache.org>","You can take a look at

https://github.com/apache/spark/blob/master/dev/run-tests

dev/mima



"
qingyang li <liqingyang1985@gmail.com>,"Thu, 10 Jul 2014 17:51:32 +0800","when insert data into one table which is on tachyon, how can i
 control the data position?",dev@spark.apache.org,"when insert data (the data is small, it will not be partitioned
automatically)into one table which is on tachyon, how can i control the
data position,  i mean how can i point which machine the data should exist
on?
if we can not control, what is the data assign strategy of tachyon or spark?
"
RJ Nowling <rnowling@gmail.com>,"Thu, 10 Jul 2014 10:24:45 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","I went ahead and created JIRAs.

JIRA for Hierarchical Clustering:
https://issues.apache.org/jira/browse/SPARK-2429

JIRA for Standarized Clustering APIs:
https://issues.apache.org/jira/browse/SPARK-2430

Before submitting a PR for the standardized API, I want to implement a
few clustering algorithms for myself to get a good ""feel"" for how to
structure such an API.  If others with more experience want to dive
into designing the API, though, that would allow us to get moving more
quickly.


rote:
tering implantations will be great.
first, agree that with everyone and then build on it for the specific implementations.
h
vent
l of
er
to
more
e:
pply
el of
ling
rity
o a
 have
.
e:
html#sklearn.cluster.Ward
m>
g
with
ric
son
.com
ns?
com>
be
rest
.com
on,
nting
it
ms,
ation
s



-- 
em rnowling@gmail.com
c 954.496.2314

"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 10 Jul 2014 07:47:54 -0700 (PDT)",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.apache.org,"Might be worth checking out scikit-learn and mahout to get some broad ideas—
Sent from Mailbox


clustering implantations will be great.
 first, agree that with everyone and then build on it for the specific implementations.
of
with
com>
DecisionTree
prevent
level of
per
 to
 more
apply
level of
mainly
to
sibling
similarity
computing
also a
you have
process.
.html#sklearn.cluster.Ward
.
com>
.
com>
doing
 with
metric
reason
7@gmail.com
K-means?
.com>
 be
nearest
<rnowling@gmail.com
implementation,
implementing
 it
algorithms,
duplication
algorithms"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 10 Jul 2014 17:38:33 +0000",Feature selection interface,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I've implemented a class that does Chi-squared feature selection for RDD[LabeledPoint]. It also computes basic class/feature occurrence statistics and other methods like mutual information or information gain can be easily implemented. I would like to make a pull request. However, MLlib master branch doesn't have any feature selection methods implemented. So, I need to create a proper interface that my class will extend or mix. It should be easy to use from developers and users prospective.

I was thinking that there should be FeatureEvaluator that for each feature from RDD[LabeledPoint] returns RDD[((featureIndex: Int, label: Double), value: Double)].
Then there should be FeatureSelector that selects top N features or top N features group by class etc.
And the simplest one, FeatureFilter that filters the data based on set of feature indices.

Additionally, there should be the interface for FeatureEvaluators that don't use class labels, i.e. for RDD[Vector].

I am concerned that such design looks rather ""disconnected"" because there are 3 disconnected objects.

As a result of use, I would like to see something like ""val filteredData = Filter(data, ChiSquared(data).selectTop(100))"".

Any ideas or suggestions?

Best regards, Alexander
"
Patrick Wendell <patrick@databricks.com>,"Thu, 10 Jul 2014 11:15:41 -0700",Changes to sbt build have been merged,dev@spark.apache.org,"Just a heads up, we merged Prashant's work on having the sbt build read all
dependencies from Maven. Please report any issues you find on the dev list
or on JIRA.

configuration style as the maven build (-D for options and -P for maven
profiles). So this will be a change for developers:

sbt/sbt -Dhadoop.version=2.2.0 -Pyarn assembly

For now, we'll continue to support the old env-var options with a
deprecation warning.

- Patrick
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 10 Jul 2014 13:29:51 -0700",Re: Changes to sbt build have been merged,"""dev@spark.apache.org"" <dev@spark.apache.org>","Woot!



"
yao <yaoshengzhe@gmail.com>,"Thu, 10 Jul 2014 13:32:40 -0700",Re: Changes to sbt build have been merged,dev@spark.apache.org,"Cool~



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 10 Jul 2014 18:05:48 -0400",EC2 clusters ready in launch time + 30 seconds,dev <dev@spark.apache.org>,"Hi devs!

Right now it takes a non-trivial amount of time to launch EC2 clusters.
Part of this time is spent starting the EC2 instances, which is out of our
control. Another part of this time is spent installing stuff on and
configuring the instances. This, we can control.

I’d like to explore approaches to upgrading spark-ec2 so that launching a
cluster of any size generally takes only 30 seconds on top of the time to
launch the base EC2 instances. Since Amazon can launch instances
concurrently, I believe this means we should be able to launch a fully
operational Spark cluster of any size in constant time. Is that correct?

Do we already have an idea of what it would take to get to that point?

Nick
​
"
"""Ian O'Connell"" <ianoconnell@gmail.com>","Thu, 10 Jul 2014 15:15:44 -0700",sparkSQL thread safe?,dev@spark.apache.org,"Had a few quick questions...

Just wondering if right now spark sql is expected to be thread safe on
master?

doing a simple hadoop file -> RDD -> schema RDD -> write parquet

will fail in reflection code if i run these in a thread pool.

The SparkSqlSerializer, seems to create a new Kryo instance each time it
wants to serialize anything. I got a huge speedup when I had any
non-primitive type in my SchemaRDD using the ResourcePool's from Chill for
providing the KryoSerializer to it. (I can open an RB if there is some
reason not to re-use them?)

====

With the Distinct Count operator there is no map-side operations, and a
test to check for this. Is there any reason not to do a map side combine
into a set and then merge the sets later? (similar to the approximate
distinct count operator)

===

Another thing while i'm mailing.. the 1.0.1 docs have a section like:
""
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To
work around this limit, // you can use custom classes that implement the
Product interface.
""

Which sounds great, we have lots of data in thrift.. so via scrooge (
https://github.com/twitter/scrooge), we end up with ultimately instances of
traits which implement product. Though the reflection code appears to look
for the constructor of the class and base the types based on those
parameters?


Ian.
"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Jul 2014 16:50:56 -0700",Re: sparkSQL thread safe?,dev@spark.apache.org,"Hey Ian,

Thanks for bringing these up!  Responses in-line:

Just wondering if right now spark sql is expected to be thread safe on

You are probably hitting SPARK-2178
<https://issues.apache.org/jira/browse/SPARK-2178> which is caused by
SI-6240 <https://issues.scala-lang.org/browse/SI-6240>.  We have a plan to
fix this by moving the schema introspection to compile time, using macros.



Sounds like SPARK-2102 <https://issues.apache.org/jira/browse/SPARK-2102>.
 There is no reason AFAIK to not reuse the instance. A PR would be greatly
appreciated!



Thats just not an optimization that we had implemented yet... but I've just
done it here <https://github.com/apache/spark/pull/1366> and it'll be in
master soon :)




Yeah, thats true that we only look in the constructor at the moment, but I
don't think there is a really good reason for that (other than I guess we
will need to add code to make sure we skip builtin object methods).  If you
want to open a JIRA, we can try fixing this.

Michael
"
"""Nate D'Amico"" <nate@reactor8.com>","Thu, 10 Jul 2014 17:10:15 -0700",RE: EC2 clusters ready in launch time + 30 seconds,<dev@spark.apache.org>,"You are partially correct.

It's not terribly complex, but also not easy to accomplish.  Sounds like you want to manage some partially/fully baked AMI's with the core spark libs and dependencies already on the image.  Main issues that crop up are:

1) image sprawl, as libs/config/defaults/etc change, images need to be ""rebuilt"" and references updated
2) cross region support (not too huge deal now with copy functionality, just more complex image mgmt.)

If you don’t want to restrict which instance types/sizes one can use, you also have uptick in image mgmt. complexity with:

3) instance type (need both standard and hvm)

Starting to work through some automation/config stuff for spark stack on EC2 with a project, will be focusing the work through the apache bigtop effort to start, can then share with spark community directly as things progress if people are interested

Nate



Right now it takes a non-trivial amount of time to launch EC2 clusters.
Part of this time is spent starting the EC2 instances, which is out of our control. Another part of this time is spent installing stuff on and configuring the instances. This, we can control.

I’d like to explore approaches to upgrading spark-ec2 so that launching a cluster of any size generally takes only 30 seconds on top of the time to launch the base EC2 instances. Since Amazon can launch instances concurrently, I believe this means we should be able to launch a fully operational Spark cluster of any size in constant time. Is that correct?

Do we already have an idea of what it would take to get to that point?

Nick
​


"
Gary Malouf <malouf.gary@gmail.com>,"Thu, 10 Jul 2014 21:28:12 -0400",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"-1 I honestly do not know the voting rules for the Spark community, so
please excuse me if I am out of line or if Mesos compatibility is not a
concern at this point.

We just tried to run this version built against 2.3.0-cdh5.0.2 on mesos
0.18.2.  All of "
Gary Malouf <malouf.gary@gmail.com>,"Thu, 10 Jul 2014 21:36:29 -0400",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"Just realized the deadline was Monday, my apologies.  The issue
nevertheless stands.



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 10 Jul 2014 18:40:15 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Gary,

The vote technically doesn't close until I send the vote summary
e-mail, but I was planning to close and package this tonight. It's too
bad if there is a regression, it might be worth holding the release
but it really requires narrowing down the issue to get more
information about the scope and severity. Could you fork another
thread for this?

- Patrick


"
davies <davies.liu@gmail.com>,"Thu, 10 Jul 2014 23:17:48 -0700 (PDT)",Re: PySpark Driver from Jython,dev@spark.incubator.apache.org,"The function run in worker is serialized in driver, so the driver and worker
should be run in the same Python interpreter.

If you do not need c extension support, then Jython will be better than
CPython, because of the cost of serialization is much lower.

Davies



--

"
kingfly <wangfei1@huawei.com>,"Fri, 11 Jul 2014 14:29:56 +0800",what is the difference between org.spark-project.hive and org.apache.hadoop.hive,<dev@spark.apache.org>,"
-- 

Best Regards
Frank Wang | Software Engineer

Mobile: +86 18505816792
Phone: +86 571 63547
Fax:
Email: wangfei1@huawei.com
--------------------------------------------------------------------------------
Huawei Technologies Co., Ltd.
Hangzhou R&D Center
NO.410, JiangHong Road, Binjiang Area, Hangzhou, 310052, P. R. China



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 10 Jul 2014 23:50:57 -0700",Re: what is the difference between org.spark-project.hive and org.apache.hadoop.hive,"""dev@spark.apache.org"" <dev@spark.apache.org>","There are two differences:

1. We publish hive with a shaded protobuf dependency to avoid
conflicts with some Hadoop versions.
2. We publish a proper hive-exec jar that only includes hive packages.
The upstream version of hive-exec bundles a bunch of other random
dependencies in it which makes it really hard for third-party projects
to use it.


"
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 11 Jul 2014 16:50:26 +0400",How pySpark works?,dev@spark.apache.org,"Hi, I want to use pySpark, but can't understand how it works. Documentation
doesn't provide enough information.

1) How python shipped to cluster? Should machines in cluster already have
python?
2) What happens when I write some python code in ""map"" function - is it
shipped to cluster and just executed on it? How it understand all
dependencies, which my code need and ship it there? If I use Math in my
code in ""map"" does it mean, that I would ship Math class or some python
Math on cluster would be used?
3) I have c++ compiled code. Can I ship this executable with ""addPyFile""
and just use ""exec"" function from python? Would it work?

-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 11 Jul 2014 17:02:57 +0400",Random forest - is it under implementation?,dev@spark.apache.org,"Hi, I have intern, who wants to implement some ML algorithm for spark.
Which algorithm would be good idea to implement(it should be not very
difficult)? I heard someone already working on random forest, but couldn't
find proof of that.

I'm aware of new politics, where we should implement stable, good quality,
popular ML or do not do it at all.

-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Chester At Work <chester@alpinenow.com>,"Fri, 11 Jul 2014 06:43:34 -0700",Re: Random forest - is it under implementation?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Sung chung from alpine data labs presented the random Forrest implementation at Spark summit 2014. The work will be open sourced and contributed back to MLLib.

Stay tuned 



Sent from my iPad





"
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 11 Jul 2014 17:44:07 +0400",Re: Random forest - is it under implementation?,dev@spark.apache.org,"Great. Then one question left:
what would you recommend for implementation?



2014-07-11 17:43 GMT+04:00 Chester At Work <chester@alpinenow.com>:




-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Jai Kumar Singh <flukebox@flukebox.in>,"Fri, 11 Jul 2014 17:34:59 +0530",Calling Scala/Java methods which operates on RDD,dev@spark.apache.org,"HI,
  I want to write some common utility function in Scala and want to call
the same from Java/Python Spark API ( may be add some wrapper code around
scala calls). Calling Scala functions from Java works fine. I was reading
pyspark rdd code and find out that pyspark is able to call JavaRDD function
like union/zip to get same for pyspark RDD and deserializing the output and
everything works fine. But somehow I am
not able to work out really simple example. I think I am missing some
serialization/deserialization.

Can someone confirm that is it even possible to do so? Or, would it be much
easier to pass RDD data files around instead of RDD directly (from pyspark
to java/scala)?

For example, below code just add 1 to each element of RDD containing
Integers.

package flukebox.test;

object TestClass{

def testFunc(data:RDD[Int])={

  data.map(x => x+1)

}

}

Calling from python,

from pyspark import RDD

from py4j.java_gateway import java_import

java_import(sc._gateway.jvm, ""flukebox.test"")


data = sc.parallelize([1,2,3,4,5,6,7,8,9])

sc._jvm.flukebox.test.TestClass.testFunc(data._jrdd.rdd())


*This fails because testFunc get any RDD of type Byte Array.*


Any help/pointer would be highly appreciated.


Thanks & Regards,

Jai K Singh
"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 11 Jul 2014 12:08:07 -0400",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"Unless you can diagnose the problem quickly, Gary, I think we need to go ahead with this release as is. This release didn't touch the Mesos support as far as I know, so the problem might be a nondeterministic issue with your application. But on the other hand the release does fix some critical bugs that affect all users. We can always do 1.0.2 later if we discover a problem.

Matei


so
a
mesos
indefinitely.
way
<tgraves_cs@yahoo.com.invalid>
<pwendell@gmail.com>
version
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
at:
https://repository.apache.org/content/repositories/orgapachespark-1021/
Some
frame size.
make


"
Gary Malouf <malouf.gary@gmail.com>,"Fri, 11 Jul 2014 12:31:24 -0400",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),dev@spark.apache.org,"Hi Matei,

We have not had time to re-deploy the rc today, but one thing that jumps
out is the shrinking of the default akka frame size from 10MB to around
128KB by default.  That is my first suspicion for our issue - could imagine
that biting others as well.

I'll try to re-test that today - either way, understand moving forward at
this point.

Gary



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 11 Jul 2014 09:37:45 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Gary,

Why do you think the akka frame size changed? It didn't change - we
added some fixes for cases where users were setting non-default
values.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 11 Jul 2014 09:46:09 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Okay just FYI - I'm closing this vote since many people are waiting on
the release and I was hoping to package it today. If we find a
reproducible Mesos issue here, we can definitely spin the fix into a
subsequent release.




"
Patrick Wendell <pwendell@gmail.com>,"Fri, 11 Jul 2014 09:48:05 -0700",[RESULT] [VOTE] Release Apache Spark 1.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote has passed with 9 +1 votes (5 binding) and 1 -1 vote (0 binding).

+1:
Patrick Wendell*
Mark Hamstra*
DB Tsai
Krishna Sankar
Soren Macbeth
Andrew Or
Matei Zaharia*
Xiangrui Meng*
Tom Graves*

0:

-1:
Gary Malouf

"
Andrew Or <andrew@databricks.com>,"Fri, 11 Jul 2014 10:29:15 -0700",Re: How pySpark works?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Egor,

Here are a few answers to your questions:

1) Python needs to be installed on all machines, but not pyspark. The way
the executors get the pyspark code depends on which cluster manager you
use. In standalone mode, your executors need to have the actual python
files in their working directory. In yarn mode, python files are included
in the assembly jar, which is then shipped to your executor containers
through a distributed cache.

2) Pyspark is just a thin wrapper around Spark. When you write a closure in
python, it is shipped to the executors within the task itself the same way
scala closures are shipped. If you use a special library, then all of the
nodes will need to have that library pre-installed.

3) Are you trying to run your c++ code inside the ""map"" function? If so,
you need to make sure the compiled code is present in the working directory
on all the executors before-hand for python to ""exec"" it. I haven't done
this before, but maybe there are a few gotchas in doing this.

Maybe others can add more information?

Andrew


2014-07-11 5:50 GMT-07:00 Egor Pahomov <pahomov.egor@gmail.com>:

"
Reynold Xin <rxin@databricks.com>,"Fri, 11 Jul 2014 10:52:23 -0700",Re: How pySpark works?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Also take a look at this:
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals



"
Kan Zhang <kzhang@apache.org>,"Fri, 11 Jul 2014 15:40:08 -0700",Re: Calling Scala/Java methods which operates on RDD,dev@spark.apache.org,"Hi Jai,

Your suspicion is correct. In general, Python RDDs are pickled into byte
arrays and stored in Java land as RDDs of byte arrays. union/zip operates
on byte arrays directly without deserializing. Currently, Python byte
arrays only get unpickled into Java objects in special cases, like SQL
functions or saving to Sequence Files (upcoming).

Hope it helps.

Kan



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 11 Jul 2014 18:35:18 -0700",Announcing Spark 1.0.1,"""dev@spark.apache.org"" <dev@spark.apache.org>, user@spark.apache.org","I am happy to announce the availability of Spark 1.0.1! This release
includes contributions from 70 developers. Spark 1.0.0 includes fixes
across several areas of Spark, including the core API, PySpark, and
MLlib. It also includes new features in Spark's (alpha) SQL library,
including support for JSON data and performance and stability fixes.

Visit the release notes[1] to read about this release or download[2]
the release today.

[1] http://spark.apache.org/releases/spark-release-1-0-1.html
[2] http://spark.apache.org/downloads.html

"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 11 Jul 2014 20:44:15 -0700",Re: Announcing Spark 1.0.1,"""user@spark.apache.org"" <user@spark.apache.org>","Congrats to the Spark community !


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Sat, 12 Jul 2014 07:17:21 -0700",Miss-link for the document of Spark 1.0.1,dev@spark.apache.org,"Hi developpers

Conguraturations on the release of Spark 1.0.1!

I found a miss-link for the document of Spark 1.0.1.

I checked out the overview from the link
http://spark.apache.org/docs/latest/ but
this link points to the page for 1.0.0.

I also visited the link http://spark.apache.org/docs/latest/index.html
(added ""index.html"") and I could accessed the documet for 1.0.1.

Best Regards,
Kousuke



"
Sean Owen <sowen@cloudera.com>,"Sat, 12 Jul 2014 15:22:05 +0100",Re: Miss-link for the document of Spark 1.0.1,dev@spark.apache.org,"It correctly points to 1.0.1 but you may need to refresh in your browser to
see the update.

"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Sat, 12 Jul 2014 07:24:27 -0700",Re: Miss-link for the document of Spark 1.0.1,dev@spark.apache.org,"Sorry, it may be my bad.

Now I confirmed the link points to the page for 1.0.1.

Thanks
Kousuke



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 12 Jul 2014 22:02:51 -0400",Re: EC2 clusters ready in launch time + 30 seconds,dev <dev@spark.apache.org>,"


Let us know how that goes. I'm definitely interested in hearing more.

Nick
"
"""Ian O'Connell"" <ian@ianoconnell.com>","Sat, 12 Jul 2014 19:59:27 -0700",Re: sparkSQL thread safe?,dev@spark.apache.org,"Thanks for the response Michael

would be great to see.

I opened up a PR with the resource pool usage around it. I didn't include
it in the PR, but a few classes we should probably add as registered in
kryo for good perf/size:
    classOf[org.apache.spark.sql.catalyst.expressions.GenericRow],
    classOf[org.apache.spark.sql.catalyst.expressions.GenericMutableRow],
    classOf[org.apache.spark.sql.catalyst.expressions.Row],
    classOf[Array[Object]],
    scala.collection.immutable.Nil.getClass,
    scala.collection.immutable.::.getClass,
    classOf[scala.collection.immutable.::[Any]]

Thanks for adding that distinct btw, great to have it scale more.


Also more of a sparkCore thing that you might already be aware of, but I
haven't seen mentioned somewhere and was hitting me(Also if any part of
this seems wrong to you I'd love to know):

I was getting out of memory doing a bunch of ops against medium(~1TB
compressed) input sizes with simple things that should spill nicely
(distinct, reduceByKey(_ + _) ).

Anyway what I came back with(copied from an internal email):

I looked through some heap dumps from the OOM's in spark and found there
were >10k instances of DiskBlockObjectWriter's each of which were up to
300kb in size per active executor. At up to 12 concurrent tasks per host is
about 33gb of space topping out. The nodes of course were failing before
this(max mem on our ts cluster per jvm is 25gb).

The memory usage primarily comes from two places, a byte array in
LZFOutputStream and a byte array in BufferedOutputStream. These are both
output buffers along the way to disk(so when we are using the former we can
turn down/disable the latter). These are configured to be 65kb and 100kb
respectively by default. The former is not a configurable option but is
static in that library's code.

These come from the ShuffleBlockWriter, that is we get an input stream with
distinct, reduceByKey, etc..) it maintains the existing partition count. So
each task basically opens >10k files, each file handle of which has these
buffers in place for that task to write to.

Solution put in place(maybe there's a better one?):

Given:
X: The heap size for an executors JVM
Y: The number of threads/cores allowed for concurrent execution per host
Z: The expected overhead of these output streams (currently estimated at
65k + size of the output buffer * 1.1 for overheads)
K: The fraction of memory to allow be used for this overhead (configurable
parameter, default @ 0.2)

Then, the number of partitions: P = (X / Y / Z) * K

Then inside some of our root sources now:
-> After assembling the RDD, if numPartitions > P
-> coalesce to P.
This won't trigger another shuffle phase, so can easily sit inline to
source definitions.

The only real down side of this approach i've seen is that it limits the
number of tasks in this initial map phase which may not be ideal for
parallelism when loading a large dataset and then filtering heavily. It
would be more efficient to pass P into the first distinct/reduceByKey call,
but the user code would have to reference P.






"
Reynold Xin <rxin@databricks.com>,"Sat, 12 Jul 2014 23:19:30 -0700",Re: sparkSQL thread safe?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Ian,

The LZFOutputStream's large byte buffer is sort of annoying. It is much
smaller if you use the Snappy one. The downside of the Snappy one is
slightly less compression (I've seen 10 - 20% larger sizes).

If we can find a compression scheme implementation that doesn't do very
large buffers, that'd be a good idea too ... let me know if you have any
suggestions.

In the future, we plan to make shuffle write to less number of streams at
the same time.




"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 13 Jul 2014 08:56:37 -0400","how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster",dev@spark.apache.org,"Hi, all  

I’m trying the JDBC server, so the cluster is running the version compiled from branch-0.1-jdbc  

Unfortunately (and as expected), it cannot run the programs compiled with the dependency on spark 1.0 (i.e. download from maven)

1. The first error I met is the different SerializationVersionUID in ExecuterStatus  

I resolved by explicitly declare SerializationVersionUID in ExecuterStatus.scala and recompile branch-0.1-jdbc

2. Then I start the program compiled with spark-1.0, what I met is  

14/07/13 05:08:11 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@172.31.*.*:*: java.util.NoSuchElementException: key not found: 6  
14/07/13 05:08:11 WARN AppClient$ClientActor: Connection to akka.tcp://sparkMaster@172.31.*.*:* failed; waiting for master to reconnect...



I don’t understand how ""key not found: 6” comes



Also I tried to start JDBC server with spark-1.0 cluster, after resolving different SerializationVersionUID, what I met is that when I use beeline to run “show tables;”, it shows some executors get lost and tasks failed for unknown reason

Anyone can give some suggestions on how to make spark-1.0 cluster work with JDBC?  

(maybe I need to have a internal maven repo and change all spark dependency to that?)

Best,

--  
Nan Zhu

"
Haoyuan Li <haoyuan.li@gmail.com>,"Sun, 13 Jul 2014 11:47:01 -0700","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"Qingyang,

Are you asking Spark or Shark (The first email was ""Shark"", the last email
was ""Spark"".)?

Best,

Haoyuan






-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sun, 13 Jul 2014 17:53:05 -0700",Re: EC2 clusters ready in launch time + 30 seconds,dev@spark.apache.org,"It should be possible to improve cluster launch time if we are careful
walk down the list of things we do for cluster initialization and see if
there is anything we can do make things faster. Unfortunately this might be
pretty time consuming, but I don't know of a better strategy. The place to
start would be the setup.sh file at
https://github.com/mesos/spark-ec2/blob/v3/setup.sh

Here are some things that take a lot of time and could be improved:
1. Creating swap partitions on all machines. We could check if there is a
way to get EC2 to always mount a swap partition
2. Copying / syncing things across slaves. The copy-dir script is called
too many times right now and each time it pauses for a few milliseconds
between slaves [1]. This could be improved by removing unnecessary copies
3. We could make less frequently used modules like Tachyon, persistent hdfs
not a part of the default setup.

[1] https://github.com/mesos/spark-ec2/blob/v3/copy-dir.sh#L42

Thanks
Shivaram





"
qingyang li <liqingyang1985@gmail.com>,"Mon, 14 Jul 2014 13:13:10 +0800","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"Shark,  thanks for replying.
Let's me clear my question again.
----------------------------------------------
i create a table using "" create table xxx1
tblproperties(""shark.cache""=""tachyon"") as select * from xxx2""
when excuting some sql (for example , select * from xxx1) using shark,
 shark will read data into shark's memory  from tachyon's memory.
I think if each time we execute sql, shark always load data from tachyon,
it is less effient.
could we use some cache policy (such as,  CacheAllPolicy FIFOCachePolicy
LRUCachePolicy ) to cache data to invoid reading data from tachyon for each
sql query?
----------------------------------------------



2014-07-14 2:47 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:

"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Jul 2014 00:01:16 -0700",better compression codecs for shuffle blocks?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,

I was looking into the memory usage of shuffle and one annoying thing is
the default compression codec (LZF) is that the implementation we use
allocates buffers pretty generously. I did a simple experiment and found
that creating 1000 LZFOutputStream allocated 198976424 bytes (~190MB). If
we have a shuffle task that uses 10k reducers and 32 threads running
currently, the memory used by the lzf stream alone would be ~ 60GB.

In comparison, Snappy only allocates ~ 65MB for every
1k SnappyOutputStream. However, Snappy's compression is slightly lower than
LZF's. In my experience, it leads to 10 - 20% increase in size. Compression
ratio does matter here because we are sending data across the network.

In future releases we will likely change the shuffle implementation to open
less streams. Until that happens, I'm looking for compression codec
implementations that are fast, allocate small buffers, and have decent
compression ratio.

Does anybody on this list have any suggestions? If not, I will submit a
patch for 1.1 that replaces LZF with Snappy for the default compression
codec to lower memory usage.


allocation data here: https://gist.github.com/rxin/ad7217ea60e3fb36c567
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 00:34:16 -0700","Re: how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster","""dev@spark.apache.org"" <dev@spark.apache.org>","
I don't think there is a class in Spark named ExecuterStatus (sic) ...
or ExecutorStatus. Is this a class you made?

"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 14 Jul 2014 06:36:08 -0400","Re: how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster","""dev@spark.apache.org"" <dev@spark.apache.org>","Ah, sorry, sorry

It's executorState under deploy package


"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 14 Jul 2014 17:04:28 +0530",Re: better compression codecs for shuffle blocks?,dev@spark.apache.org,"We tried with lower block size for lzf, but it barfed all over the place.
Snappy was the way to go for our jobs.


Regards,
Mridul



"
Will Benton <willb@redhat.com>,"Mon, 14 Jul 2014 12:51:11 -0400 (EDT)",Profiling Spark tests with YourKit (or something else),dev@spark.apache.org,"Hi all,

I've been evaluating YourKit and would like to profile the heap and CPU usage of certain tests from the Spark test suite.  In particular, I'm very interested in tracking heap usage by allocation site.  Unfortunately, I get a lot of crashes running Spark tests with profiling (and thus allocation-site tracking) enabled in YourKit; just using the sampler works fine, but it appears that enabling the profiler breaks Utils.getCallSite.

Is there a way to make this combination work?  If not, what are people using to understand the memory and CPU behavior of Spark and Spark apps?


thanks,
wb

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 14 Jul 2014 11:02:04 -0700",Re: Profiling Spark tests with YourKit (or something else),dev@spark.apache.org,"I haven't seen issues using the JVM's own tools (jstack, jmap, hprof and such), so maybe there's a problem in YourKit or in your release of the JVM. Otherwise I'd suggest increasing the heap size of the unit tests a bit (you can do this in the SBT build file). Maybe they are very close to full and profiling pushes them over the edge.

Matei


CPU usage of certain tests from the Spark test suite.  In particular, I'm very interested in tracking heap usage by allocation site.  Unfortunately, I get a lot of crashes running Spark tests with profiling (and thus allocation-site tracking) enabled in YourKit; just using the sampler works fine, but it appears that enabling the profiler breaks Utils.getCallSite.
using to understand the memory and CPU behavior of Spark and Spark apps?


"
Michael Armbrust <michael@databricks.com>,"Mon, 14 Jul 2014 13:04:30 -0700",Re: Catalyst dependency on Spark Core,"user@spark.apache.org, dev@spark.apache.org","Yeah, sadly this dependency was introduced when someone consolidated the
logging infrastructure.  However, the dependency should be very small and
thus easy to remove, and I would like catalyst to be usable outside of
Spark.  A pull request to make this possible would be welcome.

Ideally, we'd create some sort of spark common package that has things like
logging.  That way catalyst could depend on that, without pulling in all of
Hadoop, etc.  Maybe others have opinions though, so I'm cc-ing the dev list.



"
Cody Koeninger <cody.koeninger@mediacrossing.com>,"Mon, 14 Jul 2014 15:42:38 -0500","Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"Hi all, just wanted to give a heads up that we're seeing a reproducible
deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2

If jira is a better place for this, apologies in advance - figured talking
about it on the mailing list was friendlier than randomly (re)opening jira
tickets.

I know Gary had mentioned some issues with 1.0.1 on the mailing list, once
we got a thread dump I wanted to follow up.

The thread dump shows the deadlock occurs in the synchronized block of code
that was changed in HadoopRDD.scala, for the Spark-1097 issue

Relevant portions of the thread dump are summarized below, we can provide
the whole dump if it's useful.

Found one Java-level deadlock:
=============================
""Executor task launch worker-1"":
  waiting to lock monitor 0x00007f250400c520 (object 0x00000000fae7dc30, a
org.apache.hadoop.co
nf.Configuration),
  which is held by ""Executor task launch worker-0""
""Executor task launch worker-0"":
  waiting to lock monitor 0x00007f2520495620 (object 0x00000000faeb4fc8, a
java.lang.Class),
  which is held by ""Executor task launch worker-1""


""Executor task launch worker-1"":
        at
org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
        - waiting to lock <0x00000000fae7dc30> (a
org.apache.hadoop.conf.Configuration)
        at
org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
        - locked <0x00000000faca6ff8> (a java.lang.Class for
org.apache.hadoop.conf.Configurati
on)
        at
org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
        at
org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
java:57)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
java:57)
        at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
sorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
        at java.lang.Class.newInstance0(Class.java:374)
        at java.lang.Class.newInstance(Class.java:327)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
        at
org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
        - locked <0x00000000faeb4fc8> (a java.lang.Class for
org.apache.hadoop.fs.FileSystem)
        at
org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
        at
org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
        at
org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
        at
org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)



...elided...


""Executor task launch worker-0"" daemon prio=10 tid=0x0000000001e71800
nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at
org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
        - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
org.apache.hadoop.fs.FileSystem)
        at
org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
        at
org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
        at
org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
        at
org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 14 Jul 2014 14:15:44 -0700",Re: Catalyst dependency on Spark Core,user@spark.apache.org,"Yeah, I'd just add a spark-util that has these things.

Matei


the logging infrastructure.  However, the dependency should be very small and thus easy to remove, and I would like catalyst to be usable outside of Spark.  A pull request to make this possible would be welcome.
like logging.  That way catalyst could depend on that, without pulling in all of Hadoop, etc.  Maybe others have opinions though, so I'm cc-ing the dev list.
time and evolution.
org.apache.spark.util.{Utils => SparkUtils},
independent of Spark in later release.
<aniket.bhatnagar@gmail.com>:
(http://people.apache.org/~marmbrus/talks/SparkSQLScalaDays2014.pdf), it was mentioned that Catalyst is independent of Spark. But on inspecting pom.xml of sql/catalyst module, it seems it has a dependency on Spark Core. Any particular reason for the dependency? I would love to use Catalyst outside Spark

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 14:44:04 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Cody,

This Jstack seems truncated, would you mind giving the entire stack
trace? For the second thread, for instance, we can't see where the
lock is being acquired.

- Patrick


"
Will Benton <willb@redhat.com>,"Mon, 14 Jul 2014 17:59:01 -0400 (EDT)",Re: Profiling Spark tests with YourKit (or something else),dev@spark.apache.org,"Thanks, Matei; I have also had some success with jmap and friends and will probably just stick with them!


best,
wb


	by minotaur.apache.org (Postfix) with SMTP id B9B9D1184E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:08:44 +0000 (UTC)
Received: (qmail 22299 invoked by uid 500); 14 Jul 2014 22:08:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22252 invoked by uid 500); 14 Jul 2014 22:08:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22240 invoked by uid 99); 14 Jul 2014 22:08:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:08:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:08:39 +0000
Received: by mail-qc0-f176.google.com with SMTP id i17so1613145qcy.35
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:08:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ZUj3xdEIgbXJegkgLqL7eu9lY2mcq6+l+3s5iouqtZE=;
        b=CxWnK492nQ3TMjoZW6xvRlAkGR8teRTYxhOK5HXKosxDQ/8zpxhbMa0PqrBW0a0dhb
         iz3gtAcrf1mqclONOF+ik8eyVL20AkW7+fFWrfvMx0YoARgbo5ZwnPB7I+ZPB7phZaft
         LKYF4IOiBDE65E/AQUEJvoSdIR2Uwn6/v3SdHPkgYwULgvtSTIK0Wirag22Xd+Z0OJzv
         pF8YrNZY6BtdN+5x9pWzndZ567w6a38ezr7Z6mvz7DXvYBXNKE+mJ98JaY9kXfsSrSHA
         S/BCTjXAHcsg/D7q6gA3x3eX6jkspf2CvmnV9oiioHQoleZGayjYEmvbHiyHcpBsSnAi
         CqFg==
X-Received: by 10.224.41.65 with SMTP id n1mr27427243qae.75.1405375699002;
 Mon, 14 Jul 2014 15:08:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 14 Jul 2014 15:07:58 -0700 (PDT)
In-Reply-To: <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
 <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Jul 2014 15:07:58 -0700
Message-ID: <CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc89fc50abef04fe2e87d0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc89fc50abef04fe2e87d0
Content-Type: text/plain; charset=UTF-8

The full jstack would still be useful, but our current working theory is
that this is due to the fact that Configuration#loadDefaults goes through
every Configuration object that was ever created (via
Configuration.REGISTRY) and locks it, thus introducing a dependency from
new Configuration to old, otherwise unrelated, Configuration objects that
our locking did not anticipate.

I have created https://github.com/apache/spark/pull/1409 to hopefully fix
this bug.




--047d7bdc89fc50abef04fe2e87d0--

"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 14 Jul 2014 15:21:10 -0700",Re: Profiling Spark tests with YourKit (or something else),dev@spark.apache.org,"Out of curiosity, what problems are you seeing with Utils.getCallSite?



"
Nishkam Ravi <nravi@cloudera.com>,"Mon, 14 Jul 2014 15:28:36 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object would
help. I suspect we will start seeing the ConcurrentModificationException
again. The right fix has gone into Hadoop through 10456. Unfortunately, I
don't have any bright ideas on how to synchronize this at the Spark level
without the risk of deadlocks.



"
Stephen Haberman <stephen.haberman@gmail.com>,"Mon, 14 Jul 2014 17:30:27 -0500",Re: better compression codecs for shuffle blocks?,Reynold Xin <rxin@databricks.com>,"
Just a comment from the peanut gallery, but these buffers are a real
PITA for us as well. Probably 75% of our non-user-error job failures
are related to them.

Just naively, what about not doing compression on the fly? E.g. during
the shuffle just write straight to disk, uncompressed?

For us, we always have plenty of disk space, and if you're concerned
about network transmission, you could add a separate compress step
after the blocks have been written to disk, but before being sent over
the wire.

Granted, IANAE, so perhaps this is a bad idea; either way, awesome to
see work in this area!

- Stephen


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 14 Jul 2014 15:51:43 -0700",Re: better compression codecs for shuffle blocks?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Stephen,
Often the shuffle is bound by writes to disk, so even if disks have enough
space to store the uncompressed data, the shuffle can complete faster by
writing less data.

Reynold,
This isn't a big help in the short term, but if we switch to a sort-based
shuffle, we'll only need a single LZFOutputStream per map task.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 14 Jul 2014 15:54:45 -0700",Re: better compression codecs for shuffle blocks?,dev@spark.apache.org,"You can actually turn off shuffle compression by setting spark.shuffle.compress to false. Try that out, there will still be some buffers for the various OutputStreams, but they should be smaller.

Matei




"
Michael Armbrust <michael@databricks.com>,"Mon, 14 Jul 2014 15:55:59 -0700",Change when loading/storing String data using Parquet,"dev@spark.apache.org, user@spark.apache.org","I just wanted to send out a quick note about a change in the handling of
strings when loading / storing data using parquet and Spark SQL.  Before,
Spark SQL did not support binary data in Parquet, so all binary blobs were
implicitly treated as Strings.  9fe693
<https://github.com/apache/spark/commit/9fe693b5b6ed6af34ee1e800ab89c8a11991ea38>
fixes
this limitation by adding support for binary data.

However, data written out with a prior version of Spark SQL will be missing
the annotation telling us to interpret a given column as a String, so old
string data will now be loaded as binary data.  If you would like to use
the data as a string, you will need to add a CAST to convert the datatype.

New string data written out after this change, will correctly be loaded in
as a string as now we will include an annotation about the desired type.
 Additionally, this should now interoperate correctly with other systems
that write Parquet data (hive, thrift, etc).

Michael
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 15:57:46 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Nishkam,

Aaron's fix should prevent two concurrent accesses to getJobConf (and
the Hadoop code therein). But if there is code elsewhere that tries to
mutate the configuration, then I could see how we might still have the
ConcurrentModificationException.

I looked at your patch for HADOOP-10456 and the only example you give
is of the data being accessed inside of getJobConf. Is it accessed
somewhere else too from Spark that you are aware of?

https://issues.apache.org/jira/browse/HADOOP-10456

- Patrick


"
Gary Malouf <malouf.gary@gmail.com>,"Mon, 14 Jul 2014 19:06:22 -0400","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"We use the Hadoop configuration inside of our code executing on Spark as we
need to list out files in the path.  Maybe that is why it is exposed for us.



"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Jul 2014 16:08:48 -0700",Re: better compression codecs for shuffle blocks?,Matei Zaharia <matei.zaharia@gmail.com>,"Copying Jon here since he worked on the lzf library at Ning.

Jon - any comments on this topic?



"
Nishkam Ravi <nravi@cloudera.com>,"Mon, 14 Jul 2014 16:10:33 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"HI Patrick, I'm not aware of another place where the access happens, but
it's possible that it does. The original fix synchronized on the
broadcastConf object and someone reported the same exception.



"
Andrew Ash <andrew@andrewash.com>,"Mon, 14 Jul 2014 19:18:11 -0400","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"I observed a deadlock here when using the AvroInputFormat as well. The
short of the issue is that there's one configuration object per JVM, but
multiple threads, one for each task. If each thread attempts to add a
configuration option to the Configuration object at once you get issues
because HashMap isn't thread safe.

More details to come tonight. Thanks!

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 16:22:59 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097","""dev@spark.apache.org"" <dev@spark.apache.org>","Andrew and Gary,

Would you guys be able to test
https://github.com/apache/spark/pull/1409/files and see if it solves
your problem?

- Patrick


"
Gary Malouf <malouf.gary@gmail.com>,"Mon, 14 Jul 2014 19:35:06 -0400","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"We'll try to run a build tomorrow AM.



"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 14 Jul 2014 16:46:10 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"The patch won't solve the problem where two people try to add a
configuration option at the same time, but I think there is currently an
issue where two people can try to initialize the Configuration at the same
time and still run into a ConcurrentModificationException. This at least
reduces (slightly) the scope of the exception although eliminating it may
not be possible.



"
Davies Liu <davies@databricks.com>,"Mon, 14 Jul 2014 16:52:38 -0700",Re: better compression codecs for shuffle blocks?,dev@spark.apache.org,"Maybe we could try LZ4 [1], which has better performance and smaller footprint
than LZF and Snappy. In fast scan mode, the performance is 1.5 - 2x
higher than LZF[2],
but memory used is 10x smaller than LZF (16k vs 190k).

[1] https://github.com/jpountz/lz4-java
[2] http://ning.github.io/jvm-compressor-benchmark/results/calgary/roundtrip-2013-06-06/index.html



"
Jon Hartlaub <jhartlaub@gmail.com>,"Mon, 14 Jul 2014 16:53:22 -0700",Re: better compression codecs for shuffle blocks?,Reynold Xin <rxin@databricks.com>,"Is the held memory due to just instantiating the LZFOutputStream?  If so,
I'm a surprised and I consider that a bug.

I suspect the held memory may be due to a SoftReference - memory will be
released with enough memory pressure.

Finally, is it necessary to keep 1000 (or more) decoders active?  Would it
be possible to keep an object pool of encoders and check them in and out as
needed?  I admit I have not done much homework to determine if this is
viable.

-Jon



"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 14 Jul 2014 16:53:25 -0700",SBT gen-idea doesn't work well after merging SPARK-1776,dev@spark.apache.org,"I've a clean clone of spark master repository, and I generated the
intellij project file by sbt gen-idea as usual. There are two issues
we have after merging SPARK-1776 (read dependencies from Maven).

1) After SPARK-1776, sbt gen-idea will download the dependencies from
internet even those jars are in local cache. Before merging, the
second time we run gen-idea will not download anything but use the
jars in cache.

2) The tests with spark local context can not be run in the intellij.
It will show the following exception.

The current workaround we've are checking out any snapshot before
merging to gen-idea, and then switch back to current master. But this
will not work when the master deviate too much from the latest working
snapshot.

[ERROR] [07/14/2014 16:27:49.967] [ScalaTest-run] [Remoting] Remoting
error: [Startup timed out] [
akka.remote.RemoteTransportException: Startup timed out
at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)
at akka.remote.Remoting.start(Remoting.scala:191)
at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
at org.apache.spark.SparkEnv$.create(SparkEnv.scala:153)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:117)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:132)
at org.apache.spark.mllib.util.LocalSparkContext$class.beforeAll(LocalSparkContext.scala:29)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)
at org.apache.spark.mllib.optimization.LBFGSSuite.run(LBFGSSuite.scala:27)
at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
at scala.collection.immutable.List.foreach(List.scala:318)
at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
at org.scalatest.tools.Runner$.run(Runner.scala:883)
at org.scalatest.tools.Runner.run(Runner.scala)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.util.concurrent.TimeoutException: Futures timed out
after [10000 milliseconds]
at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
at scala.concurrent.Await$.result(package.scala:107)
at akka.remote.Remoting.start(Remoting.scala:173)
... 35 more
]

An exception or error caused a run to abort: Futures timed out after
[10000 milliseconds]
java.util.concurrent.TimeoutException: Futures timed out after [10000
milliseconds]
at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
at scala.concurrent.Await$.result(package.scala:107)
at akka.remote.Remoting.start(Remoting.scala:173)
at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
at org.apache.spark.SparkEnv$.create(SparkEnv.scala:153)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:117)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:132)
at org.apache.spark.mllib.util.LocalSparkContext$class.beforeAll(LocalSparkContext.scala:29)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)
at org.apache.spark.mllib.optimization.LBFGSSuite.run(LBFGSSuite.scala:27)
at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
at scala.collection.immutable.List.foreach(List.scala:318)
at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
at org.scalatest.tools.Runner$.run(Runner.scala:883)
at org.scalatest.tools.Runner.run(Runner.scala)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 14 Jul 2014 17:06:17 -0700",Re: better compression codecs for shuffle blocks?,dev@spark.apache.org,"is (# cores * # reduce partitions), which can easily climb into the tens of
thousands for large jobs. This is a more general problem that we are
planning on fixing for our largest shuffles, as even moderate buffer sizes
can explode to use huge amounts of memory at that scale.



"
Cody Koeninger <cody.koeninger@mediacrossing.com>,"Mon, 14 Jul 2014 20:22:25 -0500","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"Here's the entire jstack output.



2014-07-14 20:00:55
Full thread dump Java HotSpot(TM) 64-Bit Server VM (23.21-b01 mixed mode):

""Attach Listener"" daemon prio=10 tid=0x00007f24f8001000 nid=0x2e0c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""spark-akka.actor.default-dispatcher-13"" daemon prio=10 tid=0x00007f24ec01e800 nid=0x2d9c waiting on condition [0x00007f24d25f1000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01c078> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""Executor task launch worker-1"" daemon prio=10 tid=0x0000000001e78800 nid=0x2d98 waiting for monitor entry [0x00007f24d29f0000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
	- waiting to lock <0x00000000fae7dc30> (a org.apache.hadoop.conf.Configuration)
	at org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
	- locked <0x00000000faca6ff8> (a java.lang.Class for org.apache.hadoop.conf.Configuration)
	at org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
	at org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at java.lang.Class.newInstance0(Class.java:374)
	at java.lang.Class.newInstance(Class.java:327)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
	at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
	- locked <0x00000000faeb4fc8> (a java.lang.Class for org.apache.hadoop.fs.FileSystem)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
	at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
	at org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
	at org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
	at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
	at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:145)
	- locked <0x00000000fafcb950> (a org.apache.hadoop.conf.Configuration)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:189)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

""Executor task launch worker-0"" daemon prio=10 tid=0x0000000001e71800 nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
	- waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for org.apache.hadoop.fs.FileSystem)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
	at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
	at org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
	at org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
	at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
	at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:145)
	- locked <0x00000000fae7dc30> (a org.apache.hadoop.conf.Configuration)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:189)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:33)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:74)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-40"" daemon prio=10 tid=0x0000000001db0000 nid=0x2d96 waiting on condition [0x00007f24d2df5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01f728> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-39"" daemon prio=10 tid=0x0000000001dae800 nid=0x2d95 waiting on condition [0x00007f24d2ff6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01f728> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-38"" daemon prio=10 tid=0x0000000001dad000 nid=0x2d94 waiting on condition [0x00007f24d31f7000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01f728> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-37"" daemon prio=10 tid=0x0000000001dab000 nid=0x2d93 waiting on condition [0x00007f24d33f8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01f728> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-36"" daemon prio=10 tid=0x0000000001da9000 nid=0x2d92 waiting on condition [0x00007f24d35f9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01f728> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-35"" daemon prio=10 tid=0x0000000001da7800 nid=0x2d91 waiting on condition [0x00007f24d37fa000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01f728> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-34"" daemon prio=10 tid=0x0000000001da6000 nid=0x2d90 waiting on condition [0x00007f24d39fb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb01f728> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:722)

""qtp805847888-33 Acceptor0 SocketConnector@0.0.0.0:52133"" daemon prio=10 tid=0x0000000001da4000 nid=0x2d8f runnable [0x00007f24d3bfc000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:522)
	at java.net.ServerSocket.accept(ServerSocket.java:490)
	at org.eclipse.jetty.server.bio.SocketConnector.accept(SocketConnector.java:117)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:722)

""HTTP_BROADCAST cleanup timer"" daemon prio=10 tid=0x0000000001d6f000 nid=0x2d8e in Object.wait() [0x00007f24d3dfd000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000fb0de2d8> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000000fb0de2d8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""Connection manager future execution context-0"" daemon prio=10 tid=0x0000000001d6a000 nid=0x2d8d waiting on condition [0x00007f24d3ffe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fb00aca0> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

""BROADCAST_VARS cleanup timer"" daemon prio=10 tid=0x0000000001d5f800 nid=0x2d8c in Object.wait() [0x00007f24d8283000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000fb00b918> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000000fb00b918> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""BLOCK_MANAGER cleanup timer"" daemon prio=10 tid=0x0000000001d5e800 nid=0x2d8b in Object.wait() [0x00007f24d8484000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000fb00b558> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000000fb00b558> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""connection-manager-thread"" daemon prio=10 tid=0x0000000001d5a000 nid=0x2d8a runnable [0x00007f24d8685000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000fb00a138> (a sun.nio.ch.Util$2)
	- locked <0x00000000fb00a128> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000fb009c70> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:303)
	at org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)

""SHUFFLE_BLOCK_MANAGER cleanup timer"" daemon prio=10 tid=0x0000000001d0d800 nid=0x2d89 in Object.wait() [0x00007f24d8886000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000fac81e98> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000000fac81e98> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""Hashed wheel timer #1"" daemon prio=10 tid=0x00007f24f010f800 nid=0x2d88 waiting on condition [0x00007f24d8a87000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503)
	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:722)

""New I/O server boss #6"" daemon prio=10 tid=0x00007f24e0121800 nid=0x2d87 runnable [0x00007f24d8c88000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000fabcb980> (a sun.nio.ch.Util$2)
	- locked <0x00000000fabcb970> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000fabcb500> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

""New I/O worker #5"" daemon prio=10 tid=0x00007f24e0110800 nid=0x2d86 runnable [0x00007f24d8e89000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000fabca618> (a sun.nio.ch.Util$2)
	- locked <0x00000000fabca608> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000fabca3d8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

""New I/O worker #4"" daemon prio=10 tid=0x00007f24e00a1000 nid=0x2d85 runnable [0x00007f24d908a000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000fabcab98> (a sun.nio.ch.Util$2)
	- locked <0x00000000fabcab88> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000fabca968> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

""New I/O boss #3"" daemon prio=10 tid=0x00007f24e0093800 nid=0x2d84 runnable [0x00007f24d928b000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000fabd5648> (a sun.nio.ch.Util$2)
	- locked <0x00000000fabd5638> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000fabd53e8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

""New I/O worker #2"" daemon prio=10 tid=0x00007f24e0044800 nid=0x2d83 runnable [0x00007f24d948c000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000fac5fba8> (a sun.nio.ch.Util$2)
	- locked <0x00000000fac5fb98> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000fac5f978> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(Thr"
"
Aaron Davidson <ilikerps@gmail.com>,Mon"," 14 Jul 2014 18:38:16 -0700""",Re: Profiling Spark tests with YourKit (or something else),dev@spark.apache.org,"Would you mind filing a JIRA for this? That does sound like something bogus
happening on the JVM/YourKit level, but this sort of diagnosis is
sufficiently important that we should be resilient against it.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 14 Jul 2014 21:43:17 -0400",ec2 clusters launched at 9fe693b5b6 are broken (?),dev <dev@spark.apache.org>,"Just launched an EC2 cluster from git hash
9fe693b5b6ed6af34ee1e800ab89c8a11991ea38. Calling take() on an RDD
accessing data in S3 yields the following error output.

I understand that NoClassDefFoundError errors may mean something in the
deployment was messed up. Is that correct? When I launch a cluster using
spark-ec2, I expect all critical deployment details to be taken care of by
the script.

So is something in the deployment executed by spark-ec2 borked?

Nick

java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException
    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(NativeS3FileSystem.java:224)
    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:214)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:176)
    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)
    at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:176)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:71)
    at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:79)
    at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:190)
    at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:188)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.dependencies(RDD.scala:188)
    at org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGScheduler.scala:1144)
    at org.apache.spark.SparkContext.getPreferredLocs(SparkContext.scala:903)
    at org.apache.spark.rdd.PartitionCoalescer.currPrefLocs(CoalescedRDD.scala:174)
    at org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:191)
    at org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:190)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
    at org.apache.spark.rdd.PartitionCoalescer$LocationIterator.<init>(CoalescedRDD.scala:185)
    at org.apache.spark.rdd.PartitionCoalescer.setupGroups(CoalescedRDD.scala:236)
    at org.apache.spark.rdd.PartitionCoalescer.run(CoalescedRDD.scala:337)
    at org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:83)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.RDD.take(RDD.scala:1036)
    at $iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
    at $iwC$$iwC$$iwC.<init>(<console>:31)
    at $iwC$$iwC.<init>(<console>:33)
    at $iwC.<init>(<console>:35)
    at <init>(<console>:37)
    at .<init>(<console>:41)
    at .<clinit>(<console>)
    at .<init>(<console>:7)
    at .<clinit>(<console>)
    at $print(<console>)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:788)
    at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1056)
    at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:614)
    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
    at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
    at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
    at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
    at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
    at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
    at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
    at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
    at org.apache.spark.repl.Main$.main(Main.scala:31)
    at org.apache.spark.repl.Main.main(Main.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException:
org.jets3t.service.S3ServiceException
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 92 more

​
"
scwf <wangfei1@huawei.com>,"Tue, 15 Jul 2014 09:49:43 +0800","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",<dev@spark.apache.org>,"hi，Cody
   i met this issue days before and i post a PR for this( https://github.com/apache/spark/pull/1385)
it's very strange that if i synchronize conf it will deadlock but it is ok when synchronize initLocalJobConfFuncOpt




-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 14 Jul 2014 22:43:49 -0400","Re: how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster","""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","I resolved the issue by setting an internal maven repository to contain the Spark-1.0.1 jar compiled from branch-0.1-jdbc and replacing the dependency to the central repository with our own repository 

I believe there should be some more lightweight way

Best, 

-- 
Nan Zhu




"
Will Benton <willb@redhat.com>,"Mon, 14 Jul 2014 23:43:46 -0400 (EDT)",Re: Profiling Spark tests with YourKit (or something else),dev@spark.apache.org,"Sure thing:

   https://issues.apache.org/jira/browse/SPARK-2486
   https://github.com/apache/spark/pull/1413

best,
wb


	by minotaur.apache.org (Postfix) with SMTP id 308C01121B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 04:05:43 +0000 (UTC)
Received: (qmail 79934 invoked by uid 500); 15 Jul 2014 04:05:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79872 invoked by uid 500); 15 Jul 2014 04:05:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79861 invoked by uid 99); 15 Jul 2014 04:05:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 04:05:41 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 04:05:38 +0000
Received: by mail-vc0-f177.google.com with SMTP id hy4so2354740vcb.36
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 21:05:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ihbOIqIRY0lohN7kvddOjFwhp82knkayjAJ+VV/QeWU=;
        b=NQIIH3IfrM4/ss20dMrh75b/E8YR13eSl3r2DMKBOz9mn2WN7s6evr8zVXXhAoYgeU
         XRaQPV4a48RCu10ELmEFge4+Ev/b0Cp8r3wwIrfGOes6EbP3SVJQ9qHYm4vZrDI2FfB4
         gqFk2GQx4mH7B2gz9s73DmgAv8r5y9iKVwL5yB8R3GIgQDt0e/VNmkUk6K8NAng1Efj2
         WaSZ0q/Sf+F6hHpfeFQoODgg+Fs9URXG6pLIdPfhUi7lyKWbUANFpnfmFMAw6wIYVm3/
         D4Bv5/MYAeLBi8PasMQJtTUBuqWYH8Bp1ysF/9WQWsGiw1nI2LbGt6Fi1OfW8X/BDFVi
         QS/g==
X-Gm-Message-State: ALoCoQlxLoKLvyUdr6p3o9ZS+qxvlkkJKMWyjkgmqRELlksQYc6EarLzkE/awEDKMWDm1JtH2d0R
X-Received: by 10.58.150.136 with SMTP id ui8mr20291577veb.14.1405397112824;
        Mon, 14 Jul 2014 21:05:12 -0700 (PDT)
Received: from mail-vc0-f181.google.com (mail-vc0-f181.google.com [209.85.220.181])
        by mx.google.com with ESMTPSA id m10sm22871770vdj.28.2014.07.14.21.05.11
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 21:05:11 -0700 (PDT)
Received: by mail-vc0-f181.google.com with SMTP id lf12so5115374vcb.26
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.52.244.81 with SMTP id xe17mr16602217vdc.24.1405397110824;
 Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
In-Reply-To: <53C488B7.402@huawei.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
	<53C488B7.402@huawei.com>
Date: Tue, 15 Jul 2014 00:05:10 -0400
Message-ID: <CA+-p3AGioD-tfQP+_n29pWnJqR_OBWnAbj2Ek_jVA_WVZLn7ag@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Andrew Ash <andrew@andrewash.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2c34a8f260604fe3383c0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c34a8f260604fe3383c0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm not sure either of those PRs will fix the concurrent adds to
Configuration issue I observed. I've got a stack trace and writeup I'll
share in an hour or two (traveling today).

k
d
=3D=3D=3D=3D=3D=3D=3D=3D

--001a11c2c34a8f260604fe3383c0--

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 21:55:27 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097","""dev@spark.apache.org"" <dev@spark.apache.org>","Andrew is your issue also a regression from 1.0.0 to 1.0.1? The
immediate priority is addressing regressions between these two
releases.

ok
ed
n
========
)
s

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 21:59:47 -0700",Re: Catalyst dependency on Spark Core,user@spark.apache.org,"Adding new build modules is pretty high overhead, so if this is a case
where a small amount of duplicated code could get rid of the
dependency, that could also be a good short-term option.

- Patrick


"
Andrew Ash <andrew@andrewash.com>,"Tue, 15 Jul 2014 01:02:50 -0400","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"I don't believe mine is a regression. But it is related to thread safety on
Hadoop Configuration objects. Should I start a new thread?

s
ck
e
g
=========
"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 14 Jul 2014 22:02:38 -0700",Re: ec2 clusters launched at 9fe693b5b6 are broken (?),dev@spark.apache.org,"This one is typically due to a mismatch between the Hadoop versions --
i.e., Spark is compiled against 1.0.4 but is running with 2.3.0 in the
classpath, or something like that. Not certain why you're seeing this with
spark-ec2, but I'm assuming this is related to the issues you posted in a
separate thread.



y
eS3FileSystem.java:224)
ystem.java:214)
176)
08)
)
)
)
)
a:32)
)
)
la:1144)
74)
fun$apply$2.apply(CoalescedRDD.scala:191)
fun$apply$2.apply(CoalescedRDD.scala:190)
RDD.scala:185)
6)
)
)
)
)
)
:57)
mpl.java:43)
)
41)
1)
op.scala:936)
a:884)
a:884)
er.scala:135)
:57)
mpl.java:43)
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 14 Jul 2014 22:11:35 -0700",Re: ec2 clusters launched at 9fe693b5b6 are broken (?),dev@spark.apache.org,"My guess is that this is related to
https://issues.apache.org/jira/browse/SPARK-2471 where the S3 library gets
excluded from the SBT assembly jar. I am not sure if the assembly jar used
in EC2 is generated using SBT though.

Shivaram


:

h
g
eS3FileSystem.java:224)
ystem.java:214)
176)
08)
)
a:32)
la:1144)
74)
fun$apply$2.apply(CoalescedRDD.scala:191)
fun$apply$2.apply(CoalescedRDD.scala:190)
RDD.scala:185)
6)
:57)
mpl.java:43)
8)
)
)
41)
8)
op.scala:936)
a:884)
a:884)
er.scala:135)
:57)
mpl.java:43)
3)
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 22:19:17 -0700",Re: ec2 clusters launched at 9fe693b5b6 are broken (?),"""dev@spark.apache.org"" <dev@spark.apache.org>, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","Yeah - this is likely caused by SPARK-2471.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 22:20:04 -0700","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Andrew,

Yeah, that would be preferable. Definitely worth investigating both,
but the regression is more pressing at the moment.

- Patrick

on
:
l
is
ack
he
ng
=========

"
Andrew Ash <andrew@andrewash.com>,"Tue, 15 Jul 2014 01:22:53 -0400",Hadoop's Configuration object isn't threadsafe,dev@spark.apache.org,"Hi Spark devs,

We discovered a very interesting bug in Spark at work last week in Spark
0.9.1 — that the way Spark uses the Hadoop Configuration object is prone to
thread safety issues.  I believe it still applies in Spark 1.0.1 as well.
 Let me explain:


*Observations*

   - Was running a relatively simple job (read from Avro files, do a map,
   do another map, write back to Avro files)
   - 412 of 413 tasks completed, but the last task was hung in RUNNING state
   - The 412 successful tasks completed in median time 3.4s
   - The last hung task didn't finish even in 20 hours
   - The executor with the hung task was responsible for 100% of one core
   of CPU usage
   - Jstack of the executor attached (relevant thread pasted below)


*Diagnosis*

After doing some code spelunking, we determined the issue was concurrent
use of a Configuration object for each task on an executor.  In Hadoop each
task runs in its own JVM, but in Spark multiple tasks can run in the same
JVM, so the single-threaded access assumptions of the Configuration object
no longer hold in Spark.

The specific issue is that the AvroRecordReader actually _modifies_ the
JobConf it's given when it's instantiated!  It adds a key for the RPC
protocol engine in the process of connecting to the Hadoop FileSystem.
 When many tasks start at the same time (like at the start of a job), many
tasks are adding this configuration item to the one Configuration object at
once.  Internally Configuration uses a java.lang.HashMap, which isn't
threadsafe… The below post is an excellent explanation of what happens in
the situation where multiple threads insert into a HashMap at the same time.

http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html

The gist is that you have a thread following a cycle of linked list nodes
indefinitely.  This exactly matches our observations of the 100% CPU core
and also the final location in the stack trace.

So it seems the way Spark shares a Configuration object between task
threads in an executor is incorrect.  We need some way to prevent
concurrent access to a single Configuration object.


*Proposed fix*

We can clone the JobConf object in HadoopRDD.getJobConf() so each task gets
its own JobConf object (and thus Configuration object).  The optimization
of broadcasting the Configuration object across the cluster can remain, but
on the other side I think it needs to be cloned for each task to allow for
concurrent access.  I'm not sure the performance implications, but the
comments suggest that the Configuration object is ~10KB so I would expect a
clone on the object to be relatively speedy.

Has this been observed before?  Does my suggested fix make sense?  I'd be
happy to file a Jira ticket and continue discussion there for the right way
to fix.


Thanks!
Andrew


P.S.  For others seeing this issue, our temporary workaround is to enable
spark.speculation, which retries failed (or hung) tasks on other machines.



""Executor task launch worker-6"" daemon prio=10 tid=0x00007f91f01fe000
nid=0x54b1 runnable [0x00007f92d74f1000]
   java.lang.Thread.State: RUNNABLE
    at java.util.HashMap.transfer(HashMap.java:601)
    at java.util.HashMap.resize(HashMap.java:581)
    at java.util.HashMap.addEntry(HashMap.java:879)
    at java.util.HashMap.put(HashMap.java:505)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
    at
org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
    at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
    at
org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
    at
org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
    at
org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
    at
org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
    at
org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
    at
org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
    at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
    at
org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
    at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
    at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
2014-07-11 15:54:08
Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.60-b09 mixed mode):

""Attach Listener"" daemon prio=10 tid=0x00007f9210001000 nid=0x5a5a waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""sparkExecutor-akka.actor.default-dispatcher-25"" daemon prio=10 tid=0x00007f9100001000 nid=0x7ff waiting on condition [0x00007f92d630c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-26"" daemon prio=10 tid=0x00007f9204005000 nid=0x7fe waiting on condition [0x00007f92d5f08000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-24"" daemon prio=10 tid=0x00007f9144196000 nid=0x7fd waiting on condition [0x00007f92d5e07000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-23"" daemon prio=10 tid=0x00007f9204003800 nid=0x7f7 waiting on condition [0x00007f92d6009000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-22"" daemon prio=10 tid=0x00007f9204002000 nid=0x7f6 waiting on condition [0x00007f92d610a000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-21"" daemon prio=10 tid=0x00007f91cc001000 nid=0x7f5 waiting on condition [0x00007f92d620b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-20"" daemon prio=10 tid=0x00007f9204001000 nid=0x7f4 waiting on condition [0x00007f92d79f8000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-19"" daemon prio=10 tid=0x00007f91c4001000 nid=0x7f3 waiting on condition [0x00007f92d77f6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkExecutor-akka.actor.default-dispatcher-18"" daemon prio=10 tid=0x00007f91fc03a800 nid=0x7e0 waiting on condition [0x00007f92d78f7000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7dec80d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-16"" daemon prio=10 tid=0x00007f913c01f800 nid=0x7df waiting on condition [0x00007f92d76f5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7ded88d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-15"" daemon prio=10 tid=0x00007f91b4001000 nid=0x7de waiting on condition [0x00007f92d75f4000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7ded88d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""spark-akka.actor.default-dispatcher-14"" daemon prio=10 tid=0x00007f91ac004000 nid=0x7dd waiting on condition [0x00007f92d7af9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7ded88d8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""Connection manager future execution context-1"" daemon prio=10 tid=0x00007f91b8001000 nid=0x7dc waiting on condition [0x00007f92dc63e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7ded8c20> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""org.apache.hadoop.hdfs.PeerCache@4404df37"" daemon prio=10 tid=0x00007f9138020800 nid=0x57ec waiting on condition [0x00007f92d5d06000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.PeerCache.run(PeerCache.java:252)
	at org.apache.hadoop.hdfs.PeerCache.access$000(PeerCache.java:39)
	at org.apache.hadoop.hdfs.PeerCache$1.run(PeerCache.java:135)
	at java.lang.Thread.run(Thread.java:745)

""RESULT_TASK cleanup timer"" daemon prio=10 tid=0x00007f9140005800 nid=0x55a8 in Object.wait() [0x00007f92d6cd4000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f9f7def00c8> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00007f9f7def00c8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""Executor task launch worker-6"" daemon prio=10 tid=0x00007f91f01fe000 nid=0x54b1 runnable [0x00007f92d74f1000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.transfer(HashMap.java:601)
	at java.util.HashMap.resize(HashMap.java:581)
	at java.util.HashMap.addEntry(HashMap.java:879)
	at java.util.HashMap.put(HashMap.java:505)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
	at org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
	at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
	at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
	at org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
	at org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
	at org.apache.spark.scheduler.Task.run(Task.scala:53)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-55"" daemon prio=10 tid=0x00007f91f012f000 nid=0x54aa waiting on condition [0x00007f92d7bfa000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7df000b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-54"" daemon prio=10 tid=0x00007f91f012c800 nid=0x54a9 waiting on condition [0x00007f92d7cfb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7df000b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-53"" daemon prio=10 tid=0x00007f91f012a800 nid=0x54a8 waiting on condition [0x00007f92d7dfc000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7df000b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-52"" daemon prio=10 tid=0x00007f91f0128800 nid=0x54a7 waiting on condition [0x00007f92d7efd000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7df000b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-51"" daemon prio=10 tid=0x00007f91f0127000 nid=0x54a6 waiting on condition [0x00007f92d7ffe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7df000b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-50"" daemon prio=10 tid=0x00007f91f0120800 nid=0x54a5 waiting on condition [0x00007f92dc139000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7df000b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-49"" daemon prio=10 tid=0x00007f91f011f000 nid=0x54a4 waiting on condition [0x00007f92dc23a000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f9f7df000b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1108297380-48 Acceptor0 SocketConnector@0.0.0.0:57403"" daemon prio=10 tid=0x00007f91f011e000 nid=0x54a3 runnable [0x00007f92dc33b000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:530)
	at java.net.ServerSocket.accept(ServerSocket.java:498)
	at org.eclipse.jetty.server.bio.SocketConnector.accept(SocketConnector.java:117)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

""MAP_OUTPUT_TRACKER cleanup timer"" daemon prio=10 tid=0x00007f91f00f8000 nid=0x54a2 in Object.wait() [0x00007f92dc43c000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f9f7df200c8> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x00007f9f7df200c8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""HTTP_BROADCAST cleanup timer"" daemon prio=10 tid=0x00007f91f00f1000 nid=0x54a1 in Object.wait() [0x00007f92dc53d000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f9f7df280c8> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x00007f9f7df280c8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""BROADCAST_VARS cleanup timer"" daemon prio=10 tid=0x00007f91f00e1000 nid=0x549f in Object.wait() [0x00007f92dc73f000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f9f7df201e0> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x00007f9f7df201e0> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""BLOCK_MANAGER cleanup timer"" daemon prio=10 tid=0x00007f91f00df800 nid=0x549e in Object.wait() [0x00007f92dc840000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f9f7df202f8> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x00007f9f7df202f8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""connection-manager-thread"" daemon prio=10 tid=0x00007f91f00d8000 nid=0x549d runnable [0x00007f92dc941000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00007f9f7df204e8> (a sun.nio.ch.Util$2)
	- locked <0x00007f9f7df20500> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00007f9f7df20470> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:283)
	at org.apache.spark.network.ConnectionManager$$anon$3.run(ConnectionManager.scala:98)

""SHUFFLE_BLOCK_MANAGER cleanup timer"" daemon prio=10 tid=0x00007f91f00a8800 nid=0x549c in Object.wait() [0x00007f92dca42000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f9f7df02660> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x00007f9f7df02660> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""Hashed wheel timer #2"" daemon prio=10 tid=0x00007f9198001000 nid=0x549b waiting on condition [0x00007f92dcb43000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503)
	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:745)

""New I/O server boss #12"" daemon prio=10 tid=0x00007f91a80df000 nid=0x549a runnable [0x00007f92dcc44000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00007f9f7df209f0> (a sun.nio.ch.Util$2)
	- locked <0x00007f9f7df20a08> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00007f9f7df20978> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""New I/O worker #11"" daemon prio=10 tid=0x00007f91a80b4800 nid=0x5499 runnable [0x00007f92dcd45000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00007f9f7df283a8> (a sun.nio.ch.Util$2)
	- locked <0x00007f9f7df28390> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00007f9f7df283c0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""New I/O worker #10"" daemon prio=10 tid=0x00007f91a808a000 nid=0x5498 runnable [0x00007f92dce46000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	-"
"
Nicholas Chammas <nicholas.chammas@gmail.com>,Tue"," 15 Jul 2014 01:32:16 -0400""",Re: ec2 clusters launched at 9fe693b5b6 are broken (?),dev <dev@spark.apache.org>,"Okie doke--added myself as a watcher on that issue.

EC2 clusters and running tests against them? It would probably be way too
cumbersome to do that for every build, but perhaps on some schedule it
could help validate that we are still deploying EC2 clusters correctly.

Would something like that be valuable?

Nick



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 14 Jul 2014 23:02:46 -0700",Re: SBT gen-idea doesn't work well after merging SPARK-1776,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks DB,

Feel free to file sub-jira's under:
https://issues.apache.org/jira/browse/SPARK-2487

I've been importing the Maven build into Intellij, it might be worth
trying that as well to see if it works.

- Patrick


"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 15 Jul 2014 10:12:33 +0200",Spark-summingbird,"""dev@spark.apache.org"" <dev@spark.apache.org>","Seems Twitter has made a bit of progress here:
https://github.com/twitter/summingbird/tree/develop/summingbird-spark

May be of interest and perhaps some devs with experience in both may be
able to help out.

N
"
Reynold Xin <rxin@databricks.com>,"Tue, 15 Jul 2014 02:01:34 -0700",Re: better compression codecs for shuffle blocks?,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI dev,

I submitted a PR making Snappy as the default compression codec:
https://github.com/apache/spark/pull/1415

Also submitted a separate PR to add lz4 support:
https://github.com/apache/spark/pull/1416



"
Sean Owen <sowen@cloudera.com>,"Tue, 15 Jul 2014 11:03:21 +0100",Re: Catalyst dependency on Spark Core,user@spark.apache.org,"Agree. You end up with a ""core"" and a ""corer core"" to distinguish
between and it ends up just being more complicated. This sounds like
something that doesn't need a module.


"
Cody Koeninger <cody.koeninger@mediacrossing.com>,"Tue, 15 Jul 2014 09:15:01 -0500",traveling next week,dev@spark.apache.org,"I'm going to be on a plane wed 23, return flight monday 28, so will miss
daily call those days.  I'll be pushing forward on projects as I can, but
skype availability may be limited, so email if you need something from me.
"
andy petrella <andy.petrella@gmail.com>,"Tue, 15 Jul 2014 19:11:01 +0200","[brainsotrming] Generalization of DStream, a ContinuousRDD ?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Dear Sparkers,

*[sorry for the lengthy email... => head to the gist
<https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a preview
:-p**]*

I would like to share some thinking I had due to a use case I faced.
Basically, as the subject announced it, it's a generalization of the
DStream currently available in the streaming project.
First of all, I'd like to say that it's only a result of some personal
thinking, alone in the dark with a use case, the spark code, a sheet of
paper and a poor pen.


DStream is a very great concept to deal with micro-batching use cases, and
it does it very well too!
Also, it hardly relies on the elapsing time to create its internal
micro-batches.
However, there are similar use cases where we need micro-batches where this
constraint on the time doesn't hold, here are two of them:
* a micro-batch has to be created every *n* events received
* a micro-batch has to be generate based on the values of the items pushed
by the source (which might even not be a stream!).

An example of use case (mine ^^) would be
* the creation of timeseries from a cold source containing timestamped
events (like S3).
* one these timeseries have cells being the mean (sum, count, ...) of one
of the fields of the event
* the mean has to be computed over a window depending on a field *timestamp*.

* a timeserie is created for each type of event (the number of types is
high)
So, in this case, it'd be interesting to have an RDD for each cell, which
will generate all cells for all neede timeseries.
It's more or less what DStream does, but here it won't help due what was
stated above.

That's how I came to a raw sketch of what could be named ContinuousRDD
(CRDD) which is basically and RDD[RDD[_]]. And, for the sake of simplicity
I've stuck with the definition of a DStream to think about it. Okay, let's
go ^^.


Looking at the DStream contract, here is something that could be drafted
around CRDD.
A *CRDD* would be a generalized concept that relies on:
* a reference space/continuum (to which data can be bound)
* a binning function that can breaks the continuum into splits.
Since *Space* is a continuum we could define it as:
* a *SpacePoint* (the origin)
* a SpacePoint=>SpacePoint (the continuous function)
* a Ordering[SpacePoint]

DStream uses a *JobGenerator* along with a DStreamGraph, which are using
timer and clock to do their work, in the case of a CRDD we'll have to
define also a point generator, as a more generic but also adaptable
concept.


So far (so good?), these definition should work quite fine for *ordered* space
for which:
* points are coming/fetched in order
* the space is fully filled (no gaps)
For these cases, the JobGenerator (f.i.) could be defined with two extra
functions:
* one is responsible to chop the batches even if the upper bound of the
batch hasn't been seen yet
* the other is responsible to handle outliers (and could wrap them into yet
another CRDD ?)


I created a gist here wrapping up the types and thus the skeleton of this
idea, you can find it here:
https://gist.github.com/andypetrella/12228eb24eea6b3e1389

WDYT?
*The answer can be: you're a fool!*
Actually, I already I am, but also I like to know why.... so some
explanations will help me :-D.

Thanks to read 'till this point.

Greetz,



 aℕdy ℙetrella
about.me/noootsab
[image: aℕdy ℙetrella on about.me]

<http://about.me/noootsab>
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 15 Jul 2014 10:36:56 -0700",Re: traveling next week,"""dev@spark.apache.org"" <dev@spark.apache.org>","Cody - did you mean to send this to the spark dev list?


"
Cody Koeninger <cody@koeninger.org>,"Tue, 15 Jul 2014 12:40:45 -0500",Re: traveling next week,dev@spark.apache.org,"No, sorry for the mixup, it was a ""helpful"" autocomplete similarity between
an internal work list and the spark dev list
:(

Switched my spark mailing list subscription back to my personal email so
you guys won't be subjected to further unwanted email.



"
Cody Koeninger <cody@koeninger.org>,"Tue, 15 Jul 2014 12:44:11 -0500","Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097",dev@spark.apache.org,"We tested that patch from aarondav's branch, and are no longer seeing that
deadlock.  Seems to have solved the problem, at least for us.



"
yao <yaoshengzhe@gmail.com>,"Tue, 15 Jul 2014 11:43:56 -0700",Re: Hadoop's Configuration object isn't threadsafe,dev@spark.apache.org,"Good catch Andrew. In addition to your proposed solution, is that possible
to fix Configuration class and make it thread-safe ? I think the fix should
be trivial, just use a ConcurrentHashMap, but I am not sure if we can push
this change upstream (will hadoop guys accept this change ? for them, it
seems they never expect Configuration object being accessed by multiple
threads).

-Shengzhe



s prone to
ch
t
y
at
ppens in
me.
ay
.
meNodeProxies.java:343)
ava:168)
29)
stem.java:125)
va:52)
)
(Executor.scala:211)
:42)
:41)
.java:1408)
1)
:1145)
a:615)
"
Andrew Ash <andrew@andrewash.com>,"Tue, 15 Jul 2014 15:56:29 -0400",Re: Hadoop's Configuration object isn't threadsafe,dev@spark.apache.org,"Hi Shengzhe,

Even if we did make Configuration threadsafe, it'd take quite some time for
that to trickle down to a Hadoop release that we could actually rely on
Spark users having installed.  I agree we should consider whether making
Configuration threadsafe is something that Hadoop should do, but for the
short term I think Spark needs to be able to handle the common scenario of
Configuration being single-threaded.

Thanks!
Andrew



e
ld
h
:
k
 is prone
l.
p,
re
t
me
t
happens in
es
re
r
h
be
le
00
meNodeProxies.java:343)
ava:168)
29)
stem.java:125)
)
va:52)
6)
(Executor.scala:211)
:42)
:41)
.java:1408)
1)
:1145)
a:615)
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 15 Jul 2014 14:20:17 -0700",Re: Hadoop's Configuration object isn't threadsafe,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Andrew,

Cloning the conf this might be a good/simple fix for this particular
problem. It's definitely worth looking into.

There are a few things we can probably do in Spark to deal with
non-thread-safety inside of the Hadoop FileSystem and Configuration
locations where we knowingly access Hadoop FileSystem and
Configuration state from multiple threads (e.g. during our own calls
to getRecordReader in this case). But this will only deal with ""writer
writer"" conflicts where we had multiple calls mutating the same object
at the same time. It won't deal with ""reader writer"" conflicts where
some of our initialization code touches state that is needed during
normal execution of other tasks.

- Patrick


"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 15 Jul 2014 15:33:19 -0700","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Very interesting ideas Andy!

Conceptually i think it makes sense. In fact, it is true that dealing with
time series data, windowing over application time, windowing over number of
events, are things that DStream does not natively support. The real
challenge is actually mapping the conceptual windows with the underlying
within the RDDs of the DStream. Another fundamental aspect is the fact that
RDDs as parallel collections, with no well-defined ordering in the records
in the RDDs. If you want to process the records in an RDD as a ordered
stream of events, you kind of have to process the stream sequentially,
which means you have to process each RDD partition one-by-one, and
therefore lose the parallelism. So implementing all these functionality may
mean adding functionality at the cost of performance. Whether that is okay
for Spark Streaming to have these OR this tradeoff is not-intuitive for
end-users and therefore should not come out-of-the-box with Spark Streaming
-- that is a definitely a question worth debating upon.

That said, for some limited usecases, like windowing over N events, can be
implemented using custom RDDs like SlidingRDD
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/rdd/SlidingRDD.scala>
without
losing parallelism. For things like app time based windows, and
random-application-event based windows, its much harder.

Interesting ideas nonetheless. I am curious to see how far we can push
using the RDD model underneath, without losing parallelism and performance.

TD




d
is
d
y
s
et
"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 15 Jul 2014 15:33:19 -0700","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Very interesting ideas Andy!

Conceptually i think it makes sense. In fact, it is true that dealing with
time series data, windowing over application time, windowing over number of
events, are things that DStream does not natively support. The real
challenge is actually mapping the conceptual windows with the underlying
within the RDDs of the DStream. Another fundamental aspect is the fact that
RDDs as parallel collections, with no well-defined ordering in the records
in the RDDs. If you want to process the records in an RDD as a ordered
stream of events, you kind of have to process the stream sequentially,
which means you have to process each RDD partition one-by-one, and
therefore lose the parallelism. So implementing all these functionality may
mean adding functionality at the cost of performance. Whether that is okay
for Spark Streaming to have these OR this tradeoff is not-intuitive for
end-users and therefore should not come out-of-the-box with Spark Streaming
-- that is a definitely a question worth debating upon.

That said, for some limited usecases, like windowing over N events, can be
implemented using custom RDDs like SlidingRDD
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/rdd/SlidingRDD.scala>
without
losing parallelism. For things like app time based windows, and
random-application-event based windows, its much harder.

Interesting ideas nonetheless. I am curious to see how far we can push
using the RDD model underneath, without losing parallelism and performance.

TD




d
is
d
y
s
et
"
Baofeng Zhang <pelickzhang@qq.com>,"Tue, 15 Jul 2014 19:30:03 -0700 (PDT)",Re: Catalyst dependency on Spark Core,dev@spark.incubator.apache.org,"Is Matei following this?

Catalyst uses the Utils to get the ClassLoader which loaded Spark.

Can Catalyst directly do ""getClass.getClassLoader"" to avoid the dependency
on core?



--

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 15 Jul 2014 19:31:56 -0700",Re: Catalyst dependency on Spark Core,dev@spark.apache.org,"Yeah, that seems like something we can inline :).


dependency
http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Catalyst-dependency-on-Spark-Core-tp7303p7358.html
Nabble.com.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 15 Jul 2014 19:31:56 -0700",Re: Catalyst dependency on Spark Core,dev@spark.apache.org,"Yeah, that seems like something we can inline :).


dependency
http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Catalyst-dependency-on-Spark-Core-tp7303p7358.html
Nabble.com.


"
Baofeng Zhang <pelickzhang@qq.com>,"Tue, 15 Jul 2014 19:57:17 -0700 (PDT)",Re: Catalyst dependency on Spark Core,dev@spark.incubator.apache.org,"I see. 

So how about let me do this simple work to make my contribution :)

It is cooool.



--

"
qingyang li <liqingyang1985@gmail.com>,"Wed, 16 Jul 2014 15:06:22 +0800","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"let's me describe my scene:
----------------------
i have 8 machines (24 core , 16G memory, per machine) of spark cluster and
when i run query sql on shark,   it will cost 2.43s,  but when i create the
same table on spark memory , i run  the same sql , it will cost 1.56s.
 data on tachyon cost more time than data on spark memory.   they all have
150 map process,  and per node 16-20 map process.
I think the reason is that when data is on tachyon, shark will let spark
slave load data from tachyon salve which is on the same node with tachyon
slave,
i have tried to set some configuration to tune shark and tachyon, but still
can not make the former more fast than 2.43s.
do anyone have some ideas ?

By the way ,  my tachyon block size is 1GB now,  i want to reset block size
,  will it work by setting tachyon.user.default.block.size.byte=8M ?  if
not,  what does tachyon.user.default.block.size.byte mean?


2014-07-14 13:13 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:

"
andy petrella <andy.petrella@gmail.com>,"Wed, 16 Jul 2014 11:37:52 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Heya TD,

Thanks for the detailed answer! Much appreciated.

Regarding order among elements within an RDD, you're definitively right,
it'd kill the //ism and would require synchronization which is completely
avoided in distributed env.

That's why, I won't push this constraint to the RDDs themselves actually,
only the Space is something that *defines* ordered elements, and thus there
are two functions that will break the RDDs based on a given (extensible,
plugable) heuristic f.i.
Since the Space is rather decoupled from the data, thus the source and the
partitions, it's the responsibility of the CRRD implementation to dictate
how (if necessary) the elements should be sorted in the RDDs... which will
require some shuffles :-s -- Or the couple (source, space) is something
intrinsically ordered (like it is for DStream).

To be more concrete an RDD would be composed of un-ordered iterator of
millions of events for which all timestamps land into the same time
interval.

WDYT, would that makes sense?

thanks again for the answer!

greetz

 aℕdy ℙetrella
about.me/noootsab
[image: aℕdy ℙetrella on about.me]

<http://about.me/noootsab>


m

h
of
at
s
ay
y
ng
e
he/spark/mllib/rdd/SlidingRDD.scala
e.
ne
ch
s
d
g
*
a
is
"
andy petrella <andy.petrella@gmail.com>,"Wed, 16 Jul 2014 11:37:52 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Heya TD,

Thanks for the detailed answer! Much appreciated.

Regarding order among elements within an RDD, you're definitively right,
it'd kill the //ism and would require synchronization which is completely
avoided in distributed env.

That's why, I won't push this constraint to the RDDs themselves actually,
only the Space is something that *defines* ordered elements, and thus there
are two functions that will break the RDDs based on a given (extensible,
plugable) heuristic f.i.
Since the Space is rather decoupled from the data, thus the source and the
partitions, it's the responsibility of the CRRD implementation to dictate
how (if necessary) the elements should be sorted in the RDDs... which will
require some shuffles :-s -- Or the couple (source, space) is something
intrinsically ordered (like it is for DStream).

To be more concrete an RDD would be composed of un-ordered iterator of
millions of events for which all timestamps land into the same time
interval.

WDYT, would that makes sense?

thanks again for the answer!

greetz

 aℕdy ℙetrella
about.me/noootsab
[image: aℕdy ℙetrella on about.me]

<http://about.me/noootsab>


m

h
of
at
s
ay
y
ng
e
he/spark/mllib/rdd/SlidingRDD.scala
e.
ne
ch
s
d
g
*
a
is
"
rapelly kartheek <kartheek.mbms@gmail.com>,"Wed, 16 Jul 2014 21:22:01 +0530",Resource allocations,dev@spark.apache.org,"Hi,

I am trying to understand how the resource allocation happens in spark. I
understand the resourceOffer method in taskScheduler. This method takes
care of locality factor while allocating the resources. This resourceOffer
method gets invoked by the corresponding cluster manager.

I am working on stand-alone spark cluster. But I am not able to locate
starting point of the resource allocation. I want to understand from end to
end how exactly resource allocations happens, given a new Application to
the Spark. Also, apart from mesos, there is a folder called Local in the
Scheduler. Can someone tell me which all files should I look into to
understand the resource allocation in stand-alone spark cluster and what is
this ""Local"" for?

Thanks in advance!!
Karthik.
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Wed, 16 Jul 2014 10:30:18 -0700",Re: Resource allocations,dev@spark.apache.org,"Hi Karthik,

The resourceOffer() method is invoked from a class implementing the
SchedulerBackend interface; in the case of a standalone cluster, it's
invoked from a CoarseGrainedSchedulerBackend (in the makeOffers() method).
 If you look in TaskSchedulerImpl.submitTasks(), it calls
backend.reviveOffers() at the end -- which signals to the SchedulerBackend
that it should offer some resources.  This somewhat confusing interface
exists for historical reasons: Spark originally used only Mesos for
scheduling, which relies on resource offers for scheduling, and so
standalone mode was done in a similar way to maximize code re-use.

The scheduler/local folder contains code specific to running in local mode.
 With local mode, all code runs in a single JVM (as opposed to having one
JVM for the driver, one for the cluster master, and one for each worker).
 This mode is typically used for testing.

-Kay


"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 16 Jul 2014 15:31:08 -0700","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"I think it makes sense, though without a concrete implementation its hard
to be sure. Applying sorting on the RDD according to the RDDs makes sense,
but I can think of two kinds of fundamental problems.

1. How do you deal with ordering across RDD boundaries. Say two consecutive
RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2, 3,
4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function through
all these records in the timestamp order. I am curious to find how this
problem can be solved without sacrificing efficiency (e.g. I can imagine
doing multiple pass magic)

2. An even more fundamental question is how do you ensure ordering with
delayed records. If you want to process in order of application time, and
records are delayed how do you deal with them.

Any ideas? ;)

TD




re
e
l
r
g
he/spark/mllib/rdd/SlidingRDD.scala
m
l
of
,
e
d
is
D
he
to
"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 16 Jul 2014 15:31:08 -0700","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"I think it makes sense, though without a concrete implementation its hard
to be sure. Applying sorting on the RDD according to the RDDs makes sense,
but I can think of two kinds of fundamental problems.

1. How do you deal with ordering across RDD boundaries. Say two consecutive
RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2, 3,
4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function through
all these records in the timestamp order. I am curious to find how this
problem can be solved without sacrificing efficiency (e.g. I can imagine
doing multiple pass magic)

2. An even more fundamental question is how do you ensure ordering with
delayed records. If you want to process in order of application time, and
records are delayed how do you deal with them.

Any ideas? ;)

TD




re
e
l
r
g
he/spark/mllib/rdd/SlidingRDD.scala
m
l
of
,
e
d
is
D
he
to
"
andy petrella <andy.petrella@gmail.com>,"Thu, 17 Jul 2014 01:11:19 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Indeed, these two cases are tightly coupled (the first one is a special
case of the second).

Actually, these ""outliers"" could be handled by a dedicated function what I
named outliersManager -- I was not so much inspired ^^, but we could name
these outliers, ""outlaws"" and thus the function would be ""sheriff"".
The purpose of this ""sheriff"" function would be to create yet another
distributed collection (RDD, CRDD, ...?) with only the --outliers-- outlaws
in it.

Because these problems have a nature which will be as different as the use
case will be, it's hard to find a generic way to tackle them. So, you
know... that's why... I put temporarily them in jail and wait for the judge
to show them the right path! (.... okay it's late in Belgium -- 1AM).

All in all, it's more or less what we would do in DStream as well actually.
Let me expand a bit this reasoning, let's assume that some data points can
come along with the time, but aren't in sync with it -- f.i., a device that
wakes up and send all it's data at once.
The DStream will package them into RDDs mixed-up with true current data
points, however, the logic of the job will have to use a 'Y' road :
* to integrate them into a database at the right place
* to simply drop them out because they're won't be part of a shown chart
* etc

In this case, the 'Y' road would be of the contract ;-), and so left at the
appreciation of the dev.

Another way, to do it would be to ignore but log them, but that would be
very crappy, non professional and useful (and of course I'm just kidding).

my0.002¢



 aℕdy ℙetrella
about.me/noootsab
[image: aℕdy ℙetrella on about.me]

<http://about.me/noootsab>


m

,
ve
3,
h
,
ly
y,
,
te
t
d
,
ty
or
an
he/spark/mllib/rdd/SlidingRDD.scala
h
.
e
t
of
s
t
,
to
f
"
andy petrella <andy.petrella@gmail.com>,"Thu, 17 Jul 2014 01:11:19 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Indeed, these two cases are tightly coupled (the first one is a special
case of the second).

Actually, these ""outliers"" could be handled by a dedicated function what I
named outliersManager -- I was not so much inspired ^^, but we could name
these outliers, ""outlaws"" and thus the function would be ""sheriff"".
The purpose of this ""sheriff"" function would be to create yet another
distributed collection (RDD, CRDD, ...?) with only the --outliers-- outlaws
in it.

Because these problems have a nature which will be as different as the use
case will be, it's hard to find a generic way to tackle them. So, you
know... that's why... I put temporarily them in jail and wait for the judge
to show them the right path! (.... okay it's late in Belgium -- 1AM).

All in all, it's more or less what we would do in DStream as well actually.
Let me expand a bit this reasoning, let's assume that some data points can
come along with the time, but aren't in sync with it -- f.i., a device that
wakes up and send all it's data at once.
The DStream will package them into RDDs mixed-up with true current data
points, however, the logic of the job will have to use a 'Y' road :
* to integrate them into a database at the right place
* to simply drop them out because they're won't be part of a shown chart
* etc

In this case, the 'Y' road would be of the contract ;-), and so left at the
appreciation of the dev.

Another way, to do it would be to ignore but log them, but that would be
very crappy, non professional and useful (and of course I'm just kidding).

my0.002¢



 aℕdy ℙetrella
about.me/noootsab
[image: aℕdy ℙetrella on about.me]

<http://about.me/noootsab>


m

,
ve
3,
h
,
ly
y,
,
te
t
d
,
ty
or
an
he/spark/mllib/rdd/SlidingRDD.scala
h
.
e
t
of
s
t
,
to
f
"
Chester Chen <chester@alpinenow.com>,"Wed, 16 Jul 2014 17:18:26 -0700",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, Sandy

    We do have some issue with this. The difference is in Yarn-Alpha and
Yarn Stable ( I noticed that in the latest build, the module name has
changed,
     yarn-alpha --> yarn
     yarn --> yarn-stable
)

For example:  MRJobConfig.class
the field:
""DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH""


In Yarn-Alpha : the field returns   java.lang.String[]

  java.lang.String[] DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;

while in Yarn-Stable, it returns a String

  java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;

So in ClientBaseSuite.scala

The following code:

    val knownDefMRAppCP: Seq[String] =
      getFieldValue[*String*, Seq[String]](classOf[MRJobConfig],

 ""DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"",
                                         Seq[String]())(a => *a.split("","")*)


works for yarn-stable, but doesn't work for yarn-alpha.

This is the only failure for the SNAPSHOT I downloaded 2 weeks ago.  I
believe this can be refactored to yarn-alpha module and make different
tests according different API signatures.

 I just update the master branch and build doesn't even compile for
Yarn-Alpha (yarn) model. Yarn-Stable compile with no error and test passed.


Does the Spark Jenkins job run against yarn-alpha ?





Here is output from yarn-alpha compilation:

I got the 40 compilation errors.

sbt/sbt clean yarn/test:compile

Using /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home as
default JAVA_HOME.

Note, this will be overridden by -java-home if it is set.

[info] Loading project definition from
/Users/chester/projects/spark/project/project

[info] Loading project definition from
/Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project

[warn] Multiple resolvers having different access mechanism configured with
same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).

[info] Loading project definition from /Users/chester/projects/spark/project

NOTE: SPARK_HADOOP_VERSION is deprecated, please use
-Dhadoop.version=2.0.5-alpha

NOTE: SPARK_YARN is deprecated, please use -Pyarn flag.

[info] Set current project to spark-parent (in build
file:/Users/chester/projects/spark/)

[success] Total time: 0 s, completed Jul 16, 2014 5:13:06 PM

[info] Updating {file:/Users/chester/projects/spark/}core...

[info] Resolving org.fusesource.jansi#jansi;1.4 ...

[info] Done updating.

[info] Updating {file:/Users/chester/projects/spark/}yarn...

[info] Updating {file:/Users/chester/projects/spark/}yarn-stable...

[info] Resolving org.fusesource.jansi#jansi;1.4 ...

[info] Done updating.

[info] Resolving commons-net#commons-net;3.1 ...

[info] Compiling 358 Scala sources and 34 Java sources to
/Users/chester/projects/spark/core/target/scala-2.10/classes...

[info] Resolving org.fusesource.jansi#jansi;1.4 ...

[info] Done updating.

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.scala:43:
constructor TaskAttemptID in class TaskAttemptID is deprecated: see
corresponding Javadoc for more information.

[warn]     new TaskAttemptID(jtIdentifier, jobId, isMap, taskId, attemptId)

[warn]     ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:501:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewHadoopJob(hadoopConfiguration)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:634:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewHadoopJob(conf)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:167:
constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
for more information.

[warn]         new TaskAttemptID(new TaskID(jID.value, true, splitID),
attemptID))

[warn]                           ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:188:
method makeQualified in class Path is deprecated: see corresponding Javadoc
for more information.

[warn]     outputPath.makeQualified(fs)

[warn]                ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:84:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]     if (!fs.getFileStatus(path).isDir) {

[warn]                                 ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:118:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]       val logDirs = if (logStatus != null)
logStatus.filter(_.isDir).toSeq else Seq[FileStatus]()

[warn]                                                               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:56:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]       if (file.isDir) 0L else file.getLen

[warn]                ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:110:
method getDefaultReplication in class FileSystem is deprecated: see
corresponding Javadoc for more information.

[warn]       fs.create(tempOutputPath, false, bufferSize,
fs.getDefaultReplication, blockSize)

[warn]                                                       ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:267:
constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
for more information.

[warn]     val taId = new TaskAttemptID(new TaskID(jobID, true, splitId),
attemptId)

[warn]                                  ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:767:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewAPIHadoopJob(hadoopConf)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:830:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewAPIHadoopJob(hadoopConf)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:185:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]           fileStatuses.filter(!_.isDir).map(_.getPath).toSeq

[warn]                                  ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new Job(conf)

[warn]               ^

[warn] 14 warnings found

[warn] Note:
/Users/chester/projects/spark/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
uses unchecked or unsafe operations.

[warn] Note: Recompile with -Xlint:unchecked for details.

[info] Compiling 15 Scala sources to
/Users/chester/projects/spark/yarn/stable/target/scala-2.10/classes...

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:26:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.YarnClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:40:
not found: value YarnClient

[error]   val yarnClient = YarnClient.createYarnClient

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:32:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:33:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:36:
object util is not a member of package org.apache.hadoop.yarn.webapp

[error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:63:
value RM_AM_MAX_ATTEMPTS is not a member of object
org.apache.hadoop.yarn.conf.YarnConfiguration

[error]     YarnConfiguration.RM_AM_MAX_ATTEMPTS,
YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS)

[error]                       ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:65:
not found: type AMRMClient

[error]   private var amClient: AMRMClient[ContainerRequest] = _

[error]                         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:91:
not found: value AMRMClient

[error]     amClient = AMRMClient.createAMRMClient()

[error]                ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:136:
not found: value WebAppUtils

[error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:40:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:618:
not found: type AMRMClient

[error]       amClient: AMRMClient[ContainerRequest],

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:596:
not found: type AMRMClient

[error]       amClient: AMRMClient[ContainerRequest],

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:577:
not found: type AMRMClient

[error]       amClient: AMRMClient[ContainerRequest],

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:452:
value CONTAINER_ID is not a member of object
org.apache.hadoop.yarn.api.ApplicationConstants.Environment

[error]     val containerIdString = System.getenv(
ApplicationConstants.Environment.CONTAINER_ID.name())

[error]
        ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:128:
value setTokens is not a member of
org.apache.hadoop.yarn.api.records.ContainerLaunchContext

[error]     amContainer.setTokens(ByteBuffer.wrap(dob.getData()))

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:36:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:37:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:39:
object util is not a member of package org.apache.hadoop.yarn.webapp

[error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:62:
not found: type AMRMClient

[error]   private var amClient: AMRMClient[ContainerRequest] = _

[error]                         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:99:
not found: value AMRMClient

[error]     amClient = AMRMClient.createAMRMClient()

[error]                ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:158:
not found: value WebAppUtils

[error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:31:
object ProtoUtils is not a member of package
org.apache.hadoop.yarn.api.records.impl.pb

[error] import org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils

[error]        ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:33:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.NMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:53:
not found: type NMClient

[error]   var nmClient: NMClient = _

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:59:
not found: value NMClient

[error]     nmClient = NMClient.createNMClient()

[error]                ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:79:
value setTokens is not a member of
org.apache.hadoop.yarn.api.records.ContainerLaunchContext

[error]     ctx.setTokens(ByteBuffer.wrap(dob.getData()))

[error]         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:35:
object ApplicationMasterProtocol is not a member of package
org.apache.hadoop.yarn.api

[error] import org.apache.hadoop.yarn.api.ApplicationMasterProtocol

[error]        ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:41:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:65:
not found: type AMRMClient

[error]     val amClient: AMRMClient[ContainerRequest],

[error]                   ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:389:
not found: type ContainerRequest

[error]     ): ArrayBuffer[ContainerRequest] = {

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:388:
not found: type ContainerRequest

[error]       hostContainers: ArrayBuffer[ContainerRequest]

[error]                                   ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:405:
not found: type ContainerRequest

[error]     val requestedContainers = new
ArrayBuffer[ContainerRequest](rackToCounts.size)

[error]                                               ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:434:
not found: type ContainerRequest

[error]     val containerRequests: List[ContainerRequest] =

[error]                                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:508:
not found: type ContainerRequest

[error]     ): ArrayBuffer[ContainerRequest] = {

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:446:
not found: type ContainerRequest

[error]         val hostContainerRequests = new
ArrayBuffer[ContainerRequest](preferredHostToCount.size)

[error]                                                     ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:458:
not found: type ContainerRequest

[error]         val rackContainerRequests: List[ContainerRequest] =
createRackResourceRequests(

[error]                                         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:467:
not found: type ContainerRequest

[error]         val containerRequestBuffer = new
ArrayBuffer[ContainerRequest](

[error]                                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:542:
not found: type ContainerRequest

[error]     ): ArrayBuffer[ContainerRequest] = {

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:545:
value newInstance is not a member of object
org.apache.hadoop.yarn.api.records.Resource

[error]     val resource = Resource.newInstance(memoryRequest,
executorCores)

[error]                             ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:550:
not found: type ContainerRequest

[error]     val requests = new ArrayBuffer[ContainerRequest]()

[error]                                    ^

[error] 40 errors found

[error] (yarn-stable/compile:compile) Compilation failed

[error] Total time: 98 s, completed Jul 16, 2014 5:14:44 PM














"
Yan Fang <yanfang724@gmail.com>,"Wed, 16 Jul 2014 17:38:06 -0700",Does RDD checkpointing store the entire state in HDFS?,dev@spark.apache.org,"Hi guys,

am wondering how the RDD checkpointing
<https://spark.apache.org/docs/latest/streaming-programming-guide.html#RDD
Checkpointing> works in Spark Streaming. When I use updateStateByKey, does
the Spark store the entire state (at one time point) into the HDFS or only
put the transformation into the HDFS? Thank you.

Best,

Fang, Yan
yanfang724@gmail.com
+1 (206) 849-4108
"
Chester Chen <chester@alpinenow.com>,"Wed, 16 Jul 2014 17:41:20 -0700",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hmm
looks like a Build script issue:

I run the command with :

sbt/sbt clean *yarn/*test:compile

but errors came from

[error] 40 errors found

[error] (*yarn-stable*/compile:compile) Compilation failed


Chester



"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 16 Jul 2014 18:53:36 -0700",Re: Does RDD checkpointing store the entire state in HDFS?,dev@spark.apache.org,"After every checkpointing interval, the latest state RDD is stored to HDFS
in its entirety. Along with that, the series of DStream transformations
that was setup with the streaming context is also stored into HDFS (the
whole DAG of DStream objects is serialized and saved).

TD



"
Chester Chen <chester@alpinenow.com>,"Wed, 16 Jul 2014 21:39:05 -0700",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Looking further, the yarn and yarn-stable are both for the stable version
of Yarn, that explains the compilation errors when using 2.0.5-alpha
version of hadoop.

the module yarn-alpha ( although is still on SparkBuild.scala), is no
longer there in sbt console.



[info] In file:/Users/chester/projects/spark/

[info]    assembly

[info]    bagel

[info]    catalyst

[info]    core

[info]    examples

[info]    graphx

[info]    hive

[info]    mllib

[info]    oldDeps

[info]    repl

[info]    spark

[info]    sql

[info]    streaming

[info]    streaming-flume

[info]    streaming-kafka

[info]    streaming-mqtt

[info]    streaming-twitter

[info]    streaming-zeromq

[info]    tools

[info]    yarn

[info]  * yarn-stable



"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Jul 2014 22:06:08 -0700",small (yet major) change going in: broadcasting RDD to reduce task size,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,

Want to give you guys a heads up that I'm working on a small (but major)
change with respect to how task dispatching works. Currently (as of Spark
1.0.1), Spark sends RDD object and closures using Akka along with the task
itself to the executors. This is however inefficient because all tasks in
the same stage use the same RDDs and closures, but we have to send these
closures and RDDs multiple times to the executors. This is especially bad
when some closure references some variable that is very large. The current
design led to users having to explicitly broadcast large variables.

The patch uses broadcast to send RDD objects and the closures to executors,
and use Akka to only send a reference to the broadcast RDD/closure along
with the partition specific information for the task. For those of you who
know more about the internals, Spark already relies on broadcast to send
the Hadoop JobConf every time it uses the Hadoop input, because the JobConf
is large.

The user-facing impact of the change include:

1. Users won't need to decide what to broadcast anymore
2. Task size will get smaller, resulting in faster scheduling and higher
task dispatch throughput.

In addition, the change will simplify some internals of Spark, removing the
need to maintain task caches and the complex logic to broadcast JobConf
(which also led to a deadlock recently).


Pull request attached: https://github.com/apache/spark/pull/1450
"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Jul 2014 22:07:08 -0700","Re: small (yet major) change going in: broadcasting RDD to reduce
 task size","""dev@spark.apache.org"" <dev@spark.apache.org>","Oops - the pull request should be https://github.com/apache/spark/pull/1452



"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 16 Jul 2014 22:12:25 -0700",Re: small (yet major) change going in: broadcasting RDD to reduce task size,dev@spark.apache.org,"Hey Reynold, just to clarify, users will still have to manually broadcast objects that they want to use *across* operations (e.g. in multiple iterations of an algorithm, or multiple map functions, or stuff like that). But they won't have to broadcast something they only use once.

Matei


https://github.com/apache/spark/pull/1452
major)
Spark
task
tasks in
these
bad
current
task. For
on
input,
higher
removing
JobConf


"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Jul 2014 22:14:22 -0700","Re: small (yet major) change going in: broadcasting RDD to reduce
 task size","""dev@spark.apache.org"" <dev@spark.apache.org>","Yup - that is correct.  Thanks for clarifying.



"
Stephen Haberman <stephen.haberman@gmail.com>,"Thu, 17 Jul 2014 00:23:32 -0500","Re: small (yet major) change going in: broadcasting RDD to reduce
 task size",Reynold Xin <rxin@databricks.com>,"
Wow. Great writeup.

I keep tabs on several open source projects that we use heavily, and
I'd be ecstatic if more major changes were this well/succinctly
explained instead of the usual ""just read the commit message/diff"".

- Stephen


"
Andrew Ash <andrew@andrewash.com>,"Thu, 17 Jul 2014 01:24:10 -0400",Re: Hadoop's Configuration object isn't threadsafe,dev@spark.apache.org,"Hi Patrick, thanks for taking a look.  I filed as
https://issues.apache.org/jira/browse/SPARK-2546

Would you recommend I pursue the cloned Configuration object approach now
and send in a PR?

Reynold's recent announcement of the broadcast RDD object patch may also
have implications of the right path forward here.  I'm not sure I fully
understand the implications though:
https://github.com/apache/spark/pull/1452

HadoopRDD.""

Thanks!
Andrew



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 16 Jul 2014 23:14:21 -0700",Re: Hadoop's Configuration object isn't threadsafe,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Andrew,

I think you are correct and a follow up to SPARK-2521 will end up
fixing this. The desing of SPARK-2521 automatically broadcasts RDD
data in tasks and the approach creates a new copy of the RDD and
associated data for each task. A natural follow-up to that patch is to
stop handling the jobConf separately (since we will now broadcast all
referents of the RDD itself) and just have it broadcasted with the
RDD. I'm not sure if Reynold plans to include this in SPARK-2521 or
afterwards, but it's likely we'd do that soon.

- Patrick


"
Andrew Ash <andrew@andrewash.com>,"Thu, 17 Jul 2014 02:24:59 -0400",Re: Hadoop's Configuration object isn't threadsafe,dev@spark.apache.org,"Sounds good -- I added comments to the ticket.

Since SPARK-2521 is scheduled for a 1.1.0 release and we can work around
with spark.speculation, I don't personally see a need for a 1.0.2 backport.

Thanks looking through this issue!



"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Jul 2014 08:40:05 +0100",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Are you setting -Pyarn-alpha? ./sbt/sbt -Pyarn-alpha, followed by
""projects"", shows it as a module. You should only build yarn-stable
*or* yarn-alpha at any given time.

I don't remember the modules changing in a while. 'yarn-alpha' is for
YARN before it stabilized, circa early Hadoop 2.0.x. 'yarn-stable' is
for beta and stable YARN, circa late Hadoop 2.0.x and onwards. 'yarn'
is code common to both, so should compile with yarn-alpha.

What's the compile error, and are you setting yarn.version? the
default is to use hadoop.version, but that defaults to 1.0.4 and there
is no such YARN.

Unless I missed it, I only see compile errors in yarn-stable, and you
are trying to compile vs YARN alpha versions no?


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 17 Jul 2014 00:44:59 -0700",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","To add, we've made some effort to yarn-alpha to work with the 2.0.x line,
but this was a time when YARN went through wild API changes.  The only line
that the yarn-alpha profile is guaranteed to work against is the 0.23 line.



"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 17 Jul 2014 03:16:41 -0700",[VOTE] Release Apache Spark 0.9.2 (RC1),dev@spark.apache.org,"Please vote on releasing the following candidate as Apache Spark version 0.9.2!

The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~meng/spark-0.9.2-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/meng.asc

The staging repository for this release can be found at:
https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/

The documentation corresponding to this release can be found at:
http://people.apache.org/~meng/spark-0.9.2-rc1-docs/

Please vote on releasing this package as Apache Spark 0.9.2!

The vote is open until Sunday, July 20, at 11:10 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.2
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

=== About this release ===
This release fixes a few high-priority bugs in 0.9.1 and has a variety
of smaller fixes. The full list is here: http://s.apache.org/d0t. Some
of the more visible patches are:

SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size
SPARK-1676: HDFS FileSystems continually pile up in the FS cache
SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
SPARK-1870: Secondary jars are not added to executor classpath for YARN

This is the second maintenance release on the 0.9 line. We plan to make
additional maintenance releases as new fixes come in.

Best,
Xiangrui

"
Chester Chen <chester@alpinenow.com>,"Thu, 17 Jul 2014 07:24:07 -0700",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","@Sean and @Sandy

   Thanks for the reply. I used to be able to see yarn-alpha and yarn
directories which corresponding to the modules.

   I guess due to the recent SparkBuild.scala changes, I did not see
yarn-alpha (by default) and I thought yarn-alpha "
Nathan Kronenfeld <nkronenfeld@oculusinfo.com>,"Thu, 17 Jul 2014 10:43:12 -0400",Compile error when compiling for cloudera,dev@spark.incubator.apache.org,"I'm trying to compile the latest code, with the hadoop-version set for
2.0.0-mr1-cdh4.6.0.

I'm getting the following error, which I don't get when I don't set the
hadoop version:

[error]
/data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
overloaded method constructor NioServerSocketChannelFactory with
alternatives:
[error]   (x$1: java.util.concurrent.Executor,x$2:
java.util.concurrent.Executor,x$3:
Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory <and>
[error]   (x$1: java.util.concurrent.Executor,x$2:
java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
[error]  cannot be applied to ()
[error]       val channelFactory = new NioServerSocketChannelFactory
[error]                            ^
[error] one error found


I don't know flume from a hole in the wall - does anyone know what I can do
to fix this?


Thanks,
         -Nathan


-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com
"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Jul 2014 15:56:05 +0100",Re: Compile error when compiling for cloudera,"""dev@spark.apache.org"" <dev@spark.apache.org>","This looks like a Jetty version problem actually. Are you bringing in
something that might be changing the version of Jetty used by Spark?
It depends a lot on how you are building things.

Good to specify exactly how your'e building here.


"
Nathan Kronenfeld <nkronenfeld@oculusinfo.com>,"Thu, 17 Jul 2014 10:58:37 -0400",Re: Compile error when compiling for cloudera,dev@spark.apache.org,"My full build command is:
./sbt/sbt -Dhadoop.version=2.0.0-mr1-cdh4.6.0 clean assembly


I've changed one line in RDD.scala, nothing else.







-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com
"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Jul 2014 15:58:20 +0100",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Looks like a real problem. I see it too. I think the same workaround
found in ClientBase.scala needs to be used here. There, the fact that
this field can be a String or String[] is handled explicitly. In fact
I think you can just call to ClientBase for this? PR it, I say.


"
Nathan Kronenfeld <nkronenfeld@oculusinfo.com>,"Thu, 17 Jul 2014 10:59:41 -0400",Re: Compile error when compiling for cloudera,dev@spark.apache.org,"er, that line being in toDebugString, where it really shouldn't affect
anything (no signature changes or the like)






-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com
"
Yan Fang <yanfang724@gmail.com>,"Thu, 17 Jul 2014 09:01:50 -0700",Re: Does RDD checkpointing store the entire state in HDFS?,dev@spark.apache.org,"Thank you, TD !

Fang, Yan
yanfang724@gmail.com
+1 (206) 849-4108



"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Jul 2014 17:25:03 +0100",Re: Compile error when compiling for cloudera,"""dev@spark.apache.org"" <dev@spark.apache.org>, Ted Malaska <ted.malaska@cloudera.com>","CC tmalaska since he touched the line in question. This is a fun one.
So, here's the line of code added last week:

val channelFactory = new NioServerSocketChannelFactory
  (Executors.newCachedThreadPool(), Executors.newCachedThreadPool());

Scala parses this as two statements, one invoking a no-arg constructor
and one making a tuple for fun. Put it on one line and it's fine.

It works with newer Netty since there is a no-arg constructor. It
fails with older Netty, which is what you get with older Hadoop.

The fix is obvious. I'm away and if nobody beats me to a PR in the
meantime, I'll propose one as an addendum to the recent JIRA.

Sean

*


"
Ted Malaska <ted.malaska@cloudera.com>,"Thu, 17 Jul 2014 12:32:56 -0400",Re: Compile error when compiling for cloudera,Sean Owen <sowen@cloudera.com>,"Don't make this change yet.  I have a 1642 that needs to get through around
the same code.

I can make this change after 1642 is through.



"
Chester Chen <chester@alpinenow.com>,"Thu, 17 Jul 2014 10:03:54 -0700",Re: Possible bug in ClientBase.scala?,"""dev@spark.apache.org"" <dev@spark.apache.org>","OK   I will create PR.

thanks




"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Jul 2014 18:25:42 +0100",Re: Compile error when compiling for cloudera,Ted Malaska <ted.malaska@cloudera.com>,"Should be an easy rebase for your PR, so I went ahead just to get this fixed up:

https://github.com/apache/spark/pull/1466


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Jul 2014 14:09:18 -0400","Re: small (yet major) change going in: broadcasting RDD to reduce
 task size",dev <dev@spark.apache.org>,"

Ditto on that. The summary of user impact was very nice. It would be good
to repeat that on the user list or release notes when this change goes out.

Nick
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 17 Jul 2014 11:12:30 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),dev@spark.apache.org,"I start the voting with a +1.

Ran tests on the release candidates and some basic operations in
spark-shell and pyspark (local and standalone).

-Xiangrui


"
"""Nick R. Katsipoulakis"" <katsip@cs.pitt.edu>","Thu, 17 Jul 2014 14:27:20 -0700",InputSplit and RecordReader control on HadoopRDD,dev@spark.apache.org,"Hello,

I am currently trying to extend some custom InputSplit and RecordReader
classes to provide to SparkContext's hadoopRDD() function.

My question is the following:

Does the value returned by InpuSplit.getLenght() and/or
RecordReader.getProgress() affect the execution of a map() function in the
Spark runtime?

I am asking because I have used these two custom classes on Hadoop and they
do not cause any problems. However, in Spark, I see that new InputSplit
objects are generated during runtime. To be more precise:

In the beginning, I see in my log file that an InputSplit object is
generated and the RecordReader object associated to it is fetching records.
At some point, the job that is handling the previous InputSplit stops, and
a new one is spawned with a new InputSplit. I do not understand why this is
happening?

Any help?

Thank you,
Nick

P.S.-1 : I am sorry for posting my question on the Developer Mailing List,
but I could not find anything similar in the User's list. Also, I really
need to understand the runtime of Spark and I believe that in the
developer's list my question will be read by contributors of Spark.

P.S.-2: I can provide more technical details if they are needed.
"
Stephen Boesch <javadba@gmail.com>,"Thu, 17 Jul 2014 16:00:26 -0700",Current way to include hive in a build,dev@spark.apache.org,"Having looked at trunk make-distribution.sh the --with-hive and --with-yarn
are now deprecated.

Here is the way I have built it:

Added to pom.xml:

   <profile>
      <id>cdh5</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <hadoop.version>2.3.0-cdh5.0.0</hadoop.version>
        <yarn.version>2.3.0-cdh5.0.0</yarn.version>
        <hbase.version>0.96.1.1-cdh5.0.0</hbase.version>
        <zookeeper.version>3.4.5-cdh5.0.0</zookeeper.version>
      </properties>
    </profile>

*mvn -Pyarn -Pcdh5 -Phive -Dhadoop.version=2.3.0-cdh5.0.1
-Dyarn.version=2.3.0-cdh5.0.0 -DskipTests clean package*


[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM .......................... SUCCESS [3.165s]
[INFO] Spark Project Core ................................ SUCCESS
[2:39.504s]
[INFO] Spark Project Bagel ............................... SUCCESS [7.596s]
[INFO] Spark Project GraphX .............................. SUCCESS [22.027s]
[INFO] Spark Project ML Library .......................... SUCCESS [36.284s]
[INFO] Spark Project Streaming ........................... SUCCESS [24.309s]
[INFO] Spark Project Tools ............................... SUCCESS [3.147s]
[INFO] Spark Project Catalyst ............................ SUCCESS [20.148s]
[INFO] Spark Project SQL ................................. SUCCESS [18.560s]
*[INFO] Spark Project Hive ................................ FAILURE
[33.962s]*

[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
(copy-dependencies) on project spark-hive_2.10: Execution copy-dependencies
of goal
org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
failed: Plugin org.apache.maven.plugins:maven-dependency-plugin:2.4 or one
of its dependencies could not be resolved: Could not find artifact
commons-logging:commons-logging:jar:1.0.4 -> [Help 1]

Anyone who is presently building with -Phive and has a suggestion for this?
"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Thu, 17 Jul 2014 17:31:04 -0700 (PDT)",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.incubator.apache.org,"Hi all, 

Cool discussion! I agree that a more standardized API for clustering, and
easy access to underlying routines, would be useful (we've also been
discussing this when trying to develop streaming clustering algorithms,
similar to https://github.com/apache/spark/pull/1361) 

For divisive, hierarchical clustering I implemented something awhile back,
here's a gist. 

https://gist.github.com/freeman-lab/5947e7c53b368fe90371

It does bisecting k-means clustering (with k=2), with a recursive class for
keeping track of the tree. I also found this much better than agglomerative
methods (for the reasons Hector points out).

This needs to be cleaned up, and can surely be optimized (esp. by replacing
the core KMeans step with existing MLLib code), but I can say I was running
it successfully on quite large data sets. 

RJ, depending on where you are in your progress, I'd be happy to help work
on this piece and / or have you use this as a jumping off point, if useful. 

-- Jeremy 



--

"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 17 Jul 2014 18:36:07 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),dev@spark.apache.org,"+1

Tested on Mac, verified CHANGES.txt is good, verified several of the bug fixes.

Matei


version 0.9.2!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
at:
https://repository.apache.org/service/l"
DB Tsai <dbtsai@dbtsai.com>,"Thu, 17 Jul 2014 18:52:35 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),dev@spark.apache.org,"+1

Tested with my Ubuntu Linux.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Will Benton <willb@redhat.com>,"Thu, 17 Jul 2014 22:51:04 -0400 (EDT)","preferred Hive/Hadoop environment for generating golden test
 outputs",dev@spark.apache.org,"Hi all,

What's the preferred environment for generating golden test outputs for new Hive tests?  In particular:

* what Hadoop version and Hive version should I be using,
* are there particular distributions people have run successfully, and 
* are there any system properties or environment variables (beyond HADOOP_HOME, HIVE_HOME, and HIVE_DEV_HOME) I need to set before running the suite?

I ask because I'm getting some errors while trying to add new tests and would like to eliminate any possible problems caused by differences between what my environment offers and what Spark expects.  (I'm currently running with the Fedora packages for Hadoop 2.2.0 and a locally-built Hive 0.12.0.)  Since I'll only be using this for generating test outputs, something as simple to set up as possible would be great.



thanks,
wb

"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Jul 2014 19:52:36 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>",1
Zongheng Yang <zongheng.y@gmail.com>,"Thu, 17 Jul 2014 20:04:14 -0700",Re: preferred Hive/Hadoop environment for generating golden test outputs,dev@spark.apache.org,"Hi Will,

These three environment variables are needed [1].

I have had success with Hive 0.12 and Hadoop 1.0.4. For Hive, getting
the source distribution seems to be required. Docs contribution will
be much appreciated!

[1] https://github.com/apache/spark/tree/master/sql#other-dependencies-for-developers

Zongheng

ew Hive tests?  In particular:
_HOME, HIVE_HOME, and HIVE_DEV_HOME) I need to set before running the suite?
ould like to eliminate any possible problems caused by differences between what my environment offers and what Spark expects.  (I'm currently running with the Fedora packages for Hadoop 2.2.0 and a locally-built Hive 0.12.0.)  Since I'll only be using this for generating test outputs, something as simple to set up as possible would be great.
e it as developer docs.)

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 17 Jul 2014 22:57:56 -0700",Re: Current way to include hive in a build,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Stephen,

The only change the build was that we ask users to run -Phive and
-Pyarn of --with-hive and --with-yarn (which internally just set
-Phive and -Pyarn). I don't think this should affect the dependency
graph.

Just to test this, what happens if you run *without* the CDH profile
and build with hadoop version 2.3.0? Does that work?

- Patrick


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 17 Jul 2014 23:13:09 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),dev@spark.apache.org,"UPDATE:

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1023/

The previous repo contains exactly the same content but mutable.
Thanks Patrick for pointing it out!

-Xiangrui


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 18 Jul 2014 00:03:53 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

- Looked through the release commits
- Looked through JIRA issues
- Ran the audit tests (one issue with the maven app test, but was also
an issue with 0.9.1 so I think it's my environment)
- Checked sigs/sums


"
Andrew Or <andrew@databricks.com>,"Fri, 18 Jul 2014 01:07:51 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1, tested on standalone cluster and ran spark shell, pyspark and SparkPi


2014-07-18 0:03 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

"
Sean Owen <sowen@cloudera.com>,"Fri, 18 Jul 2014 11:16:03 +0100",Re: Current way to include hive in a build,"""dev@spark.apache.org"" <dev@spark.apache.org>","This build invocation works just as you have it, for me. (At least, it
gets through Hive; Examples fails for a different unrelated reason.)

commons-logging 1.0.4 exists in Maven for sure. Maybe there is some
temporary problem accessing Maven's repo?


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 18 Jul 2014 11:42:29 +0000",RE: Feature selection interface,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI This is my first take on feature selection, filtering and chi-squared:
https://github.com/apache/spark/pull/1484



I've implemented a class that does Chi-squared feature selection for RDD[LabeledPoint]. It also computes basic class/feature occurrence statistics and other methods like mutual information or information gain can be easily implemented. I would like to make a pull request. However, MLlib master branch doesn't have any feature selection methods implemented. So, I need to create a proper interface that my class will extend or mix. It should be easy to use from developers and users prospective.

I was thinking that there should be FeatureEvaluator that for each feature from RDD[LabeledPoint] returns RDD[((featureIndex: Int, label: Double), value: Double)].
Then there should be FeatureSelector that selects top N features or top N features group by class etc.
And the simplest one, FeatureFilter that filters the data based on set of feature indices.

Additionally, there should be the interface for FeatureEvaluators that don't use class labels, i.e. for RDD[Vector].

I am concerned that such design looks rather ""disconnected"" because there are 3 disconnected objects.

As a result of use, I would like to see something like ""val filteredData = Filter(data, ChiSquared(data).selectTop(100))"".

Any ideas or suggestions?

Best regards, Alexander

"
RJ Nowling <rnowling@gmail.com>,"Fri, 18 Jul 2014 08:05:55 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","Nice to meet you, Jeremy!

This is great!  Hierarchical clustering was next on my list --
currently trying to get my PR for MiniBatch KMeans accepted.

If it's cool with you, I'll try converting your code to fit in with
the existing MLLib code as you suggest. I also need to review the
Decision Tree code (as suggested above) to see how much of that can be
reused.

Maybe I can ask you to do a code review for me when I'm done?








-- 
em rnowling@gmail.com
c 954.496.2314

"
RJ Nowling <rnowling@gmail.com>,"Fri, 18 Jul 2014 08:05:55 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","Nice to meet you, Jeremy!

This is great!  Hierarchical clustering was next on my list --
currently trying to get my PR for MiniBatch KMeans accepted.

If it's cool with you, I'll try converting your code to fit in with
the existing MLLib code as you suggest. I also need to review the
Decision Tree code (as suggested above) to see how much of that can be
reused.

Maybe I can ask you to do a code review for me when I'm done?








-- 
em rnowling@gmail.com
c 954.496.2314

"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Fri, 18 Jul 2014 16:36:22 +0000",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1



:
2c0ba7f411cf9a2483895091440011742246b
1023/content/


"
Meisam Fathi <meisam.fathi@gmail.com>,"Fri, 18 Jul 2014 15:50:56 -0400",Re: Building Spark against Scala 2.10.1 virtualized,dev@spark.apache.org,"Sorry for resurrecting this thread but project/SparkBuild.scala is
completely rewritten recently (after this commit
https://github.com/apache/spark/tree/628932b). Should library
dependencies be defined in pox.xml files after this commit?

Thanks
Meisam


"
Debasish Das <debasish.das83@gmail.com>,"Fri, 18 Jul 2014 13:38:07 -0700",OWLQN,dev@spark.apache.org,"Hi,

I thought OWLQN is already merged to mllib optimization but I don't see it
in the master yet...

Are there any issues in merging it in ? I see there are some merge
conflicts right now...

https://github.com/apache/spark/pull/840/

Thanks.
Deb
"
Stephen Boesch <javadba@gmail.com>,"Fri, 18 Jul 2014 13:40:20 -0700",Re: Current way to include hive in a build,dev@spark.apache.org,"Thanks v much Patrick and Sean.  I have the build working now as follows:

 mvn -Pyarn -Pcdh5 -Phive -DskipTests clean package

in Addition, I am in the midst of running some tests and so far so good.


The pom.xml changes:

Added to main/parent directory pom.xml:

    <profile>
      <id>cdh5</id>
      <properties>
        <hadoop.version>2.3.0-cdh5.0.0</hadoop.version>
        <yarn.version>2.3.0-cdh5.0.0</yarn.version>
        <zookeeper.version>3.4.5-cdh5.0.0</zookeeper.version>
        <protobuf.version>2.5.0</protobuf.version>
        <jets3t.version>0.9.0</jets3t.version>
        <hbase.version>0.96.1.1-cdh5.0.0</hbase.version>
      </properties>
    </profile>

Added four dependencies into  *examples/*pom.xml:

  Here is the one for hbase-common:

    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-common</artifactId>
      <version>${hbase.version}</version>
      <exclusions>
        <exclusion>
          <groupId>asm</groupId>
          <artifactId>asm</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.jboss.netty</groupId>
          <artifactId>netty</artifactId>
        </exclusion>
        <exclusion>
          <groupId>io.netty</groupId>
          <artifactId>netty</artifactId>
        </exclusion>
        <exclusion>
          <groupId>commons-logging</groupId>
          <artifactId>commons-logging</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.jruby</groupId>
          <artifactId>jruby-complete</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

Duplicate the above for:

      <artifactId>hbase-client</artifactId>
..
      <artifactId>hbase-protocol</artifactId>
..
      <artifactId>hbase-server</artifactId>
..



2014-07-18 3:16 GMT-07:00 Sean Owen <sowen@cloudera.com>:

"
Reynold Xin <rxin@databricks.com>,"Fri, 18 Jul 2014 13:55:11 -0700",Re: Building Spark against Scala 2.10.1 virtualized,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yes.



"
DB Tsai <dbtsai@dbtsai.com>,"Fri, 18 Jul 2014 14:01:17 -0700",Re: OWLQN,dev@spark.apache.org,"I'm working on it with weighted regularization. The problem is that
OWLQN doesn't work nicely with Updater now since all the L1 logic
should be in OWLQN instead of L1Updater.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
npanj <nitinpanj@gmail.com>,"Fri, 18 Jul 2014 14:41:53 -0700 (PDT)",How to set Java options -Xmn,dev@spark.incubator.apache.org,"Hi,
I am trying to set -Xmn  to control GC in spark.executor.extraJavaOptions
(as recommended by tuning guide), but I am getting error that
""spark.executor.extraJavaOptions is not allowed to alter memory settings"".
It seems that extraJavaOptions takes just one number, not list of java
options. 

How can I set -Xmn ?



--

"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Fri, 18 Jul 2014 23:47:46 -0700 (PDT)",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.incubator.apache.org,"Hi RJ, that sounds like a great idea. I'd be happy to look over what you put
together.

-- Jeremy



--

"
Reynold Xin <rxin@databricks.com>,"Fri, 18 Jul 2014 23:58:45 -0700","Re: small (yet major) change going in: broadcasting RDD to reduce
 task size",Nicholas Chammas <nicholas.chammas@gmail.com>,"Thanks :)

FYI the pull request has been merged and will be part of Spark 1.1.0.




"
Debasish Das <debasish.das83@gmail.com>,"Sat, 19 Jul 2014 11:02:18 -0700",Master compilation with sbt,dev@spark.apache.org,"Hi,

Is sbt still used for master compilation ? I could compile for
2.3.0-cdh5.0.2 using maven following the instructions from the website:

http://spark.apache.org/docs/latest/building-with-maven.html

But when I am trying to use sbt for local testing and then I am getting
some weird errors...Is sbt still used by developers ? I am using JDK7...

org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element
type ""settings"" must be followed by either attribute specifications, "">"" or
""/>"".

at
com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)

at
com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)

at
com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)

at
com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)

at
com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.seekCloseOfStartTag(XMLDocumentFragmentScannerImpl.java:1394)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1327)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$ContentDriver.scanRootElementHook(XMLDocumentScannerImpl.java:1292)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3122)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:880)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)

at
com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)

at
com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)

at
com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)

at
com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)

at
com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)

at
com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333)

at scala.xml.factory.XMLLoader$class.loadXML(XMLLoader.scala:40)

at scala.xml.XML$.loadXML(XML.scala:57)

at scala.xml.factory.XMLLoader$class.load(XMLLoader.scala:52)

at scala.xml.XML$.load(XML.scala:57)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:225)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:224)

at sbt.Using.apply(Using.scala:25)

at com.typesafe.sbt.pom.MavenHelper$.settingsXml(MavenHelper.scala:224)

at
com.typesafe.sbt.pom.MavenHelper$.settingsXmlServers(MavenHelper.scala:245)

at
com.typesafe.sbt.pom.MavenHelper$.createSbtCredentialsFromSettingsXml(MavenHelper.scala:291)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)

at
sbt.Scoped$RichInitialize$$anonfun$map$1$$anonfun$apply$3.apply(Structure.scala:177)

at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)

at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)

at sbt.std.Transform$$anon$4.work(System.scala:64)

at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)

at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)

at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)

at sbt.Execute.work(Execute.scala:244)

at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)

at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)

at
sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)

at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)

at java.util.concurrent.FutureTask.run(FutureTask.java:262)

at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)

at java.util.concurrent.FutureTask.run(FutureTask.java:262)

at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

at java.lang.Thread.run(Thread.java:745)

Thanks.

Deb
"
Debasish Das <debasish.das83@gmail.com>,"Sat, 19 Jul 2014 11:10:22 -0700",Re: Master compilation with sbt,dev@spark.apache.org,"I am at the reservoir sampling commit:

commit 586e716e47305cd7c2c3ff35c0e828b63ef2f6a8
Author: Reynold Xin <rxin@apache.org>
Date:   Fri Jul 18 12:41:50 2014 -0700

sbt/sbt -Dhttp.nonProxyHosts=132.197.10.21


[info] Set current project to spark-mllib (in build
file:/Users/v606014/spark-master/)


[trace] Stack trace suppressed: run last mllib/*:credentials for the full
output.

[trace] Stack trace suppressed: run last core/*:credentials for the full
output.

[error] (mllib/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4;
columnNumber: 57; Element type ""settings"" must be followed by either
attribute specifications, "">"" or ""/>"".

[error] (core/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4;
columnNumber: 57; Element type ""settings"" must be followed by either
attribute specifications, "">"" or ""/>"".

[error] Total time: 0 s, completed Jul 19, 2014 6:09:24 PM

"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 19 Jul 2014 12:50:01 -0700",Re: Master compilation with sbt,dev@spark.apache.org,".
.
.
.
.
.
.
.
.

...all works fine for me @2a732110d46712c535b75dd4f5a73761b6463aa8



"
Chester Chen <chester@alpinenow.com>,"Sat, 19 Jul 2014 13:24:31 -0700",Re: Master compilation with sbt,"""dev@spark.apache.org"" <dev@spark.apache.org>","Works for me as well:


git branch

  branch-0.9

  branch-1.0

* master

Chesters-MacBook-Pro:spark chester$ git pull --rebase

remote: Counting objects: 578, done.

remote: Compressing objects: 100% (369/369), done.

remote: Total 578 (delta 122), reused 418 (delta 71)

Receiving objects: 100% (578/578), 432.42 KiB | 354.00 KiB/s, done.

Resolving deltas: 100% (122/122), done.


   9c24974..2a73211  master     -> origin/master

   8e5604b..c93f4a0  branch-0.9 -> origin/branch-0.9

   0b0b895..7611840  branch-1.0 -> origin/branch-1.0


 * [new tag]         v0.9.2-rc1 -> v0.9.2-rc1

First, rewinding head to replay your work on top of it...

Fast-forwarded master to 2a732110d46712c535b75dd4f5a73761b6463aa8.


Chesters-MacBook-Pro:spark chester$ sbt/sbt package

....

[info] Done packaging.

[success] Total time: 146 s, completed Jul 19, 2014 1:08:52 PM






"
Patrick Wendell <pwendell@gmail.com>,"Sat, 19 Jul 2014 20:10:06 -0700",Pull requests will be automatically linked to JIRA when submitted,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just a small note, today I committed a tool that will automatically
mirror pull requests to JIRA issues, so contributors will no longer
have to manually post a pull request on the JIRA when they make one.

It will create a ""link"" on the JIRA and also make a comment to trigger
an e-mail to people watching.

This should make some things easier, such as avoiding accidental
duplicate effort on the same JIRA.

- Patrick

"
Debasish Das <debasish.das83@gmail.com>,"Sun, 20 Jul 2014 00:10:39 -0700",Re: Master compilation with sbt,dev@spark.apache.org,"I figured out the cause...brew is updated to scala 2.11 and I got the
latest scala version...




"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 20 Jul 2014 07:14:00 -0400",Re: Pull requests will be automatically linked to JIRA when submitted,"""dev@spark.apache.org"" <dev@spark.apache.org>","Awesome!


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 20 Jul 2014 11:06:07 -0400",Re: Pull requests will be automatically linked to JIRA when submitted,dev <dev@spark.apache.org>,"That's pretty neat.

How does it work? Do we just need to put the issue ID (e.g. SPARK-1234)
anywhere in the pull request?

Nick



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 20 Jul 2014 09:50:54 -0700",Re: Pull requests will be automatically linked to JIRA when submitted,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah it needs to have SPARK-XXX in the title (this is the format we
request already). It just works with small synchronization script I
wrote that we run every five minutes on Jeknins that uses the Github
and Jenkins API:

https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd139232929

- Patrick


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 20 Jul 2014 16:56:16 -0400",sbt/sbt test steals window focus on OS X,dev <dev@spark.apache.org>,"I just created SPARK-2602 <https://issues.apache.org/jira/browse/SPARK-2602> to
track this issue.

Are there others who can confirm this is an issue? Also, does this issue
extend to other OSes?

Nick
"
Hari Shreedharan <hshreedharan@cloudera.com>,"Sun, 20 Jul 2014 20:27:52 -0700",Re: sbt/sbt test steals window focus on OS X,dev@spark.apache.org,"Add this to your .bash_profile (or .bashrc) - that will fix it.

export _JAVA_OPTIONS=-Djava.awt.headless=true


Hari



"
Haoyuan Li <haoyuan.li@gmail.com>,"Mon, 21 Jul 2014 01:29:42 -0700","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"Qingyang,

Aha. Got it.

800MB data is pretty small. Loading from Tachyon does have a bit of extra
overhead. But it will have more benefit when the data size is larger. Also,
if you store the table in Tachyon, you can have different shark servers to
query the data at the same time. For more trade-off, please refer to this
page: http://tachyon-project.org/Running-Shark-on-Tachyon.html

Best,

Haoyuan






-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
aaronjosephs <aaron@placeiq.com>,"Mon, 21 Jul 2014 07:46:56 -0700 (PDT)",Pull request for PLAT-911,dev@spark.incubator.apache.org,"I found the spark Jira ticket,
https://issues.apache.org/jira/browse/SPARK-911, and noticed that no one had
done it. It seemed like a useful and interesting feature. I implemented a
pull request here ( https://github.com/apache/spark/pull/1381) and was
looking for some input on it( go easy on me first time contributor).



--

"
Erik Erlandson <eje@redhat.com>,"Mon, 21 Jul 2014 11:24:11 -0400 (EDT)",RFC:  Supporting the Scala drop Method for Spark RDDs,dev@spark.apache.org,"A few weeks ago I submitted a PR for supporting rdd.drop(n), under SPARK-2315:
https://issues.apache.org/jira/browse/SPARK-2315

Supporting the drop method would make some operations convenient, however it forces computation of >= 1 partition of the parent RDD, and so it would behave like a ""partial action"" that returns an RDD as the result.

I wrote up a discussion of these trade-offs here:
http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/

"
Andrew Ash <andrew@andrewash.com>,"Mon, 21 Jul 2014 08:27:10 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,"dev@spark.apache.org, Erik Erlandson <eje@redhat.com>","Personally I'd find the method useful -- I've often had a .csv file with a
header row that I want to drop so filter it out, which touches all
partitions anyway.  I don't have any comments on the implementation quite
yet though.



"
RJ Nowling <rnowling@gmail.com>,"Mon, 21 Jul 2014 11:36:26 -0400",Examples have SparkContext improperly labeled?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

The examples listed here

https://spark.apache.org/examples.html

refer to the spark context as ""spark"" but when running Spark Shell
uses ""sc"" for the SparkContext.

Am I missing something?

Thanks!

RJ


-- 
em rnowling@gmail.com
c 954.496.2314

"
Aniket <aniket.bhatnagar@gmail.com>,"Mon, 21 Jul 2014 08:46:54 -0700 (PDT)",Re: RFC: Supporting the Scala drop Method for Spark RDDs,dev@spark.incubator.apache.org,"I too would like this feature. Erik's post makes sense. However, shouldn't
the RDD also repartition itself after drop to effectively make use of
cluster resources?





--"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 21 Jul 2014 08:53:15 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,dev@spark.apache.org,"Sure, drop() would be useful, but breaking the ""transformations are lazy;
only actions launch jobs"" model is abhorrent -- which is not to say that we
haven't already broken that model for useful operations (cf.
RangePartitioner, which is used for sorted RDDs), but rather that each such
exception to the model is a significant source of pain that can be hard to
work with or work around.

I really wouldn't like to see another such model-breaking transformation
with dependencies on these kind of ""internal"" jobs is sometimes very
useful, so a significant reworking of Spark's Dependency model that would
allow for lazily running such internal jobs and making the results
available to subsequent stages may be something worth pursuing.



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 21 Jul 2014 09:52:15 -0700",Re: Examples have SparkContext improperly labeled?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi RJ,

Spark Shell instantiates a SparkContext for you named ""sc"".  In other apps,
the user instantiates it themself and can give the variable whatever name
they want, e.g. ""spark"".

-Sandy



"
Erik Erlandson <eje@redhat.com>,"Mon, 21 Jul 2014 13:24:14 -0400 (EDT)",Re: RFC: Supporting the Scala drop Method for Spark RDDs,dev@spark.apache.org,"

This seems like a very interesting angle.   I don't have much feel for what a solution would look like, but it sounds as if it would involve caching all operations embodied by RDD transform method code for provisional execution.  I believe that these levels of invocation are currently executed in the master, not executor nodes.



"
RJ Nowling <rnowling@gmail.com>,"Mon, 21 Jul 2014 13:28:36 -0400",Re: Examples have SparkContext improperly labeled?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for the clarification, Sandy.





-- 
em rnowling@gmail.com
c 954.496.2314

"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 21 Jul 2014 11:06:34 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,"dev@spark.apache.org, Erik Erlandson <eje@redhat.com>","Rather than embrace non-lazy transformations and add more of them, I'd
rather we 1) try to fully characterize the needs that are driving their
creation/usage; and 2) design and implement new Spark abstractions that
will allow us to meet those needs and eliminate existing non-lazy
transformation.  They really mess up things like creation of asynchronous
FutureActions, job cancellation and accounting of job resource usage, etc.,
so I'd rather we seek a way out of the existing hole rather than make it
deeper.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 21 Jul 2014 14:46:36 -0400","spark.executor.memory is not applicable when running unit test
 in Jenkins?",dev@spark.apache.org,"Hi, all  

I’m running some unit tests for my Spark applications in Jenkins

it seems that even I set spark.executor.memory to 5g, the value I got with Runtime.getRuntime.maxMemory is still around 1G

Is it saying that Jenkins limit the process to use no more than 1G (by default)? how to change that?

Thanks,


--  
Nan Zhu

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 21 Jul 2014 11:59:39 -0700",SPARK-1199 has been reverted in branch-1.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just a note - there was a fix in branch-1.0 (and Spark 1.0.1) that
introduced a new bug worse than the original one.

https://issues.apache.org/jira/browse/SPARK-1199

The original bug was an issue with case classes defined in the repl.
The fix caused a separate bug which broke most compound statements in
the repl (val x = 1; val y = x + 1).

I've reverted the original SPARK-1199 fix in the 1.0 branch. Since
repl changes are some of the riskier ones in Spark, I'm planning to
leave this fix out of 1.0.X entirely. The final, correct fix for this
will appear in Spark 1.1.

- Patrick

"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Mon, 21 Jul 2014 13:44:54 -0700",-1s on pull requests?,dev@spark.apache.org,"Hi all,

As the number of committers / contributors on Spark has increased, there
are cases where pull requests get merged before all the review comments
have been addressed. This happens say when one committer points out a
problem with the pull request, and another committer doesn't see the
earlier comment and merges the PR before the comment has been addressed.
 This is especially tricky for pull requests with a large number of
comments, because it can be difficult to notice early comments describing
blocking issues.

This also happens when something accidentally gets merged after the tests
have started but before tests have passed.

Do folks have ideas on how we can handle this issue? Are there other
projects that have good ways of handling this? It looks like for Hadoop,
people can -1 / +1 on the JIRA.

-Kay
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 21 Jul 2014 13:45:17 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,"dev@spark.apache.org, Erik Erlandson <eje@redhat.com>","You can find some of the prior, related discussion here:
https://issues.apache.org/jira/browse/SPARK-1021



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 21 Jul 2014 13:56:35 -0700",Re: -1s on pull requests?,dev@spark.apache.org,"and posts a commit status [1] (like say Travis [2]) right next to the PR.
Does anybody know of an existing tool that does this ?

Shivaram

[1] https://github.com/blog/1227-commit-status-api
[2]
http://blog.travis-ci.com/2012-09-04-pull-requests-just-got-even-more-awesome/



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 21 Jul 2014 13:59:28 -0700",Re: -1s on pull requests?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've always operated under the assumption that if a commiter makes a
comment on a PR, and that's not addressed, that should block the PR
from being merged (even without a specific ""-1""). I don't know of any
cases where this has intentionally been violated, but I do think this
happens accidentally some times.

Unfortunately, we are not allowed to use those github hooks because of
the way the ASF github integration works.

I've lately been using a custom-made tool to help review pull
committers have said LGTM on a PR (vs the ones that have commented).
We could also indicate the latest test status as Green/Yellow/Red
based on the Jenkins comments:

http://pwendell.github.io/spark-github-shim/

As a warning to potential users, my tool might crash your browser.

- Patrick


"
Neil Ferguson <nferguson@gmail.com>,"Mon, 21 Jul 2014 22:10:25 +0100","""Dynamic variables"" in Spark",dev@spark.apache.org,"Hi all

I have been adding some metrics to the ADAM project
https://github.com/bigdatagenomics/adam, which runs on Spark, and have a
proposal for an enhancement to Spark that would make this work cleaner and
easier.

I need to pass some Accumulators around, which will aggregate metrics
(timing stats and other metrics) across the cluster. However, it is
cumbersome to have to explicitly pass some ""context"" containing these
accumulators around everywhere that might need them. I can use Scala
implicits, which help slightly, but I'd still need to modify every method
in the call stack to take an implicit variable.

So, I'd like to propose that we add the ability to have ""dynamic variables""
(basically thread-local variables) to Spark. This would avoid having to
pass the Accumulators around explicitly.

My proposed approach is to add a method to the SparkContext class as
follows:

/**
 * Sets the value of a ""dynamic variable"". This value is made available to
jobs
 * without having to be passed around explicitly. During execution of a
Spark job
 * this value can be obtained from the [[SparkDynamic]] object.
 */
def setDynamicVariableValue(value: Any)

Then, when a job is executing the SparkDynamic can be accessed to obtain
the value of the dynamic variable. The implementation of this object is as
follows:

object SparkDynamic {
  private val dynamicVariable = new DynamicVariable[Any]()
  /**
   * Gets the value of the ""dynamic variable"" that has been set in the
[[SparkContext]]
   */
  def getValue: Option[Any] = {
    Option(dynamicVariable.value)
  }
  private[spark] def withValue[S](threadValue: Option[Any])(thunk: => S): S
= {
    dynamicVariable.withValue(threadValue.orNull)(thunk)
  }
}

The change involves modifying the Task object to serialize the value of the
dynamic variable, and modifying the TaskRunner class to deserialize the
value and make it available in the thread that is running the task (using
the SparkDynamic.withValue method).

I have done a quick prototype of this in this commit:
https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
and it seems to work fine in my (limited) testing. It needs more testing,
tidy-up and documentation though.

whether it needs it or not. For my use case this might not be too much of a
problem, as serializing and deserializing Accumulators looks fairly
lightweight -- however we should certainly warn users against setting a
dynamic variable containing lots of data. I thought about using broadcast
tables here, but I don't think it's possible to put Accumulators in a
broadcast table (as I understand it, they're intended for purely read-only
data).

What do people think about this proposal? My use case aside, it seems like
it would be a generally useful enhancment to be able to pass certain data
around without having to explicitly pass it everywhere.

Neil
"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 21 Jul 2014 14:19:37 -0700",Re: -1s on pull requests?,"""dev@spark.apache.org"" <dev@spark.apache.org>","There is ASF guidelines about Voting, including code review for
patches: http://www.apache.org/foundation/voting.html

Some ASF project do three +1 votes are required (to the issues like
JIRA or Github PR in this case) for a patch unless it is tagged with
lazy consensus [1] of like 48 hours.
For patches that are not critical, waiting for a while to let some
time for additional committers to review would be the best way to go.

Another thing is that all contributors need to be patience once their
patches have been submitted and pending reviewed. This is part of
being in open community.

Hope this helps.


- Henry

[1] http://www.apache.org/foundation/glossary.html#LazyConsensus


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 21 Jul 2014 18:42:31 -0400",Contributing to Spark needs PySpark build/test instructions,dev <dev@spark.apache.org>,"Contributing to Spark
<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
needs a line or two about building and testing PySpark. A call out of
run-tests, for example, would be helpful for new contributors to PySpark.

Nick
​
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 21 Jul 2014 18:54:05 -0400",Re: Contributing to Spark needs PySpark build/test instructions,dev <dev@spark.apache.org>,"For the record, the triggering discussion is here
<https://github.com/apache/spark/pull/1505#issuecomment-49671550>. I
assumed that sbt/sbt test covers all the tests required before submitting a
patch, and it appears that it doesn’t.
​



"
Christopher Nguyen <ctn@adatao.com>,"Mon, 21 Jul 2014 17:54:09 -0700","Re: ""Dynamic variables"" in Spark","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Neil, first off, I'm generally a sympathetic advocate for making changes
to Spark internals to make it easier/better/faster/more awesome.

In this case, I'm (a) not clear about what you're trying to accomplish, and
(b) a bit worried about the proposed solution.

the proposed solution is for some ""shared"" variable that may be set and
""mapped out"" and possibly ""reduced back"", but without any accompanying
accumulation semantics. And yet it doesn't seem like you only want just the
broadcast property. Can you clarify the problem statement with some
before/after client code examples?

with caution, as it may have unintended consequences beyond just serdes
payload size. For example, there is a stated intention of supporting
multiple SparkContexts in the future, and this proposed solution can make
it a bigger challenge to do so. Indeed, we had a gut-wrenching call to make
a while back on a subject related to this (see
https://github.com/mesos/spark/pull/779). Furthermore, even in a single
SparkContext application, there may be multiple ""clients"" (of that
application) whose intent to use the proposed ""SparkDynamic"" would not
necessarily be coordinated.

So, considering a ratio of a/b (benefit/cost), it's not clear to me that
the benefits are significant enough to warrant the costs. Do I
misunderstand that the benefit is to save one explicit parameter (the
""context"") in the signature/closure code?

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Tue, 22 Jul 2014 10:16:07 +0900",Suggestion for SPARK-1825,<dev@spark.apache.org>,"Hi,

 

A couple of month ago, I made a pull request to fix
https://issues.apache.org/jira/browse/SPARK-1825.

My pull request is here: https://github.com/apache/spark/pull/899 

 

But that pull request has problems:

l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.

l  The related Hadoop API is marked as '@Unstable'.

 

Here is an idea to remedy the problems: a new Spark configuration variable.

Maybe it can be named as ""spark.yarn.submit.crossplatform"".

If it is set to ""true""(default is false), the related Spark code can use the
hard-coded strings that is the same as the Hadoop API provides, thus
avoiding compile error on the Hadoop versions below 2.4.0.

 

Can someone implement this feature, if this idea is acceptable?

Currently my knowledge on Spark source code and Scala is limited to
implement it myself.

To the right person, the modification should be trivial.

You can refer to the source code changes of my pull request.

 

Thanks.

 

"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Jul 2014 20:43:11 -0700",Re: Contributing to Spark needs PySpark build/test instructions,"""dev@spark.apache.org"" <dev@spark.apache.org>","I added an automated testing section:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting

Can you take a look to see if it is what you had in mind?




 a
k
k.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 22 Jul 2014 00:20:10 -0400",Re: Contributing to Spark needs PySpark build/test instructions,dev <dev@spark.apache.org>,"Looks good. Does sbt/sbt test cover the same tests as /dev/run-tests?

I’m looking at step 5 under “Contributing Code”. Someone contributing to
PySpark will want to be directed to run something in addition to (or
instead of) sbt/sbt test, I believe.

Nick
​



ontributingtoSpark-AutomatedTesting
"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Jul 2014 21:28:32 -0700",Re: Contributing to Spark needs PySpark build/test instructions,"""dev@spark.apache.org"" <dev@spark.apache.org>","I missed that bullet point. I removed that and just pointed it towards the
instruction.



Someone contributing to
:
ontributingtoSpark-AutomatedTesting
of
"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Jul 2014 21:39:04 -0700","Re: ""Dynamic variables"" in Spark","""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for the thoughtful email, Neil and Christopher.

If I understand this correctly, it seems like the dynamic variable is just
a variant of the accumulator (a static one since it is a global object).
Accumulators are already implemented using thread-local variables under the
hood. Am I misunderstanding something?




"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 22 Jul 2014 00:44:47 -0400",Re: Contributing to Spark needs PySpark build/test instructions,dev <dev@spark.apache.org>,"That works! Thank you.



e
. Someone contributing to
ontributingtoSpark-AutomatedTesting
I
t
"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Jul 2014 22:37:04 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,"""dev@spark.apache.org"" <dev@spark.apache.org>","If the purpose is for dropping csv headers, perhaps we don't really need a
common drop and only one that drops the first line in a file? I'd really
try hard to avoid a common drop/dropWhile because they can be expensive to
do.

Note that I think we will be adding this functionality (ignoring headers)
to the CsvRDD functionality in Spark SQL.
 https://github.com/apache/spark/pull/1351



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 21 Jul 2014 22:50:24 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,"""dev@spark.apache.org"" <dev@spark.apache.org>","It could make sense to add a skipHeader argument to SparkContext.textFile?



"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Jul 2014 22:55:05 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yes, that could work. But it is not as simple as just a binary flag.

We might want to skip the first row for every file, or the header only for
the first file. The former is not really supported out of the box by the
input format I think?



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 21 Jul 2014 22:59:59 -0700",Re: RFC: Supporting the Scala drop Method for Spark RDDs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah, the input format doesn't support this behavior.  But it does tell you
the byte position of each record in the file.



"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Tue, 22 Jul 2014 16:47:20 +0900",Suggestion for SPARK-1825,<dev@spark.apache.org>,"(I'm resending this mail since it seems that it was not sent. Sorry if this
was already sent.)

Hi,

 

A couple of month ago, I made a pull request to fix
https://issues.apache.org/jira/browse/SPARK-1825.

My pull request is here: https://github.com/apache/spark/pull/899 

 

But that pull request has problems:

l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.

l  The related Hadoop API is marked as '@Unstable'.

 

Here is an idea to remedy the problems: a new Spark configuration variable.

Maybe it can be named as ""spark.yarn.submit.crossplatform"".

If it is set to ""true""(default is false), the related Spark code can use the
hard-coded strings that is the same as the Hadoop API provides, thus
avoiding compile error on the Hadoop versions below 2.4.0.

 

Can someone implement this feature, if this idea is acceptable?

Currently my knowledge on Spark source code and Scala is limited to
implement it myself.

To the right person, the modification should be trivial.

You can refer to the source code changes of my pull request.

 

Thanks.

 

"
Gerard Maas <gerard.maas@gmail.com>,"Tue, 22 Jul 2014 16:20:37 +0200",Using case classes as keys does not seem to work.,"user@spark.apache.org, dev@spark.apache.org","Using a case class as a key doesn't seem to work properly. [Spark 1.0.0]

A minimal example:

case class P(name:String)
val ps = Array(P(""alice""), P(""bob""), P(""charly""), P(""bob""))
sc.parallelize(ps).map(x=> (x,1)).reduceByKey((x,y) => x+y).collect
[Spark shell local mode] res : Array[(P, Int)] = Array((P(bob),1),
(P(bob),1), (P(abe),1), (P(charly),1))

In contrast to the expected behavior, that should be equivalent to:
sc.parallelize(ps).map(x=> (x.name,1)).reduceByKey((x,y) => x+y).collect
Array[(String, Int)] = Array((charly,1), (abe,1), (bob,2))

Any ideas why this doesn't work?

-kr, Gerard.
"
Gerard Maas <gerard.maas@gmail.com>,"Tue, 22 Jul 2014 16:30:43 +0200",Re: Using case classes as keys does not seem to work.,"user@spark.apache.org, dev@spark.apache.org","Just to narrow down the issue, it looks like the issue is in 'reduceByKey'
and derivates like 'distinct'.

groupByKey() seems to work

sc.parallelize(ps).map(x=> (x.name,1)).groupByKey().collect
res: Array[(String, Iterable[Int])] = Array((charly,ArrayBuffer(1)),
(abe,ArrayBuffer(1)), (bob,ArrayBuffer(1, 1)))




"
"""Sundaram, Muthu X."" <Muthu.X.Sundaram.ctr@sabre.com>","Tue, 22 Jul 2014 10:24:11 -0500",Tranforming flume events using Spark transformation functions,"""user@spark.apache.org"" <user@spark.apache.org>,
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi All,
  I am getting events from flume using following line.

  JavaDStream<SparkFlumeEvent> flumeStream = FlumeUtils.createStream(ssc, host, port);

Each event is a delimited record. I like to use some of the transformation functions like map and reduce on this. Do I need to convert the JavaDStream<SparkFlumeEvent> to JavaDStream<String> or can I apply these function directly on this?

I need to do following kind of operations

XXXX                     AA
YYYYY                    Delta
TTTTT                    AA
CCCC                     Southwest
XXXX                     AA

Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket, Southwest - 1 ticket.

I have to do transformations like this. Right now I am able to receives records. But I am struggling to transform them using spark transformation functions since they are not of type JavaRDD<String>.

Can I create new JavaRDD<String>? How do I create new JavaRDD?

I loop through  the events like below

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws Exception {
                     String logRecord = null;
                     List<SparkFlumeEvent> events = eventsData.collect();
                     Iterator<SparkFlumeEvent> batchedEvents = events.iterator();
                     long t1 = System.currentTimeMillis();
                     AvroFlumeEvent avroEvent = null;
                     ByteBuffer bytePayload = null;
                     // All the user level data is carried as payload in Flume Event
                     while(batchedEvents.hasNext()) {
                            SparkFlumeEvent flumeEvent = batchedEvents.next();
                            avroEvent = flumeEvent.event();
                            bytePayload = avroEvent.getBody();
                            logRecord = new String(bytePayload.array());

                            System.out.println("">>>>>>>>LOG RECORD = "" + logRecord);
}

Where do I create new JavaRDD<String>? DO I do it before this loop? How do I create this JavaRDD<String>?
In the loop I am able to get every record and I am able to print them.

I appreciate any help here.

Thanks,
Muthu


"
Gerard Maas <gerard.maas@gmail.com>,"Tue, 22 Jul 2014 18:11:54 +0200",Re: Using case classes as keys does not seem to work.,"user@spark.apache.org, dev@spark.apache.org","I created https://issues.apache.org/jira/browse/SPARK-2620 to track this.

Maybe useful to know, this is a regression on Spark 1.0.0. I tested the
same sample code on 0.9.1 and it worked (we have several jobs using case
classes as key aggregators, so it better does)

-kr, Gerard.



"
Neil Ferguson <nferguson@gmail.com>,"Tue, 22 Jul 2014 21:43:04 +0100","Re: ""Dynamic variables"" in Spark",dev@spark.apache.org,"Hi Reynold

Thanks for your reply.

Accumulators are, of course, stored in the Accumulators object as
thread-local variables. However, the Accumulators object isn't public, so
when a Task is executing there's no way to get the set of accumulators for
the current thread -- accumulators still have to be passed to every method
that needs them.

Additionally, unless an accumulator is explicitly referenced it won't be
serialized as part of a Task, and won't make it into the Accumulators
object in the first place.

I should also note that what I'm proposing is not specific to Accumulators
-- I am proposing that any data can be stored in a thread-local variable. I
think there are probably many other use cases other than my one.

Neil



"
"""Sundaram, Muthu X."" <Muthu.X.Sundaram.ctr@sabre.com>","Tue, 22 Jul 2014 15:55:23 -0500",RE: Tranforming flume events using Spark transformation functions,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org""
	<user@spark.apache.org>, ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>, ""tathagata.das1565@gmail.com""
	<tathagata.das1565@gmail.com>","I tried to map SparkFlumeEvents to String of RDDs like below. But that map and call are not at all executed. I might be doing this in a wrong way. Any help would be appreciated.

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws Exception {
                                System.out.println(""<<<<<<Inside for each...call>>>>"");

                                JavaRDD<String> records = eventsData.map(
            new Function<SparkFlumeEvent, String>() {
                                @Override
                public String call(SparkFlumeEvent flume) throws Exception {
                    String logRecord = null;
                AvroFlumeEvent avroEvent = null;
      ByteBuffer bytePayload = null;

                                                                                System.out.println(""<<<<<<Inside Map..call>>>>"");
                    /* List<SparkFlumeEvent> events = flume.collect();
                     Iterator<SparkFlumeEvent> batchedEvents = events.iterator(); 
                                                                                
                            SparkFlumeEvent flumeEvent = batchedEvents.next();*/
                            avroEvent = flume.event();
                            bytePayload = avroEvent.getBody();
                            logRecord = new String(bytePayload.array());                                
                                                                                                                System.out.println(""<<<<Record is"" + logRecord);
                                                                                
                    return logRecord;
                }
            });                                               
                                return null;
}

  I am getting events from flume using following line.

  JavaDStream<SparkFlumeEvent> flumeStream = FlumeUtils.createStream(ssc, host, port);

Each event is a delimited record. I like to use some of the transformation functions like map and reduce on this. Do I need to convert the JavaDStream<SparkFlumeEvent> to JavaDStream<String> or can I apply these function directly on this?

I need to do following kind of operations

XXXX                     AA
YYYYY                    Delta
TTTTT                    AA
CCCC                     Southwest
XXXX                     AA

Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket, Southwest - 1 ticket.

I have to do transformations like this. Right now I am able to receives records. But I am struggling to transform them using spark transformation functions since they are not of type JavaRDD<String>.

Can I create new JavaRDD<String>? How do I create new JavaRDD?

I loop through  the events like below

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws Exception {
                     String logRecord = null;
                     List<SparkFlumeEvent> events = eventsData.collect();
                     Iterator<SparkFlumeEvent> batchedEvents = events.iterator();
                     long t1 = System.currentTimeMillis();
                     AvroFlumeEvent avroEvent = null;
                     ByteBuffer bytePayload = null;
                     // All the user level data is carried as payload in Flume Event
                     while(batchedEvents.hasNext()) {
                            SparkFlumeEvent flumeEvent = batchedEvents.next();
                            avroEvent = flumeEvent.event();
                            bytePayload = avroEvent.getBody();
                            logRecord = new String(bytePayload.array());

                            System.out.println("">>>>>>>>LOG RECORD = "" + logRecord); }

Where do I create new JavaRDD<String>? DO I do it before this loop? How do I create this JavaRDD<String>?
In the loop I am able to get every record and I am able to print them.

I appreciate any help here.

Thanks,
Muthu



"
"""Sundaram, Muthu X."" <Muthu.X.Sundaram.ctr@sabre.com>","Tue, 22 Jul 2014 15:55:23 -0500",RE: Tranforming flume events using Spark transformation functions,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org""
	<user@spark.apache.org>, ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>, ""tathagata.das1565@gmail.com""
	<tathagata.das1565@gmail.com>","I tried to map SparkFlumeEvents to String of RDDs like below. But that map and call are not at all executed. I might be doing this in a wrong way. Any help would be appreciated.

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws Exception {
                                System.out.println(""<<<<<<Inside for each...call>>>>"");

                                JavaRDD<String> records = eventsData.map(
            new Function<SparkFlumeEvent, String>() {
                                @Override
                public String call(SparkFlumeEvent flume) throws Exception {
                    String logRecord = null;
                AvroFlumeEvent avroEvent = null;
      ByteBuffer bytePayload = null;

                                                                                System.out.println(""<<<<<<Inside Map..call>>>>"");
                    /* List<SparkFlumeEvent> events = flume.collect();
                     Iterator<SparkFlumeEvent> batchedEvents = events.iterator(); 
                                                                                
                            SparkFlumeEvent flumeEvent = batchedEvents.next();*/
                            avroEvent = flume.event();
                            bytePayload = avroEvent.getBody();
                            logRecord = new String(bytePayload.array());                                
                                                                                                                System.out.println(""<<<<Record is"" + logRecord);
                                                                                
                    return logRecord;
                }
            });                                               
                                return null;
}

  I am getting events from flume using following line.

  JavaDStream<SparkFlumeEvent> flumeStream = FlumeUtils.createStream(ssc, host, port);

Each event is a delimited record. I like to use some of the transformation functions like map and reduce on this. Do I need to convert the JavaDStream<SparkFlumeEvent> to JavaDStream<String> or can I apply these function directly on this?

I need to do following kind of operations

XXXX                     AA
YYYYY                    Delta
TTTTT                    AA
CCCC                     Southwest
XXXX                     AA

Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket, Southwest - 1 ticket.

I have to do transformations like this. Right now I am able to receives records. But I am struggling to transform them using spark transformation functions since they are not of type JavaRDD<String>.

Can I create new JavaRDD<String>? How do I create new JavaRDD?

I loop through  the events like below

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws Exception {
                     String logRecord = null;
                     List<SparkFlumeEvent> events = eventsData.collect();
                     Iterator<SparkFlumeEvent> batchedEvents = events.iterator();
                     long t1 = System.currentTimeMillis();
                     AvroFlumeEvent avroEvent = null;
                     ByteBuffer bytePayload = null;
                     // All the user level data is carried as payload in Flume Event
                     while(batchedEvents.hasNext()) {
                            SparkFlumeEvent flumeEvent = batchedEvents.next();
                            avroEvent = flumeEvent.event();
                            bytePayload = avroEvent.getBody();
                            logRecord = new String(bytePayload.array());

                            System.out.println("">>>>>>>>LOG RECORD = "" + logRecord); }

Where do I create new JavaRDD<String>? DO I do it before this loop? How do I create this JavaRDD<String>?
In the loop I am able to get every record and I am able to print them.

I appreciate any help here.

Thanks,
Muthu



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 22 Jul 2014 14:09:29 -0700","Re: ""Dynamic variables"" in Spark",dev@spark.apache.org,"metrics in ADAM ? --  I've run into a similar use-case with having
user-defined metrics in long-running tasks and I think a nice way to solve
this would be to have user-defined TaskMetrics.

To state my problem more clearly, lets say you have two functions you use
in a map call and want to measure how much time each of them takes. For
example, if you have a code block like the one below and you want to
measure how much time f1 takes as a fraction of the task.

a.map { l =>
   val f = f1(l)
   ... some work here ...
}

It would be really cool if we could do something like

a.map { l =>
   val start = System.nanoTime
   val f = f1(l)
   TaskMetrics.get(""f1-time"").add(System.nanoTime - start)
}

These task metrics have a different purpose from Accumulators in the sense
that we don't need to track lineage, perform commutative operations etc.
 Further we also have a bunch of code in place to aggregate task metrics
across a stage etc. So it would be great if we could also populate these in
the UI and show median/max etc.
I think counters [1] in Hadoop served a similar purpose.

Thanks
Shivaram

[1]
https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/counters




"
Neil Ferguson <nferguson@gmail.com>,"Tue, 22 Jul 2014 22:17:27 +0100","Re: ""Dynamic variables"" in Spark",dev@spark.apache.org,"Hi Christopher

Thanks for your reply. I'll try and address your points -- please let me
know if I missed anything.

Regarding clarifying the problem statement, let me try and do that with a
real-world example. I have a method that I want to measure the performance
of, which has the following signature at the moment:

def merge(target: IndelRealignmentTarget): IndelRealignmentTarget = { //
impl }

Let's assume I have a Timers class, which contains various timers (that are
internally implemented using Accumulators). Each timer lets me instrument a
function call using its ""time"" method. Let's imagine that I add that as an
implicit parameter to the above method, as follows:

def merge(target: IndelRealignmentTarget)(implicit timers: Timers):
IndelRealignmentTarget = timers.mergeTarget.time { // impl }

This is not a big problem -- I've had to add an extra parameter to the
method, but it's not a big deal. However, the call stack of this method
looks something like the following:

IndelRealignmentTarget.merge
|-RealignmentTargetFinder.joinTargets
  |-RealignmentTargetFinder.findTargets
    |-RealignmentTargetFinder$.apply
      |-RealignIndels.realignIndels
        |-RealignIndels$.apply
          |-ADAMRecordRDDFunctions.adamRealignIndels
            |-Transform.run

So, I'd have to change every one of these methods to take the extra
parameter, which is pretty cumbersome. More importantly, when developers
want to add additional metrics to the code they'll have to think about how
to get an instance of Timers to the code they're developing.

So I'd really like the Timers object to be available in a thread-local
variable when I need it, without having to pass it around.

Regarding the implications of adding additional variables to SparkContext
-- I'm not sure I understand why this change would make it more difficult
to have multiple SparkContexts in the future. Could you clarify please?
Bear in mind that I'm not proposing adding any thread-local data to
SparkContext. The SparkContext merely holds the data, which is added to a
thread-local variable at task execution time.

Regarding having multiple clients of the SparkContext -- are you talking
about having multiple applications all sharing the same SparkContext? It
seems like there's data in SparkContext that is specific to a particular
application at the moment, like the JAR files for example, so this doesn't
seem inconsistent with that. Perhaps I'm misunderstanding here.

Neil



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 22 Jul 2014 14:21:07 -0700","Re: ""Dynamic variables"" in Spark","""dev@spark.apache.org"" <dev@spark.apache.org>, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","Shivaram,

You should take a look at this patch which adds support for naming
accumulators - this is likely to get merged in soon. I actually
started this patch by supporting named TaskMetrics similar to what you
have there, but then I realized there is too much semantic overlap
with accumulators, so I just went that route.

For instance, it would be nice if any user-defined metrics are
accessible at the driver program.

https://github.com/apache/spark/pull/1309

In your example, you could just define an accumulator here on the RDD
and you'd see the incremental update in the web UI automatically.

- Patrick


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 22 Jul 2014 19:50:35 -0700",Re: [VOTE] Release Apache Spark 0.9.2 (RC1),dev@spark.apache.org,"Hi all,

The vote has passed with 7 ""+1"" votes (4 binding) and 0 ""-1"" vote:

+1:

Xiangrui Meng*
Matei Zaharia*
DB Tsai
Reynold Xin*
Patrick Wendell*
Andrew Or
Sean McNamara

I'm closing this vote and going to package v0.9.2 today. Thanks
everyone for voting!

Best,
Xiangrui


"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 22 Jul 2014 22:20:15 -0700",Re: Tranforming flume events using Spark transformation functions,"""Sundaram, Muthu X."" <Muthu.X.Sundaram.ctr@sabre.com>","This is because of the RDD's lazy evaluation! Unless you force a
transformed (mapped/filtered/etc.) RDD to give you back some data (like
RDD.count) or output the data (like RDD.saveAsTextFile()), Spark will not
do anything.

So after the eventData.map(...), if you do take(10) and then print the
result, you should seem 10 items from each batch be printed.

Also you can do the same map operation on the Dstream as well. FYI.

inputDStream.map(...).foreachRDD(...)     is equivalent to
 inputDStream.foreachRDD(     // call rdd.map(...) )

Either way you have to call some RDD ""action"" (count, collect, take,
saveAsHadoopFile, etc.)  that asks the system to something concrete with
the data.

TD





"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 22 Jul 2014 22:20:15 -0700",Re: Tranforming flume events using Spark transformation functions,"""Sundaram, Muthu X."" <Muthu.X.Sundaram.ctr@sabre.com>","This is because of the RDD's lazy evaluation! Unless you force a
transformed (mapped/filtered/etc.) RDD to give you back some data (like
RDD.count) or output the data (like RDD.saveAsTextFile()), Spark will not
do anything.

So after the eventData.map(...), if you do take(10) and then print the
result, you should seem 10 items from each batch be printed.

Also you can do the same map operation on the Dstream as well. FYI.

inputDStream.map(...).foreachRDD(...)     is equivalent to
 inputDStream.foreachRDD(     // call rdd.map(...) )

Either way you have to call some RDD ""action"" (count, collect, take,
saveAsHadoopFile, etc.)  that asks the system to something concrete with
the data.

TD





"
"""Neil Ferguson"" <nferguson@gmail.com>","Wed, 23 Jul 2014 00:30:55 -0700 (PDT)","Re: ""Dynamic variables"" in Spark",dev@spark.apache.org,"Hi Patrick.




That looks very useful. The thing that seems to be missing from Shivaram's example is the ability to access TaskMetrics statically (this is the same problem that I am trying to solve with dynamic variables).






You mention defining an accumulator on the RDD. Perhaps I am missing something here, but my understanding was that accumulators are defined in SparkContext and are not part of the RDD. Is that correct?




Neil


Shivaram,


You should take a look at this patch which adds support for naming

accumulators - this is likely to get merged in soon. I actually

started this patch by supporting named TaskMetrics similar to what you

have there, but then I realized there is too much semantic overlap

with accumulators, so I just went that route.


For instance, it would be nice if any user-defined metrics are

accessible at the driver program.


https://github.com/apache/spark/pull/1309


In your example, you could just define an accumulator here on the RDD

and you'd see the incremental update in the web UI automatically.


- Patrick






solve



use


















sense



in








r-8/counters











so

for

method



be




Accumulators

. I












object).

under













accomplish,







set and

accompanying

just





be


serdes

supporting

can


to





that

would not






(the










com>













metrics

is

containing these

Scala

every




""dynamic


having




as








execution









object








set in







 =>







value



deserialize


task









73dcea6











fairly





in a





seems


certain







"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 23 Jul 2014 12:52:41 -0400",Re: Pull requests will be automatically linked to JIRA when submitted,dev <dev@spark.apache.org>,"By the way, it looks like there’s a JIRA plugin that integrates it with
GitHub:

   -
   https://marketplace.atlassian.com/plugins/com.atlassian.jira.plugins.jira-bitbucket-connector-plugin
   -
   https://confluence.atlassian.com/display/BITBUCKET/Linking+Bitbucket+and+GitHub+accounts+to+JIRA

It does the automatic linking and shows some additional information
<https://marketplace-cdn.atlassian.com/files/images/com.atlassian.jira.plugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.png>
that might be nice to have for heavy JIRA users.

Nick
​



232929
"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 23 Jul 2014 15:16:03 -0700",Announcing Spark 0.9.2,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","I'm happy to announce the availability of Spark 0.9.2! Spark 0.9.2 is
a maintenance release with bug fixes across several areas of Spark,
including Spark Core, PySpark, MLlib, Streaming, and GraphX. We
recommend all 0.9.x users to upgrade to this stable release.
Contributions to this release came from 28 developers.

Visit the release notes[1] to read about the release and download[2]
the release today.

[1] http://spark.apache.org/releases/spark-release-0-9-2.html
[2] http://spark.apache.org/downloads.html

Best,
Xiangrui

"
Nishkam Ravi <nravi@cloudera.com>,"Wed, 23 Jul 2014 22:12:03 -0700",Re: Configuring Spark Memory,"user@spark.apache.org, dev@spark.apache.org","See if this helps:

https://github.com/nishkamravi2/SparkAutoConfig/

It's a very simple tool for auto-configuring default parameters in Spark.
Takes as input high-level parameters (like number of nodes, cores per node,
memory per node, etc) and spits out default configuration, user advice and
command line. Compile (javac SparkConfigure.java) and run (java
SparkConfigure).

Also cc'ing dev in case others are interested in helping evolve this over
time (by refining the heuristics and adding more parameters).



"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Thu, 24 Jul 2014 14:59:26 +0800",GraphX graph partitioning strategy,dev@spark.apache.org,"Hi all,

I'm implementing graph partitioning strategy for GraphX, learning from 
researches on graph computing.

I have two questions:

- a specific implement question:
In current design, only vertex ID of src and dst are provided 
(PartitionStrategy.scala).
And some strategies require knowledge about the graph (like degrees) and 
can consist more than one passes to finally produce the partition ID.
So I'm changing the PartitionStrategy.getPartition API to provide more 
info, but I don't want to make it complex. (the current one looks very 
clean)

- an open question:
What advice would you give considering partitioning, considering the 
procedure Spark adopt on graph processing?

Any advice is much appreciated.

Best Regards,
Larry Xiao

Reference

Bipartite-oriented Distributed Graph Partitioning for Big Learning.
PowerLyra : Differentiated Graph Computation and Partitioning on Skewed 
Graphs

"
Eugene Cheipesh <echeipesh@gmail.com>,"Thu, 24 Jul 2014 08:40:58 -0700 (PDT)",pre-filtered hadoop RDD use case,dev@spark.incubator.apache.org,"Hello,

I have an interesting use case for a pre-filtered RDD. I have two solutions
that I am not entirly happy with and would like to get some feedback and
thoughts. Perhaps it is a use case that could be more explicitly supported
in Spark.

My data has well defined semantics for they key values that I can use to
pre-filter an RDD to exclude those partitions and records that I will not
need from being loaded at all. In most cases this is significant savings.

Essentially the dataset is geographic image tiles, as you would see on
google maps. The entire dataset could be huge, covering an entire continent
at high resolution. But if I want to work with a subset, lets say a single
city, it makes no sense for me to load all the partitions into memory just
so I can filter them as a first step.

First attempt was to extent NewHadoopRDD as follows:

abstract class PreFilteredHadoopRDD[K, V](
    sc : SparkContext,
    inputFormatClass: Class[_ <: InputFormat[K, V]],
    keyClass: Class[K],
    valueClass: Class[V],
    @transient conf: Configuration)
  extends NewHadoopRDD(sc, inputFormatClass, keyClass, valueClass, conf)
{
  /** returns true if specific partition has relevant keys */
  def includePartition(p: Partition): Boolean

  /** returns true if the specific key in the partition passes the filter */
  def includeKey(key: K): Boolean

  override def getPartitions: Array[Partition] = {
    val partitions = super.getPartitions
    partitions.filter(includePartition)
  }

  override def compute(theSplit: Partition, context: TaskContext) = {
    val ii = super.compute(theSplit, context)
    new InterruptibleIterator(ii.context, ii.delegate.filter{case (k,v) =includeKey(k)})
  }
} 

NewHadoopRDD for reference:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala

This is nice and handles the partition portion of the issue well enough, but
by the time the iterator is created by super.compute there is no way avoid
reading the values from records that do not pass my filter. 

Since I am actually using ‘SequenceFileInputFormat’ as my InputFormat I can
do better, and avoid deserializing the values if I could get my hands on the
reader and re-implement compute(). But this does not seem possible to do
through extension because both the NewHadooprRDD.confBroadcast and
NewHadoopPartition are private. There  does not seem to be a choice but to
copy/paste extend the NewHadoopRDD.

The two solutions that are apparent are:
1. remove those private modifiers
2. factor out reader creation to a method that can be used to reimplement
compute() in a sub-class

I would be curious to hear if anybody had/has similar problem and any
thoughts on the issue. If you think there is PR in this I’d be happy to code
it up and submit it.


Thank you
--
Eugene Cheipesh



--
3.nabble.com/pre-filtered-hadoop-RDD-use-case-tp7484.html
om.

"
Aaron Davidson <ilikerps@gmail.com>,"Thu, 24 Jul 2014 10:09:23 -0700",Re: Configuring Spark Memory,user@spark.apache.org,"Whoops, I was mistaken in my original post last year. By default, there is
one executor per node per Spark Context, as you said.
""spark.executor.memory"" is the amount of memory that the application
requests for each of its executors. SPARK_WORKER_MEMORY is the amount of
memory a Spark Worker is willing to allocate in executors.

So if you were to set SPARK_WORKER_MEMORY to 8g everywhere on your cluster,
and spark.executor.memory to 4g, you would be able to run 2 simultaneous
Spark Contexts who get 4g per node. Similarly, if spark.executor.memory
were 8g, you could only run 1 Spark Context at a time on the cluster, but
it would get all the cluster's memory.



"
Martin Goodson <martin@skimlinks.com>,"Thu, 24 Jul 2014 18:14:19 +0100",Re: Configuring Spark Memory,user@spark.apache.org,"Great - thanks for the clarification Aaron. The offer stands for me to
write some documentation and an example that covers this without leaving
*any* room for ambiguity.




-- 
Martin Goodson  |  VP Data Science
(0)20 3397 1240
[image: Inline image 1]



"
Aaron Davidson <ilikerps@gmail.com>,"Thu, 24 Jul 2014 10:32:28 -0700",Re: Configuring Spark Memory,,"More documentation on this would be undoubtedly useful. Many of the
properties changed or were deprecated in Spark 1.0, and I'm not sure our
current set of documentation via userlists is up to par, since many of the
previous suggestions are deprecated.



"
Art Peel <foundart@gmail.com>,"Thu, 24 Jul 2014 11:09:04 -0700",continuing processing when errors occur,dev@spark.apache.org,"Our system works with RDDs generated from Hadoop files. It processes each
record in a Hadoop file and for a subset of those records generates output
that is written to an external system via RDD.foreach. There are no
dependencies between the records that are processed.

If writing to the external system fails (due to a detail of what is being
written) and throws an exception, I see the following behavior:

1. Spark retries the entire partition (thus wasting time and effort),
reaches the problem record and fails again.
2. It repeats step 1 until the configured number of retries is reached and
then gives up. As a result, the rest of records from that Hadoop file are
not processed.
3. The executor where the final attempt occurred is marked as failed and
told to shut down and thus I lose a core for processing the remaining
Hadoop files, thus slowing down the entire process.

For this particular problem, I know how to prevent the underlying
exception, but I'd still like to get a handle on error handling for future
situations that I haven't yet encountered.

My goal is this:
Retry the problem record only (rather than starting over at the beginning
of the partition) up to N times, then give up and move on to process the
rest of the partition.

As far as I can tell, I need to supply my own retry behavior and if I want
to process records after the problem record I have to swallow exceptions
inside the foreach block.

My 2 questions are:
1. Is there anything I can do to prevent the executor where the final retry
occurred from being shut down when a failure occurs?

2. Are there ways Spark can help me get closer to my goal of retrying only
the problem record without writing my own re-try code and swallowing
exceptions?

Regards,
Art
"
Art Peel <foundart@gmail.com>,"Thu, 24 Jul 2014 11:11:46 -0700",Re: continuing processing when errors occur,dev@spark.apache.org,"Sorry, I sent this to the dev list instead of user.  Please ignore.  I'll
re-post to the correct list.

Regards,
Art




"
Stephen Boesch <javadba@gmail.com>,"Thu, 24 Jul 2014 12:04:55 -0700",SQLQuerySuite error,dev@spark.apache.org,"Are other developers seeing the following error for the recently added
substr() method?  If not, any ideas why the following invocation of tests
would be failing for me - i.e. how the given invocation would need to be
tweaked?

mvn -Pyarn -Pcdh5 test  -pl sql/core
-DwildcardSuites=org.apache.spark.sql.SQLQuerySuite

(note cdh5 is a custom profile for cdh5.0.0 but should not be affecting
these results)

other 33 tests pass.

SQLQuerySuite:
- SPARK-2041 column name equals tablename
- SPARK-2407 Added Parser of SQL SUBSTR() *** FAILED ***
  Exception thrown while executing query:
  == Logical Plan ==
  java.lang.UnsupportedOperationException
  == Optimized Logical Plan ==
  java.lang.UnsupportedOperationException
  == Physical Plan ==
  java.lang.UnsupportedOperationException
  == Exception ==
  java.lang.UnsupportedOperationException
  java.lang.UnsupportedOperationException
  at
org.apache.spark.sql.catalyst.analysis.EmptyFunctionRegistry$.lookupFunction(FunctionRegistry.scala:33)
  at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:131)
  at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:129)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
  at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:212)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:168)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org
$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:52)
  at
org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:66)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.immutable.List.foreach(List.scala:318)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
  at scala.collection.AbstractTraversable.map(Traversable.scala:105)
  at
"
Stephen Boesch <javadba@gmail.com>,"Thu, 24 Jul 2014 13:09:53 -0700",Re: SQLQuerySuite error,dev@spark.apache.org,"OK I did find my error.  The missing step:

  mvn install

I should have republished (mvn install) all of the other modules .

The mvn -pl  will rely on the modules locally published and so the latest
code that I had git pull'ed was not being used (except  the sql/core module
code).

The tests are passing after having properly performed the mvn install
before  running with the mvn -pl sql/core.




2014-07-24 12:04 GMT-07:00 Stephen Boesch <javadba@gmail.com>:

"
Neil Ferguson <nferguson@gmail.com>,"Thu, 24 Jul 2014 21:51:46 +0100","Re: ""Dynamic variables"" in Spark",dev@spark.apache.org,"I realised that my last reply wasn't very clear -- let me try and clarify.

The patch for named accumulators looks very useful, however in Shivaram's
example he was able to retrieve the named task metrics (statically) from a
TaskMetrics object, as follows:

TaskMetrics.get(""f1-time"")

However, I don't think this would be possible with the named accumulators
-- I believe they'd need to be passed to every function that needs them,
which I think would be cumbersome in any application of reasonable
complexity.

This is what I was trying to solve with my proposal for dynamic variables
in Spark. However, the ability to retrieve named accumulators from a
thread-local would work just as well for my use case. I'd be happy to
implement either solution if there's interest.

Alternatively, if I'm missing some other way to accomplish this please let
me know.

variables by broadcasting them. I was looking at Reynold's PR [1] to
broadcast the RDD object, and I think it would be possible to take a
similar approach -- that is, broadcast the serialized form, and deserialize
when executing each task.

[1] https://github.com/apache/spark/pull/1498








"
Patrick Wendell <pwendell@gmail.com>,"Thu, 24 Jul 2014 14:17:09 -0700","Re: ""Dynamic variables"" in Spark","""dev@spark.apache.org"" <dev@spark.apache.org>","What if we have a registry for accumulators, where you can access them
statically by name?

- Patrick


"
"""Neil Ferguson"" <nferguson@gmail.com>","Thu, 24 Jul 2014 14:30:39 -0700 (PDT)","Re: ""Dynamic variables"" in Spark",dev@spark.apache.org,"That would work well for me! Do you think it would be necessary to specify which accumulators should be available in the registry, or would we just broadcast all named accumulators registered in SparkContext and make them available in the registry?




Anyway, I'm happy to make the necessary changes (unless someone else wants to).


clarify.
Shivaram's
 a
accumulators

variables
let
dynamic
deserialize
Shivaram's
same
in
you
some
to
you
For
to
the
metrics
pter-8/counters
com>
public,
accumulators
every
won't
Accumulators
variable
variables
com>
.
may be
any
want
should
just
solution
a
(of that
""SparkDynamic"" would
to
parameter

work
aggregate
 it
containing
use
""dynamic
avoid
made
.
accessed
this
set in
Option[Any])(thunk:
the
the
d273dcea6
more
for
be
against
using
Accumulators
purely
, it"
Michael Armbrust <michael@databricks.com>,"Thu, 24 Jul 2014 18:28:10 -0700",Re: SQLQuerySuite error,dev@spark.apache.org,"Thanks for reporting back.  I was pretty confused trying to reproduce the
error :)



"
John Omernik <john@omernik.com>,"Thu, 24 Jul 2014 15:27:06 -0500",Re: Configuring Spark Memory,user@spark.apache.org,"SO this is good information for standalone, but how is memory distributed
within Mesos?  There's coarse grain mode where the execute stays active, or
theres fine grained mode where it appears each task is it's only process in
mesos, how to memory allocations work in these cases? Thanks!




"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Fri, 25 Jul 2014 11:23:35 -0700",Re: Suggestion for SPARK-1825,dev@spark.apache.org,"I have a similar issue with SPARK-1767.  There are basically three ways to
resolve the issue:

1. Use reflection to access classes newer than 0.21 (or whatever the oldest
version of Hadoop is that Spark supports)
2. Add a build variant (in Maven this would be a profile) that deals with
this.
3. Auto-detect which classes are available and use those.

#1 is the easiest for end-users, but it can lead to some ugly code.

#2 makes the code look nicer, but requires some effort on the part of
people building spark.  This can also lead to headaches for IDEs, if people
don't remember to select the new profile.  (For example, in IntelliJ, you
can't see any of the yarn classes when you import the project from Maven
without the YARN profile selected.)

#3 is something that... I don't know how to do in sbt or Maven.  I've been
told that an antrun task might work here, but it seems like it could get
really tricky.

Overall, I'd lean more towards #2 here.

best,
Colin



"
Gary Malouf <malouf.gary@gmail.com>,"Fri, 25 Jul 2014 14:27:56 -0400","Kryo Issue on Spark 1.0.1, Mesos 0.18.2","dev@spark.apache.org, user@spark.apache.org","After upgrading to Spark 1.0.1 from 0.9.1 everything seemed to be going
well.  Looking at the Mesos slave logs, I noticed:

ERROR KryoSerializer: Failed to run spark.kryo.registrator
java.lang.ClassNotFoundException:
com/mediacrossing/verrazano/kryo/MxDataRegistrator

My spark-env.sh has the following when I run the Spark Shell:

export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so

export MASTER=mesos://zk://n-01:2181,n-02:2181,n-03:2181/masters

export ADD_JARS=/opt/spark/mx-lib/verrazano-assembly.jar


# -XX:+UseCompressedOops must be disabled to use more than 32GB RAM

SPARK_JAVA_OPTS=""-Xss2m -XX:+UseCompressedOops
-Dspark.local.dir=/opt/mesos-tmp -Dspark.executor.memory=4g
 -Dspark.serializer=org.apache.spark.serializer.KryoSerializer
-Dspark.kryo.registrator=com.mediacrossing.verrazano.kryo.MxDataRegistrator
-Dspark.kryoserializer.buffer.mb=16 -Dspark.akka.askTimeout=30""


I was able to verify that our custom jar was being copied to each worker,
but for some reason it is not finding my registrator class.  Is anyone else
struggling with Kryo on 1.0.x branch?
"
Ankur Dave <ankurdave@gmail.com>,"Fri, 25 Jul 2014 12:49:41 -0700",Re: GraphX graph partitioning strategy,dev@spark.apache.org,"Hi Larry,

GraphX's graph constructor leaves the edges in their original partitions by
default. To support arbitrary multipass graph partitioning, one idea is to
take advantage of that by partitioning the graph externally to GraphX
(though possibly using information from GraphX such as the degrees), then
pass the partitioned edges to GraphX.

For example, if you had an edge partitioning function that needed the full
triplet to assign a partition, you could do this as follows:

val unpartitionedGraph: Graph[Int, Int] = ...val numPartitions: Int = 128
def getTripletPartition(e: EdgeTriplet[Int, Int]): PartitionID = ...
// Get the triplets using GraphX, then use Spark to repartition
themval partitionedEdges = unpartitionedGraph.triplets
  .map(e => (getTripletPartition(e), e))
  .partitionBy(new HashPartitioner(numPartitions))
val partitionedGraph = Graph(unpartitionedGraph.vertices, partitionedEdges)


A multipass partitioning algorithm could store its results in the edge
attribute, and then you could use the code above to do the partitioning.

Ankur <http://www.ankurdave.com/>



Skewed
"
Ankur Dave <ankurdave@gmail.com>,"Fri, 25 Jul 2014 13:03:09 -0700",Re: GraphX graph partitioning strategy,dev@spark.apache.org,"Oops, the code should be:

val unpartitionedGraph: Graph[Int, Int] = ...val numPartitions: Int = 128
def getTripletPartition(e: EdgeTriplet[Int, Int]): PartitionID = ...
// Get the triplets using GraphX, then use Spark to repartition
themval partitionedEdges = unpartitionedGraph.triplets
  .map(e => (getTripletPartition(e), e))
  .partitionBy(new HashPartitioner(numPartitions))
  *.map(pair => Edge(pair._2.srcId, pair._2.dstId, pair._2.attr))*
val partitionedGraph = Graph(unpartitionedGraph.vertices, partitionedEdges)


Ankur <http://www.ankurdave.com/>
"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Fri, 25 Jul 2014 15:09:53 -0700",Re: Suggestion for SPARK-1825,dev@spark.apache.org,"So, I'm leaning more towards using reflection for this.  Maven profiles
could work, but it's tough since we have new stuff coming in in 2.4, 2.5,
etc.  and the number of profiles will multiply quickly if we have to do it
that way.  Reflection is the approach HBase took in a similar situation.

best,
Colin



"
Reynold Xin <rxin@databricks.com>,"Fri, 25 Jul 2014 15:35:18 -0700",Re: Suggestion for SPARK-1825,"""dev@spark.apache.org"" <dev@spark.apache.org>","Actually reflection is probably a better, lighter weight process for this.
An extra project brings more overhead for something simple.






"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 25 Jul 2014 16:08:50 -0700",[VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.0.2.

This release fixes a number of bugs in Spark 1.0.1.
Some of the notable ones are
- SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
SPARK-1199. The fix was reverted for 1.0.2.
- SPARK-2576: NoClassDefFoundError when executing Spark QL query on
HDFS CSV file.
The full list is at http://s.apache.org/9NJ

The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~tdas/spark-1.0.2-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/tdas.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1024/

The documentation corresponding to this release can be found at:
http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/

Please vote on releasing this package as Apache Spark 1.0.2!

The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.0.2
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 25 Jul 2014 19:23:07 -0400",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),dev <dev@spark.apache.org>,"TD, there are a couple of unresolved issues slated for 1.0.2
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC>.
Should they be edited somehow?



"
Michael Armbrust <michael@databricks.com>,"Fri, 25 Jul 2014 17:35:24 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),dev@spark.apache.org,"That query is looking at ""Fix Version"" not ""Target Version"".  The fact that
the first one is still open is only because the bug is not resolved in
master.  It is fixed in 1.0.2.  The second one is partially fixed in 1.0.2,
but is not worth blocking the release for.



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 25 Jul 2014 19:00:16 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","The most important issue in this release is actually an ammendment to
an earlier fix. The original fix caused a deadlock which was a
regression from 1.0.0->1.0.1:

Issue:
https://issues.apache.org/jira/browse/SPARK-1097

1.0.1 Fix:
https://github.com/apache/spark/pull/1273/files (had a deadlock)

1.0.2 Fix:
https://github.com/apache/spark/pull/1409/files

I failed to correctly label this on JIRA, but I've updated it!


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 25 Jul 2014 22:01:59 -0400",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","OK, thanks for the clarification.

2014년 7월 25일 금요일, Michael Armbrust<michael@databricks.com>님이 작성한 메시지:

at
2,
20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
f00e195fb258f3f70f04756e07c259a2351f
:
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 25 Jul 2014 19:03:47 -0700",Re: Suggestion for SPARK-1825,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah I agree reflection is the best solution. Whenever we do
reflection we should clearly document in the code which YARN API
version corresponds to which code path. I'm guessing since YARN is
adding new features... we'll just have to do this over time.

- Patrick


"
Gary Malouf <malouf.gary@gmail.com>,"Fri, 25 Jul 2014 23:11:48 -0400","Re: Kryo Issue on Spark 1.0.1, Mesos 0.18.2","dev@spark.apache.org, user@spark.apache.org","Maybe this is me misunderstanding the Spark system property behavior, but
I'm not clear why the class being loaded ends up having '/' rather than '.'
in it's fully qualified name.  When I tested this out locally, the '/' were
preventing the class from being loaded.



"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Sat, 26 Jul 2014 11:41:13 +0800",Re: GraphX graph partitioning strategy,dev@spark.apache.org,"Hi Ankur,

Thanks for clear advice!

I tested the 4 partitioning algorithm in GraphX, and implemented two others.
And I find EdgePartition2D performs the best.
(the two other algorithms performs only tiny bit better on graphs that 
are highly skewed or bipartite)

Larry

"
Ted Yu <yuzhihong@gmail.com>,"Fri, 25 Jul 2014 21:14:01 -0800",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","HADOOP-10456 is fixed in hadoop 2.4.1

Does this mean that synchronization
on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for hadoop
2.4.1 ?

Cheers



"
Ted Yu <yuzhihong@gmail.com>,"Sat, 26 Jul 2014 06:42:15 -0800",setting inputMetrics in HadoopRDD#compute(),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
Starting at line 203:
      try {
        /* bytesRead may not exactly equal the bytes read by a task: split
boundaries aren't
         * always at record boundaries, so tasks may need to read into
other splits to complete
         * a record. */
        inputMetrics.bytesRead = split.inputSplit.value.getLength()
      } catch {
        case e: java.io.IOException =>
          logWarning(""Unable to get input size to set InputMetrics for
task"", e)
      }
      context.taskMetrics.inputMetrics = Some(inputMetrics)

If there is IOException, context.taskMetrics.inputMetrics is set by
wrapping inputMetrics - as if there wasn't any error.

I wonder if the above code should distinguish the error condition.

Cheers
"
Reynold Xin <rxin@databricks.com>,"Sat, 26 Jul 2014 10:47:30 -0700",Re: setting inputMetrics in HadoopRDD#compute(),"""dev@spark.apache.org"" <dev@spark.apache.org>","There is one piece of information that'd be useful to know, which is the
source of the input. Even in the presence of an IOException, the input
metrics still specifies the task is reading from Hadoop.

However, I'm slightly confused by this -- I think usually we'd want to
report the number of bytes read, rather than the total input size. For
example, if there is a limit (only read the first 5 records), the actual
number of bytes read is much smaller than the total split size.

Kay, am I mis-interpreting this?




"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 26 Jul 2014 11:46:59 -0700",Re: setting inputMetrics in HadoopRDD#compute(),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm working on a patch that switches this stuff out with the Hadoop
FileSystem StatisticsData, which will both give an accurate count and allow
us to get metrics while the task is in progress.  A hitch is that it relies
on https://issues.apache.org/jira/browse/HADOOP-10688, so we still might
want a fallback for versions of Hadoop that don't have this API.



"
Reynold Xin <rxin@databricks.com>,"Sat, 26 Jul 2014 12:12:33 -0700",Re: setting inputMetrics in HadoopRDD#compute(),"""dev@spark.apache.org"" <dev@spark.apache.org>","That makes sense, Sandy.

When you add the patch, can you make sure you comment inline on why the
fallback is needed?




"
Kay Ousterhout <kayousterhout@gmail.com>,"Sat, 26 Jul 2014 12:14:50 -0700",Re: setting inputMetrics in HadoopRDD#compute(),Reynold Xin <rxin@databricks.com>,"Reynold you're totally right, as discussed offline -- I didn't think about
the limit use case when I wrote this.  Sandy, is it easy to fix this as
part of your patch to use StatisticsData?  If not, I can fix it in a
separate patch.



"
Anand Avati <avati@gluster.org>,"Sat, 26 Jul 2014 19:30:42 -0700",Re: SparkContext startup time out,"user@spark.apache.org, dev@spark.apache.org","I am bumping into this problem as well. I am trying to move to akka 2.3.x
from 2.2.x in order to port to Scala 2.11 - only akka 2.3.x is available in
Scala 2.11. All 2.2.x akka works fine, and all 2.3.x akka give the
following exception in ""new SparkContext"". Still investigating why..

  java.util.concurrent.TimeoutException: Futures timed out after
[10000 milliseconds]
  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
  at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
  at scala.concurrent.Await$.result(package.scala:107)
  at akka.remote.Remoting.start(Remoting.scala:180)
  at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
  at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:618)
  at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:615)
  at akka.actor.ActorSystemImpl._start(ActorSystem.scala:615)





"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 09:52:40 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Ted - technically I think you are correct, although I wouldn't
recommend disabling this lock. This lock is not expensive (acquired
once per task, as are many other locks already). Also, we've seen some
cases where Hadoop concurrency bugs ended up requiring multiple fixes
- concurrency of client access is not well tested in the Hadoop
codebase since most of the Hadoop tools to not use concurrent access.
So in general it's good to be conservative in what we expect of the
Hadoop client libraries.

If you'd like to discuss this further, please fork a new thread, since
this is a vote thread. Thanks!


"
"""=?utf-8?B?d2l0Z28=?="" <witgo@qq.com>","Mon, 28 Jul 2014 01:43:04 +0800",Re:[VOTE] Release Apache Spark 1.0.2 (RC1),"""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","-1
The following bug should be fixed:
https://issues.apache.org/jira/browse/SPARK-2677‍





------------------ Original ------------------
From:  ""Tathagata Das"";<tathagata.das1565@gmail.com>;
Date:  Sat, Jul 26, 2014 07:08 AM
To:  ""dev@spar"
Andrew Ash <andrew@andrewash.com>,"Sun, 27 Jul 2014 10:47:52 -0700",Re:[VOTE] Release Apache Spark 1.0.2 (RC1),dev@spark.apache.org,"Is that a regression since 1.0.0?

f00e195fb258f3f70f04756e07c259a2351f
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 10:59:44 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","SPARK-2677 is a long standing issue and not a regression. Also, as far
as I can see there is no patch for it or clear understanding of the
cause. This type of bug does not warrant holding a release. If we fix
SPARK-2677 we can just make another release with the fix.


"
"""=?utf-8?B?d2l0Z28=?="" <witgo@qq.com>","Mon, 28 Jul 2014 02:04:30 +0800",Re:[VOTE] Release Apache Spark 1.0.2 (RC1),"""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","‍It's not sure. I only tested 1.0.1, 1.0.2 (RC) version.‍
In addition, my vote is not binding.‍‍




------------------ Original ------------------
From:  ""Andrew Ash"";<andrew@andrewash.com>;
Date:  Mon, Jul 28, 2014 01:47 AM
To:  ""dev""<dev@spark.apache.org>; 

Subject:  Re:[VOTE] Release Apache Spark 1.0.2 (RC1)



Is that a regression since 1.0.0?
On Jul 27, 2014 10:43 AM, ""witgo"" <witgo@qq.com> wrote:

> -1
> The following bug should be fixed:
> https://issues.apache.org/jira/browse/SPARK-2677‍
>
>
>
>
>
> ------------------ Original ------------------
> From:  ""Tathagata Das"";<tathagata.das1565@gmail.com>;
> Date:  Sat, Jul 26, 2014 07:08 AM
> To:  ""dev@spark.apache.org""<dev@spark.apache.org>;
>
> Subject:  [VOTE] Release Apache Spark 1.0.2 (RC1)
>
>
>
> Please vote on releasing the following candidate as Apache Spark version
> 1.0.2.
>
> This release fixes a number of bugs in Spark 1.0.1.
> Some of the notable ones are
> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> SPARK-1199. The fix was reverted for 1.0.2.
> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> HDFS CSV file.
> The full list is at http://s.apache.org/9NJ
>
> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>
> The release files, including signatures, digests, etc can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1024/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.2!
>
> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> [ ] +1 Release this package as Apache Spark 1.0.2
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 11:09:40 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","I just made a note on the JIRA - I realize it might not be clear
whether it was a regression, but in fact, this issue has been observed
in earlier versions as well.


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 11:31:17 -0700",branch-1.1 will be cut on Friday,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

the release branch is cut we'll start community QA and go into the
normal triage process for merging patches into that branch.

For Spark core, we'll be conservative in merging things past the
freeze date (e.g. high priority fixes) to ensure a healthy amount of
time for testing. A key focus of this release in core is improving
overall stability and resilience of Spark core.

As always, I'll encourage of committers/contributors to help review
patches this week to so we can get as many things in as possible.
People have been quite active recently, which is great!

Good luck!
- Patrick

"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 27 Jul 2014 14:55:11 -0400",Re: branch-1.1 will be cut on Friday,dev@spark.apache.org,"Good news, we will see the official version containing JDBC in very soon! 

Also, I have several pending PRs, can anyone continue the review process in this week?

Avoid overwriting already-set SPARK_HOME in spark-submit: https://github.com/apache/spark/pull/1331

fix locality inversion bug in TaskSetManager: https://github.com/apache/spark/pull/1313 (Matei and Mridulm are working on it)

Allow multiple executor per worker in Standalone mode: https://github.com/apache/spark/pull/731 

Ensure actor is self-contained  in DAGScheduler: https://github.com/apache/spark/pull/637

Best, 

-- 
Nan Zhu





"
Ted Yu <yuzhihong@gmail.com>,"Sun, 27 Jul 2014 11:01:57 -0800",Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for replying, Patrick.

The intention of my first email was for utilizing newer hadoop releases for
their bug fixes. I am still looking for clean way of passing hadoop release
version number to individual classes.
Using newer hadoop releases would encourage pushing bug fixes / new
features upstream. Ultimately Spark code would become cleaner.

Cheers


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 12:19:36 -0700","Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark
 1.0.2 (RC1)","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Ted,

We always intend Spark to work with the newer Hadoop versions and
encourage Spark users to use the newest Hadoop versions for best
performance.

We do try to be liberal in terms of supporting older versions as well.
This is because many people run older HDFS versions and we want Spark
to read and write data from them. So far we've been willing to do this
despite some maintenance cost.

The reason is that for many users it's very expensive to do a
whole-sale upgrade of HDFS, but trying out new versions of Spark is
much easier. For instance, some of the largest scale Spark users run
fairly old or forked HDFS versions.

- Patrick


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 27 Jul 2014 13:50:29 -0700",Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark 1.0.2 (RC1),dev@spark.apache.org,"For this particular issue, it would be good to know if Hadoop provides an API to determine the Hadoop version. If not, maybe that can be added to Hadoop in its next release, and we can check for it with reflection. We recently added a SparkContext.version() method in Spark to let you tell the version.

Matei


releases for
release
some
fixes
access.
since
hadoop
<pwendell@gmail.com>
to
fact
resolved in
in
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
Spark
fix
query on
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
found
https://repository.apache.org/content/repositories/orgapachespark-1024/
at:
passes if


"
Sean Owen <sowen@cloudera.com>,"Sun, 27 Jul 2014 21:57:28 +0100","Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark
 1.0.2 (RC1)","""dev@spark.apache.org"" <dev@spark.apache.org>","Good idea, although it gets difficult in the context of multiple
distributions. Say change X is not present in version A, but present
in version B. If you depend on X, what version can you look for to
detect it? The distribution will return ""A"" or ""A+X"" or somesuch, but
testing for ""A"" will give an incorrect answer, and the code can't be
expected to look for everyone's ""A+X"" versions. Actually inspecting
the code is more robust if a bit messier.

ote:
 API to determine the Hadoop version. If not, maybe that can be added to Hadoop in its next release, and we can check for it with reflection. We recently added a SparkContext.version() method in Spark to let you tell the version.
 for
ease
rote:
oop
o
act
 in
n
ND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
on
fb6f00e195fb258f3f70f04756e07c259a2351f
d
24/
if

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 27 Jul 2014 16:52:35 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),dev@spark.apache.org,"+1

Tested this on Mac OS X.

Matei


version 1.0.2.
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
https://repository.apache.org/content/repositories/orgapachespark-1024/


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 27 Jul 2014 16:54:21 -0700",Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark 1.0.2 (RC1),dev@spark.apache.org,"We could also do this, though it would be great if the Hadoop project provided this version number as at least a baseline. It's up to distributors to decide which version they report but I imagine they won't remove stuff that's in the reported version number.

Matei


provides an API to determine the Hadoop version. If not, maybe that can be added to Hadoop in its next release, and we can check for it with reflection. We recently added a SparkContext.version() method in Spark to let you tell the version.
well.
Spark
this
releases for
release
(acquired
some
fixes
access.
the
since
hadoop
<pwendell@gmail.com>
ammendment to
The fact
resolved in
fixed in
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
Spark
fix
query on
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
found
https://repository.apache.org/content/repositories/orgapachespark-1024/
at:
passes if


"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 27 Jul 2014 20:26:12 -0400",new JDBC server test cases seems failed ?,dev@spark.apache.org,"Hi, all

It seems that the JDBC test cases are failed unexpectedly in Jenkins?


[info] - test query execution against a Hive Thrift server *** FAILED *** [info] java.sql.SQLException: Could not open connection to jdbc:hive2://localhost:45518/: java.net.ConnectException: Connection refused [info] at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146) [info] at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info] at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:110) [info] at org.apache.spark.sql.hive.thri
ftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107) [info] ... [info] Cause: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused [info] at org.apache.thrift.transport.TSocket.open(TSocket.java:185) [info] at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248) [info] at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [info] at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144) [info] at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info] at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at org.apache.spark.sql.hive.thriftserver.H
iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134) [info] ... [info] Cause: java.net.ConnectException: Connection refused [info] at java.net.PlainSocketImpl.socketConnect(Native Method) [info] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) [info] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) [info] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) [info] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [info] at java.net.Socket.connect(Socket.java:579) [info] at org.apache.thrift.transport.TSocket.open(TSocket.java:180) [info] at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248) [info] at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [info] at org.apache.hive.jdbc.HiveConn
ection.openTransport(HiveConnection.java:144) [info] ... [info] CliSuite: Executing: create table hive_test1(key int, val string);, expecting output: OK [warn] four warnings found [warn] Note: /home/jenkins/workspace/SparkPullRequestBuilder@4/core/src/test/java/org/apache/spark/JavaAPISuite.java uses or overrides a deprecated API. [warn] Note: Recompile with -Xlint:deprecation for details. [info] - simple commands *** FAILED *** [info] java.lang.AssertionError: assertion failed: Didn't find ""OK"" in the output: [info] at scala.Predef$.assert(Predef.scala:179) [info] at org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUtils.scala:70) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scala:25) [info] at org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUtils.scala:62) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scala:25) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite
$$anonfun$1.apply$mcV$sp(CliSuite.scala:53) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51) [info] at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22) [info] at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22) [log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 14/07/27 17:06:43 INFO ClientBase: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties info] ... [info] ScalaTest [info] Run completed in 41 seconds, 789 milliseconds. [info] Total number of tests run: 2 [info] Suites: completed 2, aborted 0 [info] Tests: succeeded 0, failed 2, canceled 0, ignored 0, pending 0 [info] *** 2 TESTS FAILED ***

Best, 

-- 
Nan Zhu
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)

"
Michael Armbrust <michael@databricks.com>,"Sun, 27 Jul 2014 17:53:39 -0700",Re: new JDBC server test cases seems failed ?,"dev@spark.apache.org, Cheng Lian <lian@databricks.com>, 
	Patrick Wendell <patrick@databricks.com>","How recent is this? We've already reverted this patch once due to failing
tests.  It would be helpful to include a link to the failed build.  If its
failing again we'll have to revert again.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 27 Jul 2014 21:06:48 -0400",Re: new JDBC server test cases seems failed ?,dev@spark.apache.org,"it’s 20 minutes ago  

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17259/consoleFull  

--  
Nan Zhu



ling
s

AILED ***
:146)
info]
fo] at
 at
 at
ection(HiveThriftServer2Suite.scala:131)
tatement(HiveThriftServer2Suite.scala:134)
onfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:110)
er2Suite.scala:107)
onfun$1.apply(HiveThriftServer2Suite.scala:107)
rtException:
 at
48)
sport.java:37)
:144)
info]
fo] at
 at
 at

tatement(HiveThriftServer2Suite.scala:134)
n refused
info] at
va:339)
Impl.java:200)
:182)
92) [info]
 at
48)
sport.java:37)
 CliSuite:
 output:
a/org/apache/spark/JavaAPISuite.java
ILED ***
OK"" in the
o] at
ry(TestUtils.scala:70)
te.scala:25)
(TestUtils.scala:62)
.scala:25)
y(CliSuite.scala:51)
y(CliSuite.scala:51)
scala:22)
scala:22)
j
.
ile:
laTest
Total number
Tests:
* 2 TESTS


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 18:47:24 -0700",Re: new JDBC server test cases seems failed ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm going to revert it again - Cheng can you try to look into this? Thanks.


"
Stephen Boesch <javadba@gmail.com>,"Sun, 27 Jul 2014 19:07:47 -0700",No such file or directory errors running tests,dev@spark.apache.org,"I have pulled latest from github this afternoon.   There are many many
errors:

<source_home>/assembly/target/scala-2.10: No such file or directory

This causes many tests to fail.

Here is the command line I am running

    mvn -Pyarn -Phadoop-2.3 -Phive package test
"
Reynold Xin <rxin@databricks.com>,"Sun, 27 Jul 2014 19:10:26 -0700",Re: No such file or directory errors running tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","To run through all the tests you'd need to create the assembly jar first.


I've seen this asked a few times. Maybe we should make it more obvious.



http://spark.apache.org/docs/latest/building-with-maven.html

Spark Tests in Maven

Tests are run by default via the ScalaTest Maven plugin
<http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin>.
Some of the require Spark to be packaged first, so always run mvn package
 with -DskipTests the first time. You can then run the tests with mvn
-Dhadoop.version=... test.

The ScalaTest plugin also supports running only a specific test suite as
follows:

mvn -Dhadoop.version=... -DwildcardSuites=org.apache.spark.repl.ReplSuite test






"
Stephen Boesch <javadba@gmail.com>,"Sun, 27 Jul 2014 19:35:31 -0700",Re: No such file or directory errors running tests,dev@spark.apache.org,"i Reynold,
  thanks for responding here. Yes I had looked at the building with maven
page in the past.  I have not noticed  that the ""package"" step must happen
*before *the test.  I had assumed it were a corequisite -as seen in my
command line.

So the following sequence appears to work fine (so far so good - well past
when the prior attempts failed):


 mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
mvn -Pyarn -Phadoop-2.3 -Phive test

AFA documentation,  yes adding another sentence to that same ""Building with
Maven"" page would likely be helpful to future generations.


2014-07-27 19:10 GMT-07:00 Reynold Xin <rxin@databricks.com>:

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sun, 27 Jul 2014 19:34:27 -0700 (PDT)",Can I translate the documentations of Spark in Japanese?,dev@spark.incubator.apache.org,"Hi all,

I'm Yu Ishikawa, a Japanese.
I would like to translate the documentations of Spark 1.0.x officially.
If I will translate them and send a pull request, then can you merge it ?
And where is the best directory to create the Japanese documentations ?

Best,
Yu



--

"
Reynold Xin <rxin@databricks.com>,"Sun, 27 Jul 2014 19:36:37 -0700",Re: No such file or directory errors running tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","Would you like to submit a pull request? All doc source code are in the
docs folder. Cheers.




"
Stephen Boesch <javadba@gmail.com>,"Sun, 27 Jul 2014 19:39:25 -0700",Re: No such file or directory errors running tests,dev@spark.apache.org," OK i'll do it after confirming all the tests run


2014-07-27 19:36 GMT-07:00 Reynold Xin <rxin@databricks.com>:

"
Steve Nunez <snunez@hortonworks.com>,"Sun, 27 Jul 2014 20:02:20 -0700",Re: No such file or directory errors running tests,<dev@spark.apache.org>,"Whilst were on this topic, Id be interested to see if you get hive
failures. Im trying to build on a Mac using HDP and seem to be getting
failures related to Parquet. Ill know for sure once I get in tomorrow and
confirm with engineering, but this is likely because the version of Hive
is 0.12.0, and Parquet is only supported in Hive 0.13 (HDP is 0.13)

Any idea on what it would take to bump the Hive version up to the latest?

Regards,
	- SteveN







-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

"
Stephen Boesch <javadba@gmail.com>,"Sun, 27 Jul 2014 20:12:19 -0700",Re: No such file or directory errors running tests,dev@spark.apache.org,"Hi Steve,
  I am running on the cdh5.0.0 VM (which is CentOS 6.5)   Given the
difference in O/S and Hadoop distro between us my results are not likely to
be of direct help to you. But in any case i will let you know (likely
offline).


2014-07-27 20:02 GMT-07:00 Steve Nunez <snunez@hortonworks.com>:

 hive
ing
ow and
e
my
l
ng
y
to
at
ly
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 21:48:27 -0700",Re: Can I translate the documentations of Spark in Japanese?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Yu,

I think we could definitely put a pointer to documentation in other
languages that is hosted somewhere welse, but since we are not in a
position to maintain this, I'm not sure we could merge it into the
mainline Spark codebase. I'd be interested to know what other projects
do about this situation!

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 27 Jul 2014 21:48:27 -0700",Re: Can I translate the documentations of Spark in Japanese?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Yu,

I think we could definitely put a pointer to documentation in other
languages that is hosted somewhere welse, but since we are not in a
position to maintain this, I'm not sure we could merge it into the
mainline Spark codebase. I'd be interested to know what other projects
do about this situation!

- Patrick


"
Cheng Lian <lian@databricks.com>,"Mon, 28 Jul 2014 15:31:49 +0800",Re: new JDBC server test cases seems failed ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Noticed that Nans PR is not related to SQL, but the JDBC test suites got executed. Then I checked PRs of all those Jenkins builds that failed because of the JDBC suites, it turns out that none of them touched SQL code.  The JDBC code is only contained in the assembly file when the hive-thriftserver build profile is enabled. So it seems that the root cause is related to Maven build changes that makes the JDBC suites always get executed and fail because JDBC code isn't included in the assembly jar. This also explains why I cant reproduce it locally (I always enable hive-thriftserver profile), and why once the build fail, all JDBC suites fail together.

Working on a patch to fix this. Thanks to Patrick for helping debugging this!


inconvenience.
Thanks.
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17259/consoleFull
failing
If its
Jenkins?
FAILED ***
Connection
org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146)
org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info]
[info] at
[info] at
[info] at
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134)
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:110)
ftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107)
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107)
org.apache.thrift.transport.TTransportException:
at
org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)
org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info]
[info] at
[info] at
[info] at
iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134)
refused
[info] at
java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [info]
at
org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
CliSuite:
expecting output:
/home/jenkins/workspace/SparkPullRequestBuilder@4/core/src/test/java/org/apache/spark/JavaAPISuite.java
FAILED ***
""OK"" in the
org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUtils.scala:70)
org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scala:25)
org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUtils.scala:62)
org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scala:25)
org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51)
org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51)
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
log4j
info.
profile:
ScalaTest
number
Tests:
2 TESTS


"
Sean Owen <sowen@cloudera.com>,"Mon, 28 Jul 2014 09:58:36 +0100",Re: No such file or directory errors running tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","
Yes, it's unintuitive for Maven, since package always happens after
test, which kind of makes sense in general. I suppose we could bind
the generation of needed assemblies to the test phase instead, or as
well?

"
Sean Owen <sowen@cloudera.com>,"Mon, 28 Jul 2014 10:08:55 +0100","Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark
 1.0.2 (RC1)","""dev@spark.apache.org"" <dev@spark.apache.org>","Right, the scenario is, for example, that a class is added in release
2.5.0, but has been back-ported to a 2.4.1-based release. 2.4.1 isn't
missing anything from 2.4.1. But a version of ""2.4.1"" doesn't tell you
whether or not the class is there reliably.

By the way, I just found there is already such a class,
org.apache.hadoop.util.VersionInfo:

https://github.com/apache/hadoop-common/blob/release-2.4.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java

It appears to have been around for a long time. Theoretical problems
aside, there may be cases where querying the version is a fine and
reliable solution.

vided this version number as at least a baseline. It's up to distributors to decide which version they report but I imagine they won't remove stuff that's in the reported version number.
 an API to determine the Hadoop version. If not, maybe that can be added to Hadoop in its next release, and we can check for it with reflection. We recently added a SparkContext.version() method in Spark to let you tell the version.
te:
.
s
ses for
release
ome
es
s.
nce
e:
hadoop
om>
t to
e fact
ved in
d in
20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
rk
ix
ry on
=8fb6f00e195fb258f3f70f04756e07c259a2351f
ound
-1024/
t:
es if

"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Mon, 28 Jul 2014 19:33:41 +0800",package/assemble with local spark,dev@spark.apache.org,"Hi,

How do you package an app with modified spark?

In seems sbt would resolve the dependencies, and use the official spark 
release.

Thank you!

Larry

"
jitendra shelar <jitendra.shelar410@gmail.com>,"Mon, 28 Jul 2014 19:45:55 +0530",Fraud management system implementation,dev@spark.apache.org,"Hi,

I am new to spark. I am learning spark and scala.

I had some queries.

1) Can somebody please tell me if it is possible to implement credit
card fraud management system using spark?
2) If yes, can somebody please guide me how to proceed.
3) Shall I prefer Scala or Java for this implementation?

4) Please suggest me some pointers related to Hidden Markonav Model
(HMM) and anomaly detection in data mining (using spark).

Thanks,
Jitendra

"
Steve Nunez <snunez@hortonworks.com>,"Mon, 28 Jul 2014 07:44:08 -0700",Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I saw a note earlier, perhaps on the user list, that at least one person is
using Hive 0.13. Anyone got a working build configuration for this version
of Hive?

Regards,
- Steve



-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Jul 2014 08:01:01 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I found 0.13.1 artifacts in maven:
http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar

However, Spark uses groupId of org.spark-project.hive, not org.apache.hive

Can someone tell me how it is supposed to work ?

Cheers



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 28 Jul 2014 11:15:36 -0400",Re: Fraud management system implementation,dev <dev@spark.apache.org>,"This sounds more like a user list <https://spark.apache.org/community.html>
question. This is the dev list, where people discuss things related to
contributing code and such to Spark.



"
Sean Owen <sowen@cloudera.com>,"Mon, 28 Jul 2014 16:26:29 +0100",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","The reason for org.spark-project.hive is that Spark relies on
hive-exec, but the Hive project does not publish this artifact by
itself, only with all its dependencies as an uber jar. Maybe that's
been improved. If so, you need to point at the new hive-exec and
perhaps sort out its dependencies manually in your build.


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Jul 2014 08:47:58 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","hive-exec (as of 0.13.1) is published here:
http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar

Should a JIRA be opened so that dependency on hive-metastore can be
replaced by dependency on hive-exec ?

Cheers



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 28 Jul 2014 09:13:40 -0700",Re: Fraud management system implementation,"""user@spark.apache.org"" <user@spark.apache.org>","+user list
bcc: dev list

It's definitely possible to implement credit fraud management using Spark.
 A good start would be using some of the supervised learning algorithms
that Spark provides in MLLib (logistic regression or linear SVMs).

Spark doesn't "
Sean Owen <sowen@cloudera.com>,"Mon, 28 Jul 2014 17:15:50 +0100",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yes, it is published. As of previous versions, at least, hive-exec
included all of its dependencies *in its artifact*, making it unusable
as-is because it contained copies of dependencies that clash with
versions present in other artifacts, and can't be managed with Maven
mechanisms.

I am not sure why hive-exec was not published normally, with just its
own classes. That's why it was copied, into an artifact with just
hive-exec code.

You could do the same thing for hive-exec 0.13.1.
Or maybe someone knows that it's published more 'normally' now.
I don't think hive-metastore is related to this question?

I am no expert on the Hive artifacts, just remembering what the issue
was initially in case it helps you get to a similar solution.


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Jul 2014 09:34:19 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Talked with Owen offline. He confirmed that as of 0.13, hive-exec is still
uber jar.

Right now I am facing the following error building against Hive 0.13.1 :

[ERROR] Failed to execute goal on project spark-hive_2.10: Could not
resolve dependencies for project
org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
artifacts could not be resolved:
org.spark-project.hive:hive-metastore:jar:0.13.1,
org.spark-project.hive:hive-exec:jar:0.13.1,
org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
org.spark-project.hive:hive-metastore:jar:0.13.1 in
http://repo.maven.apache.org/maven2 was cached in the local repository,
resolution will not be reattempted until the update interval of maven-repo
has elapsed or updates are forced -> [Help 1]

Some hint would be appreciated.

Cheers



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 28 Jul 2014 09:55:14 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","It would be great if the hive team can fix that issue. If not, we'll
have to continue forking our own version of Hive to change the way it
publishes artifacts.

- Patrick


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Jul 2014 10:01:17 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Owen helped me find this:
https://issues.apache.org/jira/browse/HIVE-7423

I guess this means that for Hive 0.14, Spark should be able to directly
pull in hive-exec-core.jar

Cheers



"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 28 Jul 2014 10:02:53 -0700",Re: Working Formula for Hive 0.13?,dev@spark.apache.org,"Where and how is that fork being maintained?  I'm not seeing an obviously
correct branch or tag in the main asf hive repo & github mirror.



"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 29 Jul 2014 01:16:46 +0800",Re: Working Formula for Hive 0.13?,dev@spark.apache.org,"AFAIK, according a recent talk, Hulu team in China has built Spark SQL
against Hive 0.13 (or 0.13.1?) successfully. Basically they also
re-packaged Hive 0.13 as what the Spark team did. The slides of the talk
hasn't been released yet though.



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 28 Jul 2014 10:17:16 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah so we need a model for this (Mark - do you have any ideas?). I
did this in a personal github repo. I just did it quickly because
dependency issues were blocking the 1.0 release:

https://github.com/pwendell/hive/tree/branch-0.12-shaded-protobuf

I think what we want is to have a semi official github repo with an
index to each of the shaded dependencies and what version is included
in which branch.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 28 Jul 2014 10:20:17 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've heard from Cloudera that there were hive internal changes between
0.12 and 0.13 that required code re-writing. Over time it might be
possible for us to integrate with hive using API's that are more
stable (this is the domain of Michael/Cheng/Yin more than me!). It
would be interesting to see what the Hulu folks did.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 28 Jul 2014 10:24:38 -0700",Re: Working Formula for Hive 0.13?,dev@spark.apache.org,"Getting and maintaining our own branch in the main asf hive repo is a
non-starter or isn't workable?



"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 29 Jul 2014 01:50:42 +0800",Re: Working Formula for Hive 0.13?,dev@spark.apache.org,"Exactly, forgot to mention Hulu team also made changes to cope with those
incompatibility issues, but they said that’s relatively easy once the
re-packaging work is done.


:

k
y
t
is
ot
c
0.13.1%7Cjar
n
nd
re%7C0.13.1%7Cjar
st
or
by
"
Ankur Dave <ankurdave@gmail.com>,"Mon, 28 Jul 2014 11:44:26 -0700",Re: VertexPartition and ShippableVertexPartition,"""user@spark.apache.org"" <user@spark.apache.org>","

There is a VertexPartition in the EdgePartition,which is created by
Is the VertexPartition in the EdgePartition, the Mirror Cache part?


Yes, exactly. The primary copy of each vertex is stored in the VertexRDD
using the index, values, and mask data structures, which together form a
hash map. In addition, each partition of the VertexRDD stores the
corresponding partition of the routing table to facilitate joining with the
edges. The ShippableVertexPartition class encapsulates the vertex hash map
along with a RoutingTablePartition.

After joining the vertices with the edges, the edge partitions cache their
adjacent vertices in the mirror cache. They use the VertexPartition for
this, which provides only the hash map functionality and not the routing
table.

Ankur <http://www.ankurdave.com/>
"
Steve Nunez <snunez@hortonworks.com>,"Mon, 28 Jul 2014 13:06:50 -0700",'Proper' Build Tool,"""dev@spark.apache.org"" <dev@spark.apache.org>","Gents,

It seem that until recently, building via sbt was a documented process in
the 0.9 overview:

http://spark.apache.org/docs/0.9.0/

The section on building mentions using sbt/sbt assembly. However in the
latest overview:

http://spark.apache.org/docs/latest/index.html

Theres no mention of building with sbt.

Whats the recommended way to build? What are most people using in their
daily workflow?

Cheers,
- SteveN





-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.
"
Stephen Boesch <javadba@gmail.com>,"Mon, 28 Jul 2014 13:20:32 -0700",Re: 'Proper' Build Tool,dev@spark.apache.org,"Hi Steve,
  I had the opportunity to ask this question at the Summit to Andrew Orr.
 He mentioned that with 1.0 the recommended build tool is with maven. sbt
is however still supported. You will notice that the dependencies are now
completely handled within the maven pom.xml:  the SparkBuild.scala /sbt
reads the dependencies from the pom.xml.

Andrew further suggested to look at the make-distribution.sh to see the
""recommended"" way to create builds.  Using mvn on the command line is fine
- but the aforementioned script provides a framework /guideline to set
things up properly.




2014-07-28 13:06 GMT-07:00 Steve Nunez <snunez@hortonworks.com>:

heir
to
at
ly
"
giwa <ugw.gi.world@gmail.com>,"Mon, 28 Jul 2014 13:23:19 -0700 (PDT)",Re: Can I translate the documentations of Spark in Japanese?,dev@spark.incubator.apache.org,"Hi Yu,

I could help translating Spark documentation to Japanese. Please let me
know if you need.

Best,

Ken






-- 

Kenichi Takagiwa
-------------------------------------------------------------
Keio University
Graduate School of Science and Technology
Department of Open and Environmental Systems
Faculty of Computer Science
Hiroaki Nishi Laboratory
Email: ugw.gi.world@gmail.com
Phone: +81-50-3575-6586




--"
Patrick Wendell <pwendell@gmail.com>,"Mon, 28 Jul 2014 13:25:49 -0700",Re: 'Proper' Build Tool,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah for packagers we officially recommend using maven. Spark's
dependency graph is very complicated and Maven and SBT use different
conflict resolution strategies, so we've opted to official support
Maven.

SBT is still around though and it's used more often by day-to-day developers.

- Patrick

"
Steve Nunez <snunez@hortonworks.com>,"Mon, 28 Jul 2014 13:32:38 -0700",Re: Working Formula for Hive 0.13?,<dev@spark.apache.org>,"So, do we have a short-term fix until Hive 0.14 comes out? Perhaps adding
the hive-exec jar to the spark-project repo? It doesnt look like theres
a release date schedule for 0.14.







-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 28 Jul 2014 13:33:30 -0700",Jenkins Documentation Build,"Patrick Wendell <pwendell@gmail.com>, dev@spark.apache.org","Hi Patrick,

I started to work on the documentation about my work in spark. Since
it has lots of dependencies to get the document build setup locally,
it will be nice that people are able to verify/preview the document
build for each PR. Is it possible to build the doc in Jenkins, and
have a link pointing to the doc in github comment?

Also, it will be very handy to have the test report link in the github
comment like this one,
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17283/testReport/
Then developers can easily find which tests fail the build.

(ps, seems that the build system is not merging the PR approved by
committer now.)

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 28 Jul 2014 16:34:07 -0400",Re: Can I translate the documentations of Spark in Japanese?,dev <dev@spark.apache.org>,"

I know some projects get translations crowdsourced via one website or
other. Googling real quick, it appears there are a few sites that offer
homes for this kind of work, but I wouldn't know about any of them.

Nick
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 28 Jul 2014 16:34:07 -0400",Re: Can I translate the documentations of Spark in Japanese?,dev <dev@spark.apache.org>,"

I know some projects get translations crowdsourced via one website or
other. Googling real quick, it appears there are a few sites that offer
homes for this kind of work, but I wouldn't know about any of them.

Nick
"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 28 Jul 2014 14:59:12 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Let me add my vote as well.
Did some basic tests by running simple projects with various Spark
modules. Tested checksums.

+1


"
Reynold Xin <rxin@databricks.com>,"Mon, 28 Jul 2014 15:41:00 -0700",Re: package/assemble with local spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","You can use publish-local in sbt.

If you want to be more careful, you can give Spark a different version
number and use that version number in your app.




"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Jul 2014 15:41:47 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","After manually copying hive 0.13.1 jars to local maven repo, I got the
following errors when building spark-hive_2.10 module :

[ERROR]
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala:182:
type mismatch;
 found   : String
 required: Array[String]
[ERROR]       val proc: CommandProcessor =
CommandProcessorFactory.get(tokens(0), hiveconf)
[ERROR]
 ^
[ERROR]
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala:60:
value getAllPartitionsForPruner is not a member of org.apache.
 hadoop.hive.ql.metadata.Hive
[ERROR]         client.getAllPartitionsForPruner(table).toSeq
[ERROR]                ^
[ERROR]
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala:267:
overloaded method constructor TableDesc with alternatives:
  (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
Class[_],x$3: java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
<and>
  ()org.apache.hadoop.hive.ql.plan.TableDesc
 cannot be applied to (Class[org.apache.hadoop.hive.serde2.Deserializer],
Class[(some other)?0(in value tableDesc)(in value tableDesc)], Class[?0(in
value tableDesc)(in   value tableDesc)], java.util.Properties)
[ERROR]   val tableDesc = new TableDesc(
[ERROR]                   ^
[WARNING] Class org.antlr.runtime.tree.CommonTree not found - continuing
with a stub.
[WARNING] Class org.antlr.runtime.Token not found - continuing with a stub.
[WARNING] Class org.antlr.runtime.tree.Tree not found - continuing with a
stub.
[ERROR]
     while compiling:
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
        during phase: typer
     library version: version 2.10.4
    compiler version: version 2.10.4

The above shows incompatible changes between 0.12 and 0.13.1
e.g. the first error corresponds to the following method
in CommandProcessorFactory :
  public static CommandProcessor get(String[] cmd, HiveConf conf)

Cheers


:

ere¹s
e
he
QL
m
y
d
.
e
on
e
c
m
t
n
s
to
at
ly
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Jul 2014 16:05:49 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I was looking for a class where reflection-related code should reside.

I found this but don't think it is the proper class for bridging
differences between hive 0.12 and 0.13.1:
sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala

Cheers



text.scala:182:
astoreCatalog.scala:60:
astoreCatalog.scala:267:
sc
n
b.
scala
g
here¹s
se
the
n
:
ay
e
ld
f
h
w.
he
C
e
be
ec
o
ot
on
is
.
e
r
"
Michael Armbrust <michael@databricks.com>,"Mon, 28 Jul 2014 17:22:20 -0700",Re: Working Formula for Hive 0.13?,dev@spark.apache.org,"A few things:
 - When we upgrade to Hive 0.13.0, Patrick will likely republish the
hive-exec jar just as we did for 0.12.0
 - Since we have to tie into some pretty low level APIs it is unsurprising
that the code doesn't just compile out of the box against 0.13.0
 - ScalaReflection is for determining Schema from Scala classes, not
reflection based bridge code.  Either way its unclear to if there is any
reason to use reflection to support multiple versions, instead of just
upgrading to Hive 0.13.0

it purely because you are having problems connecting to newer metastores?
 Are there some features you are hoping for?  This will help me prioritize
this effort.

Michael



.scala
text.scala:182:
astoreCatalog.scala:60:
astoreCatalog.scala:267:
],
g
 a
scala
e the
k
he
d
it
h
h
h
n.
m
es
.
at
t
e
e
ty
,
"
Steve Nunez <snunez@hortonworks.com>,"Mon, 28 Jul 2014 17:27:28 -0700",Re: Working Formula for Hive 0.13?,<dev@spark.apache.org>,"The larger goal is to get a clean compile & test in the environment I have
to use. As near as I can tell, tests fail in parquet because parquet was
only added in Hive 0.13. There could well be issues in later meta-stores,
but one thing at a time...

	- SteveN




e
ce



-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 28 Jul 2014 17:41:17 -0700",Github mirroring is running behind,"""dev@spark.apache.org"" <dev@spark.apache.org>","https://issues.apache.org/jira/browse/INFRA-8116

Just a heads up, the github mirroring is running behind. You can
follow that JIRA to keep up to date on the fix.

In the mean time you can use the Apache git itself:

https://git-wip-us.apache.org/repos/asf/spark.git

Some people have reported issues checking out Apache git as well, but
it might work.

- Patrick

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Jul 2014 17:59:29 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","bq. Either way its unclear to if there is any reason to use reflection to
support multiple versions, instead of just upgrading to Hive 0.13.0

Which Spark release would this Hive upgrade take place ?
I agree it is cleaner to upgrade Hive dependency vs. introducing reflection.

Cheers



g
e
.scala
e
text.scala:182:
astoreCatalog.scala:60:
astoreCatalog.scala:267:
scala
ke
nce the
m
e
t
o
t,
he
l
al
g
'
at
e,
t
t
ed
"
Andrew Or <andrew@databricks.com>,"Mon, 28 Jul 2014 18:52:13 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 Tested on standalone and yarn clusters


2014-07-28 14:59 GMT-07:00 Tathagata Das <tathagata.das1565@gmail.com>:

"
qingyang li <liqingyang1985@gmail.com>,"Tue, 29 Jul 2014 10:36:42 +0800","Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?",dev@spark.apache.org,"hi, haoyuan, thanks for replying.


2014-07-21 16:29 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:

"
Mubarak Seyed <spark.devuser@gmail.com>,"Mon, 28 Jul 2014 20:29:28 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),dev@spark.apache.org,"+1 (non-binding)

Tested this on Mac OS X.



"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 28 Jul 2014 22:17:30 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),dev@spark.apache.org,"+1

Tested basic spark-shell and pyspark operations and MLlib examples on a Mac.


"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 28 Jul 2014 22:46:23 -0700",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","NOTICE and LICENSE files look good
Hashes and sigs look good
No executable in the source distribution
Compile source and run standalone

+1

- Henry


"
Reynold Xin <rxin@databricks.com>,"Mon, 28 Jul 2014 22:57:32 -0700",Re: Github mirroring is running behind,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi devs,

I don't know if this is going to help, but if you can watch & vote on the
ticket, it might help ASF INFRA prioritize and triage it faster:
https://issues.apache.org/jira/browse/INFRA-8116

Please do. Thanks!




"
Reynold Xin <rxin@databricks.com>,"Tue, 29 Jul 2014 00:54:43 -0700",Re: pre-filtered hadoop RDD use case,"""dev@spark.apache.org"" <dev@spark.apache.org>","Would something like this help?

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala





ns
d
nt
e
t
=>
e/spark/rdd/NewHadoopRDD.scala
d
 InputFormat I can
o
ppy to
doop-RDD-use-case-tp7484.html
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 29 Jul 2014 11:52:50 -0400",Re: [VOTE] Release Apache Spark 1.0.2 (RC1),dev <dev@spark.apache.org>,"   - spun up an EC2 cluster successfully using spark-ec2
   - tested S3 file access from that cluster successfully

+1
​



n
f00e195fb258f3f70f04756e07c259a2351f
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 29 Jul 2014 18:35:16 +0000",RE: pre-filtered hadoop RDD use case,"""dev@spark.apache.org"" <dev@spark.apache.org>","PartitionPruningRDD.scala still only handles, as said, the partition portion of the issue. 

On the ""record pruning"" portion, although cheap fixes could be available for this issue as reported, but I believe a
fundamental issue is lack of a mechanism of processing merging/pushdown. Given the popularity of columnar, and even more intelligent, data stores,
it makes sense to support some ""post processing"" before a RDD is formed. This ""post processing"" could be performed by the RDD itself in compute(); or it could be performed by some data store which supports such pushdowns. In the later case, such processing info should be made available to the data store RDD to pass on to the stores. 

For instance, FilteredRDD requires the parent to materialize the record fully before it can start its own processing. If it could be ""merged"" with its parent, a much smaller RDD footprint would result.

""Pipelined execution"" is mentioned for NarrowDependency in code, but no implementation seems to be in place, and more optimization is desired beyond just record-oriented execution pipelining.



-----Original Message-----
From: Reynold Xin [mailto:rxin@databricks.com] 
Sent: Tuesday, July 29, 2014 12:55 AM
To: dev@spark.apache.org
Subject: Re: pre-filtered hadoop RDD use case

Would something like this help?

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala




On Thu, Jul 24, 2014 at 8:40 AM, Eugene Cheipesh <echeipesh@gmail.com>
wrote:

> Hello,
>
> I have an interesting use case for a pre-filtered RDD. I have two 
> solutions that I am not entirly happy with and would like to get some 
> feedback and thoughts. Perhaps it is a use case that could be more 
> explicitly supported in Spark.
>
> My data has well defined semantics for they key values that I can use 
> to pre-filter an RDD to exclude those partitions and records that I 
> will not need from being loaded at all. In most cases this is significant savings.
>
> Essentially the dataset is geographic image tiles, as you would see on 
> google maps. The entire dataset could be huge, covering an entire 
> continent at high resolution. But if I want to work with a subset, 
> lets say a single city, it makes no sense for me to load all the 
> partitions into memory just so I can filter them as a first step.
>
> First attempt was to extent NewHadoopRDD as follows:
>
> abstract class PreFilteredHadoopRDD[K, V](
>     sc : SparkContext,
>     inputFormatClass: Class[_ <: InputFormat[K, V]],
>     keyClass: Class[K],
>     valueClass: Class[V],
>     @transient conf: Configuration)
>   extends NewHadoopRDD(sc, inputFormatClass, keyClass, valueClass, 
> conf) {
>   /** returns true if specific partition has relevant keys */
>   def includePartition(p: Partition): Boolean
>
>   /** returns true if the specific key in the partition passes the 
> filter */
>   def includeKey(key: K): Boolean
>
>   override def getPartitions: Array[Partition] = {
>     val partitions = super.getPartitions
>     partitions.filter(includePartition)
>   }
>
>   override def compute(theSplit: Partition, context: TaskContext) = {
>     val ii = super.compute(theSplit, context)
>     new InterruptibleIterator(ii.context, ii.delegate.filter{case 
> (k,v) =>
> includeKey(k)})
>   }
> }
>
> NewHadoopRDD for reference:
>
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/ap
> ache/spark/rdd/NewHadoopRDD.scala
>
> This is nice and handles the partition portion of the issue well 
> enough, but by the time the iterator is created by super.compute there 
> is no way avoid reading the values from records that do not pass my 
> filter.
>
> Since I am actually using ‘SequenceFileInputFormat’ as my InputFormat 
> I can do better, and avoid deserializing the values if I could get my 
> hands on the reader and re-implement compute(). But this does not seem 
> possible to do through extension because both the 
> NewHadooprRDD.confBroadcast and NewHadoopPartition are private. There  
> does not seem to be a choice but to copy/paste extend the 
> NewHadoopRDD.
>
> The two solutions that are apparent are:
> 1. remove those private modifiers
> 2. factor out reader creation to a method that can be used to 
> reimplement
> compute() in a sub-class
>
> I would be curious to hear if anybody had/has similar problem and any 
> thoughts on the issue. If you think there is PR in this I’d be happy 
> to code it up and submit it.
>
>
> Thank you
> --
> Eugene Cheipesh
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/pre-filtered
> -hadoop-RDD-use-case-tp7484.html Sent from the Apache Spark Developers 
> List mailing list archive at Nabble.com.
>
"
Reynold Xin <rxin@databricks.com>,"Tue, 29 Jul 2014 11:44:07 -0700",Re: pre-filtered hadoop RDD use case,"""dev@spark.apache.org"" <dev@spark.apache.org>","I am not sure if I agree that it lacks the mechanism to do pushdowns.

Hadoop InputFormat itself provides some basic mechanism to push down
predicates already. The HBase InputFormat already implements it. In Spark,
you can also run arbitrary user code, and you can decide what to do. You
can also just subclass RDD to deal with arbitrary input sources. In the
future, we will build a more standard API to interface with external stores
in SchemaRDD.

The topic of discussion here is whether Eugene can reuse as much of
HadoopRDD/NewHadoopRDD as possible. We can certainly make the HadoopRDD
interface more pluggable, but that'd require opening up the internals of
that class and stabilize the API. I am not sure if it is something we'd
want to do in a hurry, because there is a clear workaround right now
(subclass RDD) and it is very hard to change that once the project is
committed to that API.







.
h
e/spark/rdd/PartitionPruningRDD.scala
my InputFormat
happy
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 29 Jul 2014 19:43:04 +0000",RE: pre-filtered hadoop RDD use case,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Reynold,

I agree that we should not hurry right now to modify/enhance APIs and could be satisfied with extending existing ones as much as possible. On the other hand, more intelligent data stores like HBase or Cassendra do support
 complex pushdowns, often more complex than their MR interfaces can now support. Given the increasing popularity of those stores, it will probably make sense to factor out some common behavior to put it in RDD. SchemaRDD is good for structured, tabular data. But RDD is more generic.

Again, this goal can be long-term and I did not mean to be in hurry. It just occurred to me that without extra processing info there is no way to avoid ""reimplementing"" the ""compute"" method of the NewHadoopRDD if the reader can't be changed; which brought me to the ""bigger"" picture.

Thanks and regards,

Yan


-----Origins.com] 
Sent: Tuesday, July 29, 2014 11:44 AM
To: dev@spark.apache.org
Subject: Re: pre-filtered hadoop RDD use case

I am not sure if I agree that it lacks the mechanism to do pushdowns.

Hadoop InputFormat itself provides some basic mechanism to push down predicates already. The HBase InputFormat already implements it. In Spark, you can also run arbitrary user code, and you can decide what to do. You can also just subclass RDD to deal with arbitrary input sources. In the future, we will build a more standard API to interface with external stores in SchemaRDD.

The topic of discussion here is whether Eugene can reuse as much of HadoopRDD/NewHadoopRDD as possible. We can certainly make the HadoopRDD interface more pluggable, but that'd require opening up the internals of that class and stabilize the API. I am not sure if it is something we'd want to do in a hurry, because there is a clear workaround right now (subclass RDD) and it is very hard to change that once the project is committed to that API.






On Tue, Jul 29, 2014 at 11:35 AM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com>
wrote:

> PartitionPruningRDD.scala still only handles, as said, the partition 
> portion of the issue.
>
> On the ""record pruning"" portion, although cheap fixes could be 
> available for this issue as reported, but I believe a fundamental 
> issue is lack of a mechanism of processing merging/pushdown.
> Given the popularity of columnar, and even more intelligent, data 
> stores, it makes sense to support some ""post processing"" before a RDD is formed.
> This ""post processing"" could be performed by the RDD itself in 
> compute(); or it could be performed by some data store which supports such pushdowns.
> In the later case, such processing info should be made available to 
> the data store RDD to pass on to the stores.
>
> For instance, FilteredRDD requires the parent to materialize the 
> record fully before it can start its own processing. If it could be 
> ""merged"" with its parent, a much smaller RDD footprint would result.
>
> ""Pipelined execution"" is mentioned for NarrowDependency in code, but 
> no implementation seems to be in place, and more optimization is 
> desired beyond just record-oriented execution pipelining.
>
>
>
> -----Original Message-----
> From: Reynold Xin [mai12:55 AM
> To: dev@spark.apache.org
> Subject: Re: pre-filtered hadoop RDD use case
>
> Would something like this help?
>
>
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/ap
> ache/spark/rdd/PartitionPruningRDD.scala
>
>
>
>
> On Thu, Jul 24, 2014 at 8:40 AM, Eugene Cheipesh <echeipesh@gmail.com>
> wrote:
>
> > Hello,
> >
> > I have an interesting use case for a pre-filtered RDD. I have two 
> > solutions that I am not entirly happy with and would like to get 
> > some feedback and thoughts. Perhaps it is a use case that could be 
> > more explicitly supported in Spark.
> >
> > My data has well defined semantics for they key values that I can 
> > use to pre-filter an RDD to exclude those partitions and records 
> > that I will not need from being loaded at all. In most cases this is
> significant savings.
> >
> > Essentially the dataset is geographic image tiles, as you would see 
> > on google maps. The entire dataset could be huge, covering an entire 
> > continent at high resolution. But if I want to work with a subset, 
> > lets say a single city, it makes no sense for me to load all the 
> > partitions into memory just so I can filter them as a first step.
> >
> > First attempt was to extent NewHadoopRDD as follows:
> >
> > abstract class PreFilteredHadoopRDD[K, V](
> >     sc : SparkContext,
> >     inputFormatClass: Class[_ <: InputFormat[K, V]],
> >     keyClass: Class[K],
> >     valueClass: Class[V],
> >     @transient conf: Configuration)
> >   extends NewHadoopRDD(sc, inputFormatClass, keyClass, valueClass,
> > conf) {
> >   /** returns true if specific partition has relevant keys */
> >   def includePartition(p: Partition): Boolean
> >
> >   /** returns true if the specific key in the partition passes the 
> > filter */
> >   def includeKey(key: K): Boolean
> >
> >   override def getPartitions: Array[Partition] = {
> >     val partitions = super.getPartitions
> >     partitions.filter(includePartition)
> >   }
> >
> >   override def compute(theSplit: Partition, context: TaskContext) = {
> >     val ii = super.compute(theSplit, context)
> >     new InterruptibleIterator(ii.context, ii.delegate.filter{case
> > (k,v) =>
> > includeKey(k)})
> >   }
> > }
> >
> > NewHadoopRDD for reference:
> >
> > https://github.com/apache/spark/blob/master/core/src/main/scala/org/
> > ap
> > ache/spark/rdd/NewHadoopRDD.scala
> >
> > This is nice and handles the partition portion of the issue well 
> > enough, but by the time the iterator is created by super.compute 
> > there is no way avoid reading the values from records that do not 
> > pass my filter.
> >
> > Since I am actually using ‘SequenceFileInputFormat’ as my 
> > InputFormat I can do better, and avoid deserializing the values if I 
> > could get my hands on the reader and re-implement compute(). But 
> > this does not seem possible to do through extension because both the 
> > NewHadooprRDD.confBroadcast and NewHadoopPartition are private. 
> > There does not seem to be a choice but to copy/paste extend the 
> > NewHadoopRDD.
> >
> > The two solutions that are apparent are:
> > 1. remove those private modifiers
> > 2. factor out reader creation to a method that can be used to 
> > reimplement
> > compute() in a sub-class
> >
> > I would be curious to hear if anybody had/has similar problem and 
> > any thoughts on the issue. If you think there is PR in this I’d be 
> > happy to code it up and submit it.
> >
> >
> > Thank you
> > --
> > Eugene Cheipesh
> >
> >
> >
> > --
> > View this message in context:
> > http://apache-spark-developers-list.1001551.n3.nabble.com/pre-filter
> > ed -hadoop-RDD-use-case-tp7484.html Sent from the Apache Spark 
> > Developers List mailing list archive at Nabble.com.
> >
>
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 29 Jul 2014 16:34:54 -0700",JIRA content request,dev@spark.apache.org,"Of late, I've been coming across quite a few pull requests and associated
JIRA issues that contain nothing indicating their purpose beyond a pretty
itself, a reference to the corresponding JIRA in the title combined with a
description that gives us a sketch of what the PR does is fine, but if
there is no description in at least the JIRA of *why* you think some change
to Spark would be good, then it often makes getting started on code reviews
a little harder for those of us doing the reviews.  So, I'm requesting that
if you are submitting a JIRA or pull request for something that isn't
obviously a bug or bug fix, you please include some sort of motivation in
at least the JIRA body so that the reviewers can more easily get through
the head-scratching phase of trying to figure out why Spark might be
improved by merging a pull request.
"
Reynold Xin <rxin@databricks.com>,"Tue, 29 Jul 2014 16:36:02 -0700",Re: JIRA content request,"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 on this.



"
Sean Owen <sowen@cloudera.com>,"Wed, 30 Jul 2014 00:48:48 +0100",Re: JIRA content request,"""dev@spark.apache.org"" <dev@spark.apache.org>","How about using a JIRA status like ""Documentation Required"" to mean
""burden's on you to elaborate with a motivation and/or PR"". This could
both prompt people to do so, and also let one see when a JIRA has been
waiting on the reporter for months, rather than simply never been
looked at, and should thus time out and be closed. Both of these would
probably help the JIRA backlog.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 29 Jul 2014 20:10:54 -0400",Re: JIRA content request,dev <dev@spark.apache.org>,"+1 on using JIRA workflows to manage the backlog, and +9000 on having
decent descriptions for all JIRA issues.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 29 Jul 2014 17:41:20 -0700",Re: JIRA content request,"Nicholas Chammas <nicholas.chammas@gmail.com>, dev@spark.apache.org","I agree as well. FWIW sometimes I've seen this happen due to language barriers, i.e. contributors whose primary language is not English, but we need more motivation for each change.


+1 on using JIRA workflows to manage the backlog, and +9000 on having 
decent descriptions for all JIRA issues. 



"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 29 Jul 2014 17:49:07 -0700",Re: JIRA content request,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yup, I am seeing this in some other Apache projects as well and
usually if being asked to add more information more reporter gladly
comply as requested.

Have to diligently nudge some JIRA filers at the beginning but usually
people see that more description are better and the habit get pick up
by new contributors.

- Henry


"
Cody Koeninger <cody@koeninger.org>,"Wed, 30 Jul 2014 15:12:32 -0500",replacement for SPARK_JAVA_OPTS,dev@spark.apache.org,"We were previously using SPARK_JAVA_OPTS to set java system properties via
-D.

This was used for properties that varied on a per-deployment-environment
basis, but needed to be available in the spark shell and workers.

replaced by spark-defaults.conf and command line arguments to spark-submit
or spark-shell.

However, setting spark.driver.extraJavaOptions and
spark.executor.extraJavaOptions in spark-defaults.conf is not a replacement
for SPARK_JAVA_OPTS:


$ cat conf/spark-defaults.conf
spark.driver.extraJavaOptions=-Dfoo.bar.baz=23

$ ./bin/spark-shell

scala> System.getProperty(""foo.bar.baz"")
res0: String = null


$ ./bin/spark-shell --driver-java-options ""-Dfoo.bar.baz=23""

scala> System.getProperty(""foo.bar.baz"")
res0: String = 23


Looking through the shell scripts for spark-submit and spark-class, I can
see why this is; parsing spark-defaults.conf from bash could be brittle.

But from an ergonomic point of view, it's a step back to go from a
set-it-and-forget-it configuration in spark-env.sh, to requiring command
line arguments.

I can solve this with an ad-hoc script to wrap spark-shell with the
appropriate arguments, but I wanted to bring the issue up to see if anyone
else had run into it,
or had any direction for a general solution (beyond parsing java properties
files from bash).
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 30 Jul 2014 13:32:26 -0700",Re: replacement for SPARK_JAVA_OPTS,dev@spark.apache.org,"Hi Cody,

Could you file a bug for this if there isn't one already?

For system properties SparkSubmit should be able to read those
settings and do the right thing, but that obviously won't work for
other JVM options... the current code should work fine in cluster mode
though, since the driver is a different process. :-)





-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Jul 2014 13:43:19 -0700",Re: replacement for SPARK_JAVA_OPTS,"""dev@spark.apache.org"" <dev@spark.apache.org>","Cody - in your example you are using the '=' character, but in our
documentation and tests we use a whitespace to separate the key and
value in the defaults file.

docs: http://spark.apache.org/docs/latest/configuration.html

spark.driver.extraJavaOptions -Dfoo.bar.baz=23

I'm not sure if the java properties file parser will try to interpret
the equals sign. If so you might need to do this.

spark.driver.extraJavaOptions ""-Dfoo.bar.baz=23""

Do those work for you?


"
yao <yaoshengzhe@gmail.com>,"Wed, 30 Jul 2014 14:11:18 -0700",spark 0.9.0 with hadoop 2.4 ?,dev@spark.apache.org,"Hi Everyone,

We got some yarn related errors when running spark 0.9.0 on hadoop 2.4 (but
it was okay on hadoop 2.2). I didn't find any comments said spark 0.9.0
could support hadoop 2.4, so could I assume that we have to upgrade spark
to the latest release version at this point to solve this issue ?

Best
Shengzhe
"
Cody Koeninger <cody@koeninger.org>,"Wed, 30 Jul 2014 16:18:19 -0500",Re: replacement for SPARK_JAVA_OPTS,dev@spark.apache.org,"Either whitespace or equals sign are valid properties file formats.
Here's an example:

$ cat conf/spark-defaults.conf
spark.driver.extraJavaOptions -Dfoo.bar.baz=23

$ ./bin/spark-shell -v
Using properties file: /opt/spark/conf/spark-defaults.conf
Adding default property: spark.driver.extraJavaOptions=-Dfoo.bar.baz=23

scala>  System.getProperty(""foo.bar.baz"")
res0: String = null


If you add double quotes, the resulting string value will have double
quotes.


$ cat conf/spark-defaults.conf
spark.driver.extraJavaOptions ""-Dfoo.bar.baz=23""

$ ./bin/spark-shell -v
Using properties file: /opt/spark/conf/spark-defaults.conf
Adding default property: spark.driver.extraJavaOptions=""-Dfoo.bar.baz=23""

scala>  System.getProperty(""foo.bar.baz"")
res0: String = null


Neither one of those affects the issue; the underlying problem in my case
seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
SPARK_JAVA_OPTS environment variables, but nothing parses
spark-defaults.conf before the java process is started.

Here's an example of the process running when only spark-defaults.conf is
being used:

$ ps -ef | grep spark

514       5182  2058  0 21:05 pts/2    00:00:00 bash ./bin/spark-shell -v

514       5189  5182  4 21:05 pts/2    00:00:22 /usr/local/java/bin/java
-cp
::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
-XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
org.apache.spark.deploy.SparkSubmit spark-shell -v --class
org.apache.spark.repl.Main


Here's an example of it when the command line --driver-java-options is used
(and thus things work):


$ ps -ef | grep spark
514       5392  2058  0 21:15 pts/2    00:00:00 bash ./bin/spark-shell -v
--driver-java-options -Dfoo.bar.baz=23

514       5399  5392 80 21:15 pts/2    00:00:06 /usr/local/java/bin/java
-cp
::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
-XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
-Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
--driver-java-options -Dfoo.bar.baz=23 --class org.apache.spark.repl.Main





"
Cody Koeninger <cody@koeninger.org>,"Wed, 30 Jul 2014 17:10:08 -0500",Re: replacement for SPARK_JAVA_OPTS,dev@spark.apache.org,"In addition, spark.executor.extraJavaOptions does not seem to behave as I
would expect; java arguments don't seem to be propagated to executors.


$ cat conf/spark-defaults.conf

spark.master
mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
spark.executor.extraJavaOptions -Dfoo.bar.baz=23
spark.driver.extraJavaOptions -Dfoo.bar.baz=23


$ ./bin/spark-shell

scala> sc.getConf.get(""spark.executor.extraJavaOptions"")
res0: String = -Dfoo.bar.baz=23

scala> sc.parallelize(1 to 100).map{ i => (
     |  java.net.InetAddress.getLocalHost.getHostName,
     |  System.getProperty(""foo.bar.baz"")
     | )}.collect

res1: Array[(String, String)] = Array((dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
(dn-02.mxstg,null), ...



Note that this is a mesos deployment, although I wouldn't expect that to
affect the availability of spark.driver.extraJavaOptions in a local spark
shell.



"
yao <yaoshengzhe@gmail.com>,"Wed, 30 Jul 2014 15:17:25 -0700",Re: spark 0.9.0 with hadoop 2.4 ?,dev@spark.apache.org,"I think I might find the root cause, YARN-1931 addressed the incompatible
issue. The solution for my case might be either take related Spark patches
or do an upgrade.



"
Ted Yu <yuzhihong@gmail.com>,"Wed, 30 Jul 2014 16:40:41 -0700",Re: Working Formula for Hive 0.13?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I found SPARK-2706

Let me attach tentative patch there - I still face compilation error.

Cheers



ng
s
?
ze
n.scala
he
ntext.scala:182:
tastoreCatalog.scala:60:
tastoreCatalog.scala:267:
a
.scala
ike
h
once
be
It
so
f
o
m
t
:
d
y'
?
C
.
o
e
,
ct
e
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 30 Jul 2014 18:56:49 -0700",Re: subscribe dev list for spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","See Mailing list section of:
https://spark.apache.org/community.html



"
yao <yaoshengzhe@gmail.com>,"Wed, 30 Jul 2014 21:25:53 -0700",failed to build spark with maven for both 1.0.1 and latest master branch,dev@spark.apache.org,"Hi Folks,

Today I am trying to build spark using maven; however, the following
command failed consistently for both 1.0.1 and the latest master.  (BTW, it
seems sbt works fine: *sbt/sbt -Dhadoop.version=2.4.0 -Pyarn clean
assembly)*

Environment: Mac OS Mavericks
Maven: 3.2.2 (installed by homebrew)




*export M2_HOME=/usr/local/Cellar/maven/3.2.2/libexec/export
PATH=$M2_HOME/bin:$PATHexport MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=512M
-XX:ReservedCodeCacheSize=512m""mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=2.4.0 -DskipTests clean package*

Build outputs:

[INFO] Scanning for projects...
[INFO]
------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Project Core
[INFO] Spark Project Bagel
[INFO] Spark Project GraphX
[INFO] Spark Project ML Library
[INFO] Spark Project Streaming
[INFO] Spark Project Tools
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project Hive
[INFO] Spark Project REPL
[INFO] Spark Project YARN Parent POM
[INFO] Spark Project YARN Stable API
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Kafka
[INFO] Spark Project External Flume
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External MQTT
[INFO] Spark Project Examples
[INFO]
[INFO]
------------------------------------------------------------------------
[INFO] Building Spark Project Parent POM 1.0.1
[INFO]
------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-parent ---
[INFO]
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
spark-parent ---
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @
spark-parent ---
[INFO] Source directory:
/Users/syao/git/grid/thirdparty/spark/src/main/scala added.
[INFO]
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @
spark-parent ---
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
spark-parent ---
[INFO] Add Test Source directory:
/Users/syao/git/grid/thirdparty/spark/src/test/scala
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
spark-parent ---
[INFO] No sources to compile
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:add-test-source
(add-scala-test-sources) @ spark-parent ---
[INFO] Test Source directory:
/Users/syao/git/grid/thirdparty/spark/src/test/scala added.
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:testCompile (scala-test-compile-first)
@ spark-parent ---
[INFO] No sources to compile
[INFO]
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @
spark-parent ---
[INFO]
[INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar) @
spark-parent ---
[INFO]
[INFO]
------------------------------------------------------------------------
[INFO] Building Spark Project Core 1.0.1
[INFO]
------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-core_2.10
---
[INFO]
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
spark-core_2.10 ---
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @
spark-core_2.10 ---
[INFO] Source directory:
/Users/syao/git/grid/thirdparty/spark/core/src/main/scala added.
[INFO]
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @
spark-core_2.10 ---
[INFO]
[INFO] --- exec-maven-plugin:1.2.1:exec (default) @ spark-core_2.10 ---
Archive:  lib/py4j-0.8.1-src.zip
  inflating: build/py4j/tests/java_map_test.py
 extracting: build/py4j/tests/__init__.py
  inflating: build/py4j/tests/java_gateway_test.py
  inflating: build/py4j/tests/java_callback_test.py
  inflating: build/py4j/tests/java_list_test.py
  inflating: build/py4j/tests/byte_string_test.py
  inflating: build/py4j/tests/multithreadtest.py
  inflating: build/py4j/tests/java_array_test.py
  inflating: build/py4j/tests/py4j_callback_example2.py
  inflating: build/py4j/tests/py4j_example.py
  inflating: build/py4j/tests/py4j_callback_example.py
  inflating: build/py4j/tests/finalizer_test.py
  inflating: build/py4j/tests/java_set_test.py
  inflating: build/py4j/finalizer.py
 extracting: build/py4j/__init__.py
  inflating: build/py4j/java_gateway.py
  inflating: build/py4j/protocol.py
  inflating: build/py4j/java_collections.py
 extracting: build/py4j/version.py
  inflating: build/py4j/compat.py
[INFO]
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @
spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 6 resources
[INFO] Copying 20 resources
[INFO] Copying 7 resources
[INFO] Copying 3 resources
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
spark-core_2.10 ---
[INFO] Add Test Source directory:
/Users/syao/git/grid/thirdparty/spark/core/src/test/scala
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[info] Compiling 342 Scala sources and 34 Java sources to
/Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes...
[warn] Class javax.servlet.ServletException not found - continuing with a
stub.
[error]
[error]      while compiling:
/Users/syao/git/grid/thirdparty/spark/core/src/main/scala/org/apache/spark/HttpServer.scala
[error]         during phase: typer
[error]      library version: version 2.10.4
[error]     compiler version: version 2.10.4
[error]   reconstructed args: -classpath
/Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Users/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/repository/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v20131031/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v201105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfish/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v201105071233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/Users/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/jul-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/syao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/Users/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shaded-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-databind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/Users/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/repository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/repository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Users/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Final.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/stream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-core/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/tachyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-project/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4j/py4j/0.8.1/py4j-0.8.1.jar
-deprecation -feature -bootclasspath
/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
-unchecked -language:postfixOps
[error]
[error]   last tree to typer: Ident(Server)
[error]               symbol: <none> (flags: )
[error]    symbol definition: <none>
[error]        symbol owners:
[error]       context owners: variable server -> class HttpServer ->
package spark
[error]
[error] == Enclosing template or block ==
[error]
[error] Template( // val <local HttpServer>: <notype> in class HttpServer
[error]   ""org.apache.spark.Logging"" // parents
[error]   ValDef(
[error]     private
[error]     ""_""
[error]     <tpt>
[error]     <empty>
[error]   )
[error]   // 9 statements
[error]   ValDef( // private[this] val resourceBase: <?> in class HttpServer
[error]     private <local> <paramaccessor>
[error]     ""resourceBase""
[error]     ""File""
[error]     <empty>
[error]   )
[error]   ValDef( // private[this] val securityManager: <?> in class
HttpServer
[error]     private <local> <paramaccessor>
[error]     ""securityManager""
[error]     ""SecurityManager""
[error]     <empty>
[error]   )
[error]   DefDef( // def <init>(resourceBase: java.io.File,securityManager:
org.apache.spark.SecurityManager): org.apache.spark.HttpServer in class
HttpServer
[error]     <method> <triedcooking>
[error]     ""<init>""
[error]     []
[error]     // 1 parameter list
[error]     ValDef( // resourceBase: java.io.File
[error]       <param> <paramaccessor>
[error]       ""resourceBase""
[error]       ""File""
[error]       <empty>
[error]     )
[error]     ValDef( // securityManager: org.apache.spark.SecurityManager
[error]       <param> <paramaccessor>
[error]       ""securityManager""
[error]       ""SecurityManager"" // private[package spark] class
SecurityManager extends Logging in package spark,
tree.tpe=org.apache.spark.SecurityManager
[error]       <empty>
[error]     )
[error]     <tpt> // tree.tpe=org.apache.spark.HttpServer
[error]     Block(
[error]       Apply(
[error]         super.""<init>""
[error]         Nil
[error]       )
[error]       ()
[error]     )
[error]   )
[error]   ValDef( // private[this] var server: <?> in class HttpServer
[error]     private <mutable> <local>
[error]     ""server""
[error]     ""Server""
[error]     null
[error]   )
[error]   ValDef( // private[this] var port: <?> in class HttpServer
[error]     private <mutable> <local>
[error]     ""port""
[error]     ""Int""
[error]     -1
[error]   )
[error]   DefDef( // def start(): Unit in class HttpServer
[error]     <method> <triedcooking>
[error]     ""start""
[error]     []
[error]     List(Nil)
[error]     ""scala"".""Unit"" // final abstract class Unit extends AnyVal in
package scala, tree.tpe=Unit
[error]     If(
[error]       Apply(
[error]         ""server"".""$bang$eq""
[error]         null
[error]       )
[error]       Throw(
[error]         Apply(
[error]           new ServerStateException.""<init>""
[error]           ""Server is already started""
[error]         )
[error]       )
[error]       Block(
[error]         // 16 statements
[error]         Apply(
[error]           ""logInfo""
[error]           ""Starting HTTP Server""
[error]         )
[error]         Assign(
[error]           ""server""
[error]           Apply(
[error]             new Server.""<init>""
[error]             Nil
[error]           )
[error]         )
[error]         ValDef(
[error]           0
[error]           ""connector""
[error]           <tpt>
[error]           Apply(
[error]             new SocketConnector.""<init>""
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           ""connector"".""setMaxIdleTime""
[error]           Apply(
[error]             60.""$times""
[error]             1000
[error]           )
[error]         )
[error]         Apply(
[error]           ""connector"".""setSoLingerTime""
[error]           -1
[error]         )
[error]         Apply(
[error]           ""connector"".""setPort""
[error]           0
[error]         )
[error]         Apply(
[error]           ""server"".""addConnector""
[error]           ""connector""
[error]         )
[error]         ValDef(
[error]           0
[error]           ""threadPool""
[error]           <tpt>
[error]           Apply(
[error]             new QueuedThreadPool.""<init>""
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           ""threadPool"".""setDaemon""
[error]           true
[error]         )
[error]         Apply(
[error]           ""server"".""setThreadPool""
[error]           ""threadPool""
[error]         )
[error]         ValDef(
[error]           0
[error]           ""resHandler""
[error]           <tpt>
[error]           Apply(
[error]             new ResourceHandler.""<init>""
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           ""resHandler"".""setResourceBase""
[error]           ""resourceBase"".""getAbsolutePath""
[error]         )
[error]         ValDef(
[error]           0
[error]           ""handlerList""
[error]           <tpt>
[error]           Apply(
[error]             new HandlerList.""<init>""
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           ""handlerList"".""setHandlers""
[error]           Apply(
[error]             ""Array""
[error]             // 2 arguments
[error]             ""resHandler""
[error]             Apply(
[error]               new DefaultHandler.""<init>""
[error]               Nil
[error]             )
[error]           )
[error]         )
[error]         If(
[error]           Apply(
[error]             ""securityManager"".""isAuthenticationEnabled""
[error]             Nil
[error]           )
[error]           Block(
[error]             // 3 statements
[error]             Apply(
[error]               ""logDebug""
[error]               ""HttpServer is using security""
[error]             )
[error]             ValDef(
[error]               0
[error]               ""sh""
[error]               <tpt>
[error]               Apply(
[error]                 ""setupSecurityHandler""
[error]                 ""securityManager""
[error]               )
[error]             )
[error]             Apply(
[error]               ""sh"".""setHandler""
[error]               ""handlerList""
[error]             )
[error]             Apply(
[error]               ""server"".""setHandler""
[error]               ""sh""
[error]             )
[error]           )
[error]           Block(
[error]             Apply(
[error]               ""logDebug""
[error]               ""HttpServer is not using security""
[error]             )
[error]             Apply(
[error]               ""server"".""setHandler""
[error]               ""handlerList""
[error]             )
[error]           )
[error]         )
[error]         Apply(
[error]           ""server"".""start""
[error]           Nil
[error]         )
[error]         Assign(
[error]           ""port""
[error]           Apply(
[error]             server.getConnectors()(0).""getLocalPort""
[error]             Nil
[error]           )
[error]         )
[error]       )
[error]     )
[error]   )
[error]   DefDef( // private def setupSecurityHandler: <?> in class
HttpServer
[error]     <method> private
[error]     ""setupSecurityHandler""
[error]     []
[error]     // 1 parameter list
[error]     ValDef(
[error]       <param>
[error]       ""securityMgr""
[error]       ""SecurityManager""
[error]       <empty>
[error]     )
[error]     ""ConstraintSecurityHandler""
[error]     Block(
[error]       // 16 statements
[error]       ValDef(
[error]         0
[error]         ""constraint""
[error]         <tpt>
[error]         Apply(
[error]           new Constraint.""<init>""
[error]           Nil
[error]         )
[error]       )
[error]       Apply(
[error]         ""constraint"".""setName""
[error]         ""Constraint"".""__DIGEST_AUTH""
[error]       )
[error]       Apply(
[error]         ""constraint"".""setRoles""
[error]         Apply(
[error]           ""Array""
[error]           ""user""
[error]         )
[error]       )
[error]       Apply(
[error]         ""constraint"".""setAuthenticate""
[error]         true
[error]       )
[error]       Apply(
[error]         ""constraint"".""setDataConstraint""
[error]         ""Constraint"".""DC_NONE""
[error]       )
[error]       ValDef(
[error]         0
[error]         ""cm""
[error]         <tpt>
[error]         Apply(
[error]           new ConstraintMapping.""<init>""
[error]           Nil
[error]         )
[error]       )
[error]       Apply(
[error]         ""cm"".""setConstraint""
[error]         ""constraint""
[error]       )
[error]       Apply(
[error]         ""cm"".""setPathSpec""
[error]         ""/*""
[error]       )
[error]       ValDef(
[error]         0
[error]         ""sh""
[error]         <tpt>
[error]         Apply(
[error]           new ConstraintSecurityHandler.""<init>""
[error]           Nil
[error]         )
[error]       )
[error]       ValDef(
[error]         0
[error]         ""hashLogin""
[error]         <tpt>
[error]         Apply(
[error]           new HashLoginService.""<init>""
[error]           Nil
[error]         )
[error]       )
[error]       ValDef(
[error]         0
[error]         ""userCred""
[error]         <tpt>
[error]         Apply(
[error]           new Password.""<init>""
[error]           Apply(
[error]             ""securityMgr"".""getSecretKey""
[error]             Nil
[error]           )
[error]         )
[error]       )
[error]       If(
[error]         Apply(
[error]           ""userCred"".""$eq$eq""
[error]           null
[error]         )
[error]         Throw(
[error]           Apply(
[error]             new Exception.""<init>""
[error]             ""Error: secret key is null with authentication on""
[error]           )
[error]         )
[error]         ()
[error]       )
[error]       Apply(
[error]         ""hashLogin"".""putUser""
[error]         // 3 arguments
[error]         Apply(
[error]           ""securityMgr"".""getHttpUser""
[error]           Nil
[error]         )
[error]         ""userCred""
[error]         Apply(
[error]           ""Array""
[error]           ""user""
[error]         )
[error]       )
[error]       Apply(
[error]         ""sh"".""setLoginService""
[error]         ""hashLogin""
[error]       )
[error]       Apply(
[error]         ""sh"".""setAuthenticator""
[error]         Apply(
[error]           new DigestAuthenticator.""<init>""
[error]           Nil
[error]         )
[error]       )
[error]       Apply(
[error]         ""sh"".""setConstraintMappings""
[error]         Apply(
[error]           ""Array""
[error]           ""cm""
[error]         )
[error]       )
[error]       ""sh""
[error]     )
[error]   )
[error]   DefDef( // def stop(): Unit in class HttpServer
[error]     <method> <triedcooking>
[error]     ""stop""
[error]     []
[error]     List(Nil)
[error]     ""scala"".""Unit"" // final abstract class Unit extends AnyVal in
package scala, tree.tpe=Unit
[error]     If(
[error]       Apply(
[error]         ""server"".""$eq$eq""
[error]         null
[error]       )
[error]       Throw(
[error]         Apply(
[error]           new ServerStateException.""<init>""
[error]           ""Server is already stopped""
[error]         )
[error]       )
[error]       Block(
[error]         // 2 statements
[error]         Apply(
[error]           ""server"".""stop""
[error]           Nil
[error]         )
[error]         Assign(
[error]           ""port""
[error]           -1
[error]         )
[error]         Assign(
[error]           ""server""
[error]           null
[error]         )
[error]       )
[error]     )
[error]   )
[error]   DefDef( // def uri: String in class HttpServer
[error]     <method> <triedcooking>
[error]     ""uri""
[error]     []
[error]     Nil
[error]     ""String""
[error]     If(
[error]       Apply(
[error]         ""server"".""$eq$eq""
[error]         null
[error]       )
[error]       Throw(
[error]         Apply(
[error]           new ServerStateException.""<init>""
[error]           ""Server is not started""
[error]         )
[error]       )
[error]       Return(
[error]         Apply(
[error]           ""http://"".$plus(Utils.localIpAddress).$plus("":"").""$plus""
[error]           ""port""
[error]         )
[error]       )
[error]     )
[error]   )
[error] )
[error]
[error] uncaught exception during compilation: java.lang.AssertionError
java.lang.AssertionError: assertion failed: javax.servlet.ServletException
    at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)
    at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseExceptions$1(ClassfileParser.scala:1051)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$classfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseAttributes(ClassfileParser.scala:1080)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseMethod(ClassfileParser.scala:666)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$classfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser$$anonfun$parseClass$1.apply$mcV$sp(ClassfileParser.scala:567)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser.scala:572)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scala:88)
    at
scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoaders.scala:261)
    at
scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:194)
    at
scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:210)
    at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:893)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:5064)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.scala:5218)
    at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5561)
    at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
    at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5769)
    at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5772)
    at scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1317)
    at scala.tools.nsc.typechecker.Namers$Namer.getSig$1(Namers.scala:1457)
    at scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1466)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anonfun$apply$1.apply$mcV$sp(Namers.scala:731)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anonfun$apply$1.apply(Namers.scala:730)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anonfun$apply$1.apply(Namers.scala:730)
    at
scala.tools.nsc.typechecker.Namers$Namer.scala$tools$nsc$typechecker$Namers$Namer$$logAndValidate(Namers.scala:1499)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.apply(Namers.scala:730)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.apply(Namers.scala:729)
    at
scala.tools.nsc.typechecker.Namers$$anon$1.completeImpl(Namers.scala:1614)
    at
scala.tools.nsc.typechecker.Namers$LockingTypeCompleter$class.complete(Namers.scala:1622)
    at
scala.tools.nsc.typechecker.Namers$$anon$1.complete(Namers.scala:1612)
    at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
    at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
    at
scala.tools.nsc.typechecker.MethodSynthesis$MethodSynth$class.addDerivedTrees(MethodSynthesis.scala:225)
    at
scala.tools.nsc.typechecker.Namers$Namer.addDerivedTrees(Namers.scala:55)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1917)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1917)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$1.apply(Typers.scala:1856)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$1.apply(Typers.scala:1853)
    at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
    at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at
scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1917)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:1759)
    at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5583)
    at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
    at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedStat$1(Typers.scala:2928)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3032)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3032)
    at scala.collection.immutable.List.loop$1(List."
"
Patrick Wendell <pwendell@gmail.com>,Wed"," 30 Jul 2014 23:31:02 -0700""",Re: replacement for SPARK_JAVA_OPTS,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for digging around here. I think there are a few distinct issues.

1. Properties containing the '=' character need to be escaped.
I was able to load properties fine as long as I escape the '='
character. But maybe we should document this:

== spark-defaults.conf ==
spark.foo a\=B
== shell ==
scala> sc.getConf.get(""spark.foo"")
res2: String = a=B

2. spark.driver.extraJavaOptions, when set in the properties file,
don't affect the driver when running in client mode (always the case
for mesos). We should probably document this. In this case you need to
either use --driver-java-options or set SPARK_SUBMIT_OPTS.

3. Arguments aren't propagated on Mesos (this might be because of the
other issues, or a separate bug).

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Jul 2014 23:33:31 -0700",Re: replacement for SPARK_JAVA_OPTS,"""dev@spark.apache.org"" <dev@spark.apache.org>","The third issue may be related to this:
https://issues.apache.org/jira/browse/SPARK-2022

We can take a look at this during the bug fix period for the 1.1
release next week. If we come up with a fix we can backport it into
the 1.0 branch also.


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 31 Jul 2014 00:32:35 -0700 (PDT)",Re: Can I translate the documentations of Spark in Japanese?,dev@spark.incubator.apache.org,"Hi Kenichi Takagiwa,

Thank you for commenting.
I am going to proceed with the translation, will you please help me.
Further details will be sent later.

Best,

Yu



--

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 31 Jul 2014 00:40:02 -0700 (PDT)",Re: Can I translate the documentations of Spark in Japanese?,dev@spark.incubator.apache.org,"Hi Nick,


Thank you for your comments.
I think crowdsourced translation is fit for the translation project on
github.

Best,

Yu




--

"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 31 Jul 2014 15:56:05 +0800",Re: Can I translate the documentations of Spark in Japanese?,dev@spark.apache.org,"Transifex (https://www.transifex.com/) may be helpful if you're considering
online crowdsourcing. At least you may easily maintain a unified glossary
with it among all translators.



"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 31 Jul 2014 15:56:05 +0800",Re: Can I translate the documentations of Spark in Japanese?,dev@spark.apache.org,"Transifex (https://www.transifex.com/) may be helpful if you're considering
online crowdsourcing. At least you may easily maintain a unified glossary
with it among all translators.



"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 31 Jul 2014 01:25:48 -0700",Re: failed to build spark with maven for both 1.0.1 and latest master branch,"""dev@spark.apache.org"" <dev@spark.apache.org>","Does a ""mvn clean"" or ""sbt/sbt clean"" help?

TD

it
2M
-
)
k/HttpServer.scala
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Users/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/repository/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v20131031/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v201105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfish/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v201105071233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/Users/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/jul-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/syao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/Users/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shaded-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-databind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/Users/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/repository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/repository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Users/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Final.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/stream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-core/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/tachyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-project/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4j/py4j/0.8.1/py4j-0.8.1.jar
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
ver
r:
""
n
4)
ileParser.scala:1051)
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
eParser.scala:1080)
ser.scala:666)
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
ply$mcV$sp(ClassfileParser.scala:567)
er.scala:572)
ala:88)
ders.scala:261)
scala:194)
a:210)
ala:5218)
69)
72)
17)
7)
)
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
onfun$apply$1.apply(Namers.scala:730)
onfun$apply$1.apply(Namers.scala:730)
rs$Namer$$logAndValidate(Namers.scala:1499)
ly(Namers.scala:730)
ly(Namers.scala:729)
)
mers.scala:1622)
4)
rees(MethodSynthesis.scala:225)
917)
917)
1.apply(Typers.scala:1856)
1.apply(Typers.scala:1853)
.scala:251)
.scala:251)
)
rs$Typer$$typedStat$1(Typers.scala:2928)
032)
032)
301)
scala:99)
apply(Analyzer.scala:91)
apply(Analyzer.scala:91)
ala:91)
:57)
mpl.java:43)
$mcV$sp(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
essiveCompile.scala:166)
e.scala:98)
43)
7)
7)
)
:57)
mpl.java:43)

"
Cody Koeninger <cody@koeninger.org>,"Thu, 31 Jul 2014 08:35:51 -0500",Re: replacement for SPARK_JAVA_OPTS,dev@spark.apache.org,"1. I've tried with and without escaping equals sign, it doesn't affect the
results.

2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for getting
system properties set in the local shell (although not for executors).

3. We're using the default fine-grained mesos mode, not setting
spark.mesos.coarse, so it doesn't seem immediately related to that ticket.
Should I file a bug report?



"
Andrew Ash <andrew@andrewash.com>,"Thu, 31 Jul 2014 10:47:01 -0400","Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow",dev@spark.apache.org,"Hi everyone,

I'm seeing the below exception coming out of Spark 1.0.1 when I call it
from my application.  I can't share the source to that application, but the
quick gist is that it uses Spark's Java APIs to read from Avro files in
HDFS, do processing, and write back to Avro files.  It does this by
receiving a REST call, then spinning up a new JVM as the driver application
that connects to Spark.  I'm using CDH4.4.0 and have enabled Kryo and also
speculation.  The cluster is running in standalone mode on a 6 node cluster
in AWS (not using Spark's EC2 scripts though).

The below stacktraces are reliably reproduceable on every run of the job.
 The issue seems to be that on deserialization of a task result on the
driver, Kryo spits up while reading the ClassManifest.

I've tried swapping in Kryo 2.23.1 rather than 2.21 (2.22 had some
backcompat issues) but had the same error.

Any ideas on what can be done here?

Thanks!
Andrew



In the driver (Kryo exception while deserializing a DirectTaskResult):

INFO   | jvm 1    | 2014/07/30 20:52:52 | 20:52:52.667 [Result resolver
thread-0] ERROR o.a.spark.scheduler.TaskResultGetter - Exception while
getting task result
INFO   | jvm 1    | 2014/07/30 20:52:52 |
com.esotericsoftware.kryo.KryoException: Buffer underflow.
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.io.Input.require(Input.java:156)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:762)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:624) ~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:26)
~[chill_2.10-0.3.6.jar:0.3.6]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:19)
~[chill_2.10-0.3.6.jar:0.3.6]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:147)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[na:1.7.0_65]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[na:1.7.0_65]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]


In the DAGScheduler (job gets aborted):

org.apache.spark.SparkException: Job aborted due to stage failure:
Exception while getting task result:
com.esotericsoftware.kryo.KryoException: Buffer underflow.
    at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
    at scala.Option.foreach(Option.scala:236)
    at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
    at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
    at akka.actor.ActorCell.invoke(ActorCell.scala:456)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
    at akka.dispatch.Mailbox.run(Mailbox.scala:219)
    at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


In an Executor (running tasks get killed):

14/07/29 22:57:38 INFO broadcast.HttpBroadcast: Started reading broadcast
variable 0
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
153
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
147
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
141
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
135
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
150
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
144
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
138
14/07/29 22:57:39 INFO storage.MemoryStore: ensureFreeSpace(241733) called
with curMem=0, maxMem=30870601728
14/07/29 22:57:39 INFO storage.MemoryStore: Block broadcast_0 stored as
values to memory (estimated size 236.1 KB, free 28.8 GB)
14/07/29 22:57:39 INFO broadcast.HttpBroadcast: Reading broadcast variable
0 took 0.91790748 s
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 135
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 144
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 150
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 138
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 141
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
"
yao <yaoshengzhe@gmail.com>,"Thu, 31 Jul 2014 13:36:47 -0700",Re: failed to build spark with maven for both 1.0.1 and latest master branch,dev@spark.apache.org,"Hi TD,

I've asked my colleagues to do the same thing but compile still fails.
However, maven build succeeded once I built it on my personal macbook (with
the latest MacOS Yosemite). So I guess there might be something wrong in my
build environment. Wonder if anyone tried to compile spark using maven
under Mavericks, please let me know your result.

Thanks
Shengzhe



,
512M
-
-
-
 @
@
-
-
10
 @
 a
k/HttpServer.scala
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Users/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/repository/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v20131031/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v201105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfish/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v201105071233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/Users/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/jul-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/syao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/Users/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shaded-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-databind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/Users/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/repository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/repository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Users/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Final.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/stream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-core/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/tachyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-project/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4j/py4j/0.8.1/py4j-0.8.1.jar
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
er
r
in
in
ileParser.scala:1051)
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
eParser.scala:1080)
ser.scala:666)
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
ply$mcV$sp(ClassfileParser.scala:567)
er.scala:572)
ala:88)
ders.scala:261)
scala:194)
a:210)
)
ala:5218)
1)
)
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
onfun$apply$1.apply(Namers.scala:730)
onfun$apply$1.apply(Namers.scala:730)
rs$Namer$$logAndValidate(Namers.scala:1499)
ly(Namers.scala:730)
ly(Namers.scala:729)
)
mers.scala:1622)
rees(MethodSynthesis.scala:225)
5)
917)
917)
1.apply(Typers.scala:1856)
1.apply(Typers.scala:1853)
.scala:251)
.scala:251)
1)
7)
9)
3)
)
rs$Typer$$typedStat$1(Typers.scala:2928)
032)
032)
301)
7)
)
)
scala:99)
apply(Analyzer.scala:91)
apply(Analyzer.scala:91)
ala:91)
3)
:57)
mpl.java:43)
8)
1)
$mcV$sp(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
essiveCompile.scala:166)
e.scala:98)
43)
7)
:57)
mpl.java:43)
-
-
-
-
t)
ed
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 31 Jul 2014 13:43:12 -0700",Re: failed to build spark with maven for both 1.0.1 and latest master branch,"""dev@spark.apache.org"" <dev@spark.apache.org>","The following command succeeded (on Linux) on Spark master checked out this
morning:

mvn -Pyarn -Phive -Phadoop-2.4 -DskipTests install

FYI



th
my
=512M
t
@
)
@
--
@
..
k/HttpServer.scala
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Users/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/repository/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v20131031/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v201105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfish/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v201105071233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/Users/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/jul-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/syao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/Users/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shaded-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-databind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/Users/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/repository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/repository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Users/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Final.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/stream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-core/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/tachyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-project/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4j/py4j/0.8.1/py4j-0.8.1.jar
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
ss
r
l
""
l
or
ileParser.scala:1051)
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
eParser.scala:1080)
ser.scala:666)
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
ply$mcV$sp(ClassfileParser.scala:567)
er.scala:572)
ala:88)
ders.scala:261)
scala:194)
a:210)
)
ala:5218)
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
onfun$apply$1.apply(Namers.scala:730)
onfun$apply$1.apply(Namers.scala:730)
rs$Namer$$logAndValidate(Namers.scala:1499)
ly(Namers.scala:730)
ly(Namers.scala:729)
)
mers.scala:1622)
)
rees(MethodSynthesis.scala:225)
917)
917)
1.apply(Typers.scala:1856)
1.apply(Typers.scala:1853)
.scala:251)
.scala:251)
rs$Typer$$typedStat$1(Typers.scala:2928)
032)
032)
)
301)
scala:99)
)
apply(Analyzer.scala:91)
apply(Analyzer.scala:91)
ala:91)
:57)
mpl.java:43)
2)
$mcV$sp(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
essiveCompile.scala:166)
e.scala:98)
43)
7)
9)
7)
:57)
mpl.java:43)
"
yao <yaoshengzhe@gmail.com>,"Thu, 31 Jul 2014 13:58:22 -0700",Re: failed to build spark with maven for both 1.0.1 and latest master branch,dev@spark.apache.org,"Great, thanks Ted. I just did a maven build on CentOS 6, everything looks
good. So this is more like a Mac specific issue.



is
n
g
n
e=512M
-
-
-
)
)
-
-
 @
)
k/HttpServer.scala
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Users/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/repository/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v20131031/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v201105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfish/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v201105071233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/Users/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/jul-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/syao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/Users/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shaded-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-databind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/Users/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/repository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/repository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Users/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Final.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/stream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-core/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/tachyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-project/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4j/py4j/0.8.1/py4j-0.8.1.jar
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
s
r
2)
ileParser.scala:1051)
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
eParser.scala:1080)
ser.scala:666)
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
ply$mcV$sp(ClassfileParser.scala:567)
er.scala:572)
ala:88)
ders.scala:261)
scala:194)
a:210)
)
ala:5218)
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
onfun$apply$1.apply(Namers.scala:730)
onfun$apply$1.apply(Namers.scala:730)
rs$Namer$$logAndValidate(Namers.scala:1499)
ly(Namers.scala:730)
ly(Namers.scala:729)
)
mers.scala:1622)
1)
rees(MethodSynthesis.scala:225)
5)
917)
917)
1.apply(Typers.scala:1856)
1.apply(Typers.scala:1853)
.scala:251)
.scala:251)
1)
7)
9)
rs$Typer$$typedStat$1(Typers.scala:2928)
032)
032)
301)
scala:99)
apply(Analyzer.scala:91)
apply(Analyzer.scala:91)
7)
ala:91)
:57)
mpl.java:43)
$mcV$sp(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
(AggressiveCompile.scala:99)
essiveCompile.scala:166)
e.scala:98)
43)
7)
:57)
mpl.java:43)
)
-
 [
 [
-
-
-
"
