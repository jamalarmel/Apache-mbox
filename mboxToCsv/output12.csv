Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Apr 2014 22:36:55 -0700",Re: SparkSubmit and --driver-java-options,"""dev@spark.apache.org"" <dev@spark.apache.org>","Patch here:
https://github.com/apache/spark/pull/609


"
=?iso-8859-1?Q?Nicolas_Lalev=E9e?= <nicolas.lalevee@hibnet.org>,"Thu, 1 May 2014 12:06:53 +0200",Mailing list,spark-dev@apache.org,"Hi,

Your website seems a little bit incomplete. I have found this page [1] with list the two main mailing lists, users and dev. But I see a reference to a mailing list about ""issues"" which tracks the sparks issues when it was hosted at Atlassian. I guess it has moved ? where ?
And is there any mailing about the commits ?

Also, I found it weird that there is no page that is referencing the true code source, the git at the ASF, I only found references to the git at github.

This kind of stuff is important for me when I look at an opensource project. Looking at jira workflows, dev discussion, frequency of commits, often tells me about the quality of the community hence the software. And what is great at the ASF, is that almost all of that information is open.

I am also interested in your workflow, because Ant is moving from svn to git and we're still a little bit in the grey about the workflow. I am thus intrigued how do you work with github pull requests.

Nicolas

[1] https://spark.apache.org/community.html


"
"""Muttineni, Vinay"" <vmuttineni@ebay.com>","Thu, 1 May 2014 15:52:51 +0000",Spark R Interface,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,
Any news/updates on the Spark R interface?
I see from here (http://mail-archives.apache.org/mod_mbox/spark-user/201401.mbox/%3CCAGiTW48+otjXNgTJ=tWG6faNgkG-NU4Bg31pxNmf7HLzvezvWA@mail.gmail.com%3E ) that an alpha version was to be released in January?
Thanks
Vinay
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 1 May 2014 09:21:57 -0700",Re: Spark R Interface,dev@spark.apache.org,"There is a alpha version of SparkR that you can use from
https://github.com/amplab-extras/SparkR-pkg -- It works with Spark 0.9.0
and above and has most of the basic RDD functions implemented.

Thanks
Shivaram



"
"""Muttineni, Vinay"" <vmuttineni@ebay.com>","Thu, 1 May 2014 17:40:52 +0000",RE: Spark R Interface,"""dev@spark.apache.org"" <dev@spark.apache.org>,
	""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Thank you Shivaram

-----Original Message-----
From: Shivaram Venkataraman [mailto:shivaram@eecs.berkeley.edu] 
Sent: Thursday, May 01, 2014 9:22 AM
To: dev@spark.apache.org
Subject: Re: Spark R Interface

There is a alpha version of SparkR that you can use from https://github.com/amplab-extras/SparkR-pkg -- It works with Spark 0.9.0 and above and has most of the basic RDD functions implemented.

Thanks
Shivaram


On Thu, May 1, 2014 at 8:52 AM, Muttineni, Vinay <vmuttineni@ebay.com>wrote:

> Hi All,
> Any news/updates on the Spark R interface?
> I see from here (
> http://mail-archives.apache.org/mod_mbox/spark-user/201401.mbox/%3CCAGiTW48+otjXNgTJ=tWG6faNgkG-NU4Bg31pxNmf7HLzvezvWA@mail.gmail.com%3E) that an alpha version was to be released in January?
> Thanks
> Vinay
>
"
Manu Suryavansh <suryavanshi.manu@gmail.com>,"Thu, 1 May 2014 18:45:47 -0700",Re: Spark 1.0.0 rc3,dev@spark.apache.org,"Hi,

I tried to build the 1.0.0 rc3 version with Java 8 and I got the error
- java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC
overhead limit exceeded
I am building on a Core-i7(Quad core) windows laptop with 8 GB RAM.

Earlier I had tried to build Spark 0.9.1 with Java 8 and I had gotten an
error about comparator.class not found - which was mentioned today on
another thread, so I am not getting that error now. I have successfully
build Spark 0.9.0 with Java 1.7.

[image: Inline image 1]

Thanks,
Manu






-- 
Manu Suryavansh
"
Madhu <madhu@madhu.com>,"Thu, 1 May 2014 20:16:35 -0700 (PDT)",Re: Spark 1.0.0 rc3,dev@spark.incubator.apache.org,"I'm guessing EC2 support is not there yet?

I was able to build using the binary download on both Windows 7 and RHEL 6
without issues.
I tried to create an EC2 cluster, but saw this:

~/spark-ec2
Initializing spark
~ ~/spark-ec2
ERROR: Unknown Spark version
Initializing shark
~ ~/spark-ec2 ~/spark-ec2
ERROR: Unknown Shark version

The spark dir on the EC2 master has only a conf dir, so it didn't deploy
properly.



--

"
Sean Owen <sowen@cloudera.com>,"Fri, 2 May 2014 10:24:29 +0100",Re: [jira] [Created] (SPARK-1698) Improve spark integration,dev@spark.apache.org,"#1 and #2 are not relevant the issue of jar size. These can be problems in
general, but don't think there have been issues attributable to file
clashes. Shading has mechanisms to deal with this anyway.

#3 is a problem in general too, but is not specific to shading. Where
versions collide, build processes like Maven and shading must be used to
resolve them. But this happens regardless of whether you shade a fat jar.

#4 is a real problem specific to Java 6. It does seem like it will be
important to identify and remove more unnecessary dependencies to work
around it.

But shading per se is not the problem, and it is important to make a
packaged jar for the app. What are you proposing? Dependencies to be
removed?

"
Nan Zhu <zhunanmcgill@gmail.com>,"Sat, 3 May 2014 07:14:41 -0400",Re: Spark 1.0.0 rc3,dev@spark.apache.org,"SPARK_HADOOP_VERSION=2.3.0 sbt/sbt assembly 

and copy the generated jar to lib/ directory of my application, 

it seems that sbt cannot find the dependencies in the jar?

but everything works with the pre-built jar files downloaded from the link provided by Patrick

Best, 

-- 
Nan Zhu





"
Nan Zhu <zhunanmcgill@gmail.com>,"Sat, 3 May 2014 07:14:41 -0400",Re: Spark 1.0.0 rc3,dev@spark.apache.org,"SPARK_HADOOP_VERSION=2.3.0 sbt/sbt assembly 

and copy the generated jar to lib/ directory of my application, 

it seems that sbt cannot find the dependencies in the jar?

but everything works with the pre-built jar files downloaded from the link provided by Patrick

Best, 

-- 
Nan Zhu





"
Ajay Nair <prodigyaj@gmail.com>,"Sat, 3 May 2014 08:42:53 -0700 (PDT)",Apache Spark running out of the spark shell,dev@spark.incubator.apache.org,"Hi,

I have written a code that works just about fine in the spark shell on EC2.
The ec2 script helped me configure my master and worker nodes. Now I want to
run the scala-spark code out side the interactive shell. How do I go about
doing it.

I was referring to the instructions mentioned here:
https://spark.apache.org/docs/0.9.1/quick-start.html

But this is confusing because it mentions about a simple project jar file
which I am not sure how to generate. I only have the file that runs directly
on my spark shell. Any easy intruction to get this quickly running as a job?

Thanks
AJ



--

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 3 May 2014 10:06:31 -0700",Re: Apache Spark running out of the spark shell,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi AJ,

You might find this helpful -
http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/

-Sandy



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 3 May 2014 10:06:31 -0700",Re: Apache Spark running out of the spark shell,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi AJ,

You might find this helpful -
http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/

-Sandy



"
Nicolas Garneau <ngarneau@ngarneau.com>,"Sat, 3 May 2014 13:23:59 -0400",Re: Apache Spark running out of the spark shell,dev@spark.apache.org,"Hey AJ,

I created a little sample app using the spark's quick start.
Have a look here.
Assuming you used scala, using sbt is good for running your application in standalone mode.
The configuration file which is ""simple.sbt"" in my repo, holds all the dependencies needed to build your app.

Hope this helps!

Le 2014-05-03 à 11:42, Ajay Nair <prodigyaj@gmail.com> a écrit :

EC2.
want to
about
file
directly
a job?
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459.html
Nabble.com.

Nicolas Garneau
ngarneau@ngarneau.com

"
Nicolas Garneau <ngarneau@ngarneau.com>,"Sat, 3 May 2014 13:23:59 -0400",Re: Apache Spark running out of the spark shell,dev@spark.apache.org,"Hey AJ,

I created a little sample app using the spark's quick start.
Have a look here.
Assuming you used scala, using sbt is good for running your application in standalone mode.
The configuration file which is ""simple.sbt"" in my repo, holds all the dependencies needed to build your app.

Hope this helps!

Le 2014-05-03 à 11:42, Ajay Nair <prodigyaj@gmail.com> a écrit :

EC2.
want to
about
file
directly
a job?
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459.html
Nabble.com.

Nicolas Garneau
ngarneau@ngarneau.com

"
Ajay Nair <prodigyaj@gmail.com>,"Sat, 3 May 2014 10:27:55 -0700 (PDT)",Re: Apache Spark running out of the spark shell,dev@spark.incubator.apache.org,"Thank you for the reply. Have you posted a link from where I follow the steps
?



--

"
Nicolas Garneau <ngarneau@ngarneau.com>,"Sat, 3 May 2014 13:28:48 -0400",Re: Apache Spark running out of the spark shell,dev@spark.apache.org,"Sorry, the link went wrong. I meant here:
https://github.com/ngarneau/spark-standalone

Le 2014-05-03 à 13:23, Nicolas Garneau <ngarneau@ngarneau.com> a écrit :

application in standalone mode.
dependencies needed to build your app.
on EC2.
want to
about
file
directly
a job?
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459.html
Nabble.com.

Nicolas Garneau
418.569.3097
ngarneau@ngarneau.com

"
Nicolas Garneau <ngarneau@ngarneau.com>,"Sat, 3 May 2014 13:28:48 -0400",Re: Apache Spark running out of the spark shell,dev@spark.apache.org,"Sorry, the link went wrong. I meant here:
https://github.com/ngarneau/spark-standalone

Le 2014-05-03 à 13:23, Nicolas Garneau <ngarneau@ngarneau.com> a écrit :

application in standalone mode.
dependencies needed to build your app.
on EC2.
want to
about
file
directly
a job?
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459.html
Nabble.com.

Nicolas Garneau
418.569.3097
ngarneau@ngarneau.com

"
Ajay Nair <prodigyaj@gmail.com>,"Sat, 3 May 2014 10:28:32 -0700 (PDT)",Re: Apache Spark running out of the spark shell,dev@spark.incubator.apache.org,"Thank you. Let me try this quickly !



--

"
Ajay Nair <prodigyaj@gmail.com>,"Sat, 3 May 2014 10:38:51 -0700 (PDT)",Re: Apache Spark running out of the spark shell,dev@spark.incubator.apache.org,"Quick question, where should I place your folder. Inside the spark directory.
My Spark directory is in /root/spark
So currently I tried pulling your github code in /root/spark/spark-examples
and modified my home spark directory in the scala code.
I copied the sbt folder within the spark-examples folder. But when I try
running this command

$root/spark/spark-examples: sbt/sbt package

awk: cmd. line:1: fatal: cannot open file `./project/build.properties' for
reading (No such file or directory)
Launching sbt from sbt/sbt-launch-.jar
Error: Invalid or corrupt jarfile sbt/sbt-launch-.jar


However the sbt package runs fines (Expectedly) when i run it from
/root/spark folder.

Anything I am doing wrong here?





--

"
Nicolas Garneau <ngarneau@ngarneau.com>,"Sat, 3 May 2014 14:22:58 -0400",Re: Apache Spark running out of the spark shell,dev@spark.apache.org,"Hey AJ,

As I can see your path when running sbt is:


You should be within the app's folder that contains the simple.sbt, which is spark-standalone/;



Don't forget to move the sbt folder within your app's directory.

That being said, I think you can install sbt globally on your system so you'll be able to run the sbt command everywhere on your PC.
It'll be useful when creating multiple apps.

For example, the way I'm building it from A to Z:
$ git clone https://github.com/ngarneau/spark-standalone.git
$ cd spark-standalone
-- change the path of spark's home dir
$ sbt package (assuming sbt is installed globally)
$ sbt run (assuming sbt is installed globally)

Hope this helps!

Le 2014-05-03 à 13:38, Ajay Nair <prodigyaj@gmail.com> a écrit :

directory.
/root/spark/spark-examples
try
for
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6465.html
Nabble.com.

Nicolas Garneau
ngarneau@ngarneau.com


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 3 May 2014 21:30:31 -0700",Re: Mailing list,dev@spark.apache.org,"Hi Nicolas,

Good catches on these things.

with list the two main mailing lists, users and dev. But I see a reference to a mailing list about ""issues"" which tracks the sparks issues when it was hosted at Atlassian. I guess it has moved ? where ?

Good catch, this was an old link and I’ve fixed it now. I also added the one for commits.

true code source, the git at the ASF, I only found references to the git at github.

The GitHub repo is actually a mirror managed by the ASF, but the “git tag” link at http://spark.apache.org/downloads.html also points to the source repo. The problem is that our contribution process is through GitHub so it’s easier to point people to something that they can use to contribute.

to git and we're still a little bit in the grey about the workflow. I am thus intrigued how do you work with github pull requests.

Take a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark and https://cwiki.apache.org/confluence/display/SPARK/Reviewing+and+Merging+Patches to see our contribution process. In a nutshell, it works as follows:

- Anyone can make a patch by forking the GitHub repo and sending a pull request (GitHub’s internal patch mechanism)
- Committers review the patch and ask for changes; contributors can push additional changes into their pull request to respond
- When the patch looks good, we use a script to merge it into the source Apache repo; this also squashes the changes into one commit, making the Git history sane and facilitating reverts, cherry-picks into other branches, etc.

Note by the way that using GitHub is not at all necessary for using Git. We happened to do our development on GitHub before moving to the ASF, and all our developers were used to its interface, so we stuck with it. It definitely beats attaching patches on JIRA but it may not be the first step you want to take in moving to Git.

Matei



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 3 May 2014 21:30:31 -0700",Re: Mailing list,dev@spark.apache.org,"Hi Nicolas,

Good catches on these things.

with list the two main mailing lists, users and dev. But I see a reference to a mailing list about ""issues"" which tracks the sparks issues when it was hosted at Atlassian. I guess it has moved ? where ?

Good catch, this was an old link and I’ve fixed it now. I also added the one for commits.

true code source, the git at the ASF, I only found references to the git at github.

The GitHub repo is actually a mirror managed by the ASF, but the “git tag” link at http://spark.apache.org/downloads.html also points to the source repo. The problem is that our contribution process is through GitHub so it’s easier to point people to something that they can use to contribute.

to git and we're still a little bit in the grey about the workflow. I am thus intrigued how do you work with github pull requests.

Take a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark and https://cwiki.apache.org/confluence/display/SPARK/Reviewing+and+Merging+Patches to see our contribution process. In a nutshell, it works as follows:

- Anyone can make a patch by forking the GitHub repo and sending a pull request (GitHub’s internal patch mechanism)
- Committers review the patch and ask for changes; contributors can push additional changes into their pull request to respond
- When the patch looks good, we use a script to merge it into the source Apache repo; this also squashes the changes into one commit, making the Git history sane and facilitating reverts, cherry-picks into other branches, etc.

Note by the way that using GitHub is not at all necessary for using Git. We happened to do our development on GitHub before moving to the ASF, and all our developers were used to its interface, so we stuck with it. It definitely beats attaching patches on JIRA but it may not be the first step you want to take in moving to Git.

Matei



"
Manish Amde <manish9ue@gmail.com>,"Sun, 4 May 2014 01:12:34 -0700","reduce, transform, combine",dev@spark.apache.org,"I am currently using the RDD aggregate operation to reduce (fold) per
partition and then combine using the RDD aggregate operation.
def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U)
=> U): U

I need to perform a transform operation after the seqOp and before the
combOp. The signature would look like
def foldTransformCombine[U: ClassTag](zeroReduceValue: V, zeroCombineValue:
U)(seqOp: (V, T) => V, transformOp: (V) => U, combOp: (U, U) => U): U

This is especially useful in the scenario where the transformOp is
expensive and should be performed once per partition before combining. Is
there a way to accomplish this with existing RDD operations? If yes, great
but if not, should we consider adding such a general transformation to the
list of RDD operations?

-Manish
"
DB Tsai <dbtsai@stanford.edu>,"Sun, 4 May 2014 01:40:32 -0700","Re: reduce, transform, combine",dev@spark.apache.org,"You could easily achieve this by mapPartition. However, it seems that it
can not be done by using aggregate type of operation. I can see that it's a
general useful operation. For now, you could use mapPartition.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
=?windows-1252?Q?Nicolas_Lalev=E9e?= <nicolas.lalevee@hibnet.org>,"Sun, 4 May 2014 17:12:29 +0200",Re: Mailing list,dev@spark.apache.org,"
Le 4 mai 2014 à 06:30, Matei Zaharia <matei.zaharia@gmail.com> a écrit :

[1] with list the two main mailing lists, users and dev. But I see a reference to a mailing list about ""issues"" which tracks the sparks issues when it was hosted at Atlassian. I guess it has moved ? where ?
the one for commits.
true code source, the git at the ASF, I only found references to the git at github.
tag” link at http://spark.apache.org/downloads.html also points to the source repo. The problem is that our contribution process is through GitHub so it’s easier to point people to something that they can use to contribute.
to git and we're still a little bit in the grey about the workflow. I am thus intrigued how do you work with github pull requests.
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark and https://cwiki.apache.org/confluence/display/SPARK/Reviewing+and+Merging+Patches to see our contribution process. In a nutshell, it works as follows:
pull request (GitHub’s internal patch mechanism)
push additional changes into their pull request to respond
source Apache repo; this also squashes the changes into one commit, making the Git history sane and facilitating reverts, cherry-picks into other branches, etc.

The script you're talking about, is it merge_spark_pr.py [1] ?

Git. We happened to do our development on GitHub before moving to the ASF, and all our developers were used to its interface, so we stuck with it. It definitely beats attaching patches on JIRA but it may not be the first step you want to take in moving to Git.

Your workflow is indeed interesting. I guess most of Ant committers and potential contributors have experience with github too, so at some point we'll have to handle it. I'll discuss with the Ant dev community.

Thank you Matei for the fix on the site and for the clear response.

Nicolas

[1] https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py


"
"""Manish Amde"" <manish9ue@gmail.com>","Sun, 04 May 2014 10:06:37 -0700 (PDT)","Re: reduce, transform, combine",dev@spark.apache.org,"Thanks DB. I will work with mapPartition for now.Â 


Question to the community in general: should we consider adding such an operation to RDDs especially as a developer API?


 a
(U, U)
zeroCombineValue:
U): U
Is
great
the"
Soren Macbeth <soren@yieldbot.com>,"Sun, 4 May 2014 10:35:42 -0700",bug using kryo as closure serializer,dev@spark.apache.org,"apologies for the cross-list posts, but I've gotten zero response in the
user list and I guess this list is probably more appropriate.

According to the documentation, using the KryoSerializer for closures is
supported. However, when I try to set `spark.closure.serializer` to
`org.apache.spark.serializer.KryoSerializer` thing fail pretty miserably.

The first thing that happens it that is throws exceptions over and over
that it cannot locate my registrator class, which is located in my assembly
jar like so:

14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
spark.kryo.registrator
java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:270)
at
org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
at
org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
at
org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
at
org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:724)

Now, I would expect it not to be able to find this class since it hasn't
those expections stop. Next, all the executor task die with the following
exception:

at java.nio.ByteBuffer.array(ByteBuffer.java:961)
at
org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:724)

AFAIK, I'm not doing anything out of the ordinary, just turning on kryo and
using the registrator mechanism to register a couple custom serializers.

The reason I tried turning on kryo for closure in the first place is
because of a different bug that I was hitting during fetching and
deserializing of tasks from my executors, which I detailed here:

http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html

Here's hoping some on this list can help me track down what's happening as
I didn't get a single reply on the user list.
"
Ajay Nair <prodigyaj@gmail.com>,"Sun, 4 May 2014 10:35:45 -0700 (PDT)",Re: Apache Spark running out of the spark shell,dev@spark.incubator.apache.org,"Thank you. I am trying this now



--

"
Nicolas Garneau <ngarneau@ngarneau.com>,"Sun, 4 May 2014 13:47:59 -0400",Re: Apache Spark running out of the spark shell,dev@spark.apache.org,"Hey AJ,

If you plan to launch your job on a cluster, consider using the spark-submit command.
Running this in the spark's home directory gives you a help on how to use this:

$ ./bin/spark-submit

I haven't tried it yet but considering this post, it will be the preferred way to launch jobs:
http://apache-spark-user-list.1001560.n3.nabble.com/Running-a-spark-submit-compatible-app-in-spark-shell-td4905.html

Cheers

Le 2014-05-04 à 13:35, Ajay Nair <prodigyaj@gmail.com> a écrit :

http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6472.html
Nabble.com.

Nicolas Garneau
ngarneau@ngarneau.com

"
Reynold Xin <rxin@databricks.com>,"Sun, 4 May 2014 12:50:59 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"I added the config option to use the non-default serializer. However, at
the time, Kryo fails serializing pretty much any closures so that option
was never really used / recommended.

Since then the Scala ecosystem has developed, and some other projects are
starting to use Kryo to serialize more Scala data structures, so I wouldn't
be surprised if there is a way to work around this now. However, I don't
have enough time to look into it at this point. If you do, please do post
your findings. Thanks.




"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 5 May 2014 02:09:26 +0530",Re: bug using kryo as closure serializer,dev@spark.apache.org,"Reynold how much better is kryo from spark's usage point of view
compared to the default java serialization (in general, not for
closures) ?
The numbers on kyro site are interesting, but since you have played
the most with kryo in context of spark (i think) - how do you rate it
along lines of :

1) computational overhead compared to java serialization.
2) memory overhead.
3) generated byte[] size.


Particularly given the bugs Patrick and I had looked into in past
along flush, etc I was always skeptical about using kyro.
But given the pretty nasty issues with OOM's via java serialization we
are seeing, wanted to know your thoughts on use of kyro with spark.
(Will be slightly involved to ensure everything gets registered, but I
want to go down the path assuming I hear good things in context of
spark)

Thanks,
Mridul



"
Ajay Nair <prodigyaj@gmail.com>,"Sun, 4 May 2014 15:23:39 -0700 (PDT)",Re: Apache Spark running out of the spark shell,dev@spark.incubator.apache.org,"Now I got it to work .. well almost. However I needed to copy the project/
folder to the spark-standalone folder as the package build was failing
because it could not find buil properties. After the copy the build was
successful. However when I run it I get errors but it still gives me the
output.

[error] 14/05/04 21:58:19 INFO spark.SparkContext: Job finished: count at
SimpleApp.scala:11, took 0.040651597 s
[error] 14/05/04 21:58:19 INFO scheduler.TaskSetManager: Finished TID 3 in
17 ms on localhost (progress: 2/2)
[info] Lines with a: 3, Lines with b: 2
[error] 14/05/04 21:58:19 INFO scheduler.TaskSchedulerImpl: Removed TaskSet
1.0, whose tasks have all completed, from pool 
[success] Total time: 5 s, completed May 4, 2014 9:58:20 PM


You can see the [info] that contains the output. All the lines i get mention
[errors], any reason why ?

I have configured my ec2 machines master and slave nodes and this code I
think tries to run in the local mode.



--

"
Soren Macbeth <soren@yieldbot.com>,"Sun, 4 May 2014 15:54:16 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"Thanks for the reply!

Ok, if that's the case, I'd recommend a note to that affect in the docs at
least.

Just to give some more context here, I'm working on a Clojure DSL for Spark
called Flambo, which I plan to open source shortly. If I could I'd like to
focus on the initial bug that I hit.

Exception in thread ""main"" org.apache.spark.SparkException: Job aborted:
Exception while deserializing and fetching task:
com.esotericsoftware.kryo.KryoException:
java.lang.IllegalArgumentException: Can not set final
scala.collection.convert.Wrappers field
scala.collection.convert.Wrappers$SeqWrapper.$outer to
clojure.lang.PersistentVector
Serialization trace:
$outer (scala.collection.convert.Wrappers$SeqWrapper)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
        at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
        at scala.Option.foreach(Option.scala:236)
        at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

This happens immediately after all the tasks of a reduce stage complete
successfully. Here is the function throwing the exception:

https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L43

This is where I get lost. From googling around, it seems that scala is
trying to wrap the result of my task, which contain
clojure.lang.PersistentVector objects in a scala collection, but I don't
know why it's doing that. I have a registered kryo serializer for
clojure.lang.PersistentVector.

based on this line is looks like it's trying to use the closure serializer,
yet the expection thrown is from com.esotericsoftware.kryo.KryoException:

https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L39

Would storing my RDD as MEMORY_ONLY_SER prevent the closure serializer from
trying to deal with my clojure.lang.PeristentVector class?

Where do I go from here?



"
Nicolas Garneau <ngarneau@ngarneau.com>,"Sun, 4 May 2014 19:24:38 -0400",Re: Apache Spark running out of the spark shell,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey AJ,

I have tried to run on a cluster yet, only on local mode.
I'll try to get something running on a cluster soon and keep you posted.

Nicolas Garneau



t
on
n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6478.html
com.

"
Reynold Xin <rxin@databricks.com>,"Sun, 4 May 2014 20:44:26 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"Kryo does generate code for serialization, so the CPU overhead is quite
lower than Java (which I think just uses reflection). As I understand, they
also have a new implementation that uses unsafe intrinsics, which should
lead to even higher performance.

The generated byte[] size was a lot smaller in Kryo, especially for arrays
of objects of the same type. It doesn't need to write the class name for
every object, and it also reduces the size of ints and such using zig zag
encoding.

I don't have numbers around anymore, but when I was benchmarking them, kryo
was substantially better than java. The only reason it is not on by default
is because it doesn't always work.



"
Reynold Xin <rxin@databricks.com>,"Sun, 4 May 2014 20:48:37 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"Good idea. I submitted a pull request for the doc update here:
https://github.com/apache/spark/pull/642



"
Soren Macbeth <soren@yieldbot.com>,"Sun, 4 May 2014 21:08:19 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"fwiw, it seems like it wouldn't be very difficult to integrate chill-scala,
since you're already chill-java and probably get kryo serialization of
closures and all sorts of other scala stuff for free. All that would be
needed would be to include the dependency and then update KryoSerializer to
register the stuff in chill-scala.

In that case, you could probably safely make kryo the default serializer,
which I think would be desirable in general.



"
Reynold Xin <rxin@databricks.com>,"Sun, 4 May 2014 22:04:44 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"Thanks. Do you mind playing with chill-scala a little bit and see if it
serializer to use Kryo with chill-scala, and then run through all the unit
tests.

If it works well, we can incorporate that in the next release (probably not
1.0, but after that).



"
Soren Macbeth <soren@yieldbot.com>,"Sun, 4 May 2014 22:22:02 -0700",Re: bug using kryo as closure serializer,"""dev@spark.apache.org"" <dev@spark.apache.org>","that would violate my personal oath of never writing a single line of
scala, but I might be able to do that if I can get past the issue this
issue I'm struggling with in this thread.


"
Reynold Xin <rxin@databricks.com>,"Sun, 4 May 2014 22:24:22 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"Technically you only need to change the build file, and change part of a
line in SparkEnv so you don't have to break your oath :)




"
Ajay Nair <prodigyaj@gmail.com>,"Mon, 5 May 2014 01:40:48 -0700 (PDT)",Apache spark on 27gb wikipedia data,dev@spark.incubator.apache.org,"Hi,

I am using 1 master and 3 slave workers for processing 27gb of Wikipedia
data that is tab separated and every line contains wikipedia page
information. The tab separated data has title of the page and the page
contents. I am using the regular expression to extract links as mentioned in
the site below:
http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html#running-pagerank-on-wikipedia

Although it runs fne for around 300Mb data set, it runs in to issues when I
try to execute the same code using the 27gb data from hdfs.
The error thrown is given below:
14/05/05 07:15:22 WARN scheduler.TaskSetManager: Loss was due to
java.lang.OutOfMemoryError
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.regex.Matcher.<init>(Matcher.java:224)

Is there any way to over come this issue?

My cluster is a ec2 m3.large machine.

Thanks
Ajay



--

"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 5 May 2014 14:21:44 +0530",Re: Apache spark on 27gb wikipedia data,dev@spark.apache.org,"I just thought may be we could put a warning whenever that error comes user
can tune either memoryFraction or executor memory options. And this warning
get's displayed when TaskSetManager receives task failures due to  OOM.

Prashant Sharma



"
Soren Macbeth <soren@yieldbot.com>,"Mon, 5 May 2014 08:56:42 -0700",Re: bug using kryo as closure serializer,dev@spark.apache.org,"I just took a peek at KryoSerializer and it looks like you're already using
all the scala stuff from chill in there, so I would imagine that scala
things should serialize pretty well.

Seems like the readonly bytebuffer thing might be some other sort of
downstream bug.



"
Ajay Nair <prodigyaj@gmail.com>,"Mon, 5 May 2014 09:04:46 -0700 (PDT)",Re: Apache spark on 27gb wikipedia data,dev@spark.incubator.apache.org,"Hi,

Is there any way to overcome this error? I am running this from the
spark-shell, is that the cause of concern ?



--

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 5 May 2014 13:58:25 -0700",Re: Mailing list,dev@spark.apache.org,"

Yup, that’s it.

Git. We happened to do our development on GitHub before moving to the ASF, and all our developers were used to its interface, so we stuck with it. It definitely beats attaching patches on JIRA but it may not be the first step you want to take in moving to Git.
and potential contributors have experience with github too, so at some point we'll have to handle it. I'll discuss with the Ant dev community.

Cool, glad to hear this is useful!



"
Debasish Das <debasish.das83@gmail.com>,"Mon, 5 May 2014 14:41:15 -0700",mllib vector templates,dev@spark.apache.org,"Hi,

Why mllib vector is using double as default ?

/**

 * Represents a numeric vector, whose index type is Int and value type is
Double.

 */

trait Vector extends Serializable {


  /**

   * Size of the vector.

   */

  def size: Int


  /**

   * Converts the instance to a double array.

   */

  def toArray: Array[Double]

Don't we need a template on float/double ? This will give us memory
savings...

Thanks.

Deb
"
DB Tsai <dbtsai@stanford.edu>,"Mon, 5 May 2014 14:45:02 -0700",Re: mllib vector templates,dev@spark.apache.org,"+1  Would be nice that we can use different type in Vector.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Debasish Das <debasish.das83@gmail.com>,"Mon, 5 May 2014 14:56:21 -0700",Re: mllib vector templates,dev@spark.apache.org,"Is this a breeze issue or breeze can take templates on float / double ?

If breeze can take templates then it is a minor fix for Vectors.scala right
?

Thanks.
Deb



"
DB Tsai <dbtsai@stanford.edu>,"Mon, 5 May 2014 15:02:48 -0700",Re: mllib vector templates,dev@spark.apache.org,"Breeze could take any type (Int, Long, Double, and Float) in the matrix
template.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
David Hall <dlwh@cs.berkeley.edu>,"Mon, 5 May 2014 15:06:05 -0700",Re: mllib vector templates,dev@spark.apache.org,"Lbfgs and other optimizers would not work immediately, as they require
vector spaces over double. Otherwise it should work.

"
Debasish Das <debasish.das83@gmail.com>,"Mon, 5 May 2014 15:12:15 -0700",Re: mllib vector templates,dev@spark.apache.org,"Is any one facing issues due to this ? If not then I guess doubles are
fine...

For me it's not a big deal as there is enough memory available...



"
David Hall <dlwh@cs.berkeley.edu>,"Mon, 5 May 2014 15:14:10 -0700",Re: mllib vector templates,dev@spark.apache.org,"I should mention it shouldn't be too hard to change, but it is a current
limitation.

"
DB Tsai <dbtsai@stanford.edu>,"Mon, 5 May 2014 15:40:30 -0700",Re: mllib vector templates,dev@spark.apache.org,"David,

Could we use Int, Long, Float as the data feature spaces, and Double for
optimizer?


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 5 May 2014 15:52:50 -0700",Re: mllib vector templates,dev@spark.apache.org,"I fixed index type and value type to make things simple, especially
when we need to provide Java and Python APIs. For raw features and
feature transmations, we should allow generic types. -Xiangrui


"
David Hall <dlwh@cs.berkeley.edu>,"Mon, 5 May 2014 16:05:49 -0700",Re: mllib vector templates,dev@spark.apache.org,"

Yes. Breeze doesn't allow operations on mixed types, so you'd need to
convert the double vectors to Floats if you wanted, e.g. dot product with
the weights vector.

You might also be interested in FeatureVector, which is just a wrapper
around Array[Int] that emulates an indicator vector. It supports dot
products, axpy, etc.

-- David


"
Prashant Sharma <scrapcodes@gmail.com>,"Tue, 6 May 2014 10:46:49 +0530",Re: Apache spark on 27gb wikipedia data,dev@spark.apache.org,"Try tuning the options like memoryFraction and executorMemory found here :
http://spark.apache.org/docs/latest/configuration.html.

Thanks

Prashant Sharma



"
prabeesh k <prabsmails@gmail.com>,"Tue, 6 May 2014 11:22:50 +0530",Better option to use Querying in Spark,"dev <dev@spark.apache.org>, user@spark.apache.org","Hi,

I have seen three different ways to query data from Spark

   1. Default SQL support(
   https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/sql/examples/HiveFromSpark.scala
   )
   2. Shark
   3. Blink DB

I would like know which one is more efficient

Regards.
prabeesh
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Tue, 6 May 2014 11:44:42 +0530",Re: Better option to use Querying in Spark,dev <dev@spark.apache.org>,"All three have different usecases. If you are looking for more of a
warehouse you are better off with Shark.
SparkSQL is a way to query regular data in sql like syntax leveraging
columnar store.

BlinkDB is a experiment, meant to integrate with Shark in the long term.
Not meant for production usecase directly.


Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




"
Usman Ghani <usman@platfora.com>,"Sat, 10 May 2014 01:15:23 -0700",Using spark 1.0.0 assembly in IntelliJ,dev@spark.apache.org,"*(Includes solution)*

I was having this weird issue where when I use a Spark 0.9.1 or earlier Jar
file in IntelliJ I can see its contents and IntelliJ can work with it. But
when I compile version 1.0.0 from source my jar is not being recognized by
IntelliJ even though I can build my project using ant from the command
line.

*Snapshot attached.*

[image: Inline image 1]

Basically IntelliJ on Mac always uses Apple JDK 1.6 to run. You have to
manually change it to run under 1.7 so it can look inside a 1.7 jar
compiled to 1.7 byte code. In case you run into this, use this link to
change the JVM under which intelliJ runs.

https://intellij-support.jetbrains.com/entries/23455956-Selecting-the-JDK-version-the-IDE-will-run-under
"
Gary Malouf <malouf.gary@gmail.com>,"Sat, 10 May 2014 23:02:05 -0400",Re: Spark on Scala 2.11,dev@spark.apache.org,"Considering the team just bumped to 2.10 in 0.9, I would be surprised if
this is a near term priority.



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 10 May 2014 20:35:21 -0700",LabeledPoint toString to dump LibSvm if SparseVector,dev@spark.apache.org,"Hi,

I need to change the toString on LabeledPoint to libsvm format so that I
can dump RDD[LabeledPoint] as a format that could be read by sparse
glmnet-R and other packages to benchmark mllib classification accuracy...

Basically I have to change the toString of LabeledPoint and toString of
SparseVector....

Should I add it as a PR or is it already being added ?

I added these functions toLibSvm in my internal util class for now...

def toLibSvm(labelPoint: LabeledPoint): String = {

    labelPoint.label.toString + "" "" + toLibSvm(labelPoint.features
.asInstanceOf[SparseVector])

  }

  def toLibSvm(features: SparseVector): String = {

    val indices = features.indices

    val values = features.values

    indices.zip(values).mkString("" "").replace(',', ':').replace(""("", """"
).replace("")"","""")

  }
Thanks.
Deb




"
Andrew Ash <andrew@andrewash.com>,"Thu, 8 May 2014 00:51:36 -0700",Updating docs for running on Mesos,dev@spark.apache.org,"The docs for how to run Spark on Mesos have changed very little since
0.6.0, but setting it up is much easier now than then.  Does it make sense
to revamp with the below changes?


You no longer need to build mesos yourself as pre-built versions are
available from Mesosphere: http://mesosphere.io/downloads/

And the instructions guide you towards compiling your own distribution of
Spark, when you can use the prebuilt versions of Spark as well.


I'd like to split that portion of the documentation into two sections, a
build-from-scratch section and a use-prebuilt section.  The new outline
would look something like this:


*Running Spark on Mesos*

Installing Mesos
- using prebuilt (recommended)
 - pointer to mesosphere's packages
- from scratch
 - (similar to current)


Connecting Spark to Mesos
- loading distribution into an accessible location
- Spark settings

Mesos Run Modes
- (same as current)

Running Alongside Hadoop
- (trim this down)



Does that work for people?


Thanks!
Andrew


PS Basically all the same:

http://spark.apache.org/docs/0.6.0/running-on-mesos.html
http://spark.apache.org/docs/0.6.2/running-on-mesos.html
http://spark.apache.org/docs/0.7.3/running-on-mesos.html
http://spark.apache.org/docs/0.8.1/running-on-mesos.html
http://spark.apache.org/docs/0.9.1/running-on-mesos.html
https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
"
Anand Avati <avati@gluster.org>,"Thu, 8 May 2014 18:33:30 -0700",Spark on Scala 2.11,dev@spark.apache.org,"Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
Approximate timeline?

Thanks
"
Soren Macbeth <soren@yieldbot.com>,"Thu, 8 May 2014 09:55:05 -0700",Requirements of objects stored in RDDs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

What are the requirements of objects that are stored in RDDs?

I'm still struggling with an exception I've already posted about several
times. My questions are:

1) What interfaces are objects stored in RDDs expected to implement, if any?
2) Are collections (be they scala, java or otherwise) handled differently
than other objects?

The bug I'm hitting is when I try to use my clojure DSL (which wraps the
java api) with clojure collections, specifically
clojure.lang.PersistentVectors in my RDDs. Here is the exception message:

org.apache.spark.SparkException: Job aborted: Exception while deserializing
and fetching task: com.esotericsoftware.kryo.KryoException:
java.lang.IllegalArgumentException: Can not set final scala.collecti
on.convert.Wrappers field
scala.collection.convert.Wrappers$SeqWrapper.$outer to
clojure.lang.PersistentVector

Now, this same application works fine in local mode and tests, but it fails
when run under mesos. That would seem to me to point to something around
RDD partitioning for tasks, but I'm not sure.

I don't know much scala, but according to google, SeqWrapper is part of the
implicit JavaConversion functionality of scala collections. Under what
circumstances would spark be trying to wrap my RDD objects in scala
collections?

Finally - I'd like to point out that this is not a serialization issue with
my clojure collection objects. I have registered serializers for them and
have verified they serialize and deserialize perfectly well in spark.

for a reduce stage and the results are returned to the driver.

TIA
"
Debasish Das <debasish.das83@gmail.com>,"Sun, 11 May 2014 09:40:10 -0700",LabeledPoint dump LibSVM if SparseVector,dev <dev@spark.incubator.apache.org>,"Hi,

I need to change the toString on LabeledPoint to libsvm format so that I
can dump RDD[LabeledPoint] as a format that could be read by sparse
glmnet-R and other packages to benchmark mllib classification accuracy...

Basically I have to change the toString of LabeledPoint and toString of
SparseVector....

Should I add it as a PR or is it already being added ?

I added these functions toLibSvm in my internal util class for now...

def toLibSvm(labelPoint: LabeledPoint): String = {

    labelPoint.label.toString + "" "" +
toLibSvm(labelPoint.features.asInstanceOf[SparseVector])

  }

  def toLibSvm(features: SparseVector): String = {

    val indices = features.indices

    val values = features.values

    indices.zip(values).mkString(""
"").replace(',', ':').replace(""("", """").replace("")"","""")

  }
Thanks.
Deb
"
Debasish Das <debasish.das83@gmail.com>,"Wed, 7 May 2014 22:25:58 -0700",Re: mllib vector templates,dev <dev@spark.incubator.apache.org>,"Hi,

I see ALS is still using Array[Int] but for other mllib algorithm we moved
to Vector[Double] so that it can support either dense and sparse formats...

ALS can stay in Array[Int] due to the Netflix format for input datasets
which is well defined but it helps if we move ALS to Vector[Double] as
well...that way all algorithms will be consistent...

The second issue is that toString on SparseVector does not write libsvm
format but something not very generic...can we change the
SparseVector.toString to write as libsvm output ? I am dumping a sample of
dataset to see how mllib glm compares with the glmnet-R package for QoR...

Thanks.
Deb

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 11 May 2014 14:08:27 -0700",Re: Spark on Scala 2.11,dev@spark.apache.org,"We do want to support it eventually, possibly as early as Spark 1.1 (which we’d cross-build on Scala 2.10 and 2.11). If someone wants to look at it before, feel free to do so! Scala 2.11 is very close to 2.10 so I think things will mostly work, except for possibly the REPL (which has require porting over code form the Scala REPL in each version).

Matei




"
Koert Kuipers <koert@tresata.com>,"Sun, 11 May 2014 18:29:00 -0400",Re: Spark on Scala 2.11,dev@spark.apache.org,"i believe matei has said before that he would like to crossbuild for 2.10
and 2.11, given that the difference is not as big as between 2.9 and 2.10.
but dont know when this would happen...



"
Andy Konwinski <andykonwinski@gmail.com>,"Sun, 11 May 2014 16:28:11 -0700",Re: Updating docs for running on Mesos,dev@spark.apache.org,"Thanks for suggesting this and volunteering to do it.


What trimming do you have in mind here?

https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 11 May 2014 14:16:26 -0700",Re: Updating docs for running on Mesos,"""dev@spark.apache.org"" <dev@spark.apache.org>","Andrew,

Updating these docs would be great! I think this would be a welcome change.

In terms of packaging, it would be good to mention the binaries
produced by the upstream project as well, in addition to Mesosphere.

- Patrick


"
fengshen <lwwcl1314@gmail.com>,"Mon, 12 May 2014 03:01:30 -0700 (PDT)",about spark interactive shell,dev@spark.incubator.apache.org,"i email to user listï¼Œbut no body relpy me.
so, i email to this. i  hope relpy

I am now using spark in production. but I notice spark driver including rdd
and dag... 
and the executors will try to register with the driver. 
but  in my company the executors do not register with the client because of
Network Limited.
I think the driver should run on the cluster and client should  run on the
gateway. 
Similar like:
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n6514/Spark-interactive_shell.jpg> 

i hope we take about this and how to implement this.



--
3.nabble.com/about-spark-interactive-shell-tp6514.html
om.

"
Han JU <ju.han.felix@gmail.com>,"Mon, 12 May 2014 15:16:30 +0200",[EC2] r3 instance type,dev@spark.apache.org,"Hi,

I'm modifying the ec2 script for the new r3 instance support, but there's a
problem with the instance storage.

For example, `r3.large` has a single 32GB SSD disk, the problem is that
it's a SSD with TRIM technology and is not automatically formatted and
mounted, `lsblk` gives me this after ec2_script's setup:

xvda    202:0    0   8G  0 disk
â””â”€xvda1 202:1    0   8G  0 part /
xvdb    202:16   0  30G  0 disk

I think there's some workarounds of this problem, for example we could
treat it like an EBS device and check `/dev/xvdb` by using `blkid`, howver
this needs modifying the deployment script inside the AMI and I don't know
if it's the preferred way .

Some ideas or suggestions?

Thanks.
-- 
*JU Han*

Data Engineer @ Botify.com

+33 0619608888
"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 12 May 2014 08:07:08 -0700",Re: LabeledPoint dump LibSVM if SparseVector,dev@spark.apache.org,"Hi Deb,

There is a saveAsLibSVMFile in MLUtils now. Also, I submitted a PR for
standardizing text format of vectors and labeled point:
https://github.com/apache/spark/pull/685

Best,
Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 12 May 2014 08:07:08 -0700",Re: LabeledPoint dump LibSVM if SparseVector,dev@spark.apache.org,"Hi Deb,

There is a saveAsLibSVMFile in MLUtils now. Also, I submitted a PR for
standardizing text format of vectors and labeled point:
https://github.com/apache/spark/pull/685

Best,
Xiangrui


"
Anand Avati <avati@gluster.org>,"Thu, 8 May 2014 18:33:30 -0700",Spark on Scala 2.11,dev@spark.apache.org,"Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
Approximate timeline?

Thanks
"
Soren Macbeth <soren@yieldbot.com>,"Mon, 12 May 2014 10:39:04 -0700",Bug is KryoSerializer under Mesos [work-around included],"""dev@spark.apache.org"" <dev@spark.apache.org>","I finally managed to track down the source of the kryo issues that I was
having under mesos.

What happens is the for a reason that I haven't tracked down yet, a handful
of the scala collection classes from chill-scala down get registered by the
mesos executors, but they do all get registered in the driver process.

This led to scala.Some classes which were serialized by the executors being
incorrectly deserialized as scala.collections.Wrappers$SeqWrapper in driver
during task deserialization, causing a KryoException.

I resolved this issue in my spark job by explicitly registering the classes
in my Registrator like so:


kryo.register(scala.collection.convert.Wrappers.IteratorWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.SeqWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.MapWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.JListWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.JMapWrapper.class);

Again, I'm not sure why they don't get registered in the mesos executors,
but I wanted to report wht I found as well as a workaround in case anyone
else hit this (extraordinarily frustrating) issue again.

Some interactive debugging note are available in this gist:

https://gist.github.com/sorenmacbeth/28707a7a973f7a1982dc

Cheers,
Soren
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 12 May 2014 12:02:57 -0700",Re: Bug is KryoSerializer under Mesos [work-around included],dev@spark.apache.org,"Hey Soren, are you sure that the JAR you used on the executors is for the right version of Spark? Maybe they’re running an older version. The Kryo serializer should be initialized the same way on both.

Matei


was
handful
by the
being
driver
classes
kryo.register(scala.collection.convert.Wrappers.IteratorWrapper.class);
kryo.register(scala.collection.convert.Wrappers.SeqWrapper.class);
kryo.register(scala.collection.convert.Wrappers.MapWrapper.class);
kryo.register(scala.collection.convert.Wrappers.JListWrapper.class);
kryo.register(scala.collection.convert.Wrappers.JMapWrapper.class);
executors,
anyone


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 8 May 2014 14:16:43 -0700",Any ideas on SPARK-1021?,dev@spark.apache.org,"I'm trying to decide whether attacking the underlying issue of
RangePartitioner running eager jobs in rangeBounds (i.e. SPARK-1021) is a
better option than a messy workaround for some async job-handling stuff
that I am working on.  It looks like there have been a couple of aborted
attempts to solve the problem, but no solution or clear path to one at this
point.

Has anybody made any further progress or does anyone have any new ideas on
how to proceed?
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 13 May 2014 00:54:18 +0200",Re: Spark on Scala 2.11,dev@spark.apache.org,"rote:
h weâ€™d cross-build on Scala 2.10 and 2.11). If someone wants to look at it before, feel free to do so! Scala 2.11 is very close to 2.10 so I think things will mostly work, except for possibly the REPL (which has require porting over code form the Scala REPL in each version).

Hi,

Would that be possible to have a JIRA issue for this (so I could have
a branch for the cross-build in sbt and give the task a try)?

Jacek

-- 
Jacek Laskowski | http://blog.japila.pl
""Never discourage anyone who continually makes progress, no matter how
slow."" Plato

"
Anand Avati <avati@gluster.org>,"Mon, 12 May 2014 14:47:03 -0700",Kryo not default?,dev@spark.apache.org,"Hi,
Can someone share the reason why Kryo serializer is not the default? Is
there anything to be careful about (because of which it is not enabled by
default)?
Thanks!
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 12 May 2014 14:07:19 -0700",Re: [EC2] r3 instance type,dev@spark.apache.org,"I ran into this a couple of days back as well. Yes, we need to check if
/dev/xvdb is formatted and if not create xfs or some such filesystem on it.
We will need to change the deployment script and you can do that (similar
to EBS volumes) at https://github.com/mesos/spark-ec2/blob/v2/setup-slave.sh


Thanks
Shivaram




 a
r
w
"
Anand Avati <avati@gluster.org>,"Mon, 12 May 2014 14:09:23 -0700",Re: Spark on Scala 2.11,dev@spark.apache.org,"Matei,
Thanks for confirming. I was looking specifically at the REPL part and how
it can be significantly simplified with 2.11 Scala, without having to
inherit a full copy of a refactored repl inside Spark. I am happy to
investigate/contribute a simpler 2.11 based REPL if this is were seen as a
priority (1.1 does not seem ""too far"" away.) However a 2.10 compatible
cross build would still require a separate (existing) REPL code for the
2.10 build, no?

Thanks.

te:

h
k at it
"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Mon, 12 May 2014 16:47:13 -0700",Re: Kryo not default?,dev@spark.apache.org,"

why should it be?

now) is java serialization (which means java serialization is required of
all closure attributes)



Yes. Kind of stems from above. There's still a number of api calls that use
closure attributes to serialize data to backend (see fold(), for example).
which means even if you enable kryo, some api still requires java
serialization of an attribute.

I fixed parallelize(), collect() and something else that i don't remember
already in that regard, but i think even up till now there's still a number
of apis lingering whose data parameters  wouldn't work with kryo.


"
Andrew Ash <andrew@andrewash.com>,"Mon, 12 May 2014 16:56:45 -0700",Re: Kryo not default?,dev@spark.apache.org,"As an example of where it sometimes doesn't work, in older versions of Kryo
/ Chill the Joda LocalDate class didn't serialize properly --
https://groups.google.com/forum/#!topic/cascalog-user/35cdnNIamKU



"
Reynold Xin <rxin@databricks.com>,"Mon, 12 May 2014 19:39:11 -0400",Re: Kryo not default?,dev@spark.apache.org,"The main reason is that it doesn't always work (e.g. sometimes application
program has special serialization / externalization written already for
Java which don't work in Kryo).


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 12 May 2014 16:54:40 -0700",Re: Kryo not default?,dev@spark.apache.org,"It was just because it might not work with some user data types that are Serializable. But we should investigate it, as it’s the easiest thing one can enable to improve performance.

Matei


Is
by


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 12 May 2014 18:27:36 -0700",Re: Spark on Scala 2.11,dev@spark.apache.org,"We can build the REPL separately for each version of Scala, or even give that package a different name in Scala 2.11.

Scala 2.11’s REPL actually added two flags, -Yrepl-class-based and -Yrepl-outdir, that encompass the two modifications we made to the REPL (using classes instead of objects to wrap each line, and grabbing the files from some directory). So it might be possible to run it without modifications using just a simple wrapper class around it. That would definitely simplify things!

BTW did the non-REPL parts run fine on 2.11?

Matei


how
as a
the
(which
at it
think
require
2.11?


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 12 May 2014 16:54:01 -0700",Re: Spark on Scala 2.11,dev@spark.apache.org,"Anyone can actually open a JIRA on https://issues.apache.org/jira/browse/SPARK. I’ve created one for this now: https://issues.apache.org/jira/browse/SPARK-1812.

Matei


(which we’d cross-build on Scala 2.10 and 2.11). If someone wants to look at it before, feel free to do so! Scala 2.11 is very close to 2.10 so I think things will mostly work, except for possibly the REPL (which has require porting over code form the Scala REPL in each version).


"
Andrew Ash <andrew@andrewash.com>,"Mon, 12 May 2014 20:42:49 -0700",Re: Any ideas on SPARK-1021?,dev@spark.apache.org,"This is the issue where .sortByKey() launches a cluster job when it
shouldn't because it's a transformation not an action.

https://issues.apache.org/jira/browse/SPARK-1021

I'd appreciate a fix too but don't currently have any thoughts on how to
proceed forward.

Andrew



"
Andrew Ash <andrew@andrewash.com>,"Mon, 12 May 2014 21:23:20 -0700",Re: Requirements of objects stored in RDDs,dev@spark.apache.org,"An RDD can hold objects of any type.  If you generally think of it as a
distributed Collection, then you won't ever be that far off.

As far as serialization, the contents of an RDD must be serializable.
 There are two serialization libraries you can use with Spark: normal Java
serialization or Kryo serialization.  See
https://spark.apache.org/docs/latest/tuning.html#data-serialization for
more details.

If you are using Java serialization then just implementing the Serializable
interface will work.  If you're using Kryo, then

The point that it works fine with local mode and tests but fails in Mesos,
that makes me think there's an issue with the Mesos cluster deployment.
 First, does it work properly in standalone mode?  Second, how are you
getting the Clojure libraries onto the Mesos executors?  Are they included
in your executor URI bundle, or otherwise passing a parameter that points
to the clojure jars?

Cheers,
Andrew



"
Anand Avati <avati@gluster.org>,"Mon, 12 May 2014 21:31:23 -0700",Re: Spark on Scala 2.11,dev@spark.apache.org,"te:


OK.


d
es

Exactly. I have been tracking those changes in 2.11 as well. We would need
a simple wrapper around ILoop, set the Yreplclassbased flag, an HttpServer
to export the repl-outdir, bind sc into repl namespace and maybe little
more initialization (customizing repl init code in 2.11 is not as trivial
as the <=2.10 versions, but still doable.) Still, even with all this, it
should be much simpler than pulling in and refactoring everything like
today. I already have a prototype for this in my working tree, still needs
integration testing.



Currently fighting to get all the dependencies in 2.11. Quick pointer where
I can get sources for akka-*-X.Y-shared-protobuf? Also, what's the smallest
set of dependencies to build the smallest testable subset of the project?

Thanks!


s
look at it
k
re
?
"
Andrew Ash <andrew@andrewash.com>,"Mon, 12 May 2014 21:57:49 -0700",Re: Updating docs for running on Mesos,dev@spark.apache.org,"As far as I know, the upstream doesn't release binaries, only source code.
 The downloads page <https://mesos.apache.org/downloads/> for 0.18.0 only
has a source tarball.  Is there a binary release somewhere from Mesos that
I'm missing?



"
Andrew Ash <andrew@andrewash.com>,"Mon, 12 May 2014 21:56:28 -0700",Re: Updating docs for running on Mesos,dev@spark.apache.org,"For trimming the Running Alongside Hadoop section I mostly think there
should be a separate Spark+HDFS section and have the CDH+HDP page be merged
into that one, but I supposed that's a separate docs change.



"
Andrew Ash <andrew@andrewash.com>,"Mon, 12 May 2014 21:46:13 -0700",Preliminary Parquet numbers and including .count() in Catalyst,dev@spark.apache.org,"Hi Spark devs,

First of all, huge congrats on the parquet integration with SparkSQL!  This
is an incredible direction forward and something I can see being very
broadly useful.

I was doing some preliminary tests to see how it works with one of my
workflows, and wanted to share some numbers that people might want to know
about.

I also wanted to point out that .count() doesn't seem integrated with the
rest of the optimization framework, and some big gains could be possible.


So, the numbers:

I took a table extracted from a SQL database and stored in HDFS:

   - 115 columns (several always-empty, mostly strings, some enums, some
   numbers)
   - 253,887,080 rows
   - 182,150,295,881 bytes (raw uncompressed)
   - 42,826,820,222 bytes (lzo compressed with .index file)

And I converted it to Parquet using SparkSQL's SchemaRDD.saveAsParquet()
call:

   - Converting from .lzo in HDFS to .parquet in HDFS took 635s using 42
   cores across 4 machines
   - 17,517,922,117 bytes (parquet per SparkSQL defaults)

So storing in parquet format vs lzo compresses the data down to less than
50% of the .lzo size, and under 10% of the raw uncompressed size.  Nice!


I then did some basic interactions on it:

*Row count*

   - LZO
      - lzoFile(""/path/to/lzo"").count
      - 31.632305953s
   - Parquet
      - sqlContext.parquetFile(""/path/to/parquet"").count
      - 289.129487003s

Reassembling rows from the separate column storage is clearly really
expensive.  Median task length is 33s vs 4s, and of that 33s in each task
(319 tasks total) about 1.75 seconds are spent in GC (inefficient object
allocation?)



*Count number of rows with a particular key:*

   - LZO
   - lzoFile(""/path/to/lzo"").filter(_.split(""\\|"")(0) == ""1234567890"").count
      - 73.988897511s
       - Parquet
   - sqlContext.parquetFile(""/path/to/parquet"").where('COL ===
      1234567890).count
      - 293.410470418s
       - Parquet (hand-tuned to count on just one column)
   - sqlContext.parquetFile(""/path/to/parquet"").where('COL ===
      1234567890).select('IDCOL).count
      - 1.160449187s

It looks like currently the .count() on parquet is handled incredibly
inefficiently and all the columns are materialized.  But if I select just
that relevant column and then count, then the column-oriented storage of
Parquet really shines.

There ought to be a potential optimization here such that a .count() on a
SchemaRDD backed by Parquet doesn't require re-assembling the rows, as
that's expensive.  I don't think .count() is handled specially in
SchemaRDDs, but it seems ripe for optimization.


*Count number of distinct values in a column*

   - LZO
   - lzoFile(""/path/to/lzo"").map(sel(0)).distinct.count
      - 115.582916866s
       - Parquet
   - sqlContext.parquetFile(""/path/to/parquet"").select('COL).distinct.count
      - 16.839004826 s

It turns out column selectivity is very useful!  I'm guessing that if I
could get byte counts read out of HDFS, that would just about match up with
the difference in read times.




Any thoughts on how to embed the knowledge of my hand-tuned additional
.select('IDCOL)
into Catalyst?


Thanks again for all the hard work and prep for the 1.0 release!

Andrew
"
Reynold Xin <rxin@databricks.com>,"Mon, 12 May 2014 23:41:59 -0700",Re: Preliminary Parquet numbers and including .count() in Catalyst,dev@spark.apache.org,"Thanks for the experiments and analysis!

I think Michael already submitted a patch that avoids scanning all columns
for count(*) or count(1).



"
Andrew Ash <andrew@andrewash.com>,"Mon, 12 May 2014 23:49:26 -0700",Re: Updating docs for running on Mesos,dev@spark.apache.org,"I have a draft of my proposed changes here:

https://github.com/apache/spark/pull/756
https://issues.apache.org/jira/browse/SPARK-1818

Thanks!
Andrew



"
Jacek Laskowski <jacek@japila.pl>,"Tue, 13 May 2014 01:56:07 +0200",Re: Spark on Scala 2.11,dev@spark.apache.org,"Thanks a lot!

Jacek

ote:
SPARK. Iâ€™ve created one for this now: https://issues.apache.org/jira/browse/SPARK-1812.
ich weâ€™d cross-build on Scala 2.10 and 2.11). If someone wants to look at it before, feel free to do so! Scala 2.11 is very close to 2.10 so I think things will mostly work, except for possibly the REPL (which has require porting over code form the Scala REPL in each version).



-- 
Jacek Laskowski | http://blog.japila.pl
""Never discourage anyone who continually makes progress, no matter how
slow."" Plato

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 13 May 2014 00:59:42 -0700",Re: Updating docs for running on Mesos,dev@spark.apache.org,"I’ll ask the Mesos folks about this. Unfortunately it might be tough to link only to a company’s builds; but we can perhaps include them in addition to instructions for building Mesos from Apache.

Matei


kidding -
mesos
code.
only
that
since
make
are
distribution
sections,
outline
https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html


"
Andrew Ash <andrew@andrewash.com>,"Tue, 13 May 2014 01:30:25 -0700",Re: Updating docs for running on Mesos,dev@spark.apache.org,"Completely agree about preferring to link to the upstream project rather
than a company's -- the only reason I'm using mesosphere's now is that I
see no alternative from mesos.apache.org

I included instructions for both using Mesosphere's packages and building
from scratch in the PR: https://github.com/apache/spark/pull/756


ote:

ugh to
in
e
on
s,
.html
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 13 May 2014 01:36:33 -0700",[VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.0.0!

The tag to be voted on is v1.0.0-rc5 (commit 18f0623):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=18f062303303824139998e8fc8f4158217b0dbc3

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc5/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1012/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Friday, May 16, at 09:30 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

"
Gerard Maas <gerard.maas@gmail.com>,"Tue, 13 May 2014 08:55:17 +0200",Re: Updating docs for running on Mesos,dev@spark.apache.org,"Andrew,

Mesosphere has binary releases here:
http://mesosphere.io/downloads/

(Anecdote: I actually burned a CPU building Mesos from source. No kidding -
it was coming, as the laptop was crashing from time to time, but the mesos
build was that one drop too much)

kr, Gerard.




"
Gerard Maas <gerard.maas@gmail.com>,"Tue, 13 May 2014 14:13:53 +0200",Re: Updating docs for running on Mesos,dev@spark.apache.org,"Great work!. I just left some comments in the PR. In summary, it would be
great to have more background on how Spark works on Mesos and how the
different elements interact. That will (hopefully) help understanding the
practicalities of the common assembly location (http/hdfs) and how the jobs
are distributed to the Mesos infrastructure.

Also, adding a chapter on troubleshooting (where we have spent most of our
time lately :-) would be a welcome addition.  I'm not use I've figured it
out completely as to attempt to contribute that myself.

-kr, Gerard.



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 13 May 2014 04:08:11 -0700",Multinomial Logistic Regression,dev@spark.apache.org,"Hi,

Is there a PR for multinomial logistic regression which does one-vs-all and
compare it to the other possibilities ?

@dbtsai in your strata presentation you used one vs all ? Did you add some
constraints on the fact that you penalize if mis-predicted labels are not
very far from the true label ?

Thanks.
Deb
"
Sean Owen <sowen@cloudera.com>,"Tue, 13 May 2014 14:49:06 +0100",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
Good news is that the sigs, MD5 and SHA are all correct.

Tiny note: the Maven artifacts use SHA1, while the binary artifacts
use SHA512, which took me a bit of head-scratching to figure out.

If another RC comes out, I might suggest making it SHA1 everywhere?
But there is nothing wrong with these signatures and checksums.

Now to look at the contents...

"
Sean Owen <sowen@cloudera.com>,"Tue, 13 May 2014 16:01:25 +0100",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
This is a bit of drudgery that probably needs to be done too: a review
of the LICENSE and NOTICE file. Having dumped the licenses of
dependencies, I don't believe these reflect all of the software that's
going to be distributed in 1.0.

(Good news is there's no forbidden license stuff included AFAICT.)

And good news is that NOTICE can be auto-generated, largely, with a
Maven plugin. This can be done manually for now.

And there is a license plugin that will list all known licenses of
transitive dependencies so that LICENSE can be filled out fairly
easily.

What say? want a JIRA with details?

"
Tim St Clair <tstclair@redhat.com>,"Tue, 13 May 2014 12:27:34 -0400 (EDT)",Re: Updating docs for running on Mesos,dev@spark.apache.org,"Perhaps linking to a Mesos page, which then can list the various package incantations. 

Cheers,
Tim

ly
that
e
on
s,
ine
sos.html

-- 
Cheers,
Tim
Freedom, Features, Friends, First -> Fedora
https://fedoraproject.org/wiki/SIGs/bigdata

"
Andrew Ash <andrew@andrewash.com>,"Tue, 13 May 2014 01:34:45 -0700",Re: Preliminary Parquet numbers and including .count() in Catalyst,dev@spark.apache.org,"These numbers were run on git commit 756c96 (a few days after the 1.0.0-rc3
tag).  Do you have a link to the patch that avoids scanning all columns for
count(*) or count(1)?  I'd like to give it a shot.

Andrew



"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 13 May 2014 09:59:48 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"There were a few early/test RCs this cycle that were never put to a vote.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 13 May 2014 16:02:55 -0400",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"Ah, I see, thanks 

-- 
Nan Zhu




"
Michael Malak <michaelmalak@yahoo.com>,"Tue, 13 May 2014 08:26:54 -0700 (PDT)",Serializable different behavior Spark Shell vs. Scala Shell,"""dev@spark.apache.org"" <dev@spark.apache.org>","Reposting here on dev since I didn't see a response on user:

I'm seeing different Serializable behavior in Spark Shell vs. Scala Shell. In the Spark Shell, equals() fails when I use the canonical equals() pattern of match{}, but works when I subsitute with isInstanceOf[]. I am using Spark 0.9.0/Scala 2.10.3.

Is this a bug?

Spark Shell (equals uses match{})
=================================

class C(val s:String) extends Serializable {
  override def equals(o: Any) = o match {
    case that: C => that.s == s
    case _ => false
  }
}

val x = new C(""a"")
val bos = new java.io.ByteArrayOutputStream()
val out = new java.io.ObjectOutputStream(bos)
out.writeObject(x);
val b = bos.toByteArray();
out.close
bos.close
val y = new java.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
x.equals(y)

res: Boolean = false

Spark Shell (equals uses isInstanceOf[])
========================================

class C(val s:String) extends Serializable {
  override def equals(o: Any) = if (o.isInstanceOf[C]) (o.asInstanceOf[C].s == s) else false
}

val x = new C(""a"")
val bos = new java.io.ByteArrayOutputStream()
val out = new java.io.ObjectOutputStream(bos)
out.writeObject(x);
val b = bos.toByteArray();
out.close
bos.close
val y = new java.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
x.equals(y)

res: Boolean = true

Scala Shell (equals uses match{})
=================================

class C(val s:String) extends Serializable {
  override def equals(o: Any) = o match {
    case that: C => that.s == s
    case _ => false
  }
}

val x = new C(""a"")
val bos = new java.io.ByteArrayOutputStream()
val out = new java.io.ObjectOutputStream(bos)
out.writeObject(x);
val b = bos.toByteArray();
out.close
bos.close
val y = new java.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
x.equals(y)

res: Boolean = true

"
Michael Malak <michaelmalak@yahoo.com>,"Tue, 13 May 2014 15:17:04 -0700 (PDT)",Re: Serializable different behavior Spark Shell vs. Scala Shell,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thank you for your investigation into this!

Just for completeness, I've confirmed it's a problem only in REPL, not in compiled Spark programs.

But within REPL, a direct consequence of non-same classes after serialization/deserialization also means that lookup() doesn't work:

scala> class C(val s:String) extends Serializable {
     |   override def equals(o: Any) = if (o.isInstanceOf[C]) o.asInstanceOf[C].s == s else false
     |   override def toString = s
     | }
defined class C

scala> val r = sc.parallelize(Array((new C(""a""),11),(new C(""a""),12)))
r: org.apache.spark.rdd.RDD[(C, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:14

scala> r.lookup(new C(""a""))
<console>:17: error: type mismatch;
 found   : C
 required: C
              r.lookup(new C(""a""))
                       ^
 user:
>
>I'm seeing different Serializable behavior in Spark Shell vs. Scala Shell. In the Spark Shell, equals() fails when I use the canonical equals() pattern of match{}, but works when I subsitute with isInstanceOf[]. I am using Spark 0.9.0/Scala 2.10.3.
>
>Is this a bug?
>
>Spark Shell (equals uses match{})
>=================================
>
>class C(val s:String) extends Serializable {
>  override def equals(o: Any) = o match {
>    case that: C => that.s == s
>    case _ => false
>  }
>}
>
>val x = new C(""a"")
>val bos = new java.io.ByteArrayOutputStream()
>val out = new java.io.ObjectOutputStream(bos)
>out.writeObject(x);
>val b = bos.toByteArray();
>out.close
InputStream(b)).readObject().asInstanceOf[C]
>x.equals(y)
>
>res: Boolean = false
>
>Spark Shell (equals uses isInstanceOf[])
>========================================
>
>class C(val s:String) extends Serializable {
>  override def equals(o: Any) = if (o.isInstanceOf[C]) (o.asInstanceOf[C].s == s) else false
>}
>
>val x = new C(""a"")
>val bos = new java.io.ByteArrayOutputStream()
>val out = new java.io.ObjectOutputStream(bos)
>out.writeObject(x);
>val b = bos.toByteArray();
>out.close
>bos.close
>val y = new java.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
>x.equals(y)
>
>res: Boolean = true
>
>Scala Shell (equals uses match{})
>=================================
>
>class C(val s:String) extends Serializable {
>  override def equals(o: Any) = o match {
>    case that: C => that.s == s
>    case _ => false
>  }
>}
>
>val x = new C(""a"")
>val bos = new java.io.ByteArrayOutputStream()
>val out = new java.io.ObjectOutputStream(bos)
>out.writeObject(x);
>val b = bos.toByteArray();
>out.close
>bos.close
>val y = new java.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
>x.equals(y)
>
>res: Boolean = true
>


Hmm. I see that this can be reproduced without Spark in Scala 2.11, with and without -Yrepl-class-based command line flag to the repl. Spark's REPL has the effective behavior of 2.11's -Yrepl-class-based flag. Inspecting the byte code generated, it appears -Yrepl-class-based results in the creation of ""$outer"" field in the generated classes (including class C). The first case match in equals() is resulting code along the lines of (simplified):

if (o isinstanceof Cstr && this.$outer == that.$outer) { // do string compare // }

$outer is the synthetic field object to the outer object in which the object was created (in this case, the repl environment). Now obviously, when x is taken through the bytestream and deserialized, it would have a new $outer created (it may have deserialized in a different jvm or machine for all we know). So the $outer's mismatching is expected. However I'm still trying to understand why the outers need to be the same for the case match.

"
Madhu <madhu@madhu.com>,"Tue, 13 May 2014 17:03:52 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.incubator.apache.org,"I built rc5 using sbt/sbt assembly on Linux without any problems.
There used to be an sbt.cmd for Windows build, has that been deprecated?
If so, I can document the Windows build steps that worked for me.



--

"
Anand Avati <avati@gluster.org>,"Tue, 13 May 2014 14:05:42 -0700",Re: Serializable different behavior Spark Shell vs. Scala Shell,"dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>","


Hmm. I see that this can be reproduced without Spark in Scala 2.11, with
and without -Yrepl-class-based command line flag to the repl. Spark's REPL
has the effective behavior of 2.11's -Yrepl-class-based flag. Inspecting
the byte code generated, it appears -Yrepl-class-based results in the
creation of ""$outer"" field in the generated classes (including class C).
The first case match in equals() is resulting code along the lines of
(simplified):

if (o isinstanceof Cstr && this.$outer == that.$outer) { // do string
compare // }

$outer is the synthetic field object to the outer object in which the
object was created (in this case, the repl environment). Now obviously,
when x is taken through the bytestream and deserialized, it would have a
new $outer created (it may have deserialized in a different jvm or machine
for all we know). So the $outer's mismatching is expected. However I'm
still trying to understand why the outers need to be the same for the case
match.
"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 13 May 2014 21:16:27 -0400",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"+1, replaced rc3 with rc5, all applications are working fine

Best, 

-- 
Nan Zhu





"
Madhu <madhu@madhu.com>,"Tue, 13 May 2014 18:15:33 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.incubator.apache.org,"I just built rc5 on Windows 7 and tried to reproduce the problem described in

https://issues.apache.org/jira/browse/SPARK-1712

It works on my machine:

14/05/13 21:06:47 INFO DAGScheduler: Stage 1 (sum at <console>:17) finished
in 4.548 s
14/05/13 21:06:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks
have all completed, from pool
14/05/13 21:06:47 INFO SparkContext: Job finished: sum at <console>:17, took
4.814991993 s
res1: Double = 5.000005E11

I used all defaults, no config files were changed.
Not sure if that makes a difference...



--

"
Andrew Ash <andrew@andrewash.com>,"Tue, 13 May 2014 16:02:07 -0700",Re: Class-based key in groupByKey?,"dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>","In Scala, if you override .equals() you also need to override .hashCode(),
just like in Java:

http://www.scala-lang.org/api/2.10.3/index.html#scala.AnyRef

I suspect if your .hashCode() delegates to just the hashcode of s then
you'd be good.



"
Andrew Or <andrew@databricks.com>,"Tue, 13 May 2014 11:55:34 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1


2014-05-13 6:49 GMT-07:00 Sean Owen <sowen@cloudera.com>:

"
Michael Malak <michaelmalak@yahoo.com>,"Tue, 13 May 2014 15:30:45 -0700 (PDT)",Class-based key in groupByKey?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is it permissible to use a custom class (as opposed to e.g. the built-in String or Int) for the key in groupByKey? It doesn't seem to be working for me on Spark 0.9.0/Scala 2.10.3:

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

class C(val s:String) extends Serializable {
  override def equals(o: Any) = if (o.isInstanceOf[C]) o.asInstanceOf[C].s == s else false
  override def toString = s
}

object SimpleApp {
  def main(args: Array[String]) {
    val sc = new SparkContext(""local"", ""Simple App"", null, null)
    val r1 = sc.parallelize(Array((new C(""a""),11),(new C(""a""),12)))
    println(""r1="" + r1.groupByKey.collect.mkString("";""))
    val r2 = sc.parallelize(Array((""a"",11),(""a"",12)))
    println(""r2="" + r2.groupByKey.collect.mkString("";""))
  }
}


Output
======
r1=(a,ArrayBuffer(11));(a,ArrayBuffer(12))
r2=(a,ArrayBuffer(11, 12))

"
"""=?ISO-8859-1?B?d2l0Z28=?="" <witgo@qq.com>","Wed, 14 May 2014 12:09:33 +0800",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","You need to set: 
spark.akka.frameSize         5
spark.default.parallelism    1





------------------ Original ------------------
From:  ""Madhu"";<madhu@madhu.com>;
Date:  Wed, May 14, 2014 09:15 AM
To:  ""dev""<dev@spark.incubator.apache.org>; 

Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)



I just built rc5 on Windows 7 and tried to reproduce the problem described in

https://issues.apache.org/jira/browse/SPARK-1712

It works on my machine:

14/05/13 21:06:47 INFO DAGScheduler: Stage 1 (sum at <console>:17) finished
in 4.548 s
14/05/13 21:06:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks
have all completed, from pool
14/05/13 21:06:47 INFO SparkContext: Job finished: sum at <console>:17, took
4.814991993 s
res1: Double = 5.000005E11

I used all defaults, no config files were changed.
Not sure if that makes a difference...



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc5-tp6542p6560.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
."
"""=?ISO-8859-1?B?d2l0Z28=?="" <witgo@qq.com>","Wed, 14 May 2014 08:32:53 +0800",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?="" <dev@spark.apache.org>","-1 
The following bug should be fixed: 
https://issues.apache.org/jira/browse/SPARK-1817
https://issues.apache.org/jira/browse/SPARK-1712


------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Wed, M"
DB Tsai <dbtsai@stanford.edu>,"Tue, 13 May 2014 11:38:59 -0700",Re: Multinomial Logistic Regression,dev@spark.apache.org,"Hi Deb,

For K possible outcomes in multinomial logistic regression,  we can have
K-1 independent binary logistic regression models, in which one outcome is
chosen as a ""pivot"" and then the other K-1 outcomes are separately
regressed against the pivot outcome. See my presentation for technical
detail http://www.slideshare.net/dbtsai/2014-0501-mlor

Since mllib only supports one linear model per classification model, there
will be some infrastructure work to support MLOR in mllib. But if you want
to implement yourself with the L-BFGS solver in mllib, you can follow the
equation in my slide, and it will not be too difficult.

I can give you the gradient method for multinomial logistic regression, you
just need to put the K-1 intercepts in the right place.

  def computeGradient(y: Double, x: Array[Double], lambda: Double, w:
Array[Array[Double]], b: Array[Double],
    gradient: Array[Double]): (Double, Int) = {
    val classes = b.length + 1
    val yy = y.toInt

    def alpha(i: Int): Int = {
      if (i == 0) 1 else 0
    }

    def delta(i: Int, j: Int): Int = {
      if (i == j) 1 else 0
    }

    var denominator: Double = 1.0
    val numerators: Array[Double] = Array.ofDim[Double](b.length)

    var predicted = 1

    {
      var i = 0
      var j = 0
      var acc: Double = 0
      while (i < b.length) {
        acc = b(i)
        j = 0
        while (j < x.length) {
          acc += x(j) * w(i)(j)
          j += 1
        }
        numerators(i) = math.exp(acc)
        if (i > 0 && numerators(i) > numerators(predicted - 1)) {
          predicted = i + 1
        }
        denominator += numerators(i)
        i += 1
      }
      if (numerators(predicted - 1) < 1) {
        predicted = 0
      }
    }

    {
      // gradient has dim of (classes-1) * (x.length+1)
      var i = 0
      var m1: Int = 0
      var l1: Int = 0
      while (i < (classes - 1) * (x.length + 1)) {
        m1 = i % (x.length + 1) // m0 is intercept
        l1 = (i - m1) / (x.length + 1) // l + 1 is class
        if (m1 == 0) {
          gradient(i) += (1 - alpha(yy)) * delta(yy, l1 + 1) -
numerators(l1) / denominator
        } else {
          gradient(i) += ((1 - alpha(yy)) * delta(yy, l1 + 1) -
numerators(l1) / denominator) * x(m1 - 1)
        }
        i += 1
      }
    }
    val loglike: Double = math.round(y).toInt match {
      case 0 => math.log(1.0 / denominator)
      case _ => math.log(numerators(math.round(y - 1).toInt) / denominator)
    }
    (loglike, predicted)
  }



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Andrew Ash <andrew@andrewash.com>,"Tue, 13 May 2014 20:23:58 -0700",Re: Preliminary Parquet numbers and including .count() in Catalyst,dev@spark.apache.org,"Thanks for filing -- I'm keeping my eye out for updates on that ticket.

Cheers!
Andrew



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 13 May 2014 07:52:30 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"SHA-1 is being end-of-lived so I’d actually say switch to 512 for all of them instead.


at:


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 14 May 2014 00:02:26 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey @witgo - those bugs are not severe enough to block the release,
but it would be nice to get them fixed.

At this point we are focused on severe bugs with an immediate fix, or
regressions from previous versions of Spark. Anything that misses this
release will get merged into the branch-1.0 branch and make it into
the 1.0.1 release, so people will have access to it.


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 13 May 2014 13:07:15 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all - there were some earlier RC's that were not presented to the
dev list because issues were found with them. Also, there seems to be
some issues with the reliability of the dev list e-mail. Just a heads
up.

I'll lead with a +1 for this.


"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Tue, 13 May 2014 15:17:46 +0900","Is this supported? : Spark on Windows, Hadoop YARN on Linux.",<dev@spark.apache.org>,"I'm trying to run spark-shell on Windows that uses Hadoop YARN on Linux.
Specifically, the environment is as follows:

- Client
  - OS: Windows 7
  - Spark version: 1.0.0-SNAPSHOT (git cloned 2014.5.8)
- Server
  - Platform: hortonworks sandbox 2.1

I has to modify the spark source code to apply
https://issues.apache.org/jira/browse/YARN-1824, so that the cross-platform
issues can be addressed. (that is, change $() to $$(), File.pathSeparator to
ApplicationConstants.CLASS_PATH_SEPARATOR).
Seeing this, I suspect that the Spark code for now is not prepared to
support cross-platform submit, that is, Spark on Windows -> Hadoop YARN on
Linux.

Anyways, after the modification and some configuration tweak, at least the
yarn-client mode spark-shell submitted from Windows 7 seems to try to start.
But the ApplicationManager fails to register.
Yarn server log is as follows: ('owner' is the user name of the Windows 7
machine.)

Log Type: stderr
Log Length: 1356
log4j:WARN No appenders could be found for logger
(org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for
more info.
14/05/12 01:13:54 INFO YarnSparkHadoopUtil: Using Spark's default log4j
profile: org/apache/spark/log4j-defaults.properties
14/05/12 01:13:54 INFO SecurityManager: Changing view acls to: yarn,owner
14/05/12 01:13:54 INFO SecurityManager: SecurityManager: authentication
disabled; ui acls disabled; users with view permissions: Set(yarn, owner)
14/05/12 01:13:55 INFO Slf4jLogger: Slf4jLogger started
14/05/12 01:13:56 INFO Remoting: Starting remoting
14/05/12 01:13:56 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://sparkYarnAM@sandbox.hortonworks.com:47074]
14/05/12 01:13:56 INFO Remoting: Remoting now listens on addresses:
[akka.tcp://sparkYarnAM@sandbox.hortonworks.com:47074]
14/05/12 01:13:56 INFO RMProxy: Connecting to ResourceManager at
/0.0.0.0:8030
14/05/12 01:13:56 INFO ExecutorLauncher: ApplicationAttemptId:
appattempt_1399856448891_0018_000001
14/05/12 01:13:56 INFO ExecutorLauncher: Registering the ApplicationMaster
14/05/12 01:13:56 WARN Client: Exception encountered while connecting to the
server : org.apache.hadoop.security.AccessControlException: Client cannot
authenticate via:[TOKEN]

How can I handle this error?
Or, should I give up and use Linux for my client machine?
(I want to use Windows for client, since for me it's more comfortable to
develop applications.)
BTW, I'm a newbie for Spark and Hadoop.

Thanks in advance.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 13 May 2014 11:07:12 -0400",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","just curious, where is rc4 VOTE?

I searched my gmail but didn't find that?





"
"""=?ISO-8859-1?B?d2l0Z28=?="" <witgo@qq.com>","Wed, 14 May 2014 15:17:46 +0800",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?="" <dev@spark.apache.org>","SPARK-1817 will cause users to get incorrect results  and RDD.zip is common usage .
This should be the highest priority. I think we should fix the bug,and should also test the previous release
------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Wed, May 14, 2014 03:02 PM
To:  ""dev@spark.apache.org""<dev@spark.apache.org>; 

Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)



Hey @witgo - those bugs are not severe enough to block the release,
but it would be nice to get them fixed.

At this point we are focused on severe bugs with an immediate fix, or
regressions from previous versions of Spark. Anything that misses this
release will get merged into the branch-1.0 branch and make it into
the 1.0.1 release, so people will have access to it.

On Tue, May 13, 2014 at 5:32 PM, witgo <witgo@qq.com> wrote:
> -1
> The following bug should be fixed:
> https://issues.apache.org/jira/browse/SPARK-1817
> https://issues.apache.org/jira/browse/SPARK-1712
>
>
> ------------------ Original ------------------
> From:  ""Patrick Wendell"";<pwendell@gmail.com>;
> Date:  Wed, May 14, 2014 04:07 AM
> To:  ""dev@spark.apache.org""<dev@spark.apache.org>;
>
> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>
>
>
> Hey all - there were some earlier RC's that were not presented to the
> dev list because issues were found with them. Also, there seems to be
> some issues with the reliability of the dev list e-mail. Just a heads
> up.
>
> I'll lead with a +1 for this.
>
> On Tue, May 13, 2014 at 8:07 AM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>> just curious, where is rc4 VOTE?
>>
>> I searched my gmail but didn't find that?
>>
>>
>>
>>
>> On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com> wrote:
>>
>>> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>> > The release files, including signatures, digests, etc. can be found at:
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>>>
>>> Good news is that the sigs, MD5 and SHA are all correct.
>>>
>>> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
>>> use SHA512, which took me a bit of head-scratching to figure out.
>>>
>>> If another RC comes out, I might suggest making it SHA1 everywhere?
>>> But there is nothing wrong with these signatures and checksums.
>>>
>>> Now to look at the contents...
>>>
> .
."
Soren Macbeth <soren@yieldbot.com>,"Wed, 14 May 2014 00:55:34 -0700",Re: Bug is KryoSerializer under Mesos [work-around included],"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Matei,

Yes, I'm 100% positive the jar on the executors is the same version. I am
building everything and deploying myself. Additionally, while debugging the
issue, I forked spark's git repo and added additional logging, which I
could see in the driver and executors. These debugging jars exhibited the
same behaviour.

I agree that the code being called in the executors and the driver *should*
be the same, but I found that not to be the case. I forgot to mention that
this issue only exhibits it's self under mesos; running in local mode or a
standalone cluster (with a single worker, all processes running on my
laptop) I wasn't able to reproduce the issue.

The classes in question should get registered by chill-scala's
AllScalaRegistrar here:

https://github.com/twitter/chill/blob/0.3.6/chill-scala/src/main/scala/com/twitter/chill/ScalaKryoInstantiator.scala#L166

but, for a reason I haven't tracked down, they don't in my mesos executor.
I don't really have a way to test if this is an issue specific only to my
mesos cluster, or if it exhibits in all mesos clusters.

fwiw, I am running spark-0.9.1 with hadoop 2.0.0-mr1-cdh4.6.0 under mesos
0.18.1


ote:

he Kryo
s
);
;
s,
ne
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 14 May 2014 13:04:46 -0700",[RESULT] [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of rc6.


"
Rohit Rai <rohit@tuplejump.com>,"Fri, 9 May 2014 01:28:04 +0530",Re: Problem creating objects through reflection,dev@spark.apache.org,"Hi Piotr,

The easiest solution to this for now is to write all your code (including
the case class) inside a Object and the execution part in a method in the
object. Then you can call the method on the spark shell to executed your
code.

Cheers,
Rohit


*Founder & CEO, **Tuplejump, Inc.*
____________________________
www.tuplejump.com
*The Data Engineering Platform*



J
1,
f
ot
t
nt
s.
t
]
be
e
"
Debasish Das <debasish.das83@gmail.com>,"Wed, 7 May 2014 11:57:42 -0700",Sparse vector toLibSvm API,dev@spark.apache.org,"Hi,

In the sparse vector the toString API is as follows:

 override def toString: String = {

    ""("" + size + "","" + indices.zip(values).mkString(""["", "","" ,""]"") + "")""

  }

Does it make sense to keep it consistent with libsvm format ?

What does each line of libsvm format looks like ?

Thanks.

Deb
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 14 May 2014 13:28:24 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

* Built the release from source.
* Compiled Java and Scala apps that interact with HDFS against it.
* Ran them in local mode.
* Ran them against a pseudo-distributed YARN cluster in both yarn-client
mode and yarn-cluster mode.



"
Michael Armbrust <michael@databricks.com>,"Tue, 13 May 2014 14:40:06 -0700",Re: Preliminary Parquet numbers and including .count() in Catalyst,dev@spark.apache.org,"
Yeah, you are right.  Thanks for pointing this out!

If you call .count() that is just the native Spark count, which is not
aware of the potential optimizations.  We could just override count() in a
schema RDD to be something like
""groupBy()(Count(Literal(1))).collect().head.getInt(0)""

Here is a JIRA: SPARK-1822 - SchemaRDD.count() should use the
optimizer.<https://issues.apache.org/jira/browse/SPARK-1822>

Michael
"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 13 May 2014 21:16:27 -0400",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"+1, replaced rc3 with rc5, all applications are working fine

Best, 

-- 
Nan Zhu





"
Michael Malak <michaelmalak@yahoo.com>,"Wed, 14 May 2014 14:09:21 -0700 (PDT)",map() + lookup() exception,dev@spark.apache.org,"When using map() and lookup() in conjunction, I get an exception (each independently works fine). I'm using Spark 0.9.0/Scala 2.10.3

val a = sc.parallelize(Array(11))
val m = sc.parallelize(Array((11,21)))
a.map(m.lookup(_)(0)).collect

14/05/14 15:03:35 ERROR Executor: Exception in task ID 23
scala.MatchError: null
at org.apache.spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:551)

I'm able to work around it with:

a.map((_,0)).join(m).map(_._2._2).collect"
Patrick Wendell <pwendell@gmail.com>,"Wed, 14 May 2014 13:02:10 -0700",[VOTE] Release Apache Spark 1.0.0 (rc6),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This patch has a few minor fixes on top of rc5. I've also built the
binary artifacts with Hive support enabled so people can test this
configuration. When we release 1.0 we might just release both vanilla
and Hive-enabled binaries.

The tag to be voted on is v1.0.0-rc6 (commit 54133a):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=54133abdce0246f6643a1112a5204afb2c4caa82

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc6/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachestratos-1011

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc6-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Saturday, May 17, at 20:58 UTC and passes if
amajority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 13 May 2014 20:05:43 -0700",Re: Class-based key in groupByKey?,"dev@spark.apache.org,
 Michael Malak <michaelmalak@yahoo.com>","Your key needs to implement hashCode in addition to equals.

Matei


built-in String or Int) for the key in groupByKey? It doesn't seem to be working for me on Spark 0.9.0/Scala 2.10.3:
o.asInstanceOf[C].s == s else false


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 14 May 2014 13:04:24 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm cancelling this vote in favor of rc6.


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 7 May 2014 11:31:58 -0700",Re: mllib vector templates,dev@spark.apache.org,"Hi,

I see ALS is still using Array[Int] but for other mllib algorithm we moved
to Vector[Double] so that it can support either dense and sparse formats...

I know ALS can stay in Array[Int] due to the Netflix format for input
datasets which is well defined but it helps if we move ALS to
Vector[Double] as well...that way all algorithms will be consistent...

Does it make sense ?

Thanks.
Deb




"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 15 May 2014 10:49:02 +0530","Re: [jira] [Created] (SPARK-1767) Prefer HDFS-cached replicas when
 scheduling data-local tasks",dev@spark.apache.org,"Hi Sandy,

  I assume you are referring to caching added to datanodes via new caching
api via NN ? (To preemptively mmap blocks).

I have not looked in detail, but does NN tell us about this in block
locations?
If yes, we can simply make those process local instead of node local for
executors on that node.

This would simply be a change to hadoop based rdd partitioning (what makes
it tricky is to expose currently 'alive' executors to partition)

Thanks
Mridul

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 16 May 2014 02:16:30 -0700",[VOTE] Release Apache Spark 1.0.0 (rc8),"""dev@spark.apache.org"" <dev@spark.apache.org>","[Due to ASF e-mail outage, I'm not if anyone will actually receive this.]

Please vote on releasing the following candidate as Apache Spark version 1.0.0!
This has only minor changes on top of rc7.

The tag to be voted on is v1.0.0-rc8 (commit 80eea0f):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=80eea0f111c06260ffaa780d2f3f7facd09c17bc

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc8/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1016/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Monday, May 19, at 10:15 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 15 May 2014 20:33:40 -0700",Re: mllib vector templates,dev@spark.apache.org,"I submitted a PR for standardizing the text format for vectors and
labeled data: https://github.com/apache/spark/pull/685

I didn't choose LibSVM as the default format because two reasons:

1) It doesn't contain feature dimension info in the record. We need to
scan the dataset to get that info.
2) It saves index:value tuples. Putting indices together can help data
compression. Same for value if there are many binary features.

Best,
Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 15 May 2014 20:34:29 -0700",Re: mllib vector templates,dev@spark.apache.org,"3) It is not designed for dense feature vectors.


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 16 May 2014 08:39:53 -0700",Re: (test),dev@spark.apache.org,"Yes.



"
DB Tsai <dbtsai@stanford.edu>,"Fri, 16 May 2014 08:40:29 -0700",Re: (test),dev@spark.apache.org,"Yes.

"
GlennStrycker <glenn.strycker@gmail.com>,"Fri, 16 May 2014 09:41:34 -0700 (PDT)",Scala examples for Spark do not work as written in documentation,dev@spark.incubator.apache.org,"written as

val count = spark.parallelize(1 to NUM_SAMPLES).map(i =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
).reduce(_ + _)
println(""Pi is roughly "" + 4.0 * count / NUM_SAMPLES)

This does not execute in Spark, which gives me an error:
<console>:2: error: illegal start of simple expression
         val x = Math.random()
         ^

If I rewrite the query slightly, adding in {}, it works:

val count = spark.parallelize(1 to 10000).map(i =>
   {
   val x = Math.random()
   val y = Math.random()
   if (x*x + y*y < 1) 1 else 0
   }
).reduce(_ + _)
println(""Pi is roughly "" + 4.0 * count / 10000.0)





--

"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 15 May 2014 22:48:16 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc6),dev@spark.apache.org,"So was rc5 cancelled ? Did not see a note indicating that or why ... [1]

- Mridul


[1] could have easily missed it in the email storm though !


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 16 May 2014 01:05:31 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","HI Sandy,

Just curious if the Vote is for rc5 or rc6? Gmail shows me that you
replied to the rc5 thread.

Thanks,

- Henry


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 15 May 2014 20:34:29 -0700",Re: mllib vector templates,dev@spark.apache.org,"3) It is not designed for dense feature vectors.


"
Andrew Or <andrew@databricks.com>,"Thu, 15 May 2014 10:34:44 -0700",(test),"""dev@spark.apache.org"" <dev@spark.apache.org>","Apache has been having some problems lately. Do you guys see this message?
"
Aaron Davidson <ilikerps@gmail.com>,"Fri, 16 May 2014 11:37:36 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc7),"dev@spark.apache.org, Henry Saputra <henry.saputra@gmail.com>","It was, but due to the apache infra issues, some may not have received the
email yet...


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 16 May 2014 10:48:31 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc7),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Patrick,

Just want to make sure that VOTE for rc6 also cancelled?


Thanks,

Henry


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 16 May 2014 12:38:02 -0700",Re: Scala examples for Spark do not work as written in documentation,dev@spark.apache.org,"
val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)




"
Nishkam Ravi <nravi@cloudera.com>,"Fri, 16 May 2014 10:38:03 -0700",Re: (test),dev@spark.apache.org,"Yes.



"
Tom Graves <tgraves_cs@yahoo.com>,"Fri, 16 May 2014 08:39:21 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (rc6),"""dev@spark.apache.org"" <dev@spark.apache.org>","Yes, rc5 and rc6 were cancelled. There is now an rc7.   Unfortunately the Apache mailing list issue has caused lots of emails not to come through.

Here is the details (hopefully it goes through):

Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This patch has minor documentation changes and fixes on top of rc6.

The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc7/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1015

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior


Toming that or why ... [1]

- Mridul


[1] could have easily missed ollowing candidate as Apache Spark version
 1.0.0!
>
> This patch has a few minor fixes on top of rc5. I've also built the
> binary artifacts with Hive support enabled so people can test this
> configuration. When we release 1.0 we might just release both vanilla
> and Hive-enabled binaries.
>
> The tag to be voted on is v1.0.0-rc6 (commit 54133a):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=54133abdce0246f6643a1112a5204afb2c4caa82
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc6/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachestratos-1011
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc6-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Saturday, May 17, at 20:58 UTC and passes if
> amajority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
>
 possible.
>
> changes to ML vector specification:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> changes to the streaming API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> changes to the GraphX API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior"
Reynold Xin <rxin@databricks.com>,"Fri, 16 May 2014 11:58:01 -0700",Re: Scala examples for Spark do not work as written in documentation,dev@spark.apache.org,"Thanks for pointing it out. We should update the website to fix the code.

val count = spark.parallelize(1 to NUM_SAMPLES).map { i =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)
println(""Pi is roughly "" + 4.0 * count / NUM_SAMPLES)




"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 16 May 2014 13:10:15 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc7),dev@spark.apache.org,"Sorry for the duplication, but I think this is the current VOTE candidate
-- we're not voting on rc8 yet?

+1, but just barely.  We've got quite a number of outstanding bugs
identified, and many of them have fixes in progress.  I'd hate to see those
efforts get lost in a post-1.0.0 flood of new features targeted at 1.1.0 --
in other words, I'd like to see 1.0.1 retain a high priority relative to
1.1.0.

Looking through the unresolved JIRAs, it doesn't look like any of the
identified bugs are show-stoppers or strictly regressions (although I will
note that one that I have in progress, SPARK-1749, is a bug that we
introduced with recent work -- it's not strictly a regression because we
had equally bad but different behavior when the DAGScheduler exceptions
weren't previously being handled at all vs. being slightly mis-handled
now), so I'm not currently seeing a reason not to release.



"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 16 May 2014 12:36:38 -0700",Re: Scala examples for Spark do not work as written in documentation,dev@spark.apache.org,"Actually, the better way to write the multi-line closure would be:

val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)



"
GlennStrycker <glenn.strycker@gmail.com>,"Fri, 16 May 2014 13:28:21 -0700 (PDT)","reduce only removes duplicates, cannot be arbitrary function",dev@spark.incubator.apache.org,"I am attempting to write a mapreduce job on a graph object to take an edge
list and return a new edge list.  Unfortunately I find that the current
function is

def reduce(f: (T, T) => T): T

not

def reduce(f: (T1, T2) => T3): T


I see this because the following 2 commands give different results for the
final number, which should be the same (tempMappedRDD is a MappedRDD of the
form (Edge,1), and I found the the A and B here are (1,4) and (7,3) )

tempMappedRDD.reduce( (A,B) => (Edge(A._1.srcId, A._1.dstId,
A._1.dstId.toInt), 1) )  // (Edge(1,4,4),1)
tempMappedRDD.reduce( (A,B) => (Edge(A._1.srcId, B._1.dstId,
A._1.dstId.toInt), 1) )  // (Edge(1,3,3),1)

why is the 3rd digit above a '3' in the second line, and not a '4'?  Does it
have something to do with toInt?

the really weird thing is that it is only for A, since the following
commands work correctly:

tempMappedRDD.reduce( (A,B) => (Edge(B._1.srcId, B._1.dstId,
B._1.dstId.toInt), 1) )  // (Edge(7,3,3),1)
tempMappedRDD.reduce( (A,B) => (Edge(B._1.srcId, A._1.dstId,
B._1.dstId.toInt), 1) )  // (Edge(7,4,3),1)




--

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 15 May 2014 10:23:12 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for your feedback. Since it's not a regression, it won't block
the release.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 15 May 2014 01:14:26 -0700",[VOTE] Release Apache Spark 1.0.0 (rc7),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This patch has minor documentation changes and fixes on top of rc6.

The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc7/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1015

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

"
GlennStrycker <glenn.strycker@gmail.com>,"Fri, 16 May 2014 13:33:25 -0700 (PDT)","Re: Scala examples for Spark do not work as written in
 documentation",dev@spark.incubator.apache.org,"Why does the reduce function only work on sums of keys of the same type and
does not support other functional forms?

I am having trouble in another example where instead of 1s and 0s, the
output of the map function is something like A=(1,2) and B=(3,4).  I need a
reduce function that can return something complicated based on reduce( (A,B)
=> (arbitrary fcn1 of A and B, arbitrary fcn2 of A and B) ), but I am only
getting reduce( (A,B) => (arbitrary fcn1 of A, arbitrary fcn2 of A) ).

See
http://apache-spark-developers-list.1001551.n3.nabble.com/reduce-only-removes-duplicates-cannot-be-arbitrary-function-td6606.html




--

"
DB Tsai <dbtsai@stanford.edu>,"Fri, 16 May 2014 01:46:34 -0700",Calling external classes added by sc.addJar needs to be through reflection,"user@spark.apache.org, dev@spark.apache.org, 
	Xiangrui Meng <mengxr@gmail.com>","Finally find a way out of the ClassLoader maze! It took me some times to
understand how it works; I think it worths to document it in a separated
thread.

We're trying to add external utility.jar which contains CSVRecordParser,
and we added the jar to executors through sc.addJar APIs.

If the instance of CSVRecordParser is created without reflection, it
raises *ClassNotFound
Exception*.

data.mapPartitions(lines => {
    val csvParser = new CSVRecordParser((delimiter.charAt(0))
    lines.foreach(line => {
      val lineElems = csvParser.parseLine(line)
    })
    ...
    ...
 )


If the instance of CSVRecordParser is created through reflection, it works.

data.mapPartitions(lines => {
    val loader = Thread.currentThread.getContextClassLoader
    val CSVRecordParser =
        loader.loadClass(""com.alpine.hadoop.ext.CSVRecordParser"")

    val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
        .newInstance(delimiter.charAt(0).asInstanceOf[Character])

    val parseLine = CSVRecordParser
        .getDeclaredMethod(""parseLine"", classOf[String])

    lines.foreach(line => {
       val lineElems = parseLine.invoke(csvParser,
line).asInstanceOf[Array[String]]
    })
    ...
    ...
 )


This is identical to this question,
http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection

It's not intuitive for users to load external classes through reflection,
but couple available solutions including 1) messing around
systemClassLoader by calling systemClassLoader.addURI through reflection or
2) forking another JVM to add jars into classpath before bootstrap loader
are very tricky.

Any thought on fixing it properly?

@Xiangrui,
netlib-java jniloader is loaded from netlib-java through reflection, so
this problem will not be seen.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 15 May 2014 01:14:55 -0700",[RESULT][VOTE] Release Apache Spark 1.0.0 (rc6),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of rc7.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 15 May 2014 10:25:28 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Everyone,

Just a heads up - I've sent other release candidates to the list, but
they appear to be getting swallowed (i.e. they are not on nabble). I
think there is an issue with Apache mail servers.

I'm going to keep trying... if you get duplicate e-mails I apologize in advance.


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 15 May 2014 20:33:40 -0700",Re: mllib vector templates,dev@spark.apache.org,"I submitted a PR for standardizing the text format for vectors and
labeled data: https://github.com/apache/spark/pull/685

I didn't choose LibSVM as the default format because two reasons:

1) It doesn't contain feature dimension info in the record. We need to
scan the dataset to get that info.
2) It saves index:value tuples. Putting indices together can help data
compression. Same for value if there are many binary features.

Best,
Xiangrui


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 15 May 2014 01:15:25 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc7),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'll start the voting with a +1.


"
Reynold Xin <rxin@databricks.com>,"Fri, 16 May 2014 11:45:47 -0700",Re: (test),dev@spark.apache.org,"I didn't see the original message, but only a reply.



"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 16 May 2014 11:42:20 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc7),Aaron Davidson <ilikerps@gmail.com>,"Ah ok, thanks Aaron

Just to make sure we VOTE the right RC.

Thanks,

Henry


"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 17 May 2014 04:30:17 +0530",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"Effectively this is persist without fault tolerance.
Failure of any node means complete lack of fault tolerance.
I would be very skeptical of truncating lineage if it is not reliable.

"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 16 May 2014 12:50:00 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"+1, but just barely.  We've got quite a number of outstanding bugs
identified, and many of them have fixes in progress.  I'd hate to see those
efforts get lost in a post-1.0.0 flood of new features targeted at 1.1.0 --
in other words, I'd like to see 1.0."
Patrick Wendell <pwendell@gmail.com>,"Fri, 16 May 2014 15:46:47 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc7),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

My vote threads seem to be running about 24 hours behind and/or
getting swallowed by infra e-mail.

I sent RC8 yesterday and we might send one tonight as well. I'll make
sure to close all existing ones

There have been only small ""polish"" changes in the recent RC's since
RC5. So testing any off these should be pretty equivalent. I'll make
sure I close all the other threads by tonight.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 16 May 2014 15:57:21 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc8),dev@spark.apache.org,1
Michael Armbrust <michael@databricks.com>,"Fri, 16 May 2014 18:22:13 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc8),dev@spark.apache.org,"-1

We found a regression in the way configuration is passed to executors.

https://issues.apache.org/jira/browse/SPARK-1864
https://github.com/apache/spark/pull/808

Michael



"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 16 May 2014 19:22:34 -0700",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"With 3x replication, we should be able to achieve fault tolerance.
This checkPointed RDD can be cleared if we have another in-memory
checkPointed RDD down the line. It can avoid hitting disk if we have
enough memory to use. We need to investigate more to find a good
solution. -Xiangrui


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 17 May 2014 00:51:22 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc8),Michael Armbrust <michael@databricks.com>,"Due to the issue discovered by Michael, this vote is cancelled in favor of rc9.


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 17 May 2014 00:58:38 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'll start the voting with a +1.


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 17 May 2014 00:58:09 -0700",[VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.0.0!
This has one bug fix and one minor feature on top of rc8:
SPARK-1864: https://github.com/apache/spark/pull/808
SPARK-1808: https://github.com/apache/spark/pull/799

The tag to be voted on is v1.0.0-rc9 (commit 920f947):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc9/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1017/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
amajority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 17 May 2014 00:51:46 -0700",[RESULT] [VOTE] Release Apache Spark 1.0.0 (rc8),"""dev@spark.apache.org"" <dev@spark.apache.org>","Cancelled in favor of rc9.


"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 17 May 2014 13:59:49 +0530",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Can you try moving your mapPartitions to another class/object which is
referenced only after sc.addJar ?

I would suspect CNFEx is coming while loading the class containing
mapPartitions before addJars is executed.

In general though, dynamic loading of classes means you use reflection to
instantiate it since expectation is you don't know which implementation
provides the interface ... If you statically know it apriori, you bundle it
in your classpath.

Regards
Mridul

"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 17 May 2014 14:06:53 +0530",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"We don't have 3x replication in spark :-)
And if we use replicated storagelevel, while decreasing odds of failure, it
does not eliminate it (since we are not doing a great job with replication
anyway from fault tolerance point of view).
Also it does take a nontrivial performance hit with replicated levels.

Regards,
Mridul

"
Sean Owen <sowen@cloudera.com>,"Sat, 17 May 2014 10:18:15 +0100",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
Releases happen in local minima of change, usually created by
internally enforced code freeze. Spark is incredibly busy now due to
external factors -- recently a TLP, recently discovered by a large new
audience, ease of contribution enabled by Github. It's getting like
the first year of mainstream battle-testing in a month. It's been very
hard to freeze anything! I see a number of non-trivial issues being
reported, and I don't think it has been possible to triage all of
them, even.

Given the high rate of change, my instinct would have been to release
0.10.0 now. But won't it always be very busy? I do think the rate of
significant issues will slow down.

Version ain't nothing but a number, but if it has any meaning it's the
semantic versioning meaning. 1.0 imposes extra handicaps around
striving to maintain backwards-compatibility. That may end up being
bent to fit in important changes that are going to be required in this
continuing period of change. Hadoop does this all the time
unfortunately and gets away with it, I suppose -- minor version
quite production-ready.)

Just consider this a second vote for focus on fixes and 1.0.x rather
than new features and 1.x. I think there are a few steps that could
streamline triage of this flood of contributions, and make all of this
easier, but that's for another thread.



"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 17 May 2014 16:56:49 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"I had echoed similar sentiments a while back when there was a discussion
around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
changes, add missing functionality, go through a hardening release before
1.0

But the community preferred a 1.0 :-)

Regards,
Mridul

those
1.1.0 --
will
"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 17 May 2014 17:11:39 +0530","Re: [jira] [Created] (SPARK-1867) Spark Documentation Error causes
 java.lang.IllegalStateException: unread block data",dev@spark.apache.org,"I suspect this is an issue we have fixed internally here as part of a
larger change - the issue we fixed was not a config issue but bugs in spark.

Unfortunately we plan to contribute this as part of 1.1

Regards,
Mridul

"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 17 May 2014 08:52:39 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"Which of the unresolved bugs in spark-core do you think will require an
API-breaking change to fix?  If there are none of those, then we are still
essentially on track for a 1.0.0 release.

The number of contributions and pace of change now is quite high, but I
don't think that waiting for the pace to slow before releasing 1.0 is
viable.  If Spark's short history is any guide to its near future, the pace
will not slow by any significant amount for any noteworthy length of time,
but rather will continue to increase.  What we need to be aiming for, I
think, is to have the great majority of those new contributions being made
to MLLlib, GraphX, SparkSQL and other areas of the code that we have
clearly marked as not frozen in 1.x. I think we are already seeing that,
but if I am just not recognizing breakage of our semantic versioning
guarantee that will be forced on us by some pending changes, now would be a
good time to set me straight.



"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 17 May 2014 08:53:33 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),dev@spark.apache.org,1
Andrew Ash <andrew@andrewash.com>,"Sat, 17 May 2014 08:44:50 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"+1 on the next release feeling more like a 0.10 than a 1.0

"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 17 May 2014 22:14:57 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"We made incompatible api changes whose impact we don't know yet completely
: both from implementation and usage point of view.

We had the option of getting real-world feedback from the user community if
we had gone to 0.10 but the spark developers seemed to be in a hurry to get
to 1.0 - so I made my opinion known but left it to the wisdom of larger
group of committers to decide ... I did not think it was critical enough to
do a binding -1 on.

Regards
Mridul

"
Sean Owen <sowen@cloudera.com>,"Sat, 17 May 2014 17:38:36 +0100",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
I don't have a particular one in mind, but look at
https://issues.apache.org/jira/browse/SPARK-1817?filter=12327229 for
example. There are 10 issues marked blocker or critical, that are
targeted at Core / 1.0.0 (or unset). Many are probably not critical,
not for 1.0, or wouldn't require a big change to fix. But has this
been reviewed then -- can you tell? I'd be happy for someone to tell
me to stop worrying, yeah, there's nothing too big here.



I think we'd agree core is the most important part. I'd humbly suggest
fixes and improvements to core remain exceptionally important after
1.0 and there is a long line of proposed changes, most good. Would be
great to really burn that down. Maybe that is the kind of thing I
personally would have preferred to see before a 1.0, but it's not up
to me and there are other factors at work here. I don't object
strongly or anything.

"
Andrew Or <andrew@databricks.com>,"Sat, 17 May 2014 10:08:13 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1


2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:

"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 17 May 2014 10:35:42 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"That is a past issue that we don't need to be re-opening now.  The present
issue, and what I am asking, is which pending bug fixes does anyone
anticipate will require breaking the public API guaranteed in rc9?



"
Kan Zhang <kzhang@apache.org>,"Sat, 17 May 2014 10:32:25 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"+1 on the running commentary here, non-binding of course :-)



"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 18 May 2014 00:29:25 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
Huh ? If we need to revisit based on changed circumstances, we must - the
scope of changes introduced in this release was definitely not anticipated
when 1.0 vs 0.10 discussion happened.

If folks are worried about stability of core; it is a valid concern IMO.

Having said that, I am still ok with going to 1.0; but if a conversation
starts about need for 1.0 vs going to 0.10 I want to hear more and possibly
allay the concerns and not try to muzzle the discussion.


Regards
Mridul

completely
community if
get
enough to
an
I
I
that,
api
to
like
being
release
of
being
rather
could
bugs
to
at
of
that we
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 17 May 2014 11:32:02 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

Reran my tests from rc5:

* Built the release from source.
* Compiled Java and Scala apps that interact with HDFS against it.
* Ran them in local mode.
* Ran them against a pseudo-distributed YARN cluster in both yarn-client
mode and yarn-cluster mode"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 17 May 2014 12:43:04 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"I'm not trying to muzzle the discussion.  All I am saying is that we don't
need to have the same discussion about 0.10 vs. 1.0 that we already had.
 If you can tell me about specific changes in the current release candidate
that occasion new arguments for why a 1.0 release is an unacceptable idea,
then I'm listening.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 17 May 2014 12:50:01 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"As others have said, the 1.0 milestone is about API stability, not about saying “we’ve eliminated all bugs”. The sooner you declare 1.0, the sooner users can confidently build on Spark, knowing that the application they build today will still run on Spark 1.9.9 three years from now. This is something that I’ve seen done badly (and experienced the effects thereof) in other big data projects, such as MapReduce and even YARN. The result is that you annoy users, you end up with a fragmented userbase where everyone is building against a different version, and you drastically slow down development.

With a project as fast-growing as fast-growing as Spark in particular, there will be new bugs discovered and reported continuously, especially in the non-core components. Look at the graph of # of contributors in time to Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph; “commits” changed when we started merging each patch as a single commit). This is not slowing down, and we need to have the culture now that we treat API stability and release numbers at the level expected for a 1.0 project instead of having people come in and randomly change the API.

I’ll also note that the issues marked “blocker” were marked so by their reporters, since the reporter can set the priority. I don’t consider stuff like parallelize() not partitioning ranges in the same way as other collections a blocker — it’s a bug, it would be good to fix it, but it only affects a small number of use cases. Of course if we find a real blocker (in particular a regression from a previous version, or a feature that’s just completely broken), we will delay the release for that, but at some point you have to say “okay, this fix will go into the next maintenance release”. Maybe we need to write a clear policy for what the issue priorities mean.

Finally, I believe it’s much better to have a culture where you can make releases on a regular schedule, and have the option to make a maintenance release in 3-4 days if you find new bugs, than one where you pile up stuff into each release. This is what much large project than us, like Linux, do, and it’s the only way to avoid indefinite stalling with a large contributor base. In the worst case, if you find a new bug that warrants immediate release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1 in three days with just your bug fix in it). And if you find an API that you’d like to improve, just add a new one and maybe deprecate the old one — at some point we have to respect our users and let them know that code they write today will still run tomorrow.

Matei


discussion
api
before
to
new
very
release
of
the
this
and
rather
this
<mark@clearstorydata.com
see
relative
the
(although I
we
because

"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 18 May 2014 02:10:40 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
Agreed, no point in repeating the same discussion ... I am also trying to
understand what the concerns are.

Specifically though, the scope of 1.0 (in terms of changes) went up quite a
bit - a lot of which are new changes and features; not just the initially
envisioned api changes and stability fixes.

If this is raising concerns, particularly since lot of users are depending
on stability of spark interfaces (api, env, scripts, behavior); I want to
understand better what they are - and if they are legitimately serious
enough, we will need to revisit decision to go to 1.0 instead of 0.10 ...
I hope we don't need to though given how late we are in dev cycle

Regards
Mridul

candidate
the
anticipated
possibly
hurry to
larger
require
are
but
1.0 is
of
for,
being
have
versioning
the
release
due
been
all of
rate
around
required in
version
all of
outstanding
hate
targeted
any
regression
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 17 May 2014 14:20:35 -0700",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"We do actually have replicated StorageLevels in Spark. You can use MEMORY_AND_DISK_2 or construct your own StorageLevel with your own custom replication factor.

BTW you guys should probably have this discussion on the JIRA rather than the dev list; I think the replies somehow ended up on the dev list.

Matei


failure, it
replication
<mridul@gmail.com>
reliable.
checkpointing
https://issues.apache.org/jira/browse/SPARK-1855
BlockRDD
that


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 17 May 2014 14:22:01 -0700",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"BTW for what it’s worth I agree this is a good option to add, the only tricky thing will be making sure the checkpoint blocks are not garbage-collected by the block store. I don’t think they will be though.

Matei

MEMORY_AND_DISK_2 or construct your own StorageLevel with your own custom replication factor.
than the dev list; I think the replies somehow ended up on the dev list.
failure, it
replication
levels.
<mridul@gmail.com>
reliable.
checkpointing
https://issues.apache.org/jira/browse/SPARK-1855
BlockRDD
that


"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 18 May 2014 02:35:39 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"I would make the case for interface stability not just api stability.
Particularly given that we have significantly changed some of our
interfaces, I want to ensure developers/users are not seeing red flags.

Bugs and code stability can be addressed in minor releases if found, but
behavioral change and/or interface changes would be a much more invasive
issue for our users.

Regards
Mridul

u declare 1.0, the sooner
thereof)
s
e
n
o
commitsâ€
ot
ere marked so by their
der stuff
ix it, but it only
€™s
enance
e
an make
f
o,
ontributor
â€™d
€” at
pi
ew
ry
e
he
is
nd
is
ee
ve
he
 I
se
"
Michael Malak <michaelmalak@yahoo.com>,"Sat, 17 May 2014 16:08:33 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","While developers may appreciate ""1.0 == API stability,"" I'm not sure that will be the understanding of the VP who gives the green light to a Spark-based development effort.

I fear a bug that silently produces erroneous results will be perceived like the FDIV bug, but in this case without the momentum of an existing large installed base and with a number of ""competitors"" (GridGain, H20, Stratosphere). Despite the stated intention of API stability, the perception (which becomes the reality) of ""1.0"" is that it's ready for production use -- not bullet-proof, but also not with known silent generation of erroneous results. Exceptions and crashes are much more tolerated than silent corruption of data. The result may be a reputation of the Spark team unconcerned about data integrity.

I ran into (and submitted)Â https://issues.apache.org/jira/browse/SPARK-1817Â due to the lack of zipWithIndex(). zip() with a self-created partitioned range was the way I was trying to number with IDs a collection of nodes in preparation for the GraphX constructor. For the record, it was a frequent Spark committer who escalated it to ""blocker""; I did not submit it as such. Partitioning a Scala range isn't just a toy example; it has a real-life use.

I also wonder about the REPL. Cloudera, for example, touts it as key to making Spark a ""crossover tool"" that Data Scientists can also use. The REPL can be considered an API of sorts -- not a traditional Scala or Java API, of course, but the ""API"" that a human data analyst would use. With the Scala REPL exhibiting some of the same bad behaviors as the Spark REPL, there is a question of whether the Spark REPL can even be fixed. If the Spark REPL has to be eliminated after 1.0 due to an inability to repair it, that would constitu1.0 milestone is about API stability, not about saying â€œweâ€™ve eliminated all bugsâ€. The sooner you declare 1.0, the sooner users can confidently build on Spark, knowing that the application they build today will still run on Spark 1.9.9 three years from now. This is something that Iâ€™ve seen done badly (and experienced the effects thereof) in other big data projects, such as MapReduce and even YARN. The result is that you annoy users, you end up with a fragmented userbase where everyone is building against a different version, and you drastically slow down development.

With a project as fast-growing as fast-growing as Spark in particular, there will be new bugs discovered and reported continuously, especially in the non-core components. Look at the graph of # of contributors in time to Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph; â€œcommitsâ€ changed when we started merging each patch as a single commit). This is not slowing down, and we need to have the culture now that we treat API stability and release numbers at the level expected for a 1.0 project instead of having people come in and randomly change the API.

Iâ€™ll also note that the issues marked â€œblockerâ€ were marked so by their reporters, since the reporter can set the priority. I donâ€™t consider stuff like parallelize() not partitioning ranges in the same way as other collections a blocker â€” itâ€™s a bug, it would be good to fix it, but it only affects a small number of use cases. Of course if we find a real blocker (in particular a regression from a previous version, or a feature thatâ€™s just completely broken), we will delay the release for that, but at some point you have to say â€œokay, this fix will go into the next maintenance releaseâ€. Maybe we need to write a clear policy for what the issue priorities mean.

Finally, I believe itâ€™s much better to have a culture where you can make releases on a regular schedule, and have the option to make a maintenance release in 3-4 days if you find new bugs, than one where you pile up stuff into each release. This is what much large project than us, like Linux, do, and itâ€™s the only way to avoid indefinite stalling with a large contributor base. In the worst case, if you find a new bug that warrants immediate release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1 in three days with just your bug fix in it). And if you find an API that youâ€™d like to improve, just add a new one and maybe deprecate the old one â€” at some point we have to respect our users and let them know that code they write today will still run tomorrow.

Matei

> +1 on the running commentary here, non-binding of course :-)
> 
> te:
> 
>> +1 on the next release feeling more like a 0.10 than a 1.0

>> 
>>> I had echoed similar sentiments a while back when there was a discussion
>>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
>>> changes, add missing functionality, go through a hardening release before
>>> 1.0
>>> 
>>> But the community preferred a 1.0binding commentary:
>>>> 
>>>> Releases happen in local minima of change, usually created by
>>>> internally enforced code freeze. Spark is incredibly busy now due to
>>>> external factors -- recently a TLP, recently discovered by a large new
>>>> audience, ease of contribution enabled by Github. It's getting like
>>>> the first year of mainstream battle-testing in a month. It's been very
>>>> hard to freeze anything! I see a number of non-trivial issues being
>>>> reported, and I don't think it has been possible to triage all of
>>>> them, even.
>>>> 
>>>> Given the high rate of change, my instinct would have been to release
>>>> 0.10.0 now. But won't it always be very busy? I do think the rate of
>>>> significant issues will slow down.
>>>> 
>>>> Version ain't nothing but a number, but if it has any meaning it's the
>>>> semantic versioning meaning. 1.0 imposes extra handicaps around
>>>> striving to maintain backwards-compatibility. That may end up being
>>>> bent to fit in important changes that are going to be required in this
>>>> continuing period of change. Hadoop does this all the time
>>>> unfortunately and gets away with it, I suppe, HBase is at 0.98 and
>>>> quite production-ready.)
>>>> 
>>>> Just consider this a second vote for focus on fixes and 1.0.x rather
>>>> than new features and 1.x. I think there are a few steps that could
>>>> streamline triage of this flood of contributions, and make all of this
>>>> 6, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com
>>> 
>>> wrote:
>>>>> +1, but just barely.Â  We've got quite a number of outstanding bugs
>>>>> identified, and many of them have fixes in progress.Â  I'd hate to see
>>> those
>>>>> efforts get lost in a post-1.0.0 flood of new features targeted at
>>> 1.1.0 --
>>>>> in other words, I'd like to see 1.0.1 retain a high priority relative
>>> to
>>>>> 1.1.0.
>>>of the
>>>>> identified bugs are show-stoppers or strictly regressions (although I
>>> will
>>>>> note that one that I have in progress, SPARK-1749, is a bug that we
>>>>> introduced with recent work -- it's not strictly a regression because
>>> we
>>>>> had equally bad but different behavior when the DAGScheduler
>> exceptions
>>>>> weren't previously being handled at all vs. being slightly
>> mis-handled
>>>>> now), so I'm not currently seeing a reason not to release.
>>> 
>> "
Mark Hamstra <mark@clearstorydata.com>,"Sat, 17 May 2014 16:02:17 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"I don't understand.  We never said that interfaces wouldn't change from 0.9
to 1.0.  What we are committing to is stability going forward from the
1.0.0 baseline.  Nobody is disputing that backward-incompatible behavior or
interface changes would be an issue post-1.0.0.  The question is whether
there is anything apparent now that is expected to require such disruptive
changes if we were to commit to the current release candidate as our
guaranteed 1.0.0 baseline.


e:

t
you declare 1.0, the
s thereof)
 were marked so by their
sider
 fix it, but it
r
â€™s
e
ntenance
sue
 can make
ce
€” at
y
to
e
g
of
g
er
d
s
t
we
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 17 May 2014 16:11:47 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"Yup, this is a good point, the interface includes stuff like launch scripts and environment variables. However I do think that the current features of spark-submit can all be supported in future releases. We’ll definitely have a very strict standard for modifying these later on.

Matei


flags.
but
invasive
about
the sooner
they
is
thereof)
result is
everyone
down
particular,
especially in
time to
“commits”
is not
by their
consider stuff
but it only
blocker
that’s
some
maintenance
make
maintenance
stuff
Linux, do,
contributor
immediate
in
you’d
 at
they
the api
to
large new
like
very
being
release
of
it's the
being
this
0.98 and
rather
could
this
bugs
to see
at
relative
of the
(although I
we
because


"
Andy Konwinski <andykonwinski@gmail.com>,"Sat, 17 May 2014 18:27:29 -0700",Re: can RDD be shared across mutil spark applications?,dev@spark.apache.org,"RDDs cannot currently be shared across multiple SparkContexts without using
something like the Tachyon project (which is a separate project/codebase).

Andy

"
Christopher Nguyen <ctn@adatao.com>,"Sun, 18 May 2014 12:02:03 +0800",Re: can RDD be shared across mutil spark applications?,dev@spark.apache.org,"Qing Yang, Andy is correct in answering your direct question.

At the same time, depending on your context, you may be able to apply a
pattern where you turn the single Spark application into a service, and
multiple clients if that service can indeed share access to the same RDDs.

Several groups have built apps based on this pattern, and we will also show
something with this behavior at the upcoming Spark Summit (multiple users
collaborating on named DDFs with the same underlying RDDs).

Sent while mobile. Pls excuse typos etc.

"
Liquan Pei <liquanpei@gmail.com>,"Sat, 17 May 2014 22:28:51 -0700",Matrix Multiplication of two RDD[Array[Double]]'s,dev@spark.apache.org,"Hi

I am currently implementing an algorithm involving matrix multiplication.
Basically, I have matrices represented as RDD[Array[Double]]. For example,
If I have A:RDD[Array[Double]] and B:RDD[Array[Double]] and what would be
the most efficient way to get C = A * B

Both A and B are large, so it would not be possible to save either of them
in memory.

Thanks a lot for your help!

Liquan
"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 18 May 2014 12:11:11 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"0.9

Agreed.

or

The point is, how confident are we that these are the right set of
interface definitions.
We think it is, but we could also have gone through a 0.10 to vet the
proposed 1.0 changes to stabilize them.

To give examples for which we don't have solutions currently (which we are
facing internally here btw, so not academic exercise) :

- Current spark shuffle model breaks very badly as number of partitions
increases (input and output).

- As number of nodes increase, the overhead per node keeps going up. Spark
currently is more geared towards large memory machines; when the RAM per
node is modest (8 to 16 gig) but large number of them are available, it
does not do too well.

- Current block abstraction breaks as data per block goes beyond 2 gig.

- Cogroup/join when value per key or number of keys (or both) is high
breaks currently.

- Shuffle consolidation is so badly broken it is not funny.

- Currently there is no way of effectively leveraging accelerator
cards/coprocessors/gpus from spark - to do so, I suspect we will need to
redefine OFF_HEAP.

- Effectively leveraging ssd is still an open question IMO when you have
mix of both available.

We have resolved some of these and looking at the rest. These are not
unique to our internal usage profile, I have seen most of these asked
elsewhere too.

Thankfully some of the 1.0 changes actually are geared towards helping to
alleviate some of the above (Iterable change for ex), most of the rest are
internal impl detail of spark core which helps a lot - but there are cases
where this is not so.

Unfortunately I don't know yet if the unresolved/uninvestigated issues will
require more changes or not.

Given this I am very skeptical of expecting current spark interfaces to be
sufficient for next 1 year (forget 3)

I understand this is an argument which can be made to never release 1.0 :-)
Which is why I was ok with a 1.0 instead of 0.10 release in spite of my
preference.

This is a good problem to have IMO ... People are using spark extensively
and in circumstances that we did not envision : necessitating changes even
to spark core.

But the claim that 1.0 interfaces are stable is not something I buy - they
are not, we will need to break them soon and cost of maintaining backward
compatibility will be high.

We just need to make an informed decision to live with that cost, not hand
wave it away.

Regards
Mridul

e
t
e
:
about
r you declare 1.0, the
they
is
cts
thereof)
result
n
,
especially
time
is
 were marked so by
their
onsider
to fix it, but it
blocker
thatâ€™s
some
maintenance
issue
ou can
make
maintenance
Linux,
ge
e
in
â€”
at
they
the
e
due to
large
like
n
being
f
rate of
it's
being
n
0.98
rather
could
f
bugs
to
at
of
that we
"
Xiangrui Meng <mengxr@gmail.com>,"Sat, 17 May 2014 23:26:10 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
reflection approach mentioned by DB didn't work either. I checked the
distributed cache on a worker node and found the jar there. It is also
in the Environment tab of the WebUI. The workaround is making an
assembly jar.

DB, could you create a JIRA and describe what you have found so far? Thanks!

Best,
Xiangrui


"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 18 May 2014 14:43:59 +0530",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"So I think I need to clarify a few things here - particularly since
this mail went to the wrong mailing list and a much wider audience
than I intended it for :-)


Most of the issues I mentioned are internal implementation detail of
spark core : which means, we can enhance them in future without
disruption to our userbase (ability to support large number of
input/output partitions. Note: this is of order of 100k input and
output partitions with uniform spread of keys - very rarely seen
outside of some crazy jobs).

Some of the issues I mentioned would reqiure DeveloperApi changes -
which are not user exposed : they would impact developer use of these
api's - which are mostly internally provided by spark. (Like fixing
blocks > 2G would require change to Serializer api)

A smaller faction might require interface changes - note, I am
referring specifically to configuration changes (removing/deprecating
some) and possibly newer options to submit/env, etc - I dont envision
any programming api change itself.
The only api change we did was from Seq -> Iterable - which is
actually to address some of the issues I mentioned (join/cogroup).

Remaining are bugs which need to be addressed or the feature
removed/enhanced like shuffle consolidation.

There might be semantic extension of some things like OFF_HEAP storage
level to address other computation models - but that would not have an
impact on end user - since other options would be pluggable with
default set to Tachyon so that there is no user expectation change.


So will the interface possibly change ? Sure though we will try to
keep it backwardly compatible (as we did with 1.0).
Will the api change - other than backward compatible enhancements, probably not.


Regards,
Mridul


ote:
ace
e
k
oes
aks
mix
que
e
s
ll
e
-)
n
y
d
ve
.
ut
ve
e:
er you declare 1.0, the
ects
wn
r,
t
 were marked so by
consider
 to fix it, but it
 issue
you can
rge
te
t
â€”
se
ue
en
of
te
in
of
d
at

"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 18 May 2014 09:58:44 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870

DB, could you add more info to that JIRA? Thanks!

-Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 18 May 2014 09:46:29 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Btw, I tried

rdd.map { i =>
  System.getProperty(""java.class.path"")
}.collect()

but didn't see the jars added via ""--jars"" on the executor classpath.

-Xiangrui


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 18 May 2014 19:34:29 +0200",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"Hi,

I'm curious if it's a common approach to have discussions in JIRA not here.
I don't think it's the ASF way.

Pozdrawiam,
Jacek Laskowski
http://blog.japila.pl
17 maj 2014 23:55 ""Matei Zaharia"" <matei.zaharia@gmail.com> napisaÅ‚(a):

:
,
.
5
"
Andrew Ash <andrew@andrewash.com>,"Sun, 18 May 2014 11:28:47 -0700",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"The nice thing about putting discussion on the Jira is that everything
about the bug is in one place.  So people looking to understand the
discussion a few years from now only have to look on the jira ticket rather
than also search the mailing list archives and hope commenters all put the
string ""SPARK-1855"" into the messages.



e.
a):
om
an
.
ng
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 18 May 2014 11:54:37 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","@xiangrui - we don't expect these to be present on the system
classpath, because they get dynamically added by Spark (e.g. your
application can call sc.addJar well after the JVM's have started).

@db - I'm pretty surprised to see that behavior. It's defin"
Patrick Wendell <pwendell@gmail.com>,"Sun, 18 May 2014 11:58:54 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","@db - it's possible that you aren't including the jar in the classpath
of your driver program (I think this is what mridul was suggesting).
It would be helpful to see the stack trace of the CNFE.

- Patrick


"
Andrew Ash <andrew@andrewash.com>,"Sun, 18 May 2014 12:31:20 -0700",Re: Matrix Multiplication of two RDD[Array[Double]]'s,dev@spark.apache.org,"Hi Liquan,

There is some working being done on implementing linear algebra algorithms
on Spark for use in higher-level machine learning algorithms.  That work is
happening in the MLlib project, which has a
org.apache.spark.mllib.linalgpackage you may find useful.

See
https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/mllib/linalg

MLlib) both the IndexedRowMatrix and RowMatrix implement a multiply
operation:

aash@aash-mbp~/git/spark/mllib/src/main/scala/org/apache/spark/mllib/linalg$
git grep
'def multiply'
distributed/IndexedRowMatrix.scala:  def multiply(B: Matrix):
IndexedRowMatrix = {
distributed/RowMatrix.scala:  def multiply(B: Matrix): RowMatrix = {
aash@aash-mbp~/git/spark/mllib/src/main/scala/org/apache/spark/mllib/linalg$

Can you look into using that code and let us know if it meets your needs?

Thanks!
Andrew



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 18 May 2014 12:33:18 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),dev@spark.apache.org,"I took the always fun task of testing it on Windows, and unfortunately, I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn’t quite match the master-setting behavior of the Unix based one. I’ll send a pull request to fix them soon.

Matei



yarn-client
<pwendell@gmail.com
<pwendell@gmail.com>
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
found
https://repository.apache.org/content/repositories/orgapachespark-1017/
are
associated
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
Seq[String]


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 18 May 2014 13:19:35 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Hi Patrick,

If spark-submit works correctly, user only needs to specify runtime
jars via `--jars` instead of using `sc.addJar`. Is it correct? I
checked SparkSubmit and yarn.Client but didn't find any code to handle
`args.jars` for YARN mode. So I don't know where in the code the jars
in the distributed cache are added to runtime classpath on executors.

Best,
Xiangrui


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 18 May 2014 14:36:38 -0700",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"JIRAs comments are mirrored to the issues@spark.apache.org list, so people who want to get them by email can do so. In theory one should also be able to reply to one of those emails and have the message show up in JIRA, but I donâ€™t think ours is configured that way. Iâ€™m not sure why it wouldnâ€™t be the â€œASF wayâ€ when the JIRA instance is hosted by the ASF and mirrored on ASF lists.

Matei


rather
the
here.
napisaÅ‚(a):
custom
than
levels.
have
checkpointing
fault
checkpoints


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 18 May 2014 23:19:17 +0200",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"
My understanding is that JIRA is not for discussions. In a sense it
could be used for a few opinions, but have never seen it elsewhere and
am curious if it's an approach for the project (that I might accept
ultimately, but that would require some adoption time).

What wrong with linking a discussion thread to a JIRA issue?

Jacek

-- 
Jacek Laskowski | http://blog.japila.pl
""Never discourage anyone who continually makes progress, no matter how
slow."" Plato

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 18 May 2014 16:00:48 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","I spoke with DB offline about this a little while ago and he confirmed that
he was able to access the jar from the driver.

The issue appears to be a general Java issue: you can't directly
instantiate a class from a dynamically loaded jar.

I reproduced it locally outside of Spark with:
---
    URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
File(""myotherjar.jar"").toURI().toURL() }, null);
    Thread.currentThread().setContextClassLoader(urlClassLoader);
    MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
---

I was able to load the class with reflection.




"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 18 May 2014 16:01:17 -0700",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"Ah, maybe it’s just different in other Apache projects. All the ones I’ve participated in have had their design discussions on JIRA. For example take a look at https://issues.apache.org/jira/browse/HDFS-4949. (Most design discussions in Hadoop are also on JIRA).

Hosting it this way is more convenient because most users come in looking at the issue tracker, not at mailing list archives (if only because the issue tracker is much more searchable for issues).

Matei


everything
rather
put the


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 18 May 2014 16:05:19 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Hi Sandy,

It is hard to imagine that a user needs to create an object in that
way. Since the jars are already in distributed cache before the
executor starts, is there any reason we cannot add the locally cached
jars to classpath directly?

Best,
Xiangrui


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 18 May 2014 16:05:33 -0700",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"BTW in Spark the consensus so far was that we’d use the dev@ list for high-level discussions (e.g. change in the development process, major features, proposals of new components, release votes) and keep lower-level issue tracking in JIRA. This is just how the project operated before so it was the easiest way for people to continue.

Matei


I’ve participated in have had their design discussions on JIRA. For example take a look at https://issues.apache.org/jira/browse/HDFS-4949. (Most design discussions in Hadoop are also on JIRA).
looking at the issue tracker, not at mailing list archives (if only because the issue tracker is much more searchable for issues).
everything
rather
put the
and
how


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 18 May 2014 16:49:31 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Xiangrui,

If the jars are placed in the distributed cache and loaded statically, as
the primary app jar is in YARN, then it shouldn't be an issue.  Other jars,
however, including additional jars that are sc.addJar'd and jars specified
with the spark-submit --jars argument, are loaded dynamically by executors
with a URLClassLoader.  These jars aren't next to the executors when they
start - the executors fetch them from the driver's HTTP server.



"
DB Tsai <dbtsai@stanford.edu>,"Sun, 18 May 2014 17:00:57 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"The reflection actually works. But you need to get the loader by `val
loader = Thread.currentThread.getContextClassLoader` which is set by Spark
executor. Our team verified this, and uses it as workaround.



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Sun, 18 May 2014 17:03:30 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"The jars are included in my driver, and I can successfully use them in the
driver. I'm working on a patch, and it's almost working. Will submit a PR
soon.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Sun, 18 May 2014 17:08:55 -0700",Fwd: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Since the additional jars added by sc.addJars are through http server, even
it works, we still want to have a better way due to scalability (imagine
that thousands of workers downloading jars from driver).

If we ignore the fundamental scalability issue, this can be fixed by using
the customClassloader to create a wrapped class, and in this wrapped class,
the classloader is inherited from the customClassloader so that users don't
need to do reflection in the wrapped class. I'm working on this now.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


---------- Forwarded message ----------
From: Sandy Ryza <sandy.ryza@cloudera.com>
Date: Sun, May 18, 2014 at 4:49 PM
Subject: Re: Calling external classes added by sc.addJar needs to be
through reflection
To: ""dev@spark.apache.org"" <dev@spark.apache.org>


Hey Xiangrui,

If the jars are placed in the distributed cache and loaded statically, as
the primary app jar is in YARN, then it shouldn't be an issue.  Other jars,
however, including additional jars that are sc.addJar'd and jars specified
with the spark-submit --jars argument, are loaded dynamically by executors
with a URLClassLoader.  These jars aren't next to the executors when they
start - the executors fetch them from the driver's HTTP server.



containing
reflection,
loader.loadClass(""com.alpine.hadoop.ext.CSVRecordParser"")
.newInstance(delimiter.charAt(0).asInstanceOf[Character])
http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 18 May 2014 17:28:23 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),dev@spark.apache.org,"Alright, I’ve opened https://github.com/apache/spark/pull/819 with the Windows fixes. I also found one other likely bug, https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC. I think this is due to Hadoop 1’s security code depending on a different version of org.apache.commons than Hadoop 2, but it needs investigation. Tom, any thoughts on this?

Matei


unfortunately, I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn’t quite match the master-setting behavior of the Unix based one. I’ll send a pull request to fix them soon.
yarn-client
<pwendell@gmail.com
<pwendell@gmail.com>
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
found
https://repository.apache.org/content/repositories/orgapachespark-1017/
if
are
associated
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
Seq[String]


"
qingyang li <liqingyang1985@gmail.com>,"Mon, 19 May 2014 09:13:37 +0800",Re: can RDD be shared across mutil spark applications?,dev@spark.apache.org,"thanks for sharing,  I am using tachyon to store RDD now.


2014-05-18 12:02 GMT+08:00 Christopher Nguyen <ctn@adatao.com>:

"
Tom Graves <tgraves_cs@yahoo.com>,"Sun, 18 May 2014 19:00:09 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","

Alright, Iâ€™ve opened https://github.com/apache/spark/pull/819 with the Windows fixes. I also found one other likely bug, https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC. I think this is due to Hadoop 1â€™s security code depending on a different version of org.apache.commons than Hadoop 2, but it needook the always fun task of testing it on Windows, and unfortunately, I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesnâ€™t quite match the master-setting behavior of the Unix based one. Iâ€™ll send a pull r * Compiled Java and Scala apps that interact with HDFS against it.
>> * Ran them in local mode.
>> * Ran them against a pseudo-distributed YARN cluster in both yarn-client
>> mode and yarn-cluster mode.
>> 
>> 
>:
>> 
>>> +1
>>> 
>>> 
>>> 2014-05-17 8:53 GMT-07:00 Mark Hamstra May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com
>>>>> wrot, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>
>>>>> wrote:
>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>> version
>>>>> 1.0.0!
>>>>>> This has one bug fix and one minor feature on top of rc8:
>>>>>> SPARK-1864: https://github.com/apache/spark/pull/808
>>>>>> SPARK-1808: https://github.com/apache/spark/pull/799
>>>>>> 
>>>>>> The tag to be voted on is v1.0.0-rc9 (commit 920f947):
>>>>>> 
>>>>> 
>>>> 
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
>>>>found
>>> at:
>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>>>>>> 
>>>>>> Release artifacts are signed with the following key:
>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>> 
>>>>>> The staging repository for this release can be found at:
>>>>>> 
>>>> https://repository.apache.org/content/repositories/orgapachespark-1017/
>>>>>> 
>>>>>> The documentation corresponding to this release can be found at:
>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>>>>>> 
>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>> 
>>>>>> The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
>>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>> 
>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>learn more about Apache Spark, please see
>>>>>> http://spark.apache.org/
>>>>>> 
>>>>>> == API Changes ==
>>>>>> We welcome users to compile Spark applications against 1.0. There are
>>>>>> a few API changes in this release. Here are links to the associated
>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>> possible.
>>>>
>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
>>>>>> 
>>>>>> changes to the Java API:
>>>>>> 
s/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>ming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>> 
>>>>>> changes to the GraphX API:
>>>>>> 
>>>>> 
>>>> 
>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>>> 
>>>>>> coGroup and related functions now return Iterable[T] instead of
>>> Seq[T]
>>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>> 
>>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>>>> ==> Call toSeq on the result to restore old behavior
>>>>> 
>>>> 
>>> 
> "
Patrick Wendell <pwendell@gmail.com>,"Sun, 18 May 2014 19:08:28 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Matei - the issue you found is not related to security. This patch
a few days ago broke builds for Hadoop 1 with YARN support enabled.
The patch directly altered the way we deal with commons-lang
dependency, which is what is at the base of this stack trace.

https://github.com/apache/spark/pull/754

- Patrick

ote:
ndows fixes. I also found one other likely bug, https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC. I think this is due to Hadoop 1's security code depending on a different version of org.apache.commons than Hadoop 2, but it needs investigation. Tom, any thoughts on this?
te:
I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn't quite match the master-setting behavior of the Unix based one. I'll send a pull request to fix them soon.
:
t
te:
m>
20f947eb5a22a679c0c3186cf69ee75f6041c75
7/
re
ml#from-09-to-10
ng-guide.html#upgrading-from-pre-10-versions-of-spark
ramming-guide.html#migration-guide-from-091-or-below-to-1x
ming-guide.html#upgrade-guide-from-spark-091
g]

"
"""=?ISO-8859-1?B?d2l0Z28=?="" <witgo@qq.com>","Mon, 19 May 2014 11:07:55 +0800",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?="" <dev@spark.apache.org>","How to reproduce this bug?


------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Mon, May 19, 2014 10:08 AM
To:  ""dev@spark.apache.org""<dev@spark.apache.org>; 
Cc:  ""Tom Graves""<tgraves_cs@yahoo.com>; 
Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc9)



Hey Matei - the issue you found is not related to security. This patch
a few days ago broke builds for Hadoop 1 with YARN support enabled.
The patch directly altered the way we deal with commons-lang
dependency, which is what is at the base of this stack trace.

https://github.com/apache/spark/pull/754

- Patrick

On Sun, May 18, 2014 at 5:28 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Alright, I've opened https://github.com/apache/spark/pull/819 with the Windows fixes. I also found one other likely bug, https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC. I think this is due to Hadoop 1's security code depending on a different version of org.apache.commons than Hadoop 2, but it needs investigation. Tom, any thoughts on this?
>
> Matei
>
> On May 18, 2014, at 12:33 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
>
>> I took the always fun task of testing it on Windows, and unfortunately, I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn't quite match the master-setting behavior of the Unix based one. I'll send a pull request to fix them soon.
>>
>> Matei
>>
>>
>> On May 17, 2014, at 11:32 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>>
>>> +1
>>>
>>> Reran my tests from rc5:
>>>
>>> * Built the release from source.
>>> * Compiled Java and Scala apps that interact with HDFS against it.
>>> * Ran them in local mode.
>>> * Ran them against a pseudo-distributed YARN cluster in both yarn-client
>>> mode and yarn-cluster mode.
>>>
>>>
>>> On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com> wrote:
>>>
>>>> +1
>>>>
>>>>
>>>> 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:
>>>>
>>>>> +1
>>>>>
>>>>>
>>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com
>>>>>> wrote:
>>>>>
>>>>>> I'll start the voting with a +1.
>>>>>>
>>>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>
>>>>>> wrote:
>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>> version
>>>>>> 1.0.0!
>>>>>>> This has one bug fix and one minor feature on top of rc8:
>>>>>>> SPARK-1864: https://github.com/apache/spark/pull/808
>>>>>>> SPARK-1808: https://github.com/apache/spark/pull/799
>>>>>>>
>>>>>>> The tag to be voted on is v1.0.0-rc9 (commit 920f947):
>>>>>>>
>>>>>>
>>>>>
>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
>>>>>>>
>>>>>>> The release files, including signatures, digests, etc. can be found
>>>> at:
>>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>>>>>>>
>>>>>>> Release artifacts are signed with the following key:
>>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>>>
>>>>>>> The staging repository for this release can be found at:
>>>>>>>
>>>>> https://repository.apache.org/content/repositories/orgapachespark-1017/
>>>>>>>
>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>>>>>>>
>>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>>>
>>>>>>> The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
>>>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>>>
>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>
>>>>>>> To learn more about Apache Spark, please see
>>>>>>> http://spark.apache.org/
>>>>>>>
>>>>>>> == API Changes ==
>>>>>>> We welcome users to compile Spark applications against 1.0. There are
>>>>>>> a few API changes in this release. Here are links to the associated
>>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>>> possible.
>>>>>>>
>>>>>>> changes to ML vector specification:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
>>>>>>>
>>>>>>> changes to the Java API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>>>>
>>>>>>> changes to the streaming API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>>>
>>>>>>> changes to the GraphX API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>>>>
>>>>>>> coGroup and related functions now return Iterable[T] instead of
>>>> Seq[T]
>>>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>>>
>>>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>>>>> ==> Call toSeq on the result to restore old behavior
>>>>>>
>>>>>
>>>>
>>
>"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 19 May 2014 09:45:56 +0530",Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing,dev@spark.apache.org,"My bad ... I was replying via mobile, and I did not realize responses
to JIRA mails were not mirrored to JIRA - unlike PR responses !


Regards,
Mridul


"
Sean Owen <sowen@cloudera.com>,"Mon, 19 May 2014 07:57:20 +0100",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"I might be stating the obvious for everyone, but the issue here is not
reflection or the source of the JAR, but the ClassLoader. The basic
rules are this.

""new Foo"" will use the ClassLoader that defines Foo. This is usually
the ClassLoader that loaded whatever it is that first referenced Foo
and caused it to be loaded -- usually the ClassLoader holding your
other app classes.

ClassLoaders can have a parent-child relationship. ClassLoaders always
look in their parent before themselves.

(Careful then -- in contexts like Hadoop or Tomcat where your app is
loaded in a child ClassLoader, and you reference a class that Hadoop
or Tomcat also has (like a lib class) you will get the container's
version!)

When you load an external JAR it has a separate ClassLoader which does
not necessarily bear any relation to the one containing your app
classes, so yeah it is not generally going to make ""new Foo"" work.

Reflection lets you pick the ClassLoader, yes.

I would not call setContextClassLoader.


"
Andrew Ash <andrew@andrewash.com>,"Mon, 19 May 2014 00:07:24 -0700",TorrentBroadcast aka Cornet?,dev@spark.apache.org,"Hi Spark devs,

Is the algorithm for
TorrentBroadcast<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala>the
same as Cornet from the below paper?

http://www.mosharaf.com/wp-content/uploads/orchestra-sigcomm11.pdf

If so it would be nice to include a link to the paper in the Javadoc for
the class.

Thanks!
Andrew
"
DB Tsai <dbtsai@stanford.edu>,"Mon, 19 May 2014 00:17:17 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Hi Sean,

It's true that the issue here is classloader, and due to the classloader
delegation model, users have to use reflection in the executors to pick up
the classloader in order to use those classes added by sc.addJars APIs.
However, it's very inconvenience for users, and not documented in spark.

I'm working on a patch to solve it by calling the protected method addURL
in URLClassLoader to update the current default classloader, so no
customClassLoader anymore. I wonder if this is an good way to go.

  private def addURL(url: URL, loader: URLClassLoader){
    try {
      val method: Method =
classOf[URLClassLoader].getDeclaredMethod(""addURL"", classOf[URL])
      method.setAccessible(true)
      method.invoke(loader, url)
    }
    catch {
      case t: Throwable => {
        throw new IOException(""Error, could not add URL to system
classloader"")
      }
    }
  }



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Andrew Ash <andrew@andrewash.com>,"Mon, 19 May 2014 00:26:25 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Sounds like the problem is that classloaders always look in their parents
before themselves, and Spark users want executors to pick up classes from
their custom code before the ones in Spark plus its dependencies.

Would a custom classloader that delegates to the parent after first
checking itself fix this up?



"
Sean Owen <sowen@cloudera.com>,"Mon, 19 May 2014 08:29:55 +0100",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"I don't think a customer classloader is necessary.

Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc
all run custom user code that creates new user objects without
reflection. I should go see how that's done. Maybe it's totally valid
to set the thread's context classloader for just this purpose, and I
am not thinking clearly.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 19 May 2014 00:38:49 -0700",Re: TorrentBroadcast aka Cornet?,dev@spark.apache.org,"TorrentBroadcast is actually slightly simpler, but it’s based on that. It has similar performance. I’d like to make it the default in a future version, we just haven’t had a ton of testing with it yet (kind of an oversight in this release unfortunately).

Matei


TorrentBroadcast<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala>the
for


"
Gil Vernik <GILV@il.ibm.com>,"Mon, 19 May 2014 10:58:07 +0300",queston about Spark repositories in GitHub,dev@spark.apache.org,"Hello,

I am new to the Spark community, so I apologize if I ask something 
obvious.
I follow the document about contribution to Spark where it's written that 
I need to fork the https://github.com/apache/spark repository.

I got a little bit confused since  the repository 
https://github.com/apache/spark  contains various branches: brach-0.5 - 
branch-1.0,  master, scala-2.9, streaming

What is the branch with the latest most updated code for the next planned 
release? I guess it's Spark 1.0, so should I work with  branch-1.0 or 
master?
Another question, what is Scala-2.9 and Streaming branches? 

Thanking you in advance,
Gil Vernik.




"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 19 May 2014 01:00:18 -0700",Re: queston about Spark repositories in GitHub,dev@spark.apache.org,"“master” is where development happens, while branch-1.0, branch-0.9, etc are for maintenance releases in those versions. Most likely if you want to contribute you should use master. Some of the other named branches were for big features in the past, but none are actively used now.

Matei


that 
- 
planned 



"
GlennStrycker <glenn.strycker@gmail.com>,"Mon, 19 May 2014 13:09:30 -0700 (PDT)",BUG:  graph.triplets does not return proper values,dev@spark.incubator.apache.org,"graph.triplets does not work -- it returns incorrect results

I have a graph with the following edges:

orig_graph.edges.collect
=  Array(Edge(1,4,1), Edge(1,5,1), Edge(1,7,1), Edge(2,5,1), Edge(2,6,1),
Edge(3,5,1), Edge(3,6,1), Edge(3,7,1), Edge(4,1,1), Edge(5,1,1),
Edge(5,2,1), Edge(5,3,1), Edge(6,2,1), Edge(6,3,1), Edge(7,1,1),
Edge(7,3,1))

When I run triplets.collect, I only get the last edge repeated 16 times:

orig_graph.triplets.collect
= Array(((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1))

I've also tried writing various map steps first before calling the triplet
function, but I get the same results as above.

Similarly, the example on the graphx programming guide page
(http://spark.apache.org/docs/0.9.0/graphx-programming-guide.html) is
incorrect.

val facts: RDD[String] =
  graph.triplets.map(triplet =>
    triplet.srcAttr._1 + "" is the "" + triplet.attr + "" of "" +
triplet.dstAttr._1)

does not work, but

val facts: RDD[String] =
  graph.triplets.map(triplet =>
    triplet.srcAttr + "" is the "" + triplet.attr + "" of "" + triplet.dstAttr)

does work, although the results are meaningless.  For my graph example, I
get the following line repeated 16 times:

1 is the 1 of 1



--

"
Reynold Xin <rxin@databricks.com>,"Mon, 19 May 2014 13:29:37 -0700",Re: BUG: graph.triplets does not return proper values,dev@spark.apache.org,"This was an optimization that reuses a triplet object in GraphX, and when
you do a collect directly on triplets, the same object is returned.

It has been fixed in Spark 1.0 here:
https://issues.apache.org/jira/browse/SPARK-1188

To work around in older version of Spark, you can add a copy step to it,
e.g.

graph.triplets.map(_.copy()).collect()




"
GlennStrycker <glenn.strycker@gmail.com>,"Mon, 19 May 2014 15:16:02 -0700 (PDT)",Re: BUG: graph.triplets does not return proper values,dev@spark.incubator.apache.org,"Thanks, rxin, this worked!

I am having a similar problem with .reduce... do I need to insert .copy()
functions in that statement as well?

This part works:
orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
(Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).collect

=Array((Edge(1,4,1),1), (Edge(1,5,1),1), (Edge(1,7,1),1), (Edge(2,5,1),1),
(Edge(2,6,1),1), (Edge(3,5,1),1), (Edge(3,6,1),1), (Edge(3,7,1),1),
(Edge(4,1,1),1), (Edge(5,1,1),1), (Edge(5,2,1),1), (Edge(5,3,1),1),
(Edge(6,2,1),1), (Edge(6,3,1),1), (Edge(7,1,1),1), (Edge(7,3,1),1))

But when I try adding on a reduce statement, I only get one element, not 16:
orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
(Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).reduce(
(A,B) => { if (A._1.dstId == B._1.srcId) (Edge(A._1.srcId, B._1.dstId, 2),
1) else if (A._1.srcId == B._1.dstId) (Edge(B._1.srcId, A._1.dstId, 2), 1)
else (Edge(0, 0, 0), 0) } )

=(Edge(0,0,0),0)



--

"
Reynold Xin <rxin@databricks.com>,"Mon, 19 May 2014 15:23:52 -0700",Re: BUG: graph.triplets does not return proper values,dev@spark.apache.org,"Yea unfortunately you need that as well. When 1.0 is released, you wouldn't
need to do that anymore.

BTW - you can also just check out the source code from github to build 1.0.
The current branch-1.0 branch is very already at release candidate status -
so it should be almost identical to the actual 1.0 release.

https://github.com/apache/spark/tree/branch-1.0



"
Reynold Xin <rxin@databricks.com>,"Mon, 19 May 2014 15:23:52 -0700",Re: BUG: graph.triplets does not return proper values,dev@spark.apache.org,"Yea unfortunately you need that as well. When 1.0 is released, you wouldn't
need to do that anymore.

BTW - you can also just check out the source code from github to build 1.0.
The current branch-1.0 branch is very already at release candidate status -
so it should be almost identical to the actual 1.0 release.

https://github.com/apache/spark/tree/branch-1.0



"
GlennStrycker <glenn.strycker@gmail.com>,"Mon, 19 May 2014 15:32:40 -0700 (PDT)",Re: BUG: graph.triplets does not return proper values,dev@spark.incubator.apache.org,"I tried adding .copy() everywhere, but still only get one element returned,
not even an RDD object.

orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
(Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).reduce(
(A,B) => { if (A._1.copy().dstId == B._1.copy().srcId)
(Edge(A._1.copy().srcId, B._1.copy().dstId, 2), 1) else if
(A._1.copy().srcId == B._1.copy().dstId) (Edge(B._1.copy().srcId,
A._1.copy().dstId, 2), 1) else (Edge(0, 0, 3), 1) } )

= (Edge(0,0,3),1)

I'll try getting a fresh copy of the Spark 1.0 code and see if I can get it
to work.  Thanks for your help!!



--

"
nit <nitinpanj@gmail.com>,"Mon, 19 May 2014 18:48:41 -0700 (PDT)",spark 1.0 standalone application,dev@spark.incubator.apache.org,"I am not  much comfortable with sbt. I want to build a standalone application
using spark 1.0 RC9. I can build sbt assembly for my application with Spark
0.9.1, and I think in that case spark is pulled from Aka Repository?

Now if I want to use 1.0 RC9 for my application; what is the process ?
(FYI, I was able to build spark-1.0 via sbt/assembly and I can see
sbt-assembly jar; and I think I will have to copy my jar somewhere? and
update build.sbt?)

PS: I am not sure if this is the right place for this question; but since
1.0 is still RC, I felt that this may be appropriate forum.

thank! 



--

"
liuguodong <liuguodong@jd.com>,"Tue, 20 May 2014 09:46:04 +0800","spark and impala , which is more fitter for MPP",user <user@spark.apache.org>,"Hi, ALL
 
       My question is that spark and impala , which is more fitter for MPP .
       The  motivition as below case:
        1.  three big table need make join operation; (about 100 field per table,  more than 1TB per table)
        2.  beside above tables, it is very possible to they need make join operation with n other small or middle table
        3.  for all join operation , they will be random,  meanwhile , it need  a very quick response.

look forward your authoritative help !

Best Regards




liuguodong"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 19 May 2014 22:04:19 -0400",Re: spark 1.0 standalone application,dev@spark.apache.org,"en, you have to put spark-assembly-*.jar to the lib directory of your application 

Best, 

-- 
Nan Zhu





"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 19 May 2014 22:04:19 -0400",Re: spark 1.0 standalone application,dev@spark.apache.org,"en, you have to put spark-assembly-*.jar to the lib directory of your application 

Best, 

-- 
Nan Zhu





"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 19 May 2014 22:06:12 -0400",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),dev@spark.apache.org,"just rerun my test on rc5 

everything works

build applications with sbt and the spark-*.jar which is compiled with Hadoop 2.3

+1 

-- 
Nan Zhu





"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 19 May 2014 19:03:20 -0700",Re: spark 1.0 standalone application,dev@spark.apache.org,"That's the crude way to do it.  If you run `sbt/sbt publishLocal`, then you
can resolve the artifact from your local cache in the same way that you
would resolve it if it were deployed to a remote cache.  That's just the
build step.  Actually running the application will require the necessary
jars to be accessible by the cluster nodes.



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 May 2014 19:07:16 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","Having a user add define a custom class inside of an added jar and
instantiate it directly inside of an executor is definitely supported
in Spark and has been for a really long time (several years). This is
something we do all the time in Spark.

DB - I'd hold off on a re-architecting of this until we identify
exactly what is causing the bug you are running into.

In a nutshell, when the bytecode ""new Foo()"" is run on the executor,
it will ask the driver for the class over HTTP using a custom
classloader. Something in that pipeline is breaking here, possibly
related to the YARN deployment stuff.



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 May 2014 19:10:16 -0700",Re: spark 1.0 standalone application,"""dev@spark.apache.org"" <dev@spark.apache.org>","Whenever we publish a release candidate, we create a temporary maven
repository that host the artifacts. We do this precisely for the case
you are running into (where a user wants to build an application
against it to test).

You can build against the release candidate by just adding that
repository in your sbt build, then linking against ""spark-core""
version ""1.0.0"". For rc9 the repository is in the vote e-mail:

http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc9-td6629.html


"
Sujeet Varakhedi <svsujeet@gmail.com>,"Mon, 19 May 2014 19:21:30 -0700",Re: spark 1.0 standalone application,dev@spark.apache.org,"Threads like these are great candidates to be part of the ""Contributors
guide"". I will create a JIRA to update the guide with data past threads
like these.

Sujeet



"
Reynold Xin <rxin@databricks.com>,"Mon, 19 May 2014 19:32:41 -0700",Re: BUG: graph.triplets does not return proper values,dev@spark.apache.org,"reduce always return a single element - maybe you are misunderstanding what
the reduce function in collections does.



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 19 May 2014 19:38:47 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","It just hit me why this problem is showing up on YARN and not on standalone.

The relevant difference between YARN and standalone is that, on YARN, the
app jar is loaded by the system classloader instead of Spark's custom URL
classloader.

the classes in the primary app jar].   The custom classloader knows about
[the classes in secondary app jars] and has the system classloader as its
parent.

A few relevant facts (mostly redundant with what Sean pointed out):
* Every class has a classloader that loaded it.
* When an object of class B is instantiated inside of class A, the
classloader used for loading B is the classloader that was used for loading
A.
* When a classloader fails to load a class, it lets its parent classloader
try.  If its parent succeeds, its parent becomes the ""classloader that
loaded it"".

So suppose class B is in a secondary app jar and class A is in the primary
app jar:
1. The custom classloader will try to load class A.
2. It will fail, because it only knows about the secondary jars.
3. It will delegate to its parent, the system classloader.
4. The system classloader will succeed, because it knows about the primary
app jar.
5. A's classloader will be the system classloader.
6. A tries to instantiate an instance of class B.
7. B will be loaded with A's classloader, which is the system classloader.
8. Loading B will fail, because A's classloader, which is the system
classloader, doesn't know about the secondary app jars.

In Spark standalone, A and B are both loaded by the custom classloader, so
this issue doesn't come up.

-Sandy


"
DB Tsai <dbtsai@stanford.edu>,"Mon, 19 May 2014 19:42:53 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Good summary! We fixed it in branch 0.9 since our production is still in
0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
tonight.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Andrew Ash <andrew@andrewash.com>,"Mon, 19 May 2014 20:19:32 -0700",Re: TorrentBroadcast aka Cornet?,dev@spark.apache.org,"Thanks for the info Matei.

Andrew


ote:

that. It
ure
f an
e/spark/broadcast/TorrentBroadcast.scala
r
"
nit <nitinpanj@gmail.com>,"Mon, 19 May 2014 22:00:43 -0700 (PDT)",Re: spark 1.0 standalone application,dev@spark.incubator.apache.org,"Thanks everyone. I followed Patrick's suggestion and it worked like a charm.



--

"
DB Tsai <dbtsai@stanford.edu>,"Mon, 19 May 2014 22:04:58 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"In 1.0, there is a new option for users to choose which classloader has
higher priority via spark.files.userClassPathFirst, I decided to submit the
PR for 0.9 first. We use this patch in our lab and we can use those jars
added by sc.addJar without reflection.

https://github.com/apache/spark/pull/834

Can anyone comment if it's a good approach?

Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 May 2014 22:14:49 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","We're cancelling this RC in favor of rc10. There were two blockers: an
issue with Windows run scripts and an issue with the packaging for
Hadoop 1 when hive support is bundled.

https://issues.apache.org/jira/browse/SPARK-1875
https://issues.apache.org/jira/browse/SPARK-1876

Thanks everyone for the testing. TD will be cutting rc10, since I'm
travelling this week (thanks TD!).

- Patrick

doop 2.3
;
e.org (mailto:dev@spark.apache.org)>;
 Windows fixes. I also found one other likely bug, https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC. I think this is due to Hadoop 1's security code depending on a different version of org.apache.commons than Hadoop 2, but it needs investigation. Tom, any thoughts on this?
ly, I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn't quite match the master-setting behavior of the Unix based one. I'll send a pull request to fix them soon.
client
(mailto:mark@clearstorydata.com)>:
ail.com (mailto:pwendell@gmail.com)
gmail.com (mailto:pwendell@gmail.com)>
 Spark
;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
 be found
ark-1017/
nd at:
0!
asses if
. There are
ssociated
ll as
ide.html#from-09-to-10
gramming-guide.html#upgrading-from-pre-10-versions-of-spark
g-programming-guide.html#migration-guide-from-091-or-below-to-1x
rogramming-guide.html#upgrade-guide-from-spark-091
ad of
or
Seq[String]

"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 20 May 2014 01:23:40 -0400",Re: spark 1.0 standalone application,dev@spark.apache.org,"First time to know there is a temporary maven repositoryâ€¦â€¦.

--  
Nan Zhu



Apache-Spark-1-0-0-rc9-td6629.html
 then you
ou
he
ry
ur
with
ry?
ess ?
see
e? and
ut since
-standalone-application-tp6698.html
t


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 19 May 2014 22:25:21 -0700",Re: spark 1.0 standalone application,dev@spark.apache.org,"latest rc gets pushed to
https://repository.apache.org/content/repositories/staging/org/apache/spark/spark-core_2.10/--

The artifact here is just named ""1.0.0"" (similar to the rc specific
repository that Patrick mentioned). So if you just want to build you app
against the latest staging RC you can add ""
https://repository.apache.org/content/repositories/staging"" to your
resolvers in SBT / Maven.

Thanks
Shivaram



¦.
ache-Spark-1-0-0-rc9-td6629.html
mailto:
n
ou
he
ry
to:
ur
e
alone-application-tp6698.html
t
"
Madhu <madhu@madhu.com>,"Tue, 20 May 2014 06:25:33 -0700 (PDT)",Sorting partitions in Java,dev@spark.incubator.apache.org,"I'm trying to sort data in each partition of an RDD.
I was able to do it successfully in Scala like this:

val sorted = rdd.mapPartitions(iter => {
  iter.toArray.sortWith((x, y) => x._2.compare(y._2) < 0).iterator
},
preservesPartitioning = true)

I used the same technique as in OrderedRDDFunctions.scala, so I assume it's
a reasonable way to do it.

This works well so far, but I can't seem to do the same thing in Java
because 'iter' in the Java APIs is an Iterator rather than an Iterable.
There may be an unattractive workaround, but I didn't pursue it.

Ideally, it would be nice to have an efficient, robust method in RDD to sort
each partition.
Does something like that exist?

Thanks!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

"
Tom Graves <tgraves_cs@yahoo.com>,"Tue, 20 May 2014 06:51:44 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (rc9),"""dev@spark.apache.org"" <dev@spark.apache.org>","I assume we will have an rc10 to fix the issues Matei found?

Tom


 


Hey Matei - the issue you found is not related to security. This patch
a few days ago broke builds for Hadoop 1 with YARN support enabled.
The patch directly altered the way we deal with commons-lang
dependency, which is what is at the base of this stack trace.

https://github.com/apache/spark/pull/754

- Patrick

"
GlennStrycker <glenn.strycker@gmail.com>,"Tue, 20 May 2014 07:13:26 -0700 (PDT)",Re: BUG: graph.triplets does not return proper values,dev@spark.incubator.apache.org,"Oh... ha, good point.  Sorry, I'm new to mapreduce programming and forgot
about that... I'll have to adjust my reduce function to output a vector/RDD
as the element to return.  Thanks for reminding me of this!



--

"
Sean Owen <sowen@cloudera.com>,"Tue, 20 May 2014 15:14:49 +0100",Re: Sorting partitions in Java,dev@spark.apache.org,"It's an Iterator in both Java and Scala. In both cases you need to
copy the stream of values into something List-like to sort it. An
Iterable would not change that (not sure the API can promise many
iterations anyway).

If you just want the equivalent of ""toArray"", you can use a utility
method in Commons Collections or Guava. Guava's
Lists.newArrayList(Iterator) does nicely, which you can then
Collections.sort() with a Comparator and the return its iterator()

I dug this up too, remembering a similar question:
http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C529F819F.3060901@vu.nl%3E


"
Madhu <madhu@madhu.com>,"Tue, 20 May 2014 10:10:10 -0700 (PDT)",Re: Sorting partitions in Java,dev@spark.incubator.apache.org,"Thanks Sean, I had seen that post you mentioned.

What you suggest looks an in-memory sort, which is fine if each partition is
small enough to fit in memory. Is it true that rdd.sortByKey(...) requires
partitions to fit in memory? I wasn't sure if there was some magic behind
the scenes that supports arbitrarily large sorts.

None of this is a show stopper, it just might require a little more code on
the part of the developer. If there's a requirement for Spark partitions to
fit in memory, developers will have to be aware of that and plan
sets without thinking about data size.

In the case that a developer repartitions an RDD such that some partitions
don't fit in memory, sorting those partitions requires more work. For these
cases, I think there is value in having a robust partition sorting method
that deals with it efficiently and reliably.

Is there another solution for sorting arbitrarily large partitions? If not,
I don't mind developing and contributing a solution.




-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 20 May 2014 10:12:09 -0700",Re: Sorting partitions in Java,"""dev@spark.apache.org"" <dev@spark.apache.org>","sortByKey currently requires partitions to fit in memory, but there are
plans to add external sort



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 20 May 2014 10:12:09 -0700",Re: Sorting partitions in Java,"""dev@spark.apache.org"" <dev@spark.apache.org>","sortByKey currently requires partitions to fit in memory, but there are
plans to add external sort



"
Andrew Ash <andrew@andrewash.com>,"Tue, 20 May 2014 10:16:14 -0700",Re: Sorting partitions in Java,dev@spark.apache.org,"Sandy, is there a Jira ticket for that?



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 20 May 2014 10:21:16 -0700",Re: Sorting partitions in Java,"""dev@spark.apache.org"" <dev@spark.apache.org>","There is: SPARK-545



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 20 May 2014 10:21:16 -0700",Re: Sorting partitions in Java,"""dev@spark.apache.org"" <dev@spark.apache.org>","There is: SPARK-545



"
Andrew Ash <andrew@andrewash.com>,"Tue, 20 May 2014 10:16:14 -0700",Re: Sorting partitions in Java,dev@spark.apache.org,"Sandy, is there a Jira ticket for that?



"
Andrew Ash <andrew@andrewash.com>,"Tue, 20 May 2014 10:25:14 -0700",Re: Sorting partitions in Java,dev@spark.apache.org,"Voted :)

https://issues.apache.org/jira/browse/SPARK-983



"
Andrew Ash <andrew@andrewash.com>,"Tue, 20 May 2014 10:25:14 -0700",Re: Sorting partitions in Java,dev@spark.apache.org,"Voted :)

https://issues.apache.org/jira/browse/SPARK-983



"
Sean Owen <sowen@cloudera.com>,"Tue, 20 May 2014 18:27:03 +0100",Re: Sorting partitions in Java,dev@spark.apache.org,"
Yes, but so did the Scala version you posted -- I assumed that was OK
for your use case. Regardless of what Spark does, you would copy all
values into memory with toArray.

sortByKey is something fairly different. It sorts the whole RDD by
key, not values within each key. I think Sandy is talking about
something related but not quite the same.

Do you really mean you want to sort the whole RDD?

"
Madhu <madhu@madhu.com>,"Tue, 20 May 2014 10:47:59 -0700 (PDT)",Re: Sorting partitions in Java,dev@spark.incubator.apache.org,"Sean,

No, I don't want to sort the whole RDD, sortByKey seems to be good enough
for that.

Right now, I think the code I have will work for me, but I can imagine
conditions where it will run out of memory.

I'm not completely sure if  SPARK-983
<https://issues.apache.org/jira/browse/SPARK-983>    Andrew mentioned covers
the rdd.sortPartitions() use case. Can someone comment on the scope of
SPARK-983?

Thanks!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

"
GlennStrycker <glenn.strycker@gmail.com>,"Tue, 20 May 2014 12:16:40 -0700 (PDT)",Re: BUG: graph.triplets does not return proper values,dev@spark.incubator.apache.org,"Wait a minute... doesn't a reduce function return 1 element PER key pair? 
For example, word-count mapreduce functions return a {word, count} element
for every unique word.  Is this supposed to be a 1-element RDD object?

The .reduce function for a MappedRDD or FlatMappedRDD both are of the form

    def reduce(f: (T, T) => T): T

So presumably if I pass the reduce function a list of values {(X,1), (X,1),
(X,1), (Y,1), (Y,1)} and the function is ( (A,B) => (A._1, A._2+B._2 ) ),
then I should get a final vector of {(X,3), (Y,2)}, correct?


I have the following object:

    scala> temp3
    res128: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.Edge[Int],
Int)] = MappedRDD[107] at map at <console>:27

and it contains the following:

    scala> temp3.collect
    . . .
    res129: Array[(org.apache.spark.graphx.Edge[Int], Int)] =
Array((Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(4,4,1),1), (Edge(5,4,1),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(7,4,1),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(4,5,1),1), (Edge(5,5,1),1), (Edge(1,2,1),1), (Edge(1,3,1),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(7,5,1),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(4,7,1),1), (Edge(5,7,1),1), (Edge(0,0,0),1), (E...

but when I run the following, I only get one element in the final vector:

    scala> temp3.reduce( (A,B) => (A._1, A._2+B._2 ) )
    . . .
    res130: (org.apache.spark.graphx.Edge[Int], Int) = (Edge(0,0,0),256)

I should be additionally getting { (Edge(1,2,1),1), (Edge(1,3,1),2),
(Edge(2,3,1),2), (Edge(4,5,1),1), (Edge(5,6,1),2), (Edge(6,7,1),1),
(Edge(4,7,1),1), (Edge(5,7,1),2) }



Am I not mapping something correctly before running reduce?  I've tried both
.map and .flatMap, and put in _.copy() everywhere, e.g.

temp3.flatMap(A => Seq(A)).reduce( (A,B) => (A._1, A._2+B._2 ) )
temp3.map(_.copy()).flatMap(A => Seq(A)).reduce( (A,B) => (A._1, A._2+B._2 )
)
etc.





--

"
Reynold Xin <rxin@databricks.com>,"Tue, 20 May 2014 12:42:01 -0700",Re: BUG: graph.triplets does not return proper values,dev@spark.apache.org,"You are probably looking for reduceByKey in that case.

""reduce"" just reduces everything in the collection into a single element.



"
GlennStrycker <glenn.strycker@gmail.com>,"Tue, 20 May 2014 13:00:30 -0700 (PDT)",Re: BUG: graph.triplets does not return proper values,dev@spark.incubator.apache.org,"I don't seem to have this function in my Spark installation for this object,
or the classes MappedRDD, FlatMappedRDD, EdgeRDD, VertexRDD, or Graph.

Which class should have the reduceByKey function, and how do I cast my
current RDD as this class?

Perhaps this is still due to my Spark installation being out-of-date?



--

"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 20 May 2014 13:06:21 -0700",Re: BUG: graph.triplets does not return proper values,dev@spark.apache.org,"That's all very old functionality in Spark terms, so it shouldn't have
anything to do with your installation being out-of-date.  There is also no
need to cast as long as the relevant implicit conversions are in scope:
import org.apache.spark.SparkContext._



"
Sean Owen <sowen@cloudera.com>,"Tue, 20 May 2014 21:07:30 +0100",Re: BUG: graph.triplets does not return proper values,dev@spark.apache.org,"http://spark.apache.org/docs/0.9.1/api/core/index.html#org.apache.spark.rdd.PairRDDFunctions

It becomes automagically available when your RDD contains pairs.


"
GlennStrycker <glenn.strycker@gmail.com>,"Tue, 20 May 2014 13:08:34 -0700 (PDT)",Re: BUG: graph.triplets does not return proper values,dev@spark.incubator.apache.org,"For some reason it does not appear when I hit ""tab"" in Spark shell, but when
I put everything together in one line, it DOES WORK!

orig_graph.edges.map(_.copy()).cartesian(orig_graph.edges.map(_.copy())).flatMap(
A => Seq(if (A._1.srcId == A._2.dstId) Edge(A._2.srcId,A._1.dstId,1) else if
(A._1.dstId == A._2.srcId) Edge(A._1.srcId,A._2.dstId,1) else Edge(0,0,0) )
).map(word => (word, 1)).reduceByKey(_ + _).collect

= Array((Edge(5,7,1),4), (Edge(5,6,1),4), (Edge(3,2,1),4), (Edge(5,5,1),3),
(Edge(1,3,1),4), (Edge(2,3,1),4), (Edge(6,5,1),4), (Edge(5,4,1),2),
(Edge(2,1,1),2), (Edge(6,7,1),2), (Edge(2,2,1),2), (Edge(7,5,1),4),
(Edge(3,1,1),4), (Edge(4,5,1),2), (Edge(0,0,0),192), (Edge(3,3,1),3),
(Edge(4,7,1),2), (Edge(1,2,1),2), (Edge(4,4,1),1), (Edge(6,6,1),2),
(Edge(7,4,1),2), (Edge(7,6,1),2), (Edge(7,7,1),2), (Edge(1,1,1),3))




--

"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 20 May 2014 13:13:36 -0700",[VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This has a few bug fixes on top of rc9:
SPARK-1875: https://github.com/apache/spark/pull/824
SPARK-1876: https://github.com/apache/spark/pull/819
SPARK-1878: https://github.com/apache/spark/pull/822
SPARK-1879: https://github.com/apache/spark/pull/823

The tag to be voted on is v1.0.0-rc10 (commit d8070234):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc10/

The release artifacts are signed with the following key:
https://people.apache.org/keys/committer/tdas.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1018/

The documentation corresponding to this release can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/

The full list of changes in this release can be found at:
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Friday, May 23, at 20:00 UTC and passes if
amajority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

====== API Changes ======
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

Changes to ML vector specification:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10

Changes to the Java API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

Changes to the streaming API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

Changes to the GraphX API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

Other changes:
coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

"
Andy Konwinski <andykonwinski@gmail.com>,"Tue, 20 May 2014 14:06:33 -0700",Re: Scala examples for Spark do not work as written in documentation,dev@spark.apache.org,"I fixed the bug, but I kept the parameter ""i"" instead of ""_"" since that (1)
keeps it more parallel to the python and java versions which also use
functions with a named variable and (2) doesn't require readers to know
this particular use of the ""_"" syntax in Scala.

Thanks for catching this Glenn.

Andy



"
Andrew Or <andrew@databricks.com>,"Tue, 20 May 2014 17:26:11 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1


2014-05-20 13:13 GMT-07:00 Tathagata Das <tathagata.das1565@gmail.com>:

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 20 May 2014 17:28:10 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),"""dev@spark.apache.org"" <dev@spark.apache.org>",1
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 20 May 2014 17:28:30 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"+1 (non-binding)

I have:
- checked signatures and checksums of the files
- built the code from the git repo using both sbt and mvn (against hadoop 2.3.0)
- ran a few simple jobs in local, yarn-client and yarn-cluster mode

Haven't explicitly tested any o"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 20 May 2014 17:59:16 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Talked with Sandy and DB offline. I think the best solution is sending
the secondary jars to the distributed cache of all containers rather
than just the master, and set the classpath to include spark jar,
primary app jar, and secondary jars before executor starts. In this
way, user only needs to specify secondary jars via --jars instead of
calling sc.addJar inside the code. It also solves the scalability
problem of serving all the jars via http.

If this solution sounds good, I can try to make a patch.

Best,
Xiangrui


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 20 May 2014 19:39:08 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"+1

Tested it on both Windows and Mac OS X, with both Scala and Python. Confirmed that the issues in the previous RC were fixed.

Matei


hadoop 2.3.0)
version 1.0.0!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1a"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 20 May 2014 23:09:42 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),"""dev@spark.apache.org"" <dev@spark.apache.org>","Signature and hash for source looks good
No external executable package with source - good
Compiled with git and maven - good
Ran examples and sample programs locally and standalone -good

+1

- Henry




"
Sue Cai <cai.qianwen@hotmail.co.uk>,"Wed, 21 May 2014 00:31:06 -0700 (PDT)",MLlib ALS-- Errors communicating with MapOutputTracker,dev@spark.incubator.apache.org,"Hello,

I am currently using MLlib ALS to process a large volume of data, about 1.2
billion Rating(userId, productId, rates) triples. The dataset was sepatated
into 4000 partitions for parallized computation on our yarn clusters. 

I encountered this error ""Errors communicating with MapOutputTracker"",
when trying to get the prediciton rates [model.predict(userproducts)] after
iterations.

    val predictions = model.predict(usersProducts).map{
      case Rating(user, product, rate) => ((user, product), rate)
    }

I tried to separate the iteration process and the process of culating
prediction rates value by storing the two feature matirces into file system
first; and the loading them for prediction. This time, the error occurred at
the stage of loading userFeatures. 

userfData: userId:[0.3,0.5,0.002,.....]

  val userfTuple =userfData.map{
      case (line) => {
        val arr = line.split(splitmark_1)
        val featureArr = arr(1).split(splitmark_2)
        (arr(0),featureArr)
      }
    }

Here is part of the log:
----------------------------------------------------------------------------------------------------------------------------------------------------------------

14-05-21 14:37:17 WARN [Result resolver thread-0] TaskSetManager: Loss was
due to org.apache.spark.SparkException
org.apache.spark.SparkException: Error communicating with MapOutputTracker
        at
org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:79)
        at
org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:126)
        at
org.apache.spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:43)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:244)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:235)
        at
org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:90)
        at
org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:89)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:727)
        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:723)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:220)
        at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:184)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


-------------------------------------------------------------------------------------------------------------------------------------------------------------------

I have tried several methods to solve this problem, one way was to decrease
the number of partitions(from 4000 to 3000), another was to increase the
memory of masters. Both worked, but it is still vital to track the
underneath causes there, right? 

Could anyone help me to check this problem? Thanks a lot. 

Sue Cai




--

"
"""=?ISO-8859-1?B?d2l0Z28=?="" <witgo@qq.com>","Wed, 21 May 2014 15:45:50 +0800",Re:MLlib ALS-- Errors communicating with MapOutputTracker,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","Lack of hard disk space? If yes, you can try https://github.com/apache/spark/pull/828




------------------ Original ------------------
From:  ""Sue Cai"";<cai.qianwen@hotmail.co.uk>;
Date:  Wed, May 21, 2014 03:31 PM
To:  ""dev""<dev@spark.incubator.apache.org>; 

Subject:  MLlib ALS-- Errors communicating with MapOutputTracker



Hello,

I am currently using MLlib ALS to process a large volume of data, about 1.2
billion Rating(userId, productId, rates) triples. The dataset was sepatated
into 4000 partitions for parallized computation on our yarn clusters. 

I encountered this error ""Errors communicating with MapOutputTracker"",
when trying to get the prediciton rates [model.predict(userproducts)] after
iterations.

    val predictions = model.predict(usersProducts).map{
      case Rating(user, product, rate) => ((user, product), rate)
    }

I tried to separate the iteration process and the process of culating
prediction rates value by storing the two feature matirces into file system
first; and the loading them for prediction. This time, the error occurred at
the stage of loading userFeatures. 

userfData: userId:[0.3,0.5,0.002,.....]

  val userfTuple =userfData.map{
      case (line) => {
        val arr = line.split(splitmark_1)
        val featureArr = arr(1).split(splitmark_2)
        (arr(0),featureArr)
      }
    }

Here is part of the log:
----------------------------------------------------------------------------------------------------------------------------------------------------------------

14-05-21 14:37:17 WARN [Result resolver thread-0] TaskSetManager: Loss was
due to org.apache.spark.SparkException
org.apache.spark.SparkException: Error communicating with MapOutputTracker
        at
org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:79)
        at
org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:126)
        at
org.apache.spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:43)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:244)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:235)
        at
org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:90)
        at
org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:89)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:727)
        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:723)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:220)
        at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:184)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


-------------------------------------------------------------------------------------------------------------------------------------------------------------------

I have tried several methods to solve this problem, one way was to decrease
the number of partitions(from 4000 to 3000), another was to increase the
memory of masters. Both worked, but it is still vital to track the
underneath causes there, right? 

Could anyone help me to check this problem? Thanks a lot. 

Sue Cai




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-ALS-Errors-communicating-with-MapOutputTracker-tp6740.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
."
Gerard Maas <gerard.maas@gmail.com>,"Wed, 21 May 2014 14:47:22 +0200",Re: ClassNotFoundException with Spark/Mesos (spark-shell works fine),"user@spark.apache.org, dev@spark.apache.org","Hi Tobias,

I was curious about this issue and tried to run your example on my local
Mesos. I was able to reproduce your issue using your current config:

[error] (run-main-0) org.apache.spark.SparkException: Job aborted: Task
1.0:4 failed 4 times (most recent failure: Exception failure:
java.lang.ClassNotFoundException: spark.SparkExamplesMinimal$$anonfun$2)
org.apache.spark.SparkException: Job aborted: Task 1.0:4 failed 4 times
(most recent failure: Exception failure: java.lang.ClassNotFoundException:
spark.SparkExamplesMinimal$$anonfun$2)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)

Creating a simple jar from the job and providing it through the
configuration seems to solve it:

val conf = new SparkConf()
      .setMaster(""mesos://<my_ip>:5050/"")
*
.setJars(Seq(""/sparkexample/target/scala-2.10/sparkexample_2.10-0.1.jar""))*
      .setAppName(""SparkExamplesMinimal"")

Resulting in:
14/05/21 12:03:45 INFO scheduler.DAGScheduler: Completed ResultTask(1, 1)
14/05/21 12:03:45 INFO scheduler.DAGScheduler: Stage 1 (count at
SparkExamplesMinimal.scala:50) finished in 1.120 s
14/05/21 12:03:45 INFO spark.SparkContext: Job finished: count at
SparkExamplesMinimal.scala:50, took 1.177091435 s
count: 1000000

Why the closure serialization does not work with Mesos is beyond my current
knowledge.
Would be great to hear from the experts (cross-posting to dev for that)

-kr, Gerard.














"
Gerard Maas <gerard.maas@gmail.com>,"Wed, 21 May 2014 16:19:01 +0200",Should SPARK_HOME be needed with Mesos?,dev@spark.apache.org,"Spark dev's,

I was looking into a question asked on the user list where a
ClassNotFoundException was thrown when running a job on Mesos. Curious
issue with serialization on Mesos: more details here [1]:

When trying to run that simple example on my Mesos installation, I faced
another issue: I got an error that ""SPARK_HOME"" was not set. I found that
curious b/c a local spark installation should not be required to run a job
on Mesos. All that's needed is the executor package, being the
assembly.tar.gz on a reachable location (HDFS/S3/HTTP).

I went looking into the code and indeed there's a check on SPARK_HOME [2]
regardless of the presence of the assembly but it's actually only used if
the assembly is not provided (which is a kind-of best-effort recovery
strategy).

Current flow:

if (!SPARK_HOME) fail(""No SPARK_HOME"")
else if (assembly) { use assembly) }
else { try use SPARK_HOME to build spark_executor }

Should be:
sparkExecutor =  if (assembly) {assembly}
                 else if (SPARK_HOME) {try use SPARK_HOME to build
spark_executor}
                 else { fail(""No executor found. Please provide
spark.executor.uri (preferred) or spark.home"")

What do you think?

-kr, Gerard.


[1]
http://apache-spark-user-list.1001560.n3.nabble.com/ClassNotFoundException-with-Spark-Mesos-spark-shell-works-fine-td6165.html

[2]
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala#L89
"
Gerard Maas <gerard.maas@gmail.com>,"Wed, 21 May 2014 17:32:38 +0200",Re: ClassNotFoundException with Spark/Mesos (spark-shell works fine),"user@spark.apache.org, dev@spark.apache.org","Hi Tobias,

Regarding my comment on closure serialization:

I was discussing it with my fellow Sparkers here and I totally overlooked
the fact that you need the class files to de-serialize the closures (or
whatever) on the workers, so you always need the jar file delivered to the
workers in order for it to work.

The SparkREPL  works differently. It uses some dark magic to send the
working session to the workers.

-kr, Gerard.






"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 21 May 2014 09:58:33 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,1
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 21 May 2014 13:13:52 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","This will solve the issue for jars added upon application submission, but,
on top of this, we need to make sure that anything dynamically added
through sc.addJar works as well.

To do so, we need to make sure that any jars retrieved via the driver's
HTTP server are loaded by the same classloader that loads the jars given on
app submission.  To achieve this, we need to either use the same
classloader for both system jars and user jars, or make sure that the user
jars given on app submission are under the same classloader used for
dynamically added jars.


"
DB Tsai <dbtsai@stanford.edu>,"Wed, 21 May 2014 13:40:07 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"This will be another separate story.

Since in the yarn deployment, as Sandy said, the app.jar will be always in
the systemclassloader which means any object instantiated in app.jar will
have parent loader of systemclassloader instead of custom one. As a result,
the custom class loader in yarn will never work without specifically using
reflection.

Solution will be not using system classloader in the classloader hierarchy,
and add all the resources in system one into custom one. This is the
approach of tomcat takes.

Or we can directly overwirte the system class loader by calling the
protected method `addURL` which will not work and throw exception if the
code is wrapped in security manager.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 21 May 2014 14:10:58 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"I think adding jars dynamically should work as long as the primary jar
and the secondary jars do not depend on dynamically added jars, which
should be the correct logic. -Xiangrui


"
DB Tsai <dbtsai@stanford.edu>,"Wed, 21 May 2014 14:16:22 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"How about the jars added dynamically? Those will be in customLoader's
classpath but not in the system one. Unfortunately, when we reference to
those jars added dynamically in primary jar, the default classloader will
be the system one not the custom one.

It works in standalone mode since the primary jar is not in the system
loader but custom one, so when we reference those jars added dynamically,
we can find it without reflection.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 21 May 2014 14:27:16 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is that an assumption we can make?  I think we'd run into an issue in this
situation:

*In primary jar:*
def makeDynamicObject(clazz: String) = Class.forName(clazz).newInstance()

*In app code:*
sc.addJar(""dynamicjar.jar"")
...
rdd.map(x => makeDynamicObject(""some.class.from.DynamicJar""))

It might be fair to say that the user should make sure to use the context
classloader when instantiating dynamic classes, but I think it's weird that
this code would work on Spark standalone but not on YARN.

-Sandy



"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 21 May 2014 14:41:15 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"That's a good example. If we really want to cover that case, there are
two solutions:

1. Follow DB's patch, adding jars to the system classloader. Then we
cannot put a user class in front of an existing class.
2. Do not send the primary jar and secondary jars to executors'
distributed cache. Instead, add them to ""spark.jars"" in SparkSubmit
and serve them via http by called sc.addJar in SparkContext.

What is your preference?


"
Koert Kuipers <koert@tresata.com>,"Wed, 21 May 2014 17:57:16 -0400",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"db tsai, i do not think userClassPathFirst is working, unless the classes
you load dont reference any classes already loaded by the parent
classloader (a mostly hypothetical situation)... i filed a jira for this
here:
https://issues.apache.org/jira/browse/SPARK-1863




"
Patrick Wendell <pwendell@gmail.com>,"Wed, 21 May 2014 15:15:28 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","Of these two solutions I'd definitely prefer 2 in the short term. I'd
imagine the fix is very straightforward (it would mostly just be
remove code), and we'd be making this more consistent with the
standalone mode which makes things way easier to reason about.

In the long term we'll definitely want to exploit the distributed
cache more, but at this point it's premature optimization at a high
complexity cost. Writing stuff to HDFS through is so slow anyways I'd
guess that serving it directly from the driver is still faster in most
cases (though for very large jar sizes or very large clusters, yes,
we'll need the distributed cache).

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 21 May 2014 15:22:32 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey I just looked at the fix here:
https://github.com/apache/spark/pull/848

Given that this is quite simple, maybe it's best to just go with this
and just explain that we don't support adding jars dynamically in YARN
in Spark 1.0. That seems like a reasonable thing to do.


"
Kevin Markey <kevin.markey@oracle.com>,"Wed, 21 May 2014 16:34:47 -0600",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"0

Abstaining because I'm not sure if my failures are due to Spark, 
configuration, or other factors...

Compiled and deployed RC10 for YARN, Hadoop 2.3 per Spark 1.0.0 Yarn 
documentation.  No problems.
Rebuilt applications against RC10 and Hadoop 2.3.0 (plain vanilla Apache 
release).
Updated scripts for various applications.
Application had successfully compiled and run against Spark 0.9.1 and 
Hadoop 2.3.0.
Ran in ""yarn-cluster"" mode.
Application ran to conclusion except that it ultimately failed because 
of an exception when Spark tried to clean up the staging directory.  
Also, where before Yarn would report the running program as ""RUNNING"", 
it only reported this application as ""ACCEPTED"".  It appeared to run two 
containers when the first instance never reported that it was RUNNING.

I will post a separate note to the USER list about the specifics.

Thanks
Kevin Markey




"
DB Tsai <dbtsai@stanford.edu>,"Wed, 21 May 2014 15:50:09 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"@Xiangrui
How about we send the primary jar and secondary jars into distributed cache
without adding them into the system classloader of executors. Then we add
them using custom classloader so we don't need to call secondary jars
through reflection in pri"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Wed, 21 May 2014 16:19:01 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"Hi Kevin,

Can you try https://issues.apache.org/jira/browse/SPARK-1898 to see if it
fixes your issue?

Running in YARN cluster mode, I had a similar issue where Spark was able to
create a Driver and an Executor via YARN, but then it stopped making any
progress.

Note: I was using a pre-release version of CDH5.1.0, not 2.3 like you were
using.

best,
Colin



"
Sue Cai <cai.qianwen@hotmail.co.uk>,"Wed, 21 May 2014 19:47:12 -0700 (PDT)",Re: Re:MLlib ALS-- Errors communicating with MapOutputTracker,dev@spark.incubator.apache.org,"Hi Witgo,

     Thanks a lot for your reply.

     In my second test, the user features and product features were loaded
from the file system directly,which means I did not use ALS here, and this
problem happened at the loading data stage. The way I am asking the question
was a little bit miss leading, this problem might not be ALS specific.

    As far as I had discovered, it might related to the usage of master
memory, since I could solve this problem by increase the master memory size,
or reduce the number of partitions. But I am still waiting for a better
explanation :)

   Thank you again.

Sue   



--

"
Tom Graves <tgraves_cs@yahoo.com>,"Wed, 21 May 2014 19:51:26 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),"""dev@spark.apache.org"" <dev@spark.apache.org>","Has anyone tried pyspark on yarn and got it to work?  I was having issues when I built spark on redhat but when I built on my mac it had worked,  but now when I build it on my mac it also doesn't work.

Tom


e as Apache Spark version 1.0.0!

This has a few bug fixes on top of rc9:
SPARK-1875: https://github.com/apache/spark/pull/824
SPARK-1876: https://github.com/apache/spark/pull/819
SPARK-1878: https://github.com/apache/spark/pull/822
SPARK-1879: https://github.com/apache/spark/pull/823

The tag to be voted on is v1.0.0-rc10 (commit d8070234):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc10/

The release artifacts are signed with the following key:
https://people.apache.org/keys/committer/tdas.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1018/

The documentation corresponding to this release can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/

The full list of changes in this release can be found at:
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Friday, May 23, at 20:00 UTC and passes if
amajority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

====== API Changes ======
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

Changes to ML vector specification:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10

Changes to the Java API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

Changes to the streaming API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

Changes to the GraphX API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

Other changes:
coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior"
Tom Graves <tgraves_cs@yahoo.com>,"Wed, 21 May 2014 20:18:49 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),"""dev@spark.apache.org"" <dev@spark.apache.org>","I don't think Kevin's issue would be with an api change in YarnClientImpl since in both cases he says he is using hadoop 2.3.0.  I'll take a look atin,

Can you try https://issues.apache.org/jira/browse/SPARK-1898 to see if it
fixes your issue?

Running in YARN cluster mode, I had a similar issue where Spark was able to
create a Driver and an Executor via YARN, but then it stopped making any
progress.

Note: I was using a pre-release version of CDH5.1.0, not 2.3 like you were
using.

best,
Coilures are due to Spark,
> configuration, or other factors...
>
> Compiled and deployed RC10 for YARN, Hadoop 2.3
 per Spark 1.0.0 Yarn
> documentation.  No problems.
> Rebuilt applications against RC10 and Hadoop 2.3.0 (plain vanilla Apache
> release).
> Updated scripts for various applications.
> Application had successfully compiled and run against Spark 0.9.1 and
> Hadoop 2.3.0.
> Ran in ""yarn-cluster"" mode.
> Application ran to conclusion except that it ultimately failed because of
> an exception when Spark tried to clean up the staging directory.  Also,
> where before Yarn would report the running program as ""RUNNING"", it only
ners
> when the first instance never reported that it was RUNNING.
>
and hash for source looks good
>>> No external executable package with source - good
>>> Compiled with git and maven - good
>>> Ran examples and sample programs locally and standalone -good
>>>
>>> +1
>>>
>>> - leasing the following candidate as Apache Spark version
>>>>
>>> 1.0.0!
>>>
>>>> This has a few bug fixes on top of rc9:
>>>> SPARK-1875: https://github.com/apache/spark/pull/824
>>>> SPARK-1876: https://github.com/apache/spark/pull/819
>>>> SPARK-1878: https://github.com/apache/spark/pull/822
>>>> SPARK-1879: https://github.com/apache/spark/pull/823
>>>>
>>>> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
>>>>
>=
>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>
>>>> The
 release files, including signatures, digests, etc. can be found at:
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>>>>
>>>> The release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>
>>>> The staging repository for this release can be found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1018/
>>>>
>>>> The documentation
 corresponding to this release can be found at:
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>
>>>> The full list of changes in this release can be found at:
>>>>
>>>>  https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;
>>> f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=
>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>
>>>ase this package as Apache Spark 1.0.0
>>>> [ ] -1 Do not release this package because ...
>>>>
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>>
>>>> ====== API Changes ======
>>>> We welcome users to compile Spark applications against 1.0. There are
>>>> a few API changes in this release. Here are links to the associated
>>>> upgrade guides - user facing changes have been kept as small as
>>>> possible.
>>>>
>>>> Changes to ML vector specification:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> mllib-guide.html#from-09-to-10
>>>
>>>> Changes to the Java API:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>
>>>> Changes to the streaming API:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>
>>>> Changes to the GraphX API:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> graphx-programming-guide.html#upgrade-guide-from-spark-091
terable[T] instead of Seq[T]
>>>> ==> Call toSeq on the result to restore the old behavior
>>>>
>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>> ==> Call toSeq on the result to restore old behavior
>>>>
>>>
>"
Andrew Ash <andrew@andrewash.com>,"Thu, 22 May 2014 00:19:40 -0400",Re: Should SPARK_HOME be needed with Mesos?,dev@spark.apache.org,"Hi Gerard,

I agree that your second option seems preferred.  You shouldn't have to
specify a SPARK_HOME if the executor is going to use the spark.executor.uri
instead.  Can you send in a pull request that includes your proposed
changes?

Andrew



"
npanj <nitinpanj@gmail.com>,"Wed, 21 May 2014 22:59:00 -0700 (PDT)",Graphx: GraphLoader.edgeListFile with edge weight,dev@spark.incubator.apache.org,"Hi,

For my project I needed to load a graph with edge weight; for this I have
updated GraphLoader.edgeListFile to consider third column in input file. I
will like to submit my patch for review so that it can be merged with master
branch. What is the process for submitting patches? 



--

"
Reynold Xin <rxin@databricks.com>,"Wed, 21 May 2014 23:04:29 -0700",Re: Graphx: GraphLoader.edgeListFile with edge weight,dev@spark.apache.org,"You can submit a pull request on the github mirror:
https://github.com/apache/spark

Thanks.



"
Kevin Markey <kevin.markey@oracle.com>,"Thu, 22 May 2014 01:32:57 -0600",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"I've discovered that one of the anomalies I encountered was due to a 
(embarrassing? humorous?) user error.  See the user list thread ""Failed 
RC-10 yarn-cluster job for FS closed error when cleaning up staging 
directory"" for my discussion.  With the user error corrected, the FS 
closed exception only prevents deletion of the staging directory, but 
does not affect completion with ""SUCCESS."" The FS closed exception still 
needs some investigation at least by me.

I tried the patch reported by SPARK-1898, but it didn't fix the problem 
without fixing the user error.  I did not attempt to test my fix without 
the patch, so I can't pass judgment on the patch.

Although this is merely a pseudocluster based test -- I can't 
reconfigure our cluster with RC-10 -- I'll now change my vote to...

+1.

Thanks all who helped.
Kevin





"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 22 May 2014 01:34:03 -0700",Re: Calling external classes added by sc.addJar needs to be through reflection,dev@spark.apache.org,"Hi DB,

I found it is a little hard to implement the solution I mentioned:


If you look at ApplicationMaster code, which is entry point in
yarn-cluster mode. It actually creates a thread of the user class
first and waits the user class to create a spark context. It means the
user class has to be on the classpath at that time. I think we need to
add the primary jar and secondary jars twice, once to system
classpath, and then to the executor classloader.

Best,
Xiangrui


"
MEETHU MATHEW <meethu2006@yahoo.co.in>,"Thu, 22 May 2014 18:06:00 +0800 (SGT)",Contributions to MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,


I would like to do some contributions towards the MLlib .I've a few concerns regarding the same.

1. Is there any reason for implementing the algorithms supported  by MLlib in Scala
2. Will you accept if  the contributions are done in Python or Java

Thanks,
Meethu M"
Gerard Maas <gerard.maas@gmail.com>,"Thu, 22 May 2014 17:03:31 +0200",Re: Should SPARK_HOME be needed with Mesos?,"dev@spark.apache.org, andrew@andrewash.com","Sure.  Should I create a Jira as well?

I saw there's already a broader ticket regarding the ambiguous use of
SPARK_HOME [1]  (cc: Patrick as owner of that ticket)

I don't know if it would be more relevant to remove the use of SPARK_HOME
when using mesos and have the assembly as the only way forward, or whether
that's a too radical change that might break some existing systems.

don't see installing and configuring Spark distros on a mesos master as a
way to have the mesos executor in place.

-kr, Gerard.

[1] https://issues.apache.org/jira/browse/SPARK-1110



"
Kevin Markey <kevin.markey@oracle.com>,"Thu, 22 May 2014 10:49:40 -0600",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"I retested several different cases...

1. FS closed exception shows up ONLY in RC-10, not in Spark 0.9.1, with 
both Hadoop 2.2 and 2.3.
2. SPARK-1898 has no effect for my use cases.
3. The failure to report that the underlying application is ""RUNNING"" 
and that it has succeeded is due ONLY to my user error.

The FS closed exception only effects the cleanup of the staging 
directory, not the final success or failure.  I've not yet tested the 
effect of changing my application's initialization, use, or closing of 
FileSystem.

Thanks again.
Kevin



"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 22 May 2014 10:06:25 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"Hi Kevin,


Without going and reading more of the Spark code, if your app is
explicitly close()'ing the FileSystem instance, it may be causing the
exception. If Spark is caching the FileSystem instance, your app is
probably closing that same instance (which it got from the HDFS
library's internal cache).

It would be nice if you could test that theory; it might be worth
knowing that's the case so that we can tell people not to do that.

-- 
Marcelo

"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 22 May 2014 10:16:22 -0700",Re: Contributions to MLlib,"dev@spark.apache.org, MEETHU MATHEW <meethu2006@yahoo.co.in>","Hi Meethu,

Thanks for asking! Scala is the native language in Spark. Implementing
algorithms in Scala can utilize the full power of Spark Core. Also,
Scala's syntax is very concise. Implementing ML algorithms using
different languages would increase the maintenance cost. However,
there are still much work to be done in the Python/Java land. For
example, we currently do not support distributed matrix and decision
tree in Python, and those interfaces may not be friendly for Java
users. If you would like to contribute to MLlib in Python or Java, it
would be a good place to start. Thanks!

Best,
Xiangrui


"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Thu, 22 May 2014 12:06:18 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"The FileSystem cache is something that has caused a lot of pain over the
years.  Unfortunately we (in Hadoop core) can't change the way it works now
because there are too many users depending on the current behavior.

Basically, the idea is that when you request a FileSystem with certain
options with FileSystem#get, you might get a reference to an FS object that
already exists, from our FS cache cache singleton.  Unfortunately, this
also means that someone else can change the working directory on you or
close the FS underneath you.  The FS is basically shared mutable state, and
you don't know whom you're sharing with.

It might be better for Spark to call FileSystem#newInstance, which bypasses
the FileSystem cache and always creates a new object.  If Spark can hang on
to the FS for a while, it can get the benefits of caching without the
downsides.  In HDFS, multiple FS instances can also share things like the
socket cache between them.

best,
Colin



"
Andrew Ash <andrew@andrewash.com>,"Thu, 22 May 2014 15:26:23 -0400",Re: Should SPARK_HOME be needed with Mesos?,Gerard Maas <gerard.maas@gmail.com>,"Fixing the immediate issue of requiring SPARK_HOME to be set when it's not
actually used is a separate ticket in my mind from a larger cleanup of what
SPARK_HOME means across the cluster.

I think you should file a new ticket for just this particular issue.



"
Aaron Davidson <ilikerps@gmail.com>,"Thu, 22 May 2014 12:48:23 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"In Spark 0.9.0 and 0.9.1, we stopped using the FileSystem cache correctly,
and we just recently resumed using it in 1.0 (and in 0.9.2) when this issue
was fixed: https://issues.apache.org/jira/browse/SPARK-1676

Prior to this fix, each Spark task created and cached its own FileSystems
due to a bug in how the FS cache handles UGIs. The big problem that arose
was that these FileSystems were never closed, so they just kept piling up.
There were two solutions we considered, with the following effects: (1)
Share the FS cache among all tasks and (2) Each task effectively gets its
own FS cache, and closes all of its FSes after the task completes.

We chose solution (1) for 3 reasons:
 - It does not rely on the behavior of a bug in HDFS.
 - It is the most performant option.
 - It is most consistent with the semantics of the (albeit broken) FS cache.

Since this behavior was changed in 1.0, it could be considered a
regression. We should consider the exact behavior we want out of the FS
cache. For Spark's purposes, it seems fine to cache FileSystems across
tasks, as Spark does not close FileSystems. The issue that comes up is that
user code which uses FileSystem.get() but then closes the FileSystem can
screw up Spark processes which were using that FileSystem. The workaround
for users would be to use FileSystem.newInstance() if they want full
control over the lifecycle of their FileSystems.



"
Kevin Markey <kevin.markey@oracle.com>,"Thu, 22 May 2014 14:25:46 -0600",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"Thank you, all!  This is quite helpful.

We have been arguing how to handle this issue across a growing 
application.  Unfortunately the Hadoop FileSystem java doc should say 
all this but doesn't!

Kevin



"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 22 May 2014 14:05:59 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"Hey all,

pyspark scripts on YARN.
https://issues.apache.org/jira/browse/SPARK-1900
This is a blocker and worth cutting a new RC.

We also found a fix for a known issue that prevents additional jar
files to be specified through spark-submit on YARN.
https://issues.apache.org/jira/browse/SPARK-1870
The has been fixed and will be in the next RC.

We are canceling this vote for now. We will post RC11 shortly. Thanks
everyone for testing!

TD


"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Thu, 22 May 2014 15:04:58 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"

Interesting...



Since the FS cache is in hadoop-common-project, it's not so much a bug in
HDFS as a bug in Hadoop.  So even if you're using, say, Lustre, you'll
still get the same issues with org.apache.hadoop.fs.FileSystem and its
global cache.

We chose solution (1) for 3 reasons:

 - It is the most performant option.

The current solution seems reasonable, as long as Spark processes:
1. don't change the current working directory (doing so isn't thread-safe
and will affect all other users of that FS object)
2. don't close the FileSystem object

Another solution would be to use newInstance and build your own FS cache,
essentially.  I don't think it would be that much code.  This might be
nicer because you could implement things like closing FileSystem objects
that haven't been used in a while.

cheers,
Colin



"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 22 May 2014 15:07:08 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),"""dev@spark.apache.org"" <dev@spark.apache.org>","Looks like SPARK-1900 is a blocker for YARN and might as well add
SPARK-1870 while at it.

TD or Patrick, could you kindly send [CANCEL] prefixed in the subject
email out for the RC10 Vote to help people follow the active VOTE
threads? The VOTE emails are getting a bit hard to follow.


- Henry



"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 22 May 2014 15:25:59 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"Right! Doing that.

TD


"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 22 May 2014 15:47:50 -0700",[CANCEL][VOTE] Release Apache Spark 1.0.0 (RC10),dev@spark.apache.org,"Hey all,

We are canceling the vote on RC10 because of a blocker bug in pyspark on Yarn.
https://issues.apache.org/jira/browse/SPARK-1900

Thanks everyone for testing! We will post RC11 soon.

TD

"
Gerard Maas <gerard.maas@gmail.com>,"Fri, 23 May 2014 00:59:50 +0200",Re: Should SPARK_HOME be needed with Mesos?,Andrew Ash <andrew@andrewash.com>,"ack



"
prabeesh k <prabsmails@gmail.com>,"Fri, 23 May 2014 10:22:14 +0530",java.lang.OutOfMemoryError while running Shark on Mesos,"user@spark.apache.org, dev <dev@spark.apache.org>","Hi,

I am trying to apply  inner join in shark using 64MB and 27MB files. I am
able to run the following queris on Mesos


   - ""SELECT * FROM geoLocation1 ""



   - """""" SELECT * FROM geoLocation1  WHERE  country =  '""US""' """"""


But while trying inner join as

 ""SELECT * FROM geoLocation1 g1 INNER JOIN geoBlocks1 g2 ON (g1.locId =
g2.locId)""



I am getting following error as follows.


Exception in thread ""main"" org.apache.spark.SparkException: Job aborted:
Task 1.0:7 failed 4 times (most recent failure: Exception failure:
java.lang.OutOfMemoryError: Java heap space)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
 at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
 at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
 at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
 at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
 at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
 at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


Please help me to resolve this.

Thanks in adv

regards,
prabeesh
"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 23 May 2014 11:57:55 +0530",Re: java.lang.OutOfMemoryError while running Shark on Mesos,user@spark.apache.org,"Hi Prabeesh,

Do a export _JAVA_OPTIONS=""-Xmx10g"" before starting the shark. Also you can
do a ps aux | grep shark and see how much memory it is being allocated,
mostly it should be 512mb, in that case increase the limit.

Thanks
Best Regards



"
Jim Donahue <jdonahue@adobe.com>,"Fri, 23 May 2014 21:57:40 +0000",No output from Spark Streaming program with  Spark 1.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","I¹m trying out 1.0 on a set of small Spark Streaming tests and am running
into problems.  Here¹s one of the little programs I¹ve used for a long
time ‹ it reads a Kafka stream that contains Twitter JSON tweets and does
some simple counting.  The program starts OK (it connects to the Kafka
stream fine) and generates a stream of INFO logging messages, but never
generates any output. :-(

I¹m running this in Eclipse, so there may be some class loading issue
(loading the wrong class or something like that), but I¹m not seeing
anything in the console output.

Thanks,

Jim Donahue
Adobe



val kafka_messages =
      KafkaUtils.createStream[Array[Byte], Array[Byte],
kafka.serializer.DefaultDecoder, kafka.serializer.DefaultDecoder](ssc,
propsMap, topicMap, StorageLevel.MEMORY_AND_DISK)

    
     val messages = kafka_messages.map(_._2)

     
     val total = ssc.sparkContext.accumulator(0)

     
     val startTime = new java.util.Date().getTime()

     
     val jsonstream = messages.map[JSONObject](message =>
      {val string = new String(message);
      val json = new JSONObject(string);
      total += 1
      json
      }
    )

    
    val deleted = ssc.sparkContext.accumulator(0)

    
    val msgstream = jsonstream.filter(json =>
      if (!json.has(""delete"")) true else { deleted += 1; false}
      )

    
    msgstream.foreach(rdd => {
      if(rdd.count() > 0){
      val data = rdd.map(json => (json.has(""entities""),
json.length())).collect()
      val entities: Double = data.count(t => t._1)
      val fieldCounts = data.sortBy(_._2)
      val minFields = fieldCounts(0)._2
      val maxFields = fieldCounts(fieldCounts.size - 1)._2
      val now = new java.util.Date()
      val interval = (now.getTime() - startTime) / 1000
      System.out.println(now.toString)
      System.out.println(""processing time: "" + interval + "" seconds"")
      System.out.println(""total messages: "" + total.value)
      System.out.println(""deleted messages: "" + deleted.value)
      System.out.println(""message receipt rate: "" + (total.value/interval)
+ "" per second"")
      System.out.println(""messages this interval: "" + data.length)
      System.out.println(""message fields varied between: "" + minFields + ""
and "" + maxFields)
      System.out.println(""fraction with entities is "" + (entities /
data.length))
      }
    }
    )
    
    ssc.start()


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 23 May 2014 18:00:45 -0700",Re: No output from Spark Streaming program with Spark 1.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Jim,

Do you see the same behavior if you run this outside of eclipse?

Also, what happens if you print something to standard out when setting
up your streams (i.e. not inside of the foreach) do you see that? This
could be a streaming issue, but it could also be something related to
the way it's running in eclipse.

- Patrick

nning
 a long
and does
e
)
""

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 23 May 2014 18:02:01 -0700",Re: No output from Spark Streaming program with Spark 1.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Also one other thing to try, try removing all of the logic form inside
of foreach and just printing something. It could be that somehow an
exception is being triggered inside of your foreach block and as a
result the output goes away.

:
unning
r a long
 and does
ue
g
l)
 ""

"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 23 May 2014 19:29:21 -0700",Re: No output from Spark Streaming program with Spark 1.0,dev@spark.apache.org,"Few more suggestions.
1. See the web ui, is the system running any jobs? If not, then you may
need to give the system more nodes. Basically the system should have more
cores than the number of receivers.
2. Furthermore there is a streaming specific web ui which gives more
streaming specific data.


:

:
for a long
ts and
r
ssue
ing
"
Nilesh <nilesh@nileshc.com>,"Sat, 24 May 2014 08:32:57 -0700 (PDT)",Kryo serialization for closures: a workaround,dev@spark.incubator.apache.org,"Suppose my mappers can be functions (def) that internally call other classes
and create objects and do different things inside. (Or they can even be
classes that extend (Foo) => Bar and do the processing in their apply method
- but let's ignore this case for now)

Spark supports only Java Serialization for closures and forces all the
classes inside to implement Serializable and coughs up errors when forced to
use Kryo for closures. But one cannot expect all 3rd party libraries to have
all classes extend Serializable!

Here's a workaround that I thought I'd share in case anyone comes across
this problem:

You simply need to serialize the objects before passing through the closure,
and de-serialize afterwards. This approach just works, even if your classes
aren't Serializable, because it uses Kryo behind the scenes. All you need is
some curry. ;) Here's an example of how I did it:

def genMapper(kryoWrapper: KryoSerializationWrapper[(Foo => Bar)])              
(foo: Foo) : Bar = {    kryoWrapper.value.apply(foo)}val mapper =
genMapper(KryoSerializationWrapper(new Blah(abc)))
_rdd.flatMap(mapper).collectAsMap()object Blah(abc: ABC) extends (Foo =>
Bar) {    def apply(foo: Foo) : Bar = { //This is the real function }}
Feel free to make Blah as complicated as you want, class, companion object,
nested classes, references to multiple 3rd party libs.

KryoSerializationWrapper refers to  this wrapper from amplab/shark
<https://github.com/amplab/shark/blob/master/src/main/scala/shark/execution/serialization/KryoSerializationWrapper.scala>  

Don't you think it's a good idea to have something like this inside the
framework itself? :)



--"
Reynold Xin <rxin@databricks.com>,"Sat, 24 May 2014 10:28:07 -0700",Re: Kryo serialization for closures: a workaround,dev@spark.apache.org,"Thanks for sending this in.

The ASF list doesn't support html so the formatting of the code is a little
messed up. For those who want to see the code in clearly formatted text, go
to
http://apache-spark-developers-list.1001551.n3.nabble.com/Kryo-serialization-for-closures-a-workaround-tp6787.html




"
Jim Donahue <jdonahue@adobe.com>,"Sat, 24 May 2014 23:58:03 +0000",Re: No output from Spark Streaming program with Spark 1.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","I looked at the Streaming UI for my job and it reports that it has
processed many batches, but that none of the batches had any records in
them. Unfortunately, thatâ€™s what I expected.  :-(

Iâ€™ve tried multiple test programs and Iâ€™m seeing the same thing.  The
Kafka sources are alive and well and the programs all worked on 0.9 from
Eclipse.  And thereâ€™s no indication of any failure â€” just no records are
being delivered.

Any ideas would be much appreciated â€¦


Thanks,

Jim


On 5/23/14, 7:29 PM, ""Tathagata Das"" <tathagata.das1565@gmail.com> wrote:

>Few more suggestions.
>1. See the web ui, is the system running any jobs? If not, then you may
>need to give the system more nodes. Basically the system should have more
>cores than the number of receivers.
>2. Furthermore there is a streaming specific web ui which gives more
>streaming specific data.
>
>
>On Fri, May 23, 2014 at 6:02 PM, Patrick Wendell <pwendell@gmail.com>
>wrote:
>
>> Also one other thing to try, try removing all of the logic form inside
>> of foreach and just printing something. It could be that somehow an
>> exception is being triggered inside of your foreach block and as a
>> result the output goes away.
>>
>> On Fri, May 23, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > Hey Jim,
>> >
>> > Do you see the same behavior if you run this outside of eclipse?
>> >
>> > Also, what happens if you print something to standard out when setting
>> > up your streams (i.e. not inside of the foreach) do you see that? This
>> > could be a streaming issue, but it could also be something related to
>> > the way it's running in eclipse.
>> >
>> > - Patrick
>> >
>> > On Fri, May 23, 2014 at 2:57 PM, Jim Donahue <jdonahue@adobe.com>
>>wrote:
>> >> IÂ¹m trying out 1.0 on a set of small Spark Streaming tests and am
>> running
>> >> into problems.  HereÂ¹s one of the little programs IÂ¹ve used for a
>>long
>> >> time â€¹ it reads a Kafka stream that contains Twitter JSON tweets and
>> does
>> >> some simple counting.  The program starts OK (it connects to the
>>Kafka
>> >> stream fine) and generates a stream of INFO logging messages, but
>>never
>> >> generates any output. :-(
>> >>
>> >> IÂ¹m running this in Eclipse, so there may be some class loading issue
>> >> (loading the wrong class or something like that), but IÂ¹m not seeing
>> >> anything in the console output.
>> >>
>> >> Thanks,
>> >>
>> >> Jim Donahue
>> >> Adobe
>> >>
>> >>
>> >>
>> >> val kafka_messages =
>> >>       KafkaUtils.createStream[Array[Byte], Array[Byte],
>> >> kafka.serializer.DefaultDecoder,
>>kafka.serializer.DefaultDecoder](ssc,
>> >> propsMap, topicMap, StorageLevel.MEMORY_AND_DISK)
>> >>
>> >>
>> >>      val messages = kafka_messages.map(_._2)
>> >>
>> >>
>> >>      val total = ssc.sparkContext.accumulator(0)
>> >>
>> >>
>> >>      val startTime = new java.util.Date().getTime()
>> >>
>> >>
>> >>      val jsonstream = messages.map[JSONObject](message =>
>> >>       {val string = new String(message);
>> >>       val json = new JSONObject(string);
>> >>       total += 1
>> >>       json
>> >>       }
>> >>     )
>> >>
>> >>
>> >>     val deleted = ssc.sparkContext.accumulator(0)
>> >>
>> >>
>> >>     val msgstream = jsonstream.filter(json =>
>> >>       if (!json.has(""delete"")) true else { deleted += 1; false}
>> >>       )
>> >>
>> >>
>> >>     msgstream.foreach(rdd => {
>> >>       if(rdd.count() > 0){
>> >>       val data = rdd.map(json => (json.has(""entities""),
>> >> json.length())).collect()
>> >>       val entities: Double = data.count(t => t._1)
>> >>       val fieldCounts = data.sortBy(_._2)
>> >>       val minFields = fieldCounts(0)._2
>> >>       val maxFields = fieldCounts(fieldCounts.size - 1)._2
>> >>       val now = new java.util.Date()
>> >>       val interval = (now.getTime() - startTime) / 1000
>> >>       System.out.println(now.toString)
>> >>       System.out.println(""processing time: "" + interval + "" seconds"")
>> >>       System.out.println(""total messages: "" + total.value)
>> >>       System.out.println(""deleted messages: "" + deleted.value)
>> >>       System.out.println(""message receipt rate: "" +
>> (total.value/interval)
>> >> + "" per second"")
>> >>       System.out.println(""messages this interval: "" + data.length)
>> >>       System.out.println(""message fields varied between: "" +
>>minFields
>> + ""
>> >> and "" + maxFields)
>> >>       System.out.println(""fraction with entities is "" + (entities /
>> >> data.length))
>> >>       }
>> >>     }
>> >>     )
>> >>
>> >>     ssc.start()
>> >>
>>

"
Tathagata Das <tathagata.das1565@gmail.com>,"Sat, 24 May 2014 19:46:47 -0700",Re: No output from Spark Streaming program with Spark 1.0,dev@spark.apache.org,"What does the kafka receiver status on the streaming UI say when you are
connected to the Kafka sources? Does it show any error?

Can you find out which machine the receiver is running and see the worker
logs for any exceptions / error messages? Try turning on the DEBUG level in
log4j.

TD

 thing.  The
 no records are
e
ng
is
o
 am
ed for a
weets and
g issue
seeing
"")
"
Nilesh <nilesh@nileshc.com>,"Sun, 25 May 2014 15:55:49 -0700 (PDT)",Re: all values for a key must fit in memory,dev@spark.incubator.apache.org,"I would like to clarify something. Matei mentioned that in Spark 1.0 groupBy
returns an (Key, Iterable[Value]) instead of (Key, Seq[Value]). Does this
also automatically assure us that the whole Iterable[Value] is not in fact
stored in memory? That is to say, with 1.0, will it be possible to do
groupByKey().values.map(x => while(x.hasNext) ... ) assuming x :
Iterable[Value] is larger than the RAM on a single machine? Or will this be
possible later, in subsequent versions?

Could you please propose a workaround for this for the meantime? I'm out of
ideas.

Thanks,
Nilesh



--

"
Andrew Ash <andrew@andrewash.com>,"Sun, 25 May 2014 19:27:19 -0400",Re: all values for a key must fit in memory,dev@spark.apache.org,"Hi Nilesh,

That change from Matei to change (Key, Seq[Value]) into (Key,
Iterable[Value]) was to enable the optimization in future releases without
breaking the API.  Currently though, all values on a single key are still
held in memory on a single machine.

The way I've gotten around this is by introducing another value to my Key
that goes from (Key) to (Key, randomValue % 10) for example.  Using this
you can further shard an individual key and keep from holding as much data
in memory at once.  The workaround is an ugly hack, but if it works then it
works.

Hope that helps!
Andrew



"
Andrew Ash <andrew@andrewash.com>,"Sun, 25 May 2014 19:27:19 -0400",Re: all values for a key must fit in memory,dev@spark.apache.org,"Hi Nilesh,

That change from Matei to change (Key, Seq[Value]) into (Key,
Iterable[Value]) was to enable the optimization in future releases without
breaking the API.  Currently though, all values on a single key are still
held in memory on a single machine.

The way I've gotten around this is by introducing another value to my Key
that goes from (Key) to (Key, randomValue % 10) for example.  Using this
you can further shard an individual key and keep from holding as much data
in memory at once.  The workaround is an ugly hack, but if it works then it
works.

Hope that helps!
Andrew



"
"""Shihaoliang (Shihaoliang)"" <shihaoliang@huawei.com>","Mon, 26 May 2014 00:39:30 +0000",credential transfer question,"""dev@spark.apache.org"" <dev@spark.apache.org>","

Hi,



I have view the code about UGI in spark. If spark interactive with kerberos HDFS, The spark will apply delegate token in scheduler side, and stored as credential into the UGI; And the credential will be transferred to spark executor so that they can authenticate the HDFS. My question is how does UGI do these credential transfer? does the credential transferred in encrypted way?



Thanks in advance.

Peter Shi

"
Nilesh <nilesh@nileshc.com>,"Sun, 25 May 2014 18:35:01 -0700 (PDT)",Re: all values for a key must fit in memory,dev@spark.incubator.apache.org,"Hi Andrew,

Thanks for the reply!

It's clearer about the API part now. That's what I wanted to know.

Wow, tuples, why didn't that occur to me. That's a lovely ugly hack. :) I
also came across something that solved my real problem though - the
RDD.toLocalIterator method from 1.0, the logic of which thankfully works
with 0.9.1 too, no new API changes there.

Cheers,
Nilesh



--

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 25 May 2014 19:26:50 -0700",Re: all values for a key must fit in memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","Nilesh - out of curiosity - what operation are you doing on the values
for the key?


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 25 May 2014 19:26:50 -0700",Re: all values for a key must fit in memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","Nilesh - out of curiosity - what operation are you doing on the values
for the key?


"
Nilesh <nilesh@nileshc.com>,"Sun, 25 May 2014 20:14:18 -0700 (PDT)",Re: all values for a key must fit in memory,dev@spark.incubator.apache.org,"Hi Patrick,

In this particular case, at the end of my tasks I have X different types of
keys. I need to write their values to X different files respectively. For
now I'm writing everything to the driver node's local FS.

While the number of key-value pairs can grow to millions (billions?), X is
more or less fixed at 25-30. A groupByKey followed by a map(x:
Iterable[Value] => x.foreach(destination.write(x)) would be great. But then
again, I'm not too sure about serialization issues and more likely that not
this idea would fail, but I'll try it out.

So the toLocalIterator implementation works OK for me here, though it might
turn out to be slow.

Cheers,
Nilesh

PS: Can't wait for 1.0! ^_^ Looks like it's been RC10 till now.



--

"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 26 May 2014 07:38:10 -0700",[VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This has a few important bug fixes on top of rc10:
SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
SPARK-1870: https://github.com/apache/spark/pull/848
SPARK-1897: https://github.com/apache/spark/pull/849

The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc11/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/tdas.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1019/

The documentation corresponding to this release can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Thursday, May 29, at 16:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

Changes to ML vector specification:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10

Changes to the Java API:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

Changes to the streaming API:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

Changes to the GraphX API:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

Other changes:
coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 26 May 2014 17:51:51 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"+1

Tested on Mac OS X and Windows.

Matei



version 1.0.0!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
at:
https://repository.apache.org/content/repositories/orgapachespark-1019/
http://people."
npanj <nitinpanj@gmail.com>,"Mon, 26 May 2014 18:12:25 -0700 (PDT)","Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed",dev@spark.incubator.apache.org,"I am seeing something strange with outerJoinVertices(and triangle count that
relies on this api):

Here is what I am doing:
1) Created a Graph with multiple partitions i.e created a graph with
minEdgePartitions(in api GraphLoader.edgeListFile), where minEdgePartitions
graph. Note: vertex attribute type is Int in this case
2) next I am building neighborhood ids by calling collectNeighborIds i.e.
returned vertex attribute type is Array[VertexId] ;
VertexRDD[Array[VertexId]]
3) finally join vertex ids from 2 to graph (generated in step 1) via
outerJoinVertices
4) Create a subgraph on joined graph from 3 where I only keep the edges with
ed.srcAttr != -1 && ed.dstAttr != -1 i.e. filter out null attr vertices
5) Finally checked the number edges left in subgraph from step4

I ran this program in a loop where minEdgePartitions is changed in each
iteration. When minEdgePartitions == 1 I see correct number of edges. When
minEdgePartitions == 2 result is ~1/2 number of edges; when
minEdgePartitions == 3 result is ~1/3 number of edges and so on

It seems that outerJoinVertices is returning srcAttr(and dstAtt) = nulll for
many attributes; and from numbers it seems that it might be returning null
for vertices residing on other partitions ?

Environment : I am using RC5; and 22 executers.

BUT I get correct number of edges in each iteration when I repeated my
experiment by keeping the vertex attribute type Int in step 2 (i.e. just
kept the number of vertices instead of array of vertices), which is same as
the type vertex attribute in graph before join.

Is this a know bug fixed recently? or are we supposed to set some flags when
updating the vertex attribute type? 



--

"
npanj <nitinpanj@gmail.com>,"Mon, 26 May 2014 18:26:02 -0700 (PDT)","Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed",dev@spark.incubator.apache.org,"Correction: in step 4) predicate is  ed.srcAttr != null && ed.dstAttr != null
(used -1, when when changed attr type to Int )



--

"
Ankur Dave <ankurdave@gmail.com>,"Mon, 26 May 2014 18:32:11 -0700","Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed",dev@spark.apache.org,"This is probably due to
SPARK-1931<https://issues.apache.org/jira/browse/SPARK-1931>,
which I just fixed in PR #885 <https://github.com/apache/spark/pull/885>.
Is the problem resolved if you use the current Spark master?

Ankur <http://www.ankurdave.com/>
"
Ankur Dave <ankurdave@gmail.com>,"Mon, 26 May 2014 18:32:11 -0700","Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed",dev@spark.apache.org,"This is probably due to
SPARK-1931<https://issues.apache.org/jira/browse/SPARK-1931>,
which I just fixed in PR #885 <https://github.com/apache/spark/pull/885>.
Is the problem resolved if you use the current Spark master?

Ankur <http://www.ankurdave.com/>
"
ankurdave <ankurdave@gmail.com>,"Mon, 26 May 2014 18:50:51 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.incubator.apache.org,"-1

I just fixed  SPARK-1931 <https://issues.apache.org/jira/browse/SPARK-1931> 
, which was a critical bug in Graph#partitionBy. Since this is an important
part of the GraphX API, I think Spark 1.0.0 should include the fix:
https://github.com/apache/spar"
Patrick Wendell <pwendell@gmail.com>,"Mon, 26 May 2014 22:47:29 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Ankur,

That does seem like a good fix, but right now we are only blocking the
release on major regressions that affect all components. So I don't
think this is sufficient to block it from going forward and cutting a
new candidate. This is because we are in the very late stage of the
release.

We can slot that for the 1.0.1 release and merge it into the 1.0
branch so people can get access to the fix easily.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 26 May 2014 22:57:39 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"I think the question for me would be does this only happen when you call partitionBy, or always? And how common do you expect calls to partitionBy to be? If we can wait for 1.0.1 then I’d wait on this one.

Matei



<https://issues.apache.org/jira/browse/SPARK-1931>
important
http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-RC11-tp6797p6802.html
Nabble.com.


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 26 May 2014 22:58:01 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>",1
npanj <nitinpanj@gmail.com>,"Mon, 26 May 2014 23:03:43 -0700 (PDT)","Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed",dev@spark.incubator.apache.org,"Thanks Ankur. With your fix I see expected results.



--

"
Ankur Dave <ankurdave@gmail.com>,"Tue, 27 May 2014 09:26:24 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"0

OK, I withdraw my downvote.

Ankur <http://www.ankurdave.com/>
"
Sean Owen <sowen@cloudera.com>,"Tue, 27 May 2014 23:00:42 +0100","Kafka + Spark Streaming and NoSuchMethodError, related to Manifest / reflection?",dev@spark.apache.org,"I'd like to resurrect this thread:
http://mail-archives.apache.org/mod_mbox/spark-user/201403.mbox/%3C6D657D19-1ECF-4E92-BF15-CC4762EF98BF@thekratos.com%3E

Basically when you call this particular Java-flavored overloading of
KafkaUtils.createStream:
https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala#L133

... you get

java.lang.NoSuchMethodException:
java.lang.Object.<init>(kafka.utils.VerifiableProperties)
        at java.lang.Class.getConstructor0(Class.java:2763)
        at java.lang.Class.getConstructor(Class.java:1693)
        at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:108)

This doesn't appear to be a version issue. It doesn't appear when
calling other versions of this method. Other overloadings work (well,
have other issues).

Something is making it try to instantiate java.lang.Object as if it's
a Decoder class.

I am wondering about this code at
https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala#L148

    implicit val keyCmd: Manifest[U] =
implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[U]]
    implicit val valueCmd: Manifest[T] =
implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[T]]

... where U and T are key/value Decoder types. I don't know enough
Scala to fully understand this, but is it possible this causes the
reflective call later to lose the type and try to instantiate Object?
The AnyRef made me wonder.

@tdas I'm hoping you might have some insight as it came in this commit
in January:
https://github.com/apache/spark/commit/aa99f226a691ddcb4442d60f4cd4908f434cc4ce

I'll file a JIRA if it's legitimate; just asking first.

"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Tue, 27 May 2014 18:01:20 -0400",Clearspring Analytics Version,dev@spark.apache.org,"Hi,

It looks like the version of Clearspring's stream analytics class in 1.0
branch and master is 2.5

There were some significant bug fixes in 2.6 and version 2.7 is just out
now as well.

Are there any plans to upgrade?

The QDigest deserialization code in 2.5 seems to have bugs that are fixed
in 2.6 and higher.


      <dependency>
        <groupId>com.clearspring.analytics</groupId>
        <artifactId>stream</artifactId>
        <version>2.5.1</version>-
      </dependency>



SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Reynold Xin <rxin@databricks.com>,"Tue, 27 May 2014 15:02:52 -0700",Re: Clearspring Analytics Version,dev@spark.apache.org,"Would you like to submit a pull request to update it?

Also in the latest version HyperLogLog is serializable. That means we can
get rid of the SerializableHyperLogLog class. (and move to use
HyperLogLogPlus).







"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Tue, 27 May 2014 18:11:01 -0400",Re: Clearspring Analytics Version,dev@spark.apache.org,"Great, I will submit the pull request.

Any objection to version 2.7?

-Suren







-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Reynold Xin <rxin@databricks.com>,"Tue, 27 May 2014 15:13:07 -0700",Re: Clearspring Analytics Version,dev@spark.apache.org,"2.7 sounds good. I was actually waiting for 2.7 to come out to post a JIRA
(mainly for the serializable HyperLogLogPlus class).



"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Tue, 27 May 2014 18:36:51 -0400",Re: Clearspring Analytics Version,dev@spark.apache.org,"Reynold,

My environment is not quite set up to build Spark core properly and I'm
having various dependency issues running sbt/sbt assembly.

Any change you could go ahead and submit a pull request for this if it's
easy for you? :-)

-Suren







-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 27 May 2014 15:53:21 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,1
Holden Karau <holden@pigscanfly.ca>,"Tue, 27 May 2014 16:09:00 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"+1 (I did some very basic testing with PySpark & Pandas on rc11)






-- 
Cell : 425-233-8271
"
Reynold Xin <rxin@databricks.com>,"Tue, 27 May 2014 16:56:12 -0700",Re: Clearspring Analytics Version,dev@spark.apache.org,"Posted a question on streamlib user group about API compatibility:

https://groups.google.com/forum/#!topic/stream-lib-user/4VDeKcPiTJU



"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Wed, 28 May 2014 09:25:00 +0900",About JIRA SPARK-1825,<dev@spark.apache.org>,"Could somebody please review and fix
https://issues.apache.org/jira/browse/SPARK-1825 ?

It's a cross-platform issue.

I've fixed the Spark source code based on rc5 and it's working for me now.
but totally not sure whether I've done correctly, since I'm almost new to
Spark and don't know much about the source code or API of both Spark and
YARN.

 

It would be nice that the issue be resolved before the 1.0.0 release.

Without the fix, developing a Spark application on Windows will be
inconvenient.

 

I have the source code I've fixed, but I don't' know how to upload it to be
shown by other, more Spark-capable developers. (And I'm not sure that my fix
is correct)

 

Thanks in advance. 

 

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 27 May 2014 17:52:52 -0700",Re: About JIRA SPARK-1825,dev@spark.apache.org,"Hei Taeyun, have you sent a pull request for this fix? We can review it there.

It’s too late to merge anything but blockers for 1.0.0 but we can do it for 1.0.1 or 1.1, depending how big the patch is.

Matei


now.
to
and
to be
my fix


"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Wed, 28 May 2014 10:10:38 +0900",RE: About JIRA SPARK-1825,<dev@spark.apache.org>,"I'm afraid I don't know how to send a 'pull request'. (Sorry for my
ignorance)
I've modified the source code on my PC.
I can learn how to send a pull request, or I can attach the modified source
code on the mail right now(only 2 scala files).
Which way is preferable?


there.

It's too late to merge anything but blockers for 1.0.0 but we can do it for
1.0.1 or 1.1, depending how big the patch is.

Matei




"
Reynold Xin <rxin@databricks.com>,"Tue, 27 May 2014 18:12:18 -0700",Re: About JIRA SPARK-1825,dev@spark.apache.org,"It is actually pretty simple. You will first need to fork Spark on github,
and then push your changes to it, and then follow:
https://help.github.com/articles/using-pull-requests




"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Wed, 28 May 2014 14:01:44 +0900",RE: About JIRA SPARK-1825,<dev@spark.apache.org>,"Sent a pull request. (https://github.com/apache/spark/pull/899)
Thanks.

github, and then push your changes to it, and then follow:
https://help.github.com/articles/using-pull-requests




release.



"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Tue, 27 May 2014 22:08:12 -0700",FYI -- javax.servlet dependency issue workaround,dev@spark.apache.org,"Hi all,

I had some trouble compiling an application (Shark) against Spark 1.0,
where Shark had a runtime exception (at the bottom of this message) because
it couldn't find the javax.servlet classes.  SBT seemed to have trouble
downloading the servlet APIs that are dependencies of Jetty (used by the
Spark web UI), so I had to manually add them to the application's build
file:

libraryDependencies += ""org.mortbay.jetty"" % ""servlet-api"" % ""3.0.20100224""

Not exactly sure why this happens but thought it might be useful in case
others run into the same problem.

-Kay

-------------

Exception in thread ""main"" java.lang.NoClassDefFoundError:
javax/servlet/FilterRegistration

at
org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:136)

at
org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:129)

at
org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:98)

at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:98)

at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:89)

at org.apache.spark.ui.WebUI.attachPage(WebUI.scala:64)

at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)

at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at org.apache.spark.ui.WebUI.attachTab(WebUI.scala:57)

at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:66)

at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:60)

at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)

at org.apache.spark.SparkContext.<init>(SparkContext.scala:222)

at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)

at shark.SharkContext.<init>(SharkContext.scala:42)

at shark.SharkContext.<init>(SharkContext.scala:61)

at shark.SharkEnv$.initWithSharkContext(SharkEnv.scala:78)

at shark.SharkEnv$.init(SharkEnv.scala:38)

at shark.SharkCliDriver.<init>(SharkCliDriver.scala:280)

at shark.SharkCliDriver$.main(SharkCliDriver.scala:162)

at shark.SharkCliDriver.main(SharkCliDriver.scala)

Caused by: java.lang.ClassNotFoundException:
javax.servlet.FilterRegistration

at java.net.URLClassLoader$1.run(URLClassLoader.java:366)

at java.net.URLClassLoader$1.run(URLClassLoader.java:355)

at java.security.AccessController.doPrivileged(Native Method)

at java.net.URLClassLoader.findClass(URLClassLoader.java:354)

at java.lang.ClassLoader.loadClass(ClassLoader.java:425)

at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)

at java.lang.ClassLoader.loadClass(ClassLoader.java:358)

... 23 more
"
Prashant Sharma <scrapcodes@gmail.com>,"Wed, 28 May 2014 10:47:36 +0530",Re: FYI -- javax.servlet dependency issue workaround,dev@spark.apache.org,"Also just for sake of completeness, sometimes the desired dependency might
just be an older version in that case even if you include it like above it
may get evicted (Sbt's default strategy for conflict manager is to choose
the latest version).

So to further ensure that it does include it. We can
libraryDependencies += ""org.mortbay.jetty"" % ""servlet-api"" % ""3.0.20100224""
*force()*

force it.

Thanks

Prashant Sharma



"
Sean Owen <sowen@cloudera.com>,"Wed, 28 May 2014 10:08:10 +0100",Re: FYI -- javax.servlet dependency issue workaround,dev@spark.apache.org,"This class was introduced in Servlet 3.0. We have in the dependency
tree some references to Servlet 2.5 and Servlet 3.0. The latter is a
superset of the former. So we standardized on depending on Servlet
3.0.

At least, that seems to have been successful in the Maven build, but
this is just evidence that the SBT build ends up including Servlet 2.5
dependencies.

You shouldn't have to work around it in this way. Let me see if I can
debug and propose the right exclusion for SBT.

(Related: is the SBT build going to continue to live separately from
Maven, or is it going to be auto-generated? that is -- worth fixing
this?)



"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 28 May 2014 11:47:11 +0200",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1
Built and tested locally on Mac OS X
Built and tested on AWS Ubuntu, with and without Hive support
Ran production jobs including MLlib and SparkSQL/HiveContext successfully



"
dataginjaninja <rickett.stephanie@gmail.com>,"Wed, 28 May 2014 05:03:45 -0700 (PDT)",Standard preprocessing/scaling,dev@spark.incubator.apache.org,"I searched on this, but didn't find anything general so I apologize if this
has been addressed. 

Many algorithms (SGD, SVM...) either will not converge or will run forever
if the data is not scaled. Sci-kit has  preprocessing
<http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html>  
that will subtract the mean and divide by standard dev. Of course there are
a few options with it as well.

Is there something in the works for this?



--

"
Will Benton <willb@redhat.com>,"Wed, 28 May 2014 12:34:53 -0400 (EDT)",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"+1

I made the necessary interface changes to my apps that use MLLib and tested all of my code against rc11 on Fedora 20 and OS X 10.9.3.  (The Fedora Rawhide package remains at 0.9.1 pending some additional dependency packaging work.)


best,
wb


	by mi"
Bharath Ravi Kumar <reachbach@gmail.com>,"Wed, 28 May 2014 23:48:45 +0530",LogisticRegression: Predicting continuous outcomes,dev@spark.apache.org,"I'm looking to reuse the LogisticRegression model (with SGD) to predict a
real-valued outcome variable. (I understand that logistic regression is
generally applied to predict binary outcome, but for various reasons, this
model suits our needs better than LinearRegression). Related to that I have
the following questions:

1) Can the current LogisticRegression model be used as is to train based on
binary input (i.e. explanatory) features, or is there an assumption that
the explanatory features must be continuous?

2) I intend to reuse the current class to train a model on LabeledPoints
where the label is a real value (and not 0 / 1). I'd like to know if
invoking setValidateData(false) would suffice or if one must override the
validator to achieve this.

3) I recall seeing an experimental method on the class (
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/LogisticRegression.scala)
the model is trained on real valued labels, would clearing this flag
suffice to predict an outcome that is continous in nature?

Thanks,
Bharath

P.S: I'm writing to dev@ and not user@ assuming that lib changes might be
necessary. Apologies if the mailing list is incorrect.
"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Wed, 28 May 2014 20:07:00 +0000",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","Pulled down, compiled, and tested examples on OS X and ubuntu.
Deployed app we are building on spark and poured data through it.

+1

Sean


ote:

1.0.0!
97cdb42f809cb71113a1db4194c21372242a
m-09-to-10
de.html#upgrading-from-pre-10-versions-of-spark
g-guide.html#migration-guide-from-091-or-below-to-1x
uide.html#upgrade-guide-from-spark-091


"
Will Benton <willb@redhat.com>,"Wed, 28 May 2014 16:12:43 -0400 (EDT)","ContextCleaner, weak references, and serialization",dev@spark.apache.org,"Friends, 

For context (so to speak), I did some work in the 0.9 timeframe to fix SPARK-897 (provide immediate feedback when closures aren't serializable) and SPARK-729 (make sure that free variables in closures are captured when the RDD transformations are declared).

I currently have a branch addressing SPARK-897 that builds and tests out against 0.9, 1.0, and master last I checked (https://github.com/apache/spark/pull/143).  My branch addressing SPARK-729 builds on my SPARK-897 branch, and passed the test suite in 0.9[1].  However, some things that changed or were added in 1.0 wound up depending on the old behavior.  I've been working on other things lately but would like to get these issues fixed after 1.0 goes final so I was hoping to get a bit of discussion on the best way to go forward with an issue that I haven't solved yet:

ContextCleaner uses weak references to track broadcast variables.  Because weak references obviously don't track cloned objects (or those that have been serialized and deserialized), capturing free variables in closures in the obvious way (i.e. by replacing the closure with a copy that has been serialized and deserialized) results in an undesirable situation:  we might have, e.g., live HTTP broadcast variable objects referring to filesystem resources that could be cleaned at any time because the objects that they were cloned from have become only weakly reachable.

To be clear, this isn't a problem now; it's only a problem for the way I'm proposing to fix SPARK-729.  With that said, I'm wondering if it would make more sense to fix this problem by adding a layer of indirection to reference count external and persisting resources rather than the objects that putatively own them, or if it would make more sense to take a more sophisticated (but also more potentially fragile) approach to ensuring variable capture.



thanks,
wb


[1] Serializing closures also created or uncovered a PySpark issue in 0.9 (and presumably in later versions as well) that requires further investigation, but my patch did include a workaround; here are the details: https://issues.apache.org/jira/browse/SPARK-1454

"
Will Benton <willb@redhat.com>,"Wed, 28 May 2014 16:13:06 -0400 (EDT)",Re: Kryo serialization for closures: a workaround,dev@spark.apache.org,"This is an interesting approach, Nilesh!

Someone will correct me if I'm wrong, but I don't think this could go into ClosureCleaner as a default behavior (since Kryo apparently breaks on some classes that depend on custom Java serializers, as has come up on the list recently).  But it does seem like having a function in Spark that did this for closures more transparently (to be called explicitly by clients in problem cases) could be pretty useful.


best,
wb


	by minotaur.apache.org (Postfix) with SMTP id 015EB1084B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 21:13:26 +0000 (UTC)
Received: (qmail 78930 invoked by uid 500); 28 May 2014 21:13:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78873 invoked by uid 500); 28 May 2014 21:13:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78865 invoked by uid 99); 28 May 2014 21:13:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 21:13:25 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tgraves_cs@yahoo.com designates 216.109.114.170 as permitted sender)
Received: from [216.109.114.170] (HELO nm43-vm9.bullet.mail.bf1.yahoo.com) (216.109.114.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 21:13:20 +0000
Received: from [98.139.212.153] by nm43.bullet.mail.bf1.yahoo.com with NNFMP; 28 May 2014 21:12:56 -0000
Received: from [98.139.212.194] by tm10.bullet.mail.bf1.yahoo.com with NNFMP; 28 May 2014 21:12:56 -0000
Received: from [127.0.0.1] by omp1003.mail.bf1.yahoo.com with NNFMP; 28 May 2014 21:12:56 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 326480.83815.bm@omp1003.mail.bf1.yahoo.com
Received: (qmail 62317 invoked by uid 60001); 28 May 2014 21:12:56 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1401311576; bh=4FCJy5NQ6yXI12P4LZZJJhPdb5T/WkSVdXECbt+bYM4=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=6khh/YrYR1VVw31CsS8swiIyKZRAa+38KeizbH36eP6HNoQBcqQZ+C7Ub+4DtDlgcZ7RTBuQ8Zbm6XHaznRllrK3ayDfg7Cw/AqD4sypTqJWXHINFhVRVuMCWB1b+h/Klssd3hUUexgKIcO/a4pEGXp+gr3uc2X8x2ElyIeAZF0=
X-YMail-OSG: V.nCaSoVM1mGcDLUxb7Y1T62sQvxssz2Eaam39LBn0QV_CF
 49UXJHpL4dyxLfYnprUDD7lurbVTU89STJ3Hca6rAnKRADdnaqhOSXttxsaW
 mZ7WaogEMAVt8qEl8NJL3Z6z.x2pOgv4jozIPv55NX8ILHe3lowY0j6Osdns
 g239jHqfkzBX2pC1gTciqCm3E5WMQ6FUNDzEwd67U_VINh.XYqwtCsS2QYNI
 bzYdjXEhPKTH1SM8Pm8iRhHGxfVJkIxe_SWVpZy.oTL46bv0flv0.4BYygKP
 gDO75ICqhiMRIEJWBeBjVKFaeMfbcaLG02GMXbmSvFdN4JF1GETlNvmOhr0_
 r2ppUVjTE_OYLdcleX8vuWEWCl4vDBgavqa7P3uZFdGCTHs1lG8eX605ynCE
 BRXjRm0imGLOHiHJM6F4P5mkoG1RdXCwhjYZEq1TzHzY_WUq378y1f80Af5b
 ebMoVoiJrXqU9sbuDNgsXviwrIrvXxQu3LqcboA17ZlBnVRnSbC68pzWISnK
 mKLYCbcLjuVoMYfagIld1JYYe6D0QWEs6FF7t14VN1ocCC3dF6meaAvCS0ik
 ySef208fPqrSFJ_ukF2CnrZewQW3PXQ54Va_wu_bqXnMRWSU5GouQeZVPj3D
 S6v7.Jh0P2ltezE.v0S8HpOi6jdFJDnuYtYCtMr.l0E5b6ouoSoIF.BMJ6B0
 4V1CPWA--
Received: from [204.11.79.50] by web140105.mail.bf1.yahoo.com via HTTP; Wed, 28 May 2014 14:12:56 PDT
X-Rocket-MIMEInfo: 002.001,KzEuIFRlc3RlZCBzcGFyayBvbiB5YXJuIChjbHVzdGVyIG1vZGUsIGNsaWVudCBtb2RlLCBweXNwYXJrLCBzcGFyay1zaGVsbCkgb24gaGFkb29wIDAuMjMgYW5kIDIuNC7CoAoKVG9tCgoKT24gV2VkbmVzZGF5LCBNYXkgMjgsIDIwMTQgMzowNyBQTSwgU2VhbiBNY05hbWFyYSA8U2Vhbi5NY05hbWFyYUBXZWJ0cmVuZHMuY29tPiB3cm90ZToKIAoKClB1bGxlZCBkb3duLCBjb21waWxlZCwgYW5kIHRlc3RlZCBleGFtcGxlcyBvbiBPUyBYIGFuZCB1YnVudHUuCkRlcGxveWVkIGFwcCB3ZSBhcmUgYnVpbGRpbmcBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com> <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com> 
Message-ID: <1401311576.69780.YahooMailNeo@web140105.mail.bf1.yahoo.com>
Date: Wed, 28 May 2014 14:12:56 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
To: ""dev@spark.apache.org"" <dev@spark.apache.org>
In-Reply-To: <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary=""66961340-458033912-1401311576=:69780""
X-Virus-Checked: Checked by ClamAV on apache.org

--66961340-458033912-1401311576=:69780
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

+1. Tested spark on yarn (cluster mode, client mode, pyspark, spark-shell) =
down, compiled, and tested examples on OS X and ubuntu.=0ADeployed app we a=
re building on spark and poured data through it.=0A=0A+1=0A=0ASean=0A=0A=0A=
park version 1.0.0!=0A> =0A> This has a few important bug fixes on top of r=
c10:=0A> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/85=
3=0A> SPARK-1870: https://github.com/apache/spark/pull/848=0A> SPARK-1897: =
https://github.com/apache/spark/pull/849=0A> =0A> The tag to be voted on is=
 v1.0.0-rc11 (commit c69d97cd):=0A> https://git-wip-us.apache.org/repos/asf=
?p=3Dspark.git;a=3Dcommit;h=3Dc69d97cdb42f809cb71113a1db4194c21372242a=0A> =
=0A> The release files, including signatures, digests, etc. can be found at=
:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc11/=0A> =0A> Release=0A =
artifacts are signed with the following key:=0A> https://people.apache.org/=
keys/committer/tdas.asc=0A> =0A> The staging repository for this release ca=
n be found at:=0A> https://repository.apache.org/content/repositories/orgap=
achespark-1019/=0A> =0A> The documentation corresponding to this release ca=
n be found at:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/=0A=
his package as Apache Spark 1.0.0=0A> [ ] -1 Do not release this package be=
cause ...=0A> =0A> To learn more about Apache Spark, please see=0A> http://=
spark.apache.org/=0A> =0A> =3D=3D API Changes =3D=3D=0A> We welcome users t=
o compile Spark applications against 1.0. There are=0A> a few API changes i=
n this release. Here are links to the associated=0A> upgrade guides - user =
facing changes have been kept as small as=0A> possible.=0A> =0A> Changes to=
 ML vector specification:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc=
11-docs/mllib-guide.html#from-09-to-10=0A> =0A> Changes to the Java API:=0A=
de.html#upgrading-from-pre-10-versions-of-spark=0A> =0A> Changes to the str=
eaming API:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/stream=
ing-programming-guide.html#migration-guide-from-091-or-below-to-1x=0A> =0A>=
 Changes to the GraphX API:=0A> http://people.apache.org/~tdas/spark-1.0.0-=
rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091=0A> =
=0A> Other changes:=0A> coGroup and related functions now return Iterable[T=
] instead of Seq[T]=0A> =3D=3D> Call toSeq on the result to restore the old=
 behavior=0A> =0A> SparkContext.jarOfClass returns Option[String] instead o=
f Seq[String]=0A> =3D=3D> Call toSeq on the result to restore old behavior
--66961340-458033912-1401311576=:69780--

"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 28 May 2014 18:58:21 -0700",Re: LogisticRegression: Predicting continuous outcomes,dev@spark.apache.org,"Please find my comments inline. -Xiangrui


Binary features should be okay.


I'm not sure whether the loss function makes sense with real valued
labels. We may use the assumption that the label is binary to simplify
the computation of loss. You can take a look at the code and see
whether the loss function fits your model.


If you clear the threshold, it outputs the raw scores from the
logistic function.


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 28 May 2014 19:03:24 -0700",Re: Standard preprocessing/scaling,dev@spark.apache.org,"RowMatrix has a method to compute column summary statistics. There is
a trade-off here because centering may densify the data. A utility
function that centers data would be useful for dense datasets.
-Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 28 May 2014 19:03:24 -0700",Re: Standard preprocessing/scaling,dev@spark.apache.org,"RowMatrix has a method to compute column summary statistics. There is
a trade-off here because centering may densify the data. A utility
function that centers data would be useful for dense datasets.
-Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 28 May 2014 19:04:57 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"+1

Tested apps with standalone client mode and yarn cluster and client modes.

Xiangrui


"
Andy Konwinski <andykonwinski@gmail.com>,"Wed, 28 May 2014 19:36:33 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,1
Christopher Nguyen <ctn@adatao.com>,"Wed, 28 May 2014 19:46:27 -0700",Re: LogisticRegression: Predicting continuous outcomes,dev@spark.apache.org,"Bharath, (apologies if you're already familiar with the theory): the
proposed approach may or may not be appropriate depending on the overall
transfer function in your data. In general, a single logistic regressor
cannot approximate arbitrary non-linear functions (of linear combinations
of the inputs). You can review works by, e.g., Hornik and Cybenko in the
late 80's to see if you need something more, such as a simple, one
hidden-layer neural network.

This is a good summary:
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&rep=rep1&type=pdf

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 28 May 2014 20:55:30 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"+1
Pulled & built on MacOS X, EC2 Amazon Linux
Ran test programs on OS X, 5 node c3.4xlarge cluster
Cheers
<k/>



"
DB Tsai <dbtsai@stanford.edu>,"Wed, 28 May 2014 21:23:15 -0700",Re: Standard preprocessing/scaling,dev@spark.apache.org,"Sometimes for this case, I will just standardize without centerization. I
still get good result.

Sent from my Google Nexus 5

"
DB Tsai <dbtsai@stanford.edu>,"Wed, 28 May 2014 21:23:15 -0700",Re: Standard preprocessing/scaling,dev@spark.apache.org,"Sometimes for this case, I will just standardize without centerization. I
still get good result.

Sent from my Google Nexus 5

"
Kevin Markey <kevin.markey@oracle.com>,"Wed, 28 May 2014 22:38:32 -0600",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"+1

Built -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0
Ran current version of one of my applications on 1-node pseudocluster 
(sorry, unable to test on full cluster).
yarn-cluster mode
Ran regression tests.

Thanks
Kevin



"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Thu, 29 May 2014 15:46:18 +0900",Suggestion: RDD cache depth,<dev@spark.apache.org>,"It would be nice if the RDD cache() method incorporate a depth information.

That is,

 

void test()
{

JavaRDD<.> rdd = .;

 

rdd.cache();  // to depth 1. actual caching happens.

rdd.cache();  // to depth 2. Nop as long as the storage level is the same.
Else, exception.

.

rdd.uncache();  // to depth 1. Nop.

rdd.uncache();  // to depth 0. Actual unpersist happens.

}

 

This can be useful when writing code in modular way.

When a function receives an rdd as an argument, it doesn't necessarily know
the cache status of the rdd.

But it could want to cache the rdd, since it will use the rdd multiple
times.

But with the current RDD API, it cannot determine whether it should
unpersist it or leave it alone (so that caller can continue to use that rdd
without rebuilding).

 

Thanks.

 

"
Michael Malak <michaelmalak@yahoo.com>,"Wed, 28 May 2014 23:48:29 -0700 (PDT)",GraphX triplets on 5-node graph,"""dev@spark.apache.org"" <dev@spark.apache.org>","Shouldn't I be seeing N2 and N4 in the output below? (Spark 0.9.0 REPL) Or am I missing something fundamental?


val nodes = sc.parallelize(Array((1L, ""N1""), (2L, ""N2""), (3L, ""N3""), (4L, ""N4""), (5L, ""N5""))) 
val edges = sc.parallelize(Array(Edge(1L, 2L, ""E1""), Edge(1L, 3L, ""E2""), Edge(2L, 4L, ""E3""), Edge(3L, 5L, ""E4""))) 
Graph(nodes, edges).triplets.collect 
res1: Array[org.apache.spark.graphx.EdgeTriplet[String,String]] = Array(((1,N1),(3,N3),E2), ((1,N1),(3,N3),E2), ((3,N3),(5,N5),E4), ((3,N3),(5,N5),E4))

"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 28 May 2014 23:53:37 -0700",Re: Suggestion: RDD cache depth,dev@spark.apache.org,"This is a pretty cool idea — instead of cache depth I’d call it something like reference counting. Would you mind opening a JIRA issue about it?

The issue of really composing together libraries that use RDDs nicely isn’t fully explored, but this is certainly one thing that would help with it. I’d love to look at other ones too, e.g. how to allow libraries to share scans over the same dataset.

Unfortunately using multiple cache() calls for this is probably not feasible because it would change the current meaning of multiple calls. But we can add a new API, or a parameter to the method.

Matei


information.
same.
know
that rdd


"
Reynold Xin <rxin@databricks.com>,"Wed, 28 May 2014 23:53:32 -0700",Re: GraphX triplets on 5-node graph,"dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>","Take a look at this one: https://issues.apache.org/jira/browse/SPARK-1188

It was an optimization that added user inconvenience. We got rid of that
now in Spark 1.0.




"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Thu, 29 May 2014 16:19:26 +0900",RE: Suggestion: RDD cache depth,<dev@spark.apache.org>,"Opened a JIRA issue. (https://issues.apache.org/jira/browse/SPARK-1962)

Thanks.


like reference counting. Would you mind opening a JIRA issue about it?

The issue of really composing together libraries that use RDDs nicely isn't
fully explored, but this is certainly one thing that would help with it. I'd
love to look at other ones too, e.g. how to allow libraries to share scans
over the same dataset.

Unfortunately using multiple cache() calls for this is probably not feasible
because it would change the current meaning of multiple calls. But we can
add a new API, or a parameter to the method.

Matei


information.


"
Bharath Ravi Kumar <reachbach@gmail.com>,"Thu, 29 May 2014 15:41:44 +0530",Re: LogisticRegression: Predicting continuous outcomes,dev@spark.apache.org,"Xiangrui, Christopher,

Thanks for responding.  I'll  go through the code in detail to evaluate if
the loss function used is suitable to our dataset. I'll also go through the
referred paper since I was unaware of the underlying theory. Thanks again.

-Bharath



"
"""Lizhengbing (bing, BIPA)"" <zhengbing.li@huawei.com>","Thu, 29 May 2014 12:29:33 +0000","Please change instruction about  ""Launching Applications Inside the
 Cluster""","""dev@spark.apache.org"" <dev@spark.apache.org>","The instruction address is in http://spark.apache.org/docs/0.9.0/spark-standalone.html#launching-applications-inside-the-cluster or http://spark.apache.org/docs/0.9.1/spark-standalone.html#launching-applications-inside-the-cluster

Origin instruction is:
""./bin/spark-class org.apache.spark.deploy.Client launch
   [client-options] \
   <cluster-url> <application-jar-url> <main-class> \
   [application-options] ""

If I follow this instruction, I will not run my program deployed in a spark standalone cluster properly.

Based on source code, This instruction should be changed to
""./bin/spark-class org.apache.spark.deploy.Client [client-options] launch \
   <cluster-url> <application-jar-url> <main-class> \
   [application-options] ""

That is to say: [client-options] must be put ahead of launch
"
dataginjaninja <rickett.stephanie@gmail.com>,"Thu, 29 May 2014 05:32:16 -0700 (PDT)",Re: Standard preprocessing/scaling,dev@spark.incubator.apache.org,"I do see the issue for centering sparse data. Actually, the centering is less
important than the scaling by the standard deviation. Not having unit
variance causes the convergence issues and long runtimes. 

RowMatrix will compute variance of a column?



--

"
dataginjaninja <rickett.stephanie@gmail.com>,"Thu, 29 May 2014 08:54:55 -0700 (PDT)",Timestamp support in v1.0,dev@spark.incubator.apache.org,"Can anyone verify which rc  [SPARK-1360] Add Timestamp Support for SQL #275
<https://github.com/apache/spark/pull/275>   is included in? I am running
rc3, but receiving errors with TIMESTAMP as a datatype in my Hive tables
when trying to use them in pyspark.

*The error I get:
*
14/05/29 15:44:47 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
aol
14/05/29 15:44:48 INFO ParseDriver: Parse Completed
14/05/29 15:44:48 INFO metastore: Trying to connect to metastore with URI
thrift:
14/05/29 15:44:48 INFO metastore: Waiting 1 seconds before next connection
attempt.
14/05/29 15:44:49 INFO metastore: Connected to metastore.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/spark-1.0.0-rc3/python/pyspark/sql.py"", line 189, in hql
    return self.hiveql(hqlQuery)
  File ""/opt/spark-1.0.0-rc3/python/pyspark/sql.py"", line 183, in hiveql
    return SchemaRDD(self._ssql_ctx.hiveql(hqlQuery), self)
  File
""/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"",
line 537, in __call__
  File
""/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"", line
300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o14.hiveql.
: java.lang.RuntimeException: Unsupported dataType: timestamp

*The table I loaded:*
DROP TABLE IF EXISTS aol; 
CREATE EXTERNAL TABLE aol (
	userid STRING,
	query STRING,
	query_time TIMESTAMP,
	item_rank INT,
	click_url STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/tmp/data/aol';

*The pyspark commands:*
from pyspark.sql import HiveContext
hctx= HiveContext(sc)
results = hctx.hql(""SELECT COUNT(*) FROM aol"").collect()






--

"
Andrew Ash <andrew@andrewash.com>,"Thu, 29 May 2014 09:09:46 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"I can confirm that the commit is included in the 1.0.0 release candidates
(it was committed before branch-1.0 split off from master), but I can't
confirm that it works in PySpark.  Generally the Python and Java interfaces
lag a little behind the Scala interface to Spark, but we're working to keep
that diff much smaller going forward.

Can you try the same thing in Scala?



"
Andrew Ash <andrew@andrewash.com>,"Thu, 29 May 2014 09:09:46 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"I can confirm that the commit is included in the 1.0.0 release candidates
(it was committed before branch-1.0 split off from master), but I can't
confirm that it works in PySpark.  Generally the Python and Java interfaces
lag a little behind the Scala interface to Spark, but we're working to keep
that diff much smaller going forward.

Can you try the same thing in Scala?



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 29 May 2014 09:14:09 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

I spun up a few EC2 clusters and ran my normal audit checks. Tests
passing, sigs, CHANGES and NOTICE look good

Thanks TD for helping cut this RC!


"
Michael Armbrust <michael@databricks.com>,"Thu, 29 May 2014 09:54:18 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"Thanks for reporting this!

https://issues.apache.org/jira/browse/SPARK-1964
https://github.com/apache/spark/pull/913

If you could test out that PR and see if it fixes your problems I'd really
appreciate it!

Michael



"
Michael Armbrust <michael@databricks.com>,"Thu, 29 May 2014 09:54:18 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"Thanks for reporting this!

https://issues.apache.org/jira/browse/SPARK-1964
https://github.com/apache/spark/pull/913

If you could test out that PR and see if it fixes your problems I'd really
appreciate it!

Michael



"
dataginjaninja <rickett.stephanie@gmail.com>,"Thu, 29 May 2014 09:55:22 -0700 (PDT)",Re: Timestamp support in v1.0,dev@spark.incubator.apache.org,"Yes, I get the same error:

scala> val hc = new org.apache.spark.sql.hive.HiveContext(sc)
14/05/29 16:53:40 INFO deprecation: mapred.input.dir.recursive is
deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
14/05/29 16:53:40 INFO deprecation: mapred.max.split.size is deprecated.
Instead, use mapreduce.input.fileinputformat.split.maxsize
14/05/29 16:53:40 INFO deprecation: mapred.min.split.size is deprecated.
Instead, use mapreduce.input.fileinputformat.split.minsize
14/05/29 16:53:40 INFO deprecation: mapred.min.split.size.per.rack is
deprecated. Instead, use
mapreduce.input.fileinputformat.split.minsize.per.rack
14/05/29 16:53:40 INFO deprecation: mapred.min.split.size.per.node is
deprecated. Instead, use
mapreduce.input.fileinputformat.split.minsize.per.node
14/05/29 16:53:40 INFO deprecation: mapred.reduce.tasks is deprecated.
Instead, use mapreduce.job.reduces
14/05/29 16:53:40 INFO deprecation:
mapred.reduce.tasks.speculative.execution is deprecated. Instead, use
mapreduce.reduce.speculative
14/05/29 16:53:41 WARN HiveConf: DEPRECATED: Configuration property
hive.metastore.local no longer has any effect. Make sure to provide a valid
value for hive.metastore.uris if you are connecting to a remote metastore.
14/05/29 16:53:42 WARN HiveConf: DEPRECATED: Configuration property
hive.metastore.local no longer has any effect. Make sure to provide a valid
value for hive.metastore.uris if you are connecting to a remote metastore.
hc: org.apache.spark.sql.hive.HiveContext =
org.apache.spark.sql.hive.HiveContext@36482814

scala> val results = hc.hql(""SELECT COUNT(*) FROM aol"").collect() 
14/05/29 16:53:46 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
aol
14/05/29 16:53:46 INFO ParseDriver: Parse Completed
14/05/29 16:53:47 INFO metastore: Trying to connect to metastore with URI th
14/05/29 16:53:47 INFO metastore: Waiting 1 seconds before next connection
attempt.
14/05/29 16:53:48 INFO metastore: Connected to metastore.
java.lang.RuntimeException: Unsupported dataType: timestamp
	at scala.sys.package$.error(package.scala:27)




--

"
dataginjaninja <rickett.stephanie@gmail.com>,"Thu, 29 May 2014 10:02:13 -0700 (PDT)",Re: Timestamp support in v1.0,dev@spark.incubator.apache.org,"Michael,

Will I have to rebuild after adding the change? Thanks



--

"
Michael Armbrust <michael@databricks.com>,"Thu, 29 May 2014 10:08:46 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"Yes, you'll need to download the code from that PR and reassemble Spark
(sbt/sbt assembly).



"
dataginjaninja <rickett.stephanie@gmail.com>,"Thu, 29 May 2014 10:17:03 -0700 (PDT)",Re: Timestamp support in v1.0,dev@spark.incubator.apache.org,"Darn, I was hoping just to sneak it in that file. I am not the only person
working on the cluster; if I rebuild it that means I have to redeploy
everything to all the nodes as well.  So I cannot do that ... today. If
someone else doesn't beat me to it. I can rebuild at another time. 



-----
Cheers,

Stephanie
--

"
Michael Armbrust <michael@databricks.com>,"Thu, 29 May 2014 11:11:32 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"You should be able to get away with only doing it locally.  This bug is
happening during analysis which only occurs on the driver.



"
Michael Armbrust <michael@databricks.com>,"Thu, 29 May 2014 11:11:32 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"You should be able to get away with only doing it locally.  This bug is
happening during analysis which only occurs on the driver.



"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 29 May 2014 13:23:03 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"Let me put in my +1 as well!

This voting is now closed, and it successfully passes with 13 ""+1""
votes and one ""0"" vote.
Thanks to everyone who tested the RC and voted. Here are the totals:

+1: (13 votes)
Matei Zaharia*
Mark Hamstra*
Holden Karau
Nick Pentreath*
Will Benton
Henry Saputra
Sean McNamara*
Xiangrui Meng*
Andy Konwinski*
Krishna Sankar
Kevin Markey
Patrick Wendell*
Tathagata Das*

0: (1 vote)
Ankur Dave*

-1: (0 vote)

* = binding

Please hold off announcing Spark 1.0.0 until Apache Software
Foundation makes the press release tomorrow. Thank you very much for
your cooperation.

TD


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 29 May 2014 13:23:44 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","[tl;dr stable API's are important - sorry, this is slightly meandering]

Hey - just wanted to chime in on this as I was travelling. Sean, you
bring up great points here about the velocity and stability of Spark.
Many projects have fairly customized semantics around what versions
actually mean (HBase is a good, if somewhat hard-to-comprehend,
example).

What the 1.X label means to Spark is that we are willing to guarantee
stability for Spark's core API. This is something that actually, Spark
has been doing for a while already (we've made few or no breaking
changes to the Spark core API for several years) and we want to codify
this for application developers. In this regard Spark has made a bunch
of changes to enforce the integrity of our API's:

- We went through and clearly annotated internal, or experimental
API's. This was a huge project-wide effort and included Scaladoc and
several other components to make it clear to users.
- We implemented automated byte-code verification of all proposed pull
requests that they don't break public API's. Pull requests after 1.0
will fail if they break API's that are not explicitly declared private
or experimental.

I can't possibly emphasize enough the importance of API stability.
What we want to avoid is the Hadoop approach. Candidly, Hadoop does a
poor job on this. There really isn't a well defined stable API for any
of the Hadoop components, for a few reasons:

1. Hadoop projects don't do any rigorous checking that new patches
don't break API's. Of course, the results in regular API breaks and a
poor understanding of what is a public API.
2. In several cases it's not possible to do basic things in Hadoop
without using deprecated or private API's.
3. There is significant vendor fragmentation of API's.

The main focus of the Hadoop vendors is making consistent cuts of the
core projects work together (HDFS/Pig/Hive/etc) - so API breaks are
sometimes considered ""fixed"" as long as the other projects work around
them (see [1]). We also regularly need to do archaeology (see [2]) and
directly interact with Hadoop committers to understand what API's are
stable and in which versions.

Hadoop so that application writers don't to. We'd like to retain the
property that if you build an application against the (well defined,
stable) Spark API's right now, you'll be able to run it across many
Hadoop vendors and versions for the entire Spark 1.X release cycle.

Writing apps against Hadoop can be very difficult... consider how much
more engineering effort we spent maintaining YARN support than Mesos
support. There are many factors, but one is that Mesos has a single,
narrow, stable API. We've never had to make a change in Mesos due to
an API change, for several years. YARN on the other hand, there are at
least 3 YARN API's that currently exist, which are all binary
incompatible. We'd like to offer apps the ability to build against
Spark's API and just let us deal with it.

As more vendors packaging Spark, I'd like to see us put tools in the
upstream Spark repo that do validation for vendor packages of Spark,
so that we don't end up with fragmentation. Of course, vendors can
enhance the API and are encouraged to, but we need a kernel of API's
that vendors must maintain (think POSIX) to be considered compliant
with Apache Spark. I believe some other projects like OpenStack have
done this to avoid fragmentation.

- Patrick

[1] https://issues.apache.org/jira/browse/MAPREDUCE-5830
[2] http://2.bp.blogspot.com/-GO6HF0OAFHw/UOfNEH-4sEI/AAAAAAAAAD0/dEWFFYTRgYw/s1600/output-file.png


"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 29 May 2014 13:24:15 -0700",[RESULT][VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"Hello everyone,

The vote on Spark 1.0.0 RC11 passes with13 ""+1"" votes, one ""0"" vote and no
""-1"" vote.

Thanks to everyone who tested the RC and voted. Here are the totals:

+1: (13 votes)
Matei Zaharia*
Mark Hamstra*
Holden Karau
Nick Pentreath*
Will Benton
Henry Saputra
Sean McNamara*
Xiangrui Meng*
Andy Konwinski*
Krishna Sankar
Kevin Markey
Patrick Wendell*
Tathagata Das*

0: (1 vote)
Ankur Dave*

-1: (0 vote)

Please hold off announcing Spark 1.0.0 until Apache Software Foundation
makes the press release tomorrow. Thank you very much for your cooperation.

TD
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 29 May 2014 13:33:49 -0700",Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats everyone! This is a huge accomplishment!


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 29 May 2014 14:08:20 -0700",Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"Yup, congrats all. The most impressive thing is the number of contributors to this release — with over 100 contributors, it’s becoming hard to even write the credits. Look forward to the Apache press release tomorrow.

Matei


and no
Foundation
cooperation.


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 29 May 2014 14:57:35 -0700",Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats guys! Another milestone for Apache Spark indeed =)

- Henry

ote:
s to this release â€” with over 100 contributors, itâ€™s becoming hard to even write the credits. Look forward to the Apache press release tomorrow.
 no
ion.

"
Usman Ghani <usman@platfora.com>,"Thu, 29 May 2014 17:39:27 -0700",Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"Congrats everyone. Really pumped about this.



€™s becoming
:
on
"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 29 May 2014 18:08:52 -0700",Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"Yes great work all. Special thanks to Patrick (and TD) for excellent
leadership!

â€™s becoming
e
:
"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Fri, 30 May 2014 11:06:15 +0900",Suggestion or question: Adding rdd.cancelCache() method,<dev@spark.apache.org>,"What I understand is that rdd.cache() is really
rdd.cache_this_rdd_when_it_actually_materializes().
So, somewhat esoteric problem may occur.

The example is as follows: 

void method1()
{
    JavaRDD<...> rdd =
        sc.textFile(...)
        .map(...);

    rdd.cache();
        // since the following methods can call the action methods multiple
times,
        // cache the rdd to prevent rebuilding.

    method2(rdd);  // may or may not call the action methods on rdd
    method3(rdd);  // may or may not call the action methods on rdd

    // #HERE#, the action methods could have been called or not.

    rdd.saveAsTextFile(...);
        // if none of the above methods called the action methods,
        // rdd will materialize here and cached.
    // but we don't need the cache anymore. Caching was unnecessary.
    rdd.unpersist();
}

If there were rdd.cancelCache() method and we could call it at #HERE#,
unnecessary caching could be avoided.
What cancelCache() would do is to cancel the pending request for caching, if
caching is not done yet.
It is different from unpersist(), since unpersist() undoes the caching that
has been actually done.

Will rdd.cancelCache() be really needed, or I'm misunderstanding the caching
mechanism?




"
qingyang li <liqingyang1985@gmail.com>,"Fri, 30 May 2014 11:25:26 +0800","how spark partition data when creating table like "" create table xxx
 as select * from xxx""",dev@spark.apache.org,"hi,  spark-developers, i am using shark/spark,  and i am puzzled by such
question, and  can not find any info from the web, so i ask you.
1.  how spark partition data in memory when creating table when using
""create table a tblproperties(""shark.cache""=""memory"") as select * from
table b "" ,  in another words, how many rdds will be created ? how spark
decide the number of rdds ?

2.  how spark partition data on tachyon when creating table when using
""create table a tblproperties(""shark.cache""=""tachyon"") as select * from
table b "".  in another words, how many files will be created ? how spark
decide the number of files?
i found this settings about tachyon ""tachyon.user.default.block.size.byte""
,  what it means?  could i set it to control each file size ?

thanks for any guiding  .
"
Reynold Xin <rxin@databricks.com>,"Fri, 30 May 2014 00:46:50 -0700","Re: Please change instruction about ""Launching Applications Inside
 the Cluster""",dev@spark.apache.org,"Can you take a look at the latest Spark 1.0 docs and see if they are fixed?

https://github.com/apache/spark/tree/master/docs

Thanks.



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 30 May 2014 00:48:54 -0700","Re: Please change instruction about ""Launching Applications Inside
 the Cluster""","""dev@spark.apache.org"" <dev@spark.apache.org>","They should be - in the sense that the docs now recommend using
spark-submit and thus include entirely different invocations.



"
Aniket <aniket.bhatnagar@gmail.com>,"Fri, 30 May 2014 01:12:32 -0700 (PDT)",Why does spark REPL not embed scala REPL?,dev@spark.incubator.apache.org,"My apologies in advance if this is a dev mailing list topic. I am working on
a small project to provide web interface to spark REPL. The interface will
allow people to use spark REPL and perform exploratory analysis on the data.
I already have a play application running that provides web interface to
standard scala REPL and I am just looking to extend it to optionally include
support for spark REPL. My initial idea was to include spark dependencies in
the project, create a new instance of SparkContext and bind it to a variable
(lets say 'sc') using imain.bind(""sc"", sparkContext). While theoretically
this may work, I am trying to understand why spark REPL takes a different
path by creating it's own SparkILoop, SparkIMain, etc. Can anyone help me
understand why there was a need to provide custom versions of IMain, ILoop,
etc instead of embedding the standard scala REPL and binding SparkContext
instance?

Here is my analysis so far:
1. ExecutorClassLoader - I understand this is need to load classes from
HDFS. Perhaps this could have been plugged into the standard scala REPL
using settings.embeddedDefaults(classLoaderInstance). Also, it's not clear
what ConstructorCleaner does.

2. SparkCommandLine & SparkRunnerSettings - Allow for providing an extra -i
file argument to the REPL. The standard sourcepath wouldn't have sufficed?

3. SparkExprTyper - The only difference between standard ExprTyper and
SparkExprTyper is that repldbg is replaced with logDebug. Not sure if this
was intentional/needed.

4. SparkILoop - Has a few deviations from standard ILoop class but this
could have been managed by extending or wrapping ILoop class or using
settings. Not sure what triggered the need to copy the source code and make
edits.

5. SparkILoopInit - Changes the welcome message and binds spark context in
the interpreter. Welcome message could have been changed by extending
ILoopInit.

6. SparkIMain - Contains quiet a few changes around class
loading/logging/etc but I found it very hard to figure out if extension of
IMain was an option and what exactly didn't work/will not work with IMain.

Rest of the classes seem very similar to their standard counterparts. I have
a feeling the spark REPL can be refactored to embed standard scala REPL. I
know refactoring would not help Spark project as such but would help people
embed the spark REPL much in the same way it's done with standard scala
REPL. Thoughts? 



--

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 30 May 2014 03:12:39 -0700",Announcing Spark 1.0.0,"""dev@spark.apache.org"" <dev@spark.apache.org>, user@spark.apache.org","I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
is a milestone release as the first in the 1.0 line of releases,
providing API stability for Spark's core interfaces.

Spark 1.0.0 is Spark's largest release ever, with contributions from
117 developers. I'd like to thank everyone involved in this release -
it was truly a community effort with fixes, features, and
optimizations contributed from dozens of organizations.

This release expands Spark's standard libraries, introducing a new SQL
package (SparkSQL) which lets users integrate SQL queries into
existing Spark workflows. MLlib, Spark's machine learning library, is
expanded with sparse vector support and several new algorithms. The
GraphX and Streaming libraries also introduce new features and
optimizations. Spark's core engine adds support for secured YARN
clusters, a unified tool for submitting Spark applications, and
several performance and stability improvements. Finally, Spark adds
support for Java 8 lambda syntax and improves coverage of the Java and
Python API's.

Those features only scratch the surface - check out the release notes here:
http://spark.apache.org/releases/spark-release-1-0-0.html

Note that since release artifacts were posted recently, certain
mirrors may not have working downloads for a few hours.

- Patrick

"
Christopher Nguyen <ctn@adatao.com>,"Fri, 30 May 2014 03:18:15 -0700",Re: Announcing Spark 1.0.0,user@spark.apache.org,"Awesome work, Pat et al.!

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
Kan Zhang <kzhang@apache.org>,"Fri, 30 May 2014 04:16:00 -0700",Re: Why does spark REPL not embed scala REPL?,dev@spark.apache.org,"static initializers will be run on remote worker nodes, which may fail due
to differences between driver and worker nodes. See discussion here
https://groups.google.com/d/msg/scala-internals/h27CFLoJXjE/JoobM6NiUMQJ



"
Rahul Singhal <Rahul.Singhal@guavus.com>,"Fri, 30 May 2014 11:57:45 +0000",Re: Announcing Spark 1.0.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is it intentional/ok that the tag v1.0.0 is behind tag v1.0.0-rc11?


Thanks,
Rahul Singhal








"
Sean Owen <sowen@cloudera.com>,"Fri, 30 May 2014 14:09:30 +0100","Streaming example stops outputting (Java, Kafka at least)",dev@spark.apache.org,"Guys I'm struggling to debug some strange behavior in a simple
Streaming + Java + Kafka example -- in fact, a simplified version of
JavaKafkaWordcount, that is just calling print() on a sequence of
messages.

Data is flowing, but it only appears to work for a few periods --
sometimes 0 -- before ceasing to call any actions. Sorry for lots of
log posting but it may illustrate to someone who knows this better
what is happening:



Key action in the logs seems to be as follows -- it works a few times:

...
2014-05-30 13:53:50 INFO  ReceiverTracker:58 - Stream 0 received 0 blocks
2014-05-30 13:53:50 INFO  JobScheduler:58 - Added jobs for time 1401454430000 ms
-------------------------------------------
Time: 1401454430000 ms
-------------------------------------------

2014-05-30 13:53:50 INFO  JobScheduler:58 - Starting job streaming job
1401454430000 ms.0 from job set of time 1401454430000 ms
2014-05-30 13:53:50 INFO  JobScheduler:58 - Finished job streaming job
1401454430000 ms.0 from job set of time 1401454430000 ms
2014-05-30 13:53:50 INFO  JobScheduler:58 - Total delay: 0.004 s for
time 1401454430000 ms (execution: 0.000 s)
2014-05-30 13:53:50 INFO  MappedRDD:58 - Removing RDD 2 from persistence list
2014-05-30 13:53:50 INFO  BlockManager:58 - Removing RDD 2
2014-05-30 13:53:50 INFO  BlockRDD:58 - Removing RDD 1 from persistence list
2014-05-30 13:53:50 INFO  BlockManager:58 - Removing RDD 1
2014-05-30 13:53:50 INFO  KafkaInputDStream:58 - Removing blocks of
RDD BlockRDD[1] at BlockRDD at ReceiverInputDStream.scala:69 of time
1401454430000 ms
2014-05-30 13:54:00 INFO  ReceiverTracker:58 - Stream 0 received 0 blocks
2014-05-30 13:54:00 INFO  JobScheduler:58 - Added jobs for time 1401454440000 ms
...


Then works with some additional, different output in the logs -- here
you see output is flowing too:

...
2014-05-30 13:54:20 INFO  ReceiverTracker:58 - Stream 0 received 2 blocks
2014-05-30 13:54:20 INFO  JobScheduler:58 - Added jobs for time 1401454460000 ms
2014-05-30 13:54:20 INFO  JobScheduler:58 - Starting job streaming job
1401454460000 ms.0 from job set of time 1401454460000 ms
2014-05-30 13:54:20 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Got job 1 (take at
DStream.scala:593) with 1 output partitions (allowLocal=true)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Final stage: Stage 1(take
at DStream.scala:593)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Computing the requested
partition locally
2014-05-30 13:54:20 INFO  BlockManager:58 - Found block
input-0-1401454458400 locally
2014-05-30 13:54:20 INFO  SparkContext:58 - Job finished: take at
DStream.scala:593, took 0.007007 s
2014-05-30 13:54:20 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Got job 2 (take at
DStream.scala:593) with 1 output partitions (allowLocal=true)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Final stage: Stage 2(take
at DStream.scala:593)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Computing the requested
partition locally
2014-05-30 13:54:20 INFO  BlockManager:58 - Found block
input-0-1401454459400 locally
2014-05-30 13:54:20 INFO  SparkContext:58 - Job finished: take at
DStream.scala:593, took 0.002217 s
-------------------------------------------
Time: 1401454460000 ms
-------------------------------------------
99,true,-0.11342268416043325
17,false,1.6732879882133793
...


Then keeps repeating the following with no more evidence that the
print() action is being called:

...
2014-05-30 13:54:20 INFO  JobScheduler:58 - Finished job streaming job
1401454460000 ms.0 from job set of time 1401454460000 ms
2014-05-30 13:54:20 INFO  MappedRDD:58 - Removing RDD 8 from persistence list
2014-05-30 13:54:20 INFO  JobScheduler:58 - Total delay: 0.019 s for
time 1401454460000 ms (execution: 0.015 s)
2014-05-30 13:54:20 INFO  BlockManager:58 - Removing RDD 8
2014-05-30 13:54:20 INFO  BlockRDD:58 - Removing RDD 7 from persistence list
2014-05-30 13:54:20 INFO  BlockManager:58 - Removing RDD 7
2014-05-30 13:54:20 INFO  KafkaInputDStream:58 - Removing blocks of
RDD BlockRDD[7] at BlockRDD at ReceiverInputDStream.scala:69 of time
1401454460000 ms
2014-05-30 13:54:20 INFO  MemoryStore:58 - ensureFreeSpace(100) called
with curMem=201, maxMem=2290719129
2014-05-30 13:54:20 INFO  MemoryStore:58 - Block input-0-1401454460400
stored as bytes to memory (size 100.0 B, free 2.1 GB)
2014-05-30 13:54:20 INFO  BlockManagerInfo:58 - Added
input-0-1401454460400 in memory on 192.168.1.10:60886 (size: 100.0 B,
free: 2.1 GB)
2014-05-30 13:54:20 INFO  BlockManagerMaster:58 - Updated info of
block input-0-1401454460400
2014-05-30 13:54:20 WARN  BlockManager:70 - Block
input-0-1401454460400 already exists on this machine; not re-adding it
2014-05-30 13:54:20 INFO  BlockGenerator:58 - Pushed block input-0-1401454460400
2014-05-30 13:54:21 INFO  MemoryStore:58 - ensureFreeSpace(100) called
with curMem=301, maxMem=2290719129
2014-05-30 13:54:21 INFO  MemoryStore:58 - Block input-0-1401454461400
stored as bytes to memory (size 100.0 B, free 2.1 GB)
2014-05-30 13:54:21 INFO  BlockManagerInfo:58 - Added
input-0-1401454461400 in memory on 192.168.1.10:60886 (size: 100.0 B,
free: 2.1 GB)
2014-05-30 13:54:21 INFO  BlockManagerMaster:58 - Updated info of
block input-0-1401454461400
2014-05-30 13:54:21 WARN  BlockManager:70 - Block
input-0-1401454461400 already exists on this machine; not re-adding it
2014-05-30 13:54:21 INFO  BlockGenerator:58 - Pushed block input-0-1401454461400
2014-05-30 13:54:22 INFO  MemoryStore:58 - ensureFreeSpace(99) called
with curMem=401, maxMem=2290719129
2014-05-30 13:54:22 INFO  MemoryStore:58 - Block input-0-1401454462400
stored as bytes to memory (size 99.0 B, free 2.1 GB)
2014-05-30 13:54:22 INFO  BlockManagerInfo:58 - Added
input-0-1401454462400 in memory on 192.168.1.10:60886 (size: 99.0 B,
free: 2.1 GB)
...


Occasionally it says:

...
2014-05-30 13:54:30 INFO  ReceiverTracker:58 - Stream 0 received 10 blocks
2014-05-30 13:54:30 INFO  JobScheduler:58 - Added jobs for time 1401454470000 ms
2014-05-30 13:54:30 INFO  JobScheduler:58 - Starting job streaming job
1401454470000 ms.0 from job set of time 1401454470000 ms
2014-05-30 13:54:30 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Got job 3 (take at
DStream.scala:593) with 1 output partitions (allowLocal=true)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Final stage: Stage 3(take
at DStream.scala:593)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Computing the requested
partition locally
2014-05-30 13:54:30 INFO  BlockManager:58 - Found block
input-0-1401454460400 locally
2014-05-30 13:54:30 INFO  SparkContext:58 - Job finished: take at
DStream.scala:593, took 0.003993 s
2014-05-30 13:54:30 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Got job 4 (take at
DStream.scala:593) with 9 output partitions (allowLocal=true)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Final stage: Stage 4(take
at DStream.scala:593)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Submitting Stage 4
(MappedRDD[12] at map at MappedDStream.scala:35), which has no missing
parents
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Submitting 9 missing tasks
from Stage 4 (MappedRDD[12] at map at MappedDStream.scala:35)
2014-05-30 13:54:30 INFO  TaskSchedulerImpl:58 - Adding task set 4.0
with 9 tasks
...


Output is definitely continuing to be written to Kafka; you can even
see that it seems to be acknolwedging that the stream is seeing more
data.

The same happens with operations like saving to file. It looks like
the action is no longer scheduled.

Does that ring any bells? I'm at a loss!

"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 30 May 2014 09:42:27 -0400","Re: Streaming example stops outputting (Java, Kafka at least)",dev@spark.apache.org,"Hi, Sean   

I was in the same problem

but when I changed MASTER=â€œlocalâ€ to MASTER=â€œlocal[2]â€

everything back to the normal

Hasnâ€™t get a chance to ask here

Best,  

--  
Nan Zhu



cks
4430000 ms

ob
ce list
e list
e
cks
4440000 ms
cks
4460000 ms

e
st()
e
st()
ob
ce list
e list
e
ed

1454460400
ed

1454461400
d

ocks
4470000 ms

e
st()
e
st()
ing


"
Sean Owen <sowen@cloudera.com>,"Fri, 30 May 2014 14:38:30 +0100","Re: Streaming example stops outputting (Java, Kafka at least)",dev@spark.apache.org,"Thanks Nan, that does appear to fix it. I was using ""local"". Can
anyone say whether that's to be expected or whether it could be a bug
somewhere?

local[2]â€

"
Upender Nimbekar <upentech@gmail.com>,"Fri, 30 May 2014 12:20:11 -0400",Spark 1.0.0 - Java 8,user@spark.apache.org,"Great News ! I've been awaiting this release to start doing some coding
with Spark using Java 8. Can I run Spark 1.0 examples on a virtual host
with 16 GB ram and fair descent amount of hard disk ? Or do I reaaly need
to use a cluster of machines.
Second, are there any good exmaples of using MLIB on Spark. Please shoot me
in the right direction.

Thanks
Upender


"
Andrew Ash <andrew@andrewash.com>,"Fri, 30 May 2014 09:21:04 -0700",bin/spark-shell --jars option,dev@spark.apache.org,"Hi Spark users,

In past Spark releases I always had to add jars to multiple places when
using the spark-shell, and I'm looking to cut down on those.  The --jars
option looks like it does what I want, but it doesn't work.  I did a quick
experiment on latest branch-1.0 and found this:

*# 0) jar not added anywhere*
./bin/spark-shell --master spark://aash-mbp.local:7077
spark> import org.joda.time.DateTime
[fails -- expected because the .jar isn't anywhere]

*# 1) just --jars*
./bin/spark-shell --master spark://aash-mbp.local:7077 --jars
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
[fails -- but might work on non-standalone clusters?]

*# 2) using --jars and sc.addJar()*
./bin/spark-shell --master spark://aash-mbp.local:7077 --jars
/tmp/joda-time-2.3.jar
spark> sc.addJar(""/tmp/joda-time-2.3.jar"")
spark> import org.joda.time.DateTime
[fails -- shouldn't sc.addJar() make imports possible?]

*# 3) just --driver-class-path*
./bin/spark-shell --master spark://aash-mbp.local:7077 --driver-class-path
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
spark> new DateTime()
res0: org.joda.time.DateTime = 2014-05-29T11:10:56.745-07:00
spark> sc.parallelize(1 to 10).map(k => new DateTime()).collect
[fails -- expected because jar wasn't ever sent to executors, only driver]

*# 4) using --driver-class-path and sc.addJar()*
./bin/spark-shell --master spark://aash-mbp.local:7077 --driver-class-path
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
spark> sc.addJar(""/tmp/joda-time-2.3.jar"")
spark> new DateTime()
res0: org.joda.time.DateTime = 2014-05-29T11:10:56.745-07:00
spark> sc.parallelize(1 to 10).map(k => new DateTime()).collect
[success!]


Looking at the documentation for --jars, it looks like --jars doesn't work
with standalone in cluster deployment mode.  Here are the relevant doc
entries:

  --jars JARS                 A comma-separated list of local jars to
include on the
                              driver classpath and that SparkContext.addJar
will work
                              with. Doesn't work on standalone with
'cluster' deploy mode.

  --driver-class-path         Extra class path entries to pass to the
driver. Note that
                              jars added with --jars are automatically
included in the
                              classpath.


For the --jars comment about not working with standalone, is this something
that can be fixed to make the ""1) just --jars"" path above work?  Or is
there some larger architecture reason that --jars can't work with
standalone mode?

Appreciate it!
Andrew
"
Aaron Davidson <ilikerps@gmail.com>,"Fri, 30 May 2014 09:29:48 -0700",Re: Why does spark REPL not embed scala REPL?,dev@spark.apache.org,"There's some discussion here as well on just using the Scala REPL for 2.11:
http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-on-Scala-2-11-td6506.html#a6523

Matei's response mentions the features we needed to change from the Scala
REPL (class-based wrappers and where to output the generated classes),
which were added as options to the 2.11 REPL, so we may be able to trim
down a bunch when 2.11 becomes standard.



"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Fri, 30 May 2014 12:24:21 -0400",Re: Spark 1.0.0 - Java 8,user@spark.apache.org,"With respect to virtual hosts, my team uses Vagrant/Virtualbox. We have 3
CentOS VMs with 4 GB RAM each - 2 worker nodes and a master node.

Everything works fine, though if you are using MapR, you have to make sure
they are all on the same subnet.

-Suren






-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 30 May 2014 10:53:34 -0700","Re: Streaming example stops outputting (Java, Kafka at least)","""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah - Spark streaming needs at least two threads to run. I actually
thought we warned the user if they only use one (@tdas?) but the
warning might not be working correctly - or I'm misremembering.


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 30 May 2014 14:35:35 -0400","Re: Streaming example stops outputting (Java, Kafka at least)",dev@spark.apache.org,"If local[2] is expected, then the streaming doc is actually misleading? 

as the given example is 

import org.apache.spark.api.java.function._
import org.apache.spark.streaming._
import org.apache.spark.streaming.api._
// Create a StreamingContext with a local master
val ssc = new StreamingContext(""local"", ""NetworkWordCount"", Seconds(1))

http://spark.apache.org/docs/latest/streaming-programming-guide.html

I created a JIRA and a PR 

https://github.com/apache/spark/pull/924 

-- 
Nan Zhu





"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Fri, 30 May 2014 12:05:13 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"First of all, I think it's great that you're thinking about this.  API
stability is super important and it would be good to see Spark get on top
of this.

I want to clarify a bit about Hadoop.  The problem that Hadoop faces is
that the Java package system isn't very flexible.  If you have a method in,
say, the org.apache.hadoop.hdfs.shortcircuit package that should only ever
be used by the org.apache.hadoop.hdfs.client package, there is no way to
express that.  You have to make the method public.  You can hide things by
making them package-private, but that only works if your entire project is
a single giant package, and that is not the road Hadoop devs wanted to go
down.

of course, they can be called by anyone.  To get around this limitation,
Hadoop came up with a pretty rigorous compatibility policy, discussed here:
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/InterfaceClassification.html
The basic idea is that we'd put ""interface annotations"" on every public
class.  The ""Private"" annotation meant that it was only supposed to be used
in the project itself.  ""Limited-Private"" was kind of the project and maybe
one or two closely related projects.  And ""Public"" was supposed to be the
public API.  At a finer granularity, for specific public methods, you could
add the ""VisibileForTesting"" annotation to indicate that they were only
visible to make a unit test possible.

This sounds great in theory.  But in practice, users often ignore the
annotation and just do whatever they want.  This is not because they're
mustache-twirling villains, but because they have legitimate (to them)
reasons.  For example, HBase would often find that they could get better
performance by hooking into supposedly private HDFS APIs.  Of course, they
could always ask HDFS to add public versions of those APIs.  But that takes
time, and could be contentious.  In the best case, they'd have to wait for
another Hadoop release to happen before HBase could benefit.  From their
perspective, supporting the feature on more Hadoop releases was better than
supporting it on fewer, even if the latter was the ""correct"" way of doing
things.  Then of course there were the cases where there were simple
oversights... there either was no interface annotation or the user of the
downstream project forgot to check it.

Ideally, we'd later add a @stable API and transition everyone to it.  But
that's much easier said than done.  A lot of projects just don't want to
change, because it would mean giving up compatibility with older releases
without the ""blessed"" API.  Basically, it's a tragedy of the commons.  It
would be much better for everyone if we all used public stable APIs and
never used private or unstable ones.  But each individual project feels
that it can get advantages by cheating and using (or continuing to use) the
private / unstable APIs.  Candidly, Spark is one of those projects that
continues to use deprecated and private Hadoop APIs-- mostly for
compatibility reasons, as I understand.

I think that the lesson learned here is that the compiler needs to be in
charge of preventing people from using APIs, not an annotation.
 Public/private annotations ""Just Don't Work.""  I don't know if Scala
provides any mechanisms to do this beyond what Java provides.  Even if not,
there are probably classloader and CLASSPATH tricks that could be used to
hide internals.  I also think that it makes sense to put a lot of thought
related note, there were definitely cases where Hadoop changed an API, and
the pain outweighed the gain.

There are other dimensions to compatibility... for example, Hadoop
currently leaks its CLASSPATH, so that you can't easily write a MapReduce
job without using the same versions of Guava (just to pick one random
example) that it does.  In practice, this led to a pathological fear of
updating dependencies, since we didn't want to break users who needed
specific version of their deps.  Does Spark also expose its CLASSPATH in
this way to executors?  I was under the impression that it did.

At some point we will also have to confront the Scala version issue.  Will
there be flag days where Spark jobs need to be upgraded to a new,
incompatible version of Scala to run on the latest Spark?  There are pros
and cons, but I think users will mostly see the cons.



I agree with this.  We should test these compatibility scenarios, and we
don't.  It would be awesome to do this in an automated way for Spark.



Disagree.  The problem is that we have stable APIs, but users don't want to
use them (they prefer the ancient API Doug Cutting wrote in 2008, because
it works on some old version of Hadoop).  It's hard to argue against this
kind of reasoning, since (to reiterate) it's rational from the point of
view of the individual.  This is the problem with deprecation in general--
once you've let an API out into the wild, it's very difficult to get it
back into its cage.

3. There is significant vendor fragmentation of API's.

The big difference in the last few years was that some people were creating
distributions based on Hadoop 1.x and others were creating distributions
based on 2.x.  But nobody added vendor specific APIs (or at least I haven't
heard of any).  (I can't speak for MapR... since they are proprietary, I
have not seen the code.)  Now that Hadoop 1.x is starting to die a natural
death, any differences between 2.x and 1.x are becoming less important.
 Sadly, Yahoo continues to use and develop 0.23, for now at least... But I
think their efforts are mostly directed at backporting.  They have not
added divergent APIs, to my knowledge.

best,
Colin


The main focus of the Hadoop vendors is making consistent cuts of the
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 30 May 2014 12:30:29 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
In fact it does. You can say something like ""private[foo]"" and the
annotated element will be visible for all classes under ""foo"" (where
""foo"" is any package in the hierarchy leading up to the class). That's
used a lot in Spark.

I haven't fully looked at how the @DeveloperApi is used, but I agree
with you - annotations are not a good way to do this. The Scala
feature above would be much better, but it might still leak things at
the Java bytecode level (don't know how Scala implements it under the
cover, but I assume it's not by declaring the element as a Java
""private"").

Another thing is that in Scala the default visibility is public, which
makes it very easy to inadvertently add things to the API. I'd like to
see more care in making things have the proper visibility - I
generally declare things private first, and relax that as needed.
Using @VisibleForTesting would be great too, when the Scala
private[foo] approach doesn't work.


If you're using the Spark assemblies, yes, there is a lot of things
that your app gets exposed to. For example, you can see Guava and
Jetty (and many other things) there. This is something that has always
bugged me, but I don't really have a good suggestion of how to fix it;
shading goes a certain way, but it also breaks codes that uses
reflection (e.g. Class.forName()-style class loading).

What is worse is that Spark doesn't even agree with the Hadoop code it
depends on; e.g., Spark uses Guava 14.x while Hadoop is still in Guava
11.x. So when you run your Scala app, what gets loaded?


Yes, this could be an issue - I'm not sure Scala has a policy towards
this, but updates (at least minor, e.g. 2.9 -> 2.10) tend to break
binary compatibility.

Scala also makes some API updates tricky - e.g., adding a new named
argument to a Scala method is not a binary compatible change (while,
e.g., adding a new keyword argument in a python method is just fine).
The use of implicits and other Scala features make this even more
opaque...

Anyway, not really any solutions in this message, just a few comments
I wanted to throw out there. :-)

-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 30 May 2014 14:11:36 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey guys, thanks for the insights. Also, I realize Hadoop has gotten
way better about this with 2.2+ and I think it's great progress.

We have well defined API levels in Spark and also automated checking
of API violations for new pull requests. When doing code reviews we
always enforce the narrowest possible visibility:

1. private
2. private[spark]
3. @Experimental or @DeveloperApi
4. public

Our automated checks exclude 1-3. Anything that breaks 4 will trigger
a build failure.

The Scala compiler prevents anyone external from using 1 or 2. We do
have ""bytecode public but annotated"" (3) API's that we might change.
We spent a lot of time looking into whether these can offer compiler
warnings, but we haven't found a way to do this and do not see a
better alternative at this point.

Regarding Scala compatibility, Scala 2.11+ is ""source code
compatible"", meaning we'll be able to cross-compile Spark for
different versions of Scala. We've already been in touch with Typesafe
about this and they've offered to integrate Spark into their
compatibility test suite. They've also committed to patching 2.11 with
a minor release if bugs are found.

Anyways, my point is we've actually thought a lot about this already.

The CLASSPATH thing is different than API stability, but indeed also a
form of compatibility. This is something where I'd also like to see
Spark have better isolation of user classes from Spark's own
execution...

- Patrick




"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 30 May 2014 14:36:02 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"Hi Patrick,


I understand @Experimental, but when would you use @DeveloperApi
instead of private[spark]? Seems to me that, for the API user, they
both mean very similar, if not exactly the same, thing. And the second
is actually more user-friendly since the compiler will yell at you.

Who's the ""Developer"" that the annotation refers to?

-- 
Marcelo

"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Fri, 30 May 2014 14:56:49 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
That's really excellent.  Great job.

I like the private[spark] visibility level-- sounds like this is another
way Scala has greatly improved on Java.

The Scala compiler prevents anyone external from using 1 or 2. We do

It would be nice if the production build could strip this stuff out.
 Otherwise, it feels a lot like a @private, @unstable Hadoop API... and we
know how those turned out.



Thanks, I hadn't heard about this plan.  Hopefully we can get everyone on
2.11 ASAP.


I think the best thing to do is just ""shade"" all the dependencies.  Then
they will be in a different namespace, and clients can have their own
versions of whatever dependencies they like without conflicting.  As
Marcelo mentioned, there might be a few edge cases where this breaks
reflection, but I don't think that's an issue for most libraries.  So at
worst case we could end up needing apps to follow us in lockstep for Kryo
or maybe Akka, but not the whole kit and caboodle like with Hadoop.

best,
Colin


- Patrick
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 30 May 2014 22:54:44 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Spark is a bit different than Hadoop MapReduce, so maybe that's a
source of some confusion. Spark is often used as a substrate for
building different types of analytics applications, so @DeveloperAPI
are internal API's that we'd like to expose to application writers,
but that might be more volatile. This is like the internal API's in
the linux kernel, they aren't stable, but of course we try to minimize
changes to them. If people want to write lower-level modules against
them, that's fine with us, but they know the interfaces might change.

This has worked pretty well over the years, even with many different
companies writing against those API's.

@Experimental are user-facing features we are trying out. Hopefully
that one is more clear.

In terms of making a big jar that shades all of our dependencies - I'm
curious how that would actually work in practice. It would be good to
explore. There are a few potential challenges I see:

1. If any of our dependencies encode class name information in IPC
messages, this would break. E.g. can you definitely shade the Hadoop
client, protobuf, hbase client, etc and have them send messages over
the wire? This could break things if class names are ever encoded in a
wire format.
2. Many libraries like logging subsystems, configuration systems, etc
rely on static state and initialization. I'm not totally sure how e.g.
slf4j initializes itself if you have both a shaded and non-shaded copy
of slf4j present.
3. This would mean the spark-core jar would be really massive because
it would inline all of our deps. We've actually been thinking of
avoiding the current assembly jar approach because, due to scala
specialized classes, our assemblies now have more than 65,000 class
files in them leading to all kinds of bad issues. We'd have to stick
with a big uber assembly-like jar if we decide to shade stuff.
4. I'm not totally sure how this would work when people want to e.g.
build Spark with different Hadoop versions. Would we publish different
shaded uber-jars for every Hadoop version? Would the Hadoop dep just
not be shaded... if so what about all it's dependencies.

Anyways just some things to consider... simplifying our classpath is
definitely an avenue worth exploring!





"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Sat, 31 May 2014 12:47:01 +0530",Fwd: Monitoring / Instrumenting jobs in 1.0,"dev <dev@spark.apache.org>, Praveen R <praveen@sigmoidanalytics.com>","We have a json feed of spark application interface that we use for easier
instrumentation & monitoring. Has that been considered/found relevant?
Already sent as a pull request to 0.9.0, would that work or should we
update it to 1.0.0?


Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



---------- Forwarded message ----------
From: Patrick Wendell <pwendell@gmail.com>
Date: Sat, May 31, 2014 at 9:09 AM
Subject: Re: Monitoring / Instrumenting jobs in 1.0
To: user@spark.apache.org


The main change here was refactoring the SparkListener interface which
is where we expose internal state about a Spark job to other
applications. We've cleaned up these API's a bunch and also added a
way to log all data as JSON for post-hoc analysis:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala

- Patrick

added
"
prabeesh k <prabsmails@gmail.com>,"Sat, 31 May 2014 15:52:34 +0530",Unable to execute saveAsTextFile on multi node mesos,"user@spark.apache.org, user@mesos.apache.org, dev <dev@spark.apache.org>, 
	dev <dev@mesos.apache.org>","Hi,

scenario : Read data from HDFS and apply hive query  on it and the result
is written back to HDFS.

 Scheme creation, Querying  and saveAsTextFile are working fine with
following mode

   - local mode
   - mesos cluster with single node
   - spark cluster with multi node

Schema creation and querying are working fine with mesos multi node cluster.
But  while trying to write back to HDFS using saveAsTextFile, the following
error occurs

* 14/05/30 10:16:35 INFO DAGScheduler: The failed fetch was from Stage 4
(mapPartitionsWithIndex at Operator.scala:333); marking it for resubmission*
*14/05/30 10:16:35 INFO DAGScheduler: Executor lost:
201405291518-3644595722-5050-17933-1 (epoch 148)*

Let me know your thoughts regarding this.

Regards,
prabeesh
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 31 May 2014 10:15:33 -0700",Re: Unable to execute saveAsTextFile on multi node mesos,user@spark.apache.org,"Can you look at the logs from the executor or in the UI? They should
give an exception with the reason for the task failure. Also in the
future, for this type of e-mail please only e-mail the ""user@"" list
and not both lists.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 31 May 2014 10:45:21 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),"""dev@spark.apache.org"" <dev@spark.apache.org>","
5. Shading our dependencies could mess up our external API's if we
ever return types that are outside of the spark package because we'd
then be returned shaded types that users have to deal with. E.g. where
before we returned an o.a.flume.AvroFlumeEvent we'd have to return a
some.namespace.AvroFlumeEvent. Then users downstream would have to
deal with converting our types into the correct namespace if they want
to inter-operate with other libraries. We generally try to avoid ever
returning types from other libraries, but it would be good to audit
our API's and see if we ever do this.


"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Sat, 31 May 2014 14:09:44 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"


That's a good point.  It seems to me that if Spark is returning a type in
the public API, that type is part of the public API (for better or worse).
 So this is a case where you wouldn't want to shade that type.  But it
would be nice to avoid doing this, for exactly the reasons you state...


MapReduce is used as a substrate in a lot of cases, too.  Hive has
traditionally created MR jobs to do what it needs to do.  Similarly, Oozie
can create MR jobs.  It seems that what @DeveloperAPI is pretty similar to
@LimitedPrivate in Hadoop.  If I understand correctly, your hope is that
frameworks will use @DeveloperAPI, but individual application developers
will steer clear.  That is a good plan, as long as you can ensure that the
framework developers are willing to lock their versions to a certain Spark
version.  Otherwise they will make the same arguments we've heard before,
that they don't want to transition off of a deprecated @DeveloperAPI
because they want to keep support for Spark 1.0.0 (or whatever).  We hear
these arguments in Hadoop all the time...  now that spark as a 1.0 release
they will carry more weight.  Remember, Hadoop APIs started nice and simple
too :)


Google protobuffers assume a fixed schema.  That is to say, they do not
include metadata identifying the types of what is placed in them.  The
types are determined by convention.  It is possible to change the java
package in which the protobuf classes reside with no harmful effects.  (See
HDFS-4909 for an example of this).  The RPC itself does include a java
class name for the interface we're talking to, though.  The code for
handling this is all under our control, though, so if we had to make any
minor modifications to make shading work, we could.


I guess the worst case scenario would be that the shaded version of slf4j
creates a log file, but then the app's unshaded version overwrites that log
file.  I don't see how the two versions could ""cooperate"" since they aren't
sharing static state.  The only solutions I can see are leaving slf4j
unshaded, or setting up separate log files for the spark-core versus the
application.  I haven't thought this through completely, but my gut feeling
is that if you're sharing a log file, you probably want to share the
logging code too.



I wonder if it would be possible to put Hadoop and its dependencies ""in a
box,"" (as it were) by using a separate classloader for them.  That might
solve this without requiring an uber-jar.  It would be nice to not have to
transfer all that stuff each time you start a job... in a perfect world,
the stuff that had not changed would not need to be transferred (thinking
out loud here)

best,
Colin


"
