Michael Armbrust <michael@databricks.com>,"Thu, 30 Apr 2015 17:19:10 -0700",Re: Uninitialized session in HiveContext?,Marcelo Vanzin <vanzin@cloudera.com>,"Hey Marcelo,

Thanks for the heads up!  I'm currently in the process of refactoring all
of this (to separate the metadata connection from the execution side) and
as part of this I'm making the initialization of the session not lazy.  It
would be great to hear if this also works for your internal integration
tests once the patch is up (hopefully this weekend).

Michael


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 30 Apr 2015 19:13:46 -0700",Re: Uninitialized session in HiveContext?,Michael Armbrust <michael@databricks.com>,"Hi Michael,

It would be great to see changes to make hive integration less
painful, and I can test them in our environment once you have a patch.

But I guess my question is a little more geared towards the current
code; doesn't the issue I ran into affect 1.4 and potentially earlier
versions too?





-- 
Marcelo

---------------------------------------------------------------------


"
zhazhan <zzhang@hortonworks.com>,"Thu, 30 Apr 2015 19:58:22 -0700 (MST)",Mima test failure in the master branch?,dev@spark.apache.org,"[info] spark-sql: found 1 potential binary incompatibilities (filtered 129)
[error] * method sqlParser()org.apache.spark.sql.SparkSQLParser in class
org.apache.spark.sql.SQLContext does not have a correspondent in new version
[error] filter with: ProblemFilters.excludeMissingMethodProblem



--

---------------------------------------------------------------------


"
zhazhan <zzhang@hortonworks.com>,"Thu, 30 Apr 2015 19:59:55 -0700 (MST)",Re: Mima test failure in the master branch?,dev@spark.apache.org,"Any PR open for this?



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 30 Apr 2015 20:38:00 -0700",Re: Mima test failure in the master branch?,zhazhan <zzhang@hortonworks.com>,"Looks like this has been taken care of:

commit beeafcfd6ee1e460c4d564cd1515d8781989b422
Author: Patrick Wendell <patrick@databricks.com>
Date:   Thu Apr 30 20:33:36 2015 -0700

    Revert ""[SPARK-5213] [SQL] Pluggable SQL Parser Support""


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 30 Apr 2015 20:39:44 -0700",Re: Mima test failure in the master branch?,zhazhan <zzhang@hortonworks.com>,"I reverted the patch that I think was causing this: SPARK-5213

Thanks


---------------------------------------------------------------------


"
twinkle sachdeva <twinkle.sachdeva@gmail.com>,"Fri, 1 May 2015 10:27:44 +0530",Re: Regarding KryoSerialization in Spark,Sandy Ryza <sandy.ryza@cloudera.com>,"Thanks for the info.



"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 30 Apr 2015 23:11:57 -0700 (PDT)",Re: [discuss] ending support for Java 6?,"""dev"" <dev@spark.apache.org>","+1 for this think it's high time.




We should of course do it with enough warning for users. 1.4 May be too early (not for me though!). Perhaps we specify that 1.5 will officially move to JDK7?









‚Äî
Sent from Mailbox


 for JDK 6 starting releas"
anshu shukla <anshushukla0@gmail.com>,"Fri, 1 May 2015 13:00:25 +0530",Fwd: Event generator for SPARK-Streaming from csv,"dev@spark.apache.org, user@spark.apache.org","I have the real DEBS-TAxi data in csv file , in order to operate over it
how to simulate a ""Spout"" kind  of thing as event generator using the
timestamps in CSV file.




-- 
Thanks & Regards,
Anshu Shukla
"
Niranda Perera <niranda.perera@gmail.com>,"Fri, 1 May 2015 13:14:58 +0530",Re: Custom PersistanceEngine and LeaderAgent implementation in Java,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Pls find the PR here [1]

[1] https://github.com/apache/spark/pull/5832




-- 
Niranda
"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Fri, 1 May 2015 01:46:14 -0700",Speeding up Spark build during development,dev@spark.apache.org,"Hi,
I'm making some small changes to the Spark codebase and trying it out on a
cluster. I was wondering if there's a faster way to build than running the
package target each time.
Currently I'm using: mvn -DskipTests  package

All the nodes have the same filesystem mounted at the same mount point.

Pramod
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 1 May 2015 09:29:39 +0000",Re: [discuss] ending support for Java 6?,"""dev@spark.apache.org"" <dev@spark.apache.org>","
> On 30 Apr 2015, at 21:40, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> 
> As for the idea, I'm +1. Spark is the only reason I still have jdk6
> around - exactly because I don't want to cause the issue that started
> this discussion (inadvertently using JDK7 APIs). And as has been
> pointed out, even J7 is about to go EOL real soon.

+1, perhaps with a roadmap for people to plan for

> 
> Even Hadoop is moving away (I think 2.7 will be j7-only). Hive 1.1 is
> already j7-only. And when Hadoop moves away from something, it's an
> event worthy of headlines.

The constraint here was that there were too many people ""stuck"" in Java 6, and java 7 wasn't compelling enough to pull people off a JVM they trusted to be stable at large scale. One problem with production hadoop is that across 5000 14-core servers, all race conditions will surface ‚Äîleading to a reluctance to upgrade JVMs or even OS's. There was also the fact that for a long time Hadoop wouldn't build on OSX on Java 7 (HADOOP-9350). Even today, OS/X's JDK has better rendering than java7+, leaving it nice to have around for the IDEs.


After Hadoop 2.5 shipped an announcement was made that 2.6 would be the last Java 1.6 release, with the switch taking place in November. Moving ASF Jenkins up was probably the hardest bit ( HADOOP-10530 ). 

Switching to JDK7 has enabled moving kerberos support to Java 8 (HADOOP-10786; some changes in the internal kerberos classes used directly for kerberos to work properly). See HADOOP-11090 for the JDK8 migration; Hadoop trunk will be switching to Java 8 before long

> They're still on Jetty 6!

While moving off Jetty entirely wherever possible, leaving jetty 6 on the transitive-maven-classpath in the hope of not breaking code that expects it to be there. It's not that the project likes Jetty 6 (there are threads whose sole aim is to detect jetty startup failures), but that moving off it is felt to be better than upgrading.

> 
> As for pyspark, https://github.com/apache/spark/pull/5580 should get
> rid of the last incompatibility with large assemblies, by keeping the
> python files in separate archives. If we remove support for Java 6,
> then we don't need to worry about the size of the assembly anymore.

zzhang's patch drops to Java 6 just to rebuild the assembly Jar; you can still build Java7-only classes. So it will work even before the pyspark patch goes in."
Prashant Sharma <scrapcodes@gmail.com>,"Fri, 1 May 2015 16:23:13 +0530",Re: Speeding up Spark build during development,Pramod Biligiri <pramodbiligiri@gmail.com>,"Hi Pramod,

If you are using sbt as your build, then you need to do sbt assembly once
and use sbt ~compile. Also export SPARK_PREPEND_CLASSES=1 this in your
shell and all nodes.
You can may be try this out ?

Thanks,

Prashant Sharma




"
Dean Wampler <deanwampler@gmail.com>,"Fri, 1 May 2015 06:26:11 -0500",Re: [discuss] ending support for Java 6?,Ted Yu <yuzhihong@gmail.com>,"FWIW, another reason to start planning for deprecation of Java 7, too, is
that Scala 2.12 will require Java 8. Scala 2.12 will be released early next
year.


Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


va
er
:
d
g
,
n
i
y
m
in
s
g
.)
n
e
va
er
st
"
Steven Shaw <steven@steshaw.org>,"Fri, 1 May 2015 23:04:03 +1000",Re: [discuss] ending support for Java 6?,Dean Wampler <deanwampler@gmail.com>,"
xt

‚ÄãWill 2.12 be the release that based on dotty
<https://github.com/lampepfl/dotty>?

Cheers,
Steve.
"
"""DW @ Gmail"" <deanwampler@gmail.com>","Fri, 1 May 2015 09:24:12 -0400",Re: [discuss] ending support for Java 6?,Steven Shaw <steven@steshaw.org>,"No. That will be ""3.0"" some day

Sent from my rotary phone. 



xt
"
Koert Kuipers <koert@tresata.com>,"Fri, 1 May 2015 09:27:23 -0400",Re: [discuss] ending support for Java 6?,Steven Shaw <steven@steshaw.org>,"it seems spark is happy to upgrade scala, drop older java versions, upgrade
incompatible library versions (akka), and all of this within spark 1.x
does the 1.x mean anything in terms of compatibility of dependencies? or is
that limited to its own api? what are the rules?

s
"
Stephen Carman <scarman@coldlight.com>,"Fri, 1 May 2015 13:54:48 +0000",Re: Tungsten + Flink,Sree V <sree_at_chess@yahoo.com>,"I think as long as the two frameworks follow the same paradigm for how their interfaces work it‚Äôs fine to have 2 competing frameworks. This way the frameworks have some motivation
to be the best at what they do rather than being the only choice whether you like it or not. They also seem to have some differing opinions about how to do certain things leaving me to believe
that the 2 projects exist mostly because of disagreements on fundamentals about how a system such as this should be built and scaled out.

I think spark should definitely take what it can from these projects, but otherwise they should remain separate projects going their own way.

Steve


> On Apr 29, 2015, at 8:01 PM, Sree V <sree_at_chess@yahoo.com.INVALID> wrote:
>
> I agree, Ewan.
> We should also look into combining both Flink and Spark into one.This eases the industry adaptation instead.
>
> Thanking you.
>
> With Regards
> Sree
>
>
>     On Wednesday, April 29, 2015 3:21 AM, Ewan Higgs <ewan.higgs@ugent.be> wrote:
>
>
> Hi all,
> A quick question about Tungsten. The announcement of the Tungsten
> project is on the back of Hadoop Summit in Brussels where some of the
> Flink devs were giving talks [1] on how Flink manages memory using byte
> arrays and the like to avoid the overhead of all the Java types[2]. Is
> there an opportunity for code reuse here? Spark and Flink may have
> different needs in some respects, but they work fundamentally towards
> the same goal so I imagine there could be come worthwhile collaboration.
>
> -Ewan
>
> [1] http://2015.hadoopsummit.org/brussels/speaker/?speaker=MrtonBalassi
> http://2015.hadoopsummit.org/brussels/speaker/?speaker=AljoschaKrettek
>
> [2]
> https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=53741525
> https://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
>

This e-mail is intended solely for the above-mentioned recipient and it may contain confidential or privileged information. If you have received it in error, please notify us immediately and delete the e-mail. You must not copy, distribute, disclose or take any action in reliance on it. In addition, the contents of an attachment to this e-mail may contain software viruses which could damage your own computer system. While ColdLight Solutions, LLC has taken every reasonable precaution to minimize this risk, we cannot accept liability for any damage which you sustain as a result of software viruses. You should perform your own virus checks before opening the attachment.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Ewan Higgs <ewan.higgs@ugent.be>,"Fri, 01 May 2015 17:01:31 +0200",Re: Tungsten + Flink,"Stephen Carman <scarman@coldlight.com>, 
 Sree V <sree_at_chess@yahoo.com>","I don't think it's useful to combine them since they are different 
projects. But I do think that a lot of work went into Flink's paged 
memory system built on byte buffers and if collaboration can take place 
to pop that out into like a memory subsystem library that both Spark and 
Flink can use then it should raise both ships. If the usage patterns are 
too different then sure, don't use their work. But it looks pretty generic:

https://github.com/apache/flink/tree/master/flink-core/src/main/java/org/apache/flink/core/memory

To bring this back into other threads: Flink's memory system uses 
java.nio - so it requires Java 1.7 afaik. :)

-Ewan



---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 1 May 2015 16:36:57 +0000",RE: Speeding up Spark build during development,"Pramod Biligiri <pramodbiligiri@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Pramod,

For cluster-like tests you might want to use the same code as in mllib's LocalClusterSparkContext. You can rebuild only the package that you change and then run this main class.

Best regards, Alexander

-----Odbiligiri@gmail.com] 
Sent: Friday, May 01, 2015 1:46 AM
To: dev@spark.apache.org
Subject: Speeding up Spark build during development

Hi,
I'm making some small changes to the Spark codebase and trying it out on a cluster. I was wondering if there's a faster way to build than running the package target each time.
Currently I'm using: mvn -DskipTests  package

All the nodes have the same filesystem mounted at the same mount point.

Pramod
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 1 May 2015 09:45:09 -0700",Re: Speeding up Spark build during development,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Pramod:
Please remember to run Zinc so that the build is faster.

Cheers


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Fri, 1 May 2015 12:51:41 -0400",Re: Speeding up Spark build during development,"Ted Yu <yuzhihong@gmail.com>, ""Ulanov, Alexander""
	<alexander.ulanov@hp.com>","Following what Ted said, if you leverage the `mvn` from within the
`build/` directory of Spark youπll get zinc for free which should help
speed up build times.



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Fri, 1 May 2015 19:59:03 -0700",Why does SortShuffleWriter write to disk always?,dev@spark.apache.org,"Hi,
I was trying to see if I can make Spark avoid hitting the disk for small
jobs, but I see that the SortShuffleWriter.write() always writes to disk. I
found an older thread (
http://apache-spark-user-list.1001560.n3.nabble.com/How-does-shuffle-work-in-spark-td584.html)
saying that it doesn't call fsync on this write path.

My question is why does it always write to disk?
Does it mean the reduce phase reads the result from the disk as well?
Isn't it possible to read the data from map/buffer in ExternalSorter
directly during the reduce phase?

Thanks,
Pramod
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sat, 02 May 2015 18:05:01 +0000",createDataFrame allows column names as second param in Python not in Scala,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
SQLContext.createDataFrame has different behaviour in Scala or Python :

[Row(_1=u'Alice', _2=1)]
[Row(name=u'Alice', age=1)]

and in Scala :

scala> val data = List((""Alice"", 1), (""Wonderland"", 0))
scala> sqlContext.createDataFrame(data, List(""name"", ""score""))
<console>:28: error: overloaded method value createDataFrame with
alternatives: ... cannot be applied to ...

What do you think about allowing in Scala too to have a Seq of column names
for the sake of consistency ?

Regards,

Olivier.
"
Tom Hubregtsen <thubregtsen@gmail.com>,"Fri, 1 May 2015 14:06:53 -0700 (MST)","What is the location in the source code of the computation of the
 elements in a map transformation?",dev@spark.apache.org,"I am trying to understand what the data and computation flow is in Spark, and
believe I fairly understand the Shuffle (both map and reduce side), but I do
not get what happens to the computation from the map stages. I know all maps
gets pipelined on the shuffle (when there is no other action in between),
but I can not find where the actual computation for the map happens (for
instance for rdd.map(x => x+1), where does the +1 happen?). Any pointers to
files or functions are appreciated. 

I know compute of rdd/MapPartitionsRDD.scala gets called, but I loose track
of the lambda function after this. 

Thanks,

Tom





--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 1 May 2015 14:46:08 -0700",Re: [discuss] ending support for Java 6?,Thomas Graves <tgraves@yahoo-inc.com>,"It's really hard to inspect API calls since none of us have the Java
standard library in our brain. The only way we can enforce this is to have
it in Jenkins, and Tom you are currently our mini-Jenkins server :)

Joking aside, looks like we should support Java 6 in 1.4, and in the
release notes include a message saying starting in 1.5 we will drop Java 6
support.





"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sat, 02 May 2015 08:13:20 +0000",Re: Pandas' Shift in Dataframe,"Olivier Girardot <o.girardot@lateral-thoughts.com>, 
	""Evan R. Sparks"" <evan.sparks@gmail.com>, Reynold Xin <rxin@databricks.com>","To close this thread rxin created a broader Jira to handle window functions
in Dataframes : https://issues.apache.org/jira/browse/SPARK-7322
Thanks everyone.

Le mer. 29 avr. 2015 √† 22:51, Olivier Girardot <
o.girardot@lateral-thoughts.com> a √©crit :

l.
 a
,
and
uld
:
e
od
r
 a
e
..
n
e
shift.html
"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 2 May 2015 13:09:37 -0700",Re: [discuss] ending support for Java 6?,Reynold Xin <rxin@databricks.com>,"We could build on minimum jdk we support for testing pr's - which will
automatically cause build failures in case code uses newer api ?

Regards,
Mridul


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 2 May 2015 13:42:46 -0700",Re: [discuss] ending support for Java 6?,Mridul Muralidharan <mridul@gmail.com>,1
Patrick Wendell <pwendell@gmail.com>,"Sat, 2 May 2015 19:38:50 -0700","Re: What is the location in the source code of the computation of the
 elements in a map transformation?",Tom Hubregtsen <thubregtsen@gmail.com>,"Maybe I can help a bit. What happens when you call .map(my func) is
that you create a MapPartitionsRDD that has a reference to that
closure in it's compute() function. When a job is run (jobs are run as
the result of RDD actions):

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L520

When this happens the RDD will generate ShuffleMapTask's for
physically computing the MapPartitionsRDD. The ShuffleMapTask will be
shipped to the executor and then run()

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala#L70

The ShuffleMapTask will call rdd.iterator() which will eventually call
into compute()

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L240

- Patrick


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Sat, 2 May 2015 20:53:40 -0700",Re: [discuss] ending support for Java 6?,Mridul Muralidharan <mridul@gmail.com>,"that's kinda what we're doing right now, java 7 is the default/standard on
our jenkins.

or, i vote we buy a butler's outfit for thomas and have a second jenkins
instance...  ;)


"
Koert Kuipers <koert@tresata.com>,"Sun, 3 May 2015 00:20:18 -0400",Re: [discuss] ending support for Java 6?,shane knapp <sknapp@berkeley.edu>,"i think i might be misunderstanding, but shouldnt java 6 currently be used
in jenkins?


"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 2 May 2015 21:52:42 -0700",Re: [discuss] ending support for Java 6?,shane knapp <sknapp@berkeley.edu>,"Hi Shane,

  Since we are still maintaining support for jdk6, jenkins should be
using jdk6 [1] to ensure we do not inadvertently use jdk7 or higher
api which breaks source level compat.
-source and -target is insufficient to ensure api usage is conformant
with the minimum jdk version we are supporting.

Regards,
Mridul

[1] Not jdk7 as you mentioned


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 2 May 2015 22:26:27 -0700",Re: Why does SortShuffleWriter write to disk always?,Pramod Biligiri <pramodbiligiri@gmail.com>,"I've personally prototyped completely in-memory shuffle for Spark 3 times.
However, it is unclear how big of a gain it would be to put all of these in
memory, under newer file systems (ext4, xfs). If the shuffle data is small,
they are still in the file system buffer cache anyway. Note that network
throughput is often lower than disk throughput, so it won't be a problem to
read them from disk. And not having to keep all of these stuff in-memory
substantially simplifies memory management.




"
Yijie Shen <henry.yijieshen@gmail.com>,"Sun, 3 May 2015 13:32:09 +0800","Submit & Kill Spark Application program programmatically from
 another application",dev@spark.apache.org,"Hi,

I‚Äôve posted this problem in user@spark but find no reply, therefore moved to dev@spark, sorry for duplication.

I am wondering if it is possible to¬†submit,¬†monitor¬†&¬†kill¬†spark applications from another service.

I have wrote a service this:

parse¬†user commands
translate¬†them into understandable¬†arguments¬†to an¬†already prepared Spark-SQL application
submit the application along with arguments to Spark Cluster using¬†spark-submit¬†from¬†ProcessBuilder
run generated applications' driver in¬†cluster mode.
The above 4 steps has been finished, but I have difficulties in these two:

Query¬†about the applications¬†status, for example, the percentage completion.
Kill¬†queries accordingly
What I find in¬†spark standalone documentation¬†suggest kill application using:

./bin/spark-class org.apache.spark.deploy.Client kill <master url> <driver ID>

And should¬†find
the driver ID through the standalone Master web UI at
http://<master url>:8080.

Are there any¬†programmatically methods I could get the driverID submitted by my `ProcessBuilder` and query status about the query?

Any Suggestions?

‚Äî¬†
Best Regards!
Yijie Shen"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 2 May 2015 22:37:16 -0700",Re: Why does SortShuffleWriter write to disk always?,Reynold Xin <rxin@databricks.com>,"I agree, this is better handled by the filesystem cache - not to
mention, being able to do zero copy writes.

Regards,
Mridul


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 2 May 2015 22:44:14 -0700","Re: createDataFrame allows column names as second param in Python not
 in Scala",Olivier Girardot <o.girardot@lateral-thoughts.com>,"Part of the reason is that it is really easy to just call toDF on Scala,
and we already have a lot of createDataFrame functions.

(You might find some of the cross-language differences confusing, but I'd
argue most real users just stick to one language, and developers or
trainers are the only ones that need to constantly switch between
languages).


"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Sat, 2 May 2015 23:04:07 -0700",Re: Why does SortShuffleWriter write to disk always?,Mridul Muralidharan <mridul@gmail.com>,"Thanks for the info. I agree, it makes sense the way it is designed.

Pramod


"
Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"Sun, 3 May 2015 01:52:22 -0700",LDA and PageRank Using GraphX,dev@spark.apache.org,"Hi All,
I am looking to run LDA for topic modeling and page rank algorithms that
comes with GraphX
for some data analysis. Are there are any examples (GraphX) that I can take
a look  ?

Thanks
Praveen
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sun, 03 May 2015 09:13:45 +0000","Re: createDataFrame allows column names as second param in Python not
 in Scala","Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","I have the perfect counter example where some of the data scientists
prototype in Python and the production materials is done in Scala.
But I get your point, as a matter of fact I realised the toDF method took
parameters a little while after posting this.
However the toDF still needs you to go from a List to an RDD, or create a
useless Dataframe and call toDF on it re-creating a complete data
structure. I just feel that the createDataFrame(_: Seq) is not really
useful as it is, because I think there are practically no circumstances
where you'd want to create a DataFrame without column names.

I'm not implying a n-th overloaded method should be created, rather than
change the signature of the existing method with an optional Seq of column
names.

Regards,

Olivier.

Le dim. 3 mai 2015 √† 07:44, Reynold Xin <rxin@databricks.com> a √©crit :

"
Sean Owen <sowen@cloudera.com>,"Sun, 3 May 2015 10:14:15 +0100",Re: [discuss] ending support for Java 6?,Mridul Muralidharan <mridul@gmail.com>,"Should be, but isn't what Jenkins does.
https://issues.apache.org/jira/browse/SPARK-1437

At this point it might be simpler to just decide that 1.5 will require
Java 7 and then the Jenkins setup is correct.

(NB: you can also solve this by setting bootclasspath to JDK 6 libs
even when using javac 7+ but I think this is overly complicated.)


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 3 May 2015 14:01:16 +0100",Blockers for 1.4.0,dev <dev@spark.apache.org>,"I'd like to preemptively post the current list of 35 Blockers for
release 1.4.0.
(There are 53 Critical too, and a total of 273 JIRAs targeted for
1.4.0. Clearly most of that isn't accurate, so would be good to
un-target most of that.)

As a matter of process and hygiene, it would be best to either decide
they're not Blockers at this point and reprioritize, or focus on
addressing them, as we're now in the run up to release. I suggest that
we shouldn't release with any Blockers outstanding, by definition.


SPARK-7298
Web UI
Harmonize style of new UI visualizations
Patrick Wendell

SPARK-7297
Web UI
Make timeline more discoverable
Patrick Wendell

SPARK-7284
""Documentation
 Streaming""
Update streaming documentation for Spark 1.4.0 release
Tathagata Das

SPARK-7228
SparkR
SparkR public API for 1.4 release
Shivaram Venkataraman

SPARK-7158
SQL
collect and take return different results

SPARK-7139
Streaming
Allow received block metadata to be saved to WAL and recovered on driver failure
Tathagata Das

SPARK-7111
Streaming
Exposing of input data rates of non-receiver streams like Kafka Direct stream
Saisai Shao

SPARK-6941
SQL
Provide a better error message to explain that tables created from
RDDs are immutable

SPARK-6923
SQL
Spark SQL CLI does not read Data Source schema correctly

SPARK-6906
SQL
Refactor Connection to Hive Metastore
Michael Armbrust

SPARK-6831
""Documentation
 PySpark
 SparkR
 SQL""
Document how to use external data sources

SPARK-6824
SparkR
Fill the docs for DataFrame API in SparkR

SPARK-6812
SparkR
filter() on DataFrame does not work as expected

SPARK-6811
SparkR
Building binary R packages for SparkR

SPARK-6806
""Documentation
 SparkR""
SparkR examples in programming guide
Davies Liu

SPARK-6784
SQL
Clean up all the inbound/outbound conversions for DateType
Yin Huai

SPARK-6702
""Streaming
 Web UI""
Update the Streaming Tab in Spark UI to show more batch information
Tathagata Das

SPARK-6654
Streaming
Update Kinesis Streaming impls (both KCL-based and Direct) to use
latest aws-java-sdk and kinesis-client-library

SPARK-5960
Streaming
Allow AWS credentials to be passed to KinesisUtils.createStream()
Chris Fregly

SPARK-5948
SQL
Support writing to partitioned table for the Parquet data source

SPARK-5947
SQL
First class partitioning support in data sources API

SPARK-5920
Shuffle
Use a BufferedInputStream to read local shuffle data
Kay Ousterhout

SPARK-5707
SQL
Enabling spark.sql.codegen throws ClassNotFound exception

SPARK-5517
SQL
Add input types for Java UDFs

SPARK-5463
SQL
Fix Parquet filter push-down

SPARK-5456
SQL
Decimal Type comparison issue

SPARK-5182
SQL
Partitioning support for tables created by the data source API
Cheng Lian

SPARK-5180
SQL
Data source API improvement

SPARK-4867
SQL
UDF clean up

SPARK-2973
SQL
""Use LocalRelation for all ExecutedCommands
 avoid job for take/collect()""
Cheng Lian

SPARK-2883
""Input/Output
 SQL""
Spark Support for ORCFile format

SPARK-2873
SQL
Support disk spilling in Spark SQL aggregation
Yin Huai

SPARK-1517
""Build
 Project Infra""
""Publish nightly snapshots of documentation
 maven artifacts
 and binary builds""
Nicholas Chammas

SPARK-1442
SQL
Add Window function support

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Sun, 3 May 2015 08:57:29 -0700",Re: [discuss] ending support for Java 6?,Sean Owen <sowen@cloudera.com>,"that bug predates my time at the amplab...  :)

anyways, just to restate: jenkins currently only builds w/java 7.  if you
folks need 6, i can make it happen, but it will be a (smallish) bit of work.

shane


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sun, 03 May 2015 20:04:49 +0000",Multi-Line JSON in SparkSQL,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
Is there any way in Spark SQL to load multi-line JSON data efficiently, I
think there was in the mailing list a reference to
http://pivotal-field-engineering.github.io/pmr-common/ for its
JSONInputFormat

But it's rather inaccessible considering the dependency is not available in
any public maven repo (If you know of one, I'd be glad to hear it).

Is there any plan to address this or any public recommendation ?
(considering the documentation clearly states that sqlContext.jsonFile will
not work for multi-line json(s))

Regards,

Olivier.
"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Sun, 3 May 2015 14:54:39 -0700",Re: Speeding up Spark build during development,"""York, Brennon"" <Brennon.York@capitalone.com>","This is great. I didn't know about the mvn script in the build directory.

Pramod


lp
's
.
"
Chester Chen <chester@alpinenow.com>,"Sun, 3 May 2015 15:14:23 -0700","Re: Submit & Kill Spark Application program programmatically from
 another application",Yijie Shen <henry.yijieshen@gmail.com>,"Sounds like you are in Yarn-Cluster mode.

I created a JIRA SPARK-3913
<https://issues.apache.org/jira/browse/SPARK-3913> and PR
https://github.com/apache/spark/pull/2786

is this what you looking for ?




Chester


re moved
:
r
"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 3 May 2015 15:40:48 -0700",Re: Speeding up Spark build during development,Pramod Biligiri <pramodbiligiri@gmail.com>,"https://spark.apache.org/docs/latest/building-spark.html#building-with-buildmvn


m
help
t
ng
ty
r
"
Reynold Xin <rxin@databricks.com>,"Sun, 3 May 2015 19:05:13 -0700",Re: Multi-Line JSON in SparkSQL,Olivier Girardot <o.girardot@lateral-thoughts.com>,"How does the pivotal format decides where to split the files? It seems to
me the challenge is to decide that, and on the top of my head the only way
to do this is to scan from the beginning and parse the json properly, which
makes it not possible with large files (doable for whole input with a lot
of small files though). If there is a better way, we should do it.



"
Reynold Xin <rxin@databricks.com>,"Sun, 3 May 2015 19:21:32 -0700","Re: createDataFrame allows column names as second param in Python not
 in Scala",Olivier Girardot <o.girardot@lateral-thoughts.com>,"We can't drop the existing createDataFrame one, since it breaks API
compatibility, and the existing one also automatically infers the column
name for case classes (in that case users most likely won't be declaring
names directly). If this is really a problem, we should just create a new
function (maybe more than one, since you could argue the one for Seq should
also have that ...).




n
©crit :
d
"
yunming zhang <zhangyunming1990@gmail.com>,"Sun, 3 May 2015 23:31:34 -0400",Question about PageRank with Live Journal,dev <dev@spark.apache.org>,"Hi,

I have a question about running PageRan with live journal data as suggested
by the example at

org.apache.spark.examples.graphx.LiveJournalPageRank


I ran with the following options

bin/run-example org.apache.spark.examples.graphx.LiveJournalPageRank
data/graphx/soc-LiveJournal1.txt --numEPart=1


And it seems that from the SparkUI, the data that

mapPartitions at GraphImpl.scala:235

shuffle  read size is steadily increasing all the way to 2.1GB on a single
node machine. I think the shuffle read size should be decreasing as the
number of messages decrease?  I tried with 4 partitions and it seems that
the shuffle read for mapPartitions job is decreasing as the program
progresses. But I am not sure why it is actually increasing for one
partition?

And it really destroys the performance for a single partition even though
single partition uses much less time on reduce phase than the 4-partitions
configuration on a single node.


Thanks
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 04 May 2015 04:37:08 +0000",Re: Multi-Line JSON in SparkSQL,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","I'll try to study that and get back to you.
Regards,

Olivier.

Le lun. 4 mai 2015 √† 04:05, Reynold Xin <rxin@databricks.com> a √©crit :

y
ch
I
"
Emre Sevinc <emre.sevinc@gmail.com>,"Mon, 4 May 2015 08:13:13 +0200",Re: Multi-Line JSON in SparkSQL,Olivier Girardot <o.girardot@lateral-thoughts.com>,"You can check out the following library:

   https://github.com/alexholmes/json-mapreduce

--
Emre Sevin√ß



in
ll



-- 
Emre Sevinc
"
anshu shukla <anshushukla0@gmail.com>,"Mon, 4 May 2015 12:19:52 +0530",spark log analyzer sample,"user@spark.apache.org, dev@spark.apache.org","Exception in thread ""main"" java.lang.RuntimeException:
org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot
communicate with client version 4


I am not using any hadoop facility (not even hdfs) then why it is giving
this error .

-- 
Thanks & Regards,
Anshu Shukla
"
Reynold Xin <rxin@databricks.com>,"Sun, 3 May 2015 23:53:53 -0700",Re: Multi-Line JSON in SparkSQL,Emre Sevinc <emre.sevinc@gmail.com>,"I took a quick look at that implementation. I'm not sure if it actually
handles JSON correctly, because it attempts to find the first { starting
from a random point. However, that random point could be in the middle of a
string, and thus the first { might just be part of a string, rather than a
real JSON object starting position.



 I
e
"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Mon, 4 May 2015 00:01:48 -0700",Re: Speeding up Spark build during development,Mark Hamstra <mark@clearstorydata.com>,"Using the inbuilt maven and zinc it takes around 10 minutes for each build.
Is that reasonable?
My maven opts looks like this:
$ echo $MAVEN_OPTS
-Xmx12000m -XX:MaxPermSize=2048m

I'm running it as build/mvn -DskipTests package

Should I be tweaking my Zinc/Nailgun config?

Pramod


ildmvn
.
 help
ut
or
"
Emre Sevinc <emre.sevinc@gmail.com>,"Mon, 4 May 2015 09:09:56 +0200",Re: Speeding up Spark build during development,Pramod Biligiri <pramodbiligiri@gmail.com>,"Hello Pramod,

Do you need to build the whole project every time? Generally you don't,
e.g., when I was changing some files that belong only to Spark Streaming, I
was building only the streaming (of course after having build and installed
the whole project, but that was done only once), and then the assembly.
This was much faster than trying to build the whole Spark every time.

--
Emre Sevin√ß


d.
ildmvn
ld help
ou
,



-- 
Emre Sevinc
"
"""Joe Halliwell"" <joe.halliwell@gmail.com>","Mon, 04 May 2015 00:36:54 -0700 (PDT)",Re: Multi-Line JSON in SparkSQL,"""Reynold Xin"" <rxin@databricks.com>","I think Reynold‚Äôs argument shows the impossibility of the general case.




But a ‚Äúmaximum object depth‚Äù hint could enable a new input format to do its job both efficiently and correctly in the common case where the input is an array of similarly structured objects! I‚Äôd certainly be interested in an implementation along those lines.




Cheers,

Joe



http://www.joehalliwell.com

@joehalliwell


 a
a
, I
available
jsonFile"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Mon, 4 May 2015 01:44:09 -0700",Re: Speeding up Spark build during development,Emre Sevinc <emre.sevinc@gmail.com>,"No, I just need to build one project at a time. Right now SparkSql.

Pramod


 I
ed
uildmvn
uld
t
e
"
Emre Sevinc <emre.sevinc@gmail.com>,"Mon, 4 May 2015 10:50:12 +0200",Re: Speeding up Spark build during development,Pramod Biligiri <pramodbiligiri@gmail.com>,"Just to give you an example:

When I was trying to make a small change only to the Streaming component of
Spark, first I built and installed the whole Spark project (this took about
15 minutes on my 4-core, 4 GB RAM laptop). Then, after having changed files
only in Streaming, I ran something like (in the top-level directory):

   mvn --projects streaming/ -DskipTests package

and then

   mvn --projects assembly/ -DskipTests install


This was much faster than trying to build the whole Spark from scratch,
because Maven was only building one component, in my case the Streaming
component, of Spark. I think you can use a very similar approach.

--
Emre Sevin√ß




, I
led
m
buildmvn
ould
n
t
e
se


-- 
Emre Sevinc
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 04 May 2015 12:43:37 +0000",Re: Multi-Line JSON in SparkSQL,"Joe Halliwell <joe.halliwell@gmail.com>, Reynold Xin <rxin@databricks.com>","I was wondering if it's possible to use existing Hive SerDes for this ?

Le lun. 4 mai 2015 √† 08:36, Joe Halliwell <joe.halliwell@gmail.com> a
√©crit :

 case.
t format to do
erested in
f
"
Meethu Mathew <meethu.mathew@flytxt.com>,"Mon, 4 May 2015 18:18:40 +0530",Re: Speeding up Spark build during development,dev@spark.apache.org,"*
*
** ** ** ** ** **** ** **** Hi,

  Is it really necessary to run **mvn --projects assembly/ -DskipTests 
install ? Could you please explain why this is needed?
I got the changes after running ""mvn --projects streaming/ -DskipTests 
package"".

Regards,
Meethu


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 4 May 2015 15:32:17 +0200",Update Wiki Developer instructions,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'd like to update the information about using Eclipse to develop on the
Spark project found on this page:

https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=38572224

I don't see any way to edit this page (I created an account). Since it's a
wiki, I assumed it's supposed to be editable, but unfortunately I can't
find a way. What's the proper way to update it?

thanks,
iulian

-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Sean Owen <sowen@cloudera.com>,"Mon, 4 May 2015 14:40:29 +0100",Re: Update Wiki Developer instructions,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"I think it's only committers that can edit it. I suppose you can open
a JIRA with a suggested text change if it is significant enough to
need discussion. If it's trivial, just post it here and someone can
take care of it.

24
a

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 4 May 2015 16:53:10 +0100",[ANNOUNCE] Spark branch-1.4,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Devs,

Just an announcement that I've cut Spark's branch 1.4 to form the
basis of the 1.4 release. Other than a few stragglers, this represents
the end of active feature development for Spark 1.4. Per usual, if
committers are merging any features, please be in touch so I can help
coordinate. Any new commits will need to be explicitly merged into
this branch.

I'd like to invite the community to begin testing this release. This
week I can create preview packages to help with it. Please help test
Spark 1.4 and report any regressions!

- Patrick

---------------------------------------------------------------------


"
Robin East <robin.east@xense.co.uk>,"Mon, 4 May 2015 17:22:51 +0100",Re: LDA and PageRank Using GraphX,Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"There is an LDA example in the MLlib examples. You can run it like this:

./bin/run-example mllib.LDAExample --stopwordFile <stopwords> <input documents>

stop words is a file of stop words, 1 on each line. Input documents are the text of each document, 1 document per line. To see all the options just run with no options or inputs.
that
take

"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 4 May 2015 18:31:29 +0200",Re: Update Wiki Developer instructions,Sean Owen <sowen@cloudera.com>,"Ok, here‚Äôs how it should be:

   -

   Eclipse Luna
    -

   Scala IDE 4.0
    -

   Scala Test

The easiest way is to download the Scala IDE bundle from the Scala IDE
download page <http://scala-ide.org/download/sdk.html>. It comes
pre-installed with ScalaTest. Alternatively, use the provided update site
<http://scala-ide.org/download/current.html> or Eclipse Marketplace.

Remove: ‚Äú Importing all Spark sub projects at once is not recommended.‚Äù ‚Üê
that works just fine.

Add:

If you want to develop on Scala 2.10, you need to configure a Scala
installation for the exact Scala version that‚Äôs used to compile Spark. At
the time of this writing that is Scala 2.10.4. You can do that in Eclipse
Preferences -> Scala -> Installations by pointing to the lib/ directory of
right-click, choose Scala -> Set Scala Installation and point to the 2.10.4
installation. This should clear all errors about invalid cross-compiled
libraries. A clean build should succeed.


I think it's only committers that can edit it. I suppose you can open
e
24
s
‚Äã
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Paul Brown <prb@mult.ifario.us>,"Mon, 4 May 2015 10:22:37 -0700",Re: Multi-Line JSON in SparkSQL,dev@spark.apache.org,"It's not JSON, per se, but data formats like smile (
http://en.wikipedia.org/wiki/Smile_%28data_interchange_format%29) provide
support for markers that can't be confused with content and also provide
reasonably similar ergonomics.

‚Äî
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/


a
al case.
put format to do
ut
nterested
:
y
ng
an
.
"
shane knapp <sknapp@berkeley.edu>,"Mon, 4 May 2015 11:20:08 -0700",Re: [discuss] ending support for Java 6?,Sean Owen <sowen@cloudera.com>,"...and now the workers all have java6 installed.

https://issues.apache.org/jira/browse/SPARK-1437

sadly, the built-in jenkins jdk management doesn't allow us to choose a JDK
version within matrix projects...  so we need to manage this stuff
manually.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 4 May 2015 19:23:51 +0100",Re: [discuss] ending support for Java 6?,shane knapp <sknapp@berkeley.edu>,"If we just set JAVA_HOME in dev/run-test-jenkins, I think it should work.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 4 May 2015 11:24:39 -0700",Re: [discuss] ending support for Java 6?,Patrick Wendell <pwendell@gmail.com>,"sgtm


"
Akshat Aranya <aaranya@gmail.com>,"Mon, 4 May 2015 11:27:09 -0700",Task scheduling times,dev@spark.apache.org,"Hi,

I have been investigating scheduling delays in Spark and I found some
unexplained anomalies.  In my use case, I have two stages after
collapsing the transformations: the first is a mapPartitions() and the
second is a sortByKey().  I found that the task serialization for the
first stage takes much longer than the second.

1. mapPartitions() - this launches 256 tasks in 603 ms (avg. 2.363
ms). Each task serializes to 1220 bytes.
2. sortByKey() - this launches 64 tasks in 12 ms (avg. 0.187 ms). Each
task serializes to 1139 bytes.

Note that the serialized size of the task is similar, but the avg.
scheduling time is very different.  I also instrumented the code to
print out the serialization time, and it seems like it is indeed the
serialization that takes much longer.  This seemed weird to me because
the biggest part of the Task, the taskBinary is actually directly
copied from a byte array.

Any explanation of why this happens?

Thanks,
Akshat

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 4 May 2015 11:28:37 -0700",Re: Multi-Line JSON in SparkSQL,Joe Halliwell <joe.halliwell@gmail.com>,"Joe - I think that's a legit and useful thing to do. Do you want to give it
a shot?


 case.
t format to do
erested in
f
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 4 May 2015 15:05:55 -0400",Re: Multi-Line JSON in SparkSQL,Reynold Xin <rxin@databricks.com>,"I don't know whether this is common, but we might also allow another separator for JSON objects, such as two blank lines.

Matei

give it
<joe.halliwell@gmail.com>
general case.
input format to do
input
interested in
actually
starting
middle of
than
it).


---------------------------------------------------------------------


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 04 May 2015 21:07:15 +0000",Re: Multi-Line JSON in SparkSQL,"Matei Zaharia <matei.zaharia@gmail.com>, Reynold Xin <rxin@databricks.com>","@joe, I'd be glad to help if you need.

Le lun. 4 mai 2015 √† 20:06, Matei Zaharia <matei.zaharia@gmail.com> a
√©crit :

e
ral case.
nput format to do
ly
e
"
Michael Armbrust <michael@databricks.com>,"Mon, 4 May 2015 14:49:39 -0700",Re: Speeding up Spark build during development,Meethu Mathew <meethu.mathew@flytxt.com>,"FWIW... My Spark SQL development workflow is usually to run ""build/sbt
sparkShell"" or ""build/sbt 'sql/test-only <testSuiteName>'"".  These commands
starts in as little as 30s on my laptop, automatically figure out which
subprojects need to be rebuilt, and don't require the expensive assembly
creation.


,
.
h-buildmvn
ould
e
se
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 4 May 2015 23:46:57 +0100",Thanking Test Partners,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Community testing during the QA window is an important part of the
release cycle in Spark. It helps us deliver higher quality releases by
vetting out issues not covered by our unit tests.

I was thinking that from now on, it would be nice to recognize the
organizations that donate time to help test release previews and
release candidates.

So starting with Spark 1.4, barring any major objection, I was
planning to highlight those groups as test partners when we make
release notes, similar to how we thank contributors for the release.
That is, companies or teams that commit to doing nontrivial release
testing such as testing real workloads, performance tests, or other
integration tests.

Hopefully this will provide recognition to those parts of the
community and also encourage others to help test Spark releases.

- Patrick

---------------------------------------------------------------------


"
Vinay Muttineni <vmuttineni@ebay.com>,"Mon, 4 May 2015 17:16:47 -0700",OOM error with GMMs on 4GB dataset,"user@spark.apache.org, dev@spark.apache.org","Hi, I am training a GMM with 10 gaussians on a 4 GB dataset(720,000 * 760).
The spark (1.3.1) job is allocated 120 executors with 6GB each and the
driver also has 6GB.
Spark Config Params:

.set(""spark.hadoop.validateOutputSpecs"",
""false"").set(""spark.dynamicAllocation.enabled"",
""false"").set(""spark.driver.maxResultSize"",
""4g"").set(""spark.default.parallelism"", ""300"").set(""spark.serializer"",
""org.apache.spark.serializer.KryoSerializer"").set(""spark.kryoserializer.buffer.mb"",
""500"").set(""spark.akka.frameSize"", ""256"").set(""spark.akka.timeout"", ""300"")

However, at the aggregate step (Line 168)
val sums = breezeData.aggregate(ExpectationSum.zero(k, d))(compute.value, _
+= _)

I get OOM error and the application hangs indefinitely. Is this an issue or
am I missing something?
java.lang.OutOfMemoryError: Java heap space
        at akka.util.CompactByteString$.apply(ByteString.scala:410)
        at akka.util.ByteString$.apply(ByteString.scala:22)
        at
akka.remote.transport.netty.TcpHandlers$class.onMessage(TcpSupport.scala:45)
        at
akka.remote.transport.netty.TcpServerHandler.onMessage(TcpSupport.scala:57)
        at
akka.remote.transport.netty.NettyServerHelpers$class.messageReceived(NettyHelpers.scala:43)
        at
akka.remote.transport.netty.ServerHandler.messageReceived(NettyTransport.scala:180)
        at
org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at
org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at
org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at
org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
        at
org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at
org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at
org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at
org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at
org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at
org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

15/05/04 16:23:38 ERROR util.Utils: Uncaught exception in thread
task-result-getter-2
java.lang.OutOfMemoryError: Java heap space
Exception in thread ""task-result-getter-2"" java.lang.OutOfMemoryError: Java
heap space
15/05/04 16:23:45 INFO scheduler.TaskSetManager: Finished task 1070.0 in
stage 6.0 (TID 8276) in 382069 ms on [] (160/3600)
15/05/04 16:23:54 WARN channel.DefaultChannelPipeline: An exception was
thrown by a user handler while handling an exception event ([id:
0xc57da871, ] EXCEPTION: java.lang.OutOfMemoryError: Java heap space)
java.lang.OutOfMemoryError: Java heap space
15/05/04 16:23:55 WARN channel.DefaultChannelPipeline: An exception was
thrown by a user handler while handling an exception event ([id:
0x3c3dbb0c, ] EXCEPTION: java.lang.OutOfMemoryError: Java heap space)
15/05/04 16:24:45 ERROR actor.ActorSystemImpl: Uncaught fatal error from
thread [sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down
ActorSystem [sparkDriver]



Thanks!
Vinay
"
Tathagata Das <tdas@databricks.com>,"Mon, 4 May 2015 22:35:02 -0700",Re: Speeding up Spark build during development,Michael Armbrust <michael@databricks.com>,"In addition to Michael suggestion, in my SBT workflow I also use ""~"" to
automatically kickoff build and unit test. For example,

sbt/sbt ""~streaming/test-only *BasicOperationsSuite*""

It will automatically detect any file changes in the project and start of
the compilation and testing.
So my full workflow involves changing code in IntelliJ and then
continuously running unit tests in the background on the command line using
this ""~"".

TD



ds
nt
,
g
.
h
ildmvn
he
should
in
nt
n
or
e
"
Reynold Xin <rxin@databricks.com>,"Mon, 4 May 2015 22:48:37 -0700",Re: [discuss] DataFrame function namespacing,Ted Yu <yuzhihong@gmail.com>,"After talking with people on this thread and offline, I've decided to go
with option 1, i.e. putting everything in a single ""functions"" object.



"
"""Joe Halliwell"" <joe.halliwell@gmail.com>","Tue, 05 May 2015 00:37:35 -0700 (PDT)",Re: Multi-Line JSON in SparkSQL,"""Olivier Girardot"" <o.girardot@lateral-thoughts.com>","@reynold, I‚Äôll raise a JIRA today.@oliver, let‚Äôs discuss on the ticket?




I suspect the algorithm is going to be bit fiddly and would definitely benefit from multiple heads. If possible, I think we should handle pathological cases like {‚Äú:‚Äù:‚Äù:"
Ewan Higgs <ewan.higgs@ugent.be>,"Tue, 05 May 2015 09:51:34 +0200",Re: Multi-Line JSON in SparkSQL,"Joe Halliwell <joe.halliwell@gmail.com>, 
 Olivier Girardot <o.girardot@lateral-thoughts.com>","FWIW, CSV has the same problem that renders it immune to naive partitioning.

Consider the following RFC 4180 compliant record:

1,2,""
all,of,these,are,just,one,field
"",4,5

Now, it's probably a terrible idea to give a file system awareness of 
actual file types, but couldn't HDFS handle this nearer the replication 
level? XML, JSON, and CSV are so pervasive it almost seems like it could 
be appropriate -if- enormous JSON files are considered enough of an 
issue that some basic ETL becomes a non viable solution.

-Ewan



---------------------------------------------------------------------


"
anshu shukla <anshushukla0@gmail.com>,"Tue, 5 May 2015 14:51:55 +0530",Re: Event generator for SPARK-Streaming from csv,"juan.rodriguez.hortala@gmail.com, dev@spark.apache.org","I know these methods , but i need to create events using the timestamps in
the data tuples ,means every time a new tuple  is generated using the
timestamp in a CSV file .this will be useful to simulate the data rate
 with time just like real sensor data .


ut-dstreams-and-receivers,
y.


-- 
Thanks & Regards,
Anshu Shukla
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 5 May 2015 11:36:05 +0200",Re: Speeding up Spark build during development,Tathagata Das <tdas@databricks.com>,"I'm probably the only Eclipse user here, but it seems I have the best
workflow :) At least for me things work as they should: once I imported
projects in the workspace I can build and run/debug tests from the IDE. I
only go to sbt when I need to re-create projects or I want to run the full
test suite.


iulian




ng
y
s
k
d
:
ildmvn
h
s
t
ng
an
r
l
t
ly



-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 5 May 2015 11:08:55 +0100",Re: [discuss] ending support for Java 6?,shane knapp <sknapp@berkeley.edu>,"If there is broad consensus here to drop Java 1.6 in Spark 1.5, should
we do an ANNOUNCE to user and dev?


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 5 May 2015 11:23:14 +0100",Pull request builder errors (taking Jenkins worker 3 offline),"""dev@spark.apache.org"" <dev@spark.apache.org>, shane knapp <sknapp@berkeley.edu>","For unknown reasons, pull requests on Jenkins worker 3 have been
failing with an exception[1]. After trying to fix this by clearing the
ivy and maven caches on the node, I've given up and simply blacklisted
that worker.

[error] oro#oro;2.0.8!oro.jar origin location must be absolute:
file:/home/jenkins/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar
java.lang.IllegalArgumentException: oro#oro;2.0.8!oro.jar origin
location must be absolute:
file:/home/jenkins/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar
at org.apache.ivy.util.Checks.checkAbsolute(Checks.java:57)
at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.getArchiveFileInCache(DefaultRepositoryCacheManager.java:385)
at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.download(DefaultRepositoryCacheManager.java:849)
at org.apache.ivy.plugins.resolver.BasicResolver.download(BasicResolver.java:835)
at org.apache.ivy.plugins.resolver.RepositoryResolver.download(RepositoryResolver.java:282)
at org.apache.ivy.plugins.resolver.ChainResolver.download(ChainResolver.java:219)
at org.apache.ivy.plugins.resolver.ChainResolver.download(ChainResolver.java:219)
at org.apache.ivy.core.resolve.ResolveEngine.downloadArtifacts(ResolveEngine.java:388)
at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:331)
at org.apache.ivy.Ivy.resolve(Ivy.java:517)
at sbt.IvyActions$.sbt$IvyActions$$resolve(IvyActions.scala:266)
at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:175)
at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:157)
at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:151)
at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:151)
at sbt.IvySbt$$anonfun$withIvy$1.apply(Ivy.scala:128)
at sbt.IvySbt.sbt$IvySbt$$action$1(Ivy.scala:56)
at sbt.IvySbt$$anon$4.call(Ivy.scala:64)
...

---------------------------------------------------------------------


"
Joe Halliwell <joe.halliwell@gmail.com>,"Tue, 5 May 2015 12:13:35 +0100",Re: Multi-Line JSON in SparkSQL,Ewan Higgs <ewan.higgs@ugent.be>,"I've raised the JSON-related ticket at
https://issues.apache.org/jira/browse/SPARK-7366.

@Ewan I think it would be great to support multiline CSV records too.
The motivation is very similar but my instinct is that little/nothing
of the implementation could be usefully shared, so it's better as a
separate ticket?

Cheers,
Joe

ng.
ual
ML,
te
c
 on the ticket?
ù{‚Äù:‚Äù}‚Äù}} correctly, rather than bailing
arsers in general...)
neral
 input format to
e
).



-- 
Best regards,
Joe

---------------------------------------------------------------------


"
Gidon Gershinsky <GIDON@il.ibm.com>,"Tue, 5 May 2015 16:19:39 +0300",Spark/Mesos,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all, 

I have a few questions on how Spark is integrated with Mesos - any 
details, or pointers to a design document / relevant source, will be much 
appreciated. 

I'm aware of this description, 
https://github.com/apache/spark/blob/master/docs/running-on-mesos.md

But its pretty high-level as far as the design is concerned, while I'm 
looking into lower details on how Spark actually calls the Mesos APIs, how 
it launches the tasks, etc

Namely, 
1. Does Spark creates a Mesos Framework instance for each Spark 
application (SparkContext)? 

2. Citing from the link above, 

""In ""fine-grained"" mode (default), each Spark task runs as a separate 
Mesos task ... comes with an additional overhead in launching each task  "" 


Does it mean that the Mesos slave launches a Spark Executor for each task? 
(unlikely..) Or the slave host has a number of Spark Executors 
pre-launched (one per application), and sends the task to its 
application's executor? 
What is the resource offer then? Is it a host's cpu slice offered to any 
Framework (Spark app/context), that sends the task to run on it? Or its a 
'slice of app Executor' that got idle, and is offered to its Framework? 

3. ""The ""coarse-grained"" mode will instead launch only one long-running 
Spark task on each Mesos machine, and dynamically schedule its own 
""mini-tasks"" within it. "" 

What is this special task? Is it the Spark app Executor? How these 
mini-tasks are different from 'regular' Spark tasks? How the resources are 
allocated/offered in this mode?



Regards, 
Gidon

"
Mark Stewart <mark.stewart@tapjoy.com>,"Tue, 5 May 2015 10:36:54 -0400","Re: practical usage of the new ""exactly-once"" supporting DirectKafkaInputDStream",Cody Koeninger <cody@koeninger.org>,"In case anyone else was having similar issues, the reordering and dropping
of the reduceByKey solved the issues we were having. Thank you kindly, Mr.
Koeninger.


"
Cody Koeninger <cody@koeninger.org>,"Tue, 5 May 2015 09:46:21 -0500","Re: practical usage of the new ""exactly-once"" supporting DirectKafkaInputDStream",Mark Stewart <mark.stewart@tapjoy.com>,"Glad that worked out for you.  I updated the post on my github to hopefully
clarify the issue.


"
Quang-Nhat HOANG-XUAN <hxquangnhat@gmail.com>,"Tue, 5 May 2015 17:53:52 +0200",Scan Sharing in Spark,dev@spark.apache.org,"Hi everyone,

I have two Spark jobs inside a Spark Application, which read from the same
input file.
They are executed in 2 threads.

Right now, I cache the input file into memory before executing these two
jobs.

Are there another ways to share their same input with just only one read?
I know there is something called Multiple Query Optimization, but I don't
know if it can be applicable on Spark (or SparkSQL) or not?

Thank you.

Quang-Nhat
"
shane knapp <sknapp@berkeley.edu>,"Tue, 5 May 2015 08:54:03 -0700",Re: Pull request builder errors (taking Jenkins worker 3 offline),Patrick Wendell <pwendell@gmail.com>,"taking a look now.


"
Manku Timma <manku.timma1@gmail.com>,"Tue, 5 May 2015 21:38:42 +0530",Hive.get() called without HiveConf being already set on a yarn executor,dev@spark.apache.org,"Looks like there is a case in TableReader.scala where Hive.get() is being
called without already setting it via Hive.get(hiveconf). I am running in
yarn-client mode (compiled with -Phive-provided and with hive-0.13.1a).
Basically this means the broadcasted hiveconf is not getting used and the
default HiveConf object is getting created and used -- which sounds wrong.
My understanding is that the HiveConf created on the driver should be used
on all executors for correct behaviour. The query I am running is:

      insert overwrite table X partition(month='2014-12')
      select colA, colB from Y where month='2014-12'

should have been one call to Hive.get(broadcastedHiveConf) somewhere which
runs only on the executor. Let me know if my analysis is correct and I can
file a JIRA For this.


  [1] org.apache.hadoop.hive.ql.metadata.Hive.get (Hive.java:211)
  [2]
org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobPropertiesForStorageHandler
(PlanUtils.java:810)
  [3]
org.apache.hadoop.hive.ql.plan.PlanUtils.configureInputJobPropertiesForStorageHandler
(PlanUtils.java:789)
  [4]
org.apache.spark.sql.hive.HadoopTableReader$.initializeLocalJobConfFunc
(TableReader.scala:253)
  [5] org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply
(TableReader.scala:229)
  [6] org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply
(TableReader.scala:229)
  [7] org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply
(HadoopRDD.scala:172)
  [8] org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply
(HadoopRDD.scala:172)
  [9] scala.Option.map (Option.scala:145)
  [10] org.apache.spark.rdd.HadoopRDD.getJobConf (HadoopRDD.scala:172)
  [11] org.apache.spark.rdd.HadoopRDD$$anon$1.<init> (HadoopRDD.scala:216)
  [12] org.apache.spark.rdd.HadoopRDD.compute (HadoopRDD.scala:212)
  [13] org.apache.spark.rdd.HadoopRDD.compute (HadoopRDD.scala:101)
  [14] org.apache.spark.rdd.RDD.computeOrReadCheckpoint (RDD.scala:277)
  [15] org.apache.spark.rdd.RDD.iterator (RDD.scala:244)
  [16] org.apache.spark.rdd.MapPartitionsRDD.compute
(MapPartitionsRDD.scala:35)
  [17] org.apache.spark.rdd.RDD.computeOrReadCheckpoint (RDD.scala:277)
  [18] org.apache.spark.rdd.RDD.iterator (RDD.scala:244)
  [19] org.apache.spark.rdd.MapPartitionsRDD.compute
(MapPartitionsRDD.scala:35)
  [20] org.apache.spark.rdd.RDD.computeOrReadCheckpoint (RDD.scala:277)
  [21] org.apache.spark.rdd.RDD.iterator (RDD.scala:244)
  [22] org.apache.spark.rdd.UnionRDD.compute (UnionRDD.scala:87)
  [23] org.apache.spark.rdd.RDD.computeOrReadCheckpoint (RDD.scala:277)
  [24] org.apache.spark.rdd.RDD.iterator (RDD.scala:244)
  [25] org.apache.spark.scheduler.ResultTask.runTask (ResultTask.scala:61)
  [26] org.apache.spark.scheduler.Task.run (Task.scala:64)
  [27] org.apache.spark.executor.Executor$TaskRunner.run
(Executor.scala:203)
  [28] java.util.concurrent.ThreadPoolExecutor.runWorker
(ThreadPoolExecutor.java:1,145)
  [29] java.util.concurrent.ThreadPoolExecutor$Worker.run
(ThreadPoolExecutor.java:615)
  [30] java.lang.Thread.run (Thread.java:745)
"
shane knapp <sknapp@berkeley.edu>,"Tue, 5 May 2015 09:15:41 -0700",Re: Pull request builder errors (taking Jenkins worker 3 offline),Patrick Wendell <pwendell@gmail.com>,"hmm, still happening.  looking deeper.


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 5 May 2015 09:23:42 -0700",Re: Scan Sharing in Spark,Quang-Nhat HOANG-XUAN <hxquangnhat@gmail.com>,"Scan sharing can indeed be a useful optimization in spark, because you
amortize not only the time spent scanning over the data, but also time
spent in task launch and scheduling overheads.

Here's a trivial example in scala. I'm not aware of a place in SparkSQL
where this is used - I'd imagine that most development effort is being
placed on single-query optimization right now.

//This function takes a sequence of functions of type A => B and returns a
function of A => Seq[B] where each item in the input list corresponds to a
def combineFunctions[A,B](fns: Seq[A=>B]): A => Seq[B] = {
   def combf(a: A): Seq[B] = {
     fns.map(f => f(a))
   }
  combf
}

def timesFive(x: Int) = x * 5


val data = sc.parallelize(Array(1,2,3,4,5,6,7))

//Apply this combine function to each of your data elements.
val res = data.map(sharedF)

res.take(5)

The result will look something like this.

res5: Array[Seq[Int]] = Array(List(2, 5), List(3, 10), List(4, 15), List(5,
20), List(6, 25))




"
shane knapp <sknapp@berkeley.edu>,"Tue, 5 May 2015 09:33:43 -0700",Re: Pull request builder errors (taking Jenkins worker 3 offline),Patrick Wendell <pwendell@gmail.com>,"ok, i reset the maven cache on amp-jenkins-worker-03 and some stuff is
currently building and not failing...  i'll keep a close eye on this for
now.


"
Hector Yee <hector.yee@gmail.com>,"Tue, 5 May 2015 11:15:19 -0700",Re: Spark/Mesos,Gidon Gershinsky <GIDON@il.ibm.com>,"Speaking as a user of spark on mesos

Yes it appears that each app appears as a separate framework on the mesos
master

In fine grained mode the number of executors goes up and down vs fixed in
coarse.
I would not run fine grained mode on a large cluster as it can potentially
spin up a lot of executors and DDOS the mesos master.
In a shared environment coarse grained seems better behaved as you can cap
the number of executors with spark --conf spark.cores.max
and the executors stick around as opposed to growing and shrinking per
stage.
"
Timothy Chen <tnachen@gmail.com>,"Tue, 5 May 2015 12:54:55 -0700",Re: Spark/Mesos,Gidon Gershinsky <GIDON@il.ibm.com>,"Hi Gidon,

1. Yes, each Spark application is wrapped in a new Mesos framework.

2. In fine grained mode, what happens is that Spark scheduler
specifies a custom Mesos executor per slave, and each Mesos task is a
Spark executor that will be launched by the Mesos executor. It's hard
to determine what exactly you're asking since task and executors are
both terms used in Spark and Mesos, perhaps prefixing (Mesos|Spark)
task will clarify more what you're asking about.

I'm not sure what you mean by slice of app executor, but in fine grain
mode there is a fixed cost of resource to launch a per slave executor,
and then cpu/mem cost to launch each Mesos task that launches a Spark
executor. Each framework is given offers by Mesos master and each have
the opportunity to use a offer or not.

3. In coarse-grained mode the scheduler launches
CoarseGrainedExecutorBackend on each slave, and it will be registering
back to the CoarseGrainedSchedulerBackend via the akka driverUrl. Then
the CoarseGrainedSchedulerBackend can scheduler individual Spark tasks
to those long running executor backends. These mini-tasks I believe
it's the same as Spark tasks, but instead of running a Mesos Task per
Spark task it's distributing these tasks to these long running Spark
executors.

Mesos Resources becomes more static in coarse grained mode as it will
just launch a number of these CoarseGrainedExecutorBackends and keep
them running until the driver stops. Note this is subject to change
with dynamic allocation and other Spark/Mesos patches going into
Spark.

Tim


---------------------------------------------------------------------


"
Tony Stevenson <tony@pc-tony.com>,"Tue, 5 May 2015 21:09:40 +0100",Re: Typo on Spark SQL web page,Kathy Wilson <kwilson@tableau.com>,"Kathy,

Thank you. I have CCd the project so they can resolve this.




b page (


-- 
Cheers,
Tony

----------------------------------
Tony Stevenson

tony@pc-tony.com
pctony@apache.org

http://www.pc-tony.com

GPG - 1024D/51047D66
----------------------------------
"
BenFradet <benjamin.fradet@gmail.com>,"Tue, 5 May 2015 13:19:24 -0700 (MST)",New Kafka producer API,dev@spark.apache.org,"Hi,

Since we're now supporting  Kafka 0.8.2.1
<https://github.com/apache/spark/pull/4537>  , and that there is a  new
Producer API <https://kafka.apache.org/documentation.html#producerapi>  
with this version, I was wondering if we should convert to this new API in 
KafkaTestUtils
<https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaTestUtils.scala>  
and the different examples.

Ben.



--

---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Tue, 5 May 2015 15:24:57 -0500",Re: Thanking Test Partners,Patrick Wendell <pwendell@gmail.com>,"+1

testing is super important, it'll be good to give recognition for it.


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Tue, 5 May 2015 16:25:56 -0400",Re: [discuss] ending support for Java 6?,"Patrick Wendell <pwendell@gmail.com>, shane knapp <sknapp@berkeley.edu>","+1 in favor of dropping Java1.6 support.
+1 in favor of doing a wide ANNOUNCE to the user and dev groups declaring
which version of Spark (sounds like 1.5) will drop support and when (if it
isnπt already posted somewhere) Spark 1.5 will release.








_"
Cody Koeninger <cody@koeninger.org>,"Tue, 5 May 2015 15:38:50 -0500",Re: New Kafka producer API,BenFradet <benjamin.fradet@gmail.com>,"Since that's an internal class used only for unit testing, what would the
benefit be?



"
BenFradet <benjamin.fradet@gmail.com>,"Tue, 5 May 2015 13:44:19 -0700 (MST)",Re: New Kafka producer API,dev@spark.apache.org,"Even if it's only used for testing and the examples, why not move ahead of
the deprecation and gain some performance along the way.

Plus, regarding the examples, I think it's good practice to use the
recommended API and not the legacy one.



--

---------------------------------------------------------------------


"
Junior <contato@engenhariavagas.com.br>,"Tue, 5 May 2015 13:52:29 -0700 (MST)",NP-Complete Design Choices in Spark Implementation,dev@spark.apache.org,"Is there any NP-Complete related problem that Spark or Spark Streaming
engineers needed to address efficiently during design/code implementation in
order to achieve satisfactory performance? Can you mention some example of
this NP-Complete and the implementation choice?



--

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Tue, 5 May 2015 15:55:47 -0500",Re: New Kafka producer API,BenFradet <benjamin.fradet@gmail.com>,"Regarding performance, keep in mind we'd probably have to turn all those
async calls into blocking calls for the unit tests


"
Quang-Nhat HOANG-XUAN <hxquangnhat@gmail.com>,"Tue, 5 May 2015 22:59:05 +0200",Re: Scan Sharing in Spark,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Hi,
Beside caching, is it possible if an RDD has multiple child RDDs? So I can
read the input one and  produce multiple outputs for multiple jobs which
share the input.

"
BenFradet <benjamin.fradet@gmail.com>,"Tue, 5 May 2015 14:04:49 -0700 (MST)",Re: New Kafka producer API,dev@spark.apache.org,"Yes that might be true, I will have to test that.



--

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 5 May 2015 21:24:55 +0000 (UTC)",Re: [discuss] ending support for Java 6?,"Patrick Wendell <pwendell@gmail.com>, shane knapp <sknapp@berkeley.edu>","+1. I haven't seen major objections here so I would say send announcement and see if any users have objections
Tom 


   

 If there is broad consensus here to drop Java 1.6 in Spark 1.5, should
we do an ANNOUNCE to user and dev?

te:
.
a
f
e:
† if
f
re
m"
shane knapp <sknapp@berkeley.edu>,"Tue, 5 May 2015 14:47:57 -0700",Re: [discuss] ending support for Java 6?,Tom Graves <tgraves_cs@yahoo.com>,"+1 to an announce to user and dev.  java6 is so old and sad.


"
shane knapp <sknapp@berkeley.edu>,"Tue, 5 May 2015 15:08:40 -0700",Re: Pull request builder errors (taking Jenkins worker 3 offline),Patrick Wendell <pwendell@gmail.com>,"alright, this is happening again w/this worker and i will be taking it
offline for further investigation.  i'm OOO for the rest of the day, but
will check in again later this evening.


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 May 2015 15:25:35 -0700",Re: [discuss] ending support for Java 6?,shane knapp <sknapp@berkeley.edu>,"OK I sent an email.



"
Reynold Xin <rxin@databricks.com>,"Tue, 5 May 2015 15:25:21 -0700",[ANNOUNCE] Ending Java 6 support in Spark 1.5 (Sep 2015),"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi all,

We will drop support for Java 6 starting Spark 1.5, tentative scheduled to
be released in Sep 2015. Spark 1.4, scheduled to be released in June 2015,
will be the last minor release that supports Java 6. That is to say:

Spark 1.4.x (~ Jun 2015): will work with Java 6, 7, 8.

Spark 1.5+ (~ Sep 2015): will NOT work with Java 6, but work with Java 7, 8.


PS: Oracle ended Java 6 updates in Feb 2013.
"
=?UTF-8?B?5ZC05piO55Gc?= <timberonce@gmail.com>,"Wed, 6 May 2015 09:27:01 +0800",Re: LDA and PageRank Using GraphX,Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"There is a PageRank algorithm in the lib package of graphx. And you can
find an example to invoke it in SynthBenchmark.scala in
org.apache.spark.examples.graphx.

2015-05-03 16:52 GMT+08:00 Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>
:




-- 
Mingyu Wu

Institute of Parallel and Distributed Systems

School of Software Engineering

Shanghai Jiao Tong University
"
Sean Owen <sowen@cloudera.com>,"Wed, 6 May 2015 06:57:12 +0100",Re: [ANNOUNCE] Ending Java 6 support in Spark 1.5 (Sep 2015),dev@spark.apache.org,"OK to file a JIRA to scrape out a few Java 6-specific things in the
code? and/or close issues about working with Java 6 if they're not
going to be resolved for 1.4?

I suppose this means the master builds and PR builder in Jenkins
should simply continue to use Java 7 then.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 May 2015 22:59:21 -0700",Re: [ANNOUNCE] Ending Java 6 support in Spark 1.5 (Sep 2015),Sean Owen <sowen@cloudera.com>,"Sean - Please do.


"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Tue, 5 May 2015 23:00:40 -0700",Re: Speeding up Spark build during development,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"I had to make a small change to Emre's suggestion above, in order for my
changes to get picked up. This worked for me:
mvn --projects sql/core -DskipTests install #not package
mvn --projects assembly/ -DskipTests install

Pramod

com>

l
:
of
m
t
ch
ts
l.
d
ildmvn
d
in
ich
r
,
r.
"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 5 May 2015 23:32:32 -0700",Re: Pickling error when attempting to add a method in pyspark,Stephen Boesch <javadba@gmail.com>,"Hi Stephen,

I think it would be easier to see what you implemented by showing the
branch diff link on github. There are couple utility class to make
Rating work between Scala and Python:

1. serializer: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/api/python/PythonMLLibAPI.scala#L1163
2. mark it as picklable:
https://github.com/apache/spark/blob/master/python/pyspark/mllib/common.py#L56

However, I don't recommend you following this approach. It is much
simpler to use DataFrames for serialization. You can find an example
here:

https://github.com/apache/spark/blob/master/python/pyspark/mllib/evaluation.py#L23

Best,
Xiangrui

in
 I
n-expected-zero-arguments-for-construction-of-class>
n
-expected-zero-arguments-for-construction-of-class
rk,
ting))
try"", [""x"",""y"",""weight""])):
ht))
i * 100 + d.j * 10 + d.value)}
trdd.collect()))
 14)
uction of ClassDict(for pyspark.mllib.clustering.MatrixEntry)
DictConstructor.java:23)
anonfun$apply$1.apply(PythonMLLibAPI.scala:1167)
anonfun$apply$1.apply(PythonMLLibAPI.scala:1166)
ala:48)
ala:103)
ala:47)
73)
cala:265)
ala:252)
t.scala:1523)
t.scala:1523)
)
212)
tor.java:1145)
utor.java:615)
st)
gs)
ay.pyc
yc
n'.

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 5 May 2015 23:37:57 -0700",Re: OOM error with GMMs on 4GB dataset,Vinay Muttineni <vmuttineni@ebay.com>,"Did you set `--driver-memory` with spark-submit? -Xiangrui


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 6 May 2015 07:46:06 +0100",Re: Typo on Spark SQL web page,Tony Stevenson <tony@pc-tony.com>,"Oops, fixed now.

eb page (

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 5 May 2015 23:46:59 -0700",Re: [discuss] ending support for Java 6?,Reynold Xin <rxin@databricks.com>,"assembly jar, it will use zip64. Could Python 2.x (or even 3.x) be
able to load zip64 files on PYTHONPATH? -Xiangrui


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 May 2015 23:47:54 -0700",Re: [discuss] ending support for Java 6?,Xiangrui Meng <mengxr@gmail.com>,"@tgraves can chime in, but I think this pr aims to fix it:
https://github.com/apache/spark/pull/5580

We should probably get that in for 1.4.



"
anshu shukla <anshushukla0@gmail.com>,"Wed, 6 May 2015 13:23:35 +0530",Creating topology in spark streaming,"dev@spark.apache.org, user@spark.apache.org","Please help  guys, Even  After going through all the examples given i have
not understood how to pass the  D-streams  from one bolt/logic to other
(without writing it on HDFS etc.) just like emit function in storm .
Suppose i have topology with 3  bolts(say)

*BOLT1(parse the tweets nd emit tweet using given
hashtags)=====>Bolt2(Complex logic for sentiment analysis over
tweets)=======>BOLT3(submit tweets to the sql database using spark SQL)*


Now  since Sentiment analysis will take most of the time ,we have to
increase its level of parallelism for tuning latency. Howe to increase the
levele of parallelism since the logic of topology is not clear .

-- 
Thanks & Regards,
Anshu Shukla
Indian Institute of Sciences
"
anshu shukla <anshushukla0@gmail.com>,"Wed, 6 May 2015 14:02:49 +0530",Re: Creating topology in spark streaming,ayan guha <guha.ayan@gmail.com>,"But main problem is how to increase the level of parallelism  for any
particular bolt logic .

suppose i  want  this type of topology .

https://storm.apache.org/documentation/images/topology.png

How we can manage it .




-- 
Thanks & Regards,
Anshu Shukla
"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Wed, 6 May 2015 11:17:28 +0200",Re: Creating topology in spark streaming,anshu shukla <anshushukla0@gmail.com>,"Hi,

You can use the method repartition from DStream (for the Scala API) or
JavaDStream (for the Java API)

defrepartition(numPartitions: Int): DStream
<https://spark.apache.org/docs/latest/api/scala/org/apache/spark/streaming/dstream/DStream.html>
[T]

Return a new DStream with an increased or decreased level of parallelism.
Each RDD in the returned DStream has exactly numPartitions partitions.

I think the post
http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/
on integration of Spark Streaming gives very interesting review on the
subject, although the integration with Kafka it's not up to date with
https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html

Hope that helps.

Greetings,

Juan

2015-05-06 10:32 GMT+02:00 anshu shukla <anshushukla0@gmail.com>:

"
anshu shukla <anshushukla0@gmail.com>,"Wed, 6 May 2015 15:04:59 +0530",Re: Creating topology in spark streaming,=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Thanks alot Juan,

 telling how to configure or create  a topology of different types .. i
mean how we can decide the pipelining model in spark  as done in storm  for
  https://storm.apache.org/documentation/images/topology.png .



g/dstream/DStream.html>
tion-example-tutorial/
of-spark-streaming.html
le
rm .
er
sing spark SQL)*
 the


-- 
Thanks & Regards,
Anshu Shukla
"
Gidon Gershinsky <GIDON@il.ibm.com>,"Wed, 6 May 2015 14:53:52 +0300",Re: Spark/Mesos,Timothy Chen <tnachen@gmail.com>,"Thanks Tim, a few  follow-up questions using the Mesos|Spark prefixing - 




So, to confirm - in this mode, when a Spark application/context runs a 
series of tasks, each task will launch a full Spark Executor process?
What is the cpu/mem cost of such Spark Executor process (resource sizing 
passed in the Mesos task launch request)?



I see. Is a per-task resource management somehow performed in this mode? 
In other words, if the size of CoarseGrainedExecutorBackend 
process is M megabytes of memory and C cpu cores, how many Spark tasks can 
be sent to it for execution at the same time (before starting to queue 
them at the driver process)? Is each Spark task sized in terms of cpu/mem?



What is the current target for a release (or patch landing) of the dynamic 
allocation in Spark/Mesos?


much
how
task  ""
task?
any
its a
Framework?
long-running
are
"
Sam Bessalah <samkiller.oss@gmail.com>,"Wed, 6 May 2015 15:41:53 +0200",Re: Spark/Mesos,Timothy Chen <tnachen@gmail.com>,"Hi Tim.
Just a follow up, more related to your work on the rencently merged Spark
Cluster Mode for Mesos.
Can you elaborate how it works compared to the Standalone mode.
and do you maintain the dyanamic allocation of mesos resources in the
cluster mode unlike the coarse grained mode?


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 6 May 2015 14:54:53 +0000 (UTC)",Re: [discuss] ending support for Java 6?,"Reynold Xin <rxin@databricks.com>, Xiangrui Meng <mengxr@gmail.com>","That is correct. I plan to try it out and review it today.
Tom 


rote:
   

 @tgraves can chime in, but I think this pr aims to fix it:¬†https://github.com/apache/spark/pull/5580
We should probably get that in for 1.4.


assembly jar, it will use zip64. Could Python 2.x (or even 3.x) be
able to load zip64 files on PYTHONPATH? -Xiangrui

nt
gmail.com>
:
stuff
it
ibs
)
 should
du
ond




  "
"""A.M.Chan"" <kaka_1992@163.com>","Wed, 6 May 2015 23:00:26 +0800 (CST)","jackson.databind exception in
 RDDOperationScope.jsonMapper.writeValueAsString(this)",spark-dev <dev@spark.apache.org>,"Hey, gays. I meet this exception while testing SQL/Columns.
I didn't change the pom or the core project.
In the morning, it's fine to test my PR.
I don't know what happed.


An exception or error caused a run to abort: com.fasterxml.jackson.databind.introspect.POJOPropertyBuilder.addField(Lcom/fasterxml/jackson/databind/introspect/AnnotatedField;Lcom/fasterxml/jackson/databind/PropertyName;ZZZ)V 
java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.introspect.POJOPropertyBuilder.addField(Lcom/fasterxml/jackson/databind/introspect/AnnotatedField;Lcom/fasterxml/jackson/databind/PropertyName;ZZZ)V
at com.fasterxml.jackson.module.scala.introspect.ScalaPropertiesCollector.com$fasterxml$jackson$module$scala$introspect$ScalaPropertiesCollector$$_addField(ScalaPropertiesCollector.scala:109)
at com.fasterxml.jackson.module.scala.introspect.ScalaPropertiesCollector$$anonfun$_addFields$2$$anonfun$apply$11.apply(ScalaPropertiesCollector.scala:100)
at com.fasterxml.jackson.module.scala.introspect.ScalaPropertiesCollector$$anonfun$_addFields$2$$anonfun$apply$11.apply(ScalaPropertiesCollector.scala:99)
at scala.Option.foreach(Option.scala:236)
at com.fasterxml.jackson.module.scala.introspect.ScalaPropertiesCollector$$anonfun$_addFields$2.apply(ScalaPropertiesCollector.scala:99)
at com.fasterxml.jackson.module.scala.introspect.ScalaPropertiesCollector$$anonfun$_addFields$2.apply(ScalaPropertiesCollector.scala:93)
at scala.collection.GenTraversableViewLike$Filtered$$anonfun$foreach$4.apply(GenTraversableViewLike.scala:109)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.SeqLike$$anon$2.foreach(SeqLike.scala:635)
at scala.collection.GenTraversableViewLike$Filtered$class.foreach(GenTraversableViewLike.scala:108)
at scala.collection.SeqViewLike$$anon$5.foreach(SeqViewLike.scala:80)
at com.fasterxml.jackson.module.scala.introspect.ScalaPropertiesCollector._addFields(ScalaPropertiesCollector.scala:93)
at com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collect(POJOPropertiesCollector.java:233)
at com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.collectProperties(BasicClassIntrospector.java:142)
at com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.forSerialization(BasicClassIntrospector.java:68)
at com.fasterxml.jackson.databind.introspect.BasicClassIntrospector.forSerialization(BasicClassIntrospector.java:11)
at com.fasterxml.jackson.databind.SerializationConfig.introspect(SerializationConfig.java:530)
at com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer(BeanSerializerFactory.java:133)
at com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer(SerializerProvider.java:1077)
at com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer(SerializerProvider.java:1037)
at com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer(SerializerProvider.java:445)
at com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer(SerializerProvider.java:599)
at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:93)
at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:2811)
at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:2268)
at org.apache.spark.rdd.RDDOperationScope.toJson(RDDOperationScope.scala:51)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:124)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:99)
at org.apache.spark.SparkContext.withScope(SparkContext.scala:671)
at org.apache.spark.SparkContext.parallelize(SparkContext.scala:685)





--

A.M.Chan"
Ted Yu <yuzhihong@gmail.com>,"Wed, 6 May 2015 08:59:01 -0700",Re: jackson.databind exception in RDDOperationScope.jsonMapper.writeValueAsString(this),"""A.M.Chan"" <kaka_1992@163.com>","Looks like mismatch of jackson version.
Spark uses:
    <fasterxml.jackson.version>2.4.4</fasterxml.jackson.version>

FYI


"
shane knapp <sknapp@berkeley.edu>,"Wed, 6 May 2015 10:12:26 -0700",Re: Pull request builder errors (taking Jenkins worker 3 offline),Patrick Wendell <pwendell@gmail.com>,"ok, i looked deeper and this is only happening on -03, and not linked
specifically to the pull request builder:
      3 NewSparkPullRequestBuilder
     13 Spark-Master-SBT
      4 Spark-1.4-SBT
     49 SparkPullRequestBuilder

also, it started at ~3pm on this past sunday...  and nothing was done to
this worker then.  :\

anyways, to continue testing and make sure we have no poisoned caches, i
nuked the ivy, sbt and maven caches, as well as wiping the jenkins
workspace.

i'll be turning on this worker, and i set up some monitoring and will be
watching it today.

if this fails, i'll just wipe and reinstall the node.  sorry for the
inconvenience!


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 6 May 2015 17:32:06 +0000 (UTC)",kryo version?,Dev <dev@spark.apache.org>,"Hey folks,
I had a customer ask about updating the version of kryo to get fix:¬†https://github.com/EsotericSoftware/kryo/pull/164¬†which is in 2.23.Spark currently pull sin chill 0.5.0 which pulls in kryo 2.21. ¬†I don't see a newer version of chill that has updated to kryo 2.23. ¬†
Anyone familiar with how the chill releases work to know when they might update or have run into this similar issue with kryo?
Thanks,Tom¬†"
Reynold Xin <rxin@databricks.com>,"Wed, 6 May 2015 10:37:08 -0700",Re: kryo version?,Tom Graves <tgraves_cs@yahoo.com>,"They are usually pretty responsive. We can ping chill to get them to do a
release.



"
Timothy Chen <tnachen@gmail.com>,"Wed, 6 May 2015 14:24:23 -0700",Re: Spark/Mesos,Gidon Gershinsky <GIDON@il.ibm.com>,"So, to confirm - in this mode, when a Spark application/context runs a
series of tasks, each task will launch a full SparkExecutor process?
What is the cpu/mem cost of such Spark Executor process (resource
sizing passed in the Mesos task launch request)?
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Each Spark task is launched on a running MesosExecutorBackend process,
with the task data serialized and passed to the executor. So this is
not a full JVM process, you can look at MesosExecutorBackend to see
how it's being launched.
Each Spark task is sized to be spark.task.cpus (default 1) and memory
is calculated by
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MemoryUtils.scala.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
I see. Is a per-task resource management somehow performed in this
mode? In other words, if the size of CoarseGrainedExecutorBackend
process is M megabytes of memory and C cpu cores, how many Spark tasks
can be sent to it for execution at the same time (before starting to
queue them at the driver process)? Is each Spark task sized in terms
of cpu/mem?
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

CoarseGrainedSchedulerBackend is responsible for choosing how many
tasks it will be sending to each running ExecutorBackend, you can
follow the logic at
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L167


------------------------------------------------------------------------------------------------------------------------------------------------------------------------
What is the current target for a release (or patch landing) of the
dynamic allocation in Spark/Mesos?
------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Dynamic allocation for coarse grained mode I'm hoping it lands 1.4,
but will see how it goes! (https://github.com/apache/spark/pull/4984)


---------------------------------------------------------------------


"
Timothy Chen <tnachen@gmail.com>,"Wed, 6 May 2015 14:29:52 -0700",Re: Spark/Mesos,Sam Bessalah <samkiller.oss@gmail.com>,"Hi Sam,

Spark cluster mode merge should also include documentation update that
has details what it is, but in a nutshell it's basically supporting
launching drivers that is managed in your cluster instead of launching
it yourself via client mode. YARN and Standalone both supports cluster
mode, so I've added the support for that in Mesos, and it supports HA
with zookeeper, supervise mode, and also comes with its own Mesos
cluster mode web ui that shows basic information for now. It doesn't
support PySpark yet, and in the future can ideally link to the running
driver WebUI and Mesos sandbox directly to see the results.

Cluster mode actually has nothing to do with dynamic allocation, it's
simply running and managing the Spark drivers for each app via a Mesos
framework.

Tim


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 6 May 2015 15:14:08 -0700",Re: Pull request builder errors (taking Jenkins worker 3 offline),Patrick Wendell <pwendell@gmail.com>,"ok, things are looking good...  i'll definitely be keeping an eye on this
worker, but it looks like the ivy cache somehow got poisoned and affected
the builds.

it's still not clear to me *how* this happened, but *what* happened is
clear:  the ivydata properties file for oro is-local=true for the jar, and
then it was barfing while parsing the local file: path.

all other properties files on the workers have that flag set to false, and
a nice url pointing to repo1.maven.org.

after wiping the .ivy2 repo, it was repopulated and the flag (on -03) is
now set to false, and we're good.

shane


"
Andrew Or <andrew@databricks.com>,"Wed, 6 May 2015 17:41:56 -0700",Recent Spark test failures,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear all,

I'm sure you have all noticed that the Spark tests have been fairly
unstable recently. I wanted to share a tool that I use to track which tests
have been failing most often in order to prioritize fixing these flaky
tests.

Here is an output of the tool. This spreadsheet reports the top 10 failed
tests this week (ending yesterday 5/5):
https://docs.google.com/spreadsheets/d/1Iv_UDaTFGTMad1sOQ_s4ddWr6KD3PuFIHmTSzL7LSb4

It is produced by a small project:
https://github.com/andrewor14/spark-test-failures

I have been filing JIRAs on flaky tests based on this tool. Hopefully we
can collectively stabilize the build a little more as we near the release
for Spark 1.4.

-Andrew
"
shane knapp <sknapp@berkeley.edu>,"Wed, 6 May 2015 17:44:18 -0700",[build system] quick jenkins restart thursday morning (5-6-15) 7am PDT,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","we've had a spate of issues since the power outage, and now the github pull
request builder is randomly deciding who can and can't trigger builds[1].

i think it's time for a quick restart of the master and workers, which i'll
do early tomorrow morning.  the outage should be very brief, and i'll let
everyone know how it's going.

sorry again for the inconvenience.

shane
"
Reynold Xin <rxin@databricks.com>,"Wed, 6 May 2015 17:45:09 -0700",Re: Recent Spark test failures,Andrew Or <andrew@databricks.com>,"Thanks for doing this. Testing infra is one of the most important parts of
a project, and this will make it easier to identify flaky tests.



"
Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"Wed, 6 May 2015 19:16:43 -0700",unable to extract tgz files downloaded from spark,dev@spark.apache.org,"Hi
I have been trying to install latest spark verison and downloaded the .tgz
files(ex spark-1.3.1.tgz). But, I could not extract them. It complains of
invalid tar format.
Has any seen this issue ?

Thanks
Praveen
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 6 May 2015 19:32:51 -0700",Re: unable to extract tgz files downloaded from spark,Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"
Which package type did you choose (pre-built for which distro) ?

Thanks


"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Thu, 7 May 2015 00:10:11 -0700",Re: unable to extract tgz files downloaded from spark,Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"This happens sometimes when the download gets stopped or corrupted. You can
verify the integrity of your file by comparing with the md5 and sha
signatures published here: http://www.apache.org/dist/spark/spark-1.3.1/

Pramod


"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 7 May 2015 12:51:21 +0530",SparkStreaming Workaround for BlockNotFound Exceptions,dev <dev@spark.apache.org>,"Hi

With Spark streaming (all versions), when my processing delay (around 2-4
seconds) exceeds the batch duration (being 1 second) and on a decent
scale/throughput (consuming around 100MB/s on 1+2 node standalone 15GB, 4
cores each) the job will start to throw block not found exceptions when the
Storage is set to MEMORY_ONLY (ensureFreeSpace
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/MemoryStore.scala#L444>
drops
blocks blindly). When i use MEMORY_AND_DISK* as StorageLevel, then the
performance went down drastically and the receivers ends up doing a lot of
Disk IO.

So sticking with StorageLevel as MEMORY_ONLY the workaround to get ride of
the block not found exceptions was to tell the receiver not to generate
more blocks as there are blocks which are yet to get compute.

To achieve this, i used Spark 1.3.1 with the low level kafka consumer
<https://github.com/dibbhatt/kafka-spark-consumer>, and inside my Job's
onBatchCompleted i pushed the scheduling delay to zookeeper like:

[image: Inline image 1]


And on the receiver end, if there's scheduling delay, then it will simply
sleep for that much of time without sending any blocks to the Streaming
receiver. like:
[image: Inline image 2]


I could also add a condition there not to generate blocks if the scheduling
delay kind of exceeds 2-3 times the batch duration instead of making it
sleep for whatever scheduling delay is happening.

With this, the only problem I'm having is, some batches have empty data as
the receiver went to sleep for those batches. Everything else works nicely
at scale and the block not found is totally gone.

Please let me know your thoughts on this, can we generalize this for Kakfa
receivers with Sparkstreaming? Is it possible to apply this (stopping the
receiver from generating blocks) for all sort of receivers?


Thanks
Best Regards
"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Thu, 7 May 2015 13:46:44 +0530",Spark Streaming with Tachyon : Some findings,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear All ,

I have been playing with Spark Streaming on Tachyon as the OFF_HEAP block
store  . Primary reason for evaluating Tachyon is to find if Tachyon can
solve the Spark BlockNotFoundException .

In traditional MEMORY_ONLY StorageLevel, when blocks are evicted , jobs
failed due to block not found exception and storing blocks in
MEMORY_AND_DISK is not a good option either as it impact the throughput a
lot .


To test how Tachyon behave , I took the latest spark 1.4 from master , and
used Tachyon 0.6.4 and configured Tachyon in Fault Tolerant Mode . Tachyon
is running in 3 Node AWS x-large cluster and Spark is running in 3 node AWS
x-large cluster.

I have used the low level Receiver based Kafka consumer (
https://github.com/dibbhatt/kafka-spark-consumer)  which I have written to
pull from Kafka and write Blocks to Tachyon


I found there is similar improvement in throughput (as MEMORY_ONLY case )
but very good overall memory utilization (as it is off heap store) .


But I found one issue on which I need to clarification .


In Tachyon case also , I find  BlockNotFoundException  , but due to a
different reason .  What I see TachyonBlockManager.scala put the blocks in
WriteType.TRY_CACHE configuration . And because of this Blocks ate evicted
from Tachyon Cache and when Spark try to find the block it throws
 BlockNotFoundException .

I see a pull request which discuss the same ..

https://github.com/apache/spark/pull/158#discussion_r11195271


When I modified the WriteType to CACHE_THROUGH , BlockDropException is gone
, but it again impact the throughput ..


Just curious to know , if Tachyon has any settings which can solve the
Block Eviction from Cache to Disk, other than explicitly setting
CACHE_THROUGH  ?

Regards,
Dibyendu
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 7 May 2015 09:32:50 +0100",DataFrame distinct vs RDD distinct,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
there seems to be different implementations of the ""distinct"" feature in
DataFrames and RDD and some performance issue with the DataFrame distinct
API.

In RDD.scala :

def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] =
withScope { map(x => (x, null)).reduceByKey((x, y) => x,
numPartitions).map(_._1) }
And in DataFrame :


case class Distinct(partial: Boolean, child: SparkPlan) extends UnaryNode {
override def output: Seq[Attribute] = child.output override def
requiredChildDistribution: Seq[Distribution] = if (partial)
UnspecifiedDistribution :: Nil else ClusteredDistribution(child.output) ::
Nil *override def execute(): RDD[Row] = {** child.execute().mapPartitions {
iter =>** val hashSet = new scala.collection.mutable.HashSet[Row]()* * var
currentRow: Row = null** while (iter.hasNext) {** currentRow = iter.next()**
if (!hashSet.contains(currentRow)) {** hashSet.add(currentRow.copy())** }**
}* * hashSet.iterator** }** }*}




I can try to reproduce more clearly the performance issue, but do you have
any insights into why we can't have the same distinct strategy between
DataFrame and RDD ?

Regards,

Olivier.
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 7 May 2015 09:32:50 +0100",DataFrame distinct vs RDD distinct,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
there seems to be different implementations of the ""distinct"" feature in
DataFrames and RDD and some performance issue with the DataFrame distinct
API.

In RDD.scala :

def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] =
withScope { map(x => (x, null)).reduceByKey((x, y) => x,
numPartitions).map(_._1) }
And in DataFrame :


case class Distinct(partial: Boolean, child: SparkPlan) extends UnaryNode {
override def output: Seq[Attribute] = child.output override def
requiredChildDistribution: Seq[Distribution] = if (partial)
UnspecifiedDistribution :: Nil else ClusteredDistribution(child.output) ::
Nil *override def execute(): RDD[Row] = {** child.execute().mapPartitions {
iter =>** val hashSet = new scala.collection.mutable.HashSet[Row]()* * var
currentRow: Row = null** while (iter.hasNext) {** currentRow = iter.next()**
if (!hashSet.contains(currentRow)) {** hashSet.add(currentRow.copy())** }**
}* * hashSet.iterator** }** }*}




I can try to reproduce more clearly the performance issue, but do you have
any insights into why we can't have the same distinct strategy between
DataFrame and RDD ?

Regards,

Olivier.
"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 7 May 2015 14:30:01 +0530",Re: SparkStreaming Workaround for BlockNotFound Exceptions,dev <dev@spark.apache.org>,"oops, missed the images in the previous mail.

With Spark streaming (all versions), when my processing delay (around 2-4
seconds) exceeds the batch duration (being 1 second) and on a decent
scale/throughput (consuming around 100MB/s on 1+2 node standalone 15GB, 4
cores each) the job will start to throw block not found exceptions when the
Storage is set to MEMORY_ONLY (ensureFreeSpace
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/MemoryStore.scala#L444>
drops
blocks blindly). When i use MEMORY_AND_DISK* as StorageLevel, then the
performance went down drastically and the receivers ends up doing a lot of
Disk IO.

So sticking with StorageLevel as MEMORY_ONLY the workaround to get ride of
the block not found exceptions was to tell the receiver not to generate
more blocks as there are blocks which are yet to get compute.

To achieve this, i used Spark 1.3.1 with the low level kafka consumer
<https://github.com/dibbhatt/kafka-spark-consumer>, and inside my Job's
onBatchCompleted i pushed the scheduling delay to zookeeper like:

https://www.dropbox.com/s/pm9eecw2xxf5pcp/driver.png?dl=0


‚Äã


And on the receiver end, if there's scheduling delay, then it will simply
sleep for that much of time without sending any blocks to the Streaming
receiver. like:

https://www.dropbox.com/s/wgxnn6j61x46dxo/receiver.png?dl=0


‚Äã

I could also add a condition there not to generate blocks if the scheduling
delay kind of exceeds 2-3 times the batch duration instead of making it
sleep for whatever scheduling delay is happening.

With this, the only problem I'm having is, some batches have empty data as
the receiver went to sleep for those batches. Everything else works nicely
at scale and the block not found is totally gone.

Please let me know your thoughts on this, can we generalize this for Kakfa
receivers with Sparkstreaming? Is it possible to apply this (stopping the
receiver from generating blocks) for all sort of receivers?

Thanks
Best Regards


he
he/spark/storage/MemoryStore.scala#L444> drops
f
f
s
y
a

---------------------------------------------------------------------"
bbarbieru <bbarbieru@outlook.com>,"Thu, 7 May 2015 01:50:16 -0700 (MST)",Automatic testing for Spark App developed in Python,dev@spark.apache.org,"I am trying to configure a spark application to run automatic tests using a
spark local context.The part that doesn't work is when I try importing the
functions defined in my main module intro the test_main module. I am using
__init__.py files to configure the project structure, but I guess the
pyworker is not aware of project structure.There is a more detailed
explanation of my problem  here
<http://stackoverflow.com/questions/30079934/pyspark-automatic-testing-using-nosetests> 
Thanks!



--"
Reynold Xin <rxin@databricks.com>,"Thu, 7 May 2015 02:08:00 -0700",Re: DataFrame distinct vs RDD distinct,Olivier Girardot <o.girardot@lateral-thoughts.com>,"In 1.5, we will most likely just rewrite distinct in SQL to either use the
Aggregate operator which will benefit from all the Tungsten optimizations,
or have a Tungsten version of distinct for SQL/DataFrame.


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 07 May 2015 09:23:47 +0000",Re: DataFrame distinct vs RDD distinct,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Ok, but for the moment, this seems to be killing performances on some
computations...
I'll try to give you precise figures on this between rdd and dataframe.

Olivier.

Le jeu. 7 mai 2015 √† 10:08, Reynold Xin <rxin@databricks.com> a √©crit :

e
,
t
e
::
ns {
 * var
ve
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 07 May 2015 09:23:47 +0000",Re: DataFrame distinct vs RDD distinct,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Ok, but for the moment, this seems to be killing performances on some
computations...
I'll try to give you precise figures on this between rdd and dataframe.

Olivier.

Le jeu. 7 mai 2015 √† 10:08, Reynold Xin <rxin@databricks.com> a √©crit :

e
,
t
e
::
ns {
 * var
ve
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 7 May 2015 09:32:50 +0100",DataFrame distinct vs RDD distinct,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
there seems to be different implementations of the ""distinct"" feature in
DataFrames and RDD and some performance issue with the DataFrame distinct
API.

In RDD.scala :

def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] =
withScope { map(x => (x, null)).reduceByKey((x, y) => x,
numPartitions).map(_._1) }
And in DataFrame :


case class Distinct(partial: Boolean, child: SparkPlan) extends UnaryNode {
override def output: Seq[Attribute] = child.output override def
requiredChildDistribution: Seq[Distribution] = if (partial)
UnspecifiedDistribution :: Nil else ClusteredDistribution(child.output) ::
Nil *override def execute(): RDD[Row] = {** child.execute().mapPartitions {
iter =>** val hashSet = new scala.collection.mutable.HashSet[Row]()* * var
currentRow: Row = null** while (iter.hasNext) {** currentRow = iter.next()**
if (!hashSet.contains(currentRow)) {** hashSet.add(currentRow.copy())** }**
}* * hashSet.iterator** }** }*}




I can try to reproduce more clearly the performance issue, but do you have
any insights into why we can't have the same distinct strategy between
DataFrame and RDD ?

Regards,

Olivier.
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 07 May 2015 09:23:47 +0000",Re: DataFrame distinct vs RDD distinct,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Ok, but for the moment, this seems to be killing performances on some
computations...
I'll try to give you precise figures on this between rdd and dataframe.

Olivier.

Le jeu. 7 mai 2015 √† 10:08, Reynold Xin <rxin@databricks.com> a √©crit :

e
,
t
e
::
ns {
 * var
ve
"
bbarbieru <bbarbieru@outlook.com>,"Thu, 7 May 2015 05:56:14 -0700 (MST)",Re: Automatic testing for Spark App developed in Python,dev@spark.apache.org,"	
It looks like it was a matter of where you call nosetests from, It had to be
run from within the src/spark folder since there were some other layers
above /src. But I've ran into another problem, the spark context I'm
creating is run under the default python interpreter instead of the one set
in spark/conf/spark-env.sh, so I think I should set it programmatically
within the setUpClass, Though I doubt this is as easy as setting an env.
variable, should I read the spark/conf/spark-env.sh inside the setUpClass?



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 7 May 2015 07:18:19 -0700","Re: [build system] quick jenkins restart thursday morning (5-6-15)
 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is happening now.


"
shane knapp <sknapp@berkeley.edu>,"Thu, 7 May 2015 07:48:52 -0700","Re: [build system] quick jenkins restart thursday morning (5-6-15)
 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","things are currently rebooting.


"
shane knapp <sknapp@berkeley.edu>,"Thu, 7 May 2015 08:04:41 -0700","Re: [build system] quick jenkins restart thursday morning (5-6-15)
 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","and we're back up and building.  thanks for your patience!



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 May 2015 16:25:41 +0000",Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,Spark dev list <dev@spark.apache.org>,"Details are here: https://issues.apache.org/jira/browse/SPARK-7442

It looks like something specific to building against Hadoop 2.6?

Nick
"
Reynold Xin <rxin@databricks.com>,"Thu, 7 May 2015 09:28:44 -0700",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,Nicholas Chammas <nicholas.chammas@gmail.com>,"Is this related to s3a update in 2.6?


"
Peter Rudenko <petro.rudenko@gmail.com>,"Thu, 7 May 2015 19:29:04 +0300",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,"Nicholas Chammas <nicholas.chammas@gmail.com>,
 Spark dev list <dev@spark.apache.org>","Hi Nick, had the same issue.
By default it should work with s3a protocol:

sc.textFile('s3a://bucket/file_*').count()


If you want to use s3n protocol you need to add hadoop-aws.jar to 
spark's classpath. Wich hadoop vendor (Hortonworks, Cloudera, MapR) do 
you use?

Thanks,
Peter Rudenko

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 May 2015 16:30:34 +0000",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,"Peter Rudenko <petro.rudenko@gmail.com>, Spark dev list <dev@spark.apache.org>","Hmm, I just tried changing s3n to s3a:

py4j.protocol.Py4JJavaError: An error occurred while calling
z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class
org.apache.hadoop.fs.s3a.S3AFileSystem not found

Nick
‚Äã


"
Peter Rudenko <petro.rudenko@gmail.com>,"Thu, 7 May 2015 19:35:29 +0300",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,"Nicholas Chammas <nicholas.chammas@gmail.com>,
 Spark dev list <dev@spark.apache.org>","Try to download this jar:
http://search.maven.org/remotecontent?filepath=org/apache/hadoop/hadoop-aws/2.6.0/hadoop-aws-2.6.0.jar

And add:

export CLASSPATH=$CLASSPATH:hadoop-aws-2.6.0.jar

And try to relaunch.

Thanks,
Peter Rudenko


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 May 2015 16:41:49 +0000",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,"Peter Rudenko <petro.rudenko@gmail.com>, Spark dev list <dev@spark.apache.org>","I can try that, but the issue is I understand this is supposed to work out
of the box (like it does with all the other Spark/Hadoop pre-built
packages).


-aws/2.6.0/hadoop-aws-2.6.0.jar
.spark.api.python.PythonRDD.collectAndServe.
.apache.hadoop.fs.s3a.S3AFileSystem not found
s
"
Peter Rudenko <petro.rudenko@gmail.com>,"Thu, 7 May 2015 19:48:48 +0300",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,"Nicholas Chammas <nicholas.chammas@gmail.com>,
 Spark dev list <dev@spark.apache.org>","Yep it's a Hadoop issue: https://issues.apache.org/jira/browse/HADOOP-11863

http://mail-archives.apache.org/mod_mbox/hadoop-user/201504.mbox/%3CCA+XUwYxPxLkfhOxn1jNkoUKEQQMcPWFzvXJ=u+kP28KDEjO4GQ@mail.gmail.com%3E
http://stackoverflow.com/a/28033408/3271168


So for now need to manually add that jar to classpath on hadoop-2.6.

Thanks,
Peter Rudenko


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 May 2015 16:57:18 +0000",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,"Peter Rudenko <petro.rudenko@gmail.com>, Spark dev list <dev@spark.apache.org>","Ah, thanks for the pointers.

So as far as Spark is concerned, is this a breaking change? Is it possible
that people who have working code that accesses S3 will upgrade to use
Spark-against-Hadoop-2.6 and find their code is not working all of a sudden?

Nick


wYxPxLkfhOxn1jNkoUKEQQMcPWFzvXJ=u+kP28KDEjO4GQ@mail.gmail.com%3E
t
p-aws/2.6.0/hadoop-aws-2.6.0.jar
e.spark.api.python.PythonRDD.collectAndServe.
g.apache.hadoop.fs.s3a.S3AFileSystem not found
you
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 7 May 2015 13:02:24 -0400",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,Nicholas Chammas <nicholas.chammas@gmail.com>,"We should make sure to update our docs to mention s3a as well, since many people won't look at Hadoop's docs for this.

Matei

possible
sudden?
<mailto:petro.rudenko@gmail.com>>
http://mail-archives.apache.org/mod_mbox/hadoop-user/201504.mbox/%3CCA+XUwYxPxLkfhOxn1jNkoUKEQQMcPWFzvXJ=u+kP28KDEjO4GQ@mail.gmail.com%3E
work out
<petro.rudenko@gmail.com>
http://search.maven.org/remotecontent?filepath=org/apache/hadoop/hadoop-aws/2.6.0/hadoop-aws-2.6.0.jar <http://search.maven.org/remotecontent?filepath=org/apache/hadoop/hadoop-aws/2.6.0/hadoop-aws-2.6.0.jar>
z:org.apache.spark.api.python.PythonRDD.collectAndServe.
Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
<petro.rudenko@gmail.com <mailto:petro.rudenko@gmail.com>>
do you

"
Frederick R Reiss <frreiss@us.ibm.com>,"Thu, 7 May 2015 10:36:00 -0700",Re: unable to extract tgz files downloaded from spark,Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"
Hi Praveen,

In the past I've downloaded some Spark tarballs that weren't actually
gzipped. Try using ""tar xvf"" instead of ""tar xvzf"" to extract the files.

Fred



From:	Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>
To:	dev@spark.apache.org
Date:	05/06/2015 07:18 PM
Subject:	unable to extract tgz files downloaded from spark



Hi
I have been trying to install latest spark verison and downloaded the .tgz
files(ex spark-1.3.1.tgz). But, I could not extract them. It complains of
invalid tar format.
Has any seen this issue ?

Thanks
Praveen

"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Thu, 7 May 2015 13:58:47 -0400",NoClassDefFoundError with Spark 1.3,dev <dev@spark.apache.org>,"Hi all ñ Iím attempting to build a project with SBT and run it on Spark 1.3 (this previously worked before we upgraded to CDH 5.4 with Spark 1.3).

I have the following in my build.sbt:

scalaVersion := ""2.10.4""

libraryDependencies ++= Seq(
    ""org.apache.spark"" %% ""spark-core"" % ""1.3.0"" % ""provided"",
    ""org.apache.spark"" %% ""spark-sql"" % ""1.3.0"" % ""provided"",
    ""org.apache.spark"" %% ""spark-mllib"" % ""1.3.0"" % ""provided"",
    ""log4j"" % ""log4j"" % ""1.2.15"" excludeAll(
      ExclusionRule(organization = ""com.sun.jdmk""),
      ExclusionRule(organization = ""com.sun.jmx""),
      ExclusionRule(organization = ""javax.jms"")
      )
)

When I attempt to run this program with sbt run, however, I get the following error:
java.lang.NoClassDefFoundError: org.apache.spark.Partitioner

I donít explicitly use the Partitioner class anywhere, and this seems to indicate some missing Spark libraries on the install. Do I need to confirm anything other than the presence of the Spark assembly? Iím on CDH 5.4 and Iím able to run the spark-shell without any trouble.

Any help would be much appreciated.

Thank you,
Ilya Ganelin

[cid:581A0F70-A39D-4F8D-9B4D-94E73EB6F78C]
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Michael Armbrust <michael@databricks.com>,"Thu, 7 May 2015 12:16:27 -0700",Re: DataFrame distinct vs RDD distinct,Olivier Girardot <o.girardot@lateral-thoughts.com>,"I'd happily merge a PR that changes the distinct implementation to be more
like Spark core, assuming it includes benchmarks that show better
performance for both the ""fits in memory case"" and the ""too big for memory
case"".


©crit :
in
)
)* *
*
"
anshu shukla <anshushukla0@gmail.com>,"Fri, 8 May 2015 01:10:03 +0530",Predict.scala using model for clustering In reference,"dev@spark.apache.org, user@spark.apache.org, 
	=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>","Can anyone please explain -

println(""Initalizaing the the KMeans model..."")
val model = new
KMeansModel(ssc.sparkContext.objectFile[Vector](modelFile.toString).collect())

where modelfile is  *directory to persist the model while training *


  REF-

https://github.com/databricks/reference-apps/blob/master/twitter_classifier/predict.md


-- 
Thanks & Regards,
Anshu Shukla
"
Praveen Kumar Muthuswamy <muthusamy.pk@gmail.com>,"Thu, 7 May 2015 13:00:14 -0700",Re: unable to extract tgz files downloaded from spark,Frederick R Reiss <frreiss@us.ibm.com>,"Thanks all for the replies. I seem to have downloaded the redirector html
as Sean mentioned. It works now.




"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 May 2015 20:55:33 +0000",pyspark.sql.types.StructType.fromJson() is a lie,Spark dev list <dev@spark.apache.org>,"Observe, my fellow Sparkophiles (Spark 1.3.1):

StructType(List(StructField(name,StringType,true)))
<class 'pyspark.sql.types.StructType'>
'{""fields"":[{""metadata"":{},""name"":""name"",""nullable"":true,""type"":""string""}],""type"":""struct""}'
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Applications/apache-spark/spark-1.3.1-bin-hadoop2.4/python/pyspark/sql/types.py"",
line 346, in fromJson
    return StructType([StructField.fromJson(f) for f in json[""fields""]])
TypeError: string indices must be integers, not str
))
StructType(List(StructField(name,StringType,true)))

So fromJson() doesn‚Äôt actually expect JSON, which is a string. It expects a
dictionary.

Did I misunderstand something, or is this method poorly named?

My impression was that the combination of toJson() and fromJson() would
return the original input as-is, but it looks like we needed the added step
of json.loads() to make things work.

Nick
‚Äã
"
anshu shukla <anshushukla0@gmail.com>,"Fri, 8 May 2015 02:42:45 +0530",Re: Map one RDD into two RDD,"Evo Eftimov <evo.eftimov@isecc.com>, gerard.maas@gmail.com","concluding --

The whole discussion concludes that -

1-  Framework  does not support  increasing parallelism of any task just by
any inbuilt function .
2-  User have to manualy write logic for filter output of upstream node in
DAG  to manage input to Downstream nodes (like shuffle grouping etc in
STORM)
3- If we want to increase the level of parallelism of twitter streaming
 Spout  to *get higher rate of  DStream of tweets  (to increase the rate of
input )  , how it is possible ...  *

  *val tweetStream = **TwitterUtils.createStream(ssc, Utils.getAuth)*




dden /
c
úsequential‚Äù to
?
imposed by the
ieve in
ature of
e



-- 
Thanks & Regards,
Anshu Shukla
"
shane knapp <sknapp@berkeley.edu>,"Thu, 7 May 2015 15:40:14 -0700",[build infra] quick downtime again tomorrow morning for DOCKER,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","yes, docker.  that wonderful little wrapper for linux containers will be
installed and ready for play on all of the jenkins workers tomorrow morning.

the downtime will be super quick:  i just need to kill the jenkins slaves'
ssh connections and relaunch to add the jenkins user to the docker group.

this will begin at around 7am PDT and shouldn't take long at all.

shane
"
Reynold Xin <rxin@databricks.com>,"Thu, 7 May 2015 15:50:59 -0700",Re: pyspark.sql.types.StructType.fromJson() is a lie,Nicholas Chammas <nicholas.chammas@gmail.com>,"What's the use case?

I'm wondering if we should even expose fromJSON. I think it's more a bug
than feature.


m

],""type"":""struct""}'
types.py"",
 expects a
ep
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 May 2015 22:58:47 +0000",Re: pyspark.sql.types.StructType.fromJson() is a lie,Reynold Xin <rxin@databricks.com>,"Renaming fields to get around SPARK-2775
<https://issues.apache.org/jira/browse/SPARK-2775>.

I‚Äôm doing this clunky thing:

   1. Convert a DataFrame‚Äôs schema to JSON, and then a Python dictionary.
   2. Replace the problematic characters in the schema field names.
   3.

   Convert the resulting dictionary back into JSON and then back into a
   DataFrame schema.
    4.

   Apply the new schema with the fixed field names.

Nick
‚Äã


)
}],""type"":""struct""}'
/types.py"",
)
t expects
"
Joseph Bradley <joseph@databricks.com>,"Thu, 7 May 2015 17:37:23 -0700",Re: Predict.scala using model for clustering In reference,anshu shukla <anshushukla0@gmail.com>,"A KMeansModel was trained in the previous step, and it was saved to
""modelFile"" as a Java object file.  This step is loading the model back and
reconstructing the KMeansModel, which can then be used to classify new
tweets into different clusters.
Joseph


"
zhangxiongfei <zhangxiongfei0815@163.com>,"Fri, 8 May 2015 10:58:05 +0800 (CST)","Hive can not get the schema of an external table created by Spark
 SQL API ""createExternalTable""","""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi
I was trying to create an external table named ""adclicktable""  by API ""def createExternalTable(tableName: String, path: String)"",then I can get the schema of this table successfully like below and this table can be queried normally.The data files are all Parquet files.
sqlContext.sql(""describe adclicktable"").collect
res3: Array[org.apache.spark.sql.Row] = Array([sy,int,], [cu,int,], [pj,int,], [pn,int,], [place,int,], [meterial,int,], [time,string,], [ip,string,], [uid,string,], [tags,string,], [url,string,], [ua,string,], [ucode,int,], [date,string,])
But I could not get the schema information from Hive CLI,like below:
hive> describe adclicktable;
OK
col                     array<string>           from deserializer 
Did I miss any configuration?


Zhang Xiongfei
Thanks"
Chunnan Yao <yaochunnan@gmail.com>,"Thu, 7 May 2015 22:59:06 -0700 (MST)","Possible long lineage issue when using DStream to update a normal
 RDD",dev@spark.apache.org,"Hi all, 
Recently in our project, we need to update a RDD using data regularly
received from DStream, I plan to use ""foreachRDD"" API to achieve this: 
var MyRDD = ... 
dstream.foreachRDD { rdd => 
  MyRDD = MyRDD.join(rdd)....... 
  ... 
} 

Is this usage correct? My concern is, as I am repeatedly and endlessly
reassigning MyRDD in order to update it, will it create a too long RDD
lineage to process when I want to query MyRDD later on (similar as
https://issues.apache.org/jira/browse/SPARK-4672) ? 

Maybe I should: 
1. cache or checkpoint latest MyRDD and unpersist old MyRDD every time a
dstream comes in. 
2. use the unpublished IndexedRDD
(https://github.com/amplab/spark-indexedrdd) to conduct efficient RDD
update. 

As I lack experience using Spark Streaming and indexedRDD, I am here to make
sure my thoughts are on the right track. Your wise suggestions will be
greatly appreciated.



-----
Feel the sparking Spark!
--

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 7 May 2015 23:59:37 -0700",Collect inputs on SPARK-7035: compatibility issue with DataFrame.__getattr__,dev <dev@spark.apache.org>,"Hi all,

In PySpark, a DataFrame column can be referenced using df[""abcd""]
(__getitem__) and df.abcd (__getattr__). There is a discussion on
SPARK-7035 on compatibility issues with the __getattr__ approach, and
I want to collect more inputs on this.

Basically, if in the future we introduce a new method to DataFrame, it
may break user code that uses the same attr to reference a column or
silently changes its behavior. For example, if we add name() to
DataFrame in the next release, all existing code using `df.name` to
reference a column called ""name"" will break. If we add `name()` as a
property instead of a method, all existing code using `df.name` may
still work but with a different meaning. `df.select(df.name)` no
longer selects the column called ""name"" but the column that has the
same name as `df.name`.

There are several proposed solutions:

1. Keep both df.abcd and df[""abcd""], and encourage users to use the
latter that is future proof. This is the current solution in master
(https://github.com/apache/spark/pull/5971). But I think users may be
still unaware of the compatibility issue and prefer `df.abcd` to
`df[""abcd""]` because the former could be auto-completed.
2. Drop df.abcd and support df[""abcd""] only. From Wes' comment on the
JIRA page: ""I actually dragged my feet on the _getattr_ issue for
several months back in the day, then finally added it (and tab
completion in IPython with _dir_), and immediately noticed a huge
quality-of-life improvement when using pandas for actual (esp.
interactive) work.""
3. Replace df.abcd by df.abcd_ (with a suffix ""_""). Both df.abcd_ and
df[""abcd""] would be future proof, and df.abcd_ could be
auto-completed. The tradeoff is apparently the extra ""_"" appearing in
the code.

My preference is 3 > 1 > 2. Your inputs would be greatly appreciated. Thanks!

Best,
Xiangrui

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 8 May 2015 00:18:10 -0700",Re: Collect inputs on SPARK-7035: compatibility issue with DataFrame.__getattr__,Xiangrui Meng <mengxr@gmail.com>,"I dont know much about Python style, but I think the point Wes made about
usability on the JIRA is pretty powerful. IMHO the number of methods on a
Spark DataFrame might not be much more compared to Pandas. Given that it
looks like users are okay with the possibility of collisions in Pandas I
think sticking (1) is not a bad idea.

Also is it possible to detect such collisions in Python ? A (4)th option
might be to detect that `df` contains a column named `name` and print a
warning in `df.name` which tells the user that the method is overriding the
column.

Thanks
Shivaram



"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 8 May 2015 00:40:56 -0700",Re: Collect inputs on SPARK-7035: compatibility issue with DataFrame.__getattr__,shivaram@eecs.berkeley.edu,"
This is true for interactive work. Spark's DataFrames can handle
really large datasets, which might be used in production workflows. So
I think it is reasonable for us to care more about compatibility
issues than Pandas.


Maybe we can inspect the frame `df.name` gets called and warn users in
`df.select(df.name)` but not in `name = df.name`. This could be tricky
to implement.

-Xiangrui


---------------------------------------------------------------------


"
"""Sun, Rui"" <rui.sun@intel.com>","Fri, 8 May 2015 08:10:41 +0000",[SparkR] is toDF() necessary,"""dev@spark.apache.org"" <dev@spark.apache.org>","toDF() is defined to convert an RDD to a DataFrame. But it is just a very thin wrapper of createDataFrame() by help the caller avoid input of SQLContext.

Since Scala/pySpark does not have toDF(), and we'd better keep API as narrow and simple as possible. Is toDF() really necessary? Could we eliminate it?


"
"""Haopu Wang"" <HWang@qilinsoft.com>","Fri, 8 May 2015 16:36:58 +0800",[SparkSQL] cannot filter by a DateType column,"<user@spark.apache.org>,
	<dev@spark.apache.org>","I want to filter a DataFrame based on a Date column. 

 

If the DataFrame object is constructed from a scala case class, it's
working (either compare as String or Date). But if the DataFrame is
generated by specifying a Schema to an RDD, it doesn't work. Below is
the exception and test code.

 

Do you have any idea about the error? Thank you very much!

 

================exception=================

java.lang.ClassCastException: java.sql.Date cannot be cast to
java.lang.Integer

    at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)

    at
org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToString$2$$
anonfun$apply$6.apply(Cast.scala:116)

    at
org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$cata
lyst$expressions$Cast$$buildCast(Cast.scala:111)

    at
org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToString$2.a
pply(Cast.scala:116)

    at
org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:426)

    at
org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual.eval(predic
ates.scala:305)

    at
org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$
apply$1.apply(predicates.scala:30)

    at
org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$
apply$1.apply(predicates.scala:30)

    at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)

    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)

 

================code=================

 

    val conf = new
SparkConf().setAppName(""DFTest"").setMaster(""local[*]"")

    val sc = new SparkContext(conf)

    val sqlCtx = new HiveContext(sc)

    import sqlCtx.implicits._

    

    case class Test(dt: java.sql.Date)

 

    val df = sc.makeRDD(Seq(Test(new java.sql.Date(115,4,7)))).toDF

    

    var r = df.filter(""dt >= '2015-05-06'"")

    r.explain(true)

    r.show    

    println(""======"")

    var r2 = df.filter(""dt >= cast('2015-05-06' as DATE)"")

    r2.explain(true)

    r2.show    

    println(""======"")

 

    // ""df2"" doesn't do filter correct!!

    val rdd2 = sc.makeRDD(Seq((Row(new java.sql.Date(115,4,7)))))

    

    val schema = StructType(Array(StructField(""dt"", DateType, false)))

    

    val df2 = sqlCtx.applySchema(rdd2, schema) 

    

    r = df2.filter(""dt >= '2015-05-06'"")

    r.explain(true)

    r.show    

    println(""======"")

    

    r2 = df2.filter(""dt >= cast('2015-05-06' as DATE)"")

    r2.explain(true)

    r2.show    

 

"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 8 May 2015 14:10:24 +0530",Re: NoClassDefFoundError with Spark 1.3,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Looks like the jar you provided has some missing classes. Try this:

scalaVersion := ""2.10.4""

libraryDependencies ++= Seq(
    ""org.apache.spark"" %% ""spark-core"" % ""1.3.0"",
    ""org.apache.spark"" %% ""spark-sql"" % ""1.3.0"" % ""provided"",
    ""org.apache.spark"" %% ""spark-mllib"" % ""1.3.0"" % ""provided"",
    ""log4j"" % ""log4j"" % ""1.2.15"" excludeAll(
      ExclusionRule(organization = ""com.sun.jdmk""),
      ExclusionRule(organization = ""com.sun.jmx""),
      ExclusionRule(organization = ""javax.jms"")
      )
)


Thanks
Best Regards


un it on Spark
.
eems to
m
H 5.4 and
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 8 May 2015 08:54:08 +0000",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,Matei Zaharia <matei.zaharia@gmail.com>,"
 people won't look at Hadoop's docs for this.

1. to use s3a you'll also need an amazon toolkit JAR on the cp
2. I can add a hadoop-2.6 profile that sets things up for s3a, azure and openstack swift.
3. TREAT S3A on HADOOP 2.6 AS BETA-RELEASE

For anyone thinking putting that in all-caps seems excessive, consult

https://issues.apache.org/jira/browse/HADOOP-11571

in particular, anything that queries for the block size of a file before dividing work up is dead in the water due to 
HADOOP-11584 : s3a file block size set to 0 in getFileStatus. There's also thread pooling problems if too many
writes are going on in the same JVM; this may hit output operations

Hadoop 2.7 fixes all the phase I issues, leaving those in HADOOP-11694 to look at


le
den?
mailto:petro.rudenko@gmail.com>>


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 8 May 2015 09:16:58 +0000",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,Matei Zaharia <matei.zaharia@gmail.com>,"
openstack swift.


Added: 
https://issues.apache.org/jira/browse/SPARK-7481 


 tests that individuals/orgs can run against different S3 installations & private versions; people publish their results to see that there's been good coverage of the different S3 installations with their different consistency models & auth mechanisms. 

There's also some scale tests that take time & don't get run so often but which throw up surprises (RAX UK throttling DELETE, intermittent ConnectionReset exceptions reading multi-GB s3 files). 

Amazon have some public datasets that could be used to verify that spark can read files off S3, and maybe even find some of the scale problems.

In particular, http://datasets.elasticmapreduce.s3.amazonaws.com/ publishes ngrams as a set of .gz files free for all to read

Would there be a place in the code tree for some tests to run against things like this? They're cloud integration tests rather than unit tests and nobody would want them to be on by default, but it could be good for regression testing hadoop s3 support & spark integration

---------------------------------------------------------------------


"
Olivier Girardot <ssaboum@gmail.com>,"Fri, 08 May 2015 13:40:37 +0000",Re: NoClassDefFoundError with Spark 1.3,"Akhil Das <akhil@sigmoidanalytics.com>, 
	""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","You're trying to launch using sbt run some ""provided"" dependency,
the goal of the ""provided"" scope is exactly to exclude this dependency from
runtime, considering it as ""provided"" by the environment.

You configuration is correct to create an assembly jar - but not to use sbt
run to test your project.

Regards,

Olivier.

Le ven. 8 mai 2015 √† 10:41, Akhil Das <akhil@sigmoidanalytics.com> a √©crit :

 run it on Spark
 seems to
CDH 5.4
ty
r
"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Fri, 8 May 2015 09:45:02 -0400",Re: NoClassDefFoundError with Spark 1.3,"Olivier Girardot <ssaboum@gmail.com>, Akhil Das
	<akhil@sigmoidanalytics.com>","All ñ the issue was much more subtle. Iíd accidentally included a reference to a static object in a class that I wasnít actually including in my build ñ hence the unrelated run-time error.

Thanks for the clarification on what the ìprovidedî scope means.

Ilya Ganelin

[cid:F5843713-66AA-443B-ABB0-94CDC3D88A09]

From: Olivier Girardot <ssaboum@gmail.com<mailto:ssaboum@gmail.com>>
Date: Friday, May 8, 2015 at 6:40 AM
To: Akhil Das <akhil@sigmoidanalytics.com<mailto:akhil@sigmoidanalytics.comlone.com>>
Cc: dev <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: NoClassDefFoundError with Spark 1.3

You're trying to launch using sbt run some ""provided"" dependency,
the goal of the ""provided"" scope is exactly to exclude this dependency from runtime, considering it as ""provided"" by the environment.

You configuration is correct to create an assembly jar - but not to use sbt run to test your project.

Regards,

Olivier.

Le ven. 8 mai 2015 ‡ 10:41, Akhil Das <akhil@sigmoidanalytics.com<mailto:akhil@sigmoidanalytics.com>> a Ècrit :
Looks like the jar you provided has some missing classes. Try this:

scalaVersion := ""2.10.4""

libraryDependencies ++= Seq(
    ""org.apache.spark"" %% ""spark-core"" % ""1.3.0"",
    ""org.apache.spark"" %% ""spark-sql"" % ""1.3.0"" % ""provided"",
    ""org.apache.spark"" %% ""spark-mllib"" % ""1.3.0"" % ""provided"",
    ""log4j"" % ""log4j"" % ""1.2.15"" excludeAll(
      ExclusionRule(organization = ""com.sun.jdmk""),
      ExclusionRule(organization = ""com.sun.jmx""),
      ExclusionRule(organization = ""javax.jms"")
      )
)


Thanks
Best Regards

<mailto:Ilya.Ganelin@capitalone.com>>

rk
and
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
shane knapp <sknapp@berkeley.edu>,"Fri, 8 May 2015 07:00:07 -0700",Re: [build infra] quick downtime again tomorrow morning for DOCKER,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is happening now.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 08 May 2015 14:04:25 +0000",Re: Collect inputs on SPARK-7035: compatibility issue with DataFrame.__getattr__,"Xiangrui Meng <mengxr@gmail.com>, shivaram@eecs.berkeley.edu","And a link to SPARK-7035
<https://issues.apache.org/jira/browse/SPARK-7035> (which
Xiangrui mentioned in his initial email) for the lazy.


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 08 May 2015 14:06:12 +0000",Re: [build infra] quick downtime again tomorrow morning for DOCKER,"shane knapp <sknapp@berkeley.edu>, amp-infra <amp-infra@googlegroups.com>, 
	dev <dev@spark.apache.org>","Just curious: will docker allow new capabilities for the Spark build?
(Where can I read more?)

Punya


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 08 May 2015 14:11:26 +0000",Re: Collect inputs on SPARK-7035: compatibility issue with DataFrame.__getattr__,"Nicholas Chammas <nicholas.chammas@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, 
	shivaram@eecs.berkeley.edu","Is there a foolproof way to access methods exclusively (instead of picking
between columns and methods at runtime)? Here are two ideas, neither of
which seems particularly Pythonic

   - pyspark.sql.methods(df).name()
   - df.__methods__.name()

Punya


"
shane knapp <sknapp@berkeley.edu>,"Fri, 8 May 2015 07:14:58 -0700",Re: [build infra] quick downtime again tomorrow morning for DOCKER,Punyashloka Biswal <punya.biswal@gmail.com>,"yes, absolutely.  right now i'm just getting the basics set up for a
student's build in the lab.  later on today i will be updating the spark
wiki qa infrastructure page w/more information.


"
=?UTF-8?Q?Fran=C3=A7ois_Garillot?= <francois.garillot@typesafe.com>,"Fri, 8 May 2015 16:41:36 +0200",Back-pressure for Spark Streaming,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi guys,

We[1] are doing a bit of work on Spark Streaming, to help it face
situations where the throughput of data on an InputStream is (momentarily)
susceptible to overwhelm the Receiver(s) memory.

The JIRA & design doc is here:
https://issues.apache.org/jira/browse/SPARK-7398

We'd sure appreciate your comments !

-- 
Fran√ßois Garillot
[1]: Typesafe & some helpful collaborators on benchmarking 'at scale'
"
shane knapp <sknapp@berkeley.edu>,"Fri, 8 May 2015 07:43:18 -0700",Re: [build infra] quick downtime again tomorrow morning for DOCKER,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","...and this is done.  thanks for your patience!


"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Fri, 8 May 2015 21:10:16 +0530",Re: Spark Streaming with Tachyon : Some findings,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just a followup on this Thread .

I tried Hierarchical Storage on Tachyon (
http://tachyon-project.org/Hierarchy-Storage-on-Tachyon.html ) , and that
seems to have worked and I did not see any any Spark Job failed due to
BlockNotFoundException.
below is my  Hierarchical Storage settings..

  -Dtachyon.worker.hierarchystore.level.max=2
  -Dtachyon.worker.hierarchystore.level0.alias=MEM
  -Dtachyon.worker.hierarchystore.level0.dirs.path=$TACHYON_RAM_FOLDER

-Dtachyon.worker.hierarchystore.level0.dirs.quota=$TACHYON_WORKER_MEMORY_SIZE
  -Dtachyon.worker.hierarchystore.level1.alias=HDD
  -Dtachyon.worker.hierarchystore.level1.dirs.path=/mnt/tachyon
  -Dtachyon.worker.hierarchystore.level1.dirs.quota=50GB
  -Dtachyon.worker.allocate.strategy=MAX_FREE
  -Dtachyon.worker.evict.strategy=LRU

Regards,
Dibyendu


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 8 May 2015 16:50:53 +0000",Easy way to convert Row back to case class,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I created a dataset RDD[MyCaseClass], converted it to DataFrame and saved to Parquet file, following https://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds

When I load this dataset with sqlContext.parquetFile, I get DataFrame with column names as in initial case class. I want to convert this DataFrame to RDD to perform RDD operations. However, when I convert it I get RDD[Row] and all information about row names gets lost. Could you suggest an easy way to convert DataFrame to RDD[MyCaseClass]?

Best regards, Alexander
"
Haoyuan Li <haoyuan.li@gmail.com>,"Fri, 8 May 2015 10:33:26 -0700",Re: Spark Streaming with Tachyon : Some findings,Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Thanks for the updates!

Best,

Haoyuan





-- 
Haoyuan Li
CEO, Tachyon Nexus <http://www.tachyonnexus.com/>
AMPLab, EECS, UC Berkeley http://www.cs.berkeley.edu/~haoyuan/
"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 8 May 2015 23:05:53 +0530",Re: Back-pressure for Spark Streaming,=?UTF-8?Q?Fran=C3=A7ois_Garillot?= <francois.garillot@typesafe.com>,"We had a similar issue while working on one of our usecase where we were
processing at a moderate throughput (around 500MB/S). When the processing
time exceeds the batch duration, it started to throw up blocknotfound
exceptions, i made a workaround for that issue and is explained over here
http://apache-spark-developers-list.1001551.n3.nabble.com/SparkStreaming-Workaround-for-BlockNotFound-Exceptions-td12096.html

Basically, instead of generating blocks blindly, i made the receiver sleep
if there's an increase in the scheduling delay (if scheduling delay exceeds
3 times the batch duration). This prototype is working nicely and the speed
is encouraging as its processing at 500MB/S without having any failures so
far.


Thanks
Best Regards


)
"
Will Benton <willb@redhat.com>,"Fri, 8 May 2015 14:01:08 -0400 (EDT)",Re: Easy way to convert Row back to case class,Alexander Ulanov <alexander.ulanov@hp.com>,"This might not be the easiest way, but it's pretty easy:  you can use Row(field_1, ..., field_n) as a pattern in a case match.  So if you have a data frame with foo as an int column and bar as a String columns and you want to construct instances of a case class that wraps these up, you can do something like this:

    // assuming Record is declared as case class Record(foo: Int, bar: String)
    // and df is a data frame

    df.map {
      case Row(foo: Int, bar: String) => Record(foo, bar)
    }



best,
wb


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A3211744E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 May 2015 18:03:52 +0000 (UTC)
Received: (qmail 98668 invoked by uid 500); 8 May 2015 18:03:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98579 invoked by uid 500); 8 May 2015 18:03:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98568 invoked by uid 99); 8 May 2015 18:03:50 -0000
Received: from Unknown (HELO spamd3-us-west.apache.org) (209.188.14.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 May 2015 18:03:50 +0000
Received: from localhost (localhost [127.0.0.1])
	by spamd3-us-west.apache.org (ASF Mail Server at spamd3-us-west.apache.org) with ESMTP id 2E2E11827D4
	for <dev@spark.apache.org>; Fri,  8 May 2015 18:03:50 +0000 (UTC)
X-Virus-Scanned: Debian amavisd-new at spamd3-us-west.apache.org
X-Spam-Flag: NO
X-Spam-Score: 2.28
X-Spam-Level: **
X-Spam-Status: No, score=2.28 tagged_above=-999 required=6.31
	tests=[HTML_MESSAGE=3, RCVD_IN_DNSWL_LOW=-0.7,
	RCVD_IN_MSPIKE_H3=-0.01, RCVD_IN_MSPIKE_WL=-0.01, SPF_PASS=-0.001,
	URIBL_BLOCKED=0.001] autolearn=disabled
Received: from mx1-eu-west.apache.org ([10.40.0.8])
	by localhost (spamd3-us-west.apache.org [10.40.0.10]) (amavisd-new, port 10024)
	with ESMTP id aWo1BgeBa3Kw for <dev@spark.apache.org>;
	Fri,  8 May 2015 18:03:40 +0000 (UTC)
Received: from mail-la0-f47.google.com (mail-la0-f47.google.com [209.85.215.47])
	by mx1-eu-west.apache.org (ASF Mail Server at mx1-eu-west.apache.org) with ESMTPS id BD01D24980
	for <dev@spark.apache.org>; Fri,  8 May 2015 18:03:39 +0000 (UTC)
Received: by labbd9 with SMTP id bd9so58033264lab.2
        for <dev@spark.apache.org>; Fri, 08 May 2015 11:03:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :from:date:message-id:subject:to:cc:content-type;
        bh=sSwdj28kYr/iliw2czz2Dx3F9b+1CduEfpgS9/gXtS0=;
        b=MQIbC/RRDJIBOYYGtjtVYaAAHZogjAIXqWAXzaKXDOkWcNZJ7ku/PGbiTllKWJixv9
         bASQ0ZNmO+dbi6uyWm+FX++aUSwYnskfrLBvA0FVmJjb3vtrpo7GFDa4mUM7e6kGbB2j
         4c+EBMLrvvwdPPH5fSJYZVv3tZ1BYrpMeefrxBCF1ocv2I8gGT8FTOLp/cUZ6nre40SN
         MMAzJMAEmGNzOCS8OFuuf2gNWP+P+leEPTNFHucBfT7FHjUDVI/5rYg+wDm0H7HP1ynb
         f0HcuWyY2qGiFLN7aTg78Whbu2rNH+EHn0kbdMVq4M7sH/+GJWxXU2F+6VEy8qYR0/Fn
         230g==
X-Gm-Message-State: ALoCoQndRWOrm7JgevIftG8dzT2nhjNrUOBQ8tkn7pY1PB0/+DMtZ/rIMs29tiiqz5dzg7nOotF4
X-Received: by 10.152.203.233 with SMTP id kt9mr3784491lac.21.1431108218915;
 Fri, 08 May 2015 11:03:38 -0700 (PDT)
MIME-Version: 1.0
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.25.158.1 with HTTP; Fri, 8 May 2015 11:03:18 -0700 (PDT)
In-Reply-To: <7584C2C1A1745B4CA9E83C53CF180A960ED0114D@SHSMSX101.ccr.corp.intel.com>
References: <7584C2C1A1745B4CA9E83C53CF180A960ED0114D@SHSMSX101.ccr.corp.intel.com>
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Date: Fri, 8 May 2015 11:03:18 -0700
Message-ID: <CAKx7Bf_pBZ-wk8jTU98AdAmBqWsKY7zSDWaHe+=VM3zfXq-vQw@mail.gmail.com>
Subject: Re: [SparkR] is toDF() necessary
To: ""Sun, Rui"" <rui.sun@intel.com>
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11345984061d0b051595d930

--001a11345984061d0b051595d930
Content-Type: text/plain; charset=UTF-8

Agree that toDF is not very useful. In fact it was removed from the
namespace in a recent change
https://github.com/apache/spark/commit/4e930420c19ae7773b138dfc7db8fc03b4660251

Thanks
Shivaram



--001a11345984061d0b051595d930--

"
Reynold Xin <rxin@databricks.com>,"Fri, 8 May 2015 11:09:49 -0700",Re: Easy way to convert Row back to case class,Will Benton <willb@redhat.com>,"In 1.4, you can do

row.getInt(""colName"")

In 1.5, some variant of this will come to allow you to turn a DataFrame
into a typed RDD, where the case class's field names match the column
names. https://github.com/apache/spark/pull/5713




"
shane knapp <sknapp@berkeley.edu>,"Fri, 8 May 2015 11:15:23 -0700",[build system] QA infrastructure wiki updated w/latest package installs/versions,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","so i spent a good part of the morning parsing out all of the packages and
versions of things that we have installed on our jenkins workers:

https://cwiki.apache.org/confluence/display/SPARK/Spark+QA+Infrastructure

if you're looking to set up something to mimic our build system, this
should be a great starting point!

let me know if there's anything else i can do on this page.

thanks!

shane
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 8 May 2015 19:16:23 +0100",Re: [build system] QA infrastructure wiki updated w/latest package installs/versions,amp-infra@googlegroups.com,"Thanks Shane - really useful as I know that several companies are
interested in having in-house replicas of our QA infra.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 8 May 2015 11:40:34 -0700",Re: [SparkSQL] cannot filter by a DateType column,Haopu Wang <HWang@qilinsoft.com>,"What version of Spark are you using?  It appears that at least in master we
are doing the conversion correctly, but its possible older versions of
applySchema do not.  If you can reproduce the same bug in master, can you
open a JIRA?


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 08 May 2015 18:41:49 +0000",DataFrames equivalent to SQL table namespacing and aliases,Spark dev list <dev@spark.apache.org>,"DataFrames, as far as I can tell, don‚Äôt have an equivalent to SQL‚Äôs table
aliases.

This is essential when joining dataframes that have identically named
columns.

 ""other"": ""I know""}']))>>> df2 = sqlContext.jsonRDD(sc.parallelize(['{""a"": 4, ""other"": ""I dunno""}']))>>> df12 = df1.join(df2, df1['a'] == df2['a'])>>> df12
DataFrame[a: bigint, other: string, a: bigint, other: string]>>>
df12.printSchema()
root
 |-- a: long (nullable = true)
 |-- other: string (nullable = true)
 |-- a: long (nullable = true)
 |-- other: string (nullable = true)

Now, trying any one of the following:

df12.select('a')
df12['a']
df12.a

yields this:

org.apache.spark.sql.AnalysisException: Reference 'a' is ambiguous,
could be: a#360L, a#358L.;

Same goes for accessing the other field.

This is good, but what are we supposed to do in this case?

SQL solves this by fully qualifying the column name with the table name,
and also offering table aliasing <http://dba.stackexchange.com/a/5991/2660>
in the case where you are joining a table to itself.

If we translate this directly into DataFrames lingo, perhaps it would look
something like:

df12['df1.a']
df12['df2.other']

But I‚Äôm not sure how this fits into the larger API. This certainly isn‚Äôt
backwards compatible with how joins are done now.

So what‚Äôs the recommended course of action here?

Having to unique-ify all your column names before joining doesn‚Äôt sound
like a nice solution.

Nick
‚Äã
"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Fri, 08 May 2015 18:47:15 +0000",Re: DataFrames equivalent to SQL table namespacing and aliases,"Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>","To add to the above discussion, Pandas, allows suffixing and prefixing to
solve this issue

http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.join.html

Rakesh


‚Äôs table
4,
"":
'a'])>>>
0
k
y isn‚Äôt
 sound
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 08 May 2015 18:53:31 +0000",Re: DataFrames equivalent to SQL table namespacing and aliases,"Rakesh Chalasani <vnit.rakesh@gmail.com>, Spark dev list <dev@spark.apache.org>","Oh, I didn't know about that. Thanks for the pointer, Rakesh.

I wonder why they did that, as opposed to taking the cue from SQL and
prefixing column names with a specifiable dataframe alias. The suffix
approach seems quite ugly.

Nick


html
L‚Äôs table
 4,
a"":
['a'])>>>
0
ok
ly isn‚Äôt
t sound
"
Reynold Xin <rxin@databricks.com>,"Fri, 8 May 2015 12:00:53 -0700",Re: DataFrames equivalent to SQL table namespacing and aliases,Nicholas Chammas <nicholas.chammas@gmail.com>,"You can actually just use df1['a'] in projection to differentiate.

e.g. in Scala (similar things work in Python):


scala> val df1 = Seq((1, ""one"")).toDF(""a"", ""b"")
df1: org.apache.spark.sql.DataFrame = [a: int, b: string]

scala> val df2 = Seq((2, ""two"")).toDF(""a"", ""b"")
df2: org.apache.spark.sql.DataFrame = [a: int, b: string]

scala> df1.join(df2, df1(""a"") === df2(""a"") - 1).select(*df1(""a"")*).show()
+-+
|a|
+-+
|1|
+-+





to
html
SQL‚Äôs
"":
e,
inly isn‚Äôt
ôt sound
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 08 May 2015 19:10:50 +0000",Re: DataFrames equivalent to SQL table namespacing and aliases,Reynold Xin <rxin@databricks.com>,"Ah, neat. So in the example I gave earlier, I‚Äôd do this to get columns from
specific dataframes:

DataFrame[a: bigint, other: string]>>> df12.select(df1['a'],
df2['other']).show()
a other
               4 I dunno

This perhaps should be documented in an example in the docs somewhere. I‚Äôll
open a PR for that I suppose.

Nick
‚Äã


show()
.html
 SQL‚Äôs
a"":
ainly
ôt sound
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 08 May 2015 19:49:42 +0000",Re: DataFrame distinct vs RDD distinct,"Michael Armbrust <michael@databricks.com>, 
	Olivier Girardot <o.girardot@lateral-thoughts.com>","I'll try to reproduce what has been reported to me first :) and I'll let
you know. Thanks !

Le jeu. 7 mai 2015 √† 21:16, Michael Armbrust <michael@databricks.com> a
√©crit :

e
y
©crit :
()* *
**
n
"
Michal Haris <michal.haris@visualdna.com>,"Fri, 8 May 2015 21:26:37 +0100",,"user@spark.apache.org, dev@spark.apache.org",#NAME?
Josh Rosen <rosenville@gmail.com>,"Fri, 8 May 2015 14:12:20 -0700",,Michal Haris <michal.haris@visualdna.com>,"Do you have any more specific profiling data that you can share?  I'm


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 08 May 2015 21:38:04 +0000",branch-1.4 nightly builds?,dev <dev@spark.apache.org>,"Dear Spark devs,

Does anyone maintain nightly builds for branch-1.4? I'd like to start
testing against it, and having a regularly updated build on a
well-publicized repository would be a great help!

Punya
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 08 May 2015 21:39:50 +0000",Re: branch-1.4 nightly builds?,"Punyashloka Biswal <punya.biswal@gmail.com>, dev <dev@spark.apache.org>","https://issues.apache.org/jira/browse/SPARK-1517

That issue should probably be unassigned since I am not actively working on
it. (I can't unassign myself.)

Nick


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 08 May 2015 21:43:06 +0000",Having pyspark.sql.types.StructType implement __iter__(),Spark dev list <dev@spark.apache.org>,"StructType looks an awful lot like a Python dictionary.

However, it doesn‚Äôt implement __iter__()
<https://docs.python.org/3/library/stdtypes.html#iterator-types>, so doing
a quick conversion like this doesn‚Äôt work:

StructType(List(StructField(name,StringType,true)))>>> dict(df.schema)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'StructType' object is not iterable

This would be super helpful for doing any custom schema manipulations
without having to go through the whole .json() -> json.loads() ->
manipulate() -> json.dumps() -> .fromJson() charade.

Same goes for Row, which offers an asDict()
<https://spark.apache.org/docs/1.3.1/api/python/pyspark.sql.html#pyspark.sql.Row.asDict>
method but doesn‚Äôt support the more Pythonic dict(Row).

Does this make sense?

Nick
‚Äã
"
Reynold Xin <rxin@databricks.com>,"Fri, 8 May 2015 14:57:33 -0700",Re: Having pyspark.sql.types.StructType implement __iter__(),Nicholas Chammas <nicholas.chammas@gmail.com>,"Sure.


m

g
ql.Row.asDict
"
rtimp <dolethebobdole11@gmail.com>,"Fri, 8 May 2015 15:29:47 -0700 (MST)",Intellij Spark Source Compilation,dev@spark.apache.org,"Hello,

I'm trying to compile the master branch of the spark source (25889d8) in
intellij. I followed the instructions in the wiki
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools,
namely I downloaded IntelliJ 14.1.2 with jre 1.7.0_55, imported pom.xml,
generated all sources in the maven toolbar, and compiled. I receive 3
errors:

Error:(133, 10) java:
org.apache.spark.network.sasl.SaslEncryption.EncryptedMessage is not
abstract and does not override abstract method touch(java.lang.Object) in
io.netty.util.ReferenceCounted
/home/loki11/code/spark/spark/network/common/src/main/java/org/apache/spark/network/buffer/LazyFileRegion.java
Error:(39, 14) java: org.apache.spark.network.buffer.LazyFileRegion is not
abstract and does not override abstract method touch(java.lang.Object) in
io.netty.util.ReferenceCounted
/home/loki11/code/spark/spark/network/common/src/main/java/org/apache/spark/network/protocol/MessageWithHeader.java
Error:(34, 1) java: org.apache.spark.network.protocol.MessageWithHeader is
not abstract and does not override abstract method touch(java.lang.Object)
in io.netty.util.ReferenceCounted

-DskipTests clean package succeeds as well build/sbt clean assembly as well
was build/sbt compile.

It seems to me like I'm missing some trivial intellij option (I'm normally
an eclipse user, but was having even more trouble with that). Any advice?

Thanks!




--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 8 May 2015 17:56:30 -0700",Re: Recent Spark test failures,Andrew Or <andrew@databricks.com>,"Andrew:
Do you think the -M and -A options described here can be used in test runs ?
http://scalatest.org/user_guide/using_the_runner

Cheers


"
rtimp <dolethebobdole11@gmail.com>,"Fri, 8 May 2015 18:53:21 -0700 (MST)",Re: Build fail...,dev@spark.apache.org,"Hi,

failing to compile due to the most recent commit. I tried reverting to
commit 7fd212b575b6227df5068844416e51f11740e771 (the commit prior to the
head) on that branch and recompiling, and was successful. As Ferris would
say, it is so choice.



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 8 May 2015 19:01:20 -0700",Re: Build fail...,rtimp <dolethebobdole11@gmail.com>,"Looks like you're right:

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.3-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/427/console

[error] /home/jenkins/workspace/Spark-1.3-Maven-with-YARN/HADOOP_PROFILE/hadoop-2.4/label/centos/core/src/main/scala/org/apache/spark/MapOutputTracker.scala:370:
value tryWithSafeFinally is not a member of object
org.apache.spark.util.Utils
[error]     Utils.tryWithSafeFinally {
[error]           ^


FYI



"
Andrew Or <andrew@databricks.com>,"Fri, 8 May 2015 19:13:58 -0700",Re: Build fail...,Ted Yu <yuzhihong@gmail.com>,"Thanks for pointing this out. I reverted that commit.

2015-05-08 19:01 GMT-07:00 Ted Yu <yuzhihong@gmail.com>:

"
Sean Owen <sowen@cloudera.com>,"Sat, 9 May 2015 09:59:00 +0100",Re: Build fail...,Andrew Or <andrew@databricks.com>,"Ah! of course. That one is my fault. Thank you Andrew for fixing that up.


---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Sat, 9 May 2015 12:57:19 +0200",Re: Intellij Spark Source Compilation,rtimp <dolethebobdole11@gmail.com>,"

I just updated the instructions for Eclipse users on the wiki page. I'd be
happy to update them if you let me know what went wrong.

iulian




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Brock Palen <brockp@umich.edu>,"Sat, 9 May 2015 08:03:08 -0400",Spark featured on Research Podcast,dev@spark.apache.org,"Thanks to Matei for taking time to talk to us!

You can find the full interview at: http://www.rce-cast.com/Podcast/spark.html

If you have topics for future shows please contact me off list.

Brock Palen
www.umich.edu/~brockp
Assoc. Director Advanced Research Computing - TS
XSEDE Campus Champion
brockp@umich.edu
(734)936-1985




---------------------------------------------------------------------


"
dsetrakyan <dsetrakyan@apache.org>,"Sat, 9 May 2015 10:40:08 -0700 (MST)",Re: Integration with Apache Ignite,dev@spark.apache.org,"rxin wrote

Actually Apache Ignite already comes with a very easy to use off-heap store
based on JCache (JSR107) API. I think you may be able to reuse it for the
Spark off-heap implementation. 

More information can be found here:
http://apacheignite.readme.io/v1.0/docs/off-heap-memory

I will comment in the ticket as well.


rxin wrote







--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 09 May 2015 17:53:05 +0000",Re: pyspark.sql.types.StructType.fromJson() is a lie,Reynold Xin <rxin@databricks.com>,"I've reported this in SPARK-7506
<https://issues.apache.org/jira/browse/SPARK-7506>.


tionary.
))
""}],""type"":""struct""}'
l/types.py"",
)
))
It
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 09 May 2015 17:53:45 +0000",Re: DataFrames equivalent to SQL table namespacing and aliases,Reynold Xin <rxin@databricks.com>,"I've opened an issue for a few doc fixes that the PySpark DataFrame API
needs: SPARK-7505 <https://issues.apache.org/jira/browse/SPARK-7505>


lumns
).show()
             4 I dunno
.show()
g
n.html
o SQL‚Äôs
d
d
tainly
Äôt
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 09 May 2015 18:10:02 +0000",Re: Having pyspark.sql.types.StructType implement __iter__(),Reynold Xin <rxin@databricks.com>,"I've filed SPARK-7507 <https://issues.apache.org/jira/browse/SPARK-7507> for
this.


g
sql.Row.asDict
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 09 May 2015 20:10:44 +0000",PySpark DataFrame: Preserving nesting when selecting a nested field,Spark dev list <dev@spark.apache.org>,"Take a look:

""version"": ""10.10""}}']))>>> df.printSchema()
root
 |-- settings: struct (nullable = true)
 |    |-- os: string (nullable = true)
 |    |-- version: string (nullable = true)
else.>>> # I want to preserve the schema otherwise.>>> # That means `os` should stayed nested under>>> # `settings`.
root
 |-- os: string (nullable = true)
root
 |-- settings: struct (nullable = true)
 |    |-- os: string (nullable = true)
 |    |-- version: string (nullable = true)
 |-- os: string (nullable = true)
root
 |-- settings.os: string (nullable = true)

In all cases, selecting a nested field loses the original nesting of that
field.

What I want is to select settings.os and get back a DataFrame with the
following schema:

root
 |-- settings: struct (nullable = true)
 |    |-- os: string (nullable = true)

In other words, I want to preserve the fact that os is nested under settings.
I‚Äôm doing this as a work-around for the fact that PySpark does not
currently support dropping columns,

Until direct support for such a feature lands as part of SPARK-7509
<https://issues.apache.org/jira/browse/SPARK-7509>, selecting all columns
but the ones you want to drop seems way better than directly manipulating
the schema (which is the hackier and way more complex alternative for
rolling your own ‚Äúdrop‚Äù logic).

And you want that process to preserve the schema as much as possible, which
I assume is how a native ‚Äúdrop column‚Äù method would work.

Is it possible though? Or do we have to do direct schema manipulation?

Nick
‚Äã
"
rtimp <dolethebobdole11@gmail.com>,"Sat, 9 May 2015 13:27:00 -0700 (MST)",Re: Intellij Spark Source Compilation,dev@spark.apache.org,"Hi Iulian,

Thanks for the reply! 

With respect to eclipse, I'm doing this all with a fresh download of the
scala ide (Build id: 4.0.0-vfinal-20150305-1644-Typesafe) and with a recent
pull (as of this morning) of the master branch.When I proceed through the
instructions for eclipse (creating the project files in sbt, adding scala
compiler 2.10.4 and setting it for all spark projects) I get the following
errors:

Description	Location	Resource	Path	Type
not found: type EventBatch	line 72	SparkAvroCallbackHandler.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem

Description	Location	Resource	Path	Type
not found: type SparkFlumeProtocol	line 46	SparkAvroCallbackHandler.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem

Description	Location	Resource	Path	Type
Project 'old-deps' is missing required library:
'/home/loki11/code/spark/spark/lib_managed/jars/spark-bagel_2.10-1.2.0.jar'
Build path	old-deps		Build Path Problem

Description	Location	Resource	Path	Type
Project 'old-deps' is missing required library:
'/home/loki11/code/spark/spark/lib_managed/jars/spark-bagel_2.10-1.2.0.jar'
Build path	old-deps		Build Path Problem
Project 'old-deps' is missing required library:
'/home/loki11/code/spark/spark/lib_managed/jars/spark-core_2.10-1.2.0.jar'
Build path	old-deps		Build Path Problem


Description	Location	Resource	Path	Type
value ack is not a member of Any	line 52	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
value getEventBatch is not a member of Any	line 91	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
value nack is not a member of Any	line 73	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem


I tried to include just a representative sample of each type of error in the
above (I had 30 in total).




--

---------------------------------------------------------------------


"
Chris Fregly <chris@fregly.com>,"Sat, 9 May 2015 21:14:34 -0700",Re: Spark + Kinesis,Vadim Bichutskiy <vadim.bichutskiy@gmail.com>,"hey vadim-

sorry for the delay.

if you're interested in trying to get Kinesis working one-on-one, shoot me
a direct email and we'll get it going off-list.

we can circle back and summarize our findings here.

lots of people are using Spark Streaming+Kinesis successfully.

would love to help you through this - albeit a month later!  the goal is to
have this working out of the box, so i'd like to implement anything i can
do to make that happen.

lemme know.

btw, Spark 1.4 will have some improvements to the Kinesis Spark Streaming.

TD and I have been working together on this.

thanks!

-chris

m

Stream-Feeding-and-Eating-Amazon-Kinesis-Streams-with-Python
e
f
=>*
g
sage:
me>
given
 the
rds()
 this
ach
fig and
er of
 val
ncy.
the
tml
7t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=http%3A%2F%2Fspark.apache.org%2Fdocs%2Flatest%2Fstreaming-kinesis-integration.html&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala
7t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=https%3A%2F%2Fgithub.com%2Fapache%2Fspark%2Fblob%2Fmaster%2Fextras%2Fkinesis-asl%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fexamples%2Fstreaming%2FKinesisWordCountASL.scala&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
m
1.5, you
han
fter the
use just
signs
%
read:
to other
n :=
-core_2.10""
"" %
""1.3.0"")*
r""
"
Vadim Bichutskiy <vadim.bichutskiy@gmail.com>,"Sun, 10 May 2015 00:24:02 -0400",Re: Spark + Kinesis,Chris Fregly <chris@fregly.com>,"Thanks Chris! I was just looking to get back to Spark + Kinesis
integration. Will be in touch shortly.

Vadim
·êß


e
.
-Stream-Feeding-and-Eating-Amazon-Kinesis-Streams-with-Python
r
s
=>*
r
/
Usage:
ame>
 given
e the
ards()
n this
each
nfig and
ber of
  val
m
vided""
t
html
W7t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=http%3A%2F%2Fspark.apache.org%2Fdocs%2Flatest%2Fstreaming-kinesis-integration.html&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala
W7t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=https%3A%2F%2Fgithub.com%2Fapache%2Fspark%2Fblob%2Fmaster%2Fextras%2Fkinesis-asl%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fexamples%2Fstreaming%2FKinesisWordCountASL.scala&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
11.5, you
than
after the
 use just
 signs
 %
hread:
 to other
on :=
k-core_2.10""
0"" %
 ""1.3.0"")*
ar""
"
"""Haopu Wang"" <HWang@qilinsoft.com>","Mon, 11 May 2015 09:15:25 +0800",RE: [SparkSQL] cannot filter by a DateType column,"""Michael Armbrust"" <michael@databricks.com>","Sorry, I was using Spark 1.3.x.

 

I cannot reproduce it in master.

 

But should I still open a JIRA because can I request it to be back
ported to 1.3.x branch? Thanks again!

 

________________________________

From: Michael Armbrust [mailto:michael@databricks.com] 
Sent: Saturday, May 09, 2015 2:41 AM
To: Haopu Wang
Cc: user; dev@spark.apache.org
Subject: Re: [SparkSQL] cannot filter by a DateType column

 

What version of Spark are you using?  It appears that at least in master
we are doing the conversion correctly, but its possible older versions
of applySchema do not.  If you can reproduce the same bug in master, can
you open a JIRA?

 


I want to filter a DataFrame based on a Date column. 

 

If the DataFrame object is constructed from a scala case class, it's
working (either compare as String or Date). But if the DataFrame is
generated by specifying a Schema to an RDD, it doesn't work. Below is
the exception and test code.

 

Do you have any idea about the error? Thank you very much!

 

================exception=================

java.lang.ClassCastException: java.sql.Date cannot be cast to
java.lang.Integer

    at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)

    at
org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToString$2$$
anonfun$apply$6.apply(Cast.scala:116)

    at
org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$cata
lyst$expressions$Cast$$buildCast(Cast.scala:111)

    at
org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToString$2.a
pply(Cast.scala:116)

    at
org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:426)

    at
org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual.eval(predic
ates.scala:305)

    at
org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$
apply$1.apply(predicates.scala:30)

    at
org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$
apply$1.apply(predicates.scala:30)

    at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)

    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)

 

================code=================

 

    val conf = new
SparkConf().setAppName(""DFTest"").setMaster(""local[*]"")

    val sc = new SparkContext(conf)

    val sqlCtx = new HiveContext(sc)

    import sqlCtx.implicits._

    

    case class Test(dt: java.sql.Date)

 

    val df = sc.makeRDD(Seq(Test(new java.sql.Date(115,4,7)))).toDF

    

    var r = df.filter(""dt >= '2015-05-06'"")

    r.explain(true)

    r.show    

    println(""======"")

    var r2 = df.filter(""dt >= cast('2015-05-06' as DATE)"")

    r2.explain(true)

    r2.show    

    println(""======"")

 

    // ""df2"" doesn't do filter correct!!

    val rdd2 = sc.makeRDD(Seq((Row(new java.sql.Date(115,4,7)))))

    

    val schema = StructType(Array(StructField(""dt"", DateType, false)))

    

    val df2 = sqlCtx.applySchema(rdd2, schema) 

    

    r = df2.filter(""dt >= '2015-05-06'"")

    r.explain(true)

    r.show    

    println(""======"")

    

    r2 = df2.filter(""dt >= cast('2015-05-06' as DATE)"")

    r2.explain(true)

    r2.show    

 

 

"
Ron's Yahoo! <zlgonzalez@yahoo.com.INVALID>,"Sun, 10 May 2015 19:33:50 -0700",Change for submitting to yarn in 1.3.1,Dev <dev@spark.apache.org>,"Hi,
  I used to submit my Spark yarn applications by using org.apache.spark.yarn.deploy.Client api so I can get the application id after I submit it. The following is the code that I have, but after upgrading to 1.3.1, the yarn Client class was made into a private class. Is there a particular reason why this Client class was made private?
  I know that there‚Äôs a new SparkSubmit object that can be used, but it‚Äôs not clear to me how I can use it to get the application id after submitting to the cluster.
  Thoughts?

Thanks,
Ron

class SparkLauncherServiceImpl extends SparkLauncherService {
  
  override def runApp(conf: Configuration, appName: String, queue: String): ApplicationId = {
    val ws = SparkLauncherServiceImpl.getWorkspace()
    val params = Array(""--class"", //
        ""com.xyz.sparkdb.service.impl.AssemblyServiceImpl"", //
        ""--name"", appName, //
        ""--queue"", queue, //
        ""--driver-memory"", ""1024m"", //
        ""--addJars"", getListOfDependencyJars(s""$ws/ledp/le-sparkdb/target/dependency""), //
        ""--jar"", s""file:$ws/ledp/le-sparkdb/target/le-sparkdb-1.0.3-SNAPSHOT.jar"")
    System.setProperty(""SPARK_YARN_MODE"", ""true"")
    System.setProperty(""spark.driver.extraJavaOptions"", ""-XX:PermSize=128m -XX:MaxPermSize=128m -Dsun.io.serialization.extendedDebugInfo=true"")
    val sparkConf = new SparkConf()
    val args = new ClientArguments(params, sparkConf)
    new Client(args, conf, sparkConf).runApp()
  }
  
  private def getListOfDependencyJars(baseDir: String): String = {
    val files = new File(baseDir).listFiles().filter(!_.getName().startsWith(""spark-assembly""))
    val prependedFiles = files.map(x => ""file:"" + x.getAbsolutePath())
    val result = ((prependedFiles.tail.foldLeft(new StringBuilder(prependedFiles.head))) {(acc, e) => acc.append("", "").append(e)}).toString()
    result
  }
}

"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 10 May 2015 19:43:27 -0700",Re: Change for submitting to yarn in 1.3.1,"""Ron's Yahoo!"" <zlgonzalez@yahoo.com.invalid>","We had a similar requirement, and as a stopgap, I currently use a
suboptimal impl specific workaround - parsing it out of the
stdout/stderr (based on log config).
A better means to get to this is indeed required !

Regards,
Mridul

arn.deploy.Client api so I can get the application id after I submit it. The following is the code that I have, but after upgrading to 1.3.1, the yarn Client class was made into a private class. Is there a particular reason why this Client class was made private?
but it‚Äôs not clear to me how I can use it to get the application id after submitting to the cluster.
): ApplicationId = {
/dependency""), //
HOT.jar"")
28m -XX:MaxPermSize=128m -Dsun.io.serialization.extendedDebugInfo=true"")
sWith(""spark-assembly""))
)
ndedFiles.head))) {(acc, e) => acc.append("", "").append(e)}).toString()

---------------------------------------------------------------------


"
Manku Timma <manku.timma1@gmail.com>,"Mon, 11 May 2015 09:35:49 +0530",Re: Change for submitting to yarn in 1.3.1,Mridul Muralidharan <mridul@gmail.com>,"sc.applicationId gives the yarn appid.


Is
, but
fter
))
())
"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 11 May 2015 00:36:13 -0700",Re: Change for submitting to yarn in 1.3.1,Manku Timma <manku.timma1@gmail.com>,"That works when it is launched from same process - which is
unfortunately not our case :-)

- Mridul

:
d
s. Is
d, but
d after
ly""))
h())

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 11 May 2015 01:29:08 -0700",Re: PySpark DataFrame: Preserving nesting when selecting a nested field,Nicholas Chammas <nicholas.chammas@gmail.com>,"In 1.4, you can use ""struct"" function to create a struct, e.g. you can
explicitly select out the ""version"" column, and then create a new struct
named ""settings"".


The current semantics of select basically follows closely relational
database's SQL, which is well understood and defined. I wouldn't add any
magic to select to for nested data, because it is not very well researched
& defined and we might get into conflicting scenarios.



m

,
g
t
ch
"
Michal Haris <michal.haris@visualdna.com>,"Mon, 11 May 2015 12:21:51 +0100",,Josh Rosen <rosenville@gmail.com>,"This is the stack trace of the worker thread:

org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:60)
org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.scheduler.Task.run(Task.scala:64)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:745)




-- 
Michal Haris
Technical Architect
direct line: +44 (0) 207 749 0229
www.visualdna.com | t: +44 (0) 207 734 7033,
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 11 May 2015 14:35:45 +0200",Re: Intellij Spark Source Compilation,rtimp <dolethebobdole11@gmail.com>,"Hi,

`old-deps` is not really a project, so you can simply skip it (or close
it). The rest should work fine (clean and build all).




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Mon, 11 May 2015 13:54:06 +0000",Re: YARN mode startup takes too long (10+ secs),"Taeyun Kim <taeyun.kim@innowireless.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Isn't this issue something that should be improved? Based on the following
discussion, there are two places were YARN's heartbeat interval is
respected on job start-up, but do we really need to respect it on start-up?


ut essentially
e
between
d()
thout
th
e
mode.
òMainProcessor:
the
s
pp
3a9d\blockmgr-e9ade223-a4b8-4d9f-b038-efd66adf9772
6278\httpd-def9220f-ac3a-4dd2-9ac1-2c593b94b2d9
1/
d
r
e
pp
r
TS
r
"
rtimp <dolethebobdole11@gmail.com>,"Mon, 11 May 2015 07:20:23 -0700 (MST)",Re: Intellij Spark Source Compilation,dev@spark.apache.org,"Hi,

Thanks Iulian. Yeah, I was kind of anticipating I could just ignore old-deps
ultimately. However, Even after doing a clean and build all, I get the
following still:

Description	Location	Resource	Path	Type
not found: type EventBatch	line 72	SparkAvroCallbackHandler.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type EventBatch	line 87	SparkAvroCallbackHandler.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type EventBatch	line 25	SparkSinkUtils.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type EventBatch	line 48	TransactionProcessor.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type EventBatch	line 48	TransactionProcessor.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type EventBatch	line 80	TransactionProcessor.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type EventBatch	line 146	TransactionProcessor.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type SparkFlumeProtocol	line 46	SparkAvroCallbackHandler.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type SparkFlumeProtocol	line 86	SparkSink.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: type SparkSinkEvent	line 115	TransactionProcessor.scala
/spark-streaming-flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: value SparkFlumeProtocol	line 185	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
not found: value SparkFlumeProtocol	line 194	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
Project not built due to errors in dependent project(s)
spark-streaming-flume-sink	Unknown	spark-streaming-flume		Scala Problem
value ack is not a member of Any	line 52	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
value getEventBatch is not a member of Any	line 51	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
value getEventBatch is not a member of Any	line 71	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
value getEventBatch is not a member of Any	line 91	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem
value nack is not a member of Any	line 73	SparkSinkSuite.scala
/spark-streaming-flume-sink/src/test/scala/org/apache/spark/streaming/flume/sink
Scala Problem


I'd mention that the EventBatch  and SparkFlumeProtocol were very similar to
what initially occurred for me in intellij, until I clicked the ""Generate
Sources and Update Folders For All Projects"" button in the ""Maven Projects""
tool window that the wiki suggests suggests doing.



--

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 11 May 2015 17:31:12 +0200",Re: Intellij Spark Source Compilation,rtimp <dolethebobdole11@gmail.com>,"Oh, I see. So then try to run one build on the command time firs (or try sbt
avro:generate, though I‚Äôm not sure it‚Äôs enough). I just noticed that I have
an additional source folder target/scala-2.10/src_managed/main/compiled_avro
for spark-streaming-flume-sink. I guess I built the project once and that‚Äôs
why I never saw these errors..


thanks,
iulian
‚Äã


me/sink
me/sink
me/sink
me/sink
me/sink
me/sink
a
me/sink
me/sink
me/sink
a
me/sink
me/sink
me/sink
me/sink
me/sink
me/sink
me/sink
me/sink
s""
Source-Compilation-tp12168p12195.html


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Eron Wright <ewright@live.com>,"Mon, 11 May 2015 09:02:12 -0700",[SPARK-7400] PortableDataStream UDT,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,
I'm working on SPARK-7400 for DataFrame support for PortableDataStream,  i.e. the data type associated with the RDD from sc.binaryFiles(...).     
Assuming a patch is available soon, what is the likelihood of inclusion in Spark 1.4?
Thanks 		 	   		  "
Mridul Muralidharan <mridul@gmail.com>,"Mon, 11 May 2015 09:03:33 -0700",Re: YARN mode startup takes too long (10+ secs),=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"For tiny/small clusters (particularly single tenet), you can set it to
lower value.
But for anything reasonably large or multi-tenet, the request storm
can be bad if large enough number of applications start aggressively
polling RM.
That is why the interval is set to configurable.

- Mridul


g
p?
but essentially
he
 between
ad()
ithout
t
n
e
ith
de
 mode.
ÄòMainProcessor:
 the
is
app
s
f3a9d\blockmgr-e9ade223-a4b8-4d9f-b038-efd66adf9772
b6278\httpd-def9220f-ac3a-4dd2-9ac1-2c593b94b2d9
01/
ed
ur
he
app
er
STS
1
er
r
:

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 11 May 2015 10:22:29 -0700",Re: YARN mode startup takes too long (10+ secs),Mridul Muralidharan <mridul@gmail.com>,"Wow, I hadn't noticed this, but 5 seconds is really long.  It's true that
it's configurable, but I think we need to provide a decent out-of-the-box
experience.  For comparison, the MapReduce equivalent is 1 second.

I filed https://issues.apache.org/jira/browse/SPARK-7533 for this.

-Sandy


m>
t
, but essentially
:
ce between
er
 without
 1
er
d
er mode.
e
ÄòMainProcessor:
by the
c
n
r'
3a9d\blockmgr-e9ade223-a4b8-4d9f-b038-efd66adf9772
6278\httpd-def9220f-ac3a-4dd2-9ac1-2c593b94b2d9
n
0
er
r
ur
n
,
:
"
rtimp <dolethebobdole11@gmail.com>,"Mon, 11 May 2015 10:52:52 -0700 (MST)",Re: Intellij Spark Source Compilation,dev@spark.apache.org,"Hi Iulian,

I was able to successfully compile in eclipse after, on the command line,
using sbt avro:generate followed by a sbt clean compile (and then a full
clean compile in eclipse). Thanks for your help!





--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 11 May 2015 11:40:49 -0700",Re: [SPARK-7400] PortableDataStream UDT,Eron Wright <ewright@live.com>,"Sorry it's hard to give a definitive answer due to the lack of details (I'm
not sure what exactly is entailed to have this PortableDataStream), but the
answer is probably no if we need to change some existing code and expose a
whole new data type to users.



"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Mon, 11 May 2015 18:58:37 +0000",RE: DataFrame distinct vs RDD distinct,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Michael Armbrust
	<michael@databricks.com>","Hi,

Could you suggest alternative way of implementing distinct, e.g. via fold or aggregate? Both SQL distinct and RDD distinct fail on my dataset due to overflow of Spark shuffle disk. I have 7 nodes with 300GB dedicated to Spark shuffle each. My dataset is 2B rows, the field which I'm performing distinct has 23 distinct values.

Best regards, Alexander 

-----Original Message-----
From: Olivier Girardot [mailto:o.girardot@lateral-thoughts.com] 
Sent: Friday, May 08, 2015 12:50 PM
To: Michael Armbrust; Olivier Girardot
Cc: Reynold Xin; dev@spark.apache.org
Subject: Re: DataFrame distinct vs RDD distinct

I'll try to reproduce what has been reported to me first :) and I'll let you know. Thanks !

Le jeu. 7 mai 2015 √† 21:16, Michael Armbrust <michael@databricks.com> a √©crit :

> I'd happily merge a PR that changes the distinct implementation to be 
> more like Spark core, assuming it includes benchmarks that show better 
> performance for both the ""fits in memory case"" and the ""too big for 
> memory case"".
>
> On Thu, May 7, 2015 at 2:23 AM, Olivier Girardot < 
> o.girardot@lateral-thoughts.com> wrote:
>
>> Ok, but for the moment, this seems to be killing performances on some 
>> computations...
>> I'll try to give you precise figures on this between rdd and dataframe.
>>
>> Olivier.
>>
>> Le jeu. 7 mai 2015 √† 10:08, Reynold Xin <rxin@databricks.com> a √©crit :
>>
>> > In 1.5, we will most likely just rewrite distinct in SQL to either 
>> > use
>> the
>> > Aggregate operator which will benefit from all the Tungsten
>> optimizations,
>> > or have a Tungsten version of distinct for SQL/DataFrame.
>> >
>> > On Thu, May 7, 2015 at 1:32 AM, Olivier Girardot < 
>> > o.girardot@lateral-thoughts.com> wrote:
>> >
>> >> Hi everyone,
>> >> there seems to be different implementations of the ""distinct"" 
>> >> feature
>> in
>> >> DataFrames and RDD and some performance issue with the DataFrame
>> distinct
>> >> API.
>> >>
>> >> In RDD.scala :
>> >>
>> >> def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null):
>> >> RDD[T] =
>> >> withScope { map(x => (x, null)).reduceByKey((x, y) => x,
>> >> numPartitions).map(_._1) }
>> >> And in DataFrame :
>> >>
>> >>
>> >> case class Distinct(partial: Boolean, child: SparkPlan) extends
>> UnaryNode
>> >> {
>> >> override def output: Seq[Attribute] = child.output override def
>> >> requiredChildDistribution: Seq[Distribution] = if (partial) 
>> >> UnspecifiedDistribution :: Nil else
>> ClusteredDistribution(child.output) ::
>> >>
>> > Nil *override def execute(): RDD[Row] = {**
>> child.execute().mapPartitions {
>> >> iter =>** val hashSet = new 
>> >> scala.collection.mutable.HashSet[Row]()* *
>> var
>> >> currentRow: Row = null** while (iter.hasNext) {** currentRow =
>> >> iter.next()**
>> >> if (!hashSet.contains(currentRow)) {** 
>> >> hashSet.add(currentRow.copy())**
>> >> }**
>> >> }* * hashSet.iterator** }** }*}
>> >
>> >
>> >>
>> >>
>> >>
>> >>
>> >> I can try to reproduce more clearly the performance issue, but do 
>> >> you
>> have
>> >> any insights into why we can't have the same distinct strategy 
>> >> between DataFrame and RDD ?
>> >>
>> >> Regards,
>> >>
>> >> Olivier.
>> >>
>> >
>>
>
>
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Mon, 11 May 2015 18:59:03 +0000",RE: Easy way to convert Row back to case class,"Reynold Xin <rxin@databricks.com>, Will Benton <willb@redhat.com>","Thank you for suggestions!

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Friday, May 08, 2015 11:10 AM
To: Will Benton
Cc: Ulanov, Alexander; dev@spark.apache.org
Subject: Re: Easy way to convert Row back to case class

In 1.4, you can do

row.getInt(""colName"")

In 1.5, some variant of this will come to allow you to turn a DataFrame into a typed RDD, where the case class's field names match the column names. https://github.com/apache/spark/pull/5713



On Fri, May 8, 2015 at 11:01 AM, Will Benton <willb@redhat.com<mailto:willb@redhat.com>> wrote:
This might not be the easiest way, but it's pretty easy:  you can use Row(field_1, ..., field_n) as a pattern in a case match.  So if you have a data frame with foo as an int column and bar as a String columns and you want to construct instances of a case class that wraps these up, you can do something like this:

    // assuming Record is declared as case class Record(foo: Int, bar: String)
    // and df is a data frame

    df.map {
      case Row(foo: Int, bar: String) => Record(foo, bar)
    }



best,
wb


----- Original Message -----
> From: ""Alexander Ulanov"" <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>
> To: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Sent: Friday, May 8, 2015 11:50:53 AM
> Subject: Easy way to convert Row back to case class
>
> Hi,
>
> I created a dataset RDD[MyCaseClass], converted it to DataFrame and saved to
> Parquet file, following
> https://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds
>
> When I load this dataset with sqlContext.parquetFile, I get DataFrame with
> column names as in initial case class. I want to convert this DataFrame to
> RDD to perform RDD operations. However, when I convert it I get RDD[Row] and
> all information about row names gets lost. Could you suggest an easy way to
> convert DataFrame to RDD[MyCaseClass]?
>
> Best regards, Alexander
>
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>

"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Mon, 11 May 2015 19:31:42 +0000",RE: DataFrame distinct vs RDD distinct,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Michael Armbrust
	<michael@databricks.com>","The following worked for me as a workaround for distinct:

val pf = sqlContext.parquetFile(""hdfs://file"")
val distinctValuesOfColumn4 = pf.rdd.aggregate[scala.collection.mutable.HashSet[String]](new scala.collection.mutable.HashSet[String]())( (s, v) => s += v.getString(4), (s1, s2) => s1 ++= s2)

Best regards, Alexander

-----Original Message-----
From: Ulanov, Alexander 
Sent: Monday, May 11, 2015 11:59 AM
To: Olivier Girardot; Michael Armbrust
Cc: Reynold Xin; dev@spark.apache.org
Subject: RE: DataFrame distinct vs RDD distinct

Hi,

Could you suggest alternative way of implementing distinct, e.g. via fold or aggregate? Both SQL distinct and RDD distinct fail on my dataset due to overflow of Spark shuffle disk. I have 7 nodes with 300GB dedicated to Spark shuffle each. My dataset is 2B rows, the field which I'm performing distinct has 23 distinct values.

Best regards, Alexander 

-----Original Message-----
From: Olivier Girardot [mailto:o.girardot@lateral-thoughts.com]
Sent: Friday, May 08, 2015 12:50 PM
To: Michael Armbrust; Olivier Girardot
Cc: Reynold Xin; dev@spark.apache.org
Subject: Re: DataFrame distinct vs RDD distinct

I'll try to reproduce what has been reported to me first :) and I'll let you know. Thanks !

Le jeu. 7 mai 2015 √† 21:16, Michael Armbrust <michael@databricks.com> a √©crit :

> I'd happily merge a PR that changes the distinct implementation to be 
> more like Spark core, assuming it includes benchmarks that show better 
> performance for both the ""fits in memory case"" and the ""too big for 
> memory case"".
>
> On Thu, May 7, 2015 at 2:23 AM, Olivier Girardot < 
> o.girardot@lateral-thoughts.com> wrote:
>
>> Ok, but for the moment, this seems to be killing performances on some 
>> computations...
>> I'll try to give you precise figures on this between rdd and dataframe.
>>
>> Olivier.
>>
>> Le jeu. 7 mai 2015 √† 10:08, Reynold Xin <rxin@databricks.com> a √©crit :
>>
>> > In 1.5, we will most likely just rewrite distinct in SQL to either 
>> > use
>> the
>> > Aggregate operator which will benefit from all the Tungsten
>> optimizations,
>> > or have a Tungsten version of distinct for SQL/DataFrame.
>> >
>> > On Thu, May 7, 2015 at 1:32 AM, Olivier Girardot < 
>> > o.girardot@lateral-thoughts.com> wrote:
>> >
>> >> Hi everyone,
>> >> there seems to be different implementations of the ""distinct"" 
>> >> feature
>> in
>> >> DataFrames and RDD and some performance issue with the DataFrame
>> distinct
>> >> API.
>> >>
>> >> In RDD.scala :
>> >>
>> >> def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null):
>> >> RDD[T] =
>> >> withScope { map(x => (x, null)).reduceByKey((x, y) => x,
>> >> numPartitions).map(_._1) }
>> >> And in DataFrame :
>> >>
>> >>
>> >> case class Distinct(partial: Boolean, child: SparkPlan) extends
>> UnaryNode
>> >> {
>> >> override def output: Seq[Attribute] = child.output override def
>> >> requiredChildDistribution: Seq[Distribution] = if (partial) 
>> >> UnspecifiedDistribution :: Nil else
>> ClusteredDistribution(child.output) ::
>> >>
>> > Nil *override def execute(): RDD[Row] = {**
>> child.execute().mapPartitions {
>> >> iter =>** val hashSet = new
>> >> scala.collection.mutable.HashSet[Row]()* *
>> var
>> >> currentRow: Row = null** while (iter.hasNext) {** currentRow =
>> >> iter.next()**
>> >> if (!hashSet.contains(currentRow)) {**
>> >> hashSet.add(currentRow.copy())**
>> >> }**
>> >> }* * hashSet.iterator** }** }*}
>> >
>> >
>> >>
>> >>
>> >>
>> >>
>> >> I can try to reproduce more clearly the performance issue, but do 
>> >> you
>> have
>> >> any insights into why we can't have the same distinct strategy 
>> >> between DataFrame and RDD ?
>> >>
>> >> Regards,
>> >>
>> >> Olivier.
>> >>
>> >
>>
>
>
B KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKCB  [  X  ‹öX KK[XZ[
 ] ][  X  ‹öX P \ Àò\X K ‹ô B  ‹àY][€ò[  [X[  K[XZ[
 ] Z[ \ Àò\X K ‹ô B 

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Reynold Xin <rxin@databricks.com>,"Mon, 11 May 2015 12:55:06 -0700",,Michal Haris <michal.haris@visualdna.com>,"Looks like it is spending a lot of time doing hash probing. It could be a
number of the following:

1. hash probing itself is inherently expensive compared with rest of your
workload

2. murmur3 doesn't work well with this key distribution

3. quadratic probing (triangular sequence) with a power-of-2 hash table
works really badly for this workload.

number of probes in total, and then log it. We added this probing
capability to the new Bytes2Bytes hash map we built. We should consider
just having it being reported as some built-in metrics to facilitate
debugging.

https://github.com/apache/spark/blob/b83091ae4589feea78b056827bc3b7659d271e41/unsafe/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java#L214







"
Andrew Or <andrew@databricks.com>,"Mon, 11 May 2015 13:08:46 -0700",Re: Recent Spark test failures,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

Yes, those two options can be useful, but in general I think the standard
to set is that tests should never fail. It's actually the worst if tests
fail sometimes but not others, because we can't reproduce them
deterministically. Using -M and -A actually tolerates flaky tests to a
certain extent, and I would prefer to instead increase the determinism in
these tests.

-Andrew

2015-05-08 17:56 GMT-07:00 Ted Yu <yuzhihong@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 11 May 2015 13:11:34 -0700",Re: Recent Spark test failures,Andrew Or <andrew@databricks.com>,"Makes sense.

Having high determinism in these tests would make Jenkins build stable.


"
Steve Loughran <stevel@hortonworks.com>,"Mon, 11 May 2015 20:36:40 +0000",Re: Recent Spark test failures,"""dev@spark.apache.org"" <dev@spark.apache.org>","
ts
mTSzL7LSb4


that's really nice.

I like the publishing to google spreadsheets & the eating-your-own-dogfood analysis.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 11 May 2015 20:49:16 +0000",[PySpark DataFrame] When a Row is not a Row,Spark dev list <dev@spark.apache.org>,"This is really strange.

<class 'pyspark.sql.dataframe.DataFrame'>


<class 'pyspark.sql.types.Row'>

<class 'pyspark.sql.types.Row'>

False
False

If I set a as follows, then the type checks pass fine.

a = pyspark.sql.types.Row('name')('Nick')

Is this a bug? What can I do to narrow down the source?

results is a massive DataFrame of spark-perf results.

Nick
‚Äã
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 11 May 2015 14:29:00 -0700",Re: [PySpark DataFrame] When a Row is not a Row,Nicholas Chammas <nicholas.chammas@gmail.com>,"In Row#equals():

      while (i < len) {
        if (apply(i) != that.apply(i)) {

'!=' should be !apply(i).equals(that.apply(i)) ?

Cheers


"
Archit Thakur <archit279thakur@gmail.com>,"Tue, 12 May 2015 11:32:06 +0530",Updation of spark metrics.,dev@spark.incubator.apache.org,"Hi,

I can see executor sends the TaskMetrics in the heartbeat every 10
sec(configurable). but in JobProgListener there is a check for task
completion. If the task is not completed, it doesn't update the metrics on
the UI.
Any specific reason for this.? I don't think there would be much of the
performance impact by removing that check? Please correct me.

Thanks & Regards,
Archit Thakur.
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 12 May 2015 00:50:14 -0700",Adding/Using More Resolution Types on JIRA,"""dev@spark.apache.org"" <dev@spark.apache.org>","In Spark we sometimes close issues as something other than ""Fixed"",
and this is an important part of maintaining our JIRA.

The current resolution types we use are the following:

Won't Fix - bug fix or (more often) feature we don't want to add
Invalid - issue is underspecified or not appropriate for a JIRA issue
Duplicate - duplicate of another JIRA
Cannot Reproduce - bug that could not be reproduced
Not A Problem - issue purports to represent a bug, but does not

I would like to propose adding a few new resolutions. This will
require modifying the ASF JIRA, but infra said they are open to
proposals as long as they are considered of broad interest.

My issue with the current set of resolutions are that ""Won't Fix"" is a
big catch all we use for many different things. Most often it's used
for things that aren't even bugs even though it has ""Fix"" in the name.
I'm proposing adding:

Inactive - A feature or bug that has had no activity from users or
developers in a long time
Out of Scope - A feature proposal that is not in scope given the projects goals
Later - A feature not on the immediate roadmap, but potentially of
interest longer term (this one already exists, I'm just proposing to
start using it)

I am in no way proposing changes to the decision making model around
JIRA's, notably that it is consensus based and that all resolutions
are considered tentative and fully reversible.

The benefits I see of this change would be the following:
1. Inactive: A way to clear out inactive/dead JIRA's without
indicating a decision has been made one way or the other.
2. Out of Scope: It more clearly explains closing out-of-scope
features than the generic ""Won't Fix"". Also makes it more clear to
future contributors what is considered in scope for Spark.
3. Later: A way to signal that issues aren't targeted for a near term
version. This would help avoid the mess we have now of like 200+
issues targeted at each version and target version being a very bad
indicator of actual roadmap. An alternative on this one is to have a
version called ""Later"" or ""Parking Lot"" but not close the issues.

Any thoughts on this?

- Patrick

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 12 May 2015 09:48:15 +0100",Re: Adding/Using More Resolution Types on JIRA,Patrick Wendell <pwendell@gmail.com>,"I tend to find that any large project has a lot of walking dead JIRAs, and
pretending they are simply Open causes problems. Any state is better for
these, so I favor this.

The possible objection is that this will squash or hide useful issues, but
in practice we have the opposite problem. Resolved issues are still
searchable by default, and, people aren't shy about opening duplicates
anyway. At least the semantics Later do not discourage a diligent searcher
from considering commenting on and reopening such an archived JIRA.

Patrick this could piggy back on INFRA-9513.

As a corollary I would welcome deciding that Target Version should be used
more narrowly to mean 'I really mean to help resolve this for the indicated
version'. Setting it to a future version just to mean Later should instead
turn into resolving the JIRA.

Last: if JIRAs are regularly ice-boxed this way, I think it should trigger
some reflection. Why are these JIRAs going nowhere? For completely normal
reasons or does it mean too many TODOs are filed and forgotten? That's no
comment on the current state, just something to watch.

So: yes I like the idea.

"
Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"Tue, 12 May 2015 15:46:58 +0530","Getting ""Access is denied"" error while cloning Spark source using Eclipse",dev@spark.apache.org,"Hi,

I am  trying to clone Spark source using Eclipse. After providing spark
source URL, eclipse downloads some code which I can see in download
location but as soon as downloading reaches 99% Eclipse throws ""Gi
repository clone failed. Access is denied"" error.

Has anyone encountered such a problem? I want to contribute to Apache spark
source code and I am newbie, first time trying to contribute to open source
project. Can anyone please help me in solving this error?

Regards,
Chandrash3khar Kotekar
Mobile - +91 8600011455
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 12 May 2015 17:00:51 +0530","Re: Getting ""Access is denied"" error while cloning Spark source using Eclipse",Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"May be you should check where exactly its throwing up permission denied
(possibly trying to write to some directory). Also you can try manually
cloning the git repo to a directory and then try opening that in eclipse.

Thanks
Best Regards


"
Night Wolf <nightwolfzor@gmail.com>,"Wed, 13 May 2015 00:05:01 +1000",,"""fightfate@163.com"" <fightfate@163.com>","the number of partitions if you're doing lots of object creation.


"
Night Wolf <nightwolfzor@gmail.com>,"Wed, 13 May 2015 00:08:50 +1000",,Reynold Xin <rxin@databricks.com>,"I'm seeing a similar thing with a slightly different stack trace. Ideas?

org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:205)
org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
org.apache.spark.scheduler.Task.run(Task.scala:64)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:745)



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 12 May 2015 10:52:21 -0400",,Night Wolf <nightwolfzor@gmail.com>,"It could also be that your hash function is expensive. What is the key class you have for the reduceByKey / groupByKey?

Matei

Ideas?
cala:150)
org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:205)
org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
be a
your
table
the
consider
https://github.com/apache/spark/blob/b83091ae4589feea78b056827bc3b7659d271e41/unsafe/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java#L214 <https://github.com/apache/spark/blob/b83091ae4589feea78b056827bc3b7659d271e41/unsafe/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java#L214>
<michal.haris@visualdna.com <mailto:michal.haris@visualdna.com>>
cala:150)
org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
I'm
from.
<michal.haris@visualdna.com <mailto:michal.haris@visualdna.com>>
knows
application
have
where
and is
feels
seen
would
<tel:%2B44%20%280%29%20207%20749%200229>
734 7033 <tel:%2B44%20%280%29%20207%20734%207033>,
<tel:%2B44%20%280%29%20207%20749%200229>
7033 <tel:%2B44%20%280%29%20207%20734%207033>,

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 12 May 2015 16:02:34 +0000",Re: Adding/Using More Resolution Types on JIRA,"Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","I tend to find that any large project has a lot of walking dead JIRAs, and
pretending they are simply Open causes problems. Any state is better for
these, so I favor this.

Agreed.


   1. Inactive: A way to clear out inactive/dead JIRA‚Äôs without
   indicating a decision has been made one way or the other.

 This is a good idea, and perhaps the process of closing JIRAs as Inactive
can be automated. If *nothing* about a JIRA has changed in 12 months or
more (e.g. current oldest open Spark issue; dates to Aug 2013: SPARK-867
<https://issues.apache.org/jira/browse/SPARK-867>), perhaps a bot can mark
it as such for us. (Here‚Äôs a list of stale issues
<https://issues.apache.org/jira/browse/SPARK-867?jql=project%20=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20updated%20%3C=%20-26w%20ORDER%20BY%20updated%20ASC>
).

This doesn‚Äôt mean the issue is invalid or won‚Äôt be addressed, but it gets
it out of the ‚ÄúOpen‚Äù queue, which ideally should be a high churn queue
(e.g. stuff doesn‚Äôt stay in there forever).

Nick
‚Äã


d
t
r
d
ed
d
r
ts
"
Stephen Carman <scarman@coldlight.com>,"Tue, 12 May 2015 18:03:39 +0000",s3 vfs on Mesos Slaves,"""dev@spark.apache.org"" <dev@spark.apache.org>","We have a small mesos cluster and these slaves need to have a vfs setup on them so that the slaves can pull down the data they need from S3 when spark runs.

There doesn‚Äôt seem to be any obvious way online on how to do this or how easily accomplish this. Does anyone have some best practices or some ideas about how to accomplish this?

An example stack trace when a job is ran on the mesos cluster‚Ä¶

Any idea how to get this going? Like somehow bootstrapping spark on run or something?

Thanks,
Steve


java.io.IOException: Unsupported scheme s3n for URI s3n://removed
        at com.coldlight.ccc.vfs.NeuronPath.toPath(NeuronPath.java:43)
        at com.coldlight.neuron.data.ClquetPartitionedData.makeInputStream(ClquetPartitionedData.java:465)
        at com.coldlight.neuron.data.ClquetPartitionedData.access$200(ClquetPartitionedData.java:42)
        at com.coldlight.neuron.data.ClquetPartitionedData$Iter.<init>(ClquetPartitionedData.java:330)
        at com.coldlight.neuron.data.ClquetPartitionedData.compute(ClquetPartitionedData.java:304)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
15/05/12 13:57:51 ERROR Executor: Exception in task 0.1 in stage 0.0 (TID 1)
java.lang.RuntimeException: java.io.IOException: Unsupported scheme s3n for URI s3n://removed
        at com.coldlight.neuron.data.ClquetPartitionedData.compute(ClquetPartitionedData.java:307)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Unsupported scheme s3n for URI s3n://removed
        at com.coldlight.ccc.vfs.NeuronPath.toPath(NeuronPath.java:43)
        at com.coldlight.neuron.data.ClquetPartitionedData.makeInputStream(ClquetPartitionedData.java:465)
        at com.coldlight.neuron.data.ClquetPartitionedData.access$200(ClquetPartitionedData.java:42)
        at com.coldlight.neuron.data.ClquetPartitionedData$Iter.<init>(ClquetPartitionedData.java:330)
        at com.coldlight.neuron.data.ClquetPartitionedData.compute(ClquetPartitionedData.java:304)
        ... 8 more

This e-mail is intended solely for the above-mentioned recipient and it may contain confidential or privileged information. If you have received it in error, please notify us immediately and delete the e-mail. You must not copy, distribute, disclose or take any action in reliance on it. In addition, the contents of an attachment to this e-mail may contain software viruses which could damage your own computer system. While ColdLight Solutions, LLC has taken every reasonable precaution to minimize this risk, we cannot accept liability for any damage which you sustain as a result of software viruses. You should perform your own virus checks before opening the attachment.
"
Kevin Markey <kevin.markey@oracle.com>,"Tue, 12 May 2015 12:34:59 -0600",Re: Change for submitting to yarn in 1.3.1,dev@spark.apache.org,"We have the same issue.  As result, we are stuck back on 1.0.2.

Not being able to programmatically interface directly with the Yarn 
client to obtain the application id is a show stopper for us, which is a 
real shame given the Yarn enhancements in 1.2, 1.3, and 1.4.

I understand that SparkLauncher was supposed to address these issues, 
but it really doesn't.  Yarn already provides indirection and an arm's 
length transaction for starting Spark on a cluster. The launcher 
introduces yet another layer of indirection and dissociates the Yarn 
Client from the application that launches it.

I am still reading the newest code, and we are still researching options 
to move forward.  If there are alternatives, we'd like to know.

Kevin Markey




---------------------------------------------------------------------


"
Alexey Goncharuk <alexey.goncharuk@gmail.com>,"Tue, 12 May 2015 12:51:39 -0700",Sharing memory across applications/integration,dev@spark.apache.org,"Hello Spark community,

I am currently trying to implement a proof-of-concept RDD that will allow
to integrate Apache Spark and Apache Ignite (incubating) [1]. My original
idea was to embed an Ignite node in Spark's worker process, in order for
the user code to have a direct access to in-memory data as it gives the
best performance without any need to explicitly load data into Spark.

However, after looking at the documentation and the following questions on
the user list [2], [3] I realized that it might be impossible to implement.

So can anybody in the community clarify or point me to the documentation
regarding the following questions:

   - Does worker spawn a new process for each application? Is there a way
   for workers to reuse the same process for different Spark contexts?
   - Is there a way to embed a worker in a user process?
   - Is there a way to attach a piece of user logic to a worker lifecycle
   events (initialization/destroy)?

Thanks,
Alexey

----

[1] http://ignite.incubator.apache.org/
[2]
http://apache-spark-user-list.1001560.n3.nabble.com/Embedding-Spark-Masters-Zk-Workers-SparkContext-App-in-single-JVM-clustered-sorta-for-symmetric-depl-td17711.html
[3]
http://apache-spark-user-list.1001560.n3.nabble.com/Sharing-memory-across-applications-td11845.html
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 12 May 2015 12:51:51 -0700",Re: Change for submitting to yarn in 1.3.1,Kevin Markey <kevin.markey@oracle.com>,"

Well, not fully. The launcher was supposed to solve ""how to launch a Spark
app programatically"", but in the first version nothing was added to
actually gather information about the running app. It's also limited in the
way it works because of Spark's limitations (one context per JVM, etc).

Still, adding things like this is something that is definitely in the scope
for the launcher library; information such as app id can be useful for the
code launching the app, not just in yarn mode. We just have to find a clean
way to provide that information to the caller.


Super hacky, but if you launch Spark as a child process you could parse the
stderr and get the app ID.

-- 
Marcelo
"
shane knapp <sknapp@berkeley.edu>,"Tue, 12 May 2015 13:14:09 -0700","[build system] brief downtime tomorrow morning (5-12-15, 7am PDT)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i will need to restart jenkins to finish a plugin install and resolve
https://issues.apache.org/jira/browse/SPARK-7561

this will be very brief, and i'll retrigger any errant jobs i kill.

please let me know if there are any comments/questions/concerns.

thanks!

shane
"
"""fightfate@163.com"" <fightfate@163.com>","Wed, 13 May 2015 09:07:22 +0800",,"""Night Wolf"" <nightwolfzor@gmail.com>","HiÔºå there
Which version are you using ? Actually the problem seems gone after we change our spark version from 1.2.0 to 1.3.0 

Not sure what the internal changes did.

Best,
Sun.



fightfate@163.com
 
From: Night Wolf
Date: 2015-05-12 22:05
To: fightfate@163.com
CC: Patrick Wendell; user; dev
Subject: Re: Re: Sort Shuffle performance issues about using AppendOnlyMap for large data sets
Seeing similar issues, did you find a solution? One would be to increase the number of partitions if you're doing lots of object creation. 

On Thu, Feb 12, 2015 at 7:26 PM, fightfate@163.com <fightfate@163.com> wrote:
Hi, patrick

Really glad to get your reply. 
Yes, we are doing group by operations for our work. We know that this is common for growTable when processing large data sets.

The problem actually goes to : Do we have any possible chance to self-modify the initialCapacity using specifically for our 
application? Does spark provide such configs for achieving that goal? 

We know that this is trickle to get it working. Just want to know that how could this be resolved, or from other possible channel for
we did not cover.

Expecting for your kind advice.

Thanks,
Sun.



fightfate@163.com
 
From: Patrick Wendell
Date: 2015-02-12 16:12
To: fightfate@163.com
CC: user; dev
Subject: Re: Re: Sort Shuffle performance issues about using AppendOnlyMap for large data sets
The map will start with a capacity of 64, but will grow to accommodate
new data. Are you using the groupBy operator in Spark or are you using
Spark SQL's group by? This usually happens if you are grouping or
aggregating in a way that doesn't sufficiently condense the data
created from each input partition.
 
- Patrick
 
On Wed, Feb 11, 2015 at 9:37 PM, fightfate@163.com <fightfate@163.com> wrote:
> Hi,
>
> Really have no adequate solution got for this issue. Expecting any available
> analytical rules or hints.
>
> Thanks,
> Sun.
>
> ________________________________
> fightfate@163.com
>
>
> From: fightfate@163.com
> Date: 2015-02-09 11:56
> To: user; dev
> Subject: Re: Sort Shuffle performance issues about using AppendOnlyMap for
> large data sets
> Hi,
> Problem still exists. Any experts would take a look at this?
>
> Thanks,
> Sun.
>
> ________________________________
> fightfate@163.com
>
>
> From: fightfate@163.com
> Date: 2015-02-06 17:54
> To: user; dev
> Subject: Sort Shuffle performance issues about using AppendOnlyMap for large
> data sets
> Hi, all
> Recently we had caught performance issues when using spark 1.2.0 to read
> data from hbase and do some summary work.
> Our scenario means to : read large data sets from hbase (maybe 100G+ file) ,
> form hbaseRDD, transform to schemardd,
> groupby and aggregate the data while got fewer new summary data sets,
> loading data into hbase (phoenix).
>
> Our major issue lead to : aggregate large datasets to get summary data sets
> would consume too long time (1 hour +) , while that
> should be supposed not so bad performance. We got the dump file attached and
> stacktrace from jstack like the following:
>
> From the stacktrace and dump file we can identify that processing large
> datasets would cause frequent AppendOnlyMap growing, and
> leading to huge map entrysize. We had referenced the source code of
> org.apache.spark.util.collection.AppendOnlyMap and found that
> the map had been initialized with capacity of 64. That would be too small
> for our use case.
>
> So the question is : Does anyone had encounted such issues before? How did
> that be resolved? I cannot find any jira issues for such problems and
> if someone had seen, please kindly let us know.
>
> More specified solution would goes to : Does any possibility exists for user
> defining the map capacity releatively in spark? If so, please
> tell how to achieve that.
>
> Best Thanks,
> Sun.
>
>    Thread 22432: (state = IN_JAVA)
> - org.apache.spark.util.collection.AppendOnlyMap.growTable() @bci=87,
> line=224 (Compiled frame; information may be imprecise)
> - org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.growTable()
> @bci=1, line=38 (Interpreted frame)
> - org.apache.spark.util.collection.AppendOnlyMap.incrementSize() @bci=22,
> line=198 (Compiled frame)
> -
> org.apache.spark.util.collection.AppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=201, line=145 (Compiled frame)
> -
> org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=3, line=32 (Compiled frame)
> -
> org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator)
> @bci=141, line=205 (Compiled frame)
> -
> org.apache.spark.shuffle.sort.SortShuffleWriter.write(scala.collection.Iterator)
> @bci=74, line=58 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=169, line=68 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=2, line=41 (Interpreted frame)
> - org.apache.spark.scheduler.Task.run(long) @bci=77, line=56 (Interpreted
> frame)
> - org.apache.spark.executor.Executor$TaskRunner.run() @bci=310, line=196
> (Interpreted frame)
> -
> java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker)
> @bci=95, line=1145 (Interpreted frame)
> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615
> (Interpreted frame)
> - java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)
>
>
> Thread 22431: (state = IN_JAVA)
> - org.apache.spark.util.collection.AppendOnlyMap.growTable() @bci=87,
> line=224 (Compiled frame; information may be imprecise)
> - org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.growTable()
> @bci=1, line=38 (Interpreted frame)
> - org.apache.spark.util.collection.AppendOnlyMap.incrementSize() @bci=22,
> line=198 (Compiled frame)
> -
> org.apache.spark.util.collection.AppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=201, line=145 (Compiled frame)
> -
> org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=3, line=32 (Compiled frame)
> -
> org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator)
> @bci=141, line=205 (Compiled frame)
> -
> org.apache.spark.shuffle.sort.SortShuffleWriter.write(scala.collection.Iterator)
> @bci=74, line=58 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=169, line=68 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=2, line=41 (Interpreted frame)
> - org.apache.spark.scheduler.Task.run(long) @bci=77, line=56 (Interpreted
> frame)
> - org.apache.spark.executor.Executor$TaskRunner.run() @bci=310, line=196
> (Interpreted frame)
> -
> java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker)
> @bci=95, line=1145 (Interpreted frame)
> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615
> (Interpreted frame)
> - java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)
>
>
> fightfate@163.com
> 1 attachments
> dump.png(42K) download preview

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 12 May 2015 18:25:24 -0700",[IMPORTANT] Committers please update merge script,"""dev@spark.apache.org"" <dev@spark.apache.org>","Due to an ASF infrastructure change (bug?) [1] the default JIRA
resolution status has switched to ""Pending Closed"". I've made a change
to our merge script to coerce the correct status of ""Fixed"" when
resolving [2]. Please upgrade the merge script to master.

I've manually corrected JIRA's that were closed with the incorrect
status. Let me know if you have any issues.

[1] https://issues.apache.org/jira/browse/INFRA-9646

[2] https://github.com/apache/spark/commit/1b9e434b6c19f23a01e9875a3c1966cd03ce8e2d

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Wed, 13 May 2015 10:19:28 +0800","=?utf-8?Q?=E5=9B=9E=E5=A4=8D=EF=BC=9A_?=[PySpark DataFrame]
 When a Row is not a Row",Nicholas Chammas <nicholas.chammas@gmail.com>,"The class (called Row) for rows from Spark SQL is created on the fly, is different from pyspark.sql.Row (is an public API to create Row by users).  

The reason we done it in this way is that we want to have better performance when accessing the columns. Basically, the rows are just named tuples (called `Row`).  

--  
Davies Liu
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)

Â∑≤‰ΩøÁî® Sparrow (http://www.sparrowmailapp.com/?sig)  

Âú® 2015Âπ¥5Êúà12Êó• ÊòüÊúü‰∫åÔºå‰∏äÂçà4:49ÔºåNicholas Chammas ÂÜôÈÅìÔºö



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 12 May 2015 20:54:12 -0700",Re: Change for submitting to yarn in 1.3.1,Marcelo Vanzin <vanzin@cloudera.com>,"Hey Kevin and Ron,

So is the main shortcoming of the launcher library the inability to
get an app ID back from YARN? Or are there other issues here that
fundamentally regress things for you.

It seems like adding a way to get back the appID would be a reasonable
addition to the launcher.

- Patrick


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 12 May 2015 22:38:18 -0700",@since version tag for all dataframe/sql methods,"""dev@spark.apache.org"" <dev@spark.apache.org>","I added @since version tag for all public dataframe/sql methods/classes in
this patch: https://github.com/apache/spark/pull/6101/files

public functions have @since tag. Thanks.
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 May 2015 01:52:14 -0700",lots of test warning messages from UISeleniumSuite,"""dev@spark.apache.org"" <dev@spark.apache.org>","Was looking at a PR test log just now. Can somebody take a look and remove
the warnings (or just hide them)?


15/05/13 01:49:35 INFO UISeleniumSuite: Trying to start HiveThriftServer2:
port=13125, mode=binary, attempt=0
15/05/13 01:50:28 INFO UISeleniumSuite: HiveThriftServer2 started
successfully
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [10:11] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [10:11] Ignoring the
following declarations in this rule.
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [15:41] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [15:41] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [26:14] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [26:14] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [39:24] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [39:24] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [67:23] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [67:23] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [69:190] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [69:190] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [72:31] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [72:31] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [73:45] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [73:45] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [74:45] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [74:45] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [75:44] Error in style
rule. (Invalid token ""*"". Was expecting one of: <EOF>, <S>, <IDENT>, ""}"",
"";"".)
"
Chester At Work <chester@alpinenow.com>,"Wed, 13 May 2015 05:44:16 -0700",Re: Change for submitting to yarn in 1.3.1,Patrick Wendell <pwendell@gmail.com>,"Patrick 
     There are several things we need, some of them already mentioned in the mailing list before. 

I haven't looked at the SparkLauncher code, but here are few things we need from our perspectives for Spark Yarn Client

     1) client should not be private ( unless alternative is provided) so we can call it directly.
     2) we need a way to stop the running yarn app programmatically ( the PR is already submitted) 
     3) before we start the spark job, we should have a call back to the application, which will provide the yarn container capacity (number of cores and max memory ), so spark program will not set values beyond max values (PR submitted)
     4) call back could be in form of yarn app listeners, which call back based on yarn status changes ( start, in progress, failure, complete etc), application can react based on these events in PR)
     
     5) yarn client passing arguments to spark program in the form of main program, we had experience problems when we pass a very large argument due the length limit. For example, we use json to serialize the argument and encoded, then parse them as argument. For wide columns datasets, we will run into limit. Therefore, an alternative way of passing additional larger argument is needed. We are experimenting with passing the args via a established akka messaging channel. 

    6) spark yarn client in yarn-cluster mode right now is essentially a batch job with no communication once it launched. Need to establish the communication channel so that logs, errors, status updates, progress bars, execution stages etc can be displayed on the application side. We added an akka communication channel for this (working on PR ).

       Combined with others items in this list, we are able to redirect print and error statement to application log (outside of the hadoop cluster), so spark UI equivalent progress bar via spark listener. We can show yarn progress via yarn app listener before spark started; and status can be updated during job execution.

    We are also experimenting with long running job with additional spark commands and interactions via this channel.


     Chester




     
      



Sent from my iPad


te:
t
h

k
he
pe
e
an

he

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 13 May 2015 06:45:44 -0700","Re: [build system] brief downtime tomorrow morning (5-12-15, 7am PDT)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is already done


"
Olivier Girardot <ssaboum@gmail.com>,"Wed, 13 May 2015 14:43:08 +0000",Re: @since version tag for all dataframe/sql methods,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","that's a great idea !

Le mer. 13 mai 2015 √† 07:38, Reynold Xin <rxin@databricks.com> a √©crit :

n
e
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 13 May 2015 15:25:44 +0000",Re: @since version tag for all dataframe/sql methods,"Olivier Girardot <ssaboum@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Are we not doing the same thing for the Python API?

:

©crit :
"
Yi Tian <tianyi.asiainfo@gmail.com>,"Wed, 13 May 2015 23:32:47 +0800",Re: lots of test warning messages from UISeleniumSuite,Reynold Xin <rxin@databricks.com>,"Shixiong have a PR working on this.

https://github.com/apache/spark/pull/5983

Sent from my iPhone

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 13 May 2015 15:51:11 +0000",Re: [PySpark DataFrame] When a Row is not a Row,Davies Liu <davies@databricks.com>,"Is there some way around this? For example, can Row just be an
implementation of namedtuple throughout?

from collections import namedtuple
class Row(namedtuple):
    ...

implementations of the Row class with the same name.

In my case, I was writing a method to recursively convert a Row to a dict
(since a Row can contain other Rows).

I couldn‚Äôt directly check type(obj) == pyspark.sql.types.Row so I ended up
having to do it like this:

def row_to_dict(obj):
    """"""
    Take a PySpark Row and convert it, and any of its nested Row
    objects, into Python dictionaries.
    """"""
    if isinstance(obj, list):
        return [row_to_dict(x) for x in obj]
    else:
        try:
            # We can't reliably check that this is a row object
            # due to some weird bug.
            d = obj.asDict()
            return {k: row_to_dict(v) for k, v in d.iteritems()}
        except:
            return obj

That comment about a ‚Äúweird bug‚Äù was my initial reaction, though now I
understand that we have 2 implementations of Row.

Isn‚Äôt this worth fixing? It‚Äôs just going to confuse people, IMO.

Nick


 The class (called Row) for rows from Spark SQL is created on the fly, is
d
Ôºå‰∏äÂçà4:49ÔºåNicholas Chammas ÂÜôÈÅìÔºö
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 13 May 2015 21:45:19 +0530",Re: s3 vfs on Mesos Slaves,Stephen Carman <scarman@coldlight.com>,"Did you happened to have a look at this https://github.com/abashev/vfs-s3

Thanks
Best Regards


n
rk
 or how
s
r
titionedData.java:465)
nedData.java:42)
onedData.java:330)
Data.java:304)
)
:1142)
a:617)
Data.java:307)
)
:1142)
a:617)
titionedData.java:465)
nedData.java:42)
onedData.java:330)
Data.java:304)
it
t
re
k,
f
"
Akshat Aranya <aaranya@gmail.com>,"Wed, 13 May 2015 09:18:32 -0700",Re: Task scheduling times,dev@spark.apache.org,"Hi,
Any input on this?  I'm willing to instrument further and experiment
if there are any ideas.


---------------------------------------------------------------------


"
jay vyas <jayunit100.apache@gmail.com>,"Wed, 13 May 2015 12:35:12 -0400",Re: s3 vfs on Mesos Slaves,Akhil Das <akhil@sigmoidanalytics.com>,"Might I ask why vfs?  I'm new to vfs and not sure wether or not it predates
the hadoop file system interfaces (HCFS).

After all spark natively supports any HCFS by leveraging the hadoop
FileSystem api and class loaders and so on.

So simply putting those resources on your classpath should be sufficient to
directly connect to s3. By using the sc.hadoopFile (...) commands.

is or how
titionedData.java:465)
nedData.java:42)
onedData.java:330)
Data.java:304)
:1142)
a:617)
ID
Data.java:307)
:1142)
a:617)
titionedData.java:465)
nedData.java:42)
onedData.java:330)
Data.java:304)
d
ng
"
Stephen Carman <scarman@coldlight.com>,"Wed, 13 May 2015 17:52:36 +0000",Re: s3 vfs on Mesos Slaves,jay vyas <jayunit100.apache@gmail.com>,"Thank you for the suggestions, the problem exists in the fact we need to initialize the vfs s3 driver so what you suggested Akhil wouldn‚Äôt fix the problem.

Basically a job is submitted to the cluster and it tries to pull down the data from s3, but fails because the s3 uri hasn‚Äôt been initilized in the  vfs and it doesn‚Äôt know how to handle
the URI.

What I‚Äôm asking is, how do we before the job is ran, run some bootstrapping or setup code that will let us do this initialization or configuration step for the vfs so that when it executes the job
it has the information it needs to be able to handle the s3 URI.

Thanks,
Steve

On May 13, 2015, at 12:35 PM, jay vyas <jayunit100.apache@gmail.com<mailto:jayunit100.apache@gmail.com>> wrote:


Might I ask why vfs?  I'm new to vfs and not sure wether or not it predates the hadoop file system interfaces (HCFS).

After all spark natively supports any HCFS by leveraging the hadoop FileSystem api and class loaders and so on.

So simply putting those resources on your classpath should be sufficient to directly connect to s3. By using the sc.hadoopFile (...) commands.

On May 13, 2015 12:16 PM, ""Akhil Das"" <akhil@sigmoidanalytics.com<mailto:akhil@sigmoidanalytics.com>> wrote:
Did you happened to have a look at this https://github.com/abashev/vfs-s3

Thanks
Best Regards

On Tue, May 12, 2015 at 11:33 PM, Stephen Carman <scarman@coldlight.com<mailto:scarman@coldlight.com>>
wrote:

> We have a small mesos cluster and these slaves need to have a vfs setup on
> them so that the slaves can pull down the data they need from S3 when spark
> runs.
>
> There doesn‚Äôt seem to be any obvious way online on how to do this or how
> easily accomplish this. Does anyone have some best practices or some ideas
> about how to accomplish this?
>
> An example stack trace when a job is ran on the mesos cluster‚Ä¶
>
> Any idea how to get this going? Like somehow bootstrapping spark on run or
> something?
>
> Thanks,
> Steve
>
>
> java.io.IOException: Unsupported scheme s3n for URI s3n://removed
>         at com.coldlight.ccc.vfs.NeuronPath.toPath(NeuronPath.java:43)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData.makeInputStream(ClquetPartitionedData.java:465)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData.access$200(ClquetPartitionedData.java:42)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData$Iter.<init>(ClquetPartitionedData.java:330)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData.compute(ClquetPartitionedData.java:304)
>         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
>         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
>         at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
>         at org.apache.spark.scheduler.Task.run(Task.scala:64)
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
>         at java.lang.Thread.run(Thread.java:745)
> 15/05/12 13:57:51 ERROR Executor: Exception in task 0.1 in stage 0.0 (TID
> 1)
> java.lang.RuntimeException: java.io.IOException: Unsupported scheme s3n
> for URI s3n://removed
>         at
> com.coldlight.neuron.data.ClquetPartitionedData.compute(ClquetPartitionedData.java:307)
>         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
>         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
>         at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
>         at org.apache.spark.scheduler.Task.run(Task.scala:64)
>         at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
>         at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
>         at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
>         at java.lang.Thread.run(Thread.java:745)
> Caused by: java.io.IOException: Unsupported scheme s3n for URI
> s3n://removed
>         at com.coldlight.ccc.vfs.NeuronPath.toPath(NeuronPath.java:43)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData.makeInputStream(ClquetPartitionedData.java:465)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData.access$200(ClquetPartitionedData.java:42)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData$Iter.<init>(ClquetPartitionedData.java:330)
>         at
> com.coldlight.neuron.data.ClquetPartitionedData.compute(ClquetPartitionedData.java:304)
>         ... 8 more
>
> This e-mail is intended solely for the above-mentioned recipient and it
> may contain confidential or privileged information. If you have received it
> in error, please notify us immediately and delete the e-mail. You must not
> copy, distribute, disclose or take any action in reliance on it. In
> addition, the contents of an attachment to this e-mail may contain software
> viruses which could damage your own computer system. While ColdLight
> Solutions, LLC has taken every reasonable precaution to minimize this risk,
> we cannot accept liability for any damage which you sustain as a result of
> software viruses. You should perform your own virus checks before opening
> the attachment.
>

This e-mail is intended solely for the above-mentioned recipient and it may contain confidential or privileged information. If you have received it in error, please notify us immediately and delete the e-mail. You must not copy, distribute, disclose or take any action in reliance on it. In addition, the contents of an attachment to this e-mail may contain software viruses which could damage your own computer system. While ColdLight Solutions, LLC has taken every reasonable precaution to minimize this risk, we cannot accept liability for any damage which you sustain as a result of software viruses. You should perform your own virus checks before opening the attachment.
"
Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"Wed, 13 May 2015 23:40:29 +0530",How to link code pull request with JIRA ID?,dev@spark.apache.org,"Hi,

I am new to open source contribution and trying to understand the process
starting from pulling code to uploading patch.

I have managed to pull code from GitHub. In JIRA I saw that each JIRA issue
is connected with pull request. I would like to know how do people attach
pull request details to JIRA issue?

Thanks,
Chandrash3khar Kotekar
Mobile - +91 8600011455
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 13 May 2015 18:11:56 +0000",Re: How to link code pull request with JIRA ID?,"Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>, dev@spark.apache.org","That happens automatically when you open a PR with the JIRA key in the PR
title.


"
Stephen Boesch <javadba@gmail.com>,"Wed, 13 May 2015 11:50:37 -0700",Re: How to link code pull request with JIRA ID?,Nicholas Chammas <nicholas.chammas@gmail.com>,"following up from Nicholas, it is

[SPARK-12345] Your PR description

where 12345 is the jira number.


 [MLLIB]


2015-05-13 11:11 GMT-07:00 Nicholas Chammas <nicholas.chammas@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Wed, 13 May 2015 11:56:33 -0700",Re: How to link code pull request with JIRA ID?,Stephen Boesch <javadba@gmail.com>,"Subproject tag should follow SPARK JIRA number.
e.g.

[SPARK-5277][SQL] ...

Cheers


"
Reynold Xin <rxin@databricks.com>,"Wed, 13 May 2015 13:05:52 -0700",Re: Task scheduling times,Akshat Aranya <aaranya@gmail.com>,"Maybe JIT? The 1st stage -- the scheduler code isn't JITed yet.


"
Markus Weimer <markus@weimo.de>,"Wed, 13 May 2015 15:19:25 -0700",Re: How to link code pull request with JIRA ID?,dev@spark.apache.org,"Hi,

how did you set this up? Over in the REEF incubation project, we
painstakingly create the forwards- and backwards links despite having
the IDs in the PR descriptions...

Thanks!

Markus



---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 13 May 2015 17:12:31 -0700","[build system] scheduled datacenter downtime, sunday may 17th","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","our datacenter is rejiggering our network (read: fully re-engineering large
portions from the ground up) and has downtime scheduled from 9am-3pm PDT,
this sunday may17th.

this means our jenkins instance will not be available to the outside world,
and i will be putting jenkins in to quiet mode the night before.  this will
allow any running builds to finish, and to save me from getting up @ 6am on
my day off.  :)

once things are back up and running (~3pm or earlier), i will purge the
build queue and bring jenkins out of quiet mode.

of course, stay tuned to this bat-channel for future, and potentially
riveting updates!
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 13 May 2015 18:00:42 -0700",Re: [IMPORTANT] Committers please update merge script,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All - unfortunately the fix introduced another bug, which is that
fixVersion was not updated properly. I've updated the script and had
one other person test it.

So committers please pull from master again thanks!

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 13 May 2015 19:22:16 -0700",Re: Change for submitting to yarn in 1.3.1,Chester At Work <chester@alpinenow.com>,"Hey Chester,

Thanks for sending this. It's very helpful to have this list.

The reason we made the Client API private was that it was never
intended to be used by third parties programmatically and we don't
intend to support it in its current form as a stable API. We thought
the fact that it was for internal use would be obvious since it
accepts arguments as a string array of CL args. It was always intended
for command line use and the stable API was the command line.

When we migrated the Launcher library we figured we covered most of
the use cases in the off chance someone was using the Client. It
appears we regressed one feature which was a clean way to get the app
ID.

The items you list here 2-6 all seem like new feature requests rather
than a regression caused by us making that API private.

I think the way to move forward is for someone to design a proper
long-term stable API for the things you mentioned here. That could
either be by extension of the Launcher library. Marcelo would be
natural to help with this effort since he was heavily involved in both
YARN support and the launcher. So I'm curious to hear his opinion on
how best to move forward.

I do see how apps that run Spark would benefit of having a control
plane for querying status, both on YARN and elsewhere.

- Patrick

ote:
the mailing list before.
ed from our perspectives for Spark Yarn Client
 we can call it directly.
 PR is already submitted)
application, which will provide the yarn container capacity (number of cores and max memory ), so spark program will not set values beyond max values (PR submitted)
 based on yarn status changes ( start, in progress, failure, complete etc), application can react based on these events in PR)
n program, we had experience problems when we pass a very large argument due the length limit. For example, we use json to serialize the argument and encoded, then parse them as argument. For wide columns datasets, we will run into limit. Therefore, an alternative way of passing additional larger argument is needed. We are experimenting with passing the args via a established akka messaging channel.
batch job with no communication once it launched. Need to establish the communication channel so that logs, errors, status updates, progress bars, execution stages etc can be displayed on the application side. We added an akka communication channel for this (working on PR ).
rint and error statement to application log (outside of the hadoop cluster), so spark UI equivalent progress bar via spark listener. We can show yarn progress via yarn app listener before spark started; and status can be updated during job execution.
 commands and interactions via this channel.
rote:
but
gth
et
ark
 the
cope
the
lean
ns
 the

---------------------------------------------------------------------


"
"""Chester @work"" <chester@alpinenow.com>","Wed, 13 May 2015 20:19:12 -0700",Re: Change for submitting to yarn in 1.3.1,Patrick Wendell <pwendell@gmail.com>,"Patrick
    Thanks for responding. Yes. many of are features requests not private client related. These are the things I have been working with since last year. 
    I have trying to push the PR for these changes. If the new Launcher lib is the way to go , we will try to work with new APIs. 

  Thanks
Chester

Sent from my iPhone

rote:
he mailing list before.
ed from our perspectives for Spark Yarn Client
e can call it directly.
R is already submitted)
pplication, which will provide the yarn container capacity (number of cores and max memory ), so spark program will not set values beyond max values (PR submitted)
ased on yarn status changes ( start, in progress, failure, complete etc), application can react based on these events in PR)
 program, we had experience problems when we pass a very large argument due the length limit. For example, we use json to serialize the argument and encoded, then parse them as argument. For wide columns datasets, we will run into limit. Therefore, an alternative way of passing additional larger argument is needed. We are experimenting with passing the args via a established akka messaging channel.
atch job with no communication once it launched. Need to establish the communication channel so that logs, errors, status updates, progress bars, execution stages etc can be displayed on the application side. We added an akka communication channel for this (working on PR ).
int and error statement to application log (outside of the hadoop cluster), so spark UI equivalent progress bar via spark listener. We can show yarn progress via yarn app listener before spark started; and status can be updated during job execution.
ommands and interactions via this channel.
rote:
ut
gth
et
ark
 the

cope
he
lean
ns
 the

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 14 May 2015 04:40:16 +0000",Re: How to link code pull request with JIRA ID?,"Markus Weimer <markus@weimo.de>, dev@spark.apache.org","There's no magic to it. We're doing the same, except Josh automated it in
the PR dashboard he created.

https://spark-prs.appspot.com/

Nick


"
Sean Owen <sowen@cloudera.com>,"Thu, 14 May 2015 10:12:03 +0100","Build change PSA: Hadoop 2.2 default; -Phadoop-x.y profile
 recommended for builds","user <user@spark.apache.org>, dev@spark.apache.org","This change will be merged shortly for Spark 1.4, and has a minor
implication for those creating their own Spark builds:

https://issues.apache.org/jira/browse/SPARK-7249
https://github.com/apache/spark/pull/5786

The default Hadoop dependency has actually been Hadoop 2.2 for some
time, but the defaults weren't fully consistent as a Hadoop 2.2 build.
That is what this resolves. The discussion highlights that it's
actually not great to rely on the Hadoop defaults, if you care at all
about the Hadoop binding, and that it's good practice to set some
-Phadoop-x.y profile in any build.


The net changes are:

If you don't care about Hadoop at all, you could ignore this. You will
get a consistent Hadoop 2.2 binding by default now. Still, you may
wish to set a Hadoop profile.

If you build for Hadoop 1, you need to set -Phadoop-1 now.

If you build for Hadoop 2.2, you should still set -Phadoop-2.2 even
though this is the default and is a no-op profile now.

You can continue to set other Hadoop profiles and override
hadoop.version; these are unaffected.

---------------------------------------------------------------------


"
Santiago Mola <smola@stratio.com>,"Thu, 14 May 2015 12:29:12 +0200",Simple SQL queries producing invalid results [SPARK-6743],dev@spark.apache.org,"Hi,

Could someone give a look to this issue?

[SPARK-6743] Join with empty projection on one side produces invalid results
https://issues.apache.org/jira/browse/SPARK-6743

Thank you,
-- 

Santiago M. Mola


<http://www.stratio.com/>
V√≠a de las dos Castillas, 33, √Åtica 4, 3¬™ Planta
28224 Pozuelo de Alarc√≥n, Madrid
Tel: +34 91 828 6473 // www.stratio.com // *@stratiobd
<https://twitter.com/StratioBD>*
"
Josh Rosen <rosenville@gmail.com>,"Thu, 14 May 2015 08:45:23 -0700",Re: How to link code pull request with JIRA ID?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Spark PRs didn't always used to handle the JIRA linking.  We used to rely
on a Jenkins job that ran
https://github.com/apache/spark/blob/master/dev/github_jira_sync.py.  We
switched this over to Spark PRs at a time when the Jenkins GitHub Pull
Request Builder plugin was having flakiness issues, but as far as I know
that old script should still work.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 14 May 2015 11:57:26 -0700",Re: How to link code pull request with JIRA ID?,Josh Rosen <rosenville@gmail.com>,"Yeah I wrote the original script and I intentionally made it easy for
other projects to use (you'll just need to tweak some variables at the
top). You just need somewhere to run it... we were using a jenkins
cluster to run it every 5 minutes.

BTW - I looked and there is one instance where it hard cores the
string ""SPARK-"", but that should be easy to change. I'm happy to
review a patch that makes that prefix a variable.

https://github.com/apache/spark/blob/master/dev/github_jira_sync.py#L71

- Patrick


---------------------------------------------------------------------


"
will-ob <will.obrien@tapjoy.com>,"Thu, 14 May 2015 13:44:44 -0700 (MST)","Re: practical usage of the new ""exactly-once"" supporting
 DirectKafkaInputDStream",dev@spark.apache.org,"Hey Cody (et. al.),

Few more questions related to this. It sounds like our missing data issues
appear fixed with this approach. Could you shed some light on a few
questions that came up?

---------------------

Processing our data inside a single foreachPartition function appears to be
very different from the pattern seen in the programming guide. Does this
become problematic with additional, interleaved reduce/filter/map steps?

```
# typical?
rdd
  .map { ... }
  .reduce { ... }
  .filter { ... }
  .reduce { ... }
  .foreachRdd { writeToDb }

# with foreachPartition
rdd.foreachPartition { case (iter) =>
  iter
    .map { ... }
    .reduce { ... }
    .filter { ... }
    .reduce { ... }
}

```
---------------------------------

Could the above be simplified by having

one kafka partition per DStream, rather than
one kafka partition per RDD partition

?

That way, we wouldn't need to do our processing inside each partition as
there would only be one set of kafka metadata to commit.

Presumably, one could `join` DStreams when topic-level aggregates were
needed.

It seems this was the approach of Michael Noll in his blog post.
(http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/)
Although, his primary motivation appears to be maintaining high-throughput /
parallelism rather than kafka metadata.

---------------------------------


""... there is no long-running receiver task that occupies a core per stream
regardless of what the message volume is.""

Is this because data is retrieved by polling rather than maintaining a
socket? Is it still the case that there is only one receiver process per
DStream? If so, maybe it is wise to keep DStreams and Kafka partitions 1:1
.. else discover the machine's NIC limit?

Can you think of a reason not to do this? Cluster utilization, or the like,
perhaps?

--------------------------------

And seems a silly question, but does `foreachPartition` guarantee that a
single worker will process the passed function? Or might two workers split
the work?

Eg. foreachPartition(f)

Worker 1:     f( Iterator[partition 1 records 1 - 50] )
Worker 2:     f( Iterator[partition 1 records 51 - 100] )

It is unclear from the scaladocs
(https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD).
But you can imagine, if it is critical that this data be committed in a
single transaction, that two workers will have issues.



-- Will O




--

---------------------------------------------------------------------


"
badgerpants <mark.stewart@tapjoy.com>,"Thu, 14 May 2015 13:59:49 -0700 (MST)",Error recovery strategies using the DirectKafkaInputDStream,dev@spark.apache.org,"We've been using the new DirectKafkaInputDStream to implement an exactly once
processing solution that tracks the provided offset ranges within the same
transaction that persists our data results. When an exception is thrown
within the processing loop and the configured number of retries are
exhausted the stream will skip to the end of the failed range of offsets and
continue on with the next  RDD. 

Makes sense but we're wondering how others would handle recovering from
failures. In our case the cause of the exception was a temporary outage of a
needed service. Since the transaction rolled back at the point of failure
our offset tracking table retained the correct offsets updated so we simply
needed to restart the Spark process whereupon it happily picked up at the
correct point and continued. Short of the restart do people have any good
ideas for how we might recover?

FWIW We've looked at setting spark.task.maxFailures param to a large value
and looked for a property that would increase the wait between attempts.
This might mitigate the issue when the availability problem is short lived
but wouldn't completely eliminate the need to restart.

Any thoughts, ideas welcome.



--

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 14 May 2015 16:21:47 -0500","Re: practical usage of the new ""exactly-once"" supporting DirectKafkaInputDStream",will-ob <will.obrien@tapjoy.com>,"If the transformation you're trying to do really is per-partition, it
shouldn't matter whether you're using scala methods or spark methods.  The
parallel speedup you're getting is all from doing the work on multiple
machines, and shuffle or caching or other benefits of spark aren't a factor.

If using scala methods bothers you, do all of your transformation using
spark methods, collect the results back to the driver, and save them with
the offsets there:

stream.foreachRDD { rdd =>
  val offsets = rdd.asInstanceOf[HasOffsets].offsetRanges
  val results = rdd.some.chain.of.spark.calls.collect
  save(offsets, results)
}

My work-in-progress slides for my talk at the upcoming spark conference are
here

http://koeninger.github.io/kafka-exactly-once/

if that clarifies that point a little bit (slides 20 vs 21)

The direct stream doesn't use long-running receivers, so the concerns that
blog post is trying to address don't really apply.

Under normal operation a given partition of an rdd is only going to be
handled by a single executor at a time (as long as you don't turn on
speculative execution... or I suppose it might be possible in some kind of
network partition situation).  Transactionality should save you even if
something weird happens though.


"
Reynold Xin <rxin@databricks.com>,"Thu, 14 May 2015 15:39:14 -0700",testing HTML email,"""dev@spark.apache.org"" <dev@spark.apache.org>","Testing html emails ...

Hello

*This is bold*

This is a link <http://databricks.com/>
"
Reynold Xin <rxin@databricks.com>,"Thu, 14 May 2015 15:41:58 -0700",testing attachment,"""dev@spark.apache.org"" <dev@spark.apache.org>","This is a link

This email should have some attachment

---------------------------------------------------------------------"
Cody Koeninger <cody@koeninger.org>,"Thu, 14 May 2015 17:43:18 -0500",Re: Error recovery strategies using the DirectKafkaInputDStream,badgerpants <mark.stewart@tapjoy.com>,"I think as long as you have adequate monitoring and Kafka retention, the
simplest solution is safest - let it crash.

"
Cody Koeninger <cody@koeninger.org>,"Thu, 14 May 2015 16:33:15 -0500","Re: practical usage of the new ""exactly-once"" supporting DirectKafkaInputDStream",will-ob <will.obrien@tapjoy.com>,"Sorry, realized I probably didn't fully answer your question about my blog
post, as opposed to Michael Nolls.

The direct stream is really blunt, a given RDD partition is just a kafka
topic/partition and an upper / lower bound for the range of offsets.  When
an executor computes the partition, it connects to kafka and pulls only
those messages, then closes the connection.  There's no long running
receiver at all, no caching of connections (I found caching sockets didn't
matter much).

You get much better cluster utilization that way, because if a partition is
relatively small compared to the others in the RDD, the executor gets done
with it and gets scheduled another one to work one.  With long running
receivers spark acts like the receiver takes up a core even if it isn't
doing much.  Look at the CPU graph on slide 13 of the link i posted.


"
Haoyuan Li <haoyuan.li@gmail.com>,"Thu, 14 May 2015 16:12:20 -0700",Re: s3 vfs on Mesos Slaves,Stephen Carman <scarman@coldlight.com>,"Another way is to configure S3 as Tachyon's under storage system, and then
run Spark on Tachyon.

More info: http://tachyon-project.org/Setup-UFS.html

Best,

Haoyuan


 fix the
 in the
is or how
titionedData.java:465)
nedData.java:42)
onedData.java:330)
Data.java:304)
:1142)
a:617)
ID
Data.java:307)
:1142)
a:617)
titionedData.java:465)
nedData.java:42)
onedData.java:330)
Data.java:304)
d
ng
it
t
re
k,
f



-- 
Haoyuan Li
CEO, Tachyon Nexus <http://www.tachyonnexus.com/>
AMPLab, EECS, UC Berkeley http://www.cs.berkeley.edu/~haoyuan/
"
Scott walent <scottwalent@gmail.com>,"Thu, 14 May 2015 16:55:55 -0700",Spark Summit 2015 - June 15-17 - Dev list invite,"dev@spark.apache.org, user@spark.apache.org","*Join the Apache Spark community at the fourth Spark Summit in San
Francisco on June 15, 2015. At Spark Summit 2015 you will hear keynotes
from NASA, the CIA, Toyota, Databricks, AWS, Intel, MapR, IBM, Cloudera,
Hortonworks, Timeful, O'Reilly, and Andreessen Horowitz. 260 talks proposal
were submitted by the community, and 55 were accepted. This year you‚Äôll
hear about Spark in use at companies including Uber, Airbnb, Netflix,
Taobao, Red Hat, Edmunds, Oracle and more.  See the full agenda at
http://spark-summit.org/2015 <http://spark-summit.org/2015>.  *




*If you are new to Spark or looking to improve on your knowledge of the
technology, we have three levels of Spark Training: Intro to Spark,
Advanced DevOps with Spark, and Data Science with Spark. Space is limited
and we will sell out so register now. Use promo code ""DevList15"" to save
15% when registering before June 1, 2015. Register at
http://spark-summit.org/2015/register
<http://spark-summit.org/2015/register>.I look forward to seeing you
there.Best, Scott & The Spark Summit Organizers*
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 15 May 2015 00:11:54 +0000",RE: testing HTML email,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Testing too. Recently I got few undelivered mails to dev-list.

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Thursday, May 14, 2015 3:39 PM
To: dev@spark.apache.org
Subject: testing HTML email

Testing html emails ...

Hello

This is bold

This is a link<http://databricks.com/>


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 14 May 2015 18:41:16 -0700",Re: Change for submitting to yarn in 1.3.1,Chester At Work <chester@alpinenow.com>,"Hi Chester,

Thanks for the feedback. A few of those are great candidates for
improvements to the launcher library.



Patrick already touched on this subject, but I believe Client should be
kept private. If we want to expose functionality for code launching Spark
apps, Spark should provide an interface for that so that other cluster
managers can benefit. It also keeps the API more consistent (everybody uses
the same API regardless of what's the underlying cluster manager).



My first reaction to this was ""with the app id, you can talk to YARN
directly and do that"". But given what I wrote above, I guess it would make
sense for something like this to be exposed through the library too.



I'm not sure exactly what you mean here, but it feels like we're starting
to get into ""wrapping the YARN API"" territory. Someone who really cares
about that information can easily fetch it from YARN, the same way Spark
would.



Exposing some sort of status for the running application does sound useful.



I believe you're talking about command line arguments to your application
here? I'm not sure what Spark can do to alleviate this. With YARN, if you
need to pass a lot of information to your application, I'd recommend
creating a file and adding it to ""--files"" when calling spark-submit. It's
not optimal, since the file will be distributed to all executors (not just
the AM), but it should help if you're really running into problems, and is
simpler than using akka.



This is another thing I'm a little unsure about. It seems that if you
really need to, you can implement your own SparkListener to do all this,
and then you can transfer all that data to wherever you need it to be.
Exposing something like this in Spark would just mean having to support
some new RPC mechanism as a public API, which is kinda burdensome. You
mention yourself you've done something already, which is an indication that
you can do it with existing Spark APIs, which makes it not a great
candidate for a core Spark feature. I guess you can't get logs with that
mechanism, but you can use your custom log4j configuration for that if you
really want to pipe logs to a different location.

But those are a good starting point - knowing what people need is the first
step in adding a new feature. :-)

-- 
Marcelo
"
Yi Zhang <zhangyi76@yahoo.com.INVALID>,"Fri, 15 May 2015 03:33:19 +0000 (UTC)",Does Spark SQL (JDBC) support nest select with current version,Dev <dev@spark.apache.org>,"The sql statement is like this:select t1._salory¬†as salory,
t1._name¬†as employeeName,
(select _name from¬†mock_locations¬†t3 where t3._id¬†= t1._location_id¬†) as locationName
from mock_employees t1
inner join mock_locations t2
on t1._location_id¬†= t2._id
where t1._salory¬†> t2._max_price

I noticed the issue¬†[SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRA¬†is still in the progress. And somebody commented it that Spark 1.3 would support it. So I don't know current status for this feature. ¬†Thanks.
Regards,Yi
| ¬† |
| ¬† | ¬† | ¬† | ¬† | ¬† |
| [SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRAjava.lang.RuntimeException: Unsupported language features in query: select customerid from sparkbug where customerid in (select customerid from sparkbug where customerid in (2,3))TOK_QUERY TOK_FROM TOK_TABREF TOK_TABNAME sparkbug TOK_INSERT TOK_DE... |
|  |
| View on issues.apache.org | Preview by Yahoo |
|  |
| ¬† |

 ¬†"
Yi Zhang <zhangyi76@yahoo.com.INVALID>,"Fri, 15 May 2015 05:09:54 +0000 (UTC)",Re: Does Spark SQL (JDBC) support nest select with current version,"Yi Zhang <zhangyi76@yahoo.com>, Dev <dev@spark.apache.org>","If I pass the whole statement as dbtable to sqlContext.load() method as below:val query =
  """"""
    (select t1._salory as salory,
    |t1._name as employeeName,
    |(select _name from mock_locations t3 where t3._id = t1._location_id ) as locationName
    |from mock_employees t1
    |inner join mock_locations t2
    |on t1._location_id = t2._id
    |where t1._salory > t2._max_price) EMP
  """""".stripMargin
val dataFrame = sqlContext.load(""jdbc"", Map(
  ""url"" -> url,
  ""driver"" -> ""com.mysql.jdbc.Driver"",
  ""dbtable"" -> query
)) 
It works. However, I can't invoke sql() method to solve this problem. And why?



   

 The sql statement is like this:select t1._salory¬†as salory,
t1._name¬†as employeeName,
(select _name from¬†mock_locations¬†t3 where t3._id¬†= t1._location_id¬†) as locationName
from mock_employees t1
inner join mock_locations t2
on t1._location_id¬†= t2._id
where t1._salory¬†> t2._max_price

I noticed the issue¬†[SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRA¬†is still in the progress. And somebody commented it that Spark 1.3 would support it. So I don't know current status for this feature. ¬†Thanks.
Regards,Yi
| ¬† |
| ¬† | ¬† | ¬† | ¬† | ¬† |
| [SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRAjava.lang.RuntimeException: Unsupported language features in query: select customerid from sparkbug where customerid in (select customerid from sparkbug where customerid in (2,3))TOK_QUERY TOK_FROM TOK_TABREF TOK_TABNAME sparkbug TOK_INSERT TOK_DE... |
|  |
| View on issues.apache.org | Preview by Yahoo |
|  |
| ¬† |

 ¬†

  "
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 15 May 2015 05:38:10 +0000",RE: Does Spark SQL (JDBC) support nest select with current version,"Yi Zhang <zhangyi76@yahoo.com>, Dev <dev@spark.apache.org>","You need to register the ‚ÄúdataFrame‚Äù as a table first and then do queries on it? Do you mean that also failed?

From: Yi Zhang [mailto:zhangyi76@yahoo.com.INVALID]
Sent: Friday, May 15, 2015 1:10 PM
To: Yi Zhang; Dev
Subject: Re: Does Spark SQL (JDBC) support nest select with current version

If I pass the whole statement as dbtable to sqlContext.load() method as below:

val query =
  """"""
    (select t1._salory as salory,
    |t1._name as employeeName,
    |(select _name from mock_locations t3 where t3._id = t1._location_id ) as locationName
    |from mock_employees t1
    |inner join mock_locations t2
    |on t1._location_id = t2._id
    |where t1._salory > t2._max_price) EMP
  """""".stripMargin
val dataFrame = sqlContext.load(""jdbc"", Map(
  ""url"" -> url,
  ""driver"" -> ""com.mysql.jdbc.Driver"",
  ""dbtable"" -> query
))

It works. However, I can't invoke sql() method to solve this problem. And why?



On Friday, May 15, 2015 11:33 AM, Yi Zhang <zhangyi76@yahoo.com.INVALID<mailto:zhangyi76@yahoo.com.INVALID>> wrote:

The sql statement is like this:
select t1._salory as salory,
t1._name as employeeName,
(select _name from mock_locations t3 where t3._id = t1._location_id ) as locationName
from mock_employees t1
inner join mock_locations t2
on t1._location_id = t2._id
where t1._salory > t2._max_price

I noticed the issue [SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRA<https://issues.apache.org/jira/browse/SPARK-4226> is still in the progress. And somebody commented it that Spark 1.3 would support it. So I don't know current status for this feature.  Thanks.

Regards,
Yi












[SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRA<https://issues.apache.org/jira/browse/SPARK-4226>
java.lang.RuntimeException: Unsupported language features in query: select customerid from sparkbug where customerid in (select customerid from sparkbug where customerid in (2,3)) TOK_QUERY TOK_FROM TOK_TABREF TOK_TABNAME sparkbug TOK_INSERT TOK_DE...


View on issues.apache.org<https://issues.apache.org/jira/browse/SPARK-4226>

Preview by Yahoo







"
Yi Zhang <zhangyi76@yahoo.com.INVALID>,"Fri, 15 May 2015 05:58:36 +0000 (UTC)",Re: Does Spark SQL (JDBC) support nest select with current version,"""Cheng, Hao"" <hao.cheng@intel.com>, Dev <dev@spark.apache.org>","@Hao,Because the querying joined more than one table, if I register data frame as temp table, Spark can't disguise which table is correct. I don't how to set dbtable and register temp table.¬†
Any suggestion? 


ote:
   

 #yiv0864379581 #yiv0864379581 --"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 15 May 2015 06:09:21 +0000",RE: Does Spark SQL (JDBC) support nest select with current version,"Yi Zhang <zhangyi76@yahoo.com>, Dev <dev@spark.apache.org>","Spark SQL just load the query result as a new source (via JDBC), so DO NOT confused with the Spark SQL tables. They are totally independent database systems.

From: Yi ZMay 15, 2015 1:59 PM
To: Cheng, Hao; Dev
Subject: Re: Does Spark SQL (JDBC) support nest select with current version

@Hao,
Because the querying joined more than one table, if I register data frame as temp table, Spark can't disguise which table is correct. I don't how to set dbtable and register temp table.

Any suggestion?


On Friday, May 15, 2015 1:38 PM, ""Cheng, Hao"" <hao.cheng@intel.com<mailto:hao.cheng@intel.com>> wrote:

You need to register the ‚ÄúdataFrame‚Äù as a table first and then do queries on it? Do you mean that also failed?

From: Yy, May 15, 2015 1:10 PM
To: Yi Zhang; Dev
Subject: Re: Does Spark SQL (JDBC) support nest select with current version

If I pass the whole statement as dbtable to sqlContext.load() method as below:

val query =
  """"""
    (select t1._salory as salory,
    |t1._name as employeeName,
    |(select _name from mock_locations t3 where t3._id = t1._location_id ) as locationName
    |from mock_employees t1
    |inner join mock_locations t2
    |on t1._location_id = t2._id
    |where t1._salory > t2._max_price) EMP
  """""".stripMargin
val dataFrame = sqlContext.load(""jdbc"", Map(
  ""url"" -> url,
  ""driver"" -> ""com.mysql.jdbc.Driver"",
  ""dbtable"" -> query
))

It works. However, I can't invoke sql() method to solve this problem. And why?



On Friday, May 15, 2015 11:33 AM, Yio.com.INVALID>> wrote:

The sql statement is like this:
select t1._salory as salory,
t1._name as employeeName,
(select _name from mock_locations t3 where t3._id = t1._location_id ) as locationName
from mock_employees t1
inner join mock_locations t2
on t1._location_id = t2._id
where t1._salory > t2._max_price

I noticed the issue [SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRA<https://issues.apache.org/jira/browse/SPARK-4226> is still in the progress. And somebody commented it that Spark 1.3 would support it. So I don't know current status for this feature.  Thanks.

Regards,
Yi












[SPARK-4226] SparkSQL - Add support for subqueries in predicates - ASF JIRA<https://issues.apache.org/jira/browse/SPARK-4226>
java.lang.RuntimeException: Unsupported language features in query: select customerid from sparkbug where customerid in (select customerid from sparkbug where customerid in (2,3)) TOK_QUERY TOK_FROM TOK_TABREF TOK_TABNAME sparkbug TOK_INSERT TOK_DE...


View on issues.apache.org<https://issues.apache.org/jira/browse/SPARK-4226>

Preview by Yahoo








"
Chester At Work <chester@alpinenow.com>,"Thu, 14 May 2015 23:18:29 -0700",Re: Change for submitting to yarn in 1.3.1,Marcelo Vanzin <vanzin@cloudera.com>,"Marcelo
     Thanks for the comments. All my requirements are from our work over last year in yarn-cluster mode. So I am biased on the yarn side.

      It's true some of the task might be able accomplished  with a separate yarn API call, the API just does not same to be that nature any more if we do that way.

       I had a great discussion (face to face) at data bricks today with Andrew Or to see how to address these requirements.

       For #3 Andrew points out that recent new feature of dynamic resource allocation make this requirement less important. As once the dynamic resource allocation is enabled, user doesn't need specify the number of executors or memories up front as before. In spark 1.x, user needs to specify these numbers, for small cluster, these jobs got killed immediately as if the memory specified is larger than yarn max memory. Also we were hoping to dynamically determine the executors and memory needed based on the data size, but make sure they are not exceeding the max.
         With dynamic resource allocation, I think we can just let spark handle this dynamically.

       For #4 spark context status tracker can give information, but you need to pull it based on certain time interval. Some kind event based call backs would be nice.

       For #5 yes, it's about the command line args. These are args are the input for the spark jobs. Seems a bit too much to create a file just to specify spark job args. These args could be few thousands columns in machine learning jobs.

       For #6 we was thinking our needs for communication is not special to us,  other applications may need this as well. But this maybe request too much changes in  spark

       In our case, we did the followings 
         1) we modified the yarn client to expose yarn app listener, so it call back on events based on spark yarn report interval (default to 1 sec). This gives us the container start, app in progress, failed, killed events

         2) in our own spark job, we wrap the main method with a akka actor which communicate with the actor in the application job submitter. A logger and spark job listener are created. Spark job listener send message to the logger. Logger relay the message to the application via akka actor. Std out and error are redirect to logger as well. Depending on the type of the messages, the application will update the UI (witch shows the progress bar) or log the message directly to the log file, or update the job state. We are using log4j, the issue is that in yarn cluster mode, the log are inside the cluster, not in the application, which is out side the cluster. We want to capture the cluster or error messages directly in the application log.
      
       I will put some design doc and actual code in my pull request later, as Andrew requested. This PR is unlikely to get merge in, but it will show the idea I am talking about here.

     Thanks for listening and responding 

Chester

Sent from my iPad


nts to the launcher library.
rote:
e can call it directly.
pt private. If we want to expose functionality for code launching Spark apps, Spark should provide an interface for that so that other cluster managers can benefit. It also keeps the API more consistent (everybody uses the same API regardless of what's the underlying cluster manager).
R is already submitted)
tly and do that"". But given what I wrote above, I guess it would make sense for something like this to be exposed through the library too.
pplication, which will provide the yarn container capacity (number of cores and max memory ), so spark program will not set values beyond max values (PR submitted)
o get into ""wrapping the YARN API"" territory. Someone who really cares about that information can easily fetch it from YARN, the same way Spark would.
ased on yarn status changes ( start, in progress, failure, complete etc), application can react based on these events in PR)
.
 program, we had experience problems when we pass a very large argument due the length limit. For example, we use json to serialize the argument and encoded, then parse them as argument. For wide columns datasets, we will run into limit. Therefore, an alternative way of passing additional larger argument is needed. We are experimenting with passing the args via a established akka messaging channel.
ere? I'm not sure what Spark can do to alleviate this. With YARN, if you need to pass a lot of information to your application, I'd recommend creating a file and adding it to ""--files"" when calling spark-submit. It's not optimal, since the file will be distributed to all executors (not just the AM), but it should help if you're really running into problems, and is simpler than using akka.
atch job with no communication once it launched. Need to establish the communication channel so that logs, errors, status updates, progress bars, execution stages etc can be displayed on the application side. We added an akka communication channel for this (working on PR ).
ly need to, you can implement your own SparkListener to do all this, and then you can transfer all that data to wherever you need it to be. Exposing something like this in Spark would just mean having to support some new RPC mechanism as a public API, which is kinda burdensome. You mention yourself you've done something already, which is an indication that you can do it with existing Spark APIs, which makes it not a great candidate for a core Spark feature. I guess you can't get logs with that mechanism, but you can use your custom log4j configuration for that if you really want to pipe logs to a different location.
t step in adding a new feature. :-) 
"
Niranda Perera <niranda.perera@gmail.com>,"Fri, 15 May 2015 13:16:43 +0530",Tentative due dates for Spark 1.3.2 release,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

May I know the tentative release dates for spark 1.3.2?

rgds

-- 
Niranda
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 15 May 2015 09:23:24 -0700",Re: Recent Spark test failures,Andrew Or <andrew@databricks.com>,"Jenkins build against hadoop 2.4 has been unstable recently:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/

I haven't found the test which hung / failed in recent Jenkins builds.

But PR builder has several green builds lately:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/

Maybe PR builder doesn't build against hadoop 2.4 ?

Cheers


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 15 May 2015 10:07:39 -0700",Re: Recent Spark test failures,Andrew Or <andrew@databricks.com>,"From
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/32831/consoleFull
:

[info] Building Spark with these arguments: -Pyarn -Phadoop-2.3
-Dhadoop.version=2.3.0 -Pkinesis-asl -Phive -Phive-thriftserver


Should PR builder cover hadoop 2.4 as well ?


Thanks



"
Frederick R Reiss <frreiss@us.ibm.com>,"Fri, 15 May 2015 10:02:32 -0700",Re: Recent Spark test failures,Ted Yu <yuzhihong@gmail.com>,"
The PR builder seems to be building against Hadoop 2.3. In the log for the
most recent successful build (
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/32805/consoleFull
) I see:

=========================================================================
Building Spark
=========================================================================
[info] Compile with Hive 0.13.1
[info] Building Spark with these arguments: -Pyarn -Phadoop-2.3
-Dhadoop.version=2.3.0 -Pkinesis-asl -Phive -Phive-thriftserver
...
=========================================================================
Running Spark unit tests
=========================================================================
[info] Running Spark tests with these arguments: -Pyarn -Phadoop-2.3
-Dhadoop.version=2.3.0 -Pkinesis-asl test

Is anyone testing individual pull requests against Hadoop 2.4 or 2.6 before
the code is declared ""clean""?

Fred



From:	Ted Yu <yuzhihong@gmail.com>
To:	Andrew Or <andrew@databricks.com>
Cc:	""dev@spark.apache.org"" <dev@spark.apache.org>
Date:	05/15/2015 09:29 AM
Subject:	Re: Recent Spark test failures



Jenkins build against hadoop 2.4 has been unstable recently:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/

I haven't found the test which hung / failed in recent Jenkins builds.

But PR builder has several green builds lately:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/

Maybe PR builder doesn't build against hadoop 2.4 ?

Cheers

  Makes sense.

  Having high†determinism in these tests would make Jenkins build stable.

ote:
   Hi Ted,

   Yes, those two options can be useful, but in general I think the
   standard to set is that tests should never fail. It's actually the worst
   if tests fail sometimes but not others, because we can't reproduce them
   deterministically. Using -M and -A actually tolerates flaky tests to a
   certain extent, and I would prefer to instead increase the determinism
   in these tests.

   -Andrew

   2015-05-08 17:56 GMT-07:00 Ted Yu <yuzhihong@gmail.com>:

     Andrew:
     Do you think the -M and -A options described here can be used in test
     runs ?
     http://scalatest.org/user_guide/using_the_runner

     Cheers

      Dear all,

      I'm sure you have all noticed that the Spark tests have been fairly
      unstable recently. I wanted to share a tool that I use to track which
      tests
      have been failing most often in order to prioritize fixing these
      flaky
      tests.

      Here is an output of the tool. This spreadsheet reports the top 10
      failed
      tests this week (ending yesterday 5/5):
      https://docs.google.com/spreadsheets/d/1Iv_UDaTFGTMad1sOQ_s4ddWr6KD3PuFIHmTSzL7LSb4


      It is produced by a small project:
      https://github.com/andrewor14/spark-test-failures

      I have been filing JIRAs on flaky tests based on this tool. Hopefully
      we
      can collectively stabilize the build a little more as we near the
      release
      for Spark 1.4.

      -Andrew




"
Sean Owen <sowen@cloudera.com>,"Fri, 15 May 2015 18:30:54 +0100",Re: Recent Spark test failures,Frederick R Reiss <frreiss@us.ibm.com>,"You all are looking only at the pull request builder. It just does one
build to sanity-check a pull request, since that already takes 2 hours and
would be prohibitive to build all configurations for every push. There is a
different set of Jenkins jobs that periodically tests master against a lot
more configurations, including Hadoop 2.4.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 15 May 2015 10:40:53 -0700",Re: Change for submitting to yarn in 1.3.1,Chester At Work <chester@alpinenow.com>,"Hi Chester,



The problem is that large command lines are not a Spark limitation, but a
platform limitation. So there's little that Spark can do if you run into
platform limits. Spark could automate the ""write all these args to a file
instead"" approach, but I wonder how many people actually run into that
issue to justify the extra code in Spark.

-- 
Marcelo
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 May 2015 11:22:57 -0700",Re: Tentative due dates for Spark 1.3.2 release,Niranda Perera <niranda.perera@gmail.com>,"Hi Niranda,

Maintenance releases are not done on a predetermined schedule but
instead according to which fixes show up and their severity. Since we
just did a 1.3.1 release I'm not sure I see 1.3.2 on the immediate
horizon.

However, the maintenance releases are simply builds at the head of the
respective release branches (in this case branch-1.3). They never
introduce new API's. If you have a particular bug fix you are waiting
for, you can always build Spark off of that branch.

- Patrick


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 15 May 2015 11:34:46 -0700",Re: Recent Spark test failures,Sean Owen <sowen@cloudera.com>,"bq. would be prohibitive to build all configurations for every push

Agreed.

Can PR builder rotate testing against hadoop 2.3, 2.4, 2.6 and 2.7 (each
test run still uses one hadoop profile) ?

This way we would have some coverage for each of the major hadoop releases.

Cheers


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 15 May 2015 11:40:57 -0700",Re: Recent Spark test failures,Ted Yu <yuzhihong@gmail.com>,"Funny thing, since I asked this question in a PR a few minutes ago...

Ignoring the rotation suggestion for a second, can the PR builder at least
cover hadoop 2.2? That's the actual version used to create the official
Spark artifacts for maven, and the oldest version Spark supports for YARN..

Kinda the same argument as the ""why do we build with java 7 when we support
java 6"" discussion we had recently.





-- 
Marcelo
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 May 2015 11:53:21 -0700",Re: Recent Spark test failures,Marcelo Vanzin <vanzin@cloudera.com>,"The PR builder currently builds against Hadoop 2.3.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 May 2015 11:55:32 -0700",Re: Recent Spark test failures,Marcelo Vanzin <vanzin@cloudera.com>,"Sorry premature send:

The PR builder currently builds against Hadoop 2.3
https://github.com/apache/spark/blob/master/dev/run-tests#L54

We can set this to whatever we want. 2.2 might make sense since it's the
default in our published artifacts.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 May 2015 12:57:37 -0700",Re: Adding/Using More Resolution Types on JIRA,Nicholas Chammas <nicholas.chammas@gmail.com>,"If there is no further feedback on this I will ask ASF Infra to add
the new fields ""Out of Scope"" and ""Inactive"".

- Patrick


---------------------------------------------------------------------


"
Chunnan Yao <yaochunnan@gmail.com>,"Sat, 16 May 2015 18:18:45 -0700 (MST)",How can I do pair-wise computation between RDD feature columns?,dev@spark.apache.org,"Hi all, 
Recently I've ran into a scenario to conduct two sample tests between all
paired combination of columns of an RDD. But the networking load and
generation of pair-wise computation is too time consuming. That has puzzled
me for a long time. I want to conduct Wilcoxon rank-sum test
(http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) here, and get the
top k most similar pairs. 

To be more concrete, I want to: 
input: original = RDD[Array[Double](3000)] 
output: a matrix M of the size 3000x3000, where M{i}{j} equals to the result
of a certain statistical test between RDD columns, that is,
original.map(_(i)) and original.map(_(j)) 

I've read the source code of Pearson and Spearman's correlation in MLlib
Statistics, as well as the implementation of the DIMSUM algorithm in
RowMatrix.scala, cuz they all conduct pair-wise computation between columns 
in a paralleled way. However, it seems that the reason why those tests are
applicable in Spark is because they only exploit column-summary info (sum of
all elements in RDD[Double[) and information in the same array, to be
explicit, they are all similar to the following: 
input: original = RDD[Array[Double](3000)] 
step1: summary = original.aggregate 
step2: summary_br = sc.broadcast(summary) 
step3: result =  original.map{i => val summary_v = summary_br.value; some
computation on i}.aggregate 
output: result: a matrix of 3000x3000 

They do not require info exchange between different records in RDD. However,
wilcoxon test requires co-ranking between pairs. It seems I have to generate
pair-wise computations one by one on RDD columns. This will conduct at least
(n^2-n)/2 jobs, which is nearly 5000000 when n=3000. It is not acceptable. 

Does anyone have better ideas? This is really torturing me cuz I have a
related project on hand!



-----
Feel the sparking Spark!
--

---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 16 May 2015 18:43:29 -0700",Re: How can I do pair-wise computation between RDD feature columns?,Chunnan Yao <yaochunnan@gmail.com>,"I opened it up today but it should help you:

https://github.com/apache/spark/pull/6213


"
Davies Liu <davies@databricks.com>,"Sun, 17 May 2015 01:26:32 -0700",Re: [SparkR] is toDF() necessary,shivaram@eecs.berkeley.edu,"toDF() is first introduced in Scala and Python (because
createDataFrame is too long), is used in lots places, I think it's
useful.


---------------------------------------------------------------------


"
Peter Prettenhofer <peter.prettenhofer@gmail.com>,"Sun, 17 May 2015 14:48:40 +0200",Resource usage of a spark application,dev@spark.apache.org,"Hi all,

I'm looking for a way to measure the current memory / cpu usage of a spark
application to provide users feedback how much resources are actually being
used.
It seems that the metric system provides this information to some extend.
It logs metrics on application level (nr of cores granted) and on the JVM
level (memory usage).
Is this the recommended way to gather this kind of information? If so, how
do i best map a spark application to the corresponding JVM processes?

If not, should i rather request this information from the resource manager
(e.g. Mesos/YARN)?

thanks,
 Peter

-- 
Peter Prettenhofer
"
Akhil Das <akhil@sigmoidanalytics.com>,"Sun, 17 May 2015 20:40:34 +0530",Re: Resource usage of a spark application,Peter Prettenhofer <peter.prettenhofer@gmail.com>,"You can either pull the high level information from your resource manager,
or if you want more control/specific information you can write a script and
pull the resource usage information from the OS. Something like this
<http://www.itsprite.com/linux3-shell-scripts-to-monitor-the-process-resource-in-linux/>
will help.

Thanks
Best Regards


"
Josh Rosen <rosenville@gmail.com>,"Sun, 17 May 2015 10:25:53 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",shane knapp <sknapp@berkeley.edu>,"Reminder: the network migration has started this morning, so Jenkins is
currently down.

Status updates on the migration are being published at
http://ucbsystems.org/


"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 13:44:49 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"jenkins is being a little recalcitrant and i'm looking at logs to see why
it won't start.
"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 13:51:02 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"ok, i think it's time to reboot the jenkins master.


"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 14:06:48 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"machine rebooted, but auth is completely broken (web and CLI on the
server).  i'm trying to fix this now.


"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 14:13:07 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"auth is fixed, and jenkins is out of quiet mode and now building.  sorry
for the delay!


"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 14:32:08 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"actually, LDAP auth is fixed, but if you have a local account that i've
created for you, it's not letting you log in to jenkins' UI.

looking at this now.


"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 14:34:35 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"...and we've lost network connectivity again.

things are still very flaky.  more updates as they come.


"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 14:46:53 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"...and we're back up.  it looks like things are going to be flaky for the
next couple of hours, so i'm going to let this stabilize before i dig in to
the auth problems.


"
shane knapp <sknapp@berkeley.edu>,"Sun, 17 May 2015 18:02:19 -0700","Re: [build system] scheduled datacenter downtime, sunday may 17th",Josh Rosen <rosenville@gmail.com>,"our sysadmins fixed the auth issue about an hour ago...  /etc/shadow's
perms got borked somehow and that was breaking logins for local (non-ldap)
accounts.

we're all green.


"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 18 May 2015 09:50:22 +0530",Re: Tentative due dates for Spark 1.3.2 release,Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,

Is there a separate location where I could download all the patches of each
branch to this date, so that i could apply it locally?

rgds





-- 
Niranda
"
Reynold Xin <rxin@databricks.com>,"Sun, 17 May 2015 21:21:09 -0700",Re: Tentative due dates for Spark 1.3.2 release,Niranda Perera <niranda.perera@gmail.com>,"You can just look at this branch, can't you?
https://github.com/apache/spark/tree/branch-1.3



"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 18 May 2015 09:54:09 +0530",Re: Tentative due dates for Spark 1.3.2 release,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

sorry, my mistake. can do that. thanks




-- 
Niranda
"
Tarek Elgamal <tarek.elgamal@gmail.com>,"Mon, 18 May 2015 13:13:05 +0300",Contribute code to MLlib,dev@spark.apache.org,"Hi,

I would like to contribute an algorithm to the MLlib project. I have
implemented a scalable PCA algorithm on spark. It is scalable for both tall
and fat matrices and the paper around it is accepted for publication in
SIGMOD 2015 conference. I looked at the guidelines in the following link:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-MLlib-specificContributionGuidelines

I believe that most of the guidelines applies in my case, however, the code
is written in java and it was not clear in the guidelines whether MLLib
project accepts java code or not.
My algorithm can be found under this repository:
https://github.com/Qatar-Computing-Research-Institute/sPCA

Any help on how to make it suitable for MLlib project will be greatly
appreciated.

Best Regards,
Tarek Elgamal
"
Tom Hubregtsen <thubregtsen@gmail.com>,"Mon, 18 May 2015 08:36:32 -0700 (MST)","Re: What is the location in the source code of the computation of
 the elements in a map transformation?",dev@spark.apache.org,"Hi Patrick, 

Thank you very much for your response. I am almost there, but am not sure
about my conclusion. Let me try to approach it from a different angle.

I would like to time the impact of a particular lambda function, or if
possible, more broadly measure the the impact of any map function. I would
like to do this by inserting timers into the code. What are my options for
this? I thought of two:
-  write an actual function instead of the lamdba, and start and end this
function with a timer, writing it's value into an accumulator. 
- in RDD.scala, insert a start and stop timer into compute(). 

The difference being that in the first option, I will measure more closely
the effects of the computation (but not the computation of my lambda, I
would measure the computation of a function equal to the lambda), whereas in
the second I would measure my lambda function but also measure more
overhead. Are there any more options? Did I miss any pro's or con's?

Thanks again,

Tom



--

---------------------------------------------------------------------


"
"""Fernando O."" <fotero@gmail.com>","Mon, 18 May 2015 13:43:58 -0300",Fwd: Problem building master on 2.11,dev@spark.apache.org,"I just noticed I sent this to users instead of dev:
---------- Forwarded message ----------
From: Fernando O. <fotero@gmail.com>
Date: Sat, May 16, 2015 at 4:09 PM
Subject: Problem building master on 2.11
To: ""user@spark.apache.org"" <user@spark.apache.org>


Is anyone else having issues when building spark from git?
I created a jira ticket with a Docker file that reproduces the issue.

The error:
/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/UploadBlock.java:56:
error: not found: type Type
  protected Type type() { return Type.UPLOAD_BLOCK; }


https://issues.apache.org/jira/browse/SPARK-7670
"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Mon, 18 May 2015 13:12:41 -0400",[SparkSQL] HiveContext multithreading bug?,dev <dev@spark.apache.org>,"Hi folks,

wanted to get a sanity check before opening a JIRA. I am trying to do the
following:

create a HiveContext, then from different threads:

1. Create a DataFrame
2. Name said df via registerTempTable
3. do a simple query via sql and dropTempTable

My understanding is that since HiveContext is thread safe this should work
fine. But I get into a state where step 3 cannot find the table:
org.apache.spark.sql.AnalysisException:
no such table

I put the full code and error observed here:
https://gist.github.com/yanakad/7faea20ca980e8085234

I don't think the parquet file itself is relevant but I can provide it if
I'm wrong. I can reproduce this in about 1 in 5 runs...

Thanks!
"
Imran Rashid <irashid@cloudera.com>,"Mon, 18 May 2015 16:02:45 -0500",Re: Spark 1.3.1 / Hadoop 2.6 package has broken S3 access,Steve Loughran <stevel@hortonworks.com>,"


we could re-open https://issues.apache.org/jira/browse/SPARK-4746

part of the point of going with tags instead of just unit / integration
dichotomy was to give us flexibility to add things like this

the basic prototyping for it is done, needs to be brought up to date and
polished.
"
Joseph Bradley <joseph@databricks.com>,"Mon, 18 May 2015 14:18:28 -0700",Re: Contribute code to MLlib,Tarek Elgamal <tarek.elgamal@gmail.com>,"Hi Tarek,


Algorithm: PCA is of course a critical algorithm.  The main question is how
your algorithm/implementation differs from the current PCA.  If it's
different and potentially better, I'd recommend opening up a JIRA for
explaining & discussing it.

Java/Scala: We really do require that algorithms be in Scala, for the sake
of maintainability.  The conversion should be doable if you're willing
since Scala is a pretty friendly language.  If you create the JIRA, you
could also ask for help there to see if someone can collaborate with you to
convert the code to Scala.

Thanks!
Joseph


"
"""daniel.mescheder"" <daniel.mescheder@realimpactanalytics.com>","Tue, 19 May 2015 02:43:58 -0700 (MST)","Performance & Memory Issues When Creating Many Columns in GROUP BY
 (spark-sql)",dev@spark.apache.org,"Dear List,
We have run into serious problems trying to run a larger than average number of aggregations in a GROUP BY query. Symptoms of this problem are OutOfMemory exceptions and unreasonably long processing times due to GC. The problem occurs when the following two conditions are met:
 - The number of groups is relatively large (growing with the size of the dataset)
 - The number of columns is relatively large
To reproduce, paste the following gist into your spark-shell (I'm running 1.3.1): https://gist.github.com/DanielMe/9467bb0d9ad3aa639429 <https://gist.github.com/DanielMe/9467bb0d9ad3aa639429>
This example is relatively small in size:
 - The size of the input is 10ÀÜ6 * 64bit = 8MB
 - The size of the output should be around 3 * 10ÀÜ8 * 64bit = 2.4GB
 - The aggregations themselves are just ""count(1)"" and hence not so difficult to compute
I am running this on a cluster with three 61GB worker machines and an equally equipped master with the following spark-defaults.conf:
spark.executor.memory=55g
spark.driver.memory=55g
The result: The workers will choke with ""java.lang.OutOfMemoryError: GC overhead limit exceeded"". In fact, if you play with the num_columns parameter you should observe an unreasonable amount of time spent on GC even for lower values. If you run this on a desktop machine, low values of num_columns should already lead to OOM crashes.
My questions are:
 - What causes this behaviour?
 - Can/should catalyst be able to automatically optimize queries of this kind to run in reasonable time or at least not crash?
 - What are  possible workarounds to achieve the desired effect? (Even if that means not using DataFrames but going down to the raw RDD level)
Our preliminary analysis of the situation concluded that what is blowing up is in fact the hashTable in Aggregate::doExecute which will try to store the cross product of groups and columns on each partition. In fact, we managed to mitigate the issue a bit by
 - reducing the size of the partitions (which will make these hash tables smaller)
 - pre-partitioning the data using a HashPartitioner on the key (which will reduce the number of different groups per partition)
The latter actually seems to be a sensible thing to do whenever num_columns*num_groups > num_rows because in this setting the amount of data we have to shuffle around after the first aggregation step is actually larger than the amount of data we had initially. Could this be something that catalyst should take into account when creating a physical plan?
Thanks in advance.
Kind regards,

Daniel 





--
3.nabble.com/Performance-Memory-Issues-When-Creating-Many-Columns-in-GROUP-BY-spark-sql-tp12313.html
om."
Edoardo Vacchi <uncommonnonsense@gmail.com>,"Tue, 19 May 2015 13:16:36 +0200",[Catalyst] RFC: Using PartialFunction literals instead of objects,dev@spark.apache.org,"Hi everybody,

At the moment, Catalyst rules are defined using two different types of rules:
`Rule[LogicalPlan]` and `Strategy` (which in turn maps to
`GenericStrategy[SparkPlan]`).

I propose to introduce utility methods to

  a) reduce the boilerplate to define rewrite rules
  b) turning them back into what they essentially represent: function types.

These changes would be backwards compatible, and would greatly help in
understanding what the code does. Personally, I feel like the current
use of objects is redundant and possibly confusing.

## `Rule[LogicalPlan]`

The analyzer and optimizer use `Rule[LogicalPlan]`, which, besides
defining a default `val ruleName`
only defines the method `apply(plan: TreeType): TreeType`.
Because the body of such method is always supposed to read `plan match
pf`, with `pf`
being some `PartialFunction[LogicalPlan, LogicalPlan]`, we can
conclude that `Rule[LogicalPlan]`
might be substituted by a PartialFunction.

I propose the following:

a) Introduce the utility method

    def rule(pf: PartialFunction[LogicalPlan, LogicalPlan]): Rule[LogicalPlan] =
      new Rule[LogicalPlan] {
        def apply (plan: LogicalPlan): LogicalPlan = plan transform pf
      }

b) progressively replace the boilerplate-y object definitions; e.g.

    object MyRewriteRule extends Rule[LogicalPlan] {
      def apply(plan: LogicalPlan): LogicalPlan = plan transform {
        case ... => ...
    }

with

    // define a Rule[LogicalPlan]
    val MyRewriteRule = rule {
      case ... => ...
    }

it might also be possible to make rule method `implicit`, thereby
further reducing MyRewriteRule to:

    // define a PartialFunction[LogicalPlan, LogicalPlan]
    // the implicit would convert it into a Rule[LogicalPlan] at the use sites
    val MyRewriteRule = {
      case ... => ...
    }


## Strategies

A similar solution could be applied to shorten the code for
Strategies, which are total functions
only because they are all supposed to manage the default case,
possibly returning `Nil`. In this case
we might introduce the following utility methods:

/**
 * Generate a Strategy from a PartialFunction[LogicalPlan, SparkPlan].
 * The partial function must therefore return *one single* SparkPlan
for each case.
 * The method will automatically wrap them in a [[Seq]].
 * Unhandled cases will automatically return Seq.empty
 */
protected def rule(pf: PartialFunction[LogicalPlan, SparkPlan]): Strategy =
  new Strategy {
    def apply(plan: LogicalPlan): Seq[SparkPlan] =
      if (pf.isDefinedAt(plan)) Seq(pf.apply(plan)) else Seq.empty
  }

/**
 * Generate a Strategy from a PartialFunction[ LogicalPlan, Seq[SparkPlan] ].
 * The partial function must therefore return a Seq[SparkPlan] for each case.
 * Unhandled cases will automatically return Seq.empty
 */
protected def seqrule(pf: PartialFunction[LogicalPlan,
Seq[SparkPlan]]): Strategy =
  new Strategy {
    def apply(plan: LogicalPlan): Seq[SparkPlan] =
      if (pf.isDefinedAt(plan)) pf.apply(plan) else Seq.empty[SparkPlan]
  }

Thanks in advance
e.v.

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 19 May 2015 15:40:31 +0200",Re: Problem building master on 2.11,"""Fernando O."" <fotero@gmail.com>","There's an open PR to fix it. If you could try it and report back on the PR
it'd be great. More likely to get in fast.

https://github.com/apache/spark/pull/6260




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Trevor Grant <trevor.d.grant@gmail.com>,"Tue, 19 May 2015 08:56:40 -0500",Re: Contribute code to MLlib,Joseph Bradley <joseph@databricks.com>," There are most likely advantages and disadvantages to Tarek's algorithm
against the current implementation, and different scenarios where each is
more appropriate.

Would we not offer multiple PCA algorithms and let the user choose?

Trevor

Trevor Grant
Data Scientist

*""Fortunate is he, who is able to know the causes of things.""  -Virgil*



"
Tim Ellison <t.p.ellison@gmail.com>,"Tue, 19 May 2015 15:20:23 +0100",Re: [ANNOUNCE] Ending Java 6 support in Spark 1.5 (Sep 2015),dev@spark.apache.org,"Sean,

Did the JIRA get created?  If so I can't find it so a pointer would be
helpful.

Regards,
Tim


---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Tue, 19 May 2015 14:22:11 +0000",Re: RDD split into multiple RDDs,"=?UTF-8?B?U8OpYmFzdGllbiBTb3VicsOpLUxhbmFiw6hyZQ==?= <s.soubre@gmail.com>, 
	dev@spark.apache.org","To do it in one pass, conceptually what you would need to do is to consume
the entire parent iterator and store the values either in memory or on
disk, which is generally something you want to avoid given that the parent
iterator length is unbounded. If you need to start spilling to disk, you
might actually get better performance just from doing multiple passes,
provided that you don't have that many unique keys. In fact, the filter
approach that you mentioned earlier is conceptually the same as the
implementation of randomSplit, where each of the split RDDs has access to
the full parent RDD then does the sample.

In addition, building the map is actually very cheap. Since its lazy, you
only do the filters when you need to iterate across the rdd of a specific
key.

s,
h
en
e)
keys, I
on
lb6hpq+state:results
ll
DD
un
by
m
g
e
n,
"
Sean Owen <sowen@cloudera.com>,"Tue, 19 May 2015 15:37:37 +0100",Re: [ANNOUNCE] Ending Java 6 support in Spark 1.5 (Sep 2015),Tim Ellison <t.p.ellison@gmail.com>,"No, I didn't yet. I was hoping to change the default version and make
a few obvious changes to take advantage of it all at once. Go ahead
with a JIRA. I can look into it this evening.

We have just a little actual Java code so the new language features
might be nice to use there but won't have a big impact.

However we might do well to replace some Guava usages with standard
JDK equivalents. I'd have to see just how much disruption it would
cause.


---------------------------------------------------------------------


"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Tue, 19 May 2015 08:41:33 -0700",Re: Contribute code to MLlib,Trevor Grant <trevor.d.grant@gmail.com>,"Hi Trevor, Tarek

You make non standard algorithms (PCA or otherwise) available to users of
Spark as Spark Packages.
http://spark-packages.org
https://databricks.com/blog/2014/12/22/announcing-spark-packages.html

With the availability of spark packages, adding powerful experimental /
alternative machine learning algorithms to the pipeline has never been
easier. I would suggest that route in scenarios where one machine learning
algorithm is not clearly better in the common scenarios than an existing
implementation in MLLib.

If your algorithm is for a large class of use cases better than the
existing PCA implementation, then we should open a JIRA and discuss the
relative strengths/ weaknesses (perhaps with some benchmarks) so we can
better understand if it makes sense to switch out the existing PCA
implementation and make yours the default.

Ram


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 19 May 2015 09:10:14 -0700",[VOTE] Release Apache Spark 1.4.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.0!

The tag to be voted on is v1.4.0-rc1 (commit 777a081):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=777a08166f1fb144146ba32581d4632c3466541e

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.4.0-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1092/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.4.0-rc1-docs/

Please vote on releasing this package as Apache Spark 1.4.0!

The vote is open until Friday, May 22, at 17:03 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.3 workload and running on this release candidate,
then reporting any regressions.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.4 QA period,
so -1 votes should only occur for significant regressions from 1.3.1.
Bugs already present in 1.3.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 19 May 2015 09:13:08 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","A couple of other process things:

1. Please *keep voting* (+1/-1) on this thread even if we find some
issues, until we cut RC2. This lets us pipeline the QA.
2. The SQL team owes a JIRA clean-up (forthcoming shortly)... there
are still a few ""Blocker's"" that aren't.



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 19 May 2015 17:20:10 +0100",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Before I vote, I wanted to point out there are still 9 Blockers for 1.4.0.
I'd like to use this status to really mean ""must happen before the
release"". Many of these may be already fixed, or aren't really blockers --
can just be updated accordingly.

I bet at least one will require further work if it's really meant for 1.4,
so all this means is there is likely to be another RC. We should still kick
the tires on RC1.

(I also assume we should be extra conservative about what is merged into
1.4 at this point.)


SPARK-6784 SQL Clean up all the inbound/outbound conversions for
DateType Adrian
Wang

SPARK-6811 SparkR Building binary R packages for SparkR Shivaram
Venkataraman

SPARK-6941 SQL Provide a better error message to explain that tables
created from RDDs are immutable
SPARK-7158 SQL collect and take return different results
SPARK-7478 SQL Add a SQLContext.getOrCreate to maintain a singleton
instance of SQLContext Tathagata Das

SPARK-7616 SQL Overwriting a partitioned parquet table corrupt data Cheng
Lian

SPARK-7654 SQL DataFrameReader and DataFrameWriter for input/output API Reynold
Xin

SPARK-7662 SQL Exception of multi-attribute generator anlysis in projection

SPARK-7713 SQL Use shared broadcast hadoop conf for partitioned table scan. Yin
Huai



"
Punyashloka Biswal <punya.biswal@gmail.com>,"Tue, 19 May 2015 16:32:18 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),"Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","When publishing future RCs to the staging repository, would it be possible
to use a version number that includes the ""rc1"" designation? In the current
setup, when I run a build against the artifacts at
https://repository.apache.org/content/repositories/orgapachespark-1092/org/apache/spark/spark-core_2.10/1.4.0/,
my local Maven cache will get polluted with things that claim to be 1.4.0
but aren't. It would be preferable for the version number to be 1.4.0-rc1
instead.

Thanks!
Punya


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 19 May 2015 09:39:05 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Punyashloka Biswal <punya.biswal@gmail.com>,"Punya,

Let me see if I can publish these under rc1 as well. In the future
this will all be automated but current it's a somewhat manual task.

- Patrick


---------------------------------------------------------------------


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Tue, 19 May 2015 16:40:41 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Thanks! I realize that manipulating the published version in the pom is a
bit inconvenient but it's really useful to have clear version identifiers
when we're juggling different versions and testing them out. For example,
this will come in handy when we compare 1.4.0-rc1 and 1.4.0-rc2 in a couple
of weeks :)

Punya


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 19 May 2015 09:41:06 -0700",branch-1.4 merge ettiquite,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Since we are now voting, please tread very carefully with branch-1.4 merges.

For instances, bug fixes that don't represent regressions from 1.3.X,
these probably shouldn't be merged unless they are extremely simple
and well reviewed.

As usual mature/core components (e.g. Spark core) are more sensitive
than newer/edge ones (e.g. Dataframes).

I'm happy to provide guidance to people if they are on the fence about
patches. Ultimately this ends up being a matter of judgement and
assessing risk of specific patches. Just ping me on github.

- Patrick

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 19 May 2015 11:35:57 -0700",Re: [Catalyst] RFC: Using PartialFunction literals instead of objects,Edoardo Vacchi <uncommonnonsense@gmail.com>,"Overall this seems like a reasonable proposal to me.  Here are a few
thoughts:

 - There is some debugging utility to the ruleName, so we would probably
want to at least make that an argument to the rule function.
 - We also have had rules that operate on SparkPlan, though since there is
only one ATM maybe we don't need sugar there.
 - I would not call the sugar for creating Strategies rule/seqrule, as I
think the one-to-one vs one-to-many distinction is useful.
 - I'm generally pro-refactoring to make the code nicer, especially when
its not official public API, but I do think its important to maintain
source compatibility (which I think you are) when possible as there are
other projects using catalyst.
 - Finally, we'll have to balance this with other code changes / conflicts.

You should probably open a JIRA and we can continue the discussion there.


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Tue, 19 May 2015 19:43:16 +0000",Re: Resource usage of a spark application,"Peter Prettenhofer <peter.prettenhofer@gmail.com>, dev@spark.apache.org","Hi Peter, a few months ago I was using MetricsSystem to export to Graphite
and then view in Grafana; relevant scripts and some instructions are here
<https://github.com/hammerlab/grafana-spark-dashboards/> if you want to
take a look.


"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 19 May 2015 14:29:52 -0700",OT: Key types which have potential issues,dev@spark.apache.org,"Hi,

  I vaguely remember issues with using float/double as keys in MR (and spark ?).
But cant seem to find documentation/analysis about the same.

Does anyone have some resource/link I can refer to ?


Thanks,
Mridul

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Tue, 19 May 2015 17:36:49 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Quick tests from my side - looks OK. The results are same or very similar
to 1.3.1. Will add dataframes et al in future tests.

+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 17:42 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.3.1
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
       Center And Scale OK
2.5. RDD operations OK
      State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
       Model evaluation/optimization (rank, numIter, lambda) with itertools
OK

Cheers
<k/>


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 19 May 2015 18:27:39 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","HI all,

I've created another release repository where the release is
identified with the version 1.4.0-rc1:

https://repository.apache.org/content/repositories/orgapachespark-1093/


---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Wed, 20 May 2015 14:30:00 +0530","Regarding ""Connecting spark to Mesos"" documentation","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi List,

In  the documentation of Connecting Spark to Mesos 
<http://spark.apache.org/docs/latest/running-on-mesos.html#connecting-spark-to-mesos>, 
is it possible to modify and write in detail the step  ""Create a binary 
package using make-distribution.sh --tgz"" ? When we use custom compiled 
version of Spark, mostly we specify a hadoop version (which is not the 
default one). In this case, make-distribution.sh should be supplied the 
same maven options we used for building spark. This is not specified  in 
the documentation. Please correct me , if I am wrong.

Regards,
Meethu Mathew
"
Edoardo Vacchi <uncommonnonsense@gmail.com>,"Wed, 20 May 2015 12:08:08 +0200",Re: [Catalyst] RFC: Using PartialFunction literals instead of objects,Michael Armbrust <michael@databricks.com>,"Thanks for the prompt feedback; I have further expanded on your
suggestions on this JIRA
https://issues.apache.org/jira/browse/SPARK-7754


---------------------------------------------------------------------


"
Hemant Bhanawat <hemant9379@gmail.com>,"Wed, 20 May 2015 16:10:16 +0530",Spark Streaming - Design considerations/Knobs,"dev@spark.apache.org, user@spark.apache.org","Hi,

I have compiled a list (from online sources) of knobs/design considerations
that need to be taken care of by applications running on spark streaming.
Is my understanding correct?  Any other important design consideration that
I should take care of?


   - A DStream is associated with a single receiver. For attaining read
   parallelism multiple receivers i.e. multiple DStreams need to be created.
   - A receiver is run within an executor. It occupies one core. Ensure
   that there are enough cores for processing after receiver slots are booked
   i.e. spark.cores.max should take the receiver slots into account.
   - The receivers are allocated to executors in a round robin fashion.
   - When data is received from a stream source, receiver creates blocks of
   data.  A new block of data is generated every blockInterval milliseconds. N
   blocks of data are created during the batchInterval where N =
   batchInterval/blockInterval.
   - These blocks are distributed by the BlockManager of the current
   executor to the block managers of other executors. After that, the Network
   Input Tracker running on the driver is informed about the block locations
   for further processing.
   - A RDD is created on the driver for the blocks created during the
   batchInterval. The blocks generated during the batchInterval are partitions
   of the RDD. Each partition is a task in spark. blockInterval==
   batchinterval would mean that a single partition is created and probably it
   is processed locally.
   - Having bigger blockinterval means bigger blocks. A high value of
   spark.locality.wait increases the chance of processing a block on the local
   node. A balance needs to be found out between these two parameters to
   ensure that the bigger blocks are processed locally.
   - Instead of relying on batchInterval and blockInterval, you can define
   the number of partitions by calling dstream.repartition(n). This reshuffles
   the data in RDD randomly to create n number of partitions.
   - An RDD's processing is scheduled by driver's jobscheduler as a job. At
   a given point of time only one job is active. So, if one job is executing
   the other jobs are queued.
   - If you have two dstreams there will be two RDDs formed and there will
   be two jobs created which will be scheduled one after the another.
   - To avoid this, you can union two dstreams. This will ensure that a
   single unionRDD is formed for the two RDDs of the dstreams. This unionRDD
   is then considered as a single job. However the partitioning of the RDDs is
   not impacted.
   - If the batch processing time is more than batchinterval then obviously
   the receiver's memory will start filling up and will end up in throwing
   exceptions (most probably BlockNotFoundException). Currently there is  no
   way to pause the receiver.
   - For being fully fault tolerant, spark streaming needs to enable
   checkpointing. Checkpointing increases the batch processing time.
   - The frequency of metadata checkpoint cleaning can be controlled using
   spark.cleaner.ttl. But, data checkpoint cleaning happens automatically when
   the RDDs in the checkpoint are no more required.



Thanks,
Hemant
"
Sean Owen <sowen@cloudera.com>,"Wed, 20 May 2015 13:03:18 +0100",userClassPathFirst and loader constraint violation,dev@spark.apache.org,"(Marcelo you might have some insight on this one)

Warning: this may just be because I'm doing something non-standard --
trying embed Spark in a Java app and feed it all the classpath it
needs manually. But this was surprising enough I wanted to ask.

I have an app that includes among other things SLF4J. I have set
spark.{driver,executor}.userClassPathFirst to true. If I run it and
let it start a Spark job, it quickly fails with:

2015-05-20 04:35:01,747 WARN  TaskSetManager:71 Lost task 0.0 in stage
0.0 (TID 0, x.cloudera.com): java.lang.LinkageError: loader constraint
violation: loader (instance of
org/apache/spark/util/ChildFirstURLClassLoader) previously initiated
loading for a different type with name ""org/slf4j/Logger""
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at org.apache.spark.util.ChildFirstURLClassLoader.liftedTree1$1(MutableURLClassLoader.scala:74)
at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:73)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at org.apache.spark.streaming.kafka.KafkaRDD.compute(KafkaRDD.scala:89)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
...

I can see that this class was loaded from my app JAR:

[Loaded org.slf4j.Logger from file:/home/sowen/oryx-batch-2.0.0-SNAPSHOT.jar]

I'm assuming it's also loaded in some Spark classloader.
Tracing the code, I don't see that it ever gets to consulting any
other classloader; this happens during its own child-first attempt to
load the class.

This didn't happen in 1.2, FWIW, when the implementation was
different, but that's only to say it was different, not correct.

Anyone have thoughts on what this indicates? something to be expected
or surprising?

I think that disabling userClassPathFirst gets rid of this of course,
although that may cause other issues later.

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 20 May 2015 14:05:42 +0100",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Signature, hashes, LICENSE/NOTICE, source tarball looks OK. I built
for Hadoop 2.6 (-Pyarn -Phive -Phadoop-2.6) on Ubuntu from source and
tests pass. The release looks OK except that I'd like to resolve the
Blockers before giving a +1.

I'm seeing some test failures, and wanted to cross-check with others.
They're all in Hive. Some I think are due to Java 8 differences and
are just test issues; they expect an exact output from a query plan
and some HashSet ordering differences make it trivially different. If
so, I've seen this in the past and we could ignore it for now, but
would be good to get a second set of eyes. The trace is big so it's at
the end.

When rerunning with Java 7 I get a different error due to Hive version support:

- success sanity check *** FAILED ***
  java.lang.RuntimeException: [download failed:
org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed:
commons-net#commons-net;3.1!commons-net.jar]
  at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:972)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62)
  ...



Hive / possible Java 8 test issue:

- windowing.q -- 20. testSTATs *** FAILED ***
  Results do not match for windowing.q -- 20. testSTATs:
  == Parsed Logical Plan ==
  'WithWindowDefinition Map(w1 -> WindowSpecDefinition ROWS BETWEEN 2
PRECEDING AND 2 FOLLOWING)
   'Project ['p_mfgr,'p_name,'p_size,UnresolvedWindowExpression
WindowSpecReference(w1)
   UnresolvedWindowFunction stddev
    UnresolvedAttribute [p_retailprice]
   AS sdev#159481,UnresolvedWindowExpression WindowSpecReference(w1)
   UnresolvedWindowFunction stddev_pop
    UnresolvedAttribute [p_retailprice]
   AS sdev_pop#159482,UnresolvedWindowExpression WindowSpecReference(w1)
   UnresolvedWindowFunction collect_set
    UnresolvedAttribute [p_size]
   AS uniq_size#159483,UnresolvedWindowExpression WindowSpecReference(w1)
   UnresolvedWindowFunction variance
    UnresolvedAttribute [p_retailprice]
   AS var#159484,UnresolvedWindowExpression WindowSpecReference(w1)
   UnresolvedWindowFunction corr
    UnresolvedAttribute [p_size]
    UnresolvedAttribute [p_retailprice]
   AS cor#159485,UnresolvedWindowExpression WindowSpecReference(w1)
   UnresolvedWindowFunction covar_pop
    UnresolvedAttribute [p_size]
    UnresolvedAttribute [p_retailprice]
   AS covarp#159486]
    'UnresolvedRelation [part], None

  == Analyzed Logical Plan ==
  p_mfgr: string, p_name: string, p_size: int, sdev: double, sdev_pop:
double, uniq_size: array<int>, var: double, cor: double, covarp:
double
  Project [p_mfgr#159489,p_name#159488,p_size#159492,sdev#159481,sdev_pop#159482,uniq_size#159483,var#159484,cor#159485,covarp#159486]
   Window [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494],
[HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
sdev#159481,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
sdev_pop#159482,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet(p_size#159492)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
uniq_size#159483,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
var#159484,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation(p_size#159492,p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
cor#159485,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance(p_size#159492,p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
covarp#159486], WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2
FOLLOWING
    Project [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494]
     MetastoreRelation default, part, None

  == Optimized Logical Plan ==
  Project [p_mfgr#159489,p_name#159488,p_size#159492,sdev#159481,sdev_pop#159482,uniq_size#159483,var#159484,cor#159485,covarp#159486]
   Window [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494],
[HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
sdev#159481,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
sdev_pop#159482,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet(p_size#159492)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
uniq_size#159483,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
var#159484,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation(p_size#159492,p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
cor#159485,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance(p_size#159492,p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
covarp#159486], WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2
FOLLOWING
    Project [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494]
     MetastoreRelation default, part, None

  == Physical Plan ==
  Project [p_mfgr#159489,p_name#159488,p_size#159492,sdev#159481,sdev_pop#159482,uniq_size#159483,var#159484,cor#159485,covarp#159486]
   Window [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494],
[HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
sdev#159481,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
sdev_pop#159482,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet(p_size#159492)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
uniq_size#159483,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance(p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
var#159484,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation(p_size#159492,p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
cor#159485,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance(p_size#159492,p_retailprice#159494)
WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS
covarp#159486], WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2
FOLLOWING
    Sort [p_mfgr#159489 ASC,p_mfgr#159489 ASC,p_name#159488 ASC], false
     Exchange (HashPartitioning 2)
      HiveTableScan
[p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494],
(MetastoreRelation default, part, None), None


...


---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Wed, 20 May 2015 09:35:23 -0500",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"-1

discovered I accidentally removed master & worker json endpoints, will
restore
https://issues.apache.org/jira/browse/SPARK-7760


"
Trevor Grant <trevor.d.grant@gmail.com>,"Wed, 20 May 2015 11:05:07 -0500",Re: Contribute code to MLlib,Ram Sriharsha <sriharsha.ram@gmail.com>,"Hey Ram,

I'm not speaking to Tarek's package specifically but to the spirit of
MLib.  There are a number of method/algorithms for PCA, I'm not sure by
what criterion the current one is considered 'standard'.

It is rare to find ANY machine learning algo that is 'clearly better' than
any other.  They are all tools, they have their place and time.  I agree
that it makes sense to field new algorithms as packages and then integrate
into MLib once they are 'proven' (in terms of stability/performance/anyone
cares).  That being said, if MLib takes the stance that 'what we have is
good enough unless something is *clearly* better', then it will never grow
into a suite with the depth and richness of sklearn. From a practitioner's
stand point, its nice to have everything I could ever want ready in an
'off-the-shelf' form.

'A large number of use cases better than existing' shouldn't be a criteria
when selecting what to include in MLib.  The important question should be,
'Are you willing to take on responsibility for maintaining this because you
may be the only person on earth who understands the mechanics AND how to
code it?'.   Obviously we don't want any random junk algo included.  But
trying to say, 'this way of doing PCA is better than that way in a large
class of cases' is like trying to say 'geometry is more important than
calculus in large class of cases"", maybe its true- but geometry won't help
you if you are in a case where you need calculus.

This all relies on the assumption that MLib is destined to be a rich data
science/machine learning package.  It may be that the goal is to make the
project as lightweight and parsimonious as possible, if so excuse me for
speaking out of turn.



"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Wed, 20 May 2015 09:31:29 -0700",Re: Contribute code to MLlib,Trevor Grant <trevor.d.grant@gmail.com>,"Hi Trevor

Good point, I didn't mean that some algorithm has to be clearly better than
another in every scenario to be included in MLLib. However, even if someone
is willing to be the maintainer of a piece of code, it does not make sense
to accept every possible algorithm into the core library.

That said, the specific algorithms should be discussed in the JIRA: as you
point out, there is no clear way to decide what algorithm to include and
what not to, and usually mature algorithms that serve a wide variety of
scenarios are easier to argue about but nothing prevents anyone from
opening a ticket to discuss any specific machine learning algorithm.

My suggestion was simply that for purposes of making experimental or newer
algorithms available to Spark users, it doesn't necessarily have to be in
the core library. Spark packages are good enough in this respect.

Isn't it better for newer algorithms to take this route and prove
themselves before we bring them into the core library? Especially given the
barrier to using spark packages is very low.

Ram




"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Wed, 20 May 2015 10:01:37 -0700",Re: Contribute code to MLlib,Trevor Grant <trevor.d.grant@gmail.com>,"Hi Trevor

I'm attaching the MLLib contribution guideline here:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-MLlib-specificContributionGuidelines

It speaks to widely known and accepted algorithms but not to whether an algorithm has to be better than another in every scenario etc

I think the guideline explains what a good contribution to the core library should look like better than I initially attempted to !

Sent from my iPhone

:
n another in every scenario to be included in MLLib. However, even if someone is willing to be the maintainer of a piece of code, it does not make sense to accept every possible algorithm into the core library.
 point out, there is no clear way to decide what algorithm to include and what not to, and usually mature algorithms that serve a wide variety of scenarios are easier to argue about but nothing prevents anyone from opening a ticket to discuss any specific machine learning algorithm.
 algorithms available to Spark users, it doesn't necessarily have to be in the core library. Spark packages are good enough in this respect.
es before we bring them into the core library? Especially given the barrier to using spark packages is very low.
rote:
b.  There are a number of method/algorithms for PCA, I'm not sure by what criterion the current one is considered 'standard'.  
n any other.  They are all tools, they have their place and time.  I agree that it makes sense to field new algorithms as packages and then integrate into MLib once they are 'proven' (in terms of stability/performance/anyone cares).  That being said, if MLib takes the stance that 'what we have is good enough unless something is clearly better', then it will never grow into a suite with the depth and richness of sklearn. From a practitioner's stand point, its nice to have everything I could ever want ready in an 'off-the-shelf' form. 
a when selecting what to include in MLib.  The important question should be, 'Are you willing to take on responsibility for maintaining this because you may be the only person on earth who understands the mechanics AND how to code it?'.   Obviously we don't want any random junk algo included.  But trying to say, 'this way of doing PCA is better than that way in a large class of cases' is like trying to say 'geometry is more important than calculus in large class of cases"", maybe its true- but geometry won't help you if you are in a case where you need calculus. 
 science/machine learning package.  It may be that the goal is to make the project as lightweight and parsimonious as possible, if so excuse me for speaking out of turn. 
f Spark as Spark Packages.
lternative machine learning algorithms to the pipeline has never been easier. I would suggest that route in scenarios where one machine learning algorithm is not clearly better in the common scenarios than an existing implementation in MLLib.
ting PCA implementation, then we should open a JIRA and discuss the relative strengths/ weaknesses (perhaps with some benchmarks) so we can better understand if it makes sense to switch out the existing PCA implementation and make yours the default.
m against the current implementation, and different scenarios where each is more appropriate.
ints:
s how your algorithm/implementation differs from the current PCA.  If it's different and potentially better, I'd recommend opening up a JIRA for explaining & discussing it.
ake of maintainability.  The conversion should be doable if you're willing since Scala is a pretty friendly language.  If you create the JIRA, you could also ask for help there to see if someone can collaborate with you to convert the code to Scala.
mplemented a scalable PCA algorithm on spark. It is scalable for both tall and fat matrices and the paper around it is accepted for publication in SIGMOD 2015 conference. I looked at the guidelines in the following link:
rk#ContributingtoSpark-MLlib-specificContributionGuidelines
e code is written in java and it was not clear in the guidelines whether MLLib project accepts java code or not. 
 appreciated. 
"
Debasish Das <debasish.das83@gmail.com>,"Wed, 20 May 2015 10:06:43 -0700",IndexedRowMatrix semantics,dev <dev@spark.apache.org>,"Hi,

For indexedrowmatrix and rowmatrix, both take RDD(vector)....is it possible
that it has intermixed dense and sparse vector....basically I am
considering a gemv flow when indexedrowmatrix has dense flag true, dot flow
otherwise...

Thanks.
Deb
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 20 May 2015 10:08:58 -0700",Re: userClassPathFirst and loader constraint violation,Sean Owen <sowen@cloudera.com>,"Hmm... this seems to be particular to logging (KafkaRDD.scala:89 in my tree
is a log statement). I'd expect KafkaRDD to be loaded from the system class
loader - or are you repackaging it in your app?

I'd have to investigate more to come with an accurate explanation here...
but it seems that the initialization of the logging system, which happens
after SparkSubmit runs and sets the context class loader to be an instance
of ChildFirstURLClassLoader, is causing things to blow up. I'll see if I
can spend some cycles coming up with a proper explanation (and hopefully a
fix or workaround).

For now, you could probably avoid this by not repackaging the logging
dependencies in your app.






-- 
Marcelo
"
Joseph Bradley <joseph@databricks.com>,"Wed, 20 May 2015 11:55:28 -0700",Re: Contribute code to MLlib,Ram Sriharsha <sriharsha.ram@gmail.com>,"Hi Trevor,

I may be repeating what Ram said, but to 2nd it, a few points:

We do want MLlib to become an extensive and rich ML library; as you said,
scikit-learn is a great example.  To make that happen, we of course need to
include important algorithms.  ""Important"" is hazy, but roughly means being
useful to a large number of users, improving a large number of use cases
(above what is currently available), and being well-established and tested.

Others and I may not be familiar with Tarek's algorithm (since it is so
new), so it will be important to discuss details on JIRA to establish the
cases in which the algorithm improves over current PCA.  That may require
discussion, community testing, etc.  If we establish that it is a clear
improvement in a large domain, then it could be valuable to have in MLlib
proper.  It's always going to be hard to tell where to draw the line, so
less common algorithms will require more testing before we commit to
including them in MLlib.

I like the Spark package suggestion since it would allow users immediately
start using the code, while the discussion on JIRA happens.  (Plus, if
package users find it useful, they can report that on the JIRA.)

Joseph


"
Tathagata Das <tdas@databricks.com>,"Wed, 20 May 2015 13:51:31 -0700",Re: Spark Streaming - Design considerations/Knobs,Hemant Bhanawat <hemant9379@gmail.com>,"Correcting the ones that are incorrect or incomplete. BUT this is good list
for things to remember about Spark Streaming.



received the block, and another where the block was replicated) that has
the blocks irrespective of block interval, unless non-local scheduling
kicks in (as you observed next).


(print, foreachRDD, saveAsXFiles) and the number of RDD actions in those
output operations.

dstream1.union(dstream2).foreachRDD { rdd => rdd.count() }    // one Spark
job per batch

dstream1.union(dstream2).foreachRDD { rdd => { rdd.count() ; rdd.count() }
}    // TWO Spark jobs per batch

dstream1.foreachRDD { rdd => rdd.count } ; dstream2.foreachRDD { rdd =>
rdd.count }  // TWO Spark jobs per batch


spark.streaming.receiver.maxRate

data checkpointing, needed by only some operations, increase batch
processing time. Read -
http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing
Furthemore, with checkpoint you can recover computation, but you may loose
some data (that was received but not processed before driver failed) for
some sources. Enabling write ahead logs and reliable source + receiver,
allow zero data loss. Read - WAL in
http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-semantics

cleaning. What are you are probably talking about is cleaning of shuffle
and other data in the executors. That can be cleaned using
spark.cleaner.ttl, but it is a brute force hammer and can clean more stuff
than you intend. Its not recommended to use that. Rather Spark has
GC-triggered cleaning of all that, when RDD objects are GCed, their shuffle
data, cached data, etc are also cleaned in the executors. You can trigger
GC based cleaning by called System.gc() in the driver periodically.


"
Joseph Bradley <joseph@databricks.com>,"Wed, 20 May 2015 13:54:21 -0700",Re: IndexedRowMatrix semantics,Debasish Das <debasish.das83@gmail.com>,"I believe it works with a mix of DenseVector and SparseVector types.
Joseph


"
Jeremy Lucas <jeremyalucas@gmail.com>,"Wed, 20 May 2015 23:23:18 +0000",Representing a recursive data type in Spark SQL,dev@spark.apache.org,"Spark SQL has proven to be quite useful in applying a partial schema to
large JSON logs and being able to write plain SQL to perform a wide variety
of operations over this data. However, one small thing that keeps coming
back to haunt me is the lack of support for recursive data types, whereby a
member of a complex/struct value can be of the same type as the
complex/struct value itself.

I am hoping someone may be able to point me in the right direction of where
to start to build out such capabilities, as I'd be happy to contribute, but
am very new to this particular component of the Spark project.
"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Wed, 20 May 2015 17:24:41 -0700",Low throughput and effect of GC in SparkSql GROUP BY,dev@spark.apache.org,"Hi,
Somewhat similar to Daniel Mescheder's mail yesterday on SparkSql, I have a
data point regarding the performance of Group By, indicating there's
excessive GC and it's impacting the throughput. I want to know if the new
memory manager for aggregations (https://github.com/apache/spark/pull/5725/)
is going to address this kind of issue.

I only have a small amount of data on each node (~360MB) with a large heap
size (18 Gig). I still see 2-3 minor collections happening whenever I do a
Select Sum() with a group by(). I have tried with different sizes for Young
Generation without much effect, though not with different GC algorithms
(Hm..I ought to try reducing the rdd storage fraction perhaps).

I have made a chart of my results [1] by adding timing code to
Aggregates.scala. The query is actually Query 2 from Berkeley's AmpLab
benchmark, running over 10 million records. The chart is from one of the 4
worker nodes in the cluster.

I am trying to square this with a claim on the Project Tungsten blog post
[2]: ""When profiling Spark user applications, we‚Äôve found that a large
fraction of the CPU time is spent waiting for data to be fetched from main
memory. ""

Am I correct in assuming that SparkSql is yet to reach that level of
efficiency, at least in aggregation operations?

Thanks.

[1] -
https://docs.google.com/spreadsheets/d/1HSqYfic3n5s9i4Wsi1Qg0FKN_AWz2vV7_6RRMrtzplQ/edit#gid=481134174
[2]
https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html

Pramod
"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Thu, 21 May 2015 00:38:59 +0000",Re: Representing a recursive data type in Spark SQL,"Jeremy Lucas <jeremyalucas@gmail.com>, dev@spark.apache.org","Hi Jeremy:

Row is a collect of 'Any'. So, you can be used as a recursive data type. Is
this what you were looking for?

Example:
val x = sc.parallelize(Array.range(0,10)).map(x => Row(Row(x),
Row(x.toString)))

Rakesh




"
Jeremy Lucas <jeremyalucas@gmail.com>,"Thu, 21 May 2015 01:07:30 +0000",Re: Representing a recursive data type in Spark SQL,"Rakesh Chalasani <vnit.rakesh@gmail.com>, dev@spark.apache.org","Hey Rakesh,

To clarify, what I was referring to is when doing something like this:

sqlContext.applySchema(rdd, mySchema)

mySchema must be a well-defined StructType, which presently does not allow
for a recursive type.



"
Reynold Xin <rxin@databricks.com>,"Wed, 20 May 2015 21:18:35 -0700",Re: Low throughput and effect of GC in SparkSql GROUP BY,Pramod Biligiri <pramodbiligiri@gmail.com>,"Does this turn codegen on? I think the performance is fairly different when
codegen is turned on.

For 1.5, we are investigating having codegen on by default, so users get
much better performance out of the box.



p
a
ng
4
large
n
6RRMrtzplQ/edit#gid=481134174
oser-to-bare-metal.html
"
Reynold Xin <rxin@databricks.com>,"Wed, 20 May 2015 21:40:27 -0700","Re: Performance & Memory Issues When Creating Many Columns in GROUP
 BY (spark-sql)","""daniel.mescheder"" <daniel.mescheder@realimpactanalytics.com>","It is a lot of columns, but I'm not sure if that's why it is running out of
memory. In Spark SQL, we are not yet doing external aggregation when the
number of keys is large in the aggregation hashmap. We will fix this and
have external aggregation in 1.5.



B
f
re
ly
mory-Issues-When-Creating-Many-Columns-in-GROUP-BY-spark-sql-tp12313.html>
"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Wed, 20 May 2015 23:43:36 -0700",Re: Low throughput and effect of GC in SparkSql GROUP BY,Reynold Xin <rxin@databricks.com>,"I hadn't turned on codegen. I enabled it and ran it again, it is running
4-5 times faster now! :)
Since my log statements are no longer appearing, I presume the code path
seems quite different from the earlier hashmap related stuff in
Aggregates.scala?

Pramod


m
e
w
 I
or
).
 4
t
 large
in
_6RRMrtzplQ/edit#gid=481134174
loser-to-bare-metal.html
"
Reynold Xin <rxin@databricks.com>,"Wed, 20 May 2015 23:47:56 -0700",Re: Low throughput and effect of GC in SparkSql GROUP BY,Pramod Biligiri <pramodbiligiri@gmail.com>,"Yup it is a different path. It runs GeneratedAggregate.


re's
ew
r I
for
s).
e 4
that a
from
7_6RRMrtzplQ/edit#gid=481134174
closer-to-bare-metal.html
"
Hemant Bhanawat <hemant9379@gmail.com>,"Thu, 21 May 2015 14:35:07 +0530",Re: Spark Streaming - Design considerations/Knobs,Tathagata Das <tdas@databricks.com>,"Honestly, given the length of my email, I didn't expect a reply. :-) Thanks
for reading and replying. However, I have a follow-up question:

I don't think if I understand the block replication completely. Are the
blocks replicated immediately after they are received by the receiver? Or
are they kept on the receiver node only and are moved only on shuffle? Has
the replication something to do with locality.wait?

Thanks,
Hemant


"
Peter Prettenhofer <peter.prettenhofer@gmail.com>,"Thu, 21 May 2015 11:22:15 +0200",Re: Resource usage of a spark application,Ryan Williams <ryan.blake.williams@gmail.com>,"Thanks Akhil, Ryan!

@Akhil: YARN can only tell me how much vcores my app has been granted but
not actual cpu usage, right? Pulling mem/cpu usage from the OS means i need
to map JVM executor processes to the context they belong to, right?

@Ryan: what a great blog post -- this is super relevant for me to analyze
the state of the cluster as a whole. However, it seems to me that those
metrics are mostly reported globally and not per spark application.

2015-05-19 21:43 GMT+02:00 Ryan Williams <ryan.blake.williams@gmail.com>:



-- 
Peter Prettenhofer
"
zhangxiongfei  <zhangxiongfei0815@163.com>,"Thu, 21 May 2015 18:43:45 +0800 (CST)",Re:Re: Low throughput and effect of GC in SparkSql GROUP BY,"""Pramod Biligiri"" <pramodbiligiri@gmail.com>","Hi Pramod


 Is your data compressed? I encountered similar problem,however, after turned codegen on, the GC time was still very long.The size of  input data for my map task is about 100M lzo file.
My query is """"select ip, count(*) as c from stage_bitauto_adclick_d group by ip sort by c limit 100""""


Thanks
Zhang Xiongfei




At 2015-05-21 12:18:35, ""Reynold Xin"" <rxin@databricks.com> wrote:

Does this turn codegen on? I think the performance is fairly different when codegen is turned on.


For 1.5, we are investigating having codegen on by default, so users get much better performance out of the box.




On Wed, May 20, 2015 at 5:24 PM, Pramod Biligiri <pramodbiligiri@gmail.com> wrote:

Hi,
Somewhat similar to Daniel Mescheder's mail yesterday on SparkSql, I have a data point regarding the performance of Group By, indicating there's excessive GC and it's impacting the throughput. I want to know if the new memory manager for aggregations (https://github.com/apache/spark/pull/5725/) is going to address this kind of issue.


I only have a small amount of data on each node (~360MB) with a large heap size (18 Gig). I still see 2-3 minor collections happening whenever I do a Select Sum() with a group by(). I have tried with different sizes for Young Generation without much effect, though not with different GC algorithms (Hm..I ought to try reducing the rdd storage fraction perhaps).


I have made a chart of my results [1] by adding timing code to Aggregates.scala. The query is actually Query 2 from Berkeley's AmpLab benchmark, running over 10 million records. The chart is from one of the 4 worker nodes in the cluster.


I am trying to square this with a claim on the Project Tungsten blog post [2]: ""When profiling Spark user applications, we°Øve found that a large fraction of the CPU time is spent waiting for data to be fetched from main memory. ""


Am I correct in assuming that SparkSql is yet to reach that level of efficiency, at least in aggregation operations?



Thanks.


[1] - https://docs.google.com/spreadsheets/d/1HSqYfic3n5s9i4Wsi1Qg0FKN_AWz2vV7_6RRMrtzplQ/edit#gid=481134174
[2] https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html


Pramod

"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 21 May 2015 16:30:12 +0530",Re: Resource usage of a spark application,Peter Prettenhofer <peter.prettenhofer@gmail.com>,"Yes Peter that's correct, you need to identify the processes and with that
you can pull the actual usage metrics.

Thanks
Best Regards


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Thu, 21 May 2015 14:27:54 +0000",Re: Resource usage of a spark application,Peter Prettenhofer <peter.prettenhofer@gmail.com>,"

Thanks! You can definitely analyze metrics per-application in several ways:

   - If you're running Spark on YARN, use the ""app"" URL param
   <https://github.com/hammerlab/grafana-spark-dashboards#appyarn-app-id>
   to specify a YARN application ID, which will set the Spark application ID
   as well as parse job start/end times.
   - Set the ""prefix"" URL param
   <https://github.com/hammerlab/grafana-spark-dashboards#prefixmetric-prefix>
   to your Spark app's ID, and all metrics will be namespaced to that app ID.
      - You actually have to do one of these two, otherwise it doesn't know
      what app's metrics to look for; it is set up specifically to view per-app
      metrics.
   - There is a dropdown in the upper-left of the page (sorry, don't have a
   screenshot right now) that will let you select from all app IDs that
   graphite has seen metrics from.

Let me know, here or in issues on the repo, if you have any issues with
that or that doesn't make sense!


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 21 May 2015 17:08:47 +0200","Why use ""lib_managed"" for the Sbt build?","""dev@spark.apache.org"" <dev@spark.apache.org>","I‚Äôm trying to understand why Sbt is configured to pull all libs under
lib_managed.

   - it seems like unnecessary duplication (I will have those libraries
   under ./m2, via maven anyway)
   - every time I call make-distribution I lose lib_managed (via mvn clean
   install) and have to wait to download again all jars next time I use sbt
   - Eclipse does not handle relative paths very well (source attachments
   from lib_managed don‚Äôt always work)

So, what is the advantage of putting all dependencies in there, instead of
using the default `~/.ivy2`?

cheers,
iulian
‚Äã
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Yijie Shen <henry.yijieshen@gmail.com>,"Fri, 22 May 2015 00:37:07 +0800",Tungsten's Vectorized Execution,Spark-dev <dev@spark.apache.org>,"Hi all,

I‚Äôve seen the Blog of Project Tungsten¬†here, it sounds awesome to me!

I‚Äôve also noticed there is a plan to change the code generation from record-at-a-time evaluation to a vectorized one, which interests me most.

What‚Äôs the status of vectorized evaluation? ¬†Is this an inner effort of Databricks or welcome to be involved? ¬†¬†

Since I‚Äôve done similar stuffs on Spark SQL, I would like to get involved if that‚Äôs possible.



Yours,

Yijie"
Akshat Aranya <aaranya@gmail.com>,"Thu, 21 May 2015 10:15:31 -0700",Customizing Akka configuration for Spark,dev@spark.apache.org,"Hi,

Is there some way to customize the Akka configuration for Spark?
Specifically, I want to experiment with custom serialization for messages
that are send between the driver and executors in standalone mode.

Thanks,
Akshat
"
Tathagata Das <tdas@databricks.com>,"Thu, 21 May 2015 11:54:38 -0700","Re: Spark Streaming with Tachyon : Data Loss on Receiver Failure due
 to WAL error",Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Looks like somehow the file size reported by the FSInputDStream of
Tachyon's FileSystem interface, is returning zero.


"
Santiago Mola <smola@stratio.com>,"Thu, 21 May 2015 22:06:08 +0200",Re: Adding/Using More Resolution Types on JIRA,Patrick Wendell <pwendell@gmail.com>,"2015-05-12 9:50 GMT+02:00 Patrick Wendell <pwendell@gmail.com>:


Why is this needed? Every JIRA listing can be sorted by activity. That gets
the inactive ones out of your view quickly. I do not see any reason why an
issue should be closed because of this. If it's inactive, maybe it's because
it falls on some of the other categories (out of scope, later, won't fix).

I can only think about a case where closing an inactive issue makes sense:

*  A bug was reported long time ago. Nobody was able to reproduce (after
   trying actively) and the reporter is no longer around to provide more
info.

That is a much more specific case than ""Inactivity"", and a lot of large
scale
open source projects use specific resolutions for this.

requests?
I see a tendency in the Spark project to do unusual things with issues / PRs
just to maintain the numbers low. For example, closing PRs after a couple of
weeks of inactivity just to shrink the queue or closing active issues just
for the
shake of it.

Honestly, this looks a lot like trying to game metrics. But maybe there is
something that I am missing.

Maybe what it is actually needed is to improve the lifecycle of an issue
while
it is alive, instead of trying to kill it earlier. Some examples of this
that are
used on other projects are the ""incomplete"" status to signal that there is
more
info required from the reporter in order to take further action. Also
""confirmed""
to acknowledge that a bug is confirmed to be present and needs action by
a developer.

Best,
-- 

Santiago M. Mola


<http://www.stratio.com/>
V√≠a de las dos Castillas, 33, √Åtica 4, 3¬™ Planta
28224 Pozuelo de Alarc√≥n, Madrid
Tel: +34 91 828 6473 // www.stratio.com // *@stratiobd
<https://twitter.com/StratioBD>*
"
Santiago Mola <smola@stratio.com>,"Thu, 21 May 2015 22:32:03 +0200",Re: Adding/Using More Resolution Types on JIRA,Patrick Wendell <pwendell@gmail.com>,"Some examples to illustrate my point. A couple of issues from the oldest
open issues
in the SQL component:

[SQL] spark-sql exits while encountered an error
https://issues.apache.org/jira/browse/SPARK-4572
This is an incomplete report that nobody can take action on. It can be
resolved
as ""Incomplete"", instead of Inactive. In fact, there is no need to wait
that much
to close it as ""Incomplete"".

Using a field in a WHERE clause that is not in the schema does not throw an
exception
https://issues.apache.org/jira/browse/SPARK-5305
This is also ""Incomplete"", it is not reproducible currently and it is
missing version info.

Rather than a need of these new statuses, it seems that there is a need of
more
people assessing and triaging new issues.

Best,
-- 

Santiago M. Mola


<http://www.stratio.com/>
V√≠a de las dos Castillas, 33, √Åtica 4, 3¬™ Planta
28224 Pozuelo de Alarc√≥n, Madrid
Tel: +34 91 828 6473 // www.stratio.com // *@stratiobd
<https://twitter.com/StratioBD>*
"
Sean Owen <sowen@cloudera.com>,"Thu, 21 May 2015 21:39:33 +0100",Re: Adding/Using More Resolution Types on JIRA,Santiago Mola <smola@stratio.com>,"
I don't think sorting helps or that browsing is the issue. What if
you're searching for Open Critical issues concerning Pyspark? If the
list is full of issues that are actually out of scope, later, won't
fix, then that's a problem.


Yes, that's ""CannotReproduce"".

I think the walking-dead JIRAs we have in mind are some combination
of: a JIRA opened without a lot of detail, that might or might not be
a problem, nobody else seemed to have the problem and/or nobody cared
to investigate, much has changed since anyway so might be obsolete.
WontFix, CannotReproduce, NotAProblem are all possibly reasonable
resolutions. If this is just about semantics, I also don't feel a
strong need for a new state.



Game for whose benefit? nobody is being evaluated on this stuff. This
is being proposed for real reasons, not for fun.

A bunch of JIRA cruft is a symptom, not a cause. Something is wrong
somewhere if people file JIRAs and they go nowhere. Everyone's time is
wasted and with no conclusion, there's no feedback or learning
anywhere. So it keeps happening. Is it bad JIRAs? scope issues? lack
of follow up from developer or contributor? all of the above?

I actually think it's mostly bad JIRAs: too large, too invasive, not
that useful, hacky fixes to a facet of a problem, incomplete
description, duplicate, etc.

I think it's more useful to actually close these to communicate back
clearly what is not going to be accepted. Things can be reopened if
needed. Silently ignoring them forever as an Open JIRA seems less
constructive.



Yes, best to try to make the process better. That's why I started with
things like a more comprehensive
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
to make better contributions in the first place. By the time dead
JIRAs are closed, something's already gone wrong and time has been
wasted. But we still need that culture of not letting stuff sit
around.

I don't mind the idea of an Unresolved ""InformationNeeded"" status,
yeah. I don't actually think that would solve a problem though. The
dead JIRAs are ones that never got any follow up, or, that got a lot
of follow-up from the contributor but aren't going to be merged.

---------------------------------------------------------------------


"
Marco Shaw <marco.shaw@gmail.com>,"Thu, 21 May 2015 17:41:21 -0300",Spark MOOC - early access,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","*Hi Spark Devs and Users,BerkeleyX and Databricks are currently developing
two Spark-related MOOC on edX (intro
<https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x>,
ml
<https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1xthe first of which starts on June 1st.  Together these courses have over
75K enrolled students!To help students perform exercises course content, we
have created a Vagrant box that contains Spark and IPython (running on
Ubuntu 32-bit).  This will simplify user setup and helps us support them.
We are writing to give you early access to the VM environment and the first
assignment, and to request your help to test out the VM/assignment before
we unleash it to 75K people (see instructions below). We‚Äôve provided
instructions below.  We‚Äôre happy to help if you have any difficulties
getting the VM setup; please feel free to contact me (marco.shaw@gmail.com
<marco.shaw@gmail.com>)  with any issues, comments, or
questions.Sincerely,Marco ShawSpark MOOC TA_____________(This is being sent
as an HTML formatted email.  Some of the links have been duplicated just in
case.)1. Install VirtualBox here
<https://www.virtualbox.org/wiki/Downloads> on your OS (see Windows
tutorial here <https://www.youtube.com/watch?v=06Sf-m64fcY>
(https://www.youtube.com/watch?v=06Sf-m64fcY
<https://www.youtube.com/watch?v=06Sf-m64fcY>))2. Install Vagrant here
<https://www.vagrantup.com/downloads.html> on your OS (see Windows tutorial
here <https://www.youtube.com/watch?v=LZVS23BaA1I>
(https://www.youtube.com/watch?v=LZVS23BaA1I
<https://www.youtube.com/watch?v=LZVS23BaA1I>))3) Install virtual machine
using the following steps: (see Windows tutorial here
<https://www.youtube.com/watch?v=ZuJCqHC7IYc>
(https://www.youtube.com/watch?v=ZuJCqHC7IYc
<https://www.youtube.com/watch?v=ZuJCqHC7IYc>))a. Create a custom directory
(e.g. c:\users\marco\myvagrant or /home/marco/myvagrant)b. Download the
file
<https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/Vagrantfileto the custom directory (NOTE: It must be named exactly ""Vagrantfile"" with
no extension)c. Open a DOS prompt (Windows) or terminal (Mac/Linux) to the
custom directory and issue the command ""vagrant up""4) Perform basic
commands in VM as described below: (see Windows tutorial here
<https://www.youtube.com/watch?v=bkteLH77IR0>
(https://www.youtube.com/watch?v=bkteLH77IR0
<https://www.youtube.com/watch?v=bkteLH77IR0>))a. To start the VM, from a
DOS prompt (Windows) or terminal (Mac/Linux), issue the command ""vagrant
up"".b. To stop the VM, from a DOS prompt (Windows) or terminal (Mac/Linux),
issue the command ""vagrant halt"".c. To erase or delete the VM, from a DOS
prompt (Windows) or terminal (Mac/Linux), issue the command ""vagrant
browser to ""http://localhost:8001 <http://localhost:8001/>"".5) Using test
notebook as described below: (see Windows tutorial here
<https://www.youtube.com/watch?v=mlfAmyF3Q-s>
(https://www.youtube.com/watch?v=mlfAmyF3Q-s
<https://www.youtube.com/watch?v=mlfAmyF3Q-s>))a. To start the VM, from a
DOS prompt (Windows) or terminal (Mac/Linux), issue the command ""vagrant
to ""http://localhost:8001 <http://localhost:8001/>"".c. Upload this IPython
notebook:
https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/vm_test_student.ipynb
<https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/vm_test_student.ipynb>.d.
Run through the notebook.6) Play around with the first MOOC assignment
(email Marco for details when you get to this point).7) Please answer the
following questionsa. What machine are you using (OS, RAM, CPU, age)?b. How
long did the entire process take?c. How long did the VM download take?
Relatedly, where are you located?d. Do you have any other
comments/suggestions?*
"
Davies Liu <davies@databricks.com>,"Thu, 21 May 2015 13:59:56 -0700",Re: Tungsten's Vectorized Execution,Yijie Shen <henry.yijieshen@gmail.com>,"We have not start to prototype the vectorized one yet, will evaluated
in 1.5 and may targeted for 1.6.

We're glad to hear some feedback/suggestions/comments from your side!

te:
 me!
rom
ort of
involved if

---------------------------------------------------------------------


"
Santiago Mola <smola@stratio.com>,"Thu, 21 May 2015 23:03:01 +0200",Re: Adding/Using More Resolution Types on JIRA,Sean Owen <sowen@cloudera.com>,"2015-05-21 22:39 GMT+02:00 Sean Owen <sowen@cloudera.com>:

Sure. That is why I was talking about the Inactive resolution specifically.
The
combination of Priority + other statuses are enough to solve these issues. A
minor/trivial issue that is incomplete is probably not going to hurt much to
someone looking for critical open issues.


Sorry. That was unfortunate on my side.


Agreed. That is basically my point.

n
identifying this kind of issue, should I ask in the issue itself to resolve
it in a
specific way?

Best,
-- 

Santiago M. Mola


<http://www.stratio.com/>
V√≠a de las dos Castillas, 33, √Åtica 4, 3¬™ Planta
28224 Pozuelo de Alarc√≥n, Madrid
Tel: +34 91 828 6473 // www.stratio.com // *@stratiobd
<https://twitter.com/StratioBD>*
"
Sean Owen <sowen@cloudera.com>,"Thu, 21 May 2015 22:10:55 +0100",Re: Adding/Using More Resolution Types on JIRA,Santiago Mola <smola@stratio.com>,"
If you mean you intend to consider them resolved, then yeah we agree a
lot. The names of the resolved states don't matter nearly so much to
me. For instance, in your examples, I could call those CannotReproduce
or Incomplete. I would not want to leave them Open in that state
indefinitely.


I think reviewing JIRAs actually contributes to a better overall
process, so I'd just dive in. Anything to advance a JIRA / PR to a
resolution is very helpful. Ask for more info, investigate a problem
to confirm it or fail to reproduce, propose a fix, identify
duplicates, flag JIRAs for closing, review changes, say you think the
change is good, etc. -- among the most helpful things anyone can do
right now IMHO.

---------------------------------------------------------------------


"
Trevor Grant <trevor.d.grant@gmail.com>,"Thu, 21 May 2015 16:11:19 -0500",Re: Contribute code to MLlib,Joseph Bradley <joseph@databricks.com>,"Thank you Ram and Joseph.

I am also hoping to contribute to MLib once my Scala gets up to snuff, this
is the guidance I needed for how to proceed when ready.

Best wishes,
Trevor




"
Kevin Markey <kevin.markey@oracle.com>,"Thu, 21 May 2015 18:21:12 -0600",Re: Change for submitting to yarn in 1.3.1,dev@spark.apache.org,"âPNG

"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 22 May 2015 00:24:32 +0000",RE: Using CUDA within Spark / boosting linear algebra,"Xiangrui Meng <mengxr@gmail.com>, Sean Owen <sowen@cloudera.com>","Hi,

There is a major update on the benchmarks. I've performed them on a newer hardware with 2 CPUs and 3 GPUs. The latter can be used by NVBLAS for parallelizing matrix-matrix multiplication. Results are in the same spreadsheet as previously:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
Previous results are on the separate sheet of the same document. I have also created a github page with benchmark source code and few explanatory comments:
https://github.com/avulanov/scala-blas

I was able to use all 3 GPUs for NVBLAS but BIDMat used only one GPU. 

John, could you suggest how to force BIDMat to use all GPUs? Also, could you suggest how to test Double matrices multiplication in BIDMat-cuda (in GPU and with copy from/to main memory)?

Best regards, Alexander


-----Original Message-----
From: Ulanov, Alexander 
Sent: Wednesday, April 01, 2015 12:11 PM
To: Xiangrui Meng; Sean Owen
Cc: Evan R. Sparks; Sam Halliday; dev@spark.apache.org; jfcanny
Subject: RE: Using CUDA within Spark / boosting linear algebra

FYI, I've added instructions to Netlib-java wiki, Sam added the link to them from the project's readme.md https://github.com/fommil/netlib-java/wiki/NVBLAS

Best regards, Alexander
-----Original Message-----
From: Xiangrui Meng [mailto:mengxr@gmail.com]
Sent: Monday, March 30, 2015 2:43 PM
To: Sean Owen
Cc: Evan R. Sparks; Sam Halliday; dev@spark.apache.org; Ulanov, Alexander; jfcanny
Subject: Re: Using CUDA within Spark / boosting linear algebra

Hi Alex,

Since it is non-trivial to make nvblas work with netlib-java, it would be great if you can send the instructions to netlib-java as part of the README. Hopefully we don't need to modify netlib-java code to use nvblas.

Best,
Xiangrui

On Thu, Mar 26, 2015 at 9:54 AM, Sean Owen <sowen@cloudera.com> wrote:
> The license issue is with libgfortran, rather than OpenBLAS.
>
> (FWIW I am going through the motions to get OpenBLAS set up by default 
> on CDH in the near future, and the hard part is just handling
> libgfortran.)
>
> On Thu, Mar 26, 2015 at 4:07 PM, Evan R. Sparks <evan.sparks@gmail.com> wrote:
>> Alright Sam - you are the expert here. If the GPL issues are 
>> unavoidable, that's fine - what is the exact bit of code that is GPL?
>>
>> The suggestion to use OpenBLAS is not to say it's the best option, 
>> but that it's a *free, reasonable default* for many users - keep in 
>> mind the most common deployment for Spark/MLlib is on 64-bit linux on EC2[1].
>> Additionally, for many of the problems we're targeting, this 
>> reasonable default can provide a 1-2 orders of magnitude improvement 
>> in performance over the f2jblas implementation that netlib-java falls back on.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> additional commands, e-mail: dev-help@spark.apache.org
>
B KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKCB  [  X  ‹öX KK[XZ[
 ] ][  X  ‹öX P \ Àò\X K ‹ô B  ‹àY][€ò[  [X[  K[XZ[
 ] Z[ \ Àò\X K ‹ô B 
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 21 May 2015 17:55:42 -0700",Re: Change for submitting to yarn in 1.3.1,Kevin Markey <kevin.markey@oracle.com>,"Hi Kevin,

I read through your e-mail and I see two main things you're talking about.

- You want a public YARN ""Client"" class and don't really care about
anything else.

In you message you already mention why that's not a good idea. It's much
better to have a standardized submission API. As you noticed by working
with the previous Client API, it's not for the faint of heart; SparkSubmit
hides a lot of the complexity, and does so in a way that is transparent to
the caller. Whether you're submitting a Scala, Python, or R app against
standlone, yarn, mesos, or local, the interface is the same.

You may argue that for your particular use case you don't care about
anything other than Scala apps on YARN cluster mode, but Spark does need to
care about more than that.

I still think that once we have a way to expose more information about the
application being launched (more specifically, the app id), then doing
anything else you may want to do that is specific to YARN is up to you and
pretty easy to do. But I strongly believe that having different ways to
launch apps in Spark is not good design.


- You have some restriction that you app servers cannot fork processes

Honestly, I didn't really understand what that is about. Why can't you fork
processes? Is it a restriction regarding what you can deploy on the server
(e.g. you cannot have a full Spark installation, everything needs to be
contained in a jar that is deployed in the app server)?

I really don't believe this is about the inability to fork a process, so it
must be something else.

The unfortunate reality is that Spark is really not friendly multiple
things being launched from the same JVM. Doing that is prone to apps
running all over each other and overwriting configs and other things, which
then we can study adding a way to launch multiple Spark apps from the same
JVM, but right now that's just asking for (hard to debug) trouble.

It might be possible to add support for launching subprocesses without
having to invoke the shell scripts; that would have limitations (e.g. no
""spark-env.sh"" support). In fact I did something like that in the first
implementation of the launcher library, but was asked to go through the
shell scripts during code review. (I even had a different method that
launched in the same VM, but that one suffered from all the problems
described in the paragraph above.)



e
ng Spark
t or for
*.
es
n
.
te
arkLauncher
ain.
her.launch()
all to *yarn.Client
nd
*starts a *new
e
he
ly
a
*
the mailing list before.
ed from our perspectives for Spark Yarn Client
 we can call it directly.
 PR is already submitted)
application, which will provide the yarn container capacity (number of cores and max memory ), so spark program will not set values beyond max values (PR submitted)
 based on yarn status changes ( start, in progress, failure, complete etc), application can react based on these events in PR)
n program, we had experience problems when we pass a very large argument due the length limit. For example, we use json to serialize the argument and encoded, then parse them as argument. For wide columns datasets, we will run into limit. Therefore, an alternative way of passing additional larger argument is needed. We are experimenting with passing the args via a established akka messaging channel.
batch job with no communication once it launched. Need to establish the communication channel so that logs, errors, status updates, progress bars, execution stages etc can be displayed on the application side. We added an akka communication channel for this (working on PR ).
rint and error statement to application log (outside of the hadoop cluster), so spark UI equivalent progress bar via spark listener. We can show yarn progress via yarn app listener before spark started; and status can be updated during job execution.
 commands and interactions via this channel.
 <kevin.markey@oracle.com>
t
rk
he
pe
e
an
the


-- 
Marcelo
âPNG

"
Pramod Biligiri <pramodbiligiri@gmail.com>,"Thu, 21 May 2015 18:36:07 -0700",Re: Re: Low throughput and effect of GC in SparkSql GROUP BY,zhangxiongfei <zhangxiongfei0815@163.com>,"Hi Zhang,
No my data is not compressed. I'm trying to minimize the load on the CPU.
The GC time reduced for me after codegen.

Pramod


a
m
e
w
 I
or
).
 4
t
 large
in
_6RRMrtzplQ/edit#gid=481134174
loser-to-bare-metal.html
"
Nathan Kronenfeld <nkronenfeld@uncharted.software>,"Thu, 21 May 2015 22:30:51 -0400",Re: Change for submitting to yarn in 1.3.1,Kevin Markey <kevin.markey@oracle.com>,"
Is this new? We've been submitting jobs directly from a programatically
created spark context (instead of through spark-submit) from the beginning
(from 0.7.x to 1.2) - to a local cluster.

In moving to 1.3 on Yarn cluster recently, we've had no end of problems
trying to switch this over (though I think we're almost there).

Why would one want to eliminate this possibility?

     -Nathan
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 21 May 2015 19:34:47 -0700",Re: Change for submitting to yarn in 1.3.1,Nathan Kronenfeld <nkronenfeld@uncharted.software>,"Hi Nathan,


Instantiating SparkContext directly works. Well, sorta: it has limitations.
For example, see discussions about Spark not really liking multiple
contexts in the same JVM. It also does not work in ""cluster"" deploy mode.

-- 
Marcelo
"
Nathan Kronenfeld <nkronenfeld@uncharted.software>,"Thu, 21 May 2015 22:43:48 -0400",Re: Change for submitting to yarn in 1.3.1,Marcelo Vanzin <vanzin@cloudera.com>," Thanks, Marcelo


bit of Caveat Emptor.
"
Nathan Kronenfeld <nkronenfeld@uncharted.software>,"Thu, 21 May 2015 22:46:26 -0400",Testing spark applications,dev <dev@spark.apache.org>,"
Speaking of this - is there a standard way of writing unit tests that
require a SparkContext?

We've ended up copying out the code of SharedSparkContext to our own
testing hierarchy, but it occurs to me someone would have published a test
jar by now if that was the best way.

          -Nathan
"
Koert Kuipers <koert@tresata.com>,"Thu, 21 May 2015 23:04:09 -0400",Re: Change for submitting to yarn in 1.3.1,Nathan Kronenfeld <nkronenfeld@uncharted.software>,"we also launch jobs programmatically, both on standalone mode and
yarn-client mode. in standalone mode it always worked, in yarn-client mode
we ran into some issues and were forced to use spark-submit, but i still
have on my todo list to move back to a normal java launch without
spark-submit at some point.
for me spark is a library that i use to do distributed computations within
my app, and ideally a  library should not tell me how to launch my app. i
mean, if multiple libraries that i use all had their own launch script i
would get stuck very quickly.... hadoop jar vs spark-submit vs kiji launch
vs hbase jar.... bad idea, i think!

however i do understand the practical reasons why spark-submit can about...



"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Fri, 22 May 2015 09:05:10 +0530","Re: Spark Streaming with Tachyon : Data Loss on Receiver Failure due
 to WAL error",Tathagata Das <tdas@databricks.com>,"Hi Tathagata,

Thanks for looking into this. Further investigating I found that the issue
is with Tachyon does not support File Append. The streaming receiver which
writes to WAL when failed, and again restarted, not able to append to same
WAL file after restart.

I raised this with Tachyon user group, and Haoyuan told that within 3
months time Tachyon file append will be ready. Will revisit this issue
again then .

Regards,
Dibyendu



"
Reynold Xin <rxin@databricks.com>,"Thu, 21 May 2015 22:39:29 -0700",Re: Testing spark applications,Nathan Kronenfeld <nkronenfeld@uncharted.software>,"It is just 15 lines of code to copy, isn't it?


"
Josh Rosen <rosenville@gmail.com>,"Thu, 21 May 2015 23:03:17 -0700",Re: Testing spark applications,Reynold Xin <rxin@databricks.com>,"I think that @holdenk's *spark-testing-base* project publishes some of
these test classes as well as some helper classes for testing streaming
jobs: https://github.com/holdenk/spark-testing-base


"
Reynold Xin <rxin@databricks.com>,"Fri, 22 May 2015 00:05:03 -0700",Re: Tungsten's Vectorized Execution,Davies Liu <davies@databricks.com>,"Yijie,

As Davies said, it will take us a while to get to vectorized execution.
However, before that, we are going to refactor code generation to push it
into each expression: https://issues.apache.org/jira/browse/SPARK-7813

of expressions to create code-gen versions, and it'd be great to get as
much help as possible from the community.





to me!
 from
t.
ffort of
t
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Fri, 22 May 2015 14:34:28 +0000",Available Functions in SparkR,"""dev@spark.apache.org"" <dev@spark.apache.org>","Iíve built Spark 1.4.0 for Hadoop 2.6 in a CDH5.4 and am testing SparkR. Iíve loaded up SparkR using the executable in /bin. The library import library(SparkR) seems to no longer import some of the same functions as it did for SparkR before the merge, e.g. textFile, lapply, etc. but it does include sparkR.init, take, and other original functions. How is it planned to access the full set of functions in the repl with the coming version of SparkR?

Thanks,
Alek Eskilson

CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024.
"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Fri, 22 May 2015 20:04:51 +0530",Unable to build from assembly,dev <dev@spark.apache.org>,"Hello,

I updated my master from upstream recently, and on running

build/sbt assembly

it gives me this error

[error]
/home/manoj/spark/examples/src/main/java/org/apache/spark/examples/ml/JavaDeveloperApiExample.java:106:
error: MyJavaLogisticRegression is not abstract and does not override
abstract method setDefault(ParamPair<?>...) in Params
[error] class MyJavaLogisticRegression
[error] ^
[error]
/home/manoj/spark/examples/src/main/java/org/apache/spark/examples/ml/JavaDeveloperApiExample.java:168:
error: MyJavaLogisticRegressionModel is not abstract and does not override
abstract method setDefault(ParamPair<?>...) in Params
[error] class MyJavaLogisticRegressionModel
[error] ^
[error] 2 errors
[error] (examples/compile:compile) javac returned nonzero exit code

It was working fine before this.

Could someone please guide me on what could be wrong?



-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"Fri, 22 May 2015 16:34:34 +0200",Spark Bug: Counting twice with different results,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have recognized a strange behavior of spark core in combination with
mllib. Running my pipeline results in a RDD.
Calling count() on this RDD results in 160055.
Calling count() directly afterwards results in 160044 and so on.
The RDD seems to be unstable.

How can that be? Do you maybe have an explanation or guidance for
further investigation? I'm investigating for 3 days now and can't
isolate the bug.

Unfortunately I can't provide a minimal working example only using
Spark. At the moment I try to reproduce the bug with only using the
Spark API to hand it over to someone more experienced.

I recognized this behavior while investigating SPARK-5480. Trying to
build a graph and calculate the transitive closure on such a unstable
RDD results in a IndexOutOfBoundsException -1.

My first suspicion is that
org.apache.spark.mllib.rdd.RDDFunctions.sliding causes the problems.
Replacing my algorithm which uses the sliding window solves the problem.

fine. That makes it hard to investigate because every run takes several
minutes. Also generated data does not produce the bug.

I didn't open a Jira ticket yet because I can't tell how to reproduce it.

I'm running Spark 1.3.1 in standalone mode with HDFS on a 10 node cluster.

Thanks for your advise,
Niklas


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 22 May 2015 07:43:18 -0700",Re: Unable to build from assembly,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"What version of Java do you use ?

Can you run this command first ?
build/sbt clean

BTW please see [SPARK-7498] [MLLIB] add varargs back to setDefault

Cheers


"
Edoardo Vacchi <uncommonnonsense@gmail.com>,"Fri, 22 May 2015 17:12:21 +0200",Re: Unable to build from assembly,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"confirming. master has been broken in the morning; currently it should
be ok, though


---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Fri, 22 May 2015 15:14:41 +0000",UDTs and StringType upgrade issue for Spark 1.4.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","We ran into an issue regarding Strings in UDTs when upgrading to Spark
1.4.0-rc. I understand that it's a non-public APIs, so it's expected, but I
just wanted to bring it up for awareness and so we can maybe change the
release notes to mention them =)

Our UDT was serializing to a StringType, but now strings are represented
internally as UTF8String, so we had to change our UDT to use
UTF8String.apply() and UTF8String.toString() to convert back to String.
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 22 May 2015 09:27:47 -0700",Re: Change for submitting to yarn in 1.3.1,Kevin Markey <kevin.markey@oracle.com>,"Hi Kevin,

interface for all this...



Since you're not afraid to use private APIs, and to avoid using ugly
reflection hacks, you could abuse the fact that private things in Scala are
not really private most of the time. For example (trimmed to show just
stuff that might be interesting to you):

# javap -classpath
/opt/cloudera/parcels/CDH/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar
org.apache.spark.deploy.yarn.Client
Compiled from ""Client.scala""
public class org.apache.spark.deploy.yarn.Client implements
org.apache.spark.Logging {
  ...
  public org.apache.hadoop.yarn.client.api.YarnClient
org$apache$spark$deploy$yarn$Client$$yarnClient();
  public void run();
  public
org.apache.spark.deploy.yarn.Client(org.apache.spark.deploy.yarn.ClientArguments,
org.apache.hadoop.conf.Configuration, org.apache.spark.SparkConf);
  public
org.apache.spark.deploy.yarn.Client(org.apache.spark.deploy.yarn.ClientArguments,
org.apache.spark.SparkConf);
  public
org.apache.spark.deploy.yarn.Client(org.apache.spark.deploy.yarn.ClientArguments);
}

So it should be easy to write a small Java wrapper around this. No less
hacky than relying on the ""private-but-public"" code of before.

-- 
Marcelo
"
Sean Owen <sowen@cloudera.com>,"Fri, 22 May 2015 18:20:31 +0100",Re: Spark Bug: Counting twice with different results,Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"This is expected for example if your RDD is the result of random
sampling, or if the underlying source is not consistent. You haven't
shown any code.


---------------------------------------------------------------------


"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Sat, 23 May 2015 00:13:10 +0530",Re: Unable to build from assembly,Edoardo Vacchi <uncommonnonsense@gmail.com>,"A clean build worked.

Thanks everyone for the help!





-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Justin Uang <justin.uang@gmail.com>,"Fri, 22 May 2015 19:56:51 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),"Imran Rashid <irashid@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","I'm working on one of the Palantir teams using Spark, and here is our
feedback:

We have encountered three issues when upgrading to spark 1.4.0. I'm not
sure they qualify as a -1, as they come from using non-public APIs and
multiple spark contexts for the purposes of testing, but I do want to bring
them up for awareness =)

   1. Our UDT was serializing to a StringType, but now strings are
   represented internally as UTF8String, so we had to change our UDT to use
   UTF8String.apply() and UTF8String.toString() to convert back to String.
   2. createDataFrame when using UDTs used to accept things in the
   serialized catalyst form. Now, they're supposed to be in the UDT java class
   form (I think this change would've affected us in 1.3.1 already, since we
   were in 1.3.0)
   3. derby database lifecycle management issue with HiveContext. We have
   been using a SparkContextResource JUnit Rule that we wrote, and it sets up
   then tears down a SparkContext and HiveContext between unit test runs
   within the same process (possibly the same thread as well). Multiple
   contexts are not being used at once. It used to work in 1.3.0, but now when
   we try to create the HiveContext for the second unit test, then it
   complains with the following exception. I have a feeling it might have
   something to do with the Hive object being thread local, and us not
   explicitly closing the HiveContext and everything it holds. The full stack
   trace is here: https://gist.github.com/justinuang/0403d49cdeedf91727cd

Caused by: java.sql.SQLException: Failed to start database
'metastore_db' with class loader
org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@5dea2446,
see the next exception for details.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown
Source)



"
Michael Armbrust <michael@databricks.com>,"Fri, 22 May 2015 14:19:33 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Justin Uang <justin.uang@gmail.com>,"Thanks for the feedback.  As you stated UDTs are explicitly not a public
api as we knew we were going to be making breaking changes to them.  We
hope to stabilize / open them up in future releases.  Regarding the Hive
issue, have you tried using TestHive instead.  This is what we use for
testing and it takes care of creating temporary directories for all
storage.  It also has a reset() function that you can call in-between
tests.  If this doesn't work for you, maybe open a JIRA and we can discuss
more there.


"
Andrew Psaltis <psaltis.andrew@gmail.com>,"Fri, 22 May 2015 15:44:14 -0600",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","All,
Should all the docs work from
http://people.apache.org/~pwendell/spark-1.4.0-rc1-docs/ ? If so the R API
docs 404.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 22 May 2015 15:33:10 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Andrew Psaltis <psaltis.andrew@gmail.com>,"Thanks for catching this. I'll check with Patrick to see why the R API docs
are not getting included.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 22 May 2015 17:41:05 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thanks Andrew, the doc issue should be fixed in RC2 (if not, please
chine in!). R was missing in the build envirionment.

- Patrick


---------------------------------------------------------------------


"
jameszhouyi <yiazhou@gmail.com>,"Fri, 22 May 2015 20:41:38 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),dev@spark.apache.org,"We came across a Spark SQL issue
(https://issues.apache.org/jira/browse/SPARK-7119) that cause query to fail.
I not sure that if vote -1 to this RC1.



--

---------------------------------------------------------------------


"
Yijie Shen <henry.yijieshen@gmail.com>,"Sat, 23 May 2015 15:37:48 +0800",Re: Tungsten's Vectorized Execution,"Liu Davies <davies@databricks.com>, Xin Reynold
 <rxin@databricks.com>","Davies and Reynold,

Glad to hear about the status.

I‚Äôve seen [SPARK-7813](https://issues.apache.org/jira/browse/SPARK-7813) and watching it now.

If I understand correctly, it‚Äôs aimed at moving CodeGenerator‚Äôs expressionEvaluator‚Äôs code-gen logic into each expressions‚Äô eval() and eliminating to chose between row evaluation methods in Physical Operators?¬†
What‚Äôs the reason motives this refactoring job? to use code-gen version aggressively in evaluation?




Yijie,

As Davies said, it will take us a while to get to vectorized execution. However, before that, we are going to refactor code generation to push it into each expression: https://issues.apache.org/jira/browse/SPARK-7813

ts of expressions to create code-gen versions, and it'd be great to get as much help as possible from the community.¬†




te:
We have not start to prototype the vectorized one yet, will evaluated
in 1.5 and may targeted for 1.6.

We're glad to hear some feedback/suggestions/comments from your side!

to me!
 from
t.
inner effort of
t involved if

---------------------------------------------------------------------
For additional commands, e-mail: dev-help@spark.apache.org


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 23 May 2015 07:06:17 -0700",Re: [IMPORTANT] Committers please update merge script,Patrick Wendell <pwendell@gmail.com>,"INFRA-9646 has been resolved.

FYI


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 23 May 2015 14:14:25 -0700",Re: [IMPORTANT] Committers please update merge script,Ted Yu <yuzhihong@gmail.com>,"Thanks Ted - there is no need for people to upgrade at this point,
since the changes in the release scripts just modify it not to rely on
default behavior.


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 23 May 2015 18:04:33 -0700",Kryo option changed,dev <dev@spark.apache.org>,"Hi,

I am on last week's master but all the examples that set up the following

.set(""spark.kryoserializer.buffer"", ""8m"")

are failing with the following error:

Exception in thread ""main"" java.lang.IllegalArgumentException:
spark.kryoserializer.buffer must be less than 2048 mb, got: + 8192 mb.
looks like buffer.mb is deprecated...Is ""8m"" is not the right syntax to get
8mb kryo buffer or it shuld be ""8mb""

Thanks.
Deb
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 23 May 2015 18:10:25 -0700",Re: Kryo option changed,Debasish Das <debasish.das83@gmail.com>,"bq. it shuld be ""8mb""

Please use the above syntax.

Cheers


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 23 May 2015 18:16:49 -0700",spark packages,dev <dev@spark.apache.org>,"Hi,

Is it possible to add GPL/LGPL code on spark packages or it must be
licensed under Apache as well ?

I want to expose Professor Tim Davis's LGPL library for sparse algebra and
ECOS GPL library through the package.

Thanks.
Deb
"
Debasish Das <debasish.das83@gmail.com>,"Sat, 23 May 2015 18:24:36 -0700",Re: Kryo option changed,Ted Yu <yuzhihong@gmail.com>,"Tried ""8mb""...still I am failing on the same error...


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 23 May 2015 18:37:51 -0700",Re: Kryo option changed,Debasish Das <debasish.das83@gmail.com>,"Pardon me.

Please use '8192k'

Cheers


"
Josh Rosen <rosenville@gmail.com>,"Sat, 23 May 2015 19:54:53 -0700",Re: Kryo option changed,Ted Yu <yuzhihong@gmail.com>,"Which commit of master are you building off?  It looks like there was a
bugfix for an issue related to KryoSerializer buffer configuration:
https://github.com/apache/spark/pull/5934

That patch was committed two weeks ago, but you mentioned that you're
building off a newer version of master.  Could you confirm the commit that
you're running?  If this used to work but now throws an error, then this is
a regression that should be fixed; we shouldn't require you to perform a mb
-> kb conversion to work around this.


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 23 May 2015 20:09:22 -0700",Power iteration clustering,dev <dev@spark.apache.org>,"Hi,

What was the motivation to write power iteration clustering using graphx
and not a vector matrix multiplication over similarity matrix represented
as say coordinate matrix ?

We can use gemv in that flow to block the computation.

Over graphx can we do all k eigen vector computation together because I
matrix multiply flow is generic for kernel regression or classification
flows.

Thanks.
Deb
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 23 May 2015 20:10:01 -0700",Re: spark packages,Debasish Das <debasish.das83@gmail.com>,"Yes - spark packages can include non ASF licenses.


---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Sat, 23 May 2015 22:12:17 -0700",Re: spark packages,Patrick Wendell <pwendell@gmail.com>,"I thought LGPL is okay but GPL is not okay for Apache project.



-- 
Sent from my iPhone
"
Reynold Xin <rxin@databricks.com>,"Sat, 23 May 2015 22:15:01 -0700",Re: spark packages,DB Tsai <dbtsai@dbtsai.com>,"That's the nice thing about Spark packages. It is just a package index for
libraries and applications built on top of Spark and not part of the Spark
codebase, so it is not restricted to follow only ASF-compatible licenses.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 24 May 2015 00:14:08 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of RC2.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 24 May 2015 00:13:48 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),jameszhouyi <yiazhou@gmail.com>,"Hey jameszhouyi,

Since SPARK-7119 is not a regression from earlier versions, we won't
hold the release for it. However, please comment on the JIRA if it is
affecting you... it will help us prioritize the bug.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 24 May 2015 00:22:21 -0700",[VOTE] Release Apache Spark 1.4.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.0!

The tag to be voted on is v1.4.0-rc2 (commit 03fb26a3):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=03fb26a3e50e00739cc815ba4e2e82d71d003168

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.0]
https://repository.apache.org/content/repositories/orgapachespark-1103/
[published as version: 1.4.0-rc2]
https://repository.apache.org/content/repositories/orgapachespark-1104/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-docs/

Please vote on releasing this package as Apache Spark 1.4.0!

The vote is open until Wednesday, May 27, at 08:12 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What has changed since RC1 ==
Below is a list of bug fixes that went into this RC:
http://s.apache.org/U1M

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.3 workload and running on this release candidate,
then reporting any regressions.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.4 QA period,
so -1 votes should only occur for significant regressions from 1.3.1.
Bugs already present in 1.3.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 24 May 2015 00:41:38 -0700",[ANNOUNCE] Nightly maven and package builds for Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

This week I got around to setting up nightly builds for Spark on
Jenkins. I'd like feedback on these and if it's going well I can merge
the relevant automation scripts into Spark mainline and document it on
the website. Right now I'm doing:

1. SNAPSHOT's of Spark master and release branches published to ASF
Maven snapshot repo:

https://repository.apache.org/content/repositories/snapshots/org/apache/spark/

These are usable by adding this repository in your build and using a
snapshot version (e.g. 1.3.2-SNAPSHOT).

2. Nightly binary package builds and doc builds of master and release versions.

http://people.apache.org/~pwendell/spark-nightly/

These build 4 times per day and are tagged based on commits.

If anyone has feedback on these please let me know.

Thanks!
- Patrick

---------------------------------------------------------------------


"
Tathagata Das <tdas@databricks.com>,"Sun, 24 May 2015 00:44:44 -0700",Re: Spark Streaming - Design considerations/Knobs,Hemant Bhanawat <hemant9379@gmail.com>,"Blocks are replicated immediately, before the driver launches any jobs
using them.


"
Sean Owen <sowen@cloudera.com>,"Sun, 24 May 2015 09:36:54 +0100",Re: spark packages,DB Tsai <dbtsai@dbtsai.com>,"I dont believe we are talking about adding things to the Apache project,
but incidentally LGPL is not OK in Apache projects either.

"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 24 May 2015 06:05:54 -0700",Re: SparkSQL errors in 1.4 rc when using with Hive 0.12 metastore,Cheolsoo Park <piaozhexiu@gmail.com>,"This discussion belongs on the dev list.  Please post any replies there.


"
Debasish Das <debasish.das83@gmail.com>,"Sun, 24 May 2015 07:47:54 -0700",Re: spark packages,Sean Owen <sowen@cloudera.com>,"Yup netlib lgpl right now is activated through a profile...if we can reuse
the same idea then csparse can also be added to spark with a lgpl flag. But
again as Sean said its tricky. Better to keep it on spark packages for
users to try.

"
Sean Owen <sowen@cloudera.com>,"Sun, 24 May 2015 15:56:13 +0100",Re: Kryo option changed,Debasish Das <debasish.das83@gmail.com>,"Wait, isn't the error message just saying you can't set 8mb buffers? So it
is correctly parsing the args. I don't understand why this has to do with
parsing the value. That much works.

"
Sean Owen <sowen@cloudera.com>,"Sun, 24 May 2015 15:59:45 +0100",Re: Kryo option changed,Debasish Das <debasish.das83@gmail.com>,"Ah right I misread this. I get it but I dont think the PR fixes this. Let
me comment there.

"
Debasish Das <debasish.das83@gmail.com>,"Sun, 24 May 2015 08:04:25 -0700",Re: Kryo option changed,Josh Rosen <rosenville@gmail.com>,"I am May 3rd commit:

commit 49549d5a1a867c3ba25f5e4aec351d4102444bc0

Author: WangTaoTheTonic <wangtao111@huawei.com>

Date:   Sun May 3 00:47:47 2015 +0100


    [SPARK-7031] [THRIFTSERVER] let thrift server take SPARK_DAEMON_MEMORY
and SPARK_DAEMON_JAVA_OPTS


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 24 May 2015 08:08:54 -0700",Re: Kryo option changed,Debasish Das <debasish.das83@gmail.com>,"Please update to the following:

commit c2f0821aad3b82dcd327e914c9b297e92526649d
Author: Zhang, Liye <liye.zhang@intel.com>
Date:   Fri May 8 09:10:58 2015 +0100

    [SPARK-7392] [CORE] bugfix: Kryo buffer size cannot be larger than 2M


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 24 May 2015 10:22:14 -0700",Re: Kryo option changed,Debasish Das <debasish.das83@gmail.com>,"The original PR from Liye didn't include test which exercises Kryo buffer
size configured in mb which is below 2GB.

In my PR, I added such a test and it passed on Jenkins:
https://github.com/apache/spark/pull/6390

FYI


"
Krishna Sankar <ksankar42@gmail.com>,"Sun, 24 May 2015 14:11:19 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 16:52 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.3"
"""Cheng, Hao"" <hao.cheng@intel.com>","Mon, 25 May 2015 03:18:17 +0000",RE: SparkSQL errors in 1.4 rc when using with Hive 0.12 metastore,"Mark Hamstra <mark@clearstorydata.com>, Cheolsoo Park
	<piaozhexiu@gmail.com>","Thanks for reporting this.

We intend to support the multiple metastore versions in a single build(hive-0.13.1) by introducing the IsolatedClientLoader, but probably you‚Äôre hitting the bug, please file a jira issue for this.

I will keep investigating on this also.

Hao


From: Mark Hamstra [mailto:mark@clearstorydata.com]
Sent: Sunday, May 24, 2015 9:06 PM
To: Cheolsoo Park
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: SparkSQL errors in 1.4 rc when using with Hive 0.12 metastore

This discussion belongs on the dev list.  Please post any replies there.

On Sat, May 23, 2015 at 10:19 PM, Cheolsoo Park <piaozhexiu@gmail.com<mailto:piaozhexiu@gmail.com>> wrote:
Hi,

I've been testing SparkSQL in 1.4 rc and found two issues. I wanted to confirm whether these are bugs or not before opening a jira.

1) I can no longer compile SparkSQL with -Phive-0.12.0. I noticed that in 1.4, IsolatedClientLoader is introduced, and different versions of Hive metastore jars can be loaded at runtime. But instead, SparkSQL no longer compiles with Hive 0.12.0.

My question is, is this intended? If so, shouldn't the hive-0.12.0 profile in POM be removed?

2) After compiling SparkSQL with -Phive-0.13.1, I ran into my 2nd problem. Since I have Hive 0.12 metastore in production, I have to use it for now. But even if I set ""spark.sql.hive.metastore.version"" and ""spark.sql.hive.metastore.jars"", SparkSQL cli throws an error as follows-

15/05/24 05:03:29 WARN RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect.
org.apache.thrift.TApplicationException: Invalid method name: 'get_functions'
at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:71)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_functions(ThriftHiveMetastore.java:2886)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_functions(ThriftHiveMetastore.java:2872)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getFunctions(HiveMetaStoreClient.java:1727)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
at com.sun.proxy.$Proxy12.getFunctions(Unknown Source)
at org.apache.hadoop.hive.ql.metadata.Hive.getFunctions(Hive.java:2670)
at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionNames(FunctionRegistry.java:674)
at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionNames(FunctionRegistry.java:662)
at org.apache.hadoop.hive.cli.CliDriver.getCommandCompletor(CliDriver.java:540)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:175)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)

What's happening is that when SparkSQL Cli starts up, it tries to fetch permanent udfs from Hive metastore (due to HIVE-6330<https://issues.apache.org/jira/browse/HIVE-6330>, which was introduced in Hive 0.13). But then, it ends up invoking an incompatible thrift function that doesn't exist in Hive 0.12. To work around this error, I have to comment out the following line of code for now-
https://goo.gl/wcfnH1

My question is, is SparkSQL that is compiled against Hive 0.13 supposed to work with Hive 0.12 metastore (by setting ""spark.sql.hive.metastore.version"" and ""spark.sql.hive.metastore.jars"")? It only works if I comment out the above line of code.

Thanks,
Cheolsoo

"
Cheolsoo Park <piaozhexiu@gmail.com>,"Sun, 24 May 2015 20:47:41 -0700",Re: SparkSQL errors in 1.4 rc when using with Hive 0.12 metastore,"""Cheng, Hao"" <hao.cheng@intel.com>","Thank you Hao for the confirmation!

I filed two jiras as follows-
https://issues.apache.org/jira/browse/SPARK-7850 (removing hive-0.12.0
profile from pom)
https://issues.apache.org/jira/browse/SPARK-7851 (thrift error with hive
metastore 0.12)



e
e
08)
functions(ThriftHiveMetastore.java:2886)
ions(ThriftHiveMetastore.java:2872)
aStoreClient.java:1727)
:57)
mpl.java:43)
etaStoreClient.java:89)
Registry.java:674)
Registry.java:662)
40)
IDriver.scala:175)
Driver.scala)
ve
o
It
"
Reynold Xin <rxin@databricks.com>,"Mon, 25 May 2015 00:26:46 -0700",Re: Tungsten's Vectorized Execution,Yijie Shen <henry.yijieshen@gmail.com>,"Yes that's exactly the reason.



K-7813)
Äôs
ô eval() and
?
ersion
:
 to me!
n from
effort of
et
"
Sean Owen <sowen@cloudera.com>,"Mon, 25 May 2015 08:37:22 +0100",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"We still have 1 blocker for 1.4:

SPARK-6784 Make sure values of partitioning columns are correctly
converted based on their data types

CC Davies Liu / Adrian Wang to check on the status of this.

There are still 50 Critical issues tagged for 1.4, and 183 issues
targeted for 1.4 in general. Obviously almost all of those won't be in
1.4. How do people want to deal with those? The field can be cleared,
but do people want to take a pass at bumping a few to 1.4.1 that
really truly are supposed to go into 1.4.1?



---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Mon, 25 May 2015 08:47:26 +0000",RE: [VOTE] Release Apache Spark 1.4.0 (RC2),"Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","Add another Blocker issue, just created! It seems a regression.

https://issues.apache.org/jira/browse/SPARK-7853


-----Original Message-----
From: Sean Owen [mailto:sowen@cloudera.com] 
Sent: Monday, May 25, 2015 3:37 PM
To: Patrick Wendell
Cc: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.4.0 (RC2)

We still have 1 blocker for 1.4:

SPARK-6784 Make sure values of partitioning columns are correctly converted based on their data types

CC Davies Liu / Adrian Wang to check on the status of this.

There are still 50 Critical issues tagged for 1.4, and 183 issues targeted for 1.4 in general. Obviously almost all of those won't be in 1.4. How do people want to deal with those? The field can be cleared, but do people want to take a pass at bumping a few to 1.4.1 that really truly are supposed to go into 1.4.1?


On Sun, May 24, 2015 at 8:22 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.4.0!
>
> The tag to be voted on is v1.4.0-rc2 (commit 03fb26a3):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=03fb26a
> 3e50e00739cc815ba4e2e82d71d003168
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-bin/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> [published as version: 1.4.0]
> https://repository.apache.org/content/repositories/orgapachespark-1103
> /
> [published as version: 1.4.0-rc2]
> https://repository.apache.org/content/repositories/orgapachespark-1104
> /
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-docs
> /
>
> Please vote on releasing this package as Apache Spark 1.4.0!
>
> The vote is open until Wednesday, May 27, at 08:12 UTC and passes if a 
> majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.4.0 [ ] -1 Do not 
> release this package because ...
>
> To learn more about Apache Spark, please see http://spark.apache.org/
>
> == What has changed since RC1 ==
> Below is a list of bug fixes that went into this RC:
> http://s.apache.org/U1M
>
> == How can I help test this release? == If you are a Spark user, you 
> can help us test this release by taking a Spark 1.3 workload and 
> running on this release candidate, then reporting any regressions.
>
> == What justifies a -1 vote for this release? == This vote is 
> happening towards the end of the 1.4 QA period, so -1 votes should 
> only occur for significant regressions from 1.3.1.
> Bugs already present in 1.3.X, minor regressions, or bugs related to 
> new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org

"
"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Mon, 25 May 2015 09:23:15 +0000",RE: [VOTE] Release Apache Spark 1.4.0 (RC2),"""Cheng, Hao"" <hao.cheng@intel.com>, Sean Owen <sowen@cloudera.com>,
	Patrick Wendell <pwendell@gmail.com>","Good catch! BTW, SPARK-6784 is duplicate to SPAKR-7790, didn't notice we changed the title of SPARK-7853..


-----Original Message-----
From: Cheng, Hao [mailto:hao.cheng@intel.com] 
Sent: Monday, May 25, 2015 4:47 PM
To: Sean Owen; Patrick Wendell
Cc: dev@spark.apache.org
Subject: RE: [VOTE] Release Apache Spark 1.4.0 (RC2)

Add another Blocker issue, just created! It seems a regression.

https://issues.apache.org/jira/browse/SPARK-7853
wen@cloudera.com]
Sent: Monday, May 25, 2015 3:37 PM
To: Patrick Wendell
Cc: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.4.0 (RC2)

We still have 1 blocker for 1.4:

SPARK-6784 Make sure values of partitioning columns are correctly converted based on their data types

CC Davies Liu / Adrian Wang to check on the status of this.

There are still 50 Critical issues tagged for 1.4, and 183 issues targeted for 1.4 in general. Obviously almost all of those won't be in 1.4. How do people want to deal with those? The field can be cleared, but do people want to take a pass at bumping a few to 1.4.1 that really truly are supposed to go into 1.4.1?


On Sun, May 24, 2015 at 8:22 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.4.0!
>
> The tag to be voted on is v1.4.0-rc2 (commit 03fb26a3):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=03fb26a
> 3e50e00739cc815ba4e2e82d71d003168
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-bin/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> [published as version: 1.4.0]
> https://repository.apache.org/content/repositories/orgapachespark-1103
> /
> [published as version: 1.4.0-rc2]
> https://repository.apache.org/content/repositories/orgapachespark-1104
> /
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-docs
> /
>
> Please vote on releasing this package as Apache Spark 1.4.0!
>
> The vote is open until Wednesday, May 27, at 08:12 UTC and passes if a 
> majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.4.0 [ ] -1 Do not 
> release this package because ...
>
> To learn more about Apache Spark, please see http://spark.apache.org/
>
> == What has changed since RC1 ==
> Below is a list of bug fixes that went into this RC:
> http://s.apache.org/U1M
>
> == How can I help test this release? == If you are a Spark user, you 
> can help us test this release by taking a Spark 1.3 workload and 
> running on this release candidate, then reporting any regressions.
>
> == What justifies a -1 vote for this release? == This vote is 
> happening towards the end of the 1.4 QA period, so -1 votes should 
> only occur for significant regressions from 1.3.1.
> Bugs already present in 1.3.X, minor regressions, or bugs related to 
> new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org

"
Igor Mazur <igor.kozlov@gmail.com>,"Mon, 25 May 2015 04:26:42 -0700 (MST)",Hive metadata operations support,dev@spark.apache.org,"Hi!

I've found that Spark only supports ExecuteStatementOperation in
SparkSQLOperationManager.

Are there any plans to support others metadata operations? 

Why this question occurs - I'm trying to connect PrestoDB through standard
hive jdbc driver and it doesn't see any tables that registered as
registerTempTable. 
Also through beeline tool I can execute ""SHOW TABLES in default"" and get
those tables, but !tables doesn't show me same.

So, if it was correct from your vision, I would try to implement others
operations.

Igor Mazur



--

---------------------------------------------------------------------


"
Olivier Girardot <ssaboum@gmail.com>,"Mon, 25 May 2015 17:42:48 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),"""Wang, Daoyuan"" <daoyuan.wang@intel.com>, ""Cheng, Hao"" <hao.cheng@intel.com>, 
	Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","I've just tested the new window functions using PySpark in the Spark 1.4.0
rc2 distribution for hadoop 2.4 with and without hive support.
It works well with the hive support enabled distribution and fails as
expected on the other one (with an explicit error :  ""Could not resolve
window function 'lead'. Note that, using window functions currently
requires a HiveContext"").

Thank you for your work.

Regards,

Olivier.

Le lun. 25 mai 2015 √† 11:25, Wang, Daoyuan <daoyuan.wang@intel.com> a
√©crit :

d
n
fb26a
, you
"
Chester Chen <chester@alpinenow.com>,"Mon, 25 May 2015 15:21:36 -0700",Re: Change for submitting to yarn in 1.3.1,"Kevin Markey <kevin.markey@oracle.com>, Andrew Or <andrew@databricks.com>","All,
     I have created a PR just for the purpose of helping document the use
case, requirements and design. As it is unlikely to get merge in. So it
only used to illustrate the problems we trying and solve and approaches we
took.

   https://github.com/apache/spark/pull/6398


    Hope this helps the discussion

Chester







"
Chester Chen <chester@alpinenow.com>,"Mon, 25 May 2015 16:00:04 -0700",Re: Change for submitting to yarn in 1.3.1,"Kevin Markey <kevin.markey@oracle.com>, Andrew Or <andrew@databricks.com>","I put the design requirements and description in the commit comment. So I
will close the PR. please refer the following commit

https://github.com/AlpineNow/spark/commit/5b336bbfe92eabca7f4c20e5d49e51bb3721da4d




"
Andrew Psaltis <psaltis.andrew@gmail.com>,"Mon, 25 May 2015 17:25:17 -0600",SparkR and RDDs,dev@spark.apache.org,"Hi,
I understand from SPARK-6799[1] and the respective merge commit [2]  that
the RDD class is private in Spark 1.4 . If I wanted to modify the old
Kmeans and/or LR examples so that the computation happened in Spark what is
the best direction to go? Sorry if I am missing something obvious, but
based on the NAMESPACE file [3] in the SparkR codebase I am having trouble
seeing the obvious direction to go.

Thanks in advance,
Andrew

[1] https://issues.apache.org/jira/browse/SPARK-6799
[2]
https://github.com/apache/spark/commit/4b91e18d9b7803dbfe1e1cf20b46163d8cb8716c
[3] https://github.com/apache/spark/blob/branch-1.4/R/pkg/NAMESPACE
"
jameszhouyi <yiazhou@gmail.com>,"Mon, 25 May 2015 19:44:25 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),dev@spark.apache.org,"Compiled:
git clone https://github.com/apache/spark.git
git checkout tags/v1.4.0-rc2
./make-distribution.sh --tgz --skip-java-test -Pyarn -Phadoop-2.4
-Dhadoop.version=2.5.0 -Phive -Phive-0.13.1 -Phive-thriftserver -DskipTests

Block issue in RC1/RC2:
https://issues.apache.org/jira/browse/SPARK-7119





--

---------------------------------------------------------------------


"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Tue, 26 May 2015 12:56:23 +0000",Re: SparkR and RDDs,"Andrew Psaltis <psaltis.andrew@gmail.com>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","hods of the RDD API have been made private, which in R means that you may still access them by using the namespace prefix SparkR with three colons, e.g. SparkR:::func(foo, bar).

So a starting place for porting old SparkR scripts from before the merge could be to identify those methods in the script belonging to the RDD class and be sure they have the namespace identifier tacked on the front. I hope that helps.

Regards,
Alek Eskilson

From: Andrew Psaltis <psaltis.andrew@gmail.com<mailto:psaltis.andrew@gmail.com>>
Date: Monday, May 25, 2015 at 6:25 PM
To: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: SparkR and RDDs

Hi,
I understand from SPARK-6799[1] and the respective merge commit [2]  that the RDD class is private in Spark 1.4 . If I wanted to modify the old Kmeans and/or LR examples so that the computation happened in Spark what is the best direction to go? Sorry if I am missing something obvious, but based on the NAMESPACE file [3] in the SparkR codebase I am having trouble seeing the obvious direction to go.

Thanks in advance,
Andrew

[1] https://issues.apache.org/jira/browse/SPARK-6799<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D6799&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=bawjeA3Y9me3xXGxKghL4_dlf7vHdFHtiV5IhMlOmtc&e=>
[2] https://github.com/apache/spark/commit/4b91e18d9b7803dbfe1e1cf20b46163d8cb8716c<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spark_commit_4b91e18d9b7803dbfe1e1cf20b46163d8cb8716c&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=Hc7ijtxcnrZ7wSOStlz0-BHH-rUXSFowCpJuNGYu5eo&e=>
[3] https://github.com/apache/spark/blob/branch-1.4/R/pkg/NAMESPACE<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spark_blob_branch-2D1.4_R_pkg_NAMESPACE&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=l64LUOvbJ53qsVYphkYJ7_kbNptBdEhsSRSWBg5zqn8&e=>

CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024.
"
Andrew Psaltis <psaltis.andrew@gmail.com>,"Tue, 26 May 2015 08:46:45 -0600",Re: SparkR and RDDs,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","Hi Alek,
Thanks for the info. You are correct ,that using the three colons does
work. Admittedly I am a R novice, but since the three colons is used to
access hidden methods, it seems pretty dirty.

Can someone shed light on the design direction being taken with SparkR?
Should I really be accessing hidden methods or will better approach
prevail? For instance, it feels like the k-means sample should really use
MLlib and not just be a port the k-means sample using hidden methods. Am I
looking at this incorrectly?

Thanks,
Andrew


ss
e
is
e
_jira_browse_SPARK-2D6799&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=bawjeA3Y9me3xXGxKghL4_dlf7vHdFHtiV5IhMlOmtc&e=>
b8716c
_spark_commit_4b91e18d9b7803dbfe1e1cf20b46163d8cb8716c&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=Hc7ijtxcnrZ7wSOStlz0-BHH-rUXSFowCpJuNGYu5eo&e=>
_spark_blob_branch-2D1.4_R_pkg_NAMESPACE&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=l64LUOvbJ53qsVYphkYJ7_kbNptBdEhsSRSWBg5zqn8&e=>
,
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 26 May 2015 17:58:22 +0200",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),jameszhouyi <yiazhou@gmail.com>,"I tried 1.4.0-rc2 binaries on a 3-node Mesos cluster, everything seemed to
work fine, both spark-shell and spark-submit. Cluster mode deployment also
worked.

+1 (non-binding)

iulian




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 26 May 2015 11:32:52 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"+1

Tested the SparkR binaries using a Standalone Hadoop 1 cluster, a YARN
Hadoop 2.4 cluster and on my Mac.

Minor thing I noticed is that on Amazon Linux AMI, the R version is 3.1.1
while the binaries seem to have been built with R 3.1.3. This leads to "
Reynold Xin <rxin@databricks.com>,"Tue, 26 May 2015 11:40:00 -0700",Re: SparkR and RDDs,Andrew Psaltis <psaltis.andrew@gmail.com>,"You definitely don't want to implement kmeans in R, since it would be very
slow. Just providing R wrappers for the MLlib implementation is the way to
go. I believe one of the major items in SparkR next is the MLlib wrappers.




I
RDD
. I
t
 is
le
g_jira_browse_SPARK-2D6799&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=bawjeA3Y9me3xXGxKghL4_dlf7vHdFHtiV5IhMlOmtc&e=>
cb8716c
e_spark_commit_4b91e18d9b7803dbfe1e1cf20b46163d8cb8716c&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=Hc7ijtxcnrZ7wSOStlz0-BHH-rUXSFowCpJuNGYu5eo&e=>
e_spark_blob_branch-2D1.4_R_pkg_NAMESPACE&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=l64LUOvbJ53qsVYphkYJ7_kbNptBdEhsSRSWBg5zqn8&e=>
n,
f
y
"
Justin Uang <justin.uang@gmail.com>,"Tue, 26 May 2015 18:57:30 +0000",Spark 1.4.0 pyspark and pylint breaking,"""dev@spark.apache.org"" <dev@spark.apache.org>","In commit 04e44b37, the migration to Python 3, pyspark/sql/types.py was
renamed to pyspark/sql/_types.py and then some magic in
pyspark/sql/__init__.py dynamically renamed the module back to types. I
imagine that this is some naming conflict with Python 3, but what was the
error that showed up?

The reason why I'm asking about this is because it's messing with pylint,
since pylint cannot now statically find the module. I tried also importing
the package so that __init__ would be run in a init-hook, but that isn't
what the discovery mechanism is using. I imagine it's probably just
crawling the directory structure.

http://stackoverflow.com/questions/9602811/how-to-tell-pylint-to-ignore-certain-imports),
where I would have to create a fake module, but I would probably be missing
a ton of pylint features on users of that module, and it's pretty hacky.
"
Davies Liu <davies@databricks.com>,"Tue, 26 May 2015 12:03:14 -0700",Re: Spark 1.4.0 pyspark and pylint breaking,Justin Uang <justin.uang@gmail.com>,"There is a module called 'types' in python 3:

davies@localhost:~/work/spark$ python3
Python 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 00:54:21)
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
<module 'types' from
'/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/types.py'>

Without renaming, our `types.py` will conflict with it when you run
unittests in pyspark/sql/ .


---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Tue, 26 May 2015 19:08:31 +0000",Re: Spark 1.4.0 pyspark and pylint breaking,Davies Liu <davies@databricks.com>,"Thanks for clarifying! I don't understand python package and modules names
that well, but I thought that the package namespacing would've helped,
since you are in pyspark.sql.types. I guess not?


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Tue, 26 May 2015 19:44:38 +0000",Re: Spark 1.4.0 pyspark and pylint breaking,"Justin Uang <justin.uang@gmail.com>, Davies Liu <davies@databricks.com>","Davies: Can we use relative imports (import .types) in the unit tests in
order to disambiguate between the global and local module?

Punya


"
Davies Liu <davies@databricks.com>,"Tue, 26 May 2015 13:16:11 -0700",Re: Spark 1.4.0 pyspark and pylint breaking,Justin Uang <justin.uang@gmail.com>,"When you run the test in python/pyspark/sql/ by

bin/spark-submit python/pyspark/sql/dataframe.py

the the current directory is the first item in sys.path, sql/types.py
will have higher priority then python3.4/types.py, the tests will
fail.


---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Tue, 26 May 2015 13:18:27 -0700",Re: Spark 1.4.0 pyspark and pylint breaking,Punyashloka Biswal <punya.biswal@gmail.com>,"I think relative imports can not help in this case.

When you run scripts in pyspark/sql, it doesn't know anything about
pyspark.sql, it
just see types.py as a separate module.


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Tue, 26 May 2015 14:27:52 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"-1

Found a new blocker SPARK-7864
<https://issues.apache.org/jira/browse/SPARK-7864> that is being resolved
by https://github.com/apache/spark/pull/6419.

2015-05-26 11:32 GMT-07:00 Shivaram Venkataraman <shivaram@eecs.berkeley.edu

fe.com
Apache-Spark-1"
Ben Mabey <ben@benmabey.com>,"Tue, 26 May 2015 16:36:24 -0600",GraphX implementation of ALS?,dev@spark.apache.org,"Hi all,
I've heard in a number of presentations Spark's ALS implementation was 
going to be moved over to a GraphX version. For example, this 
presentation on GraphX 
<https://databricks-training.s3.amazonaws.com/slides/graphx@sparksummit_2014-07.pdf>(slide 
#23) at the Spark Summit mentioned a 40 LOC version using the Pregel 
API. Looking at the ALS source on master 
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala> 
it looks like the original implementation is still being used and no use 
of GraphX can be seen. Other algorithms mentioned in the GraphX 
presentation can be found in the repo 
<https://github.com/apache/spark/tree/master/graphx/src/main/scala/org/apache/spark/graphx/lib> 
already but I don't see ALS. Could someone link me to the GraphX version 
for comparison purposes?  Also, could someone comment on why the the 
newer version isn't in use yet (i.e. are there tradeoffs with using the 
GraphX version that makes it less desirable)?

Thanks,
Ben

"
Ankur Dave <ankurdave@gmail.com>,"Tue, 26 May 2015 16:45:50 -0700",Re: GraphX implementation of ALS?,Ben Mabey <ben@benmabey.com>,"This is the latest GraphX-based ALS implementation that I'm aware of:
https://github.com/ankurdave/spark/blob/GraphXALS/graphx/src/main/scala/org/apache/spark/graphx/lib/ALS.scala

When I benchmarked it last year, it was about twice as slow as MLlib's ALS,
and I think the latter has gotten faster since then. The performance gap is
because the MLlib version implements some ALS-specific optimizations that
are hard to do using GraphX, such as storing the edges twice (partitioned
by source and by destination) to reduce communication.

Ankur <http://www.ankurdave.com/>


"
Joseph Bradley <joseph@databricks.com>,"Tue, 26 May 2015 17:53:15 -0700",Re: Power iteration clustering,Debasish Das <debasish.das83@gmail.com>,"That's a good question; I could imagine it being much more efficient if
kept in a BlockMatrix and using BLAS2 ops.


"
Ben Mabey <ben@benmabey.com>,"Tue, 26 May 2015 20:56:13 -0600",Re: GraphX implementation of ALS?,Ankur Dave <ankurdave@gmail.com>,"
Great, thanks for the link and explanation!
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 26 May 2015 20:07:01 -0700",Re: Power iteration clustering,Joseph Bradley <joseph@databricks.com>,"Ok I thought we tried that and found graphx based flow was faster due to
some inherent problem structure (graphx can compute K eigenvectors at the
same time)

I will report some stats on row similarities experiments on vector blocked
index row matrix multiply vs current pic flow...

"
Debasish Das <debasish.das83@gmail.com>,"Tue, 26 May 2015 20:24:05 -0700",Re: GraphX implementation of ALS?,Ankur Dave <ankurdave@gmail.com>,"In general for implicit feedback in als you have to do a blocked gram
matrix calculation which might not fit in graphx flow and lot of blocked
operations can be used...but if your loss is likelihood or kl divergence or
just simple sgd update rules and not least square then graphx idea makes
sense...

Lda flow uses similar idea as the loss function is defined on sparse
ratings...

"
Nitin Goyal <nitin2goyal@gmail.com>,"Wed, 27 May 2015 10:38:20 -0700 (MST)",ClosureCleaner slowing down Spark SQL queries,dev@spark.apache.org,"Hi All,

I am running a SQL query (spark version 1.2) on a table created from
unionAll of 3 schema RDDs which gets executed in roughly 400ms (200ms at
driver and roughly 200ms at executors).

If I run same query on a table created from unionAll of 27 schema RDDS, I
see that executors time is same(because of concurrency and nature of my
query) but driver time shoots to 600ms (and total query time being = 600 +
200 = 800ms).

I attached JProfiler and found that ClosureCleaner clean method is taking
time at driver(some issue related to URLClassLoader) and it linearly
increases with number of RDDs being union-ed on which query is getting
fired. This is causing my query to take a huge amount of time where I expect
the query to be executed within 400ms irrespective of number of RDDs (since
I have executors available to cater my need). PFB the links of screenshots
from Jprofiler :-

http://pasteboard.co/MnQtB4o.png

http://pasteboard.co/MnrzHwJ.png

Any help/suggestion to fix this will be highly appreciated since this needs
to be fixed for production

Thanks in Advance,
Nitin



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 27 May 2015 10:50:41 -0700",Re: ClosureCleaner slowing down Spark SQL queries,Nitin Goyal <nitin2goyal@gmail.com>,"Can you try your query using Spark 1.4.0 RC2 ?

There have been some fixes since 1.2.0
e.g.
SPARK-7233 ClosureCleaner#clean blocks concurrent job submitter threads

Cheers


"
Nitin Goyal <nitin2goyal@gmail.com>,"Wed, 27 May 2015 11:12:24 -0700 (MST)",Re: ClosureCleaner slowing down Spark SQL queries,dev@spark.apache.org,"Hi Ted,

Thanks a lot for replying. First of all, moving to 1.4.0 RC2 is not easy for
us as migration cost is big since lot has changed in Spark SQL since 1.2.

Regarding SPARK-7233, I had already looked at it few hours back and it
solves the problem for concurrent queries but my problem is just for a
single query. I also looked at the fix's code diff and it wasn't related to
the problem which seems to exist in Closure Cleaner code.

Thanks
-Nitin



--

---------------------------------------------------------------------


"
Michael Nazario <mnazario@palantir.com>,"Wed, 27 May 2015 20:41:54 +0000",RE: Spark 1.4.0 pyspark and pylint breaking,"Davies Liu <davies@databricks.com>, Punyashloka Biswal
	<punya.biswal@gmail.com>","I've done some investigation into what work needed to be done to keep the _types module named types. This isn't a relative / absolute path problem, but actually a problem with the way the tests were run.

I've filed a jira ticket on it here: https://issues.apache.org/jira/browse/SPARK-7899)

I also have a pull request for fixing this here: https://github.com/apache/spark/pull/6439

Michael
________________________________________
From: Davies Liu [davies@databricks.com]
Sent: Tuesday, May 26, 2015 4:18 PM
To: Punyashloka Biswal
Cc: Justin Uang; dev@spark.apache.org
Subject: Re: Spark 1.4.0 pyspark and pylint breaking

I think relative imports can not help in this case.

When you run scripts in pyspark/sql, it doesn't know anything about
pyspark.sql, it
just see types.py as a separate module.


:
es
ince
:

py'>

as
 I


com_questions_9602811_how-2Dto-2Dtell-2Dpylint-2Dto-2Dignore-2Dcertain-2DimJskMkGMKoYoLUUIQViRLGShPc1wislP1YdU4g&m=8-Bnuaq-HaKXNQYsouzyQuyrj1GH9MbO6JQWXBMqa_Q&s=uireqIdh4TOSVaj4QM0tNIbPIWKQ_sFQE-M32_3Q-ek&e= ),


---------------------------------------------------------------------


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 27 May 2015 14:11:14 -0700",[build system] jenkins downtime tomorrow morning ~730am PDT,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i'm going to be performing system, jenkins, and plugin updates tomorrow
morning beginning at 730am PDT.

0700:  pause build queue
0800:  kill off any errant jobs (retrigger when everything comes back up)
0800-0900:  system and plugin updates
0900-1000:  final debugging, roll back versions of plugins if thing get
borked

i'll post updates as things progress, and will be hoping to have full
service restored by 10am

shane
"
jameszhouyi <yiazhou@gmail.com>,"Wed, 27 May 2015 17:22:39 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),dev@spark.apache.org,"-1 , SPARK-7119 blocker issue



--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 27 May 2015 18:25:03 -0700",Re: SparkR and RDDs,Reynold Xin <rxin@databricks.com>,"Sorry for the delay in getting back on this. So the RDD interface is
private in the 1.4 release but as Alek mentioned you can still use it by
prefixing `SparkR:::`.

Regarding design direction -- there are two JIRAs which cover major
features we plan to work on for 1.5. SPARK-6805 tracks porting high-level
machine learning operations like `glm` and `kmeans` to SparkR using the ML
Pipeline implementation in Scala as the backend.

We are also planning to develop a parallel API where users can run native R
functions in a distributed setting and SPARK-7264 tracks this effort. If
you have specific use cases feel free to chime in on the JIRA or on the dev
mailing list.

Thanks
Shivaram


y
o
.
e
 I
t
ee
 RDD
t. I
 old
t is
ble
rg_jira_browse_SPARK-2D6799&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=bawjeA3Y9me3xXGxKghL4_dlf7vHdFHtiV5IhMlOmtc&e=>
8cb8716c
he_spark_commit_4b91e18d9b7803dbfe1e1cf20b46163d8cb8716c&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=Hc7ijtxcnrZ7wSOStlz0-BHH-rUXSFowCpJuNGYu5eo&e=>
he_spark_blob_branch-2D1.4_R_pkg_NAMESPACE&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=l64LUOvbJ53qsVYphkYJ7_kbNptBdEhsSRSWBg5zqn8&e=>
e
on,
If
fy
.
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 27 May 2015 18:43:16 -0700",Re: Available Functions in SparkR,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","For the 1.4 release the DataFrame API will be publicly available and the
documentation at
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-docs/sql-programming-guide.html
(Click on the R tab) provides a good summary of the available functions.

As I described in the other email to the dev list, we are still collecting
feedback on a parallel API for SparkR as we feel the RDD API is too
low-level. We would like to hear any use-cases you have as it will be
valuable in designing the API.

Thanks
Shivaram


 SparkR.
mport
t
d
f
m
on
ot
of
n
"
Andrew Psaltis <psaltis.andrew@gmail.com>,"Wed, 27 May 2015 21:44:25 -0600",Re: SparkR and RDDs,shivaram@eecs.berkeley.edu,"Hi Shivaram,
Thanks for the details, it is greatly appreciated.

Thanks


L
f
ev
:
m
se
m I
at
ree
e RDD
nt. I
e old
at is
uble
org_jira_browse_SPARK-2D6799&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=bawjeA3Y9me3xXGxKghL4_dlf7vHdFHtiV5IhMlOmtc&e=>
d8cb8716c
che_spark_commit_4b91e18d9b7803dbfe1e1cf20b46163d8cb8716c&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=Hc7ijtxcnrZ7wSOStlz0-BHH-rUXSFowCpJuNGYu5eo&e=>
che_spark_blob_branch-2D1.4_R_pkg_NAMESPACE&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=T9sfWUgCtxLUJ9F4B-MAmBhrH4e3aGvb_hbrENoIKho&s=l64LUOvbJ53qsVYphkYJ7_kbNptBdEhsSRSWBg5zqn8&e=>
he
te
e
ion,
 If
ify
4
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 27 May 2015 22:40:33 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC2),jameszhouyi <yiazhou@gmail.com>,"Hi James,

As I said before that is not a blocker issue for this release, thanks.
Separately, there are some comments in this code review that indicate
you may be facing a bug in your own code rather than with Spark:

https://github.com/apache/spark/pull/5688#issuecomment-104491410

Please follow up on that issue outside of the vote thread.

Thanks!


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 28 May 2015 00:29:52 -0700",Re: Representing a recursive data type in Spark SQL,Jeremy Lucas <jeremyalucas@gmail.com>,"I think it is fairly hard to support recursive data types. What I've seen
in one other proprietary system in the past is to let the user define the
depth of the nested data types, and then just expand the struct/map/list
definition to the maximum level of depth.

Would this solve your problem?





"
Tathagata Das <tdas@databricks.com>,"Thu, 28 May 2015 00:36:14 -0700",Re: problem for mark stage finished?,=?UTF-8?B?6YKT5YiaW+aKgOacr+S4reW/g10=?= <triones.deng@vipshop.com>,"Off the top of my head, I am not sure if this is a known problem. Could try
running the latest Spark (1.3.1 or even better 1.4.0 RC2 that is being
voted) and test whether the problem persists?

TD


∏≠ÂøÉ]
• 15:05
ng time later
t
-dispatcher-44]:
-dispatcher-44]:
-dispatcher-44]:
:
or.default-dispatcher-58]:
-dispatcher-44]:
:
or.default-dispatcher-58]:
-dispatcher-44]:
lt-dispatcher-49]:
or.default-dispatcher-15]:
or.default-dispatcher-15]:
spatcher-37]:
lt-dispatcher-49]:
lt-dispatcher-49]:
ispatcher-69]:
cher-37]:
ispatcher-61]:
er-37]:
ispatcher-51]:
cher-37]:
ispatcher-69]:
er-37]:
ispatcher-71]:
cher-37]:
ispatcher-49]:
er-37]:
ispatcher-36]:
er-37]:
ispatcher-65]:
cher-37]:
ispatcher-36]:
cher-37]:
ispatcher-52]:
atcher-37]:
ispatcher-63]:
r-37]:
ispatcher-36]:
r-37]:
ispatcher-33]:
r-37]:
-dispatcher-44]:
-dispatcher-44]:
-dispatcher-44]:
∏≠ÂøÉ]
• 10:32
ng time later
t
-dispatcher-44]:
‰øùÂØÜÊñá‰ª∂„ÄÇÂ¶ÇÊûúÈòÅ‰∏ãÈùûÁîµÂ≠êÈÇÆ‰ª∂ÊâÄÊåáÂÆö‰πãÊî∂‰ª∂‰∫∫ÔºåË∞®ËØ∑Á´ãÂç≥ÈÄöÁü•Êú¨‰∫∫„ÄÇÊï¨ËØ∑ÈòÅ‰∏ã‰∏çË¶Å‰ΩøÁî®„ÄÅ‰øùÂ≠ò„ÄÅÂ§çÂç∞„ÄÅÊâìÂç∞„ÄÅÊï£Â∏ÉÊú¨ÁîµÂ≠êÈÇÆ‰ª∂ÂèäÂÖ∂ÂÜÖÂÆπÔºåÊàñÂ∞ÜÂÖ∂Áî®‰∫éÂÖ∂‰ªñ‰ªª‰ΩïÁõÆÁöÑÊàñÂêë‰ªª‰Ωï‰∫∫Êä´Èú≤„ÄÇË∞¢Ë∞¢ÊÇ®ÁöÑÂêà‰ΩúÔºÅ
"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 May 2015 07:35:24 -0700",Re: [build system] jenkins downtime tomorrow morning ~730am PDT,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","well, i started early and am pretty much done.  sadly, i had to roll back
most of the plugin updates (which doesn't surprise me), but the system and
jenkins core updates went swimmingly.

anyways, we're up and building again!

now, back to my coffee...  :)


"
Debasish Das <debasish.das83@gmail.com>,"Thu, 28 May 2015 08:13:25 -0700",Streaming data + Blocked Model,dev <dev@spark.apache.org>,"Hi,

We want to keep the model created and loaded in memory through Spark batch
context since blocked matrix operations are required to optimize on runtime.

The data is streamed in through Kafka / raw sockets and Spark Streaming
Context. We want to run some prediction operations with the streaming data
and model loaded in memory through batch context.

Do I need to open up a API on top of the batch context or it is possible to
use a RDD created by batch context through streaming context ?

Most likely not since both streaming context and batch context can't exist
in the same spark job but I am curious.

If I have to open up an API, does it makes sense to come up with a generic
serving api for mllib and let all mllib algorithms expose a serving API ?
The API can be spawned using Spark's actor system itself specially since
spray is merging to akka-httpx and akka is a dependency in spark already.

May be it's not a good idea since it needs maintaining another actor system
for the API.

Thanks.
Deb
"
Peter Rudenko <petro.rudenko@gmail.com>,"Thu, 28 May 2015 21:28:42 +0300",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),"Justin Uang <justin.uang@gmail.com>, Imran Rashid <irashid@cloudera.com>,
 Patrick Wendell <pwendell@gmail.com>","Also have the same issue - all tests fail because of HiveContext / derby 
lock.

|Cause: javax.jdo.JDOFatalDataStoreException: Unable to open a test 
connection to the given database. JDBC url = 
jdbc:derby:;databaseName=metastore_db;create=true, username = APP. 
Terminating connection pool (set lazyInit to true if you expect to start 
your database after your app). Original Exception: ------ [info] 
java.sql.SQLException: Failed to start database 'metastore_db' with 
class loader 
org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@8066e0e, 
see the next exception for details. |

Also is there build for hadoop2.6? Don‚Äôt see it here: 
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-bin/ 
<http://people.apache.org/%7Epwendell/spark-releases/spark-1.4.0-rc2-bin/>

Thanks,
Peter Rudenko


‚Äã
"
Imran Rashid <irashid@cloudera.com>,"Thu, 28 May 2015 13:55:04 -0500",flaky tests & scaled timeouts,dev <dev@spark.apache.org>,"Hi,

I was just fixing a problem with too short a timeout on one of the unit
tests I added (https://issues.apache.org/jira/browse/SPARK-7919), and I was
wondering if this is a common problem w/ a lot of our flaky tests.

Its really hard to know what to set the timeouts to -- you set the timeout
so it runs comfortably on your dev machine, but then discover its far too
short for the build machines.  so up it some more, and then it turns out
there are some even slower build machines, and you need to make it longer
still ...

Scalatest actually has a feature to handle this.  You can wrap all timeouts
in scaled(...), and then you can pass in a flag with how much the timeouts
should get scaled by.

http://doc.scalatest.org/2.0/index.html#org.scalatest.concurrent.ScaledTimeSpans

eg., I was looking at DriverSuite -- its been turned off because it was too
flaky, though the 60 second timeout already seems super long. (
https://issues.apache.org/jira/browse/SPARK-7415)

does anybody know if timeouts are a common source of flaky tests?  Is it
worth trying to change existing timeouts in tests to use scaled() so keep
tests passing on slow build machines?  Does it make sense for all future
tests to always use scaled() on timeouts?

thanks,
Imran
"
Reynold Xin <rxin@databricks.com>,"Thu, 28 May 2015 11:57:47 -0700",Re: flaky tests & scaled timeouts,Imran Rashid <irashid@cloudera.com>,"My understanding is that it is good to have this, but a large fraction of
flaky tests are actually also race conditions, etc.



"
Jeremy Lucas <jeremyalucas@gmail.com>,"Thu, 28 May 2015 19:48:01 +0000",Re: Representing a recursive data type in Spark SQL,Reynold Xin <rxin@databricks.com>,"Hey Reynold,

Thanks for the suggestion. Maybe a better definition of what I mean by a
""recursive"" data structure is rather what might resemble (in Scala) the
type Map[String, Any]. With a type like this, the keys are well-defined as
strings (as this is JSON) but the values can be basically any arbitrary
value, including another Map[String, Any].

For example, in the below ""stream"" of JSON records:

{
  ""timestamp"": ""2015-01-01T00:00:00Z"",
  ""data"": {
    ""event"": ""click"",
    ""url"": ""http://mywebsite.com""
  }
}
...
{
  ""timestamp"": ""2015-01-01T08:00:00Z"",
  ""data"": {
    ""event"": ""purchase"",
    ""sku"": ""123456789"",
    ""quantity"": 1,
    ""params"": {
      ""arbitrary-param-1"": ""blah"",
      ""arbitrary-param-2"": 123456
  }
}

I am trying to figure out a way to run SparkSQL over the above JSON
records. My inclination would be to define the ""timestamp"" field as a
well-defined DateType, but the ""data"" field is way more free-form.

Also, any pointers on where to look for how data types are evaluated and
serialized/deserialized would be super helpful as well.

Thanks




"
Guozhang Wang <wangguoz@gmail.com>,"Thu, 28 May 2015 14:01:14 -0700",Question about avoiding reflection with isolated class loader,dev@spark.apache.org,"Hi,

I have a question that is probably related to SPARK-1870
<https://issues.apache.org/jira/browse/SPARK-1870>. Basically I have also
encountered the issue that with separate classloaders while developing a
programming framework where I have to use reflection inside the application
code.

To simply the question, let's say my framework code depends on a jar with
an older version and the application code depends on the same jar with a
newer version. And let's say that jar has a class named CommonClass. I used
a customized post-delegation class loader so that these two version can
exist separately into separate class loader. And if I do sth. like:

    public static void main(String arg[]) throws Exception {
        // Say this is framework code
        new CommonClass().printJarVersion();

        // Set the customized class loader as thread context class loader
        Thread thread = Thread.currentThread();
        ClassLoader oldClassLoader = thread.getContextClassLoader();
        File appJar = new File(""/dir/to/app/class/path"");
        URL[] classpath = new URL[] { appJar.toURI().toURL() };
        PostDelegationClassLoader newClassLoader = new
PostDelegationClassLoader(classpath);
        thread.setContextClassLoader(newClassLoader);

        try {
            // Say this is application code, like process().
            new CommonClass().printJarVersion();

thread.getContextClassLoader().loadClass(""CommonClass"").newInstance().printJarVersion();
        } finally {
            thread.setContextClassLoader(oldClassLoader);
        }
    }

-----------------------

It will print:

CommonClass: version 1
CommonClass: version 1
CommonClass: version 2

As one can see I have to use reflection to explicitly specify the
customized class loader if I want to create the class with the new version.
This is definitely bad for the users.

I saw there are some discussions around this issue on SPARK-1870, but not
so sure what is the final solution to this. Anyone could help me out?

-- Guozhang
"
Pala M Muthaia <mchettiar@rocketfuelinc.com>,"Thu, 28 May 2015 16:59:36 -0700",Absence of version 1.3 of spark-assembly jar,dev <dev@spark.apache.org>,"I am looking to take dependency on spark-assembly jar, version 1.3.0, for
our spark code unit tests, but it's not available on maven central (only
older versions are available). Looks like it's not getting released
anymore, is that right?

Our internal build system prevents us from including dependency jars with
overlap, and an uber jar with all dependencies was convenient.

What are my options here? Are there plans to reintroduce release of
spark-assembly?


Thanks,
pala
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 28 May 2015 22:02:17 -0400",Re: Representing a recursive data type in Spark SQL,Jeremy Lucas <jeremyalucas@gmail.com>,"Your best bet might be to use a map<string,string> in SQL and make the keys be longer paths (e.g. params_param1 and params_param2). I don't think you can have a map in some of them but not in others.

Matei

a ""recursive"" data structure is rather what might resemble (in Scala) the type Map[String, Any]. With a type like this, the keys are well-defined as strings (as this is JSON) but the values can be basically any arbitrary value, including another Map[String, Any].
records. My inclination would be to define the ""timestamp"" field as a well-defined DateType, but the ""data"" field is way more free-form.
and serialized/deserialized would be super helpful as well.
seen in one other proprietary system in the past is to let the user define the depth of the nested data types, and then just expand the struct/map/list definition to the maximum level of depth.
allow for a recursive type.
type. Is this what you were looking for?
Row(x.toString)))
to large JSON logs and being able to write plain SQL to perform a wide variety of operations over this data. However, one small thing that keeps coming back to haunt me is the lack of support for recursive data types, whereby a member of a complex/struct value can be of the same type as the complex/struct value itself.
where to start to build out such capabilities, as I'd be happy to contribute, but am very new to this particular component of the Spark project.

"
Yin Huai <yhuai@databricks.com>,"Thu, 28 May 2015 21:08:28 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Peter Rudenko <petro.rudenko@gmail.com>,"Justin,

If you are creating multiple HiveContexts in tests, you need to assign a
temporary metastore location for every HiveContext (like what we do at here
<https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L527-L543>).
Otherwise, they all try to connect to the metastore in the current dir
(look at metastore_db).

Peter,

Do you also have the same use case as Justin (creating multiple
HiveContexts in tests)? Can you explain what you meant by ""all tests""? I am
probably missing some context at here.

Thanks,

Yin



tion to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
h class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@8066e0e, see the next exception for details.
ng
se
.
class
 we
s up
 when
tack
' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$anon$1@5dea2446, see the next exception for details.
n Source)
n
7a08166f1fb144146ba32581d4632c3466541e
"
Reynold Xin <rxin@databricks.com>,"Thu, 28 May 2015 23:21:30 -0700",looking for contributors to the build task: use errorprone,"""dev@spark.apache.org"" <dev@spark.apache.org>","If somebody has some free cycles, we'd greatly appreciate some
investigation & a patch to integrate Google's errorprone with Spark's Maven
build.

https://issues.apache.org/jira/browse/SPARK-7938
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 29 May 2015 09:23:02 +0000",Dataframe's .drop in PySpark doesn't accept Column,dev <dev@spark.apache.org>,"Hi,
Testing a bit more 1.4, it seems that the .drop() method in PySpark doesn't
seem to accept a Column as input datatype :


*    .join(only_the_best, only_the_best.pol_no == df.pol_no,
""inner"").drop(only_the_best.pol_no)\* File
""/usr/local/lib/python2.7/site-packages/pyspark/sql/dataframe.py"", line
1225, in drop
jdf = self._jdf.drop(colName)
File ""/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line
523, in __call__
(new_args, temp_args) = self._get_args(args)
File ""/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line
510, in _get_args
temp_arg = converter.convert(arg, self.gateway_client)
File ""/usr/local/lib/python2.7/site-packages/py4j/java_collections.py"",
line 490, in convert
for key in object.keys():
TypeError: 'Column' object is not callable

It doesn't seem very consistent with rest of the APIs - and is especially
annoying when executing joins - because drop(""my_key"") is not a qualified
reference to the column.

What do you think about changing that ? or what is the best practice as a
workaround ?

Regards,

Olivier.
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 29 May 2015 09:45:16 +0000",Re: Dataframe's .drop in PySpark doesn't accept Column,"Olivier Girardot <o.girardot@lateral-thoughts.com>, dev <dev@spark.apache.org>","Actually, the Scala API too is only based on column name

Le ven. 29 mai 2015 √† 11:23, Olivier Girardot <
o.girardot@lateral-thoughts.com> a √©crit :

"
Peter Rudenko <petro.rudenko@gmail.com>,"Fri, 29 May 2015 18:51:59 +0300",Re: [VOTE] Release Apache Spark 1.4.0 (RC1),Yin Huai <yhuai@databricks.com>,"Hi Yin, i‚Äôm using spark-hive dependency and tests for my app work for 
spark1.3.1.
seems it‚Äôs something with hive & sbt. Running from spark-shell next 
statement works, but from sbt console in rc3 i get next error:


scala> val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
15/05/29 16:31:06 WARN ObjectStore: Version information not found in 
metastore. hive.metastore.schema.verification is not enabled so 
recording the schema version 0.13.1aa
sqlContext: org.apache.spark.sql.hive.HiveContext = 
org.apache.spark.sql.hive.HiveContext@177ac9f4

scala> val data = sqlContext.read.parquet(""caches/-1525448137"")
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for 
further details.
java.lang.IllegalArgumentException: Unable to locate hive jars to 
connect to metastore using classloader 
scala.tools.nsc.interpreter.IMain$TranslatingClassLoader. Please set 
spark.sql.hive.metastore.jars
     at 
org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:206)
     at 
org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:175)
     at 
org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:367)
     at 
org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:367)
     at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:366)
     at 
org.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:379)
     at 
org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:379)
     at 
org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:378)
     at 
org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:901)
     at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:134)
     at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
     at 
org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:419)
     at 
org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:264)


Thanks,
Peter Rudenko


‚Äã

‚Äã
"
Josh Rosen <rosenville@gmail.com>,"Fri, 29 May 2015 11:04:21 -0700",Re: ClosureCleaner slowing down Spark SQL queries,Nitin Goyal <nitin2goyal@gmail.com>,"Hey, want to file a JIRA for this?  This will make it easier to track
progress on this issue.  Definitely upload the profiler screenshots there,
too, since that's helpful information.

https://issues.apache.org/jira/browse/SPARK




"
Yin Huai <yhuai@databricks.com>,"Fri, 29 May 2015 11:56:30 -0700",Re: ClosureCleaner slowing down Spark SQL queries,Josh Rosen <rosenville@gmail.com>,"For Spark SQL internal operations, probably we can just
create MapPartitionsRDD directly (like
https://github.com/apache/spark/commit/5287eec5a6948c0c6e0baaebf35f512324c0679a
).


"
Justin Uang <justin.uang@gmail.com>,"Fri, 29 May 2015 19:54:24 +0000",Using UDFs in Java without registration,"""dev@spark.apache.org"" <dev@spark.apache.org>","I would like to define a UDF in Java via a closure and then use it without
registration. In Scala, I believe there are two ways to do this:

    myUdf = functions.udf({ _ + 5})
    myDf.select(myUdf(myDf(""age"")))

or

    myDf.select(functions.callUDF({_ + 5}, DataTypes.IntegerType,
myDf(""age"")))

However, both of these don't work for Java UDF. The first one requires
TypeTags. For the second one, I was able to hack it by creating a scala
AbstractFunction1 and using callUDF, which requires declaring the catalyst
DataType instead of using TypeTags. However, it was still nasty because I
had to return a scala map instead of a java map.

Is there first class support for creating
a org.apache.spark.sql.UserDefinedFunction that works with
the org.apache.spark.sql.api.java.UDF1<T1, R>? I'm fine with having to
declare the catalyst type when creating it.

If it doesn't exist, I would be happy to work on it =)

Justin
"
sara mustafa <eng.sara.mustafa@gmail.com>,"Fri, 29 May 2015 13:40:38 -0700 (MST)",Saving DataFrame in Tachyon,dev@spark.apache.org,"Hi All,
I have Spark-1.3.0 and Tachyon-0.5.0. When I am trying to save RDD in
tachyon, it success. But for saving a DataFrame it fails with the following
error:

java.lang.IllegalArgumentException: Wrong FS:
tachyon://localhost:19998/myres, expected: hdfs://localhost:54310
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)

Here is my code:
sc.hadoopConfiguration.set(""fs.tachyon.impl"", ""tachyon.hadoop.TFS"")
val region = sc.textFile(""tachyon://localhost:19998/user/hduser/region.tbl"")
val nation = sc.textFile(""tachyon://localhost:19998/user/hduser/nation.tbl"")
val regionSchemaString = ""R_REGIONKEY R_NAME R_COMMENT""
val nationSchemaString = ""N_NATIONKEY N_NAME N_REGIONKEY N_COMMENT""
val regionSchema =
    StructType(
      regionSchemaString.split("" "").map(fieldName => StructField(fieldName,
StringType, true)))
val nationSchema =
    StructType(
      nationSchemaString.split("" "").map(fieldName => StructField(fieldName,
StringType, true)))
val regionRowRDD = region.map(_.split(""\\|"")).map(r => Row(r(0), r(1),
r(2)))
val nationRowRDD = nation.map(_.split(""\\|"")).map(r => Row(r(0), r(1), r(2),
r(3)))
val regionDataFrame = sqlContext.createDataFrame(regionRowRDD, regionSchema)
val nationDataFrame = sqlContext.createDataFrame(nationRowRDD, nationSchema)
regionDataFrame.registerTempTable(""REGION"")
nationDataFrame.registerTempTable(""NATION"")
 val df = sqlContext.sql(""select * from REGION r, NATION n  where
r.R_REGIONKEY = n.N_REGIONKEY "")
df.count()
df.save(""tachyon://localhost:19998/myres"")

Could anyone help me please?

Thanks,
  
  

  





--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 29 May 2015 16:40:27 -0700",[VOTE] Release Apache Spark 1.4.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.0!

The tag to be voted on is v1.4.0-rc3 (commit dd109a8):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=dd109a8746ec07c7c83995890fc2c0cd7a693730

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc3-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.0]
https://repository.apache.org/content/repositories/orgapachespark-1109/
[published as version: 1.4.0-rc3]
https://repository.apache.org/content/repositories/orgapachespark-1110/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc3-docs/

Please vote on releasing this package as Apache Spark 1.4.0!

The vote is open until Tuesday, June 02, at 00:32 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What has changed since RC1 ==
Below is a list of bug fixes that went into this RC:
http://s.apache.org/vN

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.3 workload and running on this release candidate,
then reporting any regressions.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.4 QA period,
so -1 votes should only occur for significant regressions from 1.3.1.
Bugs already present in 1.3.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 29 May 2015 16:34:09 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for all the discussion on the vote thread. I am canceling this
vote in favor of RC3.


---------------------------------------------------------------------


"
Mike Ringenburg <mikeri@cray.com>,"Sat, 30 May 2015 01:38:45 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"The Configuration link on the docs appears to be broken.

Mike




Please vote on releasing the following candidate as Apache Spark version 1.4.0!

The tag to be voted on is v1.4.0-rc3 (commit dd109a8):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=dd109a8746ec07c7c83995890fc2c0cd7a693730

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc3-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.0]
https://repository.apache.org/content/repositories/orgapachespark-1109/
[published as version: 1.4.0-rc3]
https://repository.apache.org/content/repositories/orgapachespark-1110/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc3-docs/

Please vote on releasing this package as Apache Spark 1.4.0!

The vote is open until Tuesday, June 02, at 00:32 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What has changed since RC1 ==
Below is a list of bug fixes that went into this RC:
http://s.apache.org/vN

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.3 workload and running on this release candidate,
then reporting any regressions.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.4 QA period,
so -1 votes should only occur for significant regressions from 1.3.1.
Bugs already present in 1.3.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>

"
Taka Shinagawa <taka.epsilon@gmail.com>,"Fri, 29 May 2015 20:07:38 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Mike Ringenburg <mikeri@cray.com>,"Mike,

The broken Configuration link can be fixed if you add a missing dash '-' on
the first line in docs/configuration.md and run 'jekyll build'.

https://github.com/apache/spark/pull/6513


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 29 May 2015 22:48:19 -0700",StreamingContextSuite fails with NoSuchMethodError,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I ran the following command on 1.4.0 RC3:

mvn -Phadoop-2.4 -Dhadoop.version=2.7.0 -Pyarn -Phive package

I saw the following failure:

^[[32mStreamingContextSuite:^[[0m
^[[32m- from no conf constructor^[[0m
^[[32m- from no conf + spark home^[[0m
^[[32m- from no conf + spark home + env^[[0m
^[[32m- from conf with settings^[[0m
^[[32m- from existing SparkContext^[[0m
^[[32m- from existing SparkContext with settings^[[0m
^[[31m*** RUN ABORTED ***^[[0m
^[[31m  java.lang.NoSuchMethodError:
org.apache.spark.ui.JettyUtils$.createStaticHandler(Ljava/lang/String;Ljava/lang/String;)Lorg/eclipse/jetty/servlet/ServletContextHandler;^[[0m
^[[31m  at
org.apache.spark.streaming.ui.StreamingTab.attach(StreamingTab.scala:49)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContext$$anonfun$start$2.apply(StreamingContext.scala:585)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContext$$anonfun$start$2.apply(StreamingContext.scala:585)^[[0m
^[[31m  at scala.Option.foreach(Option.scala:236)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:585)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContextSuite$$anonfun$8.apply$mcV$sp(StreamingContextSuite.scala:101)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContextSuite$$anonfun$8.apply(StreamingContextSuite.scala:96)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContextSuite$$anonfun$8.apply(StreamingContextSuite.scala:96)^[[0m
^[[31m  at
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)^[[0m
^[[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)^[[0m

Did anyone else encounter similar error ?

Cheers
"
Reynold Xin <rxin@databricks.com>,"Sat, 30 May 2015 00:42:39 -0700",Re: Using UDFs in Java without registration,Justin Uang <justin.uang@gmail.com>,"I think you are right that there is no way to call Java UDF without
registration right now. Adding another 20 methods to functions would be
scary. Maybe the best way is to have a companion object
for UserDefinedFunction, and define UDF there?

e.g.

object UserDefinedFunction {

  def define(f: org.apache.spark.api.java.function.Function0, returnType:
Class[_]): UserDefinedFunction

  // ... define a few more - maybe up to 5 arguments?
}

Ideally, we should ask for both argument class and return class, so we can
do the proper type conversion (e.g. if the UDF expects a string, but the
input expression is an int, Catalyst can automatically add a cast).
However, we haven't implemented those in UserDefinedFunction yet.





"
Tathagata Das <tdas@databricks.com>,"Sat, 30 May 2015 00:42:14 -0700",Re: StreamingContextSuite fails with NoSuchMethodError,Ted Yu <yuzhihong@gmail.com>,"Did was it a clean compilation?

TD


"
Reynold Xin <rxin@databricks.com>,"Sat, 30 May 2015 00:54:07 -0700",Re: Dataframe's .drop in PySpark doesn't accept Column,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Yea would be great to support a Column. Can you create a JIRA, and possibly
a pull request?



y
d
a
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 30 May 2015 02:37:14 -0700",Re: StreamingContextSuite fails with NoSuchMethodError,Tathagata Das <tdas@databricks.com>,"I downloaded source tar ball and ran command similar to following with:
clean package -DskipTests

Then I ran the following command. 

Fyi 



teStaticHandler(Ljava/lang/String;Ljava/lang/String;)Lorg/eclipse/jetty/servlet/ServletContextHandler;^[[0m
.scala:49)^[[0m
pply(StreamingContext.scala:585)^[[0m
pply(StreamingContext.scala:585)^[[0m
text.scala:585)^[[0m
ply$mcV$sp(StreamingContextSuite.scala:101)^[[0m
ply(StreamingContextSuite.scala:96)^[[0m
ply(StreamingContextSuite.scala:96)^[[0m
ormer.scala:22)^[[0m
0m
"
Sean Owen <sowen@cloudera.com>,"Sat, 30 May 2015 09:55:15 -0400",Sidebar: issues targeted for 1.4.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","No 1.4.0 Blockers at this point, which is great. Forking this thread
to discuss something else.

There are 92 issues targeted for 1.4.0, 28 of which are marked
Critical. Many are procedural issues like ""update docs for 1.4"" or
""check X for 1.4"". Are these resolved? They sound like things that are
definitely supposed to have finished by now. Certainly, almost all of
these are not going to be resolved for 1.4.0. Is this something we
should be concerned about? because they are predominantly filed by or
assigned to committers, not inexperienced contributors.

I'm concerned that Target Version loses meaning if this happens
frequently, and this number is ~10% of all JIRAs for 1.4. It's
tempting to say X is important and someone will do X before 1.4, and
then forget about it since it has been safely noted for later.
Meanwhile other issues that grab more immediate attention and get
worked on. This constitutes a form of project management, but de facto
it's ad-hoc and reactive. Look at how many new issues and changes have
still been coming in since the first release candidate of 1.4.0,
compared to those ""targeted"" for the release.

In an ideal world,  Target Version really is what's going to go in as
far as anyone knows and when new stuff comes up, we all have to figure
out what gets dropped to fit by the release date. Boring, standard
software project management practice. I don't know how realistic that
is, but, I'm wondering how people feel about this, who have filed
these JIRAs?

Concretely, should non-Critical issues for 1.4.0 be un-Targeted?
should they all be un-Targeted after the release?




---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Sat, 30 May 2015 14:12:13 +0000",Re: Using UDFs in Java without registration,Reynold Xin <rxin@databricks.com>,"The idea of asking for both the argument and return class is interesting. I
don't think we do that for the scala APIs currently, right? In
functions.scala, we only use the TypeTag for RT.

  def udf[RT: TypeTag, A1: TypeTag](f: Function1[A1, RT]):
UserDefinedFunction = {
    UserDefinedFunction(f, ScalaReflection.schemaFor(typeTag[RT]).dataType)
  }

There would only be a small subset of conversions that would make sense
implicitly (e.g. int to double, the typical conversions in programming
languages), but something like (double => int) might be dangerous and
(timestamp => double) wouldn't really make sense. Perhaps it's better to be
explicit about casts?

If we don't care about declaring the types of the arguments, perhaps we can
have all of the java UDF interfaces (UDF1, UDF2, etc) extend a generic
interface called UDF, then have

    def define(f: UDF, returnType: Class[_])

to simplify the APIs.



"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sat, 30 May 2015 15:13:38 +0000",Re: Dataframe's .drop in PySpark doesn't accept Column,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Jira done : https://issues.apache.org/jira/browse/SPARK-7969
I've already started working on it but it's less trivial than it seems
because I don't exactly now the inner workings of the catalog,
and how to get the qualified name of a column to match it against the
schema/catalog.

Regards,

Olivier.

Le sam. 30 mai 2015 √† 09:54, Reynold Xin <rxin@databricks.com> a √©crit :

e
e
t a
"
Justin Uang <justin.uang@gmail.com>,"Sat, 30 May 2015 15:30:26 +0000",Catalyst: Reusing already computed expressions within a projection,"""dev@spark.apache.org"" <dev@spark.apache.org>","If I do the following

    df2 = df.withColumn('y', df['x'] * 7)
    df3 = df2.withColumn('z', df2.y * 3)
    df3.explain()

Then the result is

    > Project [date#56,id#57,timestamp#58,x#59,(x#59 * 7.0) AS y#64,((x#59
* 7.0) AS y#64 * 3.0) AS z#65]
    >  PhysicalRDD [date#56,id#57,timestamp#58,x#59], MapPartitionsRDD[125]
at mapPartitions at SQLContext.scala:1163

Effectively I want to compute

    y = f(x)
    z = g(y)

The catalyst optimizer realizes that y#64 is the same as the one previously
computed, however, when building the projection, it is ignoring the fact
that it had already computed y, so it calculates `x * 7` twice.

    y = x * 7
    z = x * 7 * 3

If I wanted to make this fix, would it be possible to do the logic in the
optimizer phase? I imagine that it's difficult because the expressions in
InterpretedMutableProjection don't have access to the previous expression
results, only the input row, and that the design doesn't seem to be catered
for this.
"
Justin Uang <justin.uang@gmail.com>,"Sat, 30 May 2015 16:02:38 +0000",Re: Catalyst: Reusing already computed expressions within a projection,"""dev@spark.apache.org"" <dev@spark.apache.org>","the dag of dependencies between expressions, then convert it into several
layers of projections, where each new layer is allowed to depend on
expression results from previous projections?

Are there any pitfalls to this approach?

"
Yijie Shen <henry.yijieshen@gmail.com>,"Sun, 31 May 2015 00:37:18 +0800",Re: Dataframe's .drop in PySpark doesn't accept Column,Girardot Olivier <o.girardot@lateral-thoughts.com>,"I think just match the Column‚Äôs expr as UnresolvedAttribute and use UnresolvedAttribute‚Äôs name to match schema‚Äôs field name is enough.

Seems no need to regard expr as a more general one. :)


Jira done :¬†https://issues.apache.org/jira/browse/SPARK-7969
I've already started working on it but it's less trivial than it seems because I don't exactly now the inner workings of the catalog,¬†
and how to get the qualified name of a column to match it against the schema/catalog.

Regards,¬†

Olivier.

Le¬†sam. 30 mai 2015 √†¬†09:54, Reynold Xin <rxin@databricks.com> a √©crit¬†:
Yea would be great to support a Column. Can you create a JIRA, and possibly a pull request?


Actually, the Scala API too is only based on column name

Le¬†ven. 29 mai 2015 √†¬†11:23, Olivier Girardot <o.girardot@lateral-thoughts.com> a √©crit¬†:
Hi,¬†
Testing a bit more 1.4, it seems that the .drop() method in PySpark doesn't seem to accept a Column as input datatype :¬†

¬† ¬† .join(only_the_best, only_the_best.pol_no == df.pol_no, ""inner"").drop(only_the_best.pol_no)\
File ""/usr/local/lib/python2.7/site-packages/pyspark/sql/dataframe.py"", line 1225, in drop
jdf = self._jdf.drop(colName)
File ""/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 523, in __call__
(new_args, temp_args) = self._get_args(args)
File ""/usr/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 510, in _get_args
temp_arg = converter.convert(arg, self.gateway_client)
File ""/usr/local/lib/python2.7/site-packages/py4j/java_collections.py"", line 490, in convert
for key in object.keys():
TypeError: 'Column' object is not callable

It doesn't seem very consistent with rest of the APIs - and is especially annoying when executing joins - because drop(""my_key"") is not a qualified reference to the column.

What do you think about changing that ? or what is the best practice as a workaround ?

Regards,¬†

Olivier.

"
unioah <unioah@gmail.com>,"Sat, 30 May 2015 09:44:43 -0700 (MST)",problem with using mapPartitions,dev@spark.apache.org,"Hi,

I try to aggregate the value in each partition internally.   
For example, 

Before:
worker 1:            worker 2:
1, 2, 1                 2, 1, 2

After:
worker 1:              worker 2:
(1->2), (2->1)       (1->1), (2->2)

I try to use mappartitions, 
object MyTest {                                                                                                  
  def main(args: Array[String]) {                                                                                
    val conf = new SparkConf().setAppName(""This is a test"")                                                      
    val sc = new SparkContext(conf)                                                                              
                                                                                                                 
    val fDB = sc.parallelize(List(1, 2, 1, 2, 1, 2, 5, 5, 2), 3)                                                 
    val result = fDB.mappartitions(testMP).collect                                                      
    println(result.mkString)                                                                                     
    sc.stop                                                                                                      
  }                                                                                                              
                                                                                                                 
  def testMP(iter: Iterator[Int]): Iterator[(Long, Int)] = {                                                     
    var result = new LongMap[Int]()                                                                              
    var cur = 0l                                                                                                 
                                                                                                                 
    while (iter.hasNext) {                                                                                       
      cur = iter.next.toLong                                                                                     
      if (result.contains(cur)) {                                                                                
        result(cur) += 1                                                                                         
      } else {                                                                                                   
        result += (cur, 1)                                                                                       
      }                                                                                                          
    }                                                                                                            
    result.toList.iterator                                                                                       
  }                                                                                                              
}                                                                                                                 

But I got the error message no matter how I tried. 

Driver stacktrace:
        at
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependent
Stages(DAGScheduler.scala:1204)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
        at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
        at scala.Option.foreach(Option.scala:236)
        at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
        at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
        at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/05/30 10:41:21 ERROR SparkDeploySchedulerBackend: Asked to remove
non-existent executor 1

Anybody can help me? Thx



--

---------------------------------------------------------------------


"
Nitin Goyal <nitin2goyal@gmail.com>,"Sat, 30 May 2015 11:31:11 -0700 (MST)",Re: ClosureCleaner slowing down Spark SQL queries,dev@spark.apache.org,"Thanks Josh and Yin.

Created following JIRA for the same :-

https://issues.apache.org/jira/browse/SPARK-7970

Thanks
-Nitin



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 30 May 2015 11:38:06 -0700",Re: problem with using mapPartitions,unioah <unioah@gmail.com>,"bq.     val result = fDB.mappartitions(testMP).collect

Not sure if you pasted the above code - there was a typo: method name
should be mapPartitions

Cheers


"
Reynold Xin <rxin@databricks.com>,"Sat, 30 May 2015 11:41:52 -0700",Re: Dataframe's .drop in PySpark doesn't accept Column,Yijie Shen <henry.yijieshen@gmail.com>,"Name resolution is not as easy I think.  Wenchen can maybe give you some
advice on resolution about this one.



se
s enough.
√©crit :
e
,
ot a
s
"
Reynold Xin <rxin@databricks.com>,"Sat, 30 May 2015 12:02:31 -0700",Re: Catalyst: Reusing already computed expressions within a projection,Justin Uang <justin.uang@gmail.com>,"I think you are looking for
http://en.wikipedia.org/wiki/Common_subexpression_elimination in the
optimizer.

the optimization time might increase. Do you see a case where this can
bring you substantial performance gains?



"
Reynold Xin <rxin@databricks.com>,"Sat, 30 May 2015 12:04:08 -0700",Re: Using UDFs in Java without registration,Justin Uang <justin.uang@gmail.com>,"We added all the typetags for arguments but haven't got around to use them
yet. I think it'd make sense to have them and do the auto cast, but we can
have rules in analysis to forbid certain casts (e.g. don't auto cast double
to int).



"
unioah <unioah@gmail.com>,"Sat, 30 May 2015 13:41:23 -0700 (MST)",Re: problem with using mapPartitions,dev@spark.apache.org,"Thank you for your reply.

But the typo is not reason for the problem. 



--

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Sat, 30 May 2015 15:44:28 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 17:07 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.3"
Reynold Xin <rxin@databricks.com>,"Sat, 30 May 2015 16:44:17 -0700",please use SparkFunSuite instead of ScalaTest's FunSuite from now on,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI we merged a patch that improves unit test log debugging. In order for
that to work, all test suites have been changed to extend SparkFunSuite
instead of ScalaTest's FunSuite. We also added a rule in the Scala style
checker to fail Jenkins if FunSuite is used.

The patch that introduced SparkFunSuite:
https://github.com/apache/spark/pull/6441

Style check patch: https://github.com/apache/spark/pull/6510
"
unioah <unioah@gmail.com>,"Sat, 30 May 2015 18:03:37 -0700 (MST)",Re: problem with using mapPartitions,dev@spark.apache.org,"I solved the problem. 

It was caused by using spark-core_2.11 mvn repository. 
When I compiled with spark-core_2.10, the problem doesn't show up again. 




--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Sun, 31 May 2015 12:00:02 +1000",Re: Catalyst: Reusing already computed expressions within a projection,Reynold Xin <rxin@databricks.com>,"I think this is likely something that we'll want to do during the code
generation phase.  Though its probably not the lowest hanging fruit at this
point.


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sun, 31 May 2015 16:55:27 +0000",Re: Dataframe's .drop in PySpark doesn't accept Column,"=?UTF-8?B?6IyD5paH6Iej?= <cloud0fan@163.com>, 
	Yijie Shen <henry.yijieshen@gmail.com>","I understand the rational, but when you need to reference, for example when
using a join, some column which name is not unique, it can be confusing in
terms of API.
However I figured out that you can use a ""qualified"" name for the column
using the *other-dataframe.column_name* syntax, maybe we just need to
document this well...


Le dim. 31 mai 2015 √† 12:18, ËåÉÊñáËá£ <cloud0fan@163.com> a √©crit :

d
or
o
use
is enough.
√©crit :
ne
ne
not a
"
Justin Uang <justin.uang@gmail.com>,"Sun, 31 May 2015 18:48:59 +0000",Re: Catalyst: Reusing already computed expressions within a projection,"Michael Armbrust <michael@databricks.com>, Reynold Xin <rxin@databricks.com>","Thanks for pointing to that link! It looks like it‚Äôs useful, but it does
look more complicated than the case I‚Äôm trying to address.

In my case, we set y = f(x), then we use y later on in future projections (z
= g(y)). In that case, the analysis is trivial in that we aren‚Äôt trying to
find equivalent expressions, we actually know that z is based off of y. In
addition, we are already storing off y because it‚Äôs one of the projections,
so there‚Äôs no tradeoff between time vs memory.
Perf gains

I believe that the performance gains can be quite substantial, but can you
check that the case I bring up below will indeed benefit from such a
optimization?

For example, suppose I have a date column (unclean_date) that is stored in
some strange string format. I then use an udf or a hive function that
converts it to the Catalyst date type (cleaned_date). Next, I want to
extract one column with the month, and another with the year, so I can do
groupBys/aggregations later. Currently, every projection/expression based
off of the cleaned_date will have to do the expensive parsing again if I
avoid caching and prefer to do everything in one pass.
Code generation phase vs optimization

Is there a reason why doing it at the optimization phase is the wrong
approach? If sounds like we‚Äôre actually logically changing the order of
computation if we do my proposed approach. I do agree however if there are
lower hanging fruits, then we should tackle those first =)
‚Äã


is
o
d on
ring
e.
ions
em to
"
Reynold Xin <rxin@databricks.com>,"Sun, 31 May 2015 14:04:00 -0700",Re: Catalyst: Reusing already computed expressions within a projection,Justin Uang <justin.uang@gmail.com>,"I think Michael's bringing up code gen because the compiler (not Spark, but
javac and JVM JIT) already does common subexpression elimination, so we
might get it for free during code gen.


:

it does
ns
ôt
one of
u
der of
e
his
:
,
to
nd on
oring
ce.
sions
eem to
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 31 May 2015 16:34:25 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Krishna Sankar <ksankar42@gmail.com>,"+1 (non-binding)

Launched against a pseudo-distributed YARN cluster running Hadoop 2.6.0 and
ran some jobs.

-Sandy


"
