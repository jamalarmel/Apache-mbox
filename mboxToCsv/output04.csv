Marvin <no-reply@apache.org>,"Sun,  1 Sep 2013 13:43:29 +0000 (UTC)",Incubator PMC/Board report for Sep 2013 ([ppmc]),dev@spark.incubator.apache.org,"

Dear podling,

This email was sent by an automated system on behalf of the Apache Incubator PMC.
It is an initial reminder to give you plenty of time to prepare your quarterly
board report.

The board meeting is scheduled for Wed, 18 September 2013, 10:30:00:00 PST. The report 
for your podling will form a part of the Incubator PMC report. The Incubator PMC 
requires your report to be submitted 2 weeks before the board meeting, to allow 
sufficient time for review and submission (Wed, Sep 4th).

Please submit your report with sufficient time to allow the incubator PMC, and 
subsequently board members to review and digest. Again, the very latest you 
should submit your report is 2 weeks prior to the board meeting.

Thanks,

The Apache Incubator PMC

Submitting your Report
----------------------

Your report should contain the following:

 * Your project name
 * A brief description of your project, which assumes no knowledge of the project
   or necessarily of its field
 * A list of the three most important issues to address in the move towards 
   graduation.
 * Any issues that the Incubator PMC or ASF Board might wish/need to be aware of
 * How has the community developed since the last report
 * How has the project developed since the last report.
 
This should be appended to the Incubator Wiki page at:

  http://wiki.apache.org/incubator/September2013

Note: This manually populated. You may need to wait a little before this page is
      created from a template.

Mentors
-------
Mentors should review reports for their project(s) and sign them off on the 
Incubator wiki page. Signing off reports shows that you are following the 
project - projects that are not signed may raise alarms for the Incubator PMC.

Incubator PMC


"
Roman Shaposhnik <rvs@apache.org>,"Mon, 2 Sep 2013 14:35:33 -0700",Re: Release process?,"dev@spark.incubator.apache.org, dev@bigtop.apache.org","
A huge +1 for spending effort on frequent releases. This is one of the
key elements
for an incubator project successful graduation.

Also, even though Spark is a rather new addition to the Bigtop family we've got
a few integration tests available for it already and would love to provide the
feedback!

Thanks,
Roman.

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 2 Sep 2013 18:48:18 -0700",[Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi everyone,

In preparation for the 0.8 release, I've put together a first release candidate mostly to help mentors or more experienced Apache people check whether the licensing, POMs, etc are right: http://www.cs.berkeley.edu/~matei/spark-0.8.0-incubating-RC1.tgz. I've tried to write the LICENSE, NOTICE, and POMs as required by Apache. Please take a look if you have a chance -- the ReadMe contains instructions on how to build it and launch the shell.

This first release candidate is still missing some doc pages, so it won't be the final release, but it should have in place all the packaging stuff we'd like for 0.8, and it should be able to run out of the box on Linux, Windows and Mac OS X. Feel free to also test it for functionality; we may merge in a couple more bug fixes but most of the functionality will be there.

Matei
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 2 Sep 2013 18:49:50 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","By the way, this tar file also corresponds to commit a106ed8b97e707b36818c11d1d7211fa28636178 in our Apache git repo.

Matei


candidate mostly to help mentors or more experienced Apache people check whether the licensing, POMs, etc are right: http://www.cs.berkeley.edu/~matei/spark-0.8.0-incubating-RC1.tgz. I've tried to write the LICENSE, NOTICE, and POMs as required by Apache. Please take a look if you have a chance -- the ReadMe contains instructions on how to build it and launch the shell.
won't be the final release, but it should have in place all the packaging stuff we'd like for 0.8, and it should be able to run out of the box on Linux, Windows and Mac OS X. Feel free to also test it for functionality; we may merge in a couple more bug fixes but most of the functionality will be there.


"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 2 Sep 2013 21:32:06 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Matei,

Are we ""locking"" pull requests to github repo by tomorrow?
Meaning no more push to GitHub repo for Spark.

github repo to be merged back to ASF Git repo.

- Henry


"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Tue, 3 Sep 2013 12:59:45 +0000",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Matei, I'll try and give it a look.

My recommendation is that we start to use the Incubator release doc/guide:

http://incubator.apache.org/guides/releasemanagement.html


Note that there is a new method for publishing release candidates and
releases
that uses SVNpubsub:

http://www.apache.org/dev/release.html#upload-ci


We should probably start using that. It will require a little back and
forth
with infra but will be worth it in the end. If no one files the issue
by this evening Eastern time, I'll file it.

Alternatively, at the least, the release candidate should be
housed on Apache servers, e.g., in your p.a.o/public_html/ directory, along
with *.md5 *.asc and *.sha files, and perhaps a CHANGES.txt/change log.
Also (and this is all in the guide above) your release signing key
should be uploaded to id.apache.org and then it will appear here:

http://people.apache.org/keys/group/spark.asc


HTH and happy to help out as needed.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++








"
Michael Joyce <joyce@apache.org>,"Tue, 3 Sep 2013 07:53:56 -0700",Re: Moving JIRA to Apache,dev@spark.incubator.apache.org,"Hay all,

Quick update. I bugged INFRA a bit about this. When they were importing
last time their test server broke so they couldn't validate the export.
They're waiting for someone to fix it. Jake Farrell said he would take a
look at the test server tonight and see if he could get it up and running.
Hopefully this will be moving along shortly. I'll keep an eye out for
updates and let you know if they need a new export.


-- Joyce



"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 3 Sep 2013 09:47:23 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"So are you planning to release 0.8 from the master branch (which is at
a106ed8... now) or from branch-0.8?




"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 10:28:34 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Hi guys,


Right now the branches are the same in terms of content (though I might not have merged the latest changes into 0.8). If we add stuff into master that we won't want in 0.8 we'll break that.

doc/guide:

Cool, thanks for the pointer. I'll try to follow the steps there about signing.

requests for

We'll probably use the GitHub repo for the last few changes in this release and then switch. The reason is that there's a bit of work to do pull requests against the Apache one.

Matei
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 3 Sep 2013 10:39:40 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"What is going to be the process for making pull requests?  Can they be made
against the github mirror (https://github.com/apache/incubator-spark), or
must we use some other way?



"
Gary Malouf <malouf.gary@gmail.com>,"Tue, 3 Sep 2013 14:04:12 -0400",Upgrading to latest Spray Milestone,dev@spark.incubator.apache.org,"The spray team did some major refactors between M2 and M8.  Has anyone
tried to upgrade spark for this?  I am looking at working on it this week,
but wanted to see if anyone had already taken it on.

Gary
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 3 Sep 2013 11:07:23 -0700",Re: Upgrading to latest Spray Milestone,dev@spark.incubator.apache.org,"Spark doesn't use Spray anymore.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 3 Sep 2013 11:10:15 -0700",Re: Upgrading to latest Spray Milestone,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Gary - ya we moved away from Spray. It was a bit overkill for what
we needed (we just needed some very simple embedded web servers) and
they kept doing major API refactorings that made it really hard to
keep up-to-date.

- Patrick


"
Gary Malouf <malouf.gary@gmail.com>,"Tue, 3 Sep 2013 14:15:00 -0400",Re: Upgrading to latest Spray Milestone,dev@spark.incubator.apache.org,"My apologies, I've been using 0.7.3 and just read this on the old google
group.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 13:33:11 -0700",Re: Moving JIRA to Apache,dev@spark.incubator.apache.org,"Okay, thanks for asking them about it. I agree that it would be awesome to move JIRA over as soon as possible.

Matei


importing
export.
a
running.
to
<matei.zaharia@gmail.com
saw


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 13:38:57 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Yup, the plan is as follows:

- Make pull request against the mirror
- Code review on GitHub as usual
- Whoever merges it will simply merge it into the main Apache repo; when this propagates, the PR will be marked as merged

I found at least one other Apache project that did this: http://wiki.apache.org/cordova/ContributorWorkflow.

Matei


made
or
at
might
master
about
requests for
do


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 3 Sep 2013 14:25:59 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Okay, so is there any way to get github's compare view to be happy with
differently-named repositories?  What I want is to be able to compare and
generate a pull request between
github.com/apache/incubator-spark:masterand, e.g.,
github.com/markhamstra/spark:myBranch and not need to create a new
github.com/markhamstra/incubator-spark.  It's bad enough that the
differently-named repos don't show up in the compare-view drop-down
choices, but I also haven't been able to find a hand-crafted URL that will
make this work.




"
Evan Chan <ev@ooyala.com>,"Tue, 3 Sep 2013 16:57:01 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sorry one more clarification.

For doc pull requests for 0.8 release, should these be done against
the existing mesos/spark repo, or against the mirror at
apache/incubator-spark ?
I'm hoping to clear up a couple things in the docs before the release this week.

thanks,
-Evan





-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Sep 2013 18:18:35 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","So looks like we need to manually resolve the Github pull requests.

Or, does github automatically know that a particular merge to ASF git repo
is associated to a GitHub pull request?

- Henry



"
Michael Joyce <joyce@apache.org>,"Tue, 3 Sep 2013 18:52:48 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Henry,

I fairly certain that we'll have to manually resolve the pull requests. As
far as I know, the Github mirror is simply a read-only mirror of the
project's repository (be it svn or git). Hopefully someone will chime in
and correct me if I'm wrong.




-- Joyce



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 19:08:07 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Yes, please do the PRs against the GitHub repo for now.

Matei


this week.
with
and
will
when
be
(https://github.com/apache/incubator-spark),
<matei.zaharia@gmail.com
is at
might
about
requests
this
to do


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 19:08:59 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"As far as I understood, we will have to manually merge those PRs into the Apache repo. However, GitHub will notice that they're ""merged"" as soon as it sees those commits in the repo, and will automatically close them. At least this is my experience merging other peoples' code (sometimes I just check out their branch from their repo and merge it manually).

Matei


requests. As
in
repo
<matei.zaharia@gmail.com
when
be
(https://github.com/apache/incubator-spark),
is
about
requests
this
to


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 19:09:26 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"BTW, what docs are you planning to write? Something on make-distribution.sh would be nice.

Matei


this week.
with
and
will
when
be
(https://github.com/apache/incubator-spark),
<matei.zaharia@gmail.com
is at
might
about
requests
this
to do


"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Sep 2013 20:01:40 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Guys.

In other ASF projects I also allow people to attach the git diff to the
JIRA itself (once we have one) and apply the patch and merge manually.
I believe we could later configure ASF Jenkins to run when a patch is
attached to JIRA (like in HBase and Hadoop).

Do we want to also describe/ allow this alternative way to contribute
patches?


- Henry



"
Reynold Xin <reynoldx@gmail.com>,"Wed, 4 Sep 2013 11:08:29 +0800",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","That seems substantially more overhead than generating github pull
requests. Is there any particular reason we want to do that?



"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Sep 2013 20:18:01 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Reynold,

It  is just another way to contribute patches and can be used as transition
flow for people than come from SVN or other centralized source control
software.

- Henry



"
Michael Joyce <mltjoyce@gmail.com>,"Tue, 3 Sep 2013 20:21:51 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Agreed. It would be nice for people to be able to contribute in a way that
doesn't force them through Github. I can't see that as being painful but it
would probably be nice for others to have the option at the very least. I
think the vast majority of people when given the two options will pick
Github so we wouldn't really lose out on much. Plus it would be a bummer to
miss out on contributions because someone is uncomfortable with Github or
pull request in general.

----------------
Mike



"
Mike <spark@good-with-numbers.com>,"Wed, 4 Sep 2013 03:25:19 +0000",apache/incubator-spark repo ready to use?,Matei Zaharia <matei.zaharia@gmail.com>,"
""The plan""?  Can we do that now?  If I'm putting together a pull request 
for post-0.8, is there any reason to wait to fork 
apache/incubator-spark?

"
Tsuyoshi OZAWA <ozawa.tsuyoshi@gmail.com>,"Wed, 4 Sep 2013 12:48:44 +0900",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Hi Matei,

I've confirmed some points on my environment(OS X 10.8.4 and HotSpotVM
1.6.0_37):
1. sbt/sbt assembly works correctly.
2. sbt/sbt package works correctly.
2. sbt/sbt test works correctly(its results are grean).
4. ./run-example org.apache.spark.examples.SparkPi local 1000

Thanks,
Tsuyoshi




-- 
- Tsuyoshi

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 23:35:05 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Cool, thanks for trying it out!

Regarding the pull request model, let's see how it goes. I'd prefer not to have to support two different ways to send patches, but let's see what people ask for -- maybe existing Apache contributors will be more familiar with the JIRA way. I do want to document the model much better on our ""how to contribute"" page before our last release candidate of 0.8 though.

Matei


requests for
release
check
I've
Please
on how
won't
stuff
Linux,
we may
be


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Sep 2013 23:35:41 -0700",Re: apache/incubator-spark repo ready to use?,Mike <spark@good-with-numbers.com>,"
Nope, go for it. It would be great if people started doing this now.

Matei


"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Sep 2013 23:50:00 -0700",Re: apache/incubator-spark repo ready to use?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Matei, I think it would be better to freeze the pull requests against
mesos:spark GitHub repo and make announcement to start making pull
request against apache/incubator-spark.

We did the announcement for mailing lists move but I think we need to
do formal repo transition announcement too.

- Henry



"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Sep 2013 23:50:17 -0700",Re: apache/incubator-spark repo ready to use?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 =)


"
Evan Chan <ev@ooyala.com>,"Wed, 4 Sep 2013 00:38:30 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Matei,


Yes, I was planning to write something on make-distribution.sh, and
reference it in the standalone and Mesos deploy guides.

I was also going to document public methods in SparkContext that have
not been documented before, such as getPersistentRdds,
getExecutorStatus etc.   Some folks on my team don't realize that such
methods existed as they were not in the doc.

-Evan




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Evan Chan <ev@ooyala.com>,"Wed, 4 Sep 2013 00:40:42 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sorry just to be super clear but ""the GitHub repo"" (ie PRs for 0.8
docs) refers to:

a)  github.com/mesos/spark
b)  github.com/apache/incubator-spark

I'm assuming a) but don't want to be wrong.... thanks!




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 4 Sep 2013 08:24:25 -0700",Re: [Licensing check] Spark 0.8.0-incubating RC1,dev@spark.incubator.apache.org,"Yup, use a) for pre-0.8 stuff.

Matei


release this week.
with
compare and
that will
repo; when
they be
(https://github.com/apache/incubator-spark),
<matei.zaharia@gmail.com
(which is at
I might
into
about
requests
this
work to do


"
Roman Shaposhnik <rvs@apache.org>,"Wed, 4 Sep 2013 08:37:12 -0700",Re: apache/incubator-spark repo ready to use?,dev@spark.incubator.apache.org,"
+1. That would make a lot of sense.

Thanks,
Roman.

"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 4 Sep 2013 09:52:03 -0700",Re: Incubator PMC/Board report for Sep 2013 ([ppmc]),dev@spark.incubator.apache.org,"Hi guys,

I've written a draft update at https://wiki.apache.org/incubator/September2013#preview. Let me know how it looks.

Matei


Incubator PMC.
quarterly
PST. The report 
Incubator PMC 
to allow 
PMC, and 
latest you 
the project
towards 
aware of
this page is
on the 
the 
Incubator PMC.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 4 Sep 2013 09:59:39 -0700",Re: apache/incubator-spark repo ready to use?,dev@spark.incubator.apache.org,"Let me try to put together a doc on this. I wanted to wait for 0.8 to be out to simplify the process and immediately have a new ""how to contribute"" doc on the website, but I guess I can host this elsewhere if the release takes a while.

Matei




"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 4 Sep 2013 10:03:50 -0700",Re: Incubator PMC/Board report for Sep 2013 ([ppmc]),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

Thanks Matei.

Will sign off EOD if no more comments or updates.

- Henry


"
Andy Konwinski <andykonwinski@gmail.com>,"Wed, 4 Sep 2013 10:03:58 -0700",Re: Incubator PMC/Board report for Sep 2013 ([ppmc]),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Looks great to me Matei. Thanks for writing it up.

Andy



"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 4 Sep 2013 10:06:33 -0700",Re: apache/incubator-spark repo ready to use?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Lets stick it to the wiki for now. Some ASF projects also put their how to
contribute doc in wiki for easy update.

- Henry


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 4 Sep 2013 10:11:04 -0700",Re: apache/incubator-spark repo ready to use?,dev@spark.incubator.apache.org,"Henry, the problem is that we already have a website: http://spark.incubator.apache.org/docs/latest/contributing-to-spark.html. That's what most people read, so we need to update that.

Matei


how to
be
contribute""
release
<rvs@apache.org<javascript:;>>
<henry.saputra@gmail.com<javascript:;>>
against


"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 4 Sep 2013 10:16:04 -0700",Re: apache/incubator-spark repo ready to use?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah yes, sorry I jump the gun there. You moved the website so fast I
forget you already migrate the how to contribute page :)

Mea culpa from me.

- Henry


"
Konstantin Boudnik <cos@apache.org>,"Wed, 4 Sep 2013 19:16:03 -0700","Re: [spark] Change build and run instructions to use assemblies
	(#857)",dev@spark.incubator.apache.org,"Matei,

sorry for the late checking... I've looked into your changes and all seems to
be in order. 

Now I see that you are adding examples to the assembly, which I've pulled out
of the bigtop assembly initially. Do you think it is ok to add it back,
perhaps?

Cos

----- Forwarded message from Matei Zaharia <notifications@github.com> -----

Date: Thu, 29 Aug 2013 21:51:49 -0700
From: Matei Zaharia <notifications@github.com>
To: mesos/spark <spark@noreply.github.com>
Cc: Cos <cos@apache.org>
Subject: Re: [spark] Change build and run instructions to use assemblies
	(#857)

I'm going to merge this because a bunch of other things depend on it. If you try it on Maven and see any problems, let me know.

---
Reply to this email directly or view it on GitHub:
https://github.com/mesos/spark/pull/857#issuecomment-23540576

----- End forwarded message -----

"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 4 Sep 2013 21:17:23 -0700",Re: [spark] Change build and run instructions to use assemblies (#857),dev@spark.incubator.apache.org,"Cos, the examples are being built into a separate assembly. That should also be true in the Maven assembly project as far as I can tell.

Matei


seems to
pulled out
back,
-----
assemblies
If you try it on Maven and see any problems, let me know.


"
Konstantin Boudnik <cos@apache.org>,"Wed, 4 Sep 2013 21:22:26 -0700","Re: [spark] Change build and run instructions to use assemblies
	(#857)",dev@spark.incubator.apache.org,"Examples are built into a separate jar file, which isn't technically an
assembly. But I think it is fine - I have added some code into Bigtop to
simply reuse that jar without including it into an assembly.

I guess that answers my question. Sorry, should've experiment first ;) Thanks
Matei


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 4 Sep 2013 22:16:28 -0700",Re: [spark] Change build and run instructions to use assemblies (#857),dev@spark.incubator.apache.org,"The examples do ship with their dependencies though AFAIK. I added a Maven shade plugin entry in the examples POM. Anyway, as long as you can run with that JAR, it means it works!

Matei


an
to
Thanks
should also
seems to
pulled out
back,
<notifications@github.com> -----
assemblies
it. If you try it on Maven and see any problems, let me know.


"
Andrew Hart <ahart@apache.org>,"Wed, 04 Sep 2013 22:31:52 -0700",Re: Incubator PMC/Board report for Sep 2013 ([ppmc]),dev@spark.incubator.apache.org,"Thanks Matei, it looks great! Props for taking the lead in getting this 
put together on time. I've signed off on the wiki.

Best,
Andrew.



"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Thu, 5 Sep 2013 17:14:02 +0000",Re: Incubator PMC/Board report for Sep 2013 ([ppmc]),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","THanks Matei I signed off

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






Subject: Re: Incubator PMC/Board report for Sep 2013 ([ppmc])



"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 5 Sep 2013 19:03:57 -0700",Re: Google Groups: You've been added to Spark Developers,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, 
	""matei.zaharia@gmail.com"" <matei.zaharia@gmail.com>","Matei, I saw your test emails to Spark developer google group list but they
did not show up in the ASF dev@ list.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 5 Sep 2013 20:08:21 -0700",Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey All,

Matei asked me to pick this up because he's travelling this week. I
cut a second release candidate from the head of the 0.8 branch (on
mesos/spark gitub) to address the following issues:

- RC is now hosted in an apache web space
- RC now includes signature
- RC now includes MD5 and SHA512 digests

[tgz] http://people.apache.org/~pwendell/spark-rc/spark-0.8.0-src-incubating-RC2.tgz
[all files] http://people.apache.org/~pwendell/spark-rc/

It would be great to get feedback on the release structure. I also
changed the name to include ""src"" since we will be releasing both
source and binary releases.

I was a bit confused about how to attach my GPG key to the spark.asc
file. I took the following steps.

1. Greated a GPG key locally
2. Distributed the key to public key servers (gpg --send-key)
3. Add exported key to my apache web space:
http://people.apache.org/~pwendell/9E4FE3AF.asc
4. Added the key fingerprint at id.apage.org
5. Create an apache FOAF file with the key signature

However, this doesn't seem sufficient to get my key on this page (at
least, not yet):
http://people.apache.org/keys/group/spark.asc

Chris - are there other steps I missed? Is there a manual way to
augment this file?

- Patrick

"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 5 Sep 2013 21:14:43 -0700",Re: Spark 0.8.0-incubating RC2,dev@spark.incubator.apache.org,"Are these RCs not getting tagged in the repository, or am I just not
looking in the right place?




"
Patrick Wendell <pwendell@gmail.com>,"Thu, 5 Sep 2013 21:20:35 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","No these are posted primarily for the purpose of having the Apache
mentors look at the bundling format, they are not likely to be the
exact commit we release (though this RC was
fc6fbfe7d7e9171572c898d9e90301117517e60e).


"
Evan Chan <ev@ooyala.com>,"Thu, 5 Sep 2013 21:25:09 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Patrick,

I'm planning to submit documentation PR's against mesos/spark, by tomorrow,
is that OK?    We really should update the docs.

thanks,
Evan







-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Evan Chan <ev@ooyala.com>,"Thu, 5 Sep 2013 21:28:48 -0700",Re: off-heap RDDs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Haoyuan,

Thanks, that sounds great, exactly what we are looking for.

We might be interested in integrating Tachyon with CFS (Cassandra File
System, the Cassandra-based implementation of HDFS).

-Evan




y,
n
st
,
n
w
p
o)
.
2
rk/tachyon
ke
e""
e
be
 a
nd
e
Of
h
s.
et
So
n
e,
ss
se
e
to
ng
ly
y
e
g
rs
y
cal
p
542%2FTachyon_2013-05-09_Spark_Meetup.pdf
in
d
ap
 a
t



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 5 Sep 2013 21:58:55 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Evan,

These are posted primarily for the purpose of having the Apache
mentors look at the bundling format, they are not likely to be the
exact commit we release. Matei will be merging in some doc stuff
before the release, I'm pretty sure that includes your docs.

- Patrick


"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 6 Sep 2013 08:17:11 +0200",Apache account,dev@spark.incubator.apache.org,"Hi

I submitted my license agreement and account name request a while back, but
still haven't received any correspondence. Just wondering what I need to do
in order to follow this up?

Thanks
Nick
"
Reynold Xin <rxin@cs.berkeley.edu>,"Fri, 6 Sep 2013 15:15:53 +0800",Re: Apache account,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
        Chris Mattmann <chris.a.mattmann@jpl.nasa.gov>","Copying Chris on this one.


--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org




"
Haoyuan Li <haoyuan.li@gmail.com>,"Fri, 6 Sep 2013 01:16:12 -0700",Re: off-heap RDDs,dev@spark.incubator.apache.org,"That will be great!

Haoyuan



ce
In
is
of
ly
2
rk/tachyon
 a
us
at
ly
me
u
e
h:
to
ng
ig
o
e
n
t
h
,
e,
n
ut
 I
ut
n
.
h
ve
it
an
on
is
y.
as
t
e
542%2FTachyon_2013-05-09_Spark_Meetup.pdf
d
r
 a
s
"
Suresh Marru <smarru@apache.org>,"Fri, 6 Sep 2013 06:24:04 -0400",Re: Incubator PMC/Board report for Sep 2013 ([ppmc]),dev@spark.incubator.apache.org,"Thanks Matei, I signed off as well.

Suresh


https://wiki.apache.org/incubator/September2013#preview. Let me know how it looks.
Incubator PMC.
quarterly
10:30:00:00 PST. The report 
Incubator PMC 
meeting, to allow 
PMC, and 
latest you 
the project
towards 
be aware of
this page is
on the 
the 
Incubator PMC.


"
Andrew Hart <ahart@apache.org>,"Fri, 06 Sep 2013 07:22:24 -0700",Re: Apache account,dev@spark.incubator.apache.org,"Hi Nick,

I searched through the foundation mail archives and and see that your 
ICLA request was marked as received and accepted on July 01. However, 
for some reason, it appears your information was not added to the list 
of filed ICLAs, and you do not yet show up under the ""Unlisted CLAs"" 
section of http://people.apache.org/committer-index.html.

I will investigate further to see what might have happened.

Best,
Andrew.





"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 6 Sep 2013 20:33:03 +0000",Re: Needs a matrix library,"""dev@sis.apache.org"" <dev@sis.apache.org>","Hey Martin,

We may seriously consider using either Apache Hama here (which will
bring in Hadoop):

http://hama.apache.org/

Or alternatively, think about some Apache Spark based library:

http://spark.incubator.apache.org/

http://stackoverflow.com/questions/18453359/scala-spark-matrix-operations


It will refer to you to MLBase, which we could base it on.

I'm CC'ing the Apache Spark list here to connect the dots.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++









"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Sep 2013 13:56:39 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Chris, Henry... do you guys have feedback here? This was based
largely on your feedback in the last ""round"" :)


"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 6 Sep 2013 20:57:34 +0000",Re: Spark 0.8.0-incubating RC2,"Patrick Wendell <pwendell@gmail.com>,
        ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>","Feedback coming, sorry been swamped and only recently back from DC/DARPA
but will reply soon (hopefully tonight).

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++









"
Roman Shaposhnik <rvs@apache.org>,"Fri, 6 Sep 2013 14:00:42 -0700",Re: Needs a matrix library,dev@spark.incubator.apache.org,"
as well: http://giraph.apache.org/

Thanks,
Roman.

"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 6 Sep 2013 21:13:38 +0000",Re: Needs a matrix library,"Roman Shaposhnik <rvs@apache.org>,
        ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>","Thanks Roman, I was thinking Giraph too (knew it supported graphs but
wasn't sure it supported matrices). If Giraph supports matrices, big +1.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++








"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Sep 2013 14:14:44 -0700",Re: Spark 0.8.0-incubating RC2,"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Thanks Chris - also it appears that my key has now been added to this file:

http://people.apache.org/keys/group/spark.asc

- Patrick


"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 6 Sep 2013 21:15:20 +0000",Re: Spark 0.8.0-incubating RC2,Patrick Wendell <pwendell@gmail.com>,"Awesome was going to tell you it might take a sec to sync. Woot.

OK more tonight..

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++









"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 6 Sep 2013 14:37:56 -0700",Re: Spark 0.8.0-incubating RC2,Patrick Wendell <pwendell@gmail.com>,"HI Patrick, yeah feedback is coming. A bit swamped going to weekend,
sorry about it =(

And yeah the key should be added to groups later when LDAP sync happen.

- Henry


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 6 Sep 2013 14:38:42 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 to Chris' comment =)


"
Evan Chan <ev@ooyala.com>,"Fri, 6 Sep 2013 14:49:18 -0700",Fair scheduler documentation,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Are we ready to document the fair scheduler?    This section on the
standalone docs seems out of date....

# Job Scheduling

The standalone cluster mode currently only supports a simple FIFO scheduler
across jobs.
However, to allow multiple concurrent jobs, you can control the maximum
number of resources each Spark job will acquire.
By default, it will acquire *all* the cores in the cluster, which only
makes sense if you run just a single
job at a time. You can cap the number of cores using
`System.setProperty(""spark.cores.max"", ""10"")` (for example).
This value must be set *before* initializing your SparkContext.


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Konstantin Boudnik <cos@apache.org>,"Fri, 6 Sep 2013 15:03:02 -0700",Re: Spark 0.8.0-incubating RC2,dev@spark.incubator.apache.org,"Guys,

how about switching master to 0.9-SNAPSHOT to avoid confusion with two
branches producing same version of the different artifacts?

    https://github.com/mesos/spark/pull/902

Cos


"
Konstantin Boudnik <cos@apache.org>,"Fri, 6 Sep 2013 15:04:46 -0700",Re: Needs a matrix library,dev@spark.incubator.apache.org,"Would it be more logical to use GraphX ?
  https://amplab.cs.berkeley.edu/publication/graphx-grades/

Cos


"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Fri, 6 Sep 2013 15:13:10 -0700",Re: Needs a matrix library,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","keep forgetting this: what is graphx release roadmap?


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Sep 2013 15:19:59 -0700",Re: Fair scheduler documentation,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Matei mentioned to me that he was going to write docs for this. Matei,
is that still your intention?

- Patrick


"
hilfi alkaff <hilfialkaff@gmail.com>,"Fri, 6 Sep 2013 17:28:11 -0500",RDD placement,dev@spark.incubator.apache.org,"Hi,

the transformation. My question is, if the available bandwidth between
machines are highly varying, will this highly impact the performance?

-- 
~Hilfi Alkaff~
"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 6 Sep 2013 15:55:27 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Cos,

I just replied to your pull request to bump to 0.9-SNAPSHOT versioning =)

I think new development pulls should be made against ASF git repo. The
repo is open for pull requests now.

- Henry


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 6 Sep 2013 19:07:06 -0400",Re: Fair scheduler documentation,dev@spark.incubator.apache.org,"Yup, expect to see a pull request soon.

Matei


scheduler
maximum
only
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>


"
Reynold Xin <rxin@cs.berkeley.edu>,"Sat, 7 Sep 2013 08:09:08 +0800",Re: Needs a matrix library,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
        ""dev@sis.apache.org"" <dev@sis.apache.org>","They are asking about dedicated matrix libraries.

Neither GraphX nor Giraph are matrix libraries. These are systems that
handle large scale graph processing, which could possibly be modeled as
matrix computations.  Hama looks like a BSP framework, so I am not sure if
it has anything to do with matrix library either.

For very small matrices (3x3, 4x4), the cost of going through jni to do
native matrix operations will likely dominate the computation itself, so
you are probably better off with a simple unrolled for loop in Java.

I haven't looked into this myself, but I heard mahout-math is a decent
library.

--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org




"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Fri, 6 Sep 2013 17:25:14 -0700",Re: Needs a matrix library,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
Ah. I did not read the quoted email. Not sure why Chris was talking
about Pregel stuff, that doesn't seem what that question was about.


+1

+1 i guess this note is about JBlas and JBlas-based derivatives like Breeze


+1 although for such tiny things like 3x3, 4x4  our cost-based
optimizations are probably not going to provide any noticeable bang.
Mahout in-core math is mostly for uniform cost-optimized support of
sparse vectors along with dense.

Also, see if this makes sense, we are leaning towards commiting these
scala mappings in the current Mahout's trunk :[1]

[1] http://weatheringthrutechdays.blogspot.com/2013/07/scala-dsl-for-mahout-in-core-linear.html

-Dmitriy


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 6 Sep 2013 17:26:03 -0700",Re: Needs a matrix library,dev@spark.incubator.apache.org,"For the machine learning library that is a part of Spark 0.8 we have been
using jblas for local matrix operations. From some limited benchmarking
that we did, jblas is not much slower than optimized C++ libraries.
http://blog.mikiobraun.de/2009/04/some-benchmark-numbers-for-jblas.html has
some more details.

For more complex operations than addition and multiplication, mahout-math
is a pretty good library. There was a great discussion on pros/cons of
different Java/Scala-based matrix libraries in
https://github.com/mesos/spark/pull/736

Thanks
Shivaram



"
Konstantin Boudnik <cos@apache.org>,"Fri, 6 Sep 2013 18:18:51 -0700",Re: Spark 0.8.0-incubating RC2,dev@spark.incubator.apache.org,"As I mentioned in my reply back to you, current ASF git repo is about 4 days
behind of github, so I beleive there's going to be another merge ;)

Cos


"
Mike <spark@good-with-numbers.com>,"Sat, 7 Sep 2013 01:35:30 +0000",Re: Needs a matrix library,dev@spark.incubator.apache.org,"Why do we have spark.util.Vector?  Should it be replaced by jblas?

"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 7 Sep 2013 01:41:07 +0000",Re: Needs a matrix library,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
        ""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Thank you Shivaram!

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






Cc: ""dev@sis.apache.org"" <dev@sis.apache.org>
Subject: Re: Needs a matrix library



"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 7 Sep 2013 01:41:46 +0000",Re: Needs a matrix library,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks guys, just sharing a need here. SIS is a fully Java based
geospatial library in development at Apache, aiming to support OGC
standards. It would be great to figure out some synergy between Spark/Shark
and SIS..

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++









"
Adam Estrada <estrada.adam@gmail.com>,"Fri, 6 Sep 2013 23:27:14 -0400",Re: Needs a matrix library,dev@sis.apache.org,"I agree with that sentiment, Dr. Mattmann! It would be extremely cool to
see the distributed computation communities (eg. Spark and Hadoop) take
advantage of SIS. This is especially true for processing geospatial vector
data. Geospatial raster data is very splittable which makes it ideal for
this type of batch processing. Vector data is another beast all together
and I encourage folks in the aforementioned communities to think about how
to do this. I certainly have ideas and am all ears if someone would like to
chat about it!

Regards,
Adam



"
Adam Estrada <estrada.adam@gmail.com>,"Fri, 6 Sep 2013 23:41:14 -0400",Re: Needs a matrix library,"""dev@sis.apache.org"" <dev@sis.apache.org>, shivaram@eecs.berkeley.edu","+1 to jblas. It has a BSD license though so it might not be compatible with
the Apache v2 license. Anyone else want to weigh in on that?

Adam



"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 7 Sep 2013 04:18:06 +0000",Re: Needs a matrix library,"""dev@sis.apache.org"" <dev@sis.apache.org>,
        ""shivaram@eecs.berkeley.edu""
	<shivaram@eecs.berkeley.edu>","BSD is compatible with ALv2 per:

http://www.apache.org/legal/3party.html#category-a


It's a Category A.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






Subject: Re: Needs a matrix library



"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 7 Sep 2013 05:36:52 +0000",Re: [Licensing check] Spark 0.8.0-incubating RC1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Guys there are Github hooks set up by Jukka Zitting and others
in ASF infra that will monitor the ASF mirror on Github, and then
bring pull requests back to list as emails with links to the patches
for inclusion.

Please contact infra folks if there are questions in getting it set
up and it may require an INFRA ticket.

HTH!

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++








"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 7 Sep 2013 00:14:38 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","HI Patrick and all,

Here are my inputs and from my experience as release engineer for one
of incubating podling release before:
1. I only see source artifacts in Patrick's p.a.o URL. I assume the
pre-built ones will also be published with hash and signed?
2. For every ASF release, we need designated release engineer (RE)
that will drive the release process including determining bugs to be
included, make sure all files have the right ASF header (running maven
RAT plugin check), create release branch, update version for next
development, create release artifacts and sign them correctly. I
assume this would be Matei or Patrick?
3. The proposed source artifacts 0.8.0-RC2's signature looks good and
hash looks good. However it was generated against github mesos:spark
repo.
    Reminder that when we send proposal for release to
general@incubator.a.o we need to generate RC builds using ASF git repo
with the right tagged branch.
4. I ran RAT check for the source artifact and found a lot of source
do not have ASF license header.

 For example some in repl directory has this:

/* NSC -- new Scala compiler
 * Copyright 2005-2011 LAMP/EPFL
 * @author Paul Phillips
 */

Not sure if we need to ASF header to it since we are technically put
in under apache package.

Scala source files under mllib are missing ASF headers.


5. Add public key of RE to
http://people.apache.org/keys/group/spark.asc (@Chris do we still need
to create KEYS file in the Spark git repo?)

proposed RC build/artifacts through VOTE thread. If we got no -1 then
we could bring it to general@ncubator.a.o list for VOTE from IPMCs.

Other mentors please do definitely chime in about it.


Thanks,

Henry



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 7 Sep 2013 10:12:14 -0400",Re: Spark 0.8.0-incubating RC2,dev@spark.incubator.apache.org,"Hi Henry,


Regarding this, it's a third-party library that we included and modified under a different license. It's listed in LICENSE.

Thanks for the feedback otherwise! I'm also working on a guide on how to contribute via the Apache repo but I've been traveling the past few days and am still away today.

Matei


http://people.apache.org/~pwendell/spark-rc/spark-0.8.0-src-incubating-RC2.tgz


"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 7 Sep 2013 07:40:05 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Matei, travel safely.

I saw it was in the LICENSE but the reason why I am asking bc the package
name is changed for the repl code so I was wondering if we need to slap
additional ASF header.


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 7 Sep 2013 11:24:09 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Henry,

Thanks a lot for your feedback.

Could you let me know how you ran Apache RAT tool so I can reproduce this?

My sense is that the best ""next step"" is to do a RC that is built
against the Apache Git and also includes both `src` and `bin` in
addition to cleaned up license files. Some inline responses below.


Yes, we'll do both src and binary releases. I'll hash, and sign both.


Yes, this might be me for this release because I've got the keys
correctly set-up. I'll chat with Matei when he's back.


Next RC we will take care of this.


See comment above.


This is now finished for me :)

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 7 Sep 2013 16:46:59 -0400",Re: Spark 0.8.0-incubating RC2,dev@spark.incubator.apache.org,"I don't think we're allowed to just slap on a license header since it's mostly their code, but I'm not sure. Maybe you can ask someone who's done this before. I though it was best to use and report it under the terms of their license. Which package name it's in doesn't matter -- it's still substantially the same code.

Matei


package
slap
modified
to
days
need
the
then
<pwendell@gmail.com<javascript:;>>
http://people.apache.org/~pwendell/spark-rc/spark-0.8.0-src-incubating-RC2.tgz
spark.asc
(at


"
Roman Shaposhnik <rvs@apache.org>,"Sat, 7 Sep 2013 23:05:11 -0700",Re: Incubator PMC/Board report for Sep 2013 ([ppmc]),private@spark.incubator.apache.org,"
Ditto!

Thanks,
Roman.

P.S. Not to jinx, but you guys are one of the model incubating
projects out there!

"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 7 Sep 2013 23:25:46 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","HI Patrick,

I ran the Apache RAT tool as shown at
http://creadur.apache.org/rat/apache-rat/index.html:

java -jar apache-rat-0.10.jar ~/Downloads/spark-0.8.0-src-incubating-RC2

However we should add maven plugin to Spark pom.xml to support
integrated RAT check as part of CI later.

- Henry


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sun, 8 Sep 2013 19:58:16 +0200",Adding support for implicit feedback to ALS,dev@spark.incubator.apache.org,"Hi

I know everyone's pretty busy with getting 0.8.0 out, but as and when folks
have time it would be great to get your feedback on this PR adding support
for the 'implicit feedback' model variant to ALS:
https://github.com/apache/incubator-spark/pull/4

In particular any potential efficiency improvements, issues, and testing it
out locally and on a cluster and on some datasets!

Comments & feedback welcome.

Many thanks
Nick
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Sep 2013 11:13:30 -0700",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Henry. The MLLib files have been fixed since you ran the tool.


"
Martin Desruisseaux <martin.desruisseaux@geomatys.fr>,"Sun, 08 Sep 2013 22:23:55 +0200",Re: Needs a matrix library,dev@sis.apache.org,"Thanks all for the tips. So if I'm summarizing right:

  * Hama and Spark are designed for distributed computing. Given that
    our need is for small matrices (usually no more than 5x5),
    distributed computing would probably be too much. However I keep
    Hama and Spark in mind for the SIS ""Grid Coverage"" (or Raster)
    processing part, to come later.
  * JBlas seems to be JNI wrappers around LAPACK and BLAS Fortran
    libraries. For small matrix, the JNI cost may be larger than the
    benefit. I will keep JBlas in mind for some computations that
    require large matrix, but those computations are not expected to
    occur in the ""referencing"" part of SIS.
  * Other libraries under compatible license include Apache Commons Math
    [1] and JAMA [2].


Given that Apache Commons is a large library (the JAR file is 1.6 Mb) 
while JAMA is very small and focused on Matrix only (a 36.5 kb file), I 
would be tempted to propose JAMA. Is there any though on that?


     Martin


[1] http://commons.apache.org/proper/commons-math/
[2] http://math.nist.gov/javanumerics/jama/

"
Mike <spark@good-with-numbers.com>,"Sun, 8 Sep 2013 21:14:14 +0000",Re: Needs a matrix library,dev@spark.incubator.apache.org,"
According to http://mikiobraun.github.io/jblas/javadoc/ that's not true 
for operations that are faster to perform in the JVM.

"
Chris Mattmann <mattmann@apache.org>,"Sun, 08 Sep 2013 15:38:01 -0700",Re: Needs a matrix library,"""dev@sis.apache.org"" <dev@sis.apache.org>","Hi Martin,

OK, so looking at the license for JAMA:

http://wordhoard.northwestern.edu/userman/thirdparty/jama.html

and 
http://muuki88.github.io/jama-osgi/license.html


Looks like the later is ALv2 licensed. So JAMA looks good to me,
too.

Cheers,
Chris

Cc: ""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>
Subject: Re: Needs a matrix library




"
Adam Estrada <estrada.adam@gmail.com>,"Sun, 8 Sep 2013 20:32:26 -0400",Re: Needs a matrix library,"""dev@sis.apache.org"" <dev@sis.apache.org>","Yep. JAMA looks good to me as well. I am not all that familiar with it and
think that JBLAS would be good too but that seems like it would add quite a
few unneeded dependencies.

Adam



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 8 Sep 2013 21:36:37 -0700",Re: Google Groups: You've been added to Spark Developers,Henry Saputra <henry.saputra@gmail.com>,"Yeah, unfortunately I think this way of doing mirroring won't work. Give me a few days and I'll start closing the Google groups and getting people here (starting with the dev list first).

Matei



they did not show up in the ASF dev@ list.


"
Martin Desruisseaux <martin.desruisseaux@geomatys.fr>,"Mon, 09 Sep 2013 10:09:32 +0200",Re: Needs a matrix library,dev@sis.apache.org,"Le 09/09/13 02:32, Adam Estrada a crit :

I think that JBLAS could play a role a little bit later. For example 
when drawing a contour map from a set of stations at random locations 
(not on a grid), one method - among others - involve inverting a matrix 
having as many rows and columns than stations. For such large matrix, 
the industrially-proven LAPACK algorithms against numerical 
instabilities would be very appreciated.

But in the particular case of the sis-referencing module, the matrix 
size is the number of spatio-temporal dimensions + 1. For this reason I 
expect matrix bigger than 5x5 to be quite unusual (while possible - for 
example meteorologists use 2 time axes). For such small matrix, I think 
that JAMA would be perfect. From my reading of javadoc, JBLAS uses pure 
Java for O(1) operations like addition and multiplication by a vector, 
but still uses native code for multiplication by an other matrix and 
inversion (which we need).

Thanks all for your feedbacks!

         Martin


"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 9 Sep 2013 09:52:50 -0700",Good tips about doing a release as Apache incubator podling,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Marvin put a good checklist to help getting VOTES from IPMC in
genera@incubator.a.o list.

Just want to forward this to Spark community.

- Henry


---------- Forwarded message ----------
From: Marvin Humphrey <marvin@rectangular.com>
Date: Mon, Sep 9, 2013 at 9:22 AM
Subject: [DISCUSS] Release of Apache Allura (incubating) v1.0.0
To: ""general@incubator.apache.org"" <general@incubator.apache.org>



Perhaps not this one, though the voting on allura-dev@incubator was somewhat
irregular.

*   No ""[VOTE]"" in the subject.
*   Spread out over multiple threads.
*   No time specification.  (I recommend the phrase ""at least 72 hours"".)
*   PPMC votes claimed as ""binding"", which is ambiguous.

So long as the IPMC VOTE clears, though, those irregularities don't block the
release IMO.

I'd also like to note that the dev list archives for Allura are time-consuming
and tedious to plow through -- the signal-to-noise ratio is poor due to the
large number of auto-generated messages with trivial content.


Allura has four Mentors.  You've voted, but where are the others?

Mentors must lead the way, particularly for the first release.  ""Freelance""
reviews of release artifacts, by IPMC members who are not following the
podling's development, are by their nature superficial.  For instance, a
freelancer can run RAT and see whether there are files with missing ALv2
headers, but can't see whether files with ALv2 headers had them installed
appropriately.  We count on Mentors to endorse the podling's initial IP
handling, from supervising the code grant to monitoring the dev list and
commits list day-by-day and ensuring that everything is proper.

After the first release, we are voting on a delta, and all new changes have
happened within Apache channels which are comparatively more auditable.
However, for the initial incubating release, we are voting on development
which took place elsewhere, and Mentors have better insight than the rest of
the IPMC into the importation and assimilation of that dark matter into
Apache.


Getting enough IPMC votes for incubating releases is an age-old issue for the
Incubator.  Many long-term remedies have been discussed, but none of that will
help the acute problem faced by Allura.

In today's Incubator, the most effective strategy for an individual podling to
take is for its core contributors to become serious experts about Apache IP
and release policy and to present squeaky clean release candidates which make
a best effort to follow all known rules and guidelines.  In Allura's case, not
only would it help to run the dev list VOTEs more cleanly, but it would help
if PPMC members who vote +1 document exactly what steps they took to validate
the release candidate.

It's nice to see a list like this accompanying a +1 vote:

    *   Sums and sigs OK (log below).
    *   Build from source tarball succeeds and passes tests on [list
        platforms].
    *   Extended tests pass on [list platforms].
    *   RAT build target passes.
    *   Tarball name contains ""incubating"".
    *   Incubation DISCLAIMER included.
    *   Expanded tarball matches version control tag exactly (diff log below).
    *   LICENSE and NOTICE assembled according to
        <http://www.apache.org/dev/licensing-howto.html> per discussion at
        [link].
    *   LICENSE and NOTICE up-to-date, as no dependencies have been added
        since initial assembly.
    *   All copyleft dependencies purged as documented at [issue].
    *   Copyright date in NOTICE is current.
    *   CHANGES entry is current.
    *   Issue tracker clean (no open issues for this release).
    ...

Documented diligence by podling contributors lowers the cost of reviewing and
voting for Mentors and other IPMC members, and may help to persuade those
hanging back to participate.

Marvin Humphrey

---------------------------------------------------------------------

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 9 Sep 2013 10:07:08 -0700",Re: Adding support for implicit feedback to ALS,dev@spark.incubator.apache.org,"Cool, thanks for posting this! We'll probably merge it after the 0.8.0 release since it's a big change.

Matei


folks
support
testing it


"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Mon, 9 Sep 2013 11:28:26 -0700",Re: Adding support for implicit feedback to ALS,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sorry, not directly aimed at the PR but at implementation in whole.
See if the following is useful from my experience:

1. implicit feedback is just a corner case of more general problem:
Given preference matrix P where P_i,j in R^{0,1} and weight
(confidence)  matrix C, C_i,j \in R, and reg rate \lambda, compute
L2-regularized ALS fit.

2. since default confidence is never zero (in paper it is assumed 1,
and i will denote this real quantity as c_0), have C = C_0 + C' where
C_0_i,j = c_0. Hence, rewrite input  in terms (P, C', c_0) since C'
becomes severely sparse matrix in this case in real life.

3. It is nice when input C is known. But there are a lot of cases
where individual confidence is derived from a final set of
hyperparameters corresponding to a particular event type (search,
click, transaction etc.). Hence, convex optimization for a small set
of hyperparameters is desired (this might be outside of scope ALS
itself, but weighing and lamda per se  aren't). Still though,
crossvalidation largely relies on the fact that we want to take stuff
that follows existing entries in C' so crossvalidation helpers would
be naturally coupled with this method and should be provided.

4. i actually used pregel to avoid shuffle and sort programming model.
Matrix operations do not require guarantees produced by reducers; only
a full group guarantee. I did not benchmark this approach for really
substantial datasets though; there are known Bagel limitations IMO
which may create a problem for sufficiently large /skewed datasets. I
guess I am interested in GraphX release to replace reliance on Bagel.

5. if the task reformulation is accepted, there are further
optimizations that could be applied to blocking -- but this
implementation gets the gist of it what i did in that regard.




"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Mon, 9 Sep 2013 11:44:11 -0700",Re: Adding support for implicit feedback to ALS,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","6. It looks like convergence test is not implemented? instead of # of
iterations, i'd rather have a parameter of maximum # of iterations and
something like RMSE % fall-off to stop. I have tons of cases where
4-th iteration alreday does not improve train rmse by more than 3%.
Hyperparameter search quite often becomes more important than this and
this provides bona fide speed up in these cases (especially if you
work for a company that sees a lot of diverse datasets and you need to
figure applicability and event weighs quickly and often).



"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Mon, 09 Sep 2013 11:45:46 -0700 (PDT)",Re: Adding support for implicit feedback to ALS,dev@spark.incubator.apache.org,"In 3 are you saying that some cross validation support for picking the best lambda and alpha should be in there? Or that also the preference ""weightings"" of different event types should also be learnt? (Maybe both)

    
      


    I agree that there should be support for this, by optimising for the best RMSE, MAP or whatever. I'm just not sure whether this functionality should live in Mllib or MLI. Until MLI is released it's sort of hard to know.

    
      


    For 4, my frame of reference has been vs mahout and my own port to spark of mahouts ALS, and vs those this blocked approach is far superior. Though im sure there can be more efficiencies gained in this approach and other alternatives.

    
      


    It would certainly be great to further improve the approach as you mention in 5. I'm not sure precisely what you mean by task reformulation - how would you propose to do so?

    
      


    Nick

    
Sent from Mailbox for iPhone


where
since C'
folks
support
 it"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Mon, 9 Sep 2013 11:51:25 -0700",Re: Adding support for implicit feedback to ALS,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","st lambda and alpha should be in there? Or that also the preference ""weightings"" of different event types should also be learnt? (Maybe both)

Maybe, but helpers need to be able to provide crossvalidation support
in terms of forming K-fold sets that follow non-sparse entries in C'.
actual stepping thru hyperparameter generations -- i'd leave it
outside the scope.

best RMSE, MAP or whatever. I'm just not sure whether this functionality should live in Mllib or MLI. Until MLI is released it's sort of hard to know.
ark of mahouts ALS, and vs those this blocked approach is far superior. Though im sure there can be more efficiencies gained in this approach and other alternatives.
Yes. Blocking is superior. (Mahout's way of broadcasting the entire U
or M matrices is IMO is a little bit too unsophisticated, i have seen
that.
ntion in 5. I'm not sure precisely what you mean by task reformulation - how would you propose to do so?

Task reformulation means reformulation of the input spec.

namely, for training in this case I'd want to have P, C', c_0, lambda
and some convergence test parameter (e.g. % rmse fall-off, i use 5% by
default)


olks
ort
g it

"
Andy Konwinski <andykonwinski@gmail.com>,"Mon, 9 Sep 2013 11:51:49 -0700",Re: Good tips about doing a release as Apache incubator podling,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Henry,

This was really helpful. Thanks for forwarding. It sounds like it will give
us a leg up if community members take time to go through the checklist
Marvin included towards the end of his email (that you forwarded) and
verify all of the components of the release.

Andy



"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 9 Sep 2013 12:59:46 -0700",Re: Good tips about doing a release as Apache incubator podling,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Andy, glad it is helpful.

We want to make sure the transition to ASF have less turbulence and
make it less frustrating process for newcomers to ASF.
I am expecting more contributions and adoptions with Apache Spark once
it is fully move to  ASF repository.


- Henry



"
Tsuyoshi OZAWA <ozawa.tsuyoshi@gmail.com>,"Tue, 10 Sep 2013 11:22:24 +0900",Re: Spark 0.8.0-incubating RC2,dev@spark.incubator.apache.org,"Hi Patrick,

I ran same points which I reported on my environment(OS X 10.8.4 and
HotSpotVM 1.6.0_37)
when RC1 was released:

1. sbt/sbt assembly works correctly.
2. sbt/sbt package works correctly.
2. sbt/sbt test works correctly(its results are green).
4. ./run-example org.apache.spark.examples.SparkPi local 1000

LGTM for releasing.
Thanks, Tsuyoshi





-- 
- Tsuyoshi

"
Tsuyoshi OZAWA <ozawa.tsuyoshi@gmail.com>,"Tue, 10 Sep 2013 11:36:14 +0900",A question about JIRA for spark project,dev@spark.incubator.apache.org,"Hi,

Spark project has 2 JIRAs currently:
1. original one(https://spark-project.atlassian.net/browse/SPARK)
2. Apache's one(https://issues.apache.org/jira/browse/SPARK)

Which should I use when I contribute?

Thanks
- Tsuyoshi

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 9 Sep 2013 20:16:51 -0700",Re: A question about JIRA for spark project,dev@spark.incubator.apache.org,"Please use 1 for now, because Apache Infra hasn't finished importing our issues into 2.

Matei




"
Tsuyoshi OZAWA <ozawa.tsuyoshi@gmail.com>,"Tue, 10 Sep 2013 12:18:30 +0900",Re: A question about JIRA for spark project,dev@spark.incubator.apache.org,"OK, thank you for sharing.

Thanks, Tsuyoshi




-- 
- Tsuyoshi

"
Mingxi Wu <send2mingxiwu@gmail.com>,"Mon, 9 Sep 2013 22:53:42 -0700",how to debug spark core code?,dev@spark.incubator.apache.org,"Hi,

I wonder if there is a convenient way to debug source code of spark
from a repl test case?

Was the spark core code developed under an IDE or using println()?

thanks,

Mingxi
"
Cesar Arevalo <cesar@cesararevalo.com>,"Mon, 9 Sep 2013 23:08:18 -0700",Re: how to debug spark core code?,dev@spark.incubator.apache.org,"Hi Mingxi,

I think it usually comes down to what IDE you're most comfortable with.
Here is an article describing how to set up spark on eclipse:

http://syndeticlogic.net/?p=311

Best,
-Cesar




"
Mingxi Wu <send2mingxiwu@gmail.com>,"Mon, 9 Sep 2013 23:23:38 -0700",Re: how to debug spark core code?,dev@spark.incubator.apache.org,"thanks Cesar.



"
Reynold Xin <reynoldx@gmail.com>,"Tue, 10 Sep 2013 15:33:08 +0800",Re: how to debug spark core code?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Among the folks in Berkeley, most of us use IntelliJ / Vim / Sublime Text.

You can generate the IntelliJ project for Spark using

sbt/sbt gen-idea



"
prabeesh k <prabsmails@gmail.com>,"Tue, 10 Sep 2013 14:16:02 +0530",request to subscribe,dev@spark.incubator.apache.org,"
"
Marvin <no-reply@apache.org>,"Tue, 10 Sep 2013 14:21:29 +0000 (UTC)",Incubator PMC/Board report for Sep 2013 ([ppmc]),dev@spark.incubator.apache.org,"

Dear podling,

This email was sent by an automated system on behalf of the Apache Incubator PMC.
It is an initial reminder to give you plenty of time to prepare your quarterly
board report.

The board meeting is scheduled for Wed, 18 September 2013, 10:30:00:00 PST. The report 
for your podling will form a part of the Incubator PMC report. The Incubator PMC 
requires your report to be submitted 2 weeks before the board meeting, to allow 
sufficient time for review and submission (Wed, Sep 4th).

Please submit your report with sufficient time to allow the incubator PMC, and 
subsequently board members to review and digest. Again, the very latest you 
should submit your report is 2 weeks prior to the board meeting.

Thanks,

The Apache Incubator PMC

Submitting your Report
----------------------

Your report should contain the following:

 * Your project name
 * A brief description of your project, which assumes no knowledge of the project
   or necessarily of its field
 * A list of the three most important issues to address in the move towards 
   graduation.
 * Any issues that the Incubator PMC or ASF Board might wish/need to be aware of
 * How has the community developed since the last report
 * How has the project developed since the last report.
 
This should be appended to the Incubator Wiki page at:

  http://wiki.apache.org/incubator/September2013

Note: This manually populated. You may need to wait a little before this page is
      created from a template.

Mentors
-------
Mentors should review reports for their project(s) and sign them off on the 
Incubator wiki page. Signing off reports shows that you are following the 
project - projects that are not signed may raise alarms for the Incubator PMC.

Incubator PMC


"
Michael Joyce <joyce@apache.org>,"Tue, 10 Sep 2013 07:36:25 -0700",Re: request to subscribe,dev@spark.incubator.apache.org,"Hello!

If you'd like to subscribe to the list you'll need to send an email to
dev-subscribe@spark.incubator.apache.org instead of just dev@.

Cheers!


-- Joyce



"
Gowtham N <gowtham.n.mail@gmail.com>,"Tue, 10 Sep 2013 11:37:20 -0700",MLI dependency exception,dev@spark.incubator.apache.org,"I cloned MLI, but am unable to compile it.

I get the following dependency exception with other projects.

org.apache.spark#spark-core_2.9.3;0.8.0-SNAPSHOT: not found
org.apache.spark#spark-mllib_2.9.3;0.8.0-SNAPSHOT: not found

Why am I getting this error?

I did not change anything from build.sbt

libraryDependencies ++= Seq(
  ""org.apache.spark"" % ""spark-core_2.9.3"" % ""0.8.0-SNAPSHOT"",
  ""org.apache.spark"" % ""spark-mllib_2.9.3"" % ""0.8.0-SNAPSHOT"",
  ""org.scalatest"" %% ""scalatest"" % ""1.9.1"" % ""test""
)
"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 10 Sep 2013 11:58:51 -0700",Re: MLI dependency exception,dev@spark.incubator.apache.org,"Hi Gowtham,

You'll need to do ""sbt/sbt publish-local"" in the spark directory
before trying to build MLI.

- Evan


"
Gowtham N <gowtham.n.mail@gmail.com>,"Tue, 10 Sep 2013 13:01:32 -0700",Re: MLI dependency exception,dev@spark.incubator.apache.org,"still getting the same error.

I have spark and MLI folder within a folder called git

I did clean, package and public-local for spark.
Then for mli did clean, and then package.
I am still getting the error.

[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] ::          UNRESOLVED DEPENDENCIES         ::
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] :: org.apache.spark#spark-core_2.9.3;0.8.0-SNAPSHOT: not found
[warn] :: org.apache.spark#spark-mllib_2.9.3;0.8.0-SNAPSHOT: not found
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[error] {file:/Users/gowthamn/git/MLI/}default-0b9403/*:update:
sbt.ResolveException: unresolved dependency:
org.apache.spark#spark-core_2.9.3;0.8.0-SNAPSHOT: not found
[error] unresolved dependency:
org.apache.spark#spark-mllib_2.9.3;0.8.0-SNAPSHOT: not found

should I modify the contents of build.sbt?
Currently its

libraryDependencies ++= Seq(
  ""org.apache.spark"" % ""spark-core_2.9.3"" % ""0.8.0-SNAPSHOT"",
  ""org.apache.spark"" % ""spark-mllib_2.9.3"" % ""0.8.0-SNAPSHOT"",
  ""org.scalatest"" %% ""scalatest"" % ""1.9.1"" % ""test""
)

resolvers ++= Seq(
  ""Typesafe"" at ""http://repo.typesafe.com/typesafe/releases"",
  ""Scala Tools Snapshots"" at ""http://scala-tools.org/repo-snapshots/"",
  ""ScalaNLP Maven2"" at ""http://repo.scalanlp.org/repo"",
  ""Spray"" at ""http://repo.spray.cc""
)










-- 
Gowtham Natarajan
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 10 Sep 2013 13:10:59 -0700",Re: MLI dependency exception,dev@spark.incubator.apache.org,"Did you check out spark from the master branch of github.com/mesos/spark ?
The package names changed recently so you might need to pull. Also just
checking that you did publish-local in Spark (not public-local as specified
in the email) ?

Thanks
Shivaram



"
Gowtham N <gowtham.n.mail@gmail.com>,"Tue, 10 Sep 2013 13:14:25 -0700",Re: MLI dependency exception,"dev@spark.incubator.apache.org, shivaram@eecs.berkeley.edu","I did it as publish-local.
I forked mesos/spark to gowthamnatarajan/spark. And I am using that. I
forked a few days ago, but did a upstream update today.

For safety, I will directly clone from mesos now.







-- 
Gowtham Natarajan
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 10 Sep 2013 13:25:15 -0700",Re: MLI dependency exception,Gowtham N <gowtham.n.mail@gmail.com>,"For some more notes on how to debug this: After you do publish-local in
Spark, you should have a file in ~/.ivy2 that you can check for using
`ls
~/.ivy2/local/org.apache.spark/spark-core_2.9.3/0.8.0-SNAPSHOT/jars/spark-core_2.9.3.jar`

Or `sbt/sbt publish-local` also prints something like this on the console

 [info]  published spark-core_2.9.3 to
/home/shivaram/.ivy2/local/org.apache.spark/spark-core_2.9.3/0.8.0-SNAPSHOT/jars/spark-core_2.9.3.jar

After that MLI's build should be able to pick this jar up.

Thanks
Shivaram





"
Gowtham N <gowtham.n.mail@gmail.com>,"Tue, 10 Sep 2013 13:44:27 -0700",Re: MLI dependency exception,shivaram <shivaram@eecs.berkeley.edu>,"It worked.
I was using old master for spark, which I forked many days a ago.





-- 
Gowtham Natarajan
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 10 Sep 2013 16:32:26 -0700",Port 3030 conflict,dev@spark.incubator.apache.org,"I should have tracked it down earlier when I started seeing persistent
UISuite failures, but I finally got around to looking at it today.  We've
got an annoying port conflict problem.  We're setting SparkUI.DEFAULT_PORT
to 3030, which is the same port that Typesafe has chosen as the default
Nailgun port for Zinc server.  The upshot is that if you are a developer
using the Zinc server and trying to work on Spark, then you are going to
see port conflicts if you don't change at least one of the default
assignments.

Either we need to pick a new default spark.ui.port and change all of the
docs references from 3030 to the new port, or we need to at least add a
note to the developer docs to make developers aware of the conflict.

https://spark-project.atlassian.net/browse/SPARK-889
"
Konstantin Boudnik <cos@apache.org>,"Tue, 10 Sep 2013 16:49:21 -0700",Re: Port 3030 conflict,dev@spark.incubator.apache.org,"I think you've meant 
  https://spark-project.atlassian.net/browse/SPARK-901
?

Cos


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 10 Sep 2013 16:51:33 -0700",Re: Port 3030 conflict,dev@spark.incubator.apache.org,"You're right -- bad cut and paste from the wrong open browser window.
 Sorry and thanks for the correction.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 10 Sep 2013 18:33:44 -0700",Re: Port 3030 conflict,dev@spark.incubator.apache.org,"Hmm, good point -- might be worth it to fix this for 0.8.0.

Matei




"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Wed, 11 Sep 2013 05:05:05 -0700 (PDT)",Re: MLI dependency exception,dev@spark.incubator.apache.org,"Is mLI available? Where is the repo located?

    
Sent from Mailbox for iPhone


0-SNAPSHOT/jars/spark-core_2.9.3.jar`
console
0-SNAPSHOT/jars/spark-core_2.9.3.jar
com/mesos/spark?
just
found
found
0-SNAPSHOT"",
0-SNAPSHOT"",
,
org/repo-snapshots/"",
directory
0-SNAPSHOT"",
0-SNAPSHOT"",
""test"""
Sam Bessalah <samkiller@gmail.com>,"Wed, 11 Sep 2013 14:50:50 +0200",Re: MLI dependency exception,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","http://github.com/amplab/MLI

Sam Bessalah

te:

k-core_2.9.3.jar`
e
HOT/jars/spark-core_2.9.3.jar
te:
rk?
t

d


"
Chris Mattmann <mattmann@apache.org>,"Wed, 11 Sep 2013 08:56:28 -0500",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Patrick,

In response to your question below, no we don't need a KEYS file anymore
if we are committed to using the .asc file from the Apache Spark group
area.

Cheers,
Chris





"
Chris Mattmann <mattmann@apache.org>,"Wed, 11 Sep 2013 08:58:49 -0500",Re: Spark 0.8.0-incubating RC2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
	Patrick Wendell <pwendell@gmail.com>","Hey Patrick,

Looking good. If the license info and so forth has been vetted and
looks good which it sounds like Henry and others have checked out,
I took a look at:

http://people.apache.org/~pwendell/spark-rc/

And the only thing I would recommend adding is some CHANGES.txt file
that contains a JIRA change log of what is provided in this RC.

But I would definitely proceed to a [VOTE] thread on the RC and let's
get this going formally.

Great work.

Cheers,
Chris


Subject: Re: Spark 0.8.0-incubating RC2




"
Patrick Wendell <pwendell@gmail.com>,"Wed, 11 Sep 2013 09:33:47 -0700",Re: Spark 0.8.0-incubating RC2,Chris Mattmann <mattmann@apache.org>,"Hey Chris,

The only issue with CHANGES.txt is that we've only recently become
more disciplined about tracking issues in JIRA and tracking version
numbers when we do make JIRA issues. If we generated a CHANGES.txt
based on JIRA, it would be largely incomplete since many changes from
the beginning of the release would be missing.

What about if I created a CHANGES.txt based on the Git history? Would
that be better than not having one at all?

- Patrick


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 11 Sep 2013 11:44:53 -0700",Re: Spark 0.8.0-incubating RC2,dev@spark.incubator.apache.org,"We could also manually write an overview of the high-level changes (similar to a set of release notes) if desired.

Matei


<dev@spark.incubator.apache.org>
<dev@spark.incubator.apache.org>,
<dev@spark.incubator.apache.org>,
this
<dev@spark.incubator.apache.org>
based
<pwendell@gmail.com>
the
by
<pwendell@gmail.com>
Apache
the
just
branch
http://people.apache.org/~pwendell/spark-rc/spark-0.8.0-src-incubati
I
--send-key)
page
way to
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooya


"
Chris Mattmann <mattmann@apache.org>,"Wed, 11 Sep 2013 13:51:01 -0500",Re: Spark 0.8.0-incubating RC2,"Patrick Wendell <pwendell@gmail.com>,
	Chris Mattmann <mattmann@apache.org>","Hey Patrick,

Creating it based on git history would be fine, or writing it manually
would be fine, or an incomplete JIRA would be fine too :) Or some
combination of them just to let folks know what is in this release,
so we have something to baseline from.

Cheers!

Chris






"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 11 Sep 2013 15:04:08 -0700",Maven SCM trouble,dev@spark.incubator.apache.org,"I'm in the process of trying to build Apache-fied Spark into our stack at
ClearStory.  We've been using Maven to do Debian packaging as found in the
repl-bin module for quite a while, but that doesn't work now.  To see the
failure, you need to build repl-bin with the `deb` profile -- e.g. ""mvn
-Prepl-bin,deb install"" from the project directory.  The problem is with
the buildnumber-maven-plugin, which is used to attach the 8-digit short SHA
of the current git commit to the Debian package -- so, e.g.,
spark_0.8.0-SNAPSHOT-b993b2a3_all.deb.  The buildnumber plugin relies upon
the <scm> tag to determine which source code management system should be
used to lookup the version number.

That all worked great until the following was added to spark/pom.xml:

  <parent>
    <groupId>org.apache</groupId>
    <artifactId>apache</artifactId>
    <version>11</version>
  </parent>

The problem is that the parent pom that the above includes has this within
it:

  <scm>
    <connection>scm:svn:
http://svn.apache.org/repos/asf/maven/pom/tags/apache-11</connection
    <developerConnection>scm:svn:
https://svn.apache.org/repos/asf/maven/pom/tags/apache-11
</developerConnection>
    <url>http://svn.apache.org/viewvc/maven/pom/tags/apache-11</url>
  </scm>

And that essentially overrides the <scm> section of spark/pom.xml, which
means that the buildnumber plugin thinks that we are using SVN instead of
git, and the SHA lookup fails.

So, that is one known problem in the Maven build.  I'm not at all certain
that there aren't others or lurking problems because of different versions
of dependencies, plugins, etc. being included by that org.apache:apache
artifact and also specified within the Spark pom files.

I'm pretty sure that we can't not include the equivalent of
org.apache:apache:11, since there is a lot of important stuff in there to
do Apache release management, etc.  So, is there another, equivalent
artifact that we can use that doesn't confuse the <scm> issue, or some
other way around this problem?  It looks like there are some other
git-specific plugins out there that we may be able to use instead of
buildnumber-maven-plugin, so that may be a valid and necessary solution to
the Debian packaging problem.

Can someone who knows Maven better than I take a look at the overlap
between org.apache:apache and the Spark pom files to see if there are
problems lurking there?  Even if there aren't problems other than with the
buildnumber plugin, we may be able to trim a lot out of the Spark poms that
is already present in the apache parent pom.
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 11 Sep 2013 18:54:19 -0700",Mentors: question about Apache GitHub mirrors,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm not sure where to ask this, but here goes: since we'll receive pull requests on https://github.com/apache/incubator-spark, is there any way to subscribe the dev@ list (or some other list) to the GitHub emails from those discussions? I think that would be useful, and similar to subscribing it to JIRA. My main question is who manages the mirroring (i.e. the github.com/apache account), since they'd have to configure this.

Matei
"
Roman Shaposhnik <roman@shaposhnik.org>,"Wed, 11 Sep 2013 20:44:22 -0700",Re: Mentors: question about Apache GitHub mirrors,dev@spark.incubator.apache.org,"ote:
equests on https://github.com/apache/incubator-spark, is there any way to subscribe the dev@ list (or some other list) to the GitHub emails from those discussions? I think that would be useful, and similar to subscribing it to JIRA. My main question is who manages the mirroring (i.e. the github.com/apache account), since they'd have to configure this.

This is being managed by GitHub folks. I can ping
my connection to them to see if what you're asking
for is feasible. Stay tuned.

Thanks,
Roman.

"
Gowtham N <gowtham.n.mail@gmail.com>,"Wed, 11 Sep 2013 20:49:32 -0700",ip address for MLI,dev@spark.incubator.apache.org,"Hi,

When building spark in local mode I mention the ip address of my machine
inside the spark-env file inside conf folder.
Where do I mention it while running MLI?
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 11 Sep 2013 21:06:25 -0700",Re: Mentors: question about Apache GitHub mirrors,dev@spark.incubator.apache.org,"Awesome, thanks.

Matei


pull requests on https://github.com/apache/incubator-spark, is there any way to subscribe the dev@ list (or some other list) to the GitHub emails from those discussions? I think that would be useful, and similar to subscribing it to JIRA. My main question is who manages the mirroring (i.e. the github.com/apache account), since they'd have to configure this.


"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 11 Sep 2013 21:18:39 -0700",Re: Maven SCM trouble,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Mark,

But the spark main pom.xml contains <scm> element that should override
<scm> from the parent pom.xml.

Are you saying the pom.xml from parent Apache pom.xml overrides the
Spark <scm> definition instead?





"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Wed, 11 Sep 2013 21:25:21 -0700",Re: ip address for MLI,dev@spark.incubator.apache.org,"MLI is currently an application on top of spark - once spark is
properly configured, very little needs to be done to configure MLI.

The only place you should need to specify a network address in MLI is
when you create an MLContext - where you specify the same address as
the Spark Master you're using.


"
Gowtham N <gowtham.n.mail@gmail.com>,"Wed, 11 Sep 2013 21:28:30 -0700",Re: ip address for MLI,dev@spark.incubator.apache.org,"I am getting UnknownHostException when I do sbt/sbt assembly inside MLI.
I was getting the same exception in spark before I mentioned my ip address
inside the spark-env file.







-- 
Gowtham Natarajan
"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Thu, 12 Sep 2013 04:57:55 +0000",Re: Mentors: question about Apache GitHub mirrors,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Roman..

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Senior Computer Scientist
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-266B, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Assistant Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++








"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 11 Sep 2013 22:31:40 -0700",Re: Maven SCM trouble,dev@spark.incubator.apache.org,"Yes, exactly.  If I comment out the reference to the parent Apache pom,
then the buildnumber plugin works correctly.  Similarly if I leave the
parent Apache pom reference in place but go into my local .m2 cache and
comment out just the <scm> in the org.apache:apache pom.  In other words,
if both <scm>s are present, the buildnumber plugin only sees the SVN one
from the Apache pom, which makes things go wrong.



"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 11 Sep 2013 23:21:21 -0700",Re: Maven SCM trouble,dev@spark.incubator.apache.org,"Ah hah!  Patrick figured it out:
https://github.com/apache/incubator-spark/commit/905edf59db662868f55525118131cf102d366587.
 The buildnumber plugin and Debian packaging works correctly again.
 Thanks!  I'll go through the poms tomorrow and see if there are any other
such incompletely overriden elements that could be causing us troubles.

Right now I am seeing two problems with the maven build: 1) I just had a
build fail from exhausted PermGen, so we probably need to increase
MaxPermGen in spark/pom.xml; 2) Not all of the modules got picked up in the
recent invocations of the maven-release-plugin -- did your prepare &
release use -Phadoop2-yar,repl-bin , Patrick?




"
Gary Struthers <agilej2ee@earthlink.net>,"Wed, 11 Sep 2013 23:41:04 -0700",Re: Maven SCM trouble,dev@spark.incubator.apache.org,"It looks like scm in the pom is only used to upload artifacts to Maven Central and isn't needed for building. Explained here:
http://books.sonatype.com/nexus-book/reference/_introduction_7.html

If you run into a transitive dependency problem, there is a nice way to exclude them. Explained here:
http://books.sonatype.com/mvnref-book/reference/pom-relationships-sect-project-dependencies.html#pom-relationships-sect-conflict

By far the best Maven documentation is in Sonatype's free books. They're accurate and up to date, unlike the Apache Maven docs or StackOverflow.
http://www.sonatype.com/resources/books

Gary Struthers


https://github.com/apache/incubator-spark/commit/905edf59db662868f55525118131cf102d366587.
other
troubles.
a
in the
pom,
the
and
words,
one
override
<mark@clearstorydata.com>
stack
in
see
""mvn
with
short
relies
should be
spark/pom.xml:
http://svn.apache.org/repos/asf/maven/pom/tags/apache-11</connection
which
instead
org.apache:apache
there
equivalent
some
of
solution
overlap
are
with
poms


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 11 Sep 2013 23:44:11 -0700",Re: Maven SCM trouble,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ya I amended this for pushing the releases to maven. I actually
changed that later to a different URI because it was incorrect (should
have been https://)


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 12 Sep 2013 07:06:56 -0700",Re: Maven SCM trouble,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I was about to suggest that. Thanks Patrick.

- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Sep 2013 15:57:20 -0700",[VOTE] Release Apache Spark 0.8.0-incubating (RC3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.0. This will be the first incubator release for
Spark in Apache.

The tag to be voted on is v0.8.0-incubating (commit ffacd17):
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/files/

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-034/org/apache/spark/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/docs/

Please vote on releasing this package as Apache Spark 0.8.0-incubating!

The vote is open until Saturday, June 13th at 23:00 UTC and passes if
a majority of at least 3 +1 IPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Sep 2013 16:22:53 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey guys, we actually decided on a slightly different naming
convention for the downloads. I'm going to amend the files in the next
few minutes... in case anyone happens to be looking *this instant*
(which I doubt) hold off until I update them.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Sep 2013 16:59:42 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Fixed!


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Sep 2013 17:29:01 -0700",[VOTE] Release Apache Spark 0.8.0-incubating (RC3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.0. This will be the first incubator release for
Spark in Apache.

The tag to be voted on is v0.8.0-incubating (commit ffacd17):
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/files/

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-034/org/apache/spark/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/docs/

Please vote on releasing this package as Apache Spark 0.8.0-incubating!

The vote is open until Saturday, June 13th at 23:00 UTC and passes if
a majority of at least 3 +1 IPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/

"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 12 Sep 2013 17:33:09 -0700",Test email,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Patrick's emails to this are not making it, so I just want to try sending one.

Matei
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 12 Sep 2013 21:15:38 -0700",Test,dev@spark.incubator.apache.org,"
"
Bilal Aslam <maslam@gmail.com>,"Thu, 12 Sep 2013 22:52:50 -0700",Re: Test email,dev@spark.incubator.apache.org,"Got it :) 



~Bilal Aslam (bilalaslam.com (http://bilalaslam.com/), @bilalstartingup (https://twitter.com/bilalstartingup))
Simple, beautiful backup - getcloudcellar.com (https://www.getcloudcellar.com/)
Make your LinkedIn profile rock - wiserprofile.com (http://wiserprofile.com/)
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)




"
karthik tunga <karthik.tunga@gmail.com>,"Thu, 12 Sep 2013 23:24:45 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),dev <dev@spark.incubator.apache.org>,"""The vote is open until Saturday, *June* 13th at 23:00 UTC"". I might have
misunderstood this but was that supposed to be September 13th ?

Cheers,
Karthik



"
Jey Kottalam <jey@cs.berkeley.edu>,"Fri, 13 Sep 2013 09:04:45 -0700",Seq[T] is not covariant in T under spark-shell?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","If I execute the following sequence of inputs in both ""scala"" and
""spark-shell"" I see different results:

class Foo
class Bar(val x: Int) extends Foo
val a = Seq[Bar](new Bar(10), new Bar(20), new Bar(30))
def meow(x: Seq[Foo]) = x(0)
meow(a)


Under scala I see the following as expected:

scala> meow(a)
res0: Foo = Bar@7f020a15


Under spark-shell I see this:

scala> meow(a)
<console>:19: error: type mismatch;
 found   : Seq[this.Bar]
 required: Seq[this.Foo]
       meow(a)


It appears that somehow Seq[T] is no longer covariant? I'm sure this
is just some kind of Scala ignorance on my part, but any hints would
be appreciated.

Thanks,
-Jey

"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 13 Sep 2013 10:33:15 -0700",Re: Seq[T] is not covariant in T under spark-shell?,"dev@spark.incubator.apache.org,
 jey@cs.berkeley.edu","Yes, unfortunately this is a side-effect of how we change the Scala REPL's code generation. Before, the REPL would create a singleton object for each line:

object Line1 {
  class Foo
}

object Line2 {
  import Line1.Foo
  class Bar(val x: Int) extends Foo
}

And so on. We change this to:

class Line1 {
  class Foo
}
object Line1 { val INSTANCE = new Line1 }

class Line2 {
  import Line1.INSTANCE.Foo
  class Bar(val x: Int) extends Foo
}
object Line1 { val INSTANCE = new Line2 }

Now, Foo becomes an inner class of Line1 instead of a top-level static class, so it's bound to the Line1 instance that it belongs to, and the compiler can't prove that the Foos you pass to meow are necessarily subclasses of Line1.INSTANCE.Bar (as opposed to Bar in general). 

Unfortunately this is kind of tricky to fix without more hacking of the Scala interpreter. The reason we switched to these classes is to allow variable definitions to work correctly. For example, if you do

var x = Console.readLine()

The original Scala interpreter would do

object Line1 {
  var x = Console.readLine()
}

This is bad because it would mean that on *every* worker node that uses x, we'd be trying to call Console.readLine.

Matei




"
Suresh Marru <smarru@apache.org>,"Fri, 13 Sep 2013 14:01:45 -0400",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),dev@spark.incubator.apache.org,"Hi Patrick,

Can you please also add the gpg keys used to sign this release to a KEYS file available publicly.

Suresh


for
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating
https://repository.apache.org/content/repositories/orgapachespark-034/org/apache/spark/
0.8.0-incubating!


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 13 Sep 2013 11:28:03 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),dev@spark.incubator.apache.org,"[X] -1 Do not release this package because ...

Prior, out-of-band discussion:

Thanks for the insight Mark, we need to move this discussion to the

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 13 Sep 2013 11:59:30 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll post another RC in a bit which addresses Mark's comments (though
please continue to provide feedback on this one!).

Suresh - it's signed with the following key:

http://people.apache.org/~pwendell/9E4FE3AF.asc




"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 13 Sep 2013 14:01:27 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),dev@spark.incubator.apache.org,"So I went through org.apache:apache:11 to look for overlaps or potential
conflicts with Spark's poms -- less than I expected.  I'm not sure whether
we should add <description> and <organization> (apache-incubating instead
of apache) elements to the <project> of spark/pom.xml.  Other than that, we
could drop the maven-jar-plugin from spark/pom.xml, since it already
specified in a completely compatible way in org.apache:apache.  I didn't
find any other problems other than that in <scm>, which Patrick has already
addressed.



"
Patrick Wendell <pwendell@gmail.com>,"Sat, 14 Sep 2013 02:41:26 -0700",[VOTE] Release Apache Spark 0.8.0-incubating (RC4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.0. This will be the first incubator release for
Spark in Apache.

The tag to be voted on is v0.8.0-incubating (commit 32fc250):
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc4/files/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-046/org/apache/spark/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc4/docs/

Please vote on releasing this package as Apache Spark 0.8.0-incubating!

The vote is open until Tuesday, September 17th at 10:00 UTC and passes if
a majority of at least 3 +1 IPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/

"
Aaron Babcock <aaron.babcock@gmail.com>,"Sat, 14 Sep 2013 14:02:44 -0500",Groovy support,dev@spark.incubator.apache.org,"Hello,

I've been dipping my toes into the spark world lately but my background is
more groovy than scala. Luckily the java api works great.

However I was still missing out on the really cool spark repl. So I went
ahead and did some exploratory work into making a groovy repl for spark. I
think I've got the start of something that seems to work, so I'm going to
throw it out there for feedback.

Any interest in developing something like this further?

Here is a more in depth link with instructions on how to get started:

https://github.com/bunions1/groovy-spark-example

Would love to hear any thoughts.

thanks,
Aaron
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 15 Sep 2013 10:48:24 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC4),dev@spark.incubator.apache.org,"Hey Patrick, unfortunately, I noticed some bugs in the documentation on Mesos that I thought I had pushed a fix for, but I didn't. The fix is:
- In docs/_config.yml, change MESOS_VERSION to 0.12.1

In addition, if we do this, we might as well change it to 0.13.0 throughout. That is in docs/_config.yml, project/SparkBuild.scala, and pom.xml.

I also noticed that there's no CHANGES.txt in this release.

Otherwise, the files and code look fine. I built it and ran through the tests on Mac OS X.

Matei


for
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating
https://repository.apache.org/content/repositories/orgapachespark-046/org/apache/spark/
0.8.0-incubating!
if


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 15 Sep 2013 17:41:59 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"Whoa there, cowboy!

It's just a warning, and removing parameterized artifactIds (or versions)
is a significant change in functionality that necessitates changes in user
behavior.  At a minimum, this needs to be discussed before we go this
route.  If we really want to get rid of the warnings right away, then we
should try not to lose functionality and to require as little user behavior
change as possible.  The Maven Archetype
Plugin<http://maven.apache.org/guides/introduction/introduction-to-archetypes.html>
may
be the right way to do that.

Do we have some set-in-stone requirement to quash all maven warnings before
release?  If not, I'd recommend leaving the parameterized pom files in
place for now and change over to something like archetypes for 0.8.1 -- the
change to use archetypes is bigger than I am comfortable with making
hastily as we try to get 0.8.0 released.




"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Sep 2013 18:12:32 -0700",Re: git commit: Hard code scala version in pom files.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark,

Could you describe a user whose behavior is changed by this, and how
it is changed? This commit actually brings 0.8 in line with the 0.7
and 0.6 branches, where the scala version is hard coded in the
released artifacts:

http://repo1.maven.org/maven2/org/spark-project/spark-streaming_2.9.3/0.7.3/spark-streaming_2.9.3-0.7.3.pom

That seems to me to minimize the changes in user behavior as much as
possible. It would be bad if during the 0.8 release the format of our
released artifacts changed in a way that caused things to break for
other tool that consumes these builds downstream and isn't aware of
scala versioning.

We can have a more intricate solution on the master branch if you'd
like. This is just a fix to bring the 0.8 branch into line with our
existing releases (and since 0.8 only supports scala 2.9.3 anyways,
I'm still not sure how this could affect any users adversely).

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 15 Sep 2013 18:46:36 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"Ah sorry, I've gotten so used to using ClearStory's poms (where we make
quite a lot of use of such parameterization) that I lost track of exactly
when Spark's maven build was changed to work in a similar way.

This all revolves around a basic difference of opinion as to whether the
thing that specifies how a project is built should be a fixed, static
document or is more of a program itself or a parameterized function that
drives the build and results in an artifact.  SBT is of the latter opinion,
while Maven (at least with Maven 3) is going the other way.  That means
that building idiomatic Scala artifacts (which expect things like
cross-versioning support and artifactIds that include the Scala binary
version that was used to create them) is somewhat at odds with the Maven
philosophy.  Hard-coding artifactIds, versions, and whatever else Maven now
requires to guarantee that a pom file be a fixed, repeatable build
description works okay for a single build of an artifact; and a user of
just that built artifact won't have to change behavior if the pom is no
longer parameterized.  However, users who are not just interested in using
pre-built artifacts but also in modifying, adding to or reusing the code do
have to change their behavior if parameterized Maven builds disappear (yes,
you have pointed out the state of affairs with the 0.6 and 0.7 releases;
I'll point out that some of those making further use of the code have been
using the current, not-yet-released poms for a good while.)

Without some form of parameterized Maven builds, developers who now rely
upon such parameterized builds will have to choose to fork the Apache poms
and maintain their own parameterized build, or to repeatedly and manually
edit static Apache pom files in order to change artifactIds and dependency
versions (which is a frequent need when integrating Spark into a much
larger and more complicated technology stack), or to switch over to using
SBT in order to get parameterized builds (which, of course, would
necessitate a lot of other changes, not all of them welcome.)  Archetypes
or something similar seems like a way to satisfy Maven's new requirement
for static build configurations while at the same time providing a
parameterized way to generate that configuration or a modified version of
it -- solving the problem by adding a layer of abstraction.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Sep 2013 19:06:13 -0700",Re: git commit: Hard code scala version in pom files.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark,

Thanks for providing the detailed explanation.

My primary concern was just that this changes the published artifacts
in a way that could break downstream consumers of these poms which may
assume that artifact id's are immutable within a pom.xml file. For
now, let me revert my change and test that a few important things
still work (e.g. IDE's, etc). At a minimum I just want to make sure
things we are advising people to do don't break under this release. If
this doesn't break those things we can  move forward with the
parameterized artifacts for 0.8.0.

Just a word of caution though, there may be other downstream consumers
of the pom files for whom this will cause a problem in the future. If
someone presents a compelling reason, we'll have to think about
whether we can keep publishing them like this, since this is not
technically a valid maven format.

- Patrick


"
Jey Kottalam <jey@cs.berkeley.edu>,"Sun, 15 Sep 2013 19:19:53 -0700",Re: git commit: Hard code scala version in pom files.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm overall in favor of treating POM files as some kind of disgusting
object code that our build process just has to generate in some way,
instead of treating POM files as source code itself. An analogue would
be that we don't write machine code by hand, but instead use a
high-level language that is comparatively sane combined with a
compiler that accounts for all the bizarre quirks and details specific
to getting a working x86/ELF executable.

Maybe we should write our POM files in a macro language, and have a
Makefile that configures and generates the actual POM files seen by
Maven? This would allow us to both be in full compliance with Maven's
demands yet wrest back control of the build system.

However: for the 0.8.0 release, I support just hardcoding the scala
version in the POM files that we ship to the Maven repositories, and
revisiting this later. I think Patrick is right that Maven is warning
about these issues for some legitimate reason that may only be
encountered by downstream users, so we should proactively address the
warnings unless we're certain they can be ignored. We could revert
commit a1e7e519 on master as soon as the final 0.8.0 build is shipped.

-Jey


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 15 Sep 2013 19:32:02 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"Yes, it looks like we need to do something to get 0.8.0 shipped and
something to fix the problem longer term.  I agree that those somethings
don't have to be the same thing, and that we can take this up again once
the 0.8.0 dust has settled.

Give me a day and I'll probably have more to say about how I'd like things
to look in the future.




"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Sep 2013 19:37:49 -0700",Re: git commit: Hard code scala version in pom files.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","So Mark does that mean you'd be OK with us hard coding the scala
version in branch 0.8.0 build? It just seems like the overall simplest
solution for now. Or would this cause a large problem for you guys?

We can solve this on master for 0.9, I didn't touch master at all wrt
the maven build.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 15 Sep 2013 19:49:10 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"As long as its just a simple replacement of parameters, it's not too hard
to live with in the short term.  It's only if we let things fester and I
have to start dealing with more complicated divergence between our poms and
the Apache poms that things get more annoying.  If we can come up with a
common, satisfactory solution for the longer term, then all is good.
 Tomorrow, I'll try to get some sense of what CSD will be able to
contribute to such a solution.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Sep 2013 19:56:50 -0700",Re: git commit: Hard code scala version in pom files.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark,

OK - I will cut an RC then with the hard-coded versions. I am 100% in
support of a better solution to this which we can discuss and add to
the master branch. We could also port that solution to the 0.8 branch
for the next branch 0.8 release.

I do feel that for this particular release it's just less risky to
hard code the versions. As Jey mentioned, there could be some
unintended consequences of this downstream and I'd prefer to fully
explore those before we change the release format.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 15 Sep 2013 20:05:27 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"+1



"
Konstantin Boudnik <cos@apache.org>,"Sun, 15 Sep 2013 21:26:03 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"The warning from Maven coming because you have to have a constant version for
a deployed artifact to work/be resolvable.

or
ypes.html>
re
he
519
519
sembly/pom.xml
gel/pom.xml
re/pom.xml
amples/pom.xml
lib/pom.xml
m.xml
pl-bin/pom.xml
pl/pom.xml
reaming/pom.xml
ols/pom.xml
rn/pom.xml
"
Konstantin Boudnik <cos@apache.org>,"Sun, 15 Sep 2013 21:30:02 -0700",Re: git commit: Hard code scala version in pom files.,"dev@spark.incubator.apache.org, jey@cs.berkeley.edu","Guys, 

actually, POM files are artifacts metadata. Hence, IMO it should be treated as
a part of the source code. Simply because POM file should be a reflection on
what is going into the source code at the moment.

Makefiles are fun ;), but it would be an interesting experiment to say the
least.

Cos

rote:
ly
ion,
 now
ing
e do
yes,
een
oms
ly
ncy
ng
es
of
rote:
0.7.3/spark-streaming_2.9.3-0.7.3.pom
ions)
n we
.html
in
 --
519
---
---
ssembly/pom.xml
---
agel/pom.xml
---
ore/pom.xml
---
xamples/pom.xml
---
llib/pom.xml
---
om.xml
---
epl-bin/pom.xml
---
epl/pom.xml
---
treaming/pom.xml
---
ools/pom.xml
---
arn/pom.xml
---
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Sep 2013 21:48:49 -0700",[VOTE] Release Apache Spark 0.8.0-incubating (RC5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.0. This will be the first incubator release for
Spark in Apache.

The tag to be voted on is v0.8.0-incubating (commit d9e80d5):
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc5/files/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-051/org/apache/spark/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc5/docs/

Please vote on releasing this package as Apache Spark 0.8.0-incubating!
The vote is open until Thursday, September 19th at 05:00 UTC and passes if
a majority of at least 3 +1 IPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/

"
Shane Huang <shannie.huang@gmail.com>,"Mon, 16 Sep 2013 13:03:38 +0800",Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"Having worked with a few customers, we found the current organization of
the scripts and configuration a bit confusing and inconvenient, so we
propose to reorganize them. Below we described the specific reasons and the
rough proposal, please kindly provide your opinions and suggestions. :)

Specific reasons for re-organization:
1) Usually the application developers/users and platform administrators
belongs to two teams. So it's better to separate the scripts used by
administrators and application users, e.g. put them in sbin and bin folders
respectively
2) User level options and admin level options need to be separated.  For
example, an application user may never know how many spindles there are in
the cluster nodes, so it's often the administrator's duty to specify
spark.local.dir.
3) If there are multiple ways to specify an option, an overriding rule
should be present and should not be error-prone.
4) Currently the options are set and get using System property. It's hard
to manage and inconvenient for users. It's good to gather the options into
one file using format like xml or json.

Some previous work:
1) SPARK-544 contains a discussion about providing a configuration class
for spark https://spark-project.atlassian.net/browse/SPARK-5<https://spark-project.atlassian.net/browse/SPARK-544>
44
2) Ankur has gathered spark options into a json format
https://gist.github.com/ankurcha/5655646


Our rough proposal:

   - Scripts


   1. make an ""sbin"" folder containing all the scripts for administrators,
   specifically,
      - all service administration scripts, i.e. start-*, stop-*,
      slaves.sh, *-daemons, *-daemon scripts
      - low-level or internally used utility scripts, i.e.
      compute-classpath, spark-config, spark-class, spark-executor
   2. make a ""bin"" folder containing all the scripts for application
   developers/users, specifically,
      - user level app  running scripts, i.e. pyspark, spark-shell, and we
      propose to add a script ""spark"" for users to run applications (very much
      like spark-class but may add some more control or convenient utilities)
      - scripts for status checking, e.g. spark and hadoop version
      checking, running applications checking, etc. We can make this a separate
      script or add functionality to ""spark"" script.
   3. No wandering scripts outside the sbin and bin folders


   -  Configurations/Options and overriding rule


   1. Define a Configuration class which contains all the options available
   for Spark application. A Configuration instance can be de-/serialized
   from/to a json formatted file.
   2. Each application (SparkContext) has one Configuration instance and it
   is initialized by the application which creates it (either read from file
   or passed from command line options or env SPARK_JAVA_OPTS).
   3. When launching an Executor on a node, the Configuration is firstly
   initialized using the node-local configuration file as default. The
   Configuration passed from application driver context will override any
   options specified in default.


Any comments are welcome.

-- 
shannie.huang@gmail.com
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*
"
Henry Saputra <henry.saputra@gmail.com>,"Sun, 15 Sep 2013 22:06:56 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Looks like this VOTE thread has been cancelled.

Patrick has sent VOTE for RC5 in separate thread.

- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Sep 2013 22:08:20 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes, we've moved onto RC5, thanks.


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Sep 2013 23:09:47 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I also wrote an audit script [1] to verify various aspects of the
release binaries and ran it on this RC. People are welcome to run this
themselves, but I haven't tested it on other machines yet, and some of
the Spark tests are very sensitive to the test environment :) Output
is pasted below:

[1] https://github.com/pwendell/spark-utils/blob/master/release_auditor.py

-----------------------------------------------------
==== Verifying download integrity for artifact:
spark-0.8.0-incubating-bin-cdh4-rc5.tgz ====
[PASSED] Artifact signature verified.
[PASSED] Artifact MD5 verified.
[PASSED] Artifact SHA verified.
[PASSED] Tarball contains CHANGES.txt file
[PASSED] Tarball contains NOTICE file
[PASSED] Tarball contains LICENSE file
[PASSED] README file contains disclaimer
==== Verifying download integrity for artifact:
spark-0.8.0-incubating-bin-hadoop1-rc5.tgz ====
[PASSED] Artifact signature verified.
[PASSED] Artifact MD5 verified.
[PASSED] Artifact SHA verified.
[PASSED] Tarball contains CHANGES.txt file
[PASSED] Tarball contains NOTICE file
[PASSED] Tarball contains LICENSE file
[PASSED] README file contains disclaimer
==== Verifying download integrity for artifact:
spark-0.8.0-incubating-rc5.tgz ====
[PASSED] Artifact signature verified.
[PASSED] Artifact MD5 verified.
[PASSED] Artifact SHA verified.
[PASSED] Tarball contains CHANGES.txt file
[PASSED] Tarball contains NOTICE file
[PASSED] Tarball contains LICENSE file
[PASSED] README file contains disclaimer
==== Verifying build and tests for artifact:
spark-0.8.0-incubating-bin-cdh4-rc5.tgz ====
==> Running build
[PASSED] sbt build successful
[PASSED] Maven build successful
==> Performing unit tests
[PASSED] Tests successful
==== Verifying build and tests for artifact:
spark-0.8.0-incubating-bin-hadoop1-rc5.tgz ====
==> Running build
[PASSED] sbt build successful
[PASSED] Maven build successful
==> Performing unit tests
[PASSED] Tests successful
==== Verifying build and tests for artifact: spark-0.8.0-incubating-rc5.tgz ====
==> Running build
[PASSED] sbt build successful
[PASSED] Maven build successful
==> Performing unit tests
[PASSED] Tests successful

- Patrick


"
Mike <spark@good-with-numbers.com>,"Mon, 16 Sep 2013 07:29:05 +0000",Re: Propose to Re-organize the scripts and configurations,Shane Huang <shannie.huang@gmail.com>,"
ditto


I wonder why the work of these scripts wasn't mostly done in Scala.  
Seems roundabout to use Bash (or Python, in spark-perf) to calculate 
shell environment variables that are then read back into Scala code.


Reminiscent of what Hibernate's been doing for the past decade.  Would 
be nice if the Configuration was also exposed through an MBean or such 
so that one can check it's values with certainty.

"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 16 Sep 2013 10:52:43 +0200",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"There was another discussion on the old dev list about this:
https://groups.google.com/forum/#!msg/spark-developers/GL2_DwAeh5s/9rwQ3iDa2t4J

I tend to agree with having configuration sitting in JSON (or properties
files) and using the Typesafe Config library which can parse both.

Something I've used in my apps is along these lines:
https://gist.github.com/MLnick/6578146

It's then easy to have default config overridden with CLI for example:
val conf = cliConf.withFallback(defaultConf)

I'd be happy to be involved in working on this if there is a consensus
about best approach

N






"
Gary Struthers <lithosteps@earthlink.net>,"Sun, 15 Sep 2013 21:03:33 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"Maven provides 2 ways for users to customize properties in pom files. There is a Resource Filtering option which is disabled by default, when it's turned on Maven searches src/main/resources for overriding properties in .properties and xml files. There is also a Production Profile option that also overrides default properties in the pom. Typically, the pom has the developer default properties and the profile reads production deployment properties from .properties and xml files in src/main/resources.

http://books.sonatype.com/mvnref-book/reference/resource-filtering-sect-description.html

If you post the warnings, I'll look into them.

Gary Struthers



make
exactly
the
that
opinion,
means
binary
Maven
Maven now
of
no
using
code do
(yes,
releases;
been
rely
poms
manually
dependency
using
Archetypes
requirement
version of
http://repo1.maven.org/maven2/org/spark-project/spark-streaming_2.9.3/0.7.3/spark-streaming_2.9.3-0.7.3.pom
our
<mark@clearstorydata.com>
versions)
in
this
then we
http://maven.apache.org/guides/introduction/introduction-to-archetypes.html
warnings
in
0.8.1 --
making
http://git-wip-us.apache.org/repos/asf/incubator-spark/repo
http://git-wip-us.apache.org/repos/asf/incubator-spark/commit/a1e7e519
----------------------------------------------------------------------
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/assembly/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/bagel/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/core/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/examples/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/mllib/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/repl-bin/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/repl/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/streaming/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/tools/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/yarn/pom.xml
----------------------------------------------------------------------


"
Mike <spark@good-with-numbers.com>,"Mon, 16 Sep 2013 15:40:21 +0000",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"
After some sleep, I guess the answer's obvious: to set the ""java"" 
command line.

"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 16 Sep 2013 10:58:56 -0700",Re: git commit: Hard code scala version in pom files.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Agree with this. POM file reflects the state of the project at a
particular moment so it is metadata of the project but should be
treated as part of source code.


- Henry


"
Reynold Xin <rxin@cs.berkeley.edu>,"Mon, 16 Sep 2013 13:28:23 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org




"
Mike <spark@good-with-numbers.com>,"Tue, 17 Sep 2013 00:35:25 +0000",Re: Propose to Re-organize the scripts and configurations,Shane Huang <shannie.huang@gmail.com>,"
I'd like to see script broken out into shell functions in a common file 
that gets "".""-included in every script, where that makes sense.  
Specifically, I gather that compute-classpath.sh isn't run except as a 
subroutine, so no need to promote it as an executable.

"
"""shannie.huang"" <shannie.huang@gmail.com>","Tue, 17 Sep 2013 09:06:24 +0800",Re: Propose to Re-organize the scripts and configurations,Mike <spark@good-with-numbers.com>,"Yeah, I tend to agree that either executable or not, these common utility scripts may not need to be exposed to end user in sbin and bin folders. But it seems we must still make some of these scripts executable as they are not only called in other scripts, but also called in scala source code.





"
"""shannie.huang"" <shannie.huang@gmail.com>","Tue, 17 Sep 2013 09:18:47 +0800",Re: Propose to Re-organize the scripts and configurations,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I like the idea of using Typesafe Config. 

Nick, we'd be glad to work with you after we gathered enough opinions and come to a consensus of the approach. 


a2t4J


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 16 Sep 2013 18:39:09 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey folks, just FYI we found one minor issue with this RC (the kafka
jar in the stream pom needs to be published as ""provided"" since it's
not available in maven). Please still continue to test this and
provide feedback here until the following RC is posted later.

- Patrick


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 16 Sep 2013 20:37:45 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC5),dev@spark.incubator.apache.org,"FWIW, I tested it otherwise and it seems good modulo this issue.

Matei


this
of
https://github.com/pwendell/spark-utils/blob/master/release_auditor.py
<pwendell@gmail.com>
release for
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating
at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc5/files/
https://repository.apache.org/content/repositories/orgapachespark-051/org/apache/spark/
0.8.0-incubating!
passes


"
Andy Konwinski <andykonwinski@gmail.com>,"Mon, 16 Sep 2013 21:47:31 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Patrick, I took a quick look over your release_auditor.py script and it's
really great!

Then I ran it (had to add ""--keyserver pgp.mit.edu"" to the gpg command) and
everything passed on OS X!

Great job and +1 from me whenever you resolve the kafka jar issue you
mentioned.

Andy



"
Roman Shaposhnik <rvs@apache.org>,"Mon, 16 Sep 2013 22:43:06 -0700",Re: Mentors: question about Apache GitHub mirrors,dev@spark.incubator.apache.org,"Hi!

Update -- turns out all of my connection to GitHub left :-(
(I guess its the valley -- people move around quickly).

The best advice I got is to pop up on #github and
ask away hoping that some of the GH folks would
be there.

Sorry -- that's all I've got.

Thanks,
Roman.


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 17 Sep 2013 02:03:00 -0700",[VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.0. This will be the first incubator release for
Spark in Apache.

The tag to be voted on is v0.8.0-incubating (commit 3b85a85):
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/files/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-059/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/docs/

Please vote on releasing this package as Apache Spark 0.8.0-incubating!

The vote is open until Friday, September 20th at 09:00 UTC and passes if
a majority of at least 3 +1 IPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 17 Sep 2013 02:05:45 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks for the feedback guys. I've changed the audit script to fix
Andy's suggestion. I also added tests for building sbt and maven
projects against the staged repository to test that artifacts are
setup correctly in maven.

I've posted RC6 which adds a very small change to this RC. This vote
is therefore cancelled in favor of RC6.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 17 Sep 2013 02:06:39 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Also the URL for the release audits changed slightly:
https://github.com/pwendell/spark-utils/blob/master/release-audits/release_auditor.py


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 17 Sep 2013 10:42:36 -0700",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"Hi Shane,

I agree with all these points. Improving the configuration system is one of the main things I'd like to have in the next release.

administrators
folders

Yup, right now we don't have any attempt to install on standard system paths.


Yes, I think this should always be Configuration class in code > system properties > env vars. Over time we will deprecate the env vars and maybe even system properties.

hard
into

I think this is the main thing to do first -- pick one configuration class and change the code to use this.

administrators,
we
(very much
utilities)
separate

Makes sense.

available
de-/serialized
and it
file
firstly
any

This sounds great to me! The one thing I'll add is that we might want to prevent applications from overriding certain settings on each node, such as work directories. The best way is to probably just ignore the app's version of those settings in the Executor.

If you guys would like, feel free to write up this design on SPARK-544 and start working on it. I think it looks good.

Matei
"
Konstantin Boudnik <cos@apache.org>,"Tue, 17 Sep 2013 11:04:36 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"Gary,

I believe resource filtering isn't relevant in the case of using variables in
stead of constant artifact versions.

Cos


"
Andy Konwinski <andykonwinski@gmail.com>,"Tue, 17 Sep 2013 11:13:57 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 (see my comments on RC5)



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 17 Sep 2013 11:28:04 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"+1

Tried new staging repo to make sure the issue with RC5 is fixed.

Matei




"
Gary Struthers <lithosteps@earthlink.net>,"Tue, 17 Sep 2013 11:54:34 -0700",Re: git commit: Hard code scala version in pom files.,dev@spark.incubator.apache.org,"You're right Cos, it isn't relevant.

As long as Scala has fragile versions this will affect all Scala on Maven projects ( a Typesafe developer said fixing this must wait until Java 8). 

I think Spark should join Typesafe and some other Scala projects and ask Jason van Zyl and Stuart Mccullough at Sonatype to add the most needed SBT features to Maven. I think they would if it wasn't too hard and there is enough demand to justify the work. Sonatype wanted to fully support Ruby but Rubyists weren't that interested in Maven. Also, they're a fellow Apache project now.

Gary Struthers
 

variables in
There
it's
in
that
the
deployment
http://books.sonatype.com/mvnref-book/reference/resource-filtering-sect-description.html
disgusting
would
specific
Maven's
warning
the
shipped.
make
exactly
whether the
static
that
opinion,
means
binary
Maven
Maven now
user of
is no
in using
code do
disappear (yes,
releases;
have been
rely
Apache poms
manually
dependency
much
using
Archetypes
requirement
version of
how
0.7
http://repo1.maven.org/maven2/org/spark-project/spark-streaming_2.9.3/0.7.3/spark-streaming_2.9.3-0.7.3.pom
as
our
for
of
you'd
our
anyways,
<mark@clearstorydata.com>
versions)
changes in
this
then we
user
http://maven.apache.org/guides/introduction/introduction-to-archetypes.html
warnings
files in
0.8.1 --
making
causes
http://git-wip-us.apache.org/repos/asf/incubator-spark/repo
http://git-wip-us.apache.org/repos/asf/incubator-spark/commit/a1e7e519
http://git-wip-us.apache.org/repos/asf/incubator-spark/tree/a1e7e519
http://git-wip-us.apache.org/repos/asf/incubator-spark/diff/a1e7e519
----------------------------------------------------------------------
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/assembly/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/bagel/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/core/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/examples/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/mllib/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/repl-bin/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/repl/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/streaming/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/tools/pom.xml
----------------------------------------------------------------------
http://git-wip-us.apache.org/repos/asf/incubator-spark/blob/a1e7e519/yarn/pom.xml
----------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 17 Sep 2013 13:26:53 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"There are a few nits left to pick: 'sbt/sbt publish-local' isn't generating
correct POM files because of the way the exclusions are defined in
SparkBuild.scala using wildcards; looks like there may be some broken doc
links generated in that task, as well; DriverSuite doesn't like to run from
the maven build, complaining that 'sbt/sbt assembly' needs to be run first.

None of these is enough for me to give RC6 a -1.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 17 Sep 2013 13:46:56 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark,

Good catches here. Ya the driver suite thing is sorta annoying - we
should try to fix that in master. The audit script I wrote first does
an sbt/sbt assembly to avoid this. I agree though these shouldn't
block the release (if a blocker does come up we can revisit these
potentially when cutting a release).

- Patrick


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 17 Sep 2013 19:20:50 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"In Maven, mvn package should also create the assembly, but the non-obvious thing is that it needs to happen for all projects before mvn test for core works. Unfortunately I don't know any easy way around that.

Matei


generating
doc
run from
first.
release for
https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating
at:
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/files/
https://repository.apache.org/content/repositories/orgapachespark-059/
0.8.0-incubating!
passes if


"
Josh Rosen <rosenville@gmail.com>,"Tue, 17 Sep 2013 19:27:19 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","Maybe we could run the tests that depend on the assembly during Maven's
`integration-test` phase, which runs after the `package` phase.  I'm not
sure what its sbt equivalent is.



"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 18 Sep 2013 10:32:37 -0700",Re: Mentors: question about Apache GitHub mirrors,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Looks like we need to stop by at INFRA chat room

- Henry


"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 18 Sep 2013 10:33:48 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Patrick, dont forget to officially cast your own vote.


"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 18 Sep 2013 10:34:22 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Mark, so I assume we are getting +1 from you?  ;)


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 18 Sep 2013 10:38:47 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"If that will make the difference between releasing and not releasing, then
sure; else I'd rather see the last niggles resolved, but that's enough for
me to vote -1 -- so I'm a 0 unless you really need another +1.



"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 18 Sep 2013 10:39:19 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"""not enough"", of course.




"
Roman Shaposhnik <rvs@apache.org>,"Wed, 18 Sep 2013 10:43:08 -0700",Re: Mentors: question about Apache GitHub mirrors,dev@spark.incubator.apache.org,"
Like I said -- I don't think ASF INFRA is involved here -- you'd
have to stop by GH chat room (which is #github).

Thanks,
Roman.

"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 18 Sep 2013 10:45:00 -0700",Re: Mentors: question about Apache GitHub mirrors,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah yes it should be #github, too many times typing INFRA, sorry Roman.


- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 18 Sep 2013 10:50:12 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm a +1 as well.


"
Konstantin Boudnik <cos@apache.org>,"Wed, 18 Sep 2013 11:06:28 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"Maven package could be run with -DskipTests that will simply build... well,
the package.

+1 on the RC. The nits are indeed minor.

  Cos


"
Vadim Chekan <kot.begemot@gmail.com>,"Wed, 18 Sep 2013 13:01:18 -0700",spark.cleaner.ttl default,dev@spark.incubator.apache.org,"Hi all,

I'm trying to load streaming context from a checkpoint: ""val ctx = new
StreamingContext(""\\Temp\\spark-checkpoint"")"" and I'm getting:
""Spark Streaming cannot be used without setting spark.cleaner.ttl; set this
property before creating a SparkContext (use SPARK_JAVA_OPTS for the shell)""

Apparently MetadataCleaner delay is set to default 3600 when streaming
context is created as ""new StreamingContext(""local"", ""cpc-stub"",
Seconds(10))"" but not when ""new
StreamingContext(""\\Temp\\spark-checkpoint"")""

https://github.com/mesos/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala#L546

Is there a reason for spark.cleaner.ttl to be set to a default value in one
constructor but not in another? If it is just an omission, is github's
mesos/spark still preferred way to send pull request?

Vadim.

-- 
explicitly specified
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 18 Sep 2013 17:54:53 -0700",Re: spark.cleaner.ttl default,dev@spark.incubator.apache.org,"Good catch, we should probably just load it from the checkpoint in the latter case.

Please send pull requests to apache/incubator-spark instead -- there are some new docs on it here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Matei


new
this
shell)""
https://github.com/mesos/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala#L546
in one
IMPLICIT is

"
Reynold Xin <rxin@cs.berkeley.edu>,"Wed, 18 Sep 2013 20:56:39 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org




"
Patrick Wendell <pwendell@gmail.com>,"Thu, 19 Sep 2013 17:56:27 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","FYI this vote ends in 8 hours.


"
Ali Ghodsi <alig@cs.berkeley.edu>,"Thu, 19 Sep 2013 19:14:38 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC3),dev@spark.incubator.apache.org,"+1 on this!



"
Chris Mattmann <mattmann@apache.org>,"Thu, 19 Sep 2013 19:20:32 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm currently downloading the RC (all 127mb of the bin; then onto source).
I have a generic set of Incubator scripts so should go fine after that.

I'm giving you a preview of my minor nit:

We don't VOTE on github URLs -- we VOTE on ASF URLs (e.g., the tag). That
should be corrected in future RC emails. If all checks out, should be +1
shortly.






"
Patrick Wendell <pwendell@gmail.com>,"Thu, 19 Sep 2013 19:25:10 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Chris the tag in github is 3b85a85, which I listed in the original
vote next to the git URL. Is there another type of tag I should be
adding?


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 19 Sep 2013 19:29:20 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I think Chris was saying the VOTE should be made against ASF git repo directly:

https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=ca69992d3b82124795d16e8e0d76a50fe10435f9


- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 19 Sep 2013 19:33:03 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah I see - sounds good. For future releases we can use that URL to
describe the tag.


"
Chris Mattmann <mattmann@apache.org>,"Thu, 19 Sep 2013 19:39:10 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yep what Henry said below Patrick.

Not a huge deal I'm still likely going to be +1 (once my darned
downloads complete on my crappy cable modem lol). I just meant that
the URL reference should be an Apache URL.

Will keep you guys posted..





"
Chris Mattmann <mattmann@apache.org>,"Thu, 19 Sep 2013 19:44:27 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks!






"
Roman Shaposhnik <rvs@apache.org>,"Thu, 19 Sep 2013 20:06:38 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"
I was going to test it on a fully distributed Bigtop cluster, but hit
a few snags. That now will extend into the weekend.

Of course, that's not that big of a deal -- I can always vote on
incubator general once you guys move the vote over there.

The only minor nit for the future I've noticed is that I would
highly encourage you to follow the usual RC practices where
you name all of your artifact as final bits and have a subdirectory
that reflects the RC name. E.g. here's how a very recent
Hadoop RC looks like:
    http://people.apache.org/~acmurthy/hadoop-2.1.1-beta-rc0/

Thanks,
Roman.

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 19 Sep 2013 20:17:53 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Roman,

We can do this in the future - I wasn't sure exactly what the right
standard approach was. Just so I understand, the change you are
proposing from what is there now is just to remove rcX from the
file-names, correct?

- Patrick


"
Chris Mattmann <mattmann@apache.org>,"Thu, 19 Sep 2013 21:20:06 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick,

+1 from me (binding):


[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann% $HOME/bin/stage_apache_rc spark
0.8.0-incubating-bin-hadoop1-rc6
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/files/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100  127M  100  127M    0     0   256k      0  0:08:29  0:08:29 --:--:--
277k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   490  100   490    0     0   4437      0 --:--:-- --:--:-- --:--:--
8750
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100    94  100    94    0     0    807      0 --:--:-- --:--:-- --:--:--
1678
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann% $HOME/bin/stage_apache_rc spark
0.8.0-incubating-bin-cdh4-rc6
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/files/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100  134M  100  134M    0     0   104k      0  0:21:54  0:21:54 --:--:--
304k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   490  100   490    0     0   4435      0 --:--:-- --:--:-- --:--:--
8448
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100    91  100    91    0     0    824      0 --:--:-- --:--:-- --:--:--
1625
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann% $HOME/bin/stage_apache_rc spark 0.8.0-incubating-rc6
http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/files/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100 4460k  100 4460k    0     0   102k      0  0:00:43  0:00:43 --:--:--
223k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   490  100   490    0     0   2647      0 --:--:-- --:--:-- --:--:--
3769
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100    81  100    81    0     0    719      0 --:--:-- --:--:-- --:--:--
1528
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann% curl -O https://people.apache.org/keys/committer/pwendell.asc
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100  1919  100  1919    0     0   1964      0 --:--:-- --:--:-- --:--:--
2072
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann% gpg --import < pwendell.asc
gpg: key 9E4FE3AF: public key ""Patrick Wendell <pwendell@gmail.com>""
imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann%


SIGS check out:

[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann% $HOME/bin/verify_gpg_sigs
Verifying Signature for file spark-0.8.0-incubating-bin-cdh4-rc6.tgz.asc
gpg: Signature made Tue Sep 17 01:41:17 2013 PDT using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the
owner.
Primary key fingerprint: 5AA9 0E72 812F F246 7904  277D 548F 5FEE 9E4F E3AF
Verifying Signature for file spark-0.8.0-incubating-bin-hadoop1-rc6.tgz.asc
gpg: Signature made Tue Sep 17 01:32:32 2013 PDT using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the
owner.
Primary key fingerprint: 5AA9 0E72 812F F246 7904  277D 548F 5FEE 9E4F E3AF
Verifying Signature for file spark-0.8.0-incubating-rc6.tgz.asc
gpg: Signature made Tue Sep 17 01:13:51 2013 PDT using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the
owner.
Primary key fingerprint: 5AA9 0E72 812F F246 7904  277D 548F 5FEE 9E4F E3AF
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann%


Checksums check out:

[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann% $HOME/bin/verify_md5_checksums
md5sum: stat '*.tar.gz': No such file or directory
md5sum: stat '*.bz2': No such file or directory
md5sum: stat '*.zip': No such file or directory
spark-0.8.0-incubating-bin-cdh4-rc6.tgz: OK
spark-0.8.0-incubating-bin-hadoop1-rc6.tgz: OK
spark-0.8.0-incubating-rc6.tgz: OK
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc] mattmann%


I was able to build:

[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc/spark-0.8.0-incubating] mattmann% ./sbt/sbt assembly
Getting net.java.dev.jna jna 3.2.3 ...
downloading 
http://repo1.maven.org/maven2/net/java/dev/jna/jna/3.2.3/jna-3.2.3.jar ...
	[SUCCESSFUL ] net.java.dev.jna#jna;3.2.3!jna.jar (7552ms)
:: retrieving :: org.scala-sbt#boot-jna
	confs: [default]
	1 artifacts copied, 0 already retrieved (838kB/36ms)
Getting org.scala-sbt sbt 0.12.4 ...

..later


[warn] Strategy 'discard' was applied to 2 files
[warn] Strategy 'first' was applied to 823 files
[info] Checking every *.class/*.jar file's SHA-1.
[info] Done packaging.
[info] SHA-1: bf3ae9e2d032150075416faf132eb779b9df587e
[info] Packaging 
/Users/mattmann/tmp/apache-spark-0.8.0-incubating-rc/spark-0.8.0-incubating
/examples/target/scala-2.9.3/spark-examples-assembly-0.8.0-incubating.jar
...
[info] Done packaging.
[success] Total time: 1773 s, completed Sep 19, 2013 8:54:23 PM
[guest-wireless-upc-nat-206-117-88-006:~/tmp/apache-spark-0.8.0-incubating-
rc/spark-0.8.0-incubating] mattmann%

Cheers,

Chris




"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 20 Sep 2013 06:38:31 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Signature looks good
Hashes good
License and notice exists
Build and run tests and looks good

+1 binding


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 20 Sep 2013 06:39:13 -0700",[VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The RC should be just the directory where the artifact live but the final
name should omit the RCxx

Hmm not sure if IPMCs will be picky about this but should not be blocker to
release.

- Henry

<pwendell@gmail.com<javascript:;>>
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 20 Sep 2013 07:56:23 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Henry - one thing is that, because the filenames are not included in
the signatures, I could just alter the filenames now to not include
-RCX... would that be preferable or would that necessitate another
vote?

- Patrick


"
Roman Shaposhnik <rvs@apache.org>,"Fri, 20 Sep 2013 08:10:13 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"
Right. Basically your artifacts should look exactly like what
is going to be released when the vote passes.

Like I said -- it is a small nit, but it makes it easier for the
guys like me to test the RCs in the automated manner.

Thanks,
Roman.

"
Tom Graves <tgraves_cs@yahoo.com>,"Fri, 20 Sep 2013 10:55:19 -0700 (PDT)",Moving to apache incubator git?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey all,

Sorry if I missed it on here. I'm curious if there is a plan on when we are moving to the apache incubator git repo? When should pull requests start going there? The master branch seems a bit behind from the mesos/spark branch.

Thanks,
Tom"
Patrick Wendell <pwendell@gmail.com>,"Fri, 20 Sep 2013 11:43:59 -0700",[RESULT] [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The vote is now closed. Below are the vote totals.

+1 (7 Total)
Andy Konwinski
Matei Zaharia
Patrick Wendell
Konstantin Boudnik
Reynold Xin
Chris Mattmann*
Henry Saputra*

0 (1 Total)
Mark Hamstra

-1 (0 Total)

* = Binding Vote

As per the incubator release guide [1] I'll be sending this to the
general incubator list for a final vote from IPMC members.

[1] http://incubator.apache.org/guides/releasemanagement.html#best-practice-incubator-release-vote

- Patrick

---------- Forwarded message ----------
From: Roman Shaposhnik <rvs@apache.org>
Date: Fri, Sep 20, 2013 at 8:10 AM
Subject: Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6)
To: dev@spark.incubator.apache.org



Right. Basically your artifacts should look exactly like what
is going to be released when the vote passes.

Like I said -- it is a small nit, but it makes it easier for the
guys like me to test the RCs in the automated manner.

Thanks,
Roman.

"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 20 Sep 2013 11:52:18 -0700",Re: [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick,

Let's leave it for now. If someone veto against it the. We do the rename.

Thx,

Henry


"
Gerard Maas <gerard.maas@gmail.com>,"Fri, 20 Sep 2013 21:41:42 +0200",Spark Streaming threading model,dev@spark.incubator.apache.org,"Hi all,

Today I was contributing my (limited) Scala knowledge to some local folks
that are working on setting up a Spark Streaming cluster with Kafka. Bear
with me, I'm fairly new to Spark.

going to only one node of the cluster. The reason may lie behind this note
[0].

Further on, I was looking into KafkaInputDStream and I stumbled on the way
the dataBlocks are being populated: There are n threads running n message
handlers, each listening to a specific kafka topic stream. They all seem to
share the same reference of the blockGenerator and contribute data using
the BlockGenerator.+= method [1], which does not offer any synchronization.

Unless I missed some part, It looks like we might have a concurrency issue
when multiple kafka message handlers add data to the underlying ArrayBuffer
in the BlockGenerator.

I continued my journey through the system and ended at NetworkInputTracker
[2]. It mixes two concurrency models: The Actor-based
NetworkInputStreamActor and the 'raw' thread-based ReceiverExecutor.  Where
the NetworkInputTracker safely shares a queue with the actor, at the
expense of some locking.

(actors?) would provide a uniform programming paradigm and simplify some of
the code. But it's evident that the author, with knowledge of both models
had made specific choices for a reason.

Care to comment about the rationale behind mixing these concurrency models?
Any suggestions on what should stay and what might be candidate for
refactoring?

Thanks!

-kr, Gerard.

[0]
https://github.com/maasg/spark/blob/master/streaming/src/main/scala/spark/streaming/NetworkInputTracker.scala#L161
[1]
https://github.com/maasg/spark/blob/master/streaming/src/main/scala/spark/streaming/dstream/KafkaInputDStream.scala#L119
[2]
https://github.com/maasg/spark/blob/master/streaming/src/main/scala/spark/streaming/NetworkInputTracker.scala
"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 20 Sep 2013 13:30:01 -0700",Re: [RESULT] [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks to Patrick for driving the first Apache Spark release. Great job so far.

A bit clarification, the release VOTE passes with more than 3 +1
binding votes from Apache Spark Podling Project Management (PPMC):

+1 (7 Total)
Andy Konwinski
Matei Zaharia
Patrick Wendell
Reynold Xin
Chris Mattmann*
Henry Saputra*

(* indicates IPMC)

Since Spark is under ASF incubator we need to send another VOTE to
general@i.a.o list.

""
It is Apache policy that all releases be formally approved by the
responsible PMC. In the case of the incubator, the IPMC must approve
all releases. That means there is an additional bit of voting that the
release manager must now oversee on general@incubator in order to gain
that approval. The release manager must inform general@incubator that
the vote has passed on the podling's development list, and should
indicate any IPMC votes gained during that process. A new vote on the
release candidate artifacts must now be held on general@incubator to
seek majority consensus from the IPMC. Previous IPMC votes issued on
the project's development list count towards that goal. Even if there
are sufficient IPMC votes already, it is vital that the IPMC as whole
is informed via a VOTE e-mail on general@incubator.
""

We have 2 IPMCs Vote already so technically we need one more unless we
got veto votes against the release.

- Henry



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 20 Sep 2013 13:35:29 -0700",Re: [RESULT] [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Henry,

Sounds good. I'll send an email to general@ shortly. I didn't realize
that this vote technically counts as passing according to those rules
(since plenty of PPMC gave +1).


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 20 Sep 2013 13:37:53 -0700",Re: [RESULT] [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It is passing with Apache Spark podling/ community but we need extra
blessing from IPMCs via formal email to general@i.a.o list and in
particular we need one more IPMC vote.

- Henry


"
Vadim Chekan <kot.begemot@gmail.com>,"Fri, 20 Sep 2013 14:56:28 -0700",Re: Moving to apache incubator git?,"dev@spark.incubator.apache.org, Tom Graves <tgraves_cs@yahoo.com>","There was Matei's reply to me a couple of days ago:

some new docs on it here:







-- 
explicitly specified
"
Roman Shaposhnik <rvs@apache.org>,"Fri, 20 Sep 2013 16:39:36 -0700",Re: [RESULT] [VOTE] Release Apache Spark 0.8.0-incubating (RC6),dev@spark.incubator.apache.org,"
Correct, the release for a poddling can only happen once
it passes on general@

Like I said -- I intend to test the bits on a fully distributed cluster
in Bigtop over the weekend and then I'll vote on general@

Just to keep things clean, I personally, always try to make sure
3 IPMC votes are cast excplicitly on general@

Thanks,
Roman.

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 20 Sep 2013 22:27:25 -0700",Re: [RESULT] [VOTE] Release Apache Spark 0.8.0-incubating (RC6),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks everyone who contributed to this vote. I've created an IPMC vote here:

http://mail-archives.apache.org/mod_mbox/incubator-general/201309.mbox/%3CCABPQxsu6XwYMUxWKwRsXOvT%2B3-8%3DTTWwyHwJe_hWVb%3DNPxWuuw%40mail.gmail.com%3E

- Patrick


"
Reynold Xin <rxin@cs.berkeley.edu>,"Sat, 21 Sep 2013 10:27:58 -0700",Fwd: JVMs on single cores but parallel JVMs.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","FYI

---------- Forwarded message ----------
From: Kevin Burton <burtonator@gmail.com>
Date: Sat, Sep 21, 2013 at 9:30 AM
Subject: Re: JVMs on single cores but parallel JVMs.
To: mechanical-sympathy@googlegroups.com


ok... so I'll rephrase this a bit.

You're essentially saying that GC and background threads will need to run
to prevent foreground threads from stalling. GC , network IO, background
filesystem log flushes, etc.

... and if you're only running on ONE core this will preempt your active
threads and will increase latency.

And I guess the theory is that if you have another core free, why not just
let that other core step in and help split the load so you can have a
smaller ""stop the world"" interval.

I guess that makes some sense and probably applies to a lot of workloads.

Some points:

 - in our load, we are usually about 100% CPU on the current thread, and
100% on the other CPU... so if we trigger GC in the core, the secondary
core isn't going to necessarily execute faster.  In fact it might execute
slower due to memory locality (depending on the configuration).  I think in
most situations, applications are over-provisioned to account for load
spikes so this setup might actually warrant deployment as it would work in
practice.

- This idea is partially a distributed computing fallacy.  This GC doesn't
scale to hundreds of cores...If you're on a 64 core machine splitting out
your VMs so they are smaller, with the entire working set local to that
CPU, and segmenting GC to that core, seems to make the most sense.  You
would have GC pauses but they would be 1/Nth (where N = num of cores) of
your entire application GC hit.

- You can still use a CMS approach here where you GC in the background,
it's just done on one core with another thread.

- GC isn't infinitely parallel... You aren't going to send part of your
heap over the network and do a map/reduce style GC across 1024 servers
within a cluster.  Data locality is important.  Keeping the JVMs small and
local to the core and having lots of them seems to make a lot of sense.

- the fewer JVMs you have the more JDK lock contention you can have.
 Things like SSL are still contended (yuk) ... though JDK 1.7 has
definitely improved the situation.

... one issue of course is that OpenJDK doesn't share the permanent
generation classes.  So you see like a 128MB hit per JVM.  This works out
to about $2 per month per JVM for us so not really the end of the world.

Kevin


You received this message because you are subscribed to the Google Groups
""mechanical-sympathy"" group.
email to mechanical-sympathy+unsubscribe@googlegroups.com.
For more options, visit https://groups.google.com/groups/opt_out.
"
Shane Huang <shannie.huang@gmail.com>,"Sun, 22 Sep 2013 12:01:17 +0800",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"I summarized the opinions about Config in this post and added a comment on
SPARK-544.
Also post here below:

1) Define a Configuration class which contains all the options available
for Spark application. A Configuration instance can be de-/serialized
from/to a formatted file. Most of us tend to agree that Typesafe Config
library is a good choice for the Configuration class.
2) Each application (SparkContext) has one Configuration instance and it is
initialized by the application which creates it (either coded in app (apps
could explicitly read from io stream or command line arguments), or system
properties, or env vars).
3) For an application the overriding rule should be code > system
properties > env vars. Over time we will deprecate the env vars and maybe
even system properties.
4) When launching an Executor on a slave node, the Configuration is firstly
initialized using the node-local configuration file as default (instead of
the env vars at present), and then the Configuration passed from
application driver context will override specific options specified in
default. Certain options in app's Configuration will always override those
in node-local, because these options need to be the consistent across all
the slave nodes, e.g. spark.serializer. In this case if any such options is
other hand, some options in app's Config will never override those in
node-local. as they're not meat to be set in app, e.g. spark.local.dir







-- 
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*
"
Reynold Xin <rxin@cs.berkeley.edu>,"Sat, 21 Sep 2013 21:05:28 -0700",Re: Propose to Re-organize the scripts and configurations,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks, Shane. Can you also link to this mailing list discussion from the
JIRA ticket?


--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org




"
Shane Huang <shannie.huang@gmail.com>,"Sun, 22 Sep 2013 12:07:05 +0800",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"And I created a new issue SPARK-915 to track the re-org of scripts as
SPARK-544 only talks about Config.
https://spark-project.atlassian.net/browse/SPARK-915







-- 
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*
"
Shane Huang <shannie.huang@gmail.com>,"Sun, 22 Sep 2013 12:13:03 +0800",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"Done






-- 
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*
"
"""Xia, Junluan"" <junluan.xia@intel.com>","Mon, 23 Sep 2013 02:13:52 +0000",RE: Propose to Re-organize the scripts and configurations,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Shane and I will focus on configure and script these two features in next few days, and look forward to merging it to spark ASAP:)




ss.
he



--
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*

"
Roman Shaposhnik <rvs@apache.org>,"Mon, 23 Sep 2013 09:57:21 -0700",Project name conflict heads-up,dev@spark.incubator.apache.org,"Hi!

While testing Spark as part of the Bigtop I've come
across the following conflict on Debian-based systems:

$ apt-cache show spark
Package: spark
Priority: optional
Section: universe/devel
Description-en: SPARK programming language toolset
 SPARK is a formally-defined computer programming language based on the
 Ada programming language, intended to be secure and to support the
 development of high integrity software used in applications and systems
 where predictable and highly reliable operation is essential either for
 reasons of safety or for business integrity.
 .
 This package contains the tools necessary for checking if programs adhere
 to the SPARK rules and the tools to show freedom of runtime exceptions in
 those programs. To compile SPARK programs use any standards-compliant Ada
 compiler, such as GNAT.
Homepage: http://libre.adacore.com/libre/tools/spark-gpl-edition/


This has an obvious ramifications for Bigtop packaging, but
it may also have ramifications for PODLINGNAMESEARCH
for Apache Spark (incubating).

Speaking of which -- has anybody else started PODLINGNAMESEARCH
already?

Thanks,
Roman.

"
Konstantin Boudnik <cos@apache.org>,"Mon, 23 Sep 2013 10:52:26 -0700",Re: Project name conflict heads-up,dev@spark.incubator.apache.org,"I suggest to rename our package to spark-asf.
  Cos


"
Konstantin Boudnik <cos@boudnik.org>,"Mon, 23 Sep 2013 10:52:01 -0700",Re: Project name conflict heads-up,dev@spark.incubator.apache.org,"Let's call it spark-asf, perhaps?


"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 23 Sep 2013 11:22:45 -0700",Re: Project name conflict heads-up,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yeah, Chris created one for Apache Spark:
https://issues.apache.org/jira/browse/PODLINGNAMESEARCH-36


"
Sean Mackrory <mackrorysd@gmail.com>,"Mon, 23 Sep 2013 11:43:30 -0700",Re: Project name conflict heads-up,"""dev@bigtop.apache.org"" <dev@bigtop.apache.org>","I've been worried about this for a while, and I think we need to come up
with a strategy for dealing with it in Bigtop. In this case, it's just a
coincidence that two separate projects are named Spark. However I recently
submitted a patch for packaging Avro as a separate component in Bigtop and
had to deal with the fact that a minority of the systems we support already
packaged part of Avro in a package named 'avro', so I had to use
'avro-libs', 'avro-tools' and 'avro-doc'. Fedora's also going to start
releasing packages for Hadoop, so this problem of name conflicts is only
going to get worse.



"
Konstantin Boudnik <cos@apache.org>,"Mon, 23 Sep 2013 13:11:44 -0700",Re: Project name conflict heads-up,dev@spark.incubator.apache.org,"I'd say Fedora needs to deal with it on their end, considering that they are
coming into this post-Bigtop. Thoughts?


"
Tom Graves <tgraves_cs@yahoo.com>,"Mon, 23 Sep 2013 13:54:19 -0700 (PDT)",logging in the unit tests,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Does anyone know how to turn on the logging in the unit tests? I'm using maven and all I see in the txt output files are the test names. I want to see the normal Spark logging for stdout/stderr.

Thanks,
Tom"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 23 Sep 2013 14:01:14 -0700",Re: logging in the unit tests,"dev@spark.incubator.apache.org, Tom Graves <tgraves_cs@yahoo.com>","They should be in <module>/target/unit-tests.log -- or rather
core/target/unit-tests.log for core. I think there is a bug somewhere
where sbt puts it in that directory while maven puts it in
core/core/target/unit-tests.log.

Its configurable in core/src/test/resources/log4j.properties

Thanks
Shivaram


"
Jay Vyas <jayunit100@gmail.com>,"Mon, 23 Sep 2013 18:24:22 -0400",Re: Project name conflict heads-up,"""dev@bigtop.apache.org"" <dev@bigtop.apache.org>","Why not call it bigtop-spark .. ?

Seems to me like nobody but the root project itself should monopolize the hadoop distro primary names since there are so many builds out there... Right?

re
y
d
dy
te:



"
Konstantin Boudnik <cos@apache.org>,"Mon, 23 Sep 2013 16:01:32 -0700",Re: Project name conflict heads-up,dev@bigtop.apache.org,"
Well, because it isn't bigtop's spark, to start with ;)


What's is root project, sorry?


"
sivaraman Cj <sivacjms@gmail.com>,"Mon, 23 Sep 2013 17:51:26 -0700",Want to Contribute to spark,dev@spark.incubator.apache.org,"Hi,

My name is Sivaraman.C.J. I have 7 years of software development
experience and i got graduated from University of Minnesota, Twin
Cities in May 2013 and I currently work in Yahoo. I dont have any
experience in opensource contribution and I would like to start now.

Regards
Shiva

"
Michael Joyce <joyce@apache.org>,"Tue, 24 Sep 2013 09:06:18 -0700",Re: Want to Contribute to spark,dev@spark.incubator.apache.org,"Hi Shiva,

Thanks for stopping by! The best way to get started is to head over to the
wiki at [1]. There's a ""Contributing to Spark"" page that should help you
get started in the right direction. There's also relevant links to the JIRA
and repository in there. If you have any questions or need a place to start
feel free to ask. There's tons of friendly people here on the list that are
more than happy to help out!

[1] https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Cheers!


-- Joyce



"
Vadim Chekan <kot.begemot@gmail.com>,"Tue, 24 Sep 2013 11:20:10 -0700",Re: Want to Contribute to spark,dev@spark.incubator.apache.org,"Hi Shiva,
In addition to what Michael said, create an account on github.com, find and
fork spark https://github.com/apache/incubator-spark and commit your
modifications to *your* fork of spark. Than submit so called ""pull request""
on github site. Spark developers will see the request and will merge it
into the main repository.
Its a good idea to discuss what you intend to do before you start hacking.

Cheers,
Vadim.







-- 
explicitly specified
"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 24 Sep 2013 16:09:55 -0700",Re: Spark Streaming threading model,dev@spark.incubator.apache.org,"Responses inline. Sorry for the delay. Hope this helps!


TD




The way our input streams works is that there is only one receiver per
input stream. So if you have set up only one kafka input stream then all
the data will go to one node running the receiver. For parallel ingestion
of data, you need to create multiple kafka input streams and partition the
kafka topics across them. Then there will be multiple receivers. and

The line [0] points to is not exactly the reason. What that lines does it
runs a dummy job to make sure that all the worker nodes have connected to
master before starting the receivers.


Unless I missed some part, It looks like we might have a concurrency issue
current block generator was not exactly designed for parallel insertion and
so this is something. For the time being a simple solution would be to add
a ""synchronized"" in += function.

I actually have plans to make major changes to the block generation code
and I will keep in mind of this issue.


The justification behind the design of the NetworkInputTracker is as
follows. The ReceiverExecutor thread is meant to start the receiver and
stay blocked until the receivers are shutdown. This is because the way
receivers are started is by making an RDD out of the receiver objects and
starting a Spark job to distribute and execute them. Hence the receivers
are run as long running tasks in the workers, and in case a receiver dies
(node dies), Spark scheduler's in built fault-tolerance model will rerun
the task (i.e, start the receiver) on another node. Hence the Spark job
executing the receiver continues to be alive and the thread that started
the Spark job will stay blocked. Also this thread does not need to
communicate with any other thread. Tasks run through the actor model must
be short tasks that does not block the actor's thread, hence this activity
is fundamentally unsuited to be implemented using actors. Hence we used a
raw thread to start the Spark job that runs the executors. All other actual
communication between the receiver and tracker is handled by the
NetworkInputTrackerActor and NetworkReceiverActor (running with the
receiver).

Also, note that there is a mix of three concurrency models in the
NetworkReceiver -
(i) NetworkReceiverActor communicate with the master about block names,
(ii) the receiving thread receives data from some source and pushes it to
the block generator,
(iii) block generator has its own timer periodically forms blocks and
another thread that pushes the blocks to Spark's block manager
(blockPushingThread)

Again each of these threads have different requirements.
In case (iii), the ""blockPushingThread"" in the block generator needs to do
tasks (pushing blocks) that may take a bit of time to be done. That makes
it slightly unsuitable for being implemented as actors, for the reasons
stated earlier. Also, the timer and block pushing thread are different as
the timer running in it own thread should be execute strictly periodically
and not be delayed by variable delays in the block pushing thread.
Case (ii) could have been implemented as an actor as it just inserts a
record on an arraybuffer (i.e.m very small task). However, with rates of
more than 100K records received per second, I was unsure what the overhead
of sending each record as a message through the actor library would be
like. For this reason, we use actors only in the control plane and not the
data plane.
 In case (i), since all control plane messaging is done by actors, this was
implemented as actors.

I probably went into more detail that you wanted to know. :) Apologies if
that is case.

That said, there may definitely be a better way of implementing this
without reducing performance significantly. I am totally open for
discussion in this matter.



"
Konstantin Boudnik <cos@apache.org>,"Tue, 24 Sep 2013 22:16:25 -0700",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"Late to the game, but... Bigtop is packaging Spark now as a part of the
standard distribution - our release 0.7.0 is around the corner. And we do it
in the same way that has been done for Hadoop. Perhaps it worth looking
into...

Lemme know if you have any questions,
  Cos

rote:
rs
be
hard
ass
ors,
d we
ery
ed
nd
ly
any
h as
sion
and
"
Shane Huang <shannie.huang@gmail.com>,"Wed, 25 Sep 2013 13:20:39 +0800",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"I think it's good to have Bigtop to package Spark. But I






-- 
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*
"
Shane Huang <shannie.huang@gmail.com>,"Wed, 25 Sep 2013 13:26:43 +0800",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"I think it's good to have Bigtop to package Spark. But in this track we're
just targeting enhancing the usability of Spark itself without Bigtop.
 After all, few of our customers used Bigtop.





-- 
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*
"
Konstantin Boudnik <cos@apache.org>,"Tue, 24 Sep 2013 22:31:53 -0700",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"
Bigtop - in this particular application - is just a way of building
packages.

But what I am saying is that Bigtop has this structure in place already - it
can be just copied.

Cos

te:
te:
 do
com
is
d by
bin
stem
on
ion
l,
ns
his
nce
ant
"
Gerard Maas <gerard.maas@gmail.com>,"Wed, 25 Sep 2013 21:30:11 +0200",Re: Spark Streaming threading model,dev@spark.incubator.apache.org,"Hi Tathagata,

Many thanks for the extended answer and the clarifications on the kafka
data distribution in the cluster.

There are many points to handle, so, to start somewhere:

Case (ii) could have been implemented as an actor as it just inserts a
simplified test scenario that isolates the data cummulator case and compare
the performance of both models (actors vs threads with proper locking)
under different levels of concurrency.
Do you think this could be helpful for the project? I'm looking to
contribute and this could be an interesting starting point.

Absolutely not. The more, the better :-)

-kr, Gerard.
"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 25 Sep 2013 15:00:16 -0700",Re: Spark Streaming threading model,dev@spark.incubator.apache.org,"
Yes! actor vs threads with locking is a great test to do, since for the
kafka (and who know what other sources in future), the block generator has
to support multiple thread ingestion. I think one also needs to compare
with single thread without locking (the current model). If single thread
without locking is the fastest and thread with locking not so bad compared
to actors, then it may be better to leave the ingestion without locks for
maximum throughput for single-thread sources (e.g. Socket, and most others)
and add a lock for multi-thread sources like Kafka.



"
Roman Shaposhnik <rvs@apache.org>,"Wed, 25 Sep 2013 15:24:45 -0700",Spark 0.8.0: bits need to come from ASF infrastructure,dev@spark.incubator.apache.org,"Hi!

I see that the current download link published here:
  http://spark.incubator.apache.org/releases/spark-release-0-8-0.html
leads to:
  http://spark-project.org/download/spark-0.8.0-incubating.tgz

This needs to be corrected to be (roughly):
   http://www.apache.org/dyn/closer.cgi/incubator/spark/spark-0.8.0-incubating/spark-0.8.0-incubating.tgz

In fact, at some point it may be worth auditing your website
source and eliminate references to spark-project.org that
should really be pointing back to ASF.

Thanks,
Roman.

"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 25 Sep 2013 15:40:41 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Was there announcement that 0.8 artifact had been pushed to
http://www.apache.org/dist/incubator/spark ?

I thought the link should points to
http://www.apache.org/dist/incubator/spark/spark-0.8.0-incubating/spark-0.8.0-incubating.tgz


- Henry


"
Roman Shaposhnik <rvs@apache.org>,"Wed, 25 Sep 2013 15:45:32 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,dev@spark.incubator.apache.org,"
For the freshly released bits it is typically better to point to dyn/closer.cgi
unless you want to start building negative karma with ASF infra ;-)

Thanks,
Roman.

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 25 Sep 2013 15:48:42 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey we've actually distributed our artifacts through amazon cloudfront
in the past (and that is where the website links redirect to).

Since the apache mirrors don't distribute signatures anyways, what is
the difference between linking to an apache mirror vs using a more
robust CDN? If people want to verify the downloads they need to go to
the apache root in either case.

Is this just a cultural thing or is there some security reason?

- Patrick


"
Roman Shaposhnik <rvs@apache.org>,"Wed, 25 Sep 2013 15:56:26 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,dev@spark.incubator.apache.org,"
True, but apache dist does. IOW, it is not uncommon for those
having an automated build/fetching systems to get bits from
one of the mirrors and then get the hashes directly from dist.

In your current case, I don't think I know of a way to do that.

Now, you may say that the current CDN you guys are you using
is functioning like a mirror -- well, I'd say that it needs to be
called out like one then.

Otherwise, as a naive user I *really* have to guess where
to get the hashes.


A bit of both I guess.

Thanks,
Roman.

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 25 Sep 2013 16:08:42 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yep, we definitely need to just directly point people the location at
apache.org where they can find the hashes. I just updated the release
notes and downloads page to point to that site.

I just wanted to point out that mirroring these through a CDN seems
philosophically the same as mirroring through Apache, since in neither
case do we expect the users to trust the artifact they download. We
just need to be more explicit that we are, indeed, mirroring and
explain that the trusted root is at apache.org

- Patrick


"
Evan Chan <ev@ooyala.com>,"Wed, 25 Sep 2013 23:42:37 -0700",Re: Propose to Re-organize the scripts and configurations,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Shane, and others,

Let's work together on the configuration thing.   I had proposed in a
separate thread to use Typesafe Config to hold all configuration
(essentially a configuration class, but which can read from both JSON files
as well as -D java command line args).

Typesafe Config works much much better than a simple config class, and also
better than Hadoop configs.  It also has advantages over JSON (more
readable, comments).   It would also be the easiest to transition from the
current scheme, since the current java system properties can be seamlessly
integrated.

I would be happy to contribute this back soon because it is also a big pain
point for us.  I also have extensive experience with both Typesafe Config
and other config systems.

I would definitely start with SparkContext and work our way out from there.
   In fact I can submit a patch for everyone to test out fairly quickly
just for SparkContext.

-Evan







-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
"""Xia, Junluan"" <junluan.xia@intel.com>","Thu, 26 Sep 2013 08:24:54 +0000",RE: Propose to Re-organize the scripts and configurations,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Chan

Shane and I happened to try to contribute configure feature to spark, could we cooperate to implement it?


Let's work together on the configuration thing.   I had proposed in a
separate thread to use Typesafe Config to hold all configuration (essentially a configuration class, but which can read from both JSON files as well as -D java command line args).

Typesafe Config works much much better than a simple config class, and also better than Hadoop configs.  It also has advantages over JSON (more
readable, comments).   It would also be the easiest to transition from the
current scheme, since the current java system properties can be seamlessly integrated.

I would be happy to contribute this back soon because it is also a big pain point for us.  I also have extensive experience with both Typesafe Config and other config systems.

I would definitely start with SparkContext and work our way out from there.
   In fact I can submit a patch for everyone to test out fairly quickly just for SparkContext.

-Evan



e:

op.
r
n
.



--
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

"
Chester <chesterxgchen@yahoo.com>,"Thu, 26 Sep 2013 05:17:37 -0700",Re: Propose to Re-organize the scripts and configurations,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for Typesafe configFactory and config

Sent from my iPad


s
o


n
.
te:
e


ttp://www.twitter.com/ooyala>

"
Michael Malak <michaelmalak@yahoo.com>,"Thu, 26 Sep 2013 11:27:21 -0700 (PDT)",Kafka not shutting down cleanly; Actor serializtion?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Tathagata:


I don't believe Kafka streams are being shut down cleanly, which implies that the most recent Kafka offsets are not being committed back to Zookeeper, which implies starting/restarting a Spark Streaming process would result in duplicate events.

The simple Spark Streaming code (running in local mode) pasted below at the end of this e-mail, which uses a hard-coded queueStream as its only input stream, exits cleanly when the presence of the sentinel file is detected. However, if the queueStream is replaced with a kafkaStream, the process never exits (unless I put a System.exit() as the very last line -- to forcibly kill all threads).

In attempting to understand the Kafka shutdown process, I traced through the Spark Streaming codebase with println()s. I noticed the following:

1. Although KafkaInputDStream.scala initializes the class member variableconsumerConnector in onStart(), I don't see a corresponding consumerConnector.shutdown() anywhere such as in the onStop(). It is my understanding that it is the consumer shutdown() that commits the offsets back to Zookeeper. See the Kafka example athttps://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example#ConsumerGroupExample-FullSourceCode

2. There is a similar apparent asymmetry with executorPool, where it is not released in the onStop(). (A further minor encumbrance is that it is a variable local to onStart() rather than being a class member variable)

3. Through my println() tracing and Akka debug-level logging, I'm not seeing NetworkReceiverActor ever receiving a StopReceiver message from ReceiverExecutor. From some poking around and testing, it seems possible to successfully send any type of message to NetworkReceiverActor only prior to that NetowrkReceiverActor being serialized into an RDD on line 146 of NetworkInputTracker.scala
https://github.com/mesos/spark/blob/branch-0.8/streaming/src/main/scala/org/apache/spark/streaming/NetworkInputTracker.scala#L146

Prior to the actor being put into the RDD, messages can be sent to the actor, but not after the actor is put into the RDD. Is it possible that Akka actors are intolerant of being serialized?

4. I noticed a lot of ""TODO"" comments sprinkled throughout the code relating to shutdown/termination/cleanup.

My biggest concern is #3 above, because if my suppositions are correct, then there might be some major re-architecting involved. The other issues I could probably fix on my own and commit back.


import spark.streaming._

object SimpleSparkStreaming {
 @volatile var receivedStop = false
 val sentinelFile = new java.io.File(""/home/mmalak/stop"")

 def main(args: Array[String]) {
  val Array(master, zkQuorum, broker, group, topics, numThreads) = args

  sentinelFile.delete

  val ssc = new StreamingContext(master, ""SimpleBeta"", Seconds(1), System.getenv(""SPARK_HOME""), Seq(""./target/scala-2.9.2/my.jar""))
  ssc.checkpoint(""/home/mmalak/checkpointing"")

  ssc.queueStream(new scala.collection.mutable.Queue[spark.RDD[Int]] += ssc.sparkContext.makeRDD(List(1))).foreach(rdd =>
   println(""receivedStop["" + receivedStop + ""]""))
  ssc.start()

  while (!sentinelFile.exists) {Thread.sleep(1000)}
  println(""Stop detected"")
  receivedStop = true
  ssc.stop()
  println(""Exiting main"")
 }
}
"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 26 Sep 2013 12:03:34 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes, mirroring from CDN technically the same but we want to make sure
users know the link come from ASF domain =)

So we need to update the
http://spark.incubator.apache.org/releases/spark-release-0-8-0.html
link to download src and binary distributions.

- Henry


"
Chris Mattmann <mattmann@apache.org>,"Thu, 26 Sep 2013 15:50:42 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Guys,

Yep the link should by the dyn/closer.cgi link on the website and +1
to Roman's comment about auditing spark-project.org links to be replaced
with ASF counterparts.

Cheers,
Chris






"
Evan Chan <ev@ooyala.com>,"Thu, 26 Sep 2013 16:09:32 -0700",Re: Propose to Re-organize the scripts and configurations,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Shane, Junluan,

Definitely, let's cooperate.  Should we chat offline?

-Evan







-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 26 Sep 2013 19:02:41 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Chris et al,

I'm -1 on this because it has many negative consequences for our existing users:

1. Users who do automated downloads based on our posted URL's (of
which we get many thousands each release) will no longer work. Now if
they do ""wget XXX"" with our posted link, it will fail in a weird way
to due to the redirect page. Is there a version of the closer.cgi
script which just performs 302 redirects instead of asking me to click
on a link?

2. All other users have to click through an additional page to
download the software.

3. Amazon Cloudfront is, as a whole, much more reliable and higher
bandwidth than the mirror network.

These are my concerns, that basically we're causing our users to have
a much worse experience. I've identified these concerns with moving to
the apache mirror, but perhaps I've overlooked some benefits that
would counteract these. Are there benefits?

I completely agree that we need to send users to the signatures and
hashes at the Apache release site (to verify the release). So I did
add the link to this directly adjacent to the download.

- Patrick


"
Roman Shaposhnik <rvs@apache.org>,"Thu, 26 Sep 2013 19:16:16 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,dev@spark.incubator.apache.org,"
Nobody is saying that closer.cgi should be the only link. But it should
be the leading link. IOW, if you could say something like:

click [closer.cgi |here] to download the XXX release of Spark from
the Apache Software Foundation mirror network or [here] to download
it from Amazon's CDN it would be fine.


My biggest problem with Amazon is that it seems to disallow listing.
That said -- it doesn't matter if it become a secondary link that
you make available.

Hope this helps.

Thanks,
Roman.

"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 27 Sep 2013 02:23:24 +0000",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""<dev@spark.incubator.apache.org>"" <dev@spark.incubator.apache.org>","Hi Patrick will reply in more detail later but please know that linking to the apache download page is not a request it's a requirement. I will explain more in a bit.

Cheers,
Chris

Sent from my iPhone


 users:
te:
rg>
e:
t

"
Shane Huang <shannie.huang@gmail.com>,"Fri, 27 Sep 2013 10:24:46 +0800",Re: Propose to Re-organize the scripts and configurations,dev@spark.incubator.apache.org,"I have created a pull request to address the basic needs of our customer
for separating the admin and user scripts. Link here
https://github.com/apache/incubator-spark/pull/21. Please kindly review.
And we can also discuss if there's more functionality needed.





-- 
*Shane Huang *
*Intel Asia-Pacific R&D Ltd.*
*Email: shengsheng.huang@intel.com*
"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 26 Sep 2013 19:59:10 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,dev@spark.incubator.apache.org,"Thanks Roman and Chris,

I see here http://www.apache.org/dev/release.html#mirroring that ""Project
download pages must link to the mirrors"" but I don't see anything about
ordering.

I'm definitely +1 for including a link to the apache mirrors as required
and providing the Cloudfront link first since this seems to satisfy the
apache requirements and provide a better experience for users.

Patrick. Thanks again for all your hard work on this release and for
pushing back on parts of the Apache process as you go. That's how
do-ocracies stay healthy and evolve.

"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 26 Sep 2013 23:11:12 -0400",Re: Spark 0.8.0: bits need to come from ASF infrastructure,dev@spark.incubator.apache.org,"Maybe we can replace the link to ""official Apache download site"" in the release notes to point to the mirrors? Do the mirrors all have signatures on them too?

Matei


""Project
about
required
the
linking to
if
click
have
to
<mattmann@apache.org>
+1
<dev@spark.incubator.apache.org>
at
release
seems
neither
We
<pwendell@gmail.com>
using a
go to


"
Chris Mattmann <mattmann@apache.org>,"Thu, 26 Sep 2013 20:18:05 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Guys,

OK flight landed, so have a sec to reply in more detail:




Apache also has many 10s of thousands of downloads each release, depending
on
the project. The best example I can think of is Open Office which receives
10M downloads/day IIRC -- yes Ooo has some special downloading infra help
too,
but beyond that popular projects like Apache Lucene and Solr regularly see
4000+
downloads per day and the ASF mirroring system works fine.



You can do something like e.g.,

curl 
""http://www.apache.org/dyn/closer.cgi/incubator/spark/spark-0.8.0-incubatin
g/spark-0.8.0-incubating-bin-cdh4.tgz"" | grep http | grep tgz| sort -n
(and then some HTML strip magic)

Even better if you have Apache Tika installed, something like:

tika -t 
""http://www.apache.org/dyn/closer.cgi/incubator/spark/spark-0.8.0-incubatin
g/spark-0.8.0-incubating-bin-cdh4.tgz"" | grep http | grep tgz

produces:

http://mirror.nexcess.net/apache/incubator/spark/spark-0.8.0-incubating/spa
rk-0.8.0-incubating-bin-cdh4.tgz
 http://mirror.nexcess.net/apache/incubator/spark/spark-0.8.0-incubating/sp
ark-0.8.0-incubating-bin-cdh4.tgz
 http://apache.cs.utah.edu/incubator/spark/spark-0.8.0-incubating/spark-0.8
.0-incubating-bin-cdh4.tgz
 http://apache.tradebit.com/pub/incubator/spark/spark-0.8.0-incubating/spar
k-0.8.0-incubating-bin-cdh4.tgz
 http://www.carfab.com/apachesoftware/incubator/spark/spark-0.8.0-incubatin
g/spark-0.8.0-incubating-bin-cdh4.tgz
 http://apache.petsads.us/incubator/spark/spark-0.8.0-incubating/spark-0.8.
0-incubating-bin-cdh4.tgz
 http://www.trieuvan.com/apache/incubator/spark/spark-0.8.0-incubating/spar
k-0.8.0-incubating-bin-cdh4.tgz
 http://mirrors.ibiblio.org/apache/incubator/spark/spark-0.8.0-incubating/s
park-0.8.0-incubating-bin-cdh4.tgz
 http://mirror.olnevhost.net/pub/apache/incubator/spark/spark-0.8.0-incubat
ing/spark-0.8.0-incubating-bin-cdh4.tgz
 http://psg.mtu.edu/pub/apache/incubator/spark/spark-0.8.0-incubating/spark
-0.8.0-incubating-bin-cdh4.tgz
 http://apache.claz.org/incubator/spark/spark-0.8.0-incubating/spark-0.8.0-
incubating-bin-cdh4.tgz
 http://mirror.metrocast.net/apache/incubator/spark/spark-0.8.0-incubating/
spark-0.8.0-incubating-bin-cdh4.tgz
 http://apache.mirrors.lucidnetworks.net/incubator/spark/spark-0.8.0-incuba
ting/spark-0.8.0-incubating-bin-cdh4.tgz
 http://mirrors.gigenet.com/apache/incubator/spark/spark-0.8.0-incubating/s
park-0.8.0-incubating-bin-cdh4.tgz
 http://www.poolsaboveground.com/apache/incubator/spark/spark-0.8.0-incubat
ing/spark-0.8.0-incubating-bin-cdh4.tgz
 http://www.bizdirusa.com/mirrors/apache/incubator/spark/spark-0.8.0-incuba
ting/spark-0.8.0-incubating-bin-cdh4.tgz
 http://mirror.sdunix.com/apache/incubator/spark/spark-0.8.0-incubating/spa
rk-0.8.0-incubating-bin-cdh4.tgz
 http://download.nextag.com/apache/incubator/spark/spark-0.8.0-incubating/s
park-0.8.0-incubating-bin-cdh4.tgz
 http://www.motorlogy.com/apache/incubator/spark/spark-0.8.0-incubating/spa
rk-0.8.0-incubating-bin-cdh4.tgz
 http://mirror.cc.columbia.edu/pub/software/apache/incubator/spark/spark-0.
8.0-incubating/spark-0.8.0-incubating-bin-cdh4.tgz
 http://mirror.tcpdiag.net/apache/incubator/spark/spark-0.8.0-incubating/sp
ark-0.8.0-incubating-bin-cdh4.tgz
 http://apache.mirrors.hoobly.com/incubator/spark/spark-0.8.0-incubating/sp
ark-0.8.0-incubating-bin-cdh4.tgz
 http://www.eng.lsu.edu/mirrors/apache/incubator/spark/spark-0.8.0-incubati
ng/spark-0.8.0-incubating-bin-cdh4.tgz
 http://apache.mesi.com.ar/incubator/spark/spark-0.8.0-incubating/spark-0.8
.0-incubating-bin-cdh4.tgz
 http://mirror.symnds.com/software/Apache/incubator/spark/spark-0.8.0-incub
ating/spark-0.8.0-incubating-bin-cdh4.tgz
 http://mirror.reverse.net/pub/apache/incubator/spark/spark-0.8.0-incubatin
g/spark-0.8.0-incubating-bin-cdh4.tgz
 http://apache.osuosl.org/incubator/spark/spark-0.8.0-incubating/spark-0.8.
0-incubating-bin-cdh4.tgz
 http://www.interior-dsgn.com/apache/incubator/spark/spark-0.8.0-incubating
/spark-0.8.0-incubating-bin-cdh4.tgz
 http://mirror.cogentco.com/pub/apache/incubator/spark/spark-0.8.0-incubati
ng/spark-0.8.0-incubating-bin-cdh4.tgz
 http://apache.spinellicreations.com/incubator/spark/spark-0.8.0-incubating
/spark-0.8.0-incubating-bin-cdh4.tgz
 http://www.dsgnwrld.com/am/incubator/spark/spark-0.8.0-incubating/spark-0.
8.0-incubating-bin-cdh4.tgz
 http://apache.mirrors.pair.com/incubator/spark/spark-0.8.0-incubating/spar
k-0.8.0-incubating-bin-cdh4.tgz
 http://mirrors.sonic.net/apache/incubator/spark/spark-0.8.0-incubating/spa
rk-0.8.0-incubating-bin-cdh4.tgz
 http://apache.mirrors.tds.net/incubator/spark/spark-0.8.0-incubating/spark
-0.8.0-incubating-bin-cdh4.tgz
 http://www.gtlib.gatech.edu/pub/apache/incubator/spark/spark-0.8.0-incubat
ing/spark-0.8.0-incubating-bin-cdh4.tgz
  
http://www.eu.apache.org/dist/incubator/spark/spark-0.8.0-incubating/spark-
0.8.0-incubating-bin-cdh4.tgz
 http://www.us.apache.org/dist/incubator/spark/spark-0.8.0-incubating/spark
-0.8.0-incubating-bin-cdh4.tgz



That's just to target a mirror, but if they direct link to the actual
artifact using the naming convention,
it's not a big deal.


Based on what facts? Not trying to be argumentative but would love to see
some quantitative data
on that statement.


The benefits are pointing users to the bits as they are provided by
Apache's
mirror'ing system. Apache Spark (incubating)'s official () download home
is
at the Apache mirror'ing system.



Thanks for considering all the options Patrick. Hope the above helps to
explain.

Cheers,
Chris




"
Chris Mattmann <mattmann@apache.org>,"Thu, 26 Sep 2013 20:26:51 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Andy,



Technically we ought to promote Apache's mirroring system as the Project
Endorsed
home for the project. As an Apache member and someone who values what the
Foundation
does for its projects and communities I don't think that's much to ask.

If you guys feel strongly about the ordering of the Cloud Front first I'm
open to it, I would just appreciate seeing some existing data showing that
you guys have users who have tried the mirroring system from the ASF and it
hasn't performed as well as the Amazon one.


Here here. This project doesn't have a ""boss"" and it's not me :)
I'm just trying to spread my Apache knowledge and help you guys wear your
Apache hats too since the project lives at the ASF now. I think you'll find
the benefits of wearing those hats are many :)

Cheers,
Chris




"
Chris Mattmann <mattmann@apache.org>,"Thu, 26 Sep 2013 20:27:35 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Matei yep they have the signatures on them too.

Cheers,
Chris





"
Reynold Xin <rxin@cs.berkeley.edu>,"Thu, 26 Sep 2013 20:31:35 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Is there a way we can track the number of downloads with the apache mirrors?





-- 

--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 26 Sep 2013 21:04:46 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
Chris - I checked a bunch of the mirrors and none of them have the
signatures... am I looking in the wrong place?

http://www.interior-dsgn.com/apache/incubator/spark/spark-0.8.0-incubating/
http://mirror.tcpdiag.net/apache/incubator/spark/spark-0.8.0-incubating/
http://apache.tradebit.com/pub/incubator/spark/spark-0.8.0-incubating/

In response to the comment about thousands of downloads - I didn't
mean at all to suggest that Apache couldn't handle this number, in
fact, I'm sure they can! I just wanted to point out that we are
keeping in mind the existing habits and processes of our user base,
and trying to make the transition smooth for them.

- Patrick

"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 27 Sep 2013 13:06:38 -0400",Re: Spark 0.8.0: bits need to come from ASF infrastructure,dev@spark.incubator.apache.org,"If the mirrors don't have the signatures, then we should probably link to the mirrors and the signatures separately. It's definitely important to have a link to the mirrors so people can get this through ASF infrastructure without hitting only the main server.

It's true that they don't seem to have them, even for other projects -- for example check out http://mirror.tcpdiag.net/apache/hadoop/common/hadoop-2.0.5-alpha/.

Matei


http://www.interior-dsgn.com/apache/incubator/spark/spark-0.8.0-incubating/
http://mirror.tcpdiag.net/apache/incubator/spark/spark-0.8.0-incubating/


"
Chris Mattmann <mattmann@apache.org>,"Fri, 27 Sep 2013 10:10:15 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yeah the sigs are usually available from here:

http://www.apache.org/dist/<project name>/../

So for us:

http://www.apache.org/dist/incubator/spark/spark-0.8.0-incubating/


Cheers,
Chris






"
Evan Chan <ev@ooyala.com>,"Sat, 28 Sep 2013 15:23:28 -0700",spark.hostPort,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey guys,

Does anyone see a reason to keep the ""spark.hostPort"" system property
around?

It is cleared by lots of tests, set in two places (SparkEnv, and
StandaloneExecutorBackend),
and only used as follows:

Utils.localHostPort() reads it
localHostPort is called from
- BlockManager (which calls it, but _never_ uses the resulting output!)
- MapOutputTracker (which passes it as a value, not sure to where)

Just trying to clean house on properties.

thanks,
Evan

-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 28 Sep 2013 18:44:33 -0400",Re: spark.hostPort,dev@spark.incubator.apache.org,"Hi Evan,

I think this is an old property that isn't used anymore, so it would be good to clean it up and get rid of it.

Matei


output!)
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>


"
Evan Chan <ev@ooyala.com>,"Sat, 28 Sep 2013 16:28:12 -0700",Re: spark.hostPort,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
Does anyone see why we should not just use the base akka.* properties
defined by Akka itself?  Right now the code simply maps spark.akka.* to
akka.* when starting Akka.

The only reasons I see for keeping spark.akka is that we might not want
documentation is excellent, and using akka's own config properties means we
no longer need to keep up with this shim layer.

-Evan






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 28 Sep 2013 16:41:34 -0700",Re: Spark 0.8.0: bits need to come from ASF infrastructure,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Folks,

I updated the site to include the Apache mirror list for each file. I
actually put it is the first download location, before the direct
download link.

I played around with the mirror network a bit, the performance was not
bad, based on sampling from a few vantage points. I found it to be
always worse then CloudFront, but typically not *much* worse. So I
actually think if we can find a way to have a direct link to the
nearest Apache mirror, we could just remove the CloudFront link
entirely.

I looked into it and apparently we're not the first apache project to
have this problem. Many of the bigger projects already use some fancy
selection to embed a direct link to the closest mirror:

http://httpd.apache.org/download.cgi

- Patrick


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 28 Sep 2013 19:53:45 -0400",Re: spark.hostPort,dev@spark.incubator.apache.org,"The main reason I wanted them separate was that people might use Akka in their own application for other things. As such, the Spark Akka properties only affect Akka instances started by Spark. I think we should keep them separate to avoid messing with users' applications.

Matei


to
want
Akka's
means we
be
property
output!)
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>


"
Evan Chan <ev@ooyala.com>,"Sat, 28 Sep 2013 22:51:37 -0700",Re: spark.hostPort,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Got it.  How about we make it such that any property spark.akka.* will be
forwarded to akka as akka.* (ie with the ""spark."" part stripped).   So
there is no need to manually transcribe properties over.





-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 29 Sep 2013 10:58:35 -0400",Re: spark.hostPort,dev@spark.incubator.apache.org,"Sure, that makes sense.

Matei


be
in
properties
them
properties
to
want
Akka's
means
<matei.zaharia@gmail.com
would be
property
output!)
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>


"
Marvin <no-reply@apache.org>,"Sun, 29 Sep 2013 19:29:31 +0000 (UTC)",Incubator PMC/Board report for Oct 2013 ([ppmc]),dev@spark.incubator.apache.org,"

Dear podling,

This email was sent by an automated system on behalf of the Apache Incubator PMC.
It is an initial reminder to give you plenty of time to prepare your quarterly
board report.

The board meeting is scheduled for Wed, 16 October 2013, 10:30:00:00 PST. The report 
for your podling will form a part of the Incubator PMC report. The Incubator PMC 
requires your report to be submitted 2 weeks before the board meeting, to allow 
sufficient time for review and submission (Wed, Oct 2nd).

Please submit your report with sufficient time to allow the incubator PMC, and 
subsequently board members to review and digest. Again, the very latest you 
should submit your report is 2 weeks prior to the board meeting.

Thanks,

The Apache Incubator PMC

Submitting your Report
----------------------

Your report should contain the following:

 * Your project name
 * A brief description of your project, which assumes no knowledge of the project
   or necessarily of its field
 * A list of the three most important issues to address in the move towards 
   graduation.
 * Any issues that the Incubator PMC or ASF Board might wish/need to be aware of
 * How has the community developed since the last report
 * How has the project developed since the last report.
 
This should be appended to the Incubator Wiki page at:

  http://wiki.apache.org/incubator/October2013

Note: This manually populated. You may need to wait a little before this page is
      created from a template.

Mentors
-------
Mentors should review reports for their project(s) and sign them off on the 
Incubator wiki page. Signing off reports shows that you are following the 
project - projects that are not signed may raise alarms for the Incubator PMC.

Incubator PMC


"
Gerard Maas <gerard.maas@gmail.com>,"Mon, 30 Sep 2013 00:24:16 +0200",Re: Spark Streaming threading model,dev@spark.incubator.apache.org,"Hi Tathagata,

 > > I'm personally curious about this point. I could investigate by


I spent a good part of the weekend working on this. So far, I've isolated
the BlockGenerator with its helper threads and I've built some testing
infrastructure to generate records under high load in a controlled way and
inject them into the BlockGenerator using the += method.

So far, I've compared the performance of the original, non-synchronized
version with a simple synchronization around the append method [1].

Here're my findings so far:

Test Scenario:
BlockGenerator: Synchronized
1 data generator: TimeBoxedSeqDataGen(blockGen,10000000))
produced: Totals: records=430000000, time=31247, rec/ms=13761.32
received: 429999961  // note this Data Generator creates data blocks of the
given size, so a sum of blocks should be a multiple of 10000000
diff: 39

Test Scenario:
BlockGenerator: Synchronized
5 data generator: TimeBoxedSeqDataGen(blockGen,10000000))
produced:  Totals: records=920000000, time=421744, rec/ms=2181.42
received: 919999201 // note this Data Generator creates data blocks of the
given size, so a sum of blocks should be a multiple of 10000000
diff: 799

Test Scenario:
BlockGenerator: Original (not-synchronized)
5 data generator: TimeBoxedSeqDataGen(blockGen,10000000))
produced: Totals: records=1180000000, time=67062, rec/ms=17595.66
received:  1181328348
diff: -1328348

[1] BlockGenerator synchronized:
def += (obj: T) {
    currentBuffer.synchronized{
      currentBuffer += obj
    }
}

Three initial findings:

1st - the obvious: the lack of synchronization is an issue. The actual
number of records produced is inconsistent with the number of records
received.

2nd - the impact of synchronization is high: = 17595.66 rec/ms without sync
(current situation), to 13761.32 rec/ms in a single threaded uncontended
append (synchronized, only 1 contributing thread), to 2181.42 rec/ms with 5
threads fighting to put some data. These numbers can only be seen relative
to each other.  (I'm working on a Ubuntu VM, with 4 cores assigned and 6GB
of RAM)

3rd - there's also a concurrency issue with the way the blocks are being
'shipped'. Note those records missing even in the synchronized case? Also,
there're more records received than created in the last scenario. This is
not due to the += operator, but somewhere in the pushBlock mechanism. I
didn't get to the root cause of this yet.

This last point is of high impact as it would mean that all subclasses
of NetworkInputDStream have a data consistency issue.

What's next?  I'm going to create an actor-based version of the
BlockGenerator logic and use the same test infrastructure to compare
performance and consistency.  I will also put the code in github sothat you
can reproduce any findings.  (this is mostly weekend-work, so the next
chapter will be in a week or two from now)

Hope this helps.

-kr, Gerard.
"
