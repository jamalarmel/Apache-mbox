"""Cheng, Hao"" <hao.cheng@intel.com>","Mon, 1 Sep 2014 01:27:43 +0000","RE: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...","chutium <teng.qiu@gmail.com>, ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>","Yes, the root cause for that is the output ObjectInspector in SerDe implementation doesn't reflect the real typeinfo.

Hive actually provides the API like TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(TypeInfo) for the mapping.

You probably need to update the code at https://github.com/ogrodnek/csv-serde/blob/master/src/main/java/com/bizo/hive/serde/csv/CSVSerde.java#L60.

Hi Cheng, thank you very much for helping me to finally find out the secret of this magic...

actually we defined this external table with
    SID STRING
    REQUEST_ID STRING
    TIMES_DQ TIMESTAMP
    TOTAL_PRICE FLOAT
    ...

using ""desc table ext_fullorders"" it is only shown as
[# col_name             data_type               comment             ]
...
[times_dq               string                  from deserializer   ]
[total_price            string                  from deserializer   ]
...
because, as you said, CSVSerde sets all field object inspectors to javaStringObjectInspector and therefore there are comments ""from deserializer""

but in StorageDescriptor, are the real user defined types, using ""desc extended table ext_fullorders"" we can see his sd:StorageDescriptor
is:
FieldSchema(name:times_dq, type:timestamp, comment:null), FieldSchema(name:total_price, type:float, comment:null)

and Spark HiveContext reads the schema info from this StorageDescriptor
https://github.com/apache/spark/blob/7e191fe29bb09a8560cd75d453c4f7f662dff406/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L316

so, in the SchemaRDD, the fields in Row were filled with strings (via fillObject, all of values were retrieved from CSVSerDe with
javaStringObjectInspector)

but Spark considers that some of them are float or timestamp (schema info were got from sd:StorageDescriptor)

crazy...

and sorry for update on the weekend...

a little more about how i fand this problem and why it is a trouble for us.

we use the new spark thrift server, to query normal managed hive table, it works fine

but when we try to access the external tables with custom SerDe such as this CSVSerDe, then we will get this ClassCastException, such as:
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Float

the reason is
https://github.com/apache/spark/blob/d94a44d7caaf3fe7559d9ad7b10872fa16cf81ca/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/server/SparkSQLOperationManager.scala#L104-L105

here Spark's thrift server try to get a float value from SparkRow, because in the schema info (sd:StorageDescriptor) this column is float, but actually in SparkRow, this field was filled with string value...



--
3.nabble.com/HiveContext-schemaRDD-printSchema-get-different-dataTypes-feature-or-a-bug-really-strange-and-surpri-tp8035p8157.html
om.

---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 31 Aug 2014 22:03:37 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),chutium <teng.qiu@gmail.com>,"

Is the behavior you're seeing a regression from 1.0.2, or does 1.0.2 have
this same problem?

Nick
"
"""Ian O'Connell"" <ian@ianoconnell.com>","Sun, 31 Aug 2014 20:27:27 -0700",Re: [Spark SQL] off-heap columnar store,Evan Chan <velvia.github@gmail.com>,"I'm not sure what you mean here? Parquet is at its core just a format, you
could store that data anywhere.

Though it sounds like you saying, correct me if i'm wrong: you basically
want a columnar abstraction layer where you can provide a different backing
implementation to keep the columns rather than parquet-mr?

I.e. you want to be able to produce a schema RDD from something like
vertica, where updates should act as a write through cache back to vertica
itself?

I'm sorry it just sounds like its worth clearly defining what your key
requirement/goal is.



"
chutium <teng.qiu@gmail.com>,"Mon, 1 Sep 2014 06:36:12 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),dev@spark.incubator.apache.org,"i didn't tried with 1.0.2

it takes always too long to build spark assembly jars... more than 20min

[info] Packaging
/mnt/some-nfs/common/spark/assembly/target/scala-2.10/spark-assembly-1.1.0-SNAPSHOT-hadoop1.0.3-mapr-3.0.3.jar
...
[info] Packaging
/mnt/some-nfs/common/spark/examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.3-mapr-3.0.3.jar
...
[info] Done packaging.
[info] Done packaging.
[success] Total time: 1582 s, completed Sep 1, 2014 1:39:21 PM

is there some easily way to exclude some modules such as spark/examples or
spark/external ?



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 1 Sep 2014 11:10:08 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),chutium <teng.qiu@gmail.com>,"If this is not a confirmed regression from 1.0.2, I think it's better to
report it in a separate thread or JIRA.

I believe serious regressions are generally the only reason to block a new
release. Otherwise, if this is an old issue, it should be handled
separately.

2014ÎÖÑ 9Ïõî 1Ïùº ÏõîÏöîÏùº, chutium<teng.qiu@gmail.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:

0-SNAPSHOT-hadoop1.0.3-mapr-3.0.3.jar
0-SNAPSHOT-hadoop1.0.3-mapr-3.0.3.jar
r
ache-Spark-1-1-0-RC3-tp8147p8163.html
"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 1 Sep 2014 21:57:31 +0530",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Nicholas Chammas <nicholas.chammas@gmail.com>,"Easy or quicker way to build spark is

sbt/sbt assembly/assembly

Prashant Sharma




m

w
teng.qiu@gmail.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:
n
0-SNAPSHOT-hadoop1.0.3-mapr-3.0.3.jar
0-SNAPSHOT-hadoop1.0.3-mapr-3.0.3.jar
ache-Spark-1-1-0-RC3-tp8147p8163.html
"
chutium <teng.qiu@gmail.com>,"Mon, 1 Sep 2014 09:43:08 -0700 (PDT)","RE: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...",dev@spark.incubator.apache.org,"thanks a lot, Hao, finally solved this problem, changes of CSVSerDe are here:
https://github.com/chutium/csv-serde/commit/22c667c003e705613c202355a8791978d790591e

btw, ""add jar"" in spark hive or hive-thriftserver always doesn't work, we
build the spark with libraryDependencies += ""csv-serde"" ...

or maybe should try to add it to SPARK_CLASSPATH ?



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 1 Sep 2014 12:40:01 -0700",Re: Jira tickets for starter tasks,dev@spark.apache.org,"A number of folks have emailed me to add them, but I‚Äôve been unable to find their usernanmes in the Apache JIRA. ¬†Note that you need to have an account at issues.apache.org, which may or may not have the same email / username as your accounts on any other Apache systems, including CWiki. ¬†Even if you are an Apache committer, you might not have an account on the JIRA unless you‚Äôve created one.

Therefore, if you want to be added to the ‚ÄúContributors‚Äù group, I‚Äôll need your actual JIRA username, which you can find at¬†https://issues.apache.org/jira/secure/ViewProfile.jspa¬†when signed in to JIRA.

Note that you do not need to be a member of the contributors group in oder to open issues. ¬†If you want to be assigned an issue, you can also just comment in the issue itself and a JIRA administrator should be able to assign it to you.

ote:
Added you; you should be set!

If anyone else wants me to add them, please email me off-list so that we don‚Äôt end up flooding the dev list with replies. Thanks!



Hi Josh,
Can you add me as well?

Thanks,
Ron


ù role group in order to allow you to assign issues to yourself. I‚Äôve added this email address to that group, so you should be set!
te:
ust do

"
Andrew Or <andrew@databricks.com>,"Mon, 1 Sep 2014 18:35:33 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Prashant Sharma <scrapcodes@gmail.com>,"+1. Tested all the basic applications under both deploy modes (where
applicable) in the following environments:

- locally on OSX 10.9
- locally on Windows 8.1
- standalone cluster
- yarn cluster built with Hadoop 2.4

standalone-cluster mode is now fixed"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 1 Sep 2014 23:20:59 -0400","Run the ""Big Data Benchmark"" for new releases",dev <dev@spark.apache.org>,"What do people think of running the Big Data Benchmark
<https://amplab.cs.berkeley.edu/benchmark/> (repo
<https://github.com/amplab/benchmark>) as part of preparing every new
release of Spark?

We'd run it just for Spark and effectively use it as another type of test
to track any performance progress or regressions from release to release.

Would doing such a thing be valuable? Do we already have a way of
benchmarking Spark performance that we use regularly?

Nick
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 1 Sep 2014 21:29:26 -0700","Re: Run the ""Big Data Benchmark"" for new releases","Nicholas Chammas <nicholas.chammas@gmail.com>, dev
 <dev@spark.apache.org>","Hi Nicholas,

At Databricks we already run¬†https://github.com/databricks/spark-perf for each release, which is a more comprehensive performance test suite.

Matei


What do people think of running the Big Data Benchmark  
<https://amplab.cs.berkeley.edu/benchmark/> (repo  
<https://github.com/amplab/benchmark>) as part of preparing every new  
release of Spark?  

We'd run it just for Spark and effectively use it as another type of test  
to track any performance progress or regressions from release to release.  

Would doing such a thing be valuable? Do we already have a way of  
benchmarking Spark performance that we use regularly?  

Nick  
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 2 Sep 2014 01:02:40 -0400","Re: Run the ""Big Data Benchmark"" for new releases",Matei Zaharia <matei.zaharia@gmail.com>,"Oh, that's sweet. So, a related question then.

Did those tests pick up the performance issue reported in SPARK-3333
<https://issues.apache.org/jira/browse/SPARK-3333>? Does it make sense to
add a new test to cover that case?



"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 1 Sep 2014 22:04:30 -0700","Re: Run the ""Big Data Benchmark"" for new releases",Nicholas Chammas <nicholas.chammas@gmail.com>,"Nope, actually, they didn't find that (they found some other things that were fixed, as well as some improvements). Feel free to send a PR, but it would be good to profile the issue first to understand what slowed down. (For example is the map phase taking longer or is it the reduce phase, is there some difference in lengths of specific tasks, etc).

Matei


Oh, that's sweet. So, a related question then.¬†

Did those tests pick up the performance issue reported in SPARK-3333? Does it make sense to add a new test to cover that case?


Hi Nicholas,

At Databricks we already run¬†https://github.com/databricks/spark-perf for each release, which is a more comprehensive performance test suite.

Matei


What do people think of running the Big Data Benchmark
<https://amplab.cs.berkeley.edu/benchmark/> (repo
<https://github.com/amplab/benchmark>) as part of preparing every new
release of Spark?

We'd run it just for Spark and effectively use it as another type of test
to track any performance progress or regressions from release to release.

Would doing such a thing be valuable? Do we already have a way of
benchmarking Spark performance that we use regularly?

Nick

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 1 Sep 2014 22:07:09 -0700","Re: Run the ""Big Data Benchmark"" for new releases",Matei Zaharia <matei.zaharia@gmail.com>,"Yeah, this wasn't detected in our performance tests. We even have a
test in PySpark that I would have though might catch this (it just
schedules a bunch of really small tasks, similar to the regression
case).

https://github.com/databricks/spark-perf/blob/master/pyspark-tests/tests.py#L51

Anyways, Josh is trying to repro the regression to see if we can
figure out what is going on. If we find something for sure we should
add a test.

ote:
were fixed, as well as some improvements). Feel free to send a PR, but it would be good to profile the issue first to understand what slowed down. (For example is the map phase taking longer or is it the reduce phase, is there some difference in lengths of specific tasks, etc).
s it make sense to add a new test to cover that case?
 each release, which is a more comprehensive performance test suite.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 2 Sep 2014 01:56:42 -0400","Re: Run the ""Big Data Benchmark"" for new releases",Patrick Wendell <pwendell@gmail.com>,"Alright, sounds good! I've created databricks/spark-perf/issues/9
<https://github.com/databricks/spark-perf/issues/9> as a reminder for us to
add a new test once we've root caused SPARK-3333.



"
Niranda Perera <niranda@wso2.com>,"Tue, 2 Sep 2014 12:15:49 +0530",Getting the execution times of spark job,dev@spark.apache.org,"Hi,

I have been playing around with spark for a couple of days. I am
using spark-1.0.1-bin-hadoop1 and the Java API. The main idea of the
implementation is to run Hive queries on Spark. I used JavaHiveContext to
achieve this (As per the examples).

I have 2 questions.
1. I am wondering how I could get the execution times of a spark job? Does
Spark provide monitoring facilities in the form of an API?

2. I used a laymen way to get the execution times by enclosing a
JavaHiveContext.hql method with System.nanoTime() as follows

long start, end;
JavaHiveContext hiveCtx;
JavaSchemaRDD hiveResult;

start = System.nanoTime();
hiveResult = hiveCtx.hql(query);
end = System.nanoTime();
System.out.println(start-end);

But the result I got is drastically different from the execution times
recorded in SparkUI. Can you please explain this disparity?

Look forward to hearing from you.

rgds

-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>
"
Zongheng Yang <zongheng.y@gmail.com>,"Tue, 2 Sep 2014 00:09:11 -0700",Re: Getting the execution times of spark job,Niranda Perera <niranda@wso2.com>,"For your second question: hql() (as well as sql()) does not launch a
Spark job immediately; instead, it fires off the Spark SQL
parser/optimizer/planner pipeline first, and a Spark job will be
started after the a physical execution plan is selected. Therefore,
your hand-rolled end-to-end measurement includes the time to go
through the Spark SQL code path, and the times reported inside the UI
are the execution times of the Spark job(s) only.


---------------------------------------------------------------------


"
Yin Huai <huaiyin.thu@gmail.com>,"Tue, 2 Sep 2014 15:59:10 +0800",Re: Spark SQL Query and join different data sources.,chutium <teng.qiu@gmail.com>,"Actually, with HiveContext, you can join hive tables with registered
temporary tables.



"
scwf <wangfei1@huawei.com>,"Tue, 2 Sep 2014 16:39:00 +0800",about spark assembly jar,"""dev@spark.apache.org"" <dev@spark.apache.org>","hi, all
   I suggest spark not use assembly jar as default run-time dependency(spark-submit/spark-class depend on assembly jar),use a library of all 3rd dependency jar like hadoop/hive/hbase more reasonable.

   1 assembly jar packaged all 3rd jars into a big one, so we need rebuild this jar if we want to update the version of some component(such as hadoop)
   2 in our practice with spark, sometimes we meet jar compatibility issue, it is hard to diagnose compatibility issue with assembly jar







---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 2 Sep 2014 09:45:22 +0100",Re: about spark assembly jar,scwf <wangfei1@huawei.com>,"Hm, are you suggesting that the Spark distribution be a bag of 100
JARs? It doesn't quite seem reasonable. It does not remove version
conflicts, just pushes them to run-time, which isn't good. The
assembly is also necessary because that's where shading happens. In
development, you want to run against exactly what will be used in a
real Spark distro.


---------------------------------------------------------------------


"
scwf <wangfei1@huawei.com>,"Tue, 2 Sep 2014 17:01:40 +0800",Re: about spark assembly jar,Sean Owen <sowen@cloudera.com>,"yes, i am not sure what happens when building assembly jar and in my understanding it just package all the dependency jars to a big one.




---------------------------------------------------------------------


"
Ye Xianjin <advancedxy@gmail.com>,"Tue, 2 Sep 2014 17:09:10 +0800",Re: about spark assembly jar,Sean Owen <sowen@cloudera.com>,"Sorry, The quick reply didn't cc the dev list.

Sean, sometimes I have to use the spark-shell to confirm some behavior change. In that case, I have to reassembly the whole project.  is there another way around, not use the the big jar in development? For the original question, I have no comments. 

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)





"
scwf <wangfei1@huawei.com>,"Tue, 2 Sep 2014 17:12:08 +0800",Re: about spark assembly jar,Sean Owen <sowen@cloudera.com>,"Hi sean owen,
here are some problems when i used assembly jar
1 i put spark-assembly-*.jar to the lib directory of my application, it throw compile error

Error:scalac: Error: class scala.reflect.BeanInfo not found.
scala.tools.nsc.MissingRequirementError: class scala.reflect.BeanInfo not found.

	at scala.tools.nsc.symtab.Definitions$definitions$.getModuleOrClass(Definitions.scala:655)

	at scala.tools.nsc.symtab.Definitions$definitions$.getClass(Definitions.scala:608)

	at scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator.<init>(GenJVM.scala:127)

	at scala.tools.nsc.backend.jvm.GenJVM$JvmPhase.run(GenJVM.scala:85)

	at scala.tools.nsc.Global$Run.compileSources(Global.scala:953)

	at scala.tools.nsc.Global$Run.compile(Global.scala:1041)

	at xsbt.CachedCompiler0.run(CompilerInterface.scala:126)

	at xsbt.CachedCompiler0.liftedTree1$1(CompilerInterface.scala:102)

	at xsbt.CachedCompiler0.run(CompilerInterface.scala:102)

	at xsbt.CompilerInterface.run(CompilerInterface.scala:27)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

	at java.lang.reflect.Method.invoke(Method.java:597)

	at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)

	at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)

	at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)

	at org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(IdeaIncrementalCompiler.scala:28)

	at org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:25)

	at org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:58)

	at org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:21)

	at org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

	at java.lang.reflect.Method.invoke(Method.java:597)

	at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)
2 i test my branch which updated hive version to org.apache.hive 0.13.1
   it run successfully when use a bag of 3rd jars as dependency but throw error using assembly jar, it seems assembly jar lead to conflict
   ERROR DDLTask: java.lang.NoSuchFieldError: doubleTypeInfo
         at org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getObjectInspector(ArrayWritableObjectInspector.java:66)
         at org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.<init>(ArrayWritableObjectInspector.java:59)
         at org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.initialize(ParquetHiveSerDe.java:113)
         at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:339)
         at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:283)
         at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:189)
         at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:597)
         at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4194)
         at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:281)
         at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
         at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)







---------------------------------------------------------------------


"
Will Benton <willb@redhat.com>,"Tue, 2 Sep 2014 11:08:33 -0400 (EDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Sean Owen <sowen@cloudera.com>,"Zongheng pointed out in my SPARK-3329 PR (https://github.com/apache/spark/pull/2220) that Aaron had already fixed this issue but that it had gotten inadvertently clobbered by another patch.  I don't know how the project handles this kind of problem, but I've rewritten my SPARK-3329 branch to cherry-pick Aaron's fix (also fixing a merge conflict and handling a test case that it didn't).

The other weird spurious testsuite failures related to orderings I've seen were in ""DESCRIBE FUNCTION EXTENDED"" for functions with lists of synonyms (e.g. STDDEV).  I can't reproduce those now but will take another look later this week.



best,
wb

X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7794811461
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  2 Sep 2014 16:10:45 +0000 (UTC)
Received: (qmail 80191 invoked by uid 500); 2 Sep 2014 16:10:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80124 invoked by uid 500); 2 Sep 2014 16:10:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80113 invoked by uid 99); 2 Sep 2014 16:10:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Sep 2014 16:10:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 02 Sep 2014 16:10:18 +0000
Received: by mail-qg0-f54.google.com with SMTP id q107so6841256qgd.13
        for <dev@spark.apache.org>; Tue, 02 Sep 2014 09:10:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=S+JFtrFLZdPVDFYc/Tb4VVNeLMWjAUKVPVX6zEx2gY4=;
        b=aFGslKsqc33SpDgWWPyYHL0DKt7rMni8FCWslEYF22qZLb2C4nZPZjeXX+yKzqf20I
         yHP2zDJIyjp+rCzoBHAGppo0TxhlKhzeVh6+nH25xhZ7jxkfKDnecJ1BD3+kuYUAl5cM
         4I3megSzUkPx1fccxD92nTB4yJW4yclWZX/EyHUWL14qISOBQ/psmMxUwWfU9Kl/5e5C
         FenxhaqOzqj8YVH0wejskptbdj0tANARi1vBwcBx+k0bsoJpRzTrFoiGhWeFnd6Rhqs0
         GxjWl4aF+rUssdbkpSE1wMObTn4ZHTd4oc74G0S3/GG6qBaFEuh2H5gJ0b9GXDciJOSb
         JKVw==
X-Gm-Message-State: ALoCoQl34cXa3DOoox9szW7W3hG3b6+xCkBoUIVFJ8mYNLi/wemMPQl+1YDD7cXNGwReFgF0SGcM
MIME-Version: 1.0
X-Received: by 10.140.19.100 with SMTP id 91mr8334624qgg.32.1409674216737;
 Tue, 02 Sep 2014 09:10:16 -0700 (PDT)
Received: by 10.140.42.37 with HTTP; Tue, 2 Sep 2014 09:10:16 -0700 (PDT)
In-Reply-To: <540589E8.8020107@huawei.com>
References: <54058224.3030909@huawei.com>
	<CAMAsSd+6q=kJS5=Di8svkJLq7Dw6Q9kxLa81=o2wQ-5HwiYbww@mail.gmail.com>
	<540589E8.8020107@huawei.com>
Date: Tue, 2 Sep 2014 09:10:16 -0700
Message-ID: <CACBYxKK93uDzYs8Y_FgX8FMrvzAmxMDvLGRfja4B6V-YjX+vVA@mail.gmail.com>
Subject: Re: about spark assembly jar
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: scwf <wangfei1@huawei.com>
Cc: Sean Owen <sowen@cloudera.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135637ef02ca70502175aba
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135637ef02ca70502175aba
Content-Type: text/plain; charset=UTF-8

This doesn't help for every dependency, but Spark provides an option to
build the assembly jar without Hadoop and its dependencies.  We make use of
this in CDH packaging.

-Sandy




--001a1135637ef02ca70502175aba--

"
chutium <teng.qiu@gmail.com>,"Tue, 2 Sep 2014 09:32:25 -0700 (PDT)","hive client.getAllPartitions in lookupRelation can take a very long
 time",dev@spark.incubator.apache.org,"in our hive warehouse there are many tables with a lot of partitions, such as
scala> hiveContext.sql(""use db_external"")
scala> val result = hiveContext.sql(""show partitions et_fullorders"").count
result: Long = 5879

i noticed that this part of code:
https://github.com/apache/spark/blob/9d006c97371ddf357e0b821d5c6d1535d9b6fe41/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L55-L56

reads the whole partitions info at the beginning of plan phase, i added a
logInfo around this val partitions = ...

it shows:

scala> val result = hiveContext.sql(""select * from db_external.et_fullorders
limit 5"")
14/09/02 16:15:56 INFO ParseDriver: Parsing command: select * from
db_external.et_fullorders limit 5
14/09/02 16:15:56 INFO ParseDriver: Parse Completed
14/09/02 16:15:56 INFO HiveContext$$anon$1: getAllPartitionsForPruner
started
14/09/02 16:17:35 INFO HiveContext$$anon$1: getAllPartitionsForPruner
finished

it took about 2min to get all partitions...

is there any possible way to avoid this operation? such as only fetch the
requested partition somehow?

Thanks



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 2 Sep 2014 10:35:39 -0700","hey spark developers! intro from shane knapp, devops engineer @ AMPLab",dev <dev@spark.apache.org>,"so, i had a meeting w/the databricks guys on friday and they recommended i
send an email out to the list to say 'hi' and give you guys a quick intro.
 :)

hi!  i'm shane knapp, the new AMPLab devops engineer, and will be spending
time getting the jenkins build infrastructure up to production quality.
 much of this will be 'under the covers' work, like better system level
auth, backups, etc, but some will definitely be user facing:  timely
jenkins updates, debugging broken build infrastructure and some plugin
support.

i've been working in the bay area now since 1997 at many different
companies, and my last 10 years has been split between google and palantir.
 i'm a huge proponent of OSS, and am really happy to be able to help with
the work you guys are doing!

if anyone has any requests/questions/comments, feel free to drop me a line!

shane
"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Sep 2014 10:46:33 -0700","Re: hey spark developers! intro from shane knapp, devops engineer @ AMPLab",shane knapp <sknapp@berkeley.edu>,"Welcome, Shane!


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 2 Sep 2014 13:47:56 -0400","Re: hey spark developers! intro from shane knapp, devops engineer @ AMPLab",shane knapp <sknapp@berkeley.edu>,"Hi Shane!

Thank you for doing the Jenkins upgrade last week. It's nice to know that
infrastructure is gonna get some dedicated TLC going forward.

Welcome aboard!

Nick



"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Sep 2014 10:54:01 -0700",Re: about spark assembly jar,Sandy Ryza <sandy.ryza@cloudera.com>,"Having a SSD help tremendously with assembly time.

Without that, you can do the following in order for Spark to pick up the
compiled classes before assembly at runtime.

export SPARK_PREPEND_CLASSES=true



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 2 Sep 2014 10:59:47 -0700","Re: hey spark developers! intro from shane knapp, devops engineer @ AMPLab",Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Shane,

Thanks for your work so far and I'm really happy to see investment in
this infrastructure. This is a key productivity tool for us and
something we'd love to expand over time to improve the development
process of Spark.

- Patrick


---------------------------------------------------------------------


"
Christopher Nguyen <ctn@adatao.com>,"Tue, 2 Sep 2014 11:09:45 -0700","Re: hey spark developers! intro from shane knapp, devops engineer @ AMPLab",Patrick Wendell <pwendell@gmail.com>,"Welcome, Shane. As a former prof and eng dir at Google, I've been expecting
this to be a first-class engineering college subject. I just didn't expect
it to come through this route :-)

So congrats, and I hope you represent the beginning of a great new trend at
universities.

Sent while mobile. Please excuse typos etc.

"
rapelly kartheek <kartheek.mbms@gmail.com>,"Tue, 2 Sep 2014 20:46:41 +0530",Resource allocation,dev@spark.incubator.apache.org,"Hi,

I want to incorporate some intelligence while choosing the resources for
rdd replication. I thought, if we replicate rdd on specially chosen nodes
based on the capabilities, the next application that requires this rdd can
be executed more efficiently. But, I found that an rdd creatd by an
appplication is owned by only that application and nobody else can access
it.

Can someone tell me what kind of operations can be done on a replicated
rdd. Or to put it other way, what are the benefits of a replicated rdd or
what operations can be performed on a replicated rdd.  I just want to know
how effective is my work going to be.

I'll be happy if some other ideas in the similar line of thought are
suggested.

Thank you!!
Karthik
"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 2 Sep 2014 11:53:04 -0700",Re: about spark assembly jar,Reynold Xin <rxin@databricks.com>,"Yea, SSD + SPARK_PREPEND_CLASSES totally changed my life :)

Maybe we should add a ""developer notes"" page to document all these useful
black magic.



"
Josh Rosen <rosenville@gmail.com>,"Tue, 2 Sep 2014 11:55:42 -0700",Re: about spark assembly jar,"Cheng Lian <lian.cs.zju@gmail.com>, Reynold Xin
 <rxin@databricks.com>","SPARK_PREPEND_CLASSES is documented on the Spark Wiki (which could probably be easier to find):¬†https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools



Yea, SSD + SPARK_PREPEND_CLASSES totally changed my life :)  

Maybe we should add a ""developer notes"" page to document all these useful  
black magic.  


e:  

e  
 
to  
se  
, it  
fo  
13.1  
9)  
  
on  
n  
a  
:  
  
as  
  
----  
--  
"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 2 Sep 2014 11:57:01 -0700",Re: about spark assembly jar,Josh Rosen <rosenville@gmail.com>,"Cool, didn't notice that, thanks Josh!



"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 2 Sep 2014 12:44:15 -0700","Re: hey spark developers! intro from shane knapp, devops engineer @ AMPLab",shane knapp <sknapp@berkeley.edu>,"Welcome Shane =)


- Henry


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 2 Sep 2014 13:07:27 -0700","Re: hey spark developers! intro from shane knapp, devops engineer @ AMPLab",,"Welcome Shane! Glad to see that finally a hero jumping out to tame Jenkins
:)



"
Will Benton <willb@redhat.com>,"Tue, 2 Sep 2014 17:30:18 -0400 (EDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1

Tested Scala/MLlib apps on Fedora 20 (OpenJDK 7) and OS X 10.9 (Oracle JDK 8).


best,
wb


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.a"
Jeffrey Picard <jpicard@placeiq.com>,"Tue, 2 Sep 2014 18:13:55 -0400",Checkpointing Pregel,dev <dev@spark.apache.org>,"Hey guys,

Iím trying to run connected components on graphs that end up running for a fairly large number of iterations (25-30) and take 5-6 hours. I find more than half the time I end up getting fetch failures and losing an executor after a number of iterations. Then it has to go back and recompute pieces that it lost, which donít seem to be getting persisted at the same level as the graph so those iterations take exponentially longer and I have to kill the job because itís not worth waiting for it to finish.

The approach Iím currently trying is checkpointing the vertices and edges (and maybe the messages?) in Pregel. What Iíve been testing with so far is the below patch, which seems to be working (actually I havenít had any failures since I added this change, so I donít know if I did get one if it would recompute from the start or not) but Iím also seeing things like 5 instances of VertexRDDs being persisted all at the same time and ìreduce at VertexRDD.scala:111î runs twice each time. I was wondering if this is the proper / most efficient way of doing this checkpointing, and if not what would work better?

diff --git a/graphx/src/main/scala/org/apache/spark/graphx/Pregel.scala b/graphx/src/main/scala/org/apache/spark/graphx/Pregel.scala
index 5e55620..5be40c3 100644
--- a/graphx/src/main/scala/org/apache/spark/graphx/Pregel.scala
+++ b/graphx/src/main/scala/org/apache/spark/graphx/Pregel.scala
@@ -134,6 +134,11 @@ object Pregel extends Logging {
       g = g.outerJoinVertices(newVerts) { (vid, old, newOpt) => newOpt.getOrElse(old) }
       g.cache()

+      g.vertices.checkpoint()
+      g.vertices.count()
+      g.edges.checkpoint()
+      g.edges.count()
+
       val oldMessages = messages
       // Send new messages. Vertices that didn't get any messages don't appear in newVerts, so don't
       // get to send messages. We must cache messages so it can be materialized on the next line,
@@ -142,6 +147,7 @@ object Pregel extends Logging {
       // The call to count() materializes `messages`, `newVerts`, and the vertices of `g`. This
       // hides oldMessages (depended on by newVerts), newVerts (depended on by messages), and the
       // vertices of prevG (depended on by newVerts, oldMessages, and the vertices of g).
+         messages.checkpoint()
       activeMessages = messages.count()

       logInfo(""Pregel finished iteration "" + i)

Best Regards,

Jeffrey Picard
"
Sanghoon Lee <phoenixlee1@gmail.com>,"Wed, 3 Sep 2014 07:31:06 +0900",Ask something about spark,dev <dev@spark.apache.org>,"Hi, I am phoenixlee and a Spark programmer in Korea.

And be a good chance this time, it tries to teach college students and
office workers to Spark.
This course will be done with the support of the government. Can I use the
data(pictures, samples, etc.) in the spark homepage for this course? Of
course, I will put the comments in thanks and webpage URL. It would be a
good opportunity, even though the findings were that there is no teaching
materials ""Spark"" and education (or community) still in Korea.

Thanks.
·êß
"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Sep 2014 15:32:41 -0700",Re: Ask something about spark,Sanghoon Lee <phoenixlee1@gmail.com>,"I think in general that is fine. It would be great if your slides come with
proper attribution.



e
"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Sep 2014 16:10:21 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Cheng Lian <lian.cs.zju@gmail.com>,"+1



493b223c5f98a593bb6d7372706cc02bebad
t:
"
Kan Zhang <kzhang@apache.org>,"Tue, 2 Sep 2014 17:02:50 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Reynold Xin <rxin@databricks.com>,"+1

Verified PySpark InputFormat/OutputFormat examples.



e
493b223c5f98a593bb6d7372706cc02bebad
es
k
""
o
--
"
shane knapp <sknapp@berkeley.edu>,"Tue, 2 Sep 2014 17:07:48 -0700",quick jenkins restart,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","since our queue is really short, i'm waiting for a couple of builds to
finish and will be restarting jenkins to install/update some plugins.  the
github pull request builder looks like it has some fixes to reduce spammy
github calls, and reduce any potential rate limiting.

i'll let everyone know when it's back up...  this should be super quick
(~15 mins for tests to finish, ~2 mins for jenkins to restart).

thanks in advance!

shane
"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 2 Sep 2014 17:18:30 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),"Reynold Xin <rxin@databricks.com>, kzhang@apache.org","+1

Tested on Mac OS X.

Matei

:

+1  

Verified PySpark InputFormat/OutputFormat examples.  


:  

ote:  
e:  
racle  
 
b2d0493b223c5f98a593bb6d7372706cc02bebad  
nd  
0/  
 
sses  
  
ock  
snappy""  
l"" to  
----  
--  
"
Michael Armbrust <michael@databricks.com>,"Tue, 2 Sep 2014 17:28:42 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1



493b223c5f98a593bb6d7372706cc02bebad
nd
--
"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 2 Sep 2014 17:30:39 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>, Michael
 Armbrust <michael@databricks.com>","+1 ¬†Tested on Mac OSX, Thrift Server, SparkSQL



+1  


  

te:  
te:  
 
  
ote:  
 
k  
b2d0493b223c5f98a593bb6d7372706cc02bebad  
ound  
  
:  
 
 
th  
 
  
 
ill""  
 
----  
"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Wed, 3 Sep 2014 00:30:52 +0000",RE: [VOTE] Release Apache Spark 1.1.0 (RC3),"Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","+1
________________________________________
From: Patrick Wendell [pwendell@gmail.com]
Sent: Saturday, August 30, 2014 4:08 PM
To: dev@spark.apache.org
Subject: [VOTE] Release Apache Spark 1.1.0 (RC3)

Please vote on releasing the following candidate as A"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Tue, 2 Sep 2014 17:34:06 -0700 (PDT)",RE: [VOTE] Release Apache Spark 1.1.0 (RC3),dev@spark.incubator.apache.org,"+1



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 2 Sep 2014 17:37:00 -0700",Re: quick jenkins restart,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","and we're back and building!



"
Paolo Platter <paolo.platter@agilelab.it>,"Wed, 3 Sep 2014 00:37:21 +0000",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),"Jeremy Freeman <freeman.jeremy@gmail.com>,
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1
Tested on HDP 2.1 Sandbox, Thrift Server with Simba Shark ODBC

Paolo

Da: Jeremy Freeman<mailto:freeman.jeremy@gmail.com>
Data invio: ?mercoled?? ?3? ?settembre? ?2014 ?02?:?34
A: dev@spark.incubator.apache.org<mailto:dev@spark.incubator.apache.org>

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 2 Sep 2014 21:55:09 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"In light of the discussion on SPARK-3333, I'll revoke my ""-1"" vote. The
issue does not appear to be serious.



"
scwf <wangfei1@huawei.com>,"Wed, 3 Sep 2014 09:56:51 +0800",Re: about spark assembly jar,Cheng Lian <lian.cs.zju@gmail.com>,"Yea, SSD + SPARK_PREPEND_CLASSES is great for iterative development!

Then why it is ok with a bag of 3rd jars but throw error with assembly jar, any one have idea?




---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 2 Sep 2014 21:55:58 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Nicholas Chammas <nicholas.chammas@gmail.com>,"Thanks everyone for voting on this. There were two minor issues (one a
blocker) were found that warrant cutting a new RC. For those who voted
+1 on this release, I'd encourage you to +1 rc4 when it comes out
unless you have been testing issues specific to the EC2 scripts. This
will move the release process along.

SPARK-3332 - Issue with tagging in EC2 scripts
SPARK-3358 - Issue with regression for m3.XX instances

- Patrick


---------------------------------------------------------------------


"
Evan Chan <velvia.github@gmail.com>,"Tue, 2 Sep 2014 22:25:08 -0700",Re: [Spark SQL] off-heap columnar store,"""Ian O'Connell"" <ian@ianoconnell.com>","
Something like that.

I'd like,

1)  An API to produce a schema RDD from an RDD of columns, not rows.
  However, an RDD[Column] would not make sense, since it would be
spread out across partitions.  Perhaps what is needed is a
Seq[RDD[ColumnSegment]].    The idea is that each RDD would hold the
segments for one column.  The segments represent a range of rows.
This would then read from something like Vertica or Cassandra.

2)  A variant of 1) where you could read this data from Tachyon.
Tachyon is supposed to support a columnar representation of data, it
did for Shark 0.9.x.

The goal is basically to load columnar data from something like
Cassandra into Tachyon, with the compression ratio of columnar
storage, and the speed of InMemoryColumnarTableScan.   If data is
appended into the Tachyon representation, be able to cache it back.
The write back is not as high a priority though.

A workaround would be to read data from Cassandra/Vertica/etc. and
write back into Parquet, but this would take a long time and incur
huge I/O overhead.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 3 Sep 2014 00:02:36 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Nicholas Chammas <nicholas.chammas@gmail.com>,"I'm cancelling this release in favor of RC4. Happy voting!


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 3 Sep 2014 00:24:43 -0700",[VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc4 (commit 2f9b2bd):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=2f9b2bd7844ee8393dc9c319f4fefedf95f5e460

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc4/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1031/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc4-docs/

Please vote on releasing this package as Apache Spark 1.1.0!

The vote is open until Saturday, September 06, at 08:30 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== Regressions fixed since RC3 ==
SPARK-3332 - Issue with tagging in EC2 scripts
SPARK-3358 - Issue with regression for m3.XX instances

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.0.X will not block
this release.

== What default changes should I be aware of? ==
1. The default value of ""spark.io.compression.codec"" is now ""snappy""
--> Old behavior can be restored by switching to ""lzf""

2. PySpark now performs external spilling during aggregations.
--> Old behavior can be restored by setting ""spark.shuffle.spill"" to ""false"".

3. PySpark uses a new heuristic for determining the parallelism of
shuffle operations.
--> Old behavior can be restored by setting
""spark.default.parallelism"" to the number of cores in the cluster.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 3 Sep 2014 00:24:53 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'll kick it off with a +1


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 3 Sep 2014 00:29:31 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1

Tested locally on Mac OS X with local-cluster mode.





"
Michael Armbrust <michael@databricks.com>,"Wed, 3 Sep 2014 00:34:29 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Reynold Xin <rxin@databricks.com>,1
Xiangrui Meng <mengxr@gmail.com>,"Wed, 3 Sep 2014 01:25:53 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Michael Armbrust <michael@databricks.com>,"+1. Tested some MLlib example code.

For default changes, maybe it is useful to mention the default
broadcast factory changed to torrent.


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Wed, 3 Sep 2014 01:54:45 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Xiangrui Meng <mengxr@gmail.com>,"+1 Tested on Yarn and Windows. Also verified that standalone cluster mode
is now fixed.


2014-09-03 1:25 GMT-07:00 Xiangrui Meng <mengxr@gmail.com>:

"
Sean Owen <sowen@cloudera.com>,"Wed, 3 Sep 2014 12:06:48 +0100",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"if anyone else sees it. It doesn't happen on Linux.

[error] Test org.apache.spark.streaming.kafka.JavaKafkaStreamSuite.testKafkaStream
failed: junit.framework.AssertionFailedError: expected:<3> but was:<0>
[error]     at junit.framework.Assert.fail(Assert.java:50)
[error]     at junit.framework.Assert.failNotEquals(Assert.java:287)
[error]     at junit.framework.Assert.assertEquals(Assert.java:67)
[error]     at junit.framework.Assert.assertEquals(Assert.java:199)
[error]     at junit.framework.Assert.assertEquals(Assert.java:205)
[error]     at org.apache.spark.streaming.kafka.JavaKafkaStreamSuite.testKafkaStream(JavaKafkaStreamSuite.java:129)
[error]


---------------------------------------------------------------------


"
Matthew Farrellee <matt@redhat.com>,"Wed, 03 Sep 2014 08:36:45 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"Patrick Wendell <pwendell@gmail.com>,
        ""dev@spark.apache.org"" <dev@spark.apache.org>","+1

built from sha w/ make-distribution.sh
tested basic examples (0 data) w/ local on fedora 20 (openjdk 1.7, 
python 2.7.5)
tested detection and log processing (25GB data) w/ mesos (0.19.0) & nfs 
on rhel 7 (openjdk 1.7, python 2.7.5)



----------------"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 3 Sep 2014 10:23:17 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"

Will these changes be called out in the release notes or somewhere in the
docs?

That last one (which I believe is what we discovered as the result of
SPARK-3333 <https://issues.apache.org/jira/browse/SPARK-3333>) could have a
large impact on PySpark users.

Nick
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 3 Sep 2014 09:36:38 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nick,

Yeah we'll put those in the release notes.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 3 Sep 2014 13:06:13 -0400",spark-ec2 depends on stuff in the Mesos repo,dev <dev@spark.apache.org>,"Spawned by this discussion
<https://github.com/apache/spark/pull/1120#issuecomment-54305831>.

See these 2 lines in spark_ec2.py:

   - spark_ec2 L42
   <https://github.com/apache/spark/blob/6a72a36940311fcb3429bd34c8818bc7d513115c/ec2/spark_ec2.py#L42>
   - spark_ec2 L566
   <https://github.com/apache/spark/blob/6a72a36940311fcb3429bd34c8818bc7d513115c/ec2/spark_ec2.py#L566>

Why does the spark-ec2 script depend on stuff in the Mesos repo? Should
they be moved to the Spark repo?

Nick
‚Äã
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 3 Sep 2014 10:17:14 -0700",Re: spark-ec2 depends on stuff in the Mesos repo,Nicholas Chammas <nicholas.chammas@gmail.com>,"The spark-ec2 repository isn't a part of Mesos. Back in the days, Spark
used to be hosted in the Mesos github organization as well and so we put
scripts that were used by Spark under the same organization.

FWIW I don't think these scripts belong in the Spark repository. They are
helper scripts that setup EC2 clusters with different components like HDFS,
Spark, Tachyon etc. Also one of the motivations for creating this
repository was the ability to change these scripts without requiring a new
Spark release or a new AMI etc.

We can move the repository to a different github organization like AMPLab
if that'll make sense.

Thanks
Shivaram



115c/ec2/spark_ec2.py#L42
115c/ec2/spark_ec2.py#L566
"
Matthew Farrellee <matt@redhat.com>,"Wed, 03 Sep 2014 13:22:17 -0400",Re: spark-ec2 depends on stuff in the Mesos repo,"shivaram@eecs.berkeley.edu, Nicholas Chammas <nicholas.chammas@gmail.com>","that's not a bad idea. it would also break the circular dep in versions 
that results in spark X's ec2 script installing spark X-1 by default.

best,


matt



---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 3 Sep 2014 17:26:54 +0000",Is breeze thread safe in Spark?,"""<dev@spark.apache.org>"" <dev@spark.apache.org>","Hi,

Is breeze library called thread safe from Spark mllib code in case when native libs for blas and lapack are used? Might it be an issue when running Spark locally?

Best regards, Alexander
---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 3 Sep 2014 10:30:06 -0700",Re: spark-ec2 depends on stuff in the Mesos repo,Matthew Farrellee <matt@redhat.com>,"Actually the circular dependency doesn't depend on the spark-ec2 scripts --
The scripts contain download links to many Spark versions and you can
configure which one should be used.

Shivaram



e
ew
b
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 3 Sep 2014 10:33:16 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

- checked checksums of a few packages
- ran few jobs against yarn client/cluster using hadoop2.3 package
- played with spark-shell in yarn-client mode




-- 
Marcelo

---------------------------------------------------------------------"
Matthew Farrellee <matt@redhat.com>,"Wed, 03 Sep 2014 13:40:13 -0400",Re: spark-ec2 depends on stuff in the Mesos repo,shivaram@eecs.berkeley.edu,"oh, i see pwendell is did a patch to the release branch to make the 
release version == --spark-version default

best,


matt



---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Wed, 3 Sep 2014 13:41:21 -0400",Re: Is breeze thread safe in Spark?,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","No, it's not in all cases.   Since Breeze uses lapack under the hood,
changes to memory between different threads is bad.

There's actually a potential bug in the KMeans code where it uses +=
instead of +.





-- 
em rnowling@gmail.com
c 954.496.2314
"
Matthew Farrellee <matt@redhat.com>,"Wed, 03 Sep 2014 14:15:50 -0400",Re: Ask something about spark,"Reynold Xin <rxin@databricks.com>, Sanghoon Lee <phoenixlee1@gmail.com>","reynold,

would you folks be willing to put some creative commons license 
information on the site and its content?

best,


matt



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 3 Sep 2014 11:23:15 -0700",Re: Ask something about spark,Matthew Farrellee <matt@redhat.com>,"I am not sure if I can just go ahead and update the website with a creative
common license.

IIRC, ASF websites are also Apache 2.0 license. Might need somebody from
legal to chime in.



a
ng
"
Josh Rosen <rosenville@gmail.com>,"Wed, 3 Sep 2014 11:25:27 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>, Marcelo
 Vanzin <vanzin@cloudera.com>","+1. ¬†Tested on Windows and EC2. ¬†Confirmed that the EC2 pvm->hvm switch fixed the SPARK-3358 regression.



+1 (non-binding)  

- checked checksums of a few packages  
- ran few jobs against yarn client/cluster using hadoop2.3 package  
- played with sp"
Matthew Farrellee <matt@redhat.com>,"Wed, 03 Sep 2014 14:25:25 -0400",Re: Ask something about spark,Reynold Xin <rxin@databricks.com>,"CC or Apache, it'd be helpful to have it listed in the footer of pages

best,


matt



---------------------------------------------------------------------


"
David Hall <dlwh@cs.berkeley.edu>,"Wed, 3 Sep 2014 11:50:31 -0700",Re: Is breeze thread safe in Spark?,RJ Nowling <rnowling@gmail.com>,"In general, in Breeze we allocate separate work arrays for each call to
lapack, so it should be fine. In general concurrent modification isn't
thread safe of course, but things that ""ought"" to be thread safe really
should be.



"
RJ Nowling <rnowling@gmail.com>,"Wed, 3 Sep 2014 14:58:19 -0400",Re: Is breeze thread safe in Spark?,David Hall <dlwh@cs.berkeley.edu>,"David,

Can you confirm that += is not thread safe but + is?  I'm assuming +
allocates a new object for the write, while += doesn't.

Thanks!
RJ





-- 
em rnowling@gmail.com
c 954.496.2314
"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Wed, 3 Sep 2014 11:58:17 -0700",Re: Is breeze thread safe in Spark?,David Hall <dlwh@cs.berkeley.edu>,"Additionally, at the higher level, MLlib allocates separate Breeze
Vectors/Matrices on a Per-executor basis. The only place I can think of
where data structures might be over-written concurrently is in a
.aggregate() call, and these calls happen sequentially.

RJ - Do you have a JIRA reference for that bug?

Thanks!



"
RJ Nowling <rnowling@gmail.com>,"Wed, 3 Sep 2014 15:02:05 -0400",Re: Is breeze thread safe in Spark?,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Never filed a JIRA -- I actually forgot about it.  Let me file one now.






-- 
em rnowling@gmail.com
c 954.496.2314
"
David Hall <dlwh@cs.berkeley.edu>,"Wed, 3 Sep 2014 12:02:33 -0700",Re: Is breeze thread safe in Spark?,RJ Nowling <rnowling@gmail.com>,"mutating operations are not thread safe. Operations that don't mutate
should be thread safe. I can't speak to what Evan said, but I would guess
that the way they're using += should be safe.



"
RJ Nowling <rnowling@gmail.com>,"Wed, 3 Sep 2014 15:07:18 -0400",Re: Is breeze thread safe in Spark?,David Hall <dlwh@cs.berkeley.edu>,"Here's the JIRA:

https://issues.apache.org/jira/browse/SPARK-3384

Even if the current implementation uses += in a thread safe manner, it can
be easy to make the mistake of accidentally using += in a parallelized
context.  I suggest changing all instances of += to +.

I would encourage others to reproduce and validate this issue, though.





-- 
em rnowling@gmail.com
c 954.496.2314
"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 3 Sep 2014 12:17:50 -0700",Re: Is breeze thread safe in Spark?,RJ Nowling <rnowling@gmail.com>,"RJ, could you provide a code example that can re-produce the bug you
observed in local testing? Breeze's += is not thread-safe. But in a
Spark job, calls to a resultHandler is synchronized:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala#L52
. Let's move our discussion to the JIRA page. -Xiangrui


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 3 Sep 2014 12:23:42 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Josh Rosen <rosenville@gmail.com>,"+1.

Tested locally on OSX 10.9, built with Hadoop 2.4.1

- Checked Datanucleus jar files
- Tested Spark SQL Thrift server and CLI under local mode and standalone
cluster against MySQL backed metastore




"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 3 Sep 2014 19:30:54 +0000",Re: Is breeze thread safe in Spark?,Xiangrui Meng <mengxr@gmail.com>,"What about the allocation of a new breeze vector? Can it happen unsafe within Spark (in several threads)?

Best regards, Alexander

03.09.2014, ◊ 23:17, ""Xiangrui Meng"" <mengxr@gmail.com> Œ¡–…”¡Ã(¡):

e/spark/scheduler/JobWaiter.scala#L52
 can
ss
e:
to
t
ly
e:
,
=
--

---------------------------------------------------------------------


"
Mubarak Seyed <spark.devuser@gmail.com>,"Wed, 3 Sep 2014 13:43:20 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Cheng Lian <lian.cs.zju@gmail.com>,"+1 (non-binding)

Tested locally on Mac OS X with local-cluster mode.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 3 Sep 2014 13:43:49 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Cheng Lian <lian.cs.zju@gmail.com>,"+1

Matei


+1. 

Tested locally on OSX 10.9, built with Hadoop 2.4.1 

- Checked Datanucleus jar files 
- Tested Spark SQL Thrift server and CLI under local mode and standalone 
cluster against MySQL backed metastore 




"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 3 Sep 2014 16:59:08 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Matei Zaharia <matei.zaharia@gmail.com>,"+1 tested thrift server with our in-house application, everything works fine 

-- 
Nan Zhu





"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 3 Sep 2014 13:45:46 -0700",Re: Ask something about spark,"Matthew Farrellee <matt@redhat.com>, Reynold Xin
 <rxin@databricks.com>","I think it has to be Apache actually, it can't be CC.

Matei


CC or Apache, it'd be helpful to have it listed in the footer of pages  

best,  


matt  

rom  
  


---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Wed, 3 Sep 2014 16:02:38 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),dev@spark.incubator.apache.org,"+1



--

---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Wed, 3 Sep 2014 19:13:07 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Jeremy Freeman <freeman.jeremy@gmail.com>,"+1

on OSX Yosemite, built with Hadoop 2.4.1, Hive 0.12 testing SparkSQL,
Thrift, MySQL metastore




"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Thu, 4 Sep 2014 11:13:00 +0800",memory size for caching RDD,"user@spark.apache.org, dev@spark.apache.org","Dear all:

Spark uses memory to cache RDD and the memory size is specified by
""spark.storage.memoryFraction"".

of this part dynamically?

Thanks.

-- 
*Regards,*
*Zhaojie*
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 3 Sep 2014 22:45:24 -0700",Re: memory size for caching RDD,=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Changing this is not supported, it si immutable similar to other spark
configuration settings.

e

---------------------------------------------------------------------


"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Thu, 4 Sep 2014 14:27:15 +0800",Re: memory size for caching RDD,Patrick Wendell <pwendell@gmail.com>,"But is it possible to make t resizable? When we don't have many RDD to
cache, we can give some memory to others.


2014-09-04 13:45 GMT+08:00 Patrick Wendell <pwendell@gmail.com>:




-- 
*Regards,*
*Zhaojie*
"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Thu, 4 Sep 2014 14:31:51 +0800",Re: memory size for caching RDD,raymond.liu@intel.com,"Thanks raymond.

I duplicated the question. Please see the reply here. [?]


2014-09-04 14:27 GMT+08:00 ÁâõÂÖÜÊç∑ <nzjemail@gmail.com>:



-- 
*Regards,*
*Zhaojie*
"
"""Liu, Raymond"" <raymond.liu@intel.com>","Thu, 4 Sep 2014 06:32:55 +0000",RE: memory size for caching RDD,"=?utf-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>, Patrick Wendell
	<pwendell@gmail.com>","You don‚Äôt need to. It is not static allocated to RDD cache, it is just an up limit.
If you don‚Äôt use up the memory by RDD cache, it is always available for other usage. except those one also controlled by some memoryFraction conf. e.g. spark.shuffle.memoryFraction which you also set the up limit.

Best Regards,
Raymond Liu

From: ÁâõÂÖÜÊç∑ [mailto:nzjemail@gmail.com]
Sent: Thursday, September 04, 2014 2:27 PM
To: Patrick Wendell
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: memory size for caching RDD

But is it possible to make t resizable? When we don't have many RDD to cache, we can give some memory to others.

2014-09-04 13:45 GMT+08:00 Patrick Wendell <pwendell@gmail.com<mailto:pwendell@gmail.com>>:
Changing this is not supported, it si immutable similar to other spark
configuration settings.

On Wed, Sep 3, 2014 at 8:13 PM, ÁâõÂÖÜÊç∑ <nzjemail@gmail.com<mailto:nzjemail@gmail.com>> wrote:
> Dear all:
>
> Spark uses memory to cache RDD and the memory size is specified by
> ""spark.storage.memoryFraction"".
>
> One the Executor starts, does Spark support adjusting/resizing memory size
> of this part dynamically?
>
> Thanks.
>
> --
> *Regards,*
> *Zhaojie*



--
Regards,
Zhaojie

"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Thu, 4 Sep 2014 14:57:22 +0800",Re: memory size for caching RDD,"""Liu, Raymond"" <raymond.liu@intel.com>","Oh I see.

I want to implement something like this: sometimes I need to release some
memory for other usage even when they are occupied by some RDDs (can be
recomputed with the help of lineage when they are needed),  does spark
provide interfaces to force it to release some memory ?


2014-09-04 14:32 GMT+08:00 Liu, Raymond <raymond.liu@intel.com>:

s just
le for
.



-- 
*Regards,*
*Zhaojie*
"
"""Liu, Raymond"" <raymond.liu@intel.com>","Thu, 4 Sep 2014 07:05:16 +0000",RE: memory size for caching RDD,=?utf-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"I think there is no public API available to do this. In this case, the best you can do might be unpersist some RDDs manually. The problem is that this is done by RDD unit, not by block unit. And then, if the storage level including disk level, the data on the disk will be removed too.

Best Regards,
Raymond Liu

From: ÁâõÂÖÜÊç∑ [mailto:nzjemail@gmail.com] 
Sent: Thursday, September 04, 2014 2:57 PM
To: Liu, Raymond
Cc: Patrick Wendell; user@spark.apache.org; dev@spark.apache.org
Subject: Re: memory size for caching RDD

Oh I see. 

I want to implement something like this: sometimes I need to release some memory for other usage even when they are occupied by some RDDs (can be recomputed with the help of lineage when they are needed),¬† does spark provide interfaces to force it to release some memory ?

2014-09-04 14:32 GMT+08:00 Liu, Raymond <raymond.liu@intel.com>:
You don‚Äôt need to. It is not static allocated to RDD cache, it is just an up limit.
If you don‚Äôt use up the memory by RDD cache, it is always available for other usage. except those one also controlled by some memoryFraction conf. e.g. spark.shuffle.memoryFraction which you also set the up limit.
¬†
Best Regards,
Raymond Liu
¬†
From: ÁâõÂÖÜÊç∑ [mailto:nzjemail@gmail.com] 
Sent: Thursday, September 04, 2014 2:27 PM
To: Patrick Wendell
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: memory size for caching RDD
¬†
But is it possible to make t resizable? When we don't have many RDD to cache, we can give some memory to others.
¬†
2014-09-04 13:45 GMT+08:00 Patrick Wendell <pwendell@gmail.com>:
Changing this is not supported, it si immutable similar to other spark
configuration settings.

On Wed, Sep 3, 2014 at 8:13 PM, ÁâõÂÖÜÊç∑ <nzjemail@gmail.com> wrote:
> Dear all:
>
> Spark uses memory to cache RDD and the memory size is specified by
> ""spark.storage.memoryFraction"".
>
> One the Executor starts, does Spark support adjusting/resizing memory size
> of this part dynamically?
>
> Thanks.
>
> --
> *Regards,*
> *Zhaojie*



-- 
Regards,
Zhaojie
¬†



-- 
Regards,
Zhaojie

"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Thu, 4 Sep 2014 15:18:08 +0800",Re: memory size for caching RDD,"""Liu, Raymond"" <raymond.liu@intel.com>","ok. So can I use the similar logic as the block manager does when space
fills up ?


2014-09-04 15:05 GMT+08:00 Liu, Raymond <raymond.liu@intel.com>:

t
el
 just an
le for
.


-- 
*Regards,*
*Zhaojie*
"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Thu, 4 Sep 2014 16:05:31 +0530",Dependency hell in Spark applications,dev@spark.apache.org,"I am trying to use Kinesis as source to Spark Streaming and have run into a
dependency issue that can't be resolved without making my own custom Spark
build. The issue is that Spark is transitively dependent
on org.apache.httpcomponents:httpclient:jar:4.1.2 (I think because of
libfb303 coming from hbase and hive-serde) whereas AWS SDK is dependent
on org.apache.httpcomponents:httpclient:jar:4.2. When I package and run
Spark Streaming application, I get the following:

Caused by: java.lang.NoSuchMethodError:
org.apache.http.impl.conn.DefaultClientConnectionOperator.<init>(Lorg/apache/http/conn/scheme/SchemeRegistry;Lorg/apache/http/conn/DnsResolver;)V
        at
org.apache.http.impl.conn.PoolingClientConnectionManager.createConnectionOperator(PoolingClientConnectionManager.java:140)
        at
org.apache.http.impl.conn.PoolingClientConnectionManager.<init>(PoolingClientConnectionManager.java:114)
        at
org.apache.http.impl.conn.PoolingClientConnectionManager.<init>(PoolingClientConnectionManager.java:99)
        at
com.amazonaws.http.ConnectionManagerFactory.createPoolingClientConnManager(ConnectionManagerFactory.java:29)
        at
com.amazonaws.http.HttpClientFactory.createHttpClient(HttpClientFactory.java:97)
        at
com.amazonaws.http.AmazonHttpClient.<init>(AmazonHttpClient.java:181)
        at
com.amazonaws.AmazonWebServiceClient.<init>(AmazonWebServiceClient.java:119)
        at
com.amazonaws.AmazonWebServiceClient.<init>(AmazonWebServiceClient.java:103)
        at
com.amazonaws.services.kinesis.AmazonKinesisClient.<init>(AmazonKinesisClient.java:136)
        at
com.amazonaws.services.kinesis.AmazonKinesisClient.<init>(AmazonKinesisClient.java:117)
        at
com.amazonaws.services.kinesis.AmazonKinesisAsyncClient.<init>(AmazonKinesisAsyncClient.java:132)

I can create a custom Spark build with
org.apache.httpcomponents:httpclient:jar:4.2 included in the assembly but I
was wondering if this is something Spark devs have noticed and are looking
to resolve in near releases. Here are my thoughts on this issue:

Containers that allow running custom user code have to often resolve
dependency issues in case of conflicts between framework's and user code's
dependency. Here is how I have seen some frameworks resolve the issue:
1. Provide a child-first class loader: Some JEE containers provided a
child-first class loader that allowed for loading classes from user code
first. I don't think this approach completely solves the problem as the
framework is then susceptible to class mismatch errors.
2. Fold in all dependencies in a sub-package: This approach involves
folding all dependencies in a project specific sub-package (like
spark.dependencies). This approach is tedious because it involves building
custom version of all dependencies (and their transitive dependencies)
3. Use something like OSGi: Some frameworks has successfully used OSGi to
manage dependencies between the modules. The challenge in this approach is
to OSGify the framework and hide OSGi complexities from end user.

My personal preference is OSGi (or atleast some support for OSGi) but I
would love to hear what Spark devs are thinking in terms of resolving the
problem.

Thanks,
Aniket
"
Sean Owen <sowen@cloudera.com>,"Thu, 4 Sep 2014 11:43:41 +0100",Re: Dependency hell in Spark applications,Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Dumb question -- are you using a Spark build that includes the Kinesis
dependency? that build would have resolved conflicts like this for
you. Your app would need to use the same version of the Kinesis client
SDK, ideally.

All of these ideas are well-known, yes. In cases of super-common
dependencies like Guava, they are already shaded. This is a
less-common source of conflicts so I don't think http-client is
shaded, especially since it is not used directly by Spark. I think
this is a case of your app conflicting with a third-party dependency?

I think OSGi is deemed too over the top for things like this.


---------------------------------------------------------------------


"
Felix Garcia Borrego <fborrego@gilt.com>,"Thu, 4 Sep 2014 14:00:19 +0100",Re: Dependency hell in Spark applications,Sean Owen <sowen@cloudera.com>,"Hi,
I run into the same issue and apart from the ideas Aniket said, I only
could find a nasty workaround. Add my custom PoolingClientConnectionManager
to my classpath.

http://stackoverflow.com/questions/24788949/nosuchmethoderror-while-running-aws-s3-client-on-spark-while-javap-shows-otherwi/25488955#25488955




"
Koert Kuipers <koert@tresata.com>,"Thu, 4 Sep 2014 09:42:43 -0400",Re: Dependency hell in Spark applications,Felix Garcia Borrego <fborrego@gilt.com>,"custom spark builds should not be the answer. at least not if spark ever
wants to have a vibrant community for spark apps.

spark does support a user-classpath-first option, which would deal with
some of these issues, but I don't think it works.

"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 4 Sep 2014 15:53:25 +0000",RE: Is breeze thread safe in Spark?,"Xiangrui Meng <mengxr@gmail.com>, RJ Nowling <rnowling@gmail.com>","I've experienced something related to what we discussed. Na√ØveBayes crashes with native blas/lapack libraries for breeze/netlib on Windows: https://issues.apache.org/jira/browse/SPARK-3403
I've also attached to the issue another example with gradient that crashes in runMiniBatchSGD, probably trying to do grad1 += grad2.
Could you take a close look at this issue? It paralyzed my development for mllib...

Best regards, Alexander

-----Original Message-----
From: Xiangrui Meng [mailto:mengxr@gmail.com] 
Sent: Wednesday, September 03, 2014 11:18 PM
To: RJ Nowling
Cc: David Hall; Ulanov, Alexander; <dev@spark.apache.org>
Subject: Re: Is breeze thread safe in Spark?

RJ, could you provide a code example that can re-produce the bug you observed in local testing? Breeze's += is not thread-safe. But in a Spark job, calls to a resultHandler is synchronized:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala#L52
. Let's move our discussion to the JIRA page. -Xiangrui

On Wed, Sep 3, 2014 at 12:07 PM, RJ Nowling <rnowling@gmail.com> wrote:
> Here's the JIRA:
>
> https://issues.apache.org/jira/browse/SPARK-3384
>
> Even if the current implementation uses += in a thread safe manner, it 
> can be easy to make the mistake of accidentally using += in a 
> parallelized context.  I suggest changing all instances of += to +.
>
> I would encourage others to reproduce and validate this issue, though.
>
>
> On Wed, Sep 3, 2014 at 3:02 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
>> mutating operations are not thread safe. Operations that don't mutate 
>> should be thread safe. I can't speak to what Evan said, but I would 
>> guess that the way they're using += should be safe.
>>
>>
>> On Wed, Sep 3, 2014 at 11:58 AM, RJ Nowling <rnowling@gmail.com> wrote:
>>
>>> David,
>>>
>>> Can you confirm that += is not thread safe but + is?  I'm assuming + 
>>> allocates a new object for the write, while += doesn't.
>>>
>>> Thanks!
>>> RJ
>>>
>>>
>>> On Wed, Sep 3, 2014 at 2:50 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>>>
>>>> In general, in Breeze we allocate separate work arrays for each 
>>>> call to lapack, so it should be fine. In general concurrent 
>>>> modification isn't thread safe of course, but things that ""ought"" 
>>>> to be thread safe really should be.
>>>>
>>>>
>>>> On Wed, Sep 3, 2014 at 10:41 AM, RJ Nowling <rnowling@gmail.com> wrote:
>>>>
>>>>> No, it's not in all cases.   Since Breeze uses lapack under the hood,
>>>>> changes to memory between different threads is bad.
>>>>>
>>>>> There's actually a potential bug in the KMeans code where it uses 
>>>>> += instead of +.
>>>>>
>>>>>
>>>>> On Wed, Sep 3, 2014 at 1:26 PM, Ulanov, Alexander < 
>>>>> alexander.ulanov@hp.com>
>>>>> wrote:
>>>>>
>>>>> > Hi,
>>>>> >
>>>>> > Is breeze library called thread safe from Spark mllib code in 
>>>>> > case
>>>>> when
>>>>> > native libs for blas and lapack are used? Might it be an issue 
>>>>> > when
>>>>> running
>>>>> > Spark locally?
>>>>> >
>>>>> > Best regards, Alexander
>>>>> > ----------------------------------------------------------------
>>>>> > ----- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
>>>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>>>> >
>>>>> >
>>>>>
>>>>>
>>>>> --
>>>>> em rnowling@gmail.com
>>>>> c 954.496.2314
>>>>>
>>>>
>>>>
>>>
>>>
>>> --
>>> em rnowling@gmail.com
>>> c 954.496.2314
>>>
>>
>>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 4 Sep 2014 10:50:43 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"Patrick Wendell <pwendell@gmail.com>,
  ""dev@spark.apache.org"" <dev@spark.apache.org>","+1. Ran spark on yarn on hadoop 0.23 and 2.x.

Tom


 


Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc4 (commit 2f9b2bd):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Thu, 04 Sep 2014 19:50:45 +0200",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),dev@spark.apache.org,"
Just wanted to add, it might be related to this issue or different.
There is a regression when using pyspark to read data
from HDFS. its performance during map tasks has gone down approx 1 ->
0.5x. I have tested the 1.0.2 and the performance was fine, but the 1.1
release candidate has this issue. I tested by setting the following
properties to make sure it was not due to these.

set(""spark.io.compression.codec"",""lzf"").set(""spark.shuffle.spill"",""false"")

in conf object.

Regards,
Gurvinder


---------------------------------------------------------------------


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 4 Sep 2014 11:22:06 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","LICENSE and NOTICE files are good
Hash files are good
Signature files are good
No 3rd parties executables
Source compiled
Run local and standalone tests
Test persist off heap with Tachyon looks good

+1

- Henry


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 12:25:00 -0700",amplab jenkins is down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i am trying to get things up and running, but it looks like either the
firewall gateway or jenkins server itself is down.  i'll update as soon as
i know more.
"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 12:27:57 -0700",Re: amplab jenkins is down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","looks like a power outage in soda hall.  more updates as they happen.



"
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 5 Sep 2014 00:10:35 +0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Henry Saputra <henry.saputra@gmail.com>,"+1

Compiled, ran on yarn-hadoop-2.3 simple job.


2014-09-04 22:22 GMT+04:00 Henry Saputra <henry.saputra@gmail.com>:



-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 13:19:22 -0700",Re: amplab jenkins is down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Mike Patterson <mike@databricks.com>, Matthew L Massie <massie@cs.berkeley.edu>","looks like some hardware failed, and we're swapping in a replacement.  i
don't have more specific information yet -- including *what* failed, as our
sysadmin is super busy ATM.  the root cause was an incorrect circuit being
switched off during building maintenance.

on a side note, this incident will be accelerating our plan to move the
entire jenkins infrastructure in to a managed datacenter environment.  this
will be our major push over the next couple of weeks.  more details about
this, also, as soon as i get them.

i'm very sorry about the downtime, we'll get everything up and running ASAP.



"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 13:27:19 -0700",Re: amplab jenkins is down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Mike Patterson <mike@databricks.com>, Matthew L Massie <massie@cs.berkeley.edu>","it's a faulty power switch on the firewall, which has been swapped out.
 we're about to reboot and be good to go.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 4 Sep 2014 16:27:37 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),Gurvinder Singh <gurvinder.singh@uninett.no>,"

Could you open a JIRA <http://issues.apache.org/jira/> with a brief repro?
We'll look into it.

(You could also provide a repro in a separate thread.)

Nick
"
randomuser54 <talktorohit54@gmail.com>,"Thu, 4 Sep 2014 14:14:36 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),dev@spark.incubator.apache.org,"+1



--

---------------------------------------------------------------------


"
randomuser54 <talktorohit54@gmail.com>,"Thu, 4 Sep 2014 14:17:15 -0700 (PDT)",How to kill a Spark job running in local mode programmatically ?,dev@spark.incubator.apache.org,"I have a java class which calls SparkSubmit.scala with all the arguments to
run a spark job in a thread. I am running them in local mode for now but
also want to run them in yarn-cluster mode later.

Now, I want to kill the running spark job (which can be in local or
yarn-cluster mode) programmatically.

I know that SparkContext has a stop() method but from the thread from which
I am calling the SparkSubmit I don‚Äôt have access to it. Can someone suggest
me how to do this properly ?

Thanks.




--
3.nabble.com/How-to-kill-a-Spark-job-running-in-local-mode-programmatically-tp8279.html
om.

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 14:37:09 -0700",Re: amplab jenkins is down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Mike Patterson <mike@databricks.com>, Matthew L Massie <massie@cs.berkeley.edu>","AND WE'RE UP!

sorry that this took so long...  i'll send out a more detailed explanation
of what happened soon.

now, off to back up jenkins.

shane



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 4 Sep 2014 17:45:01 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"Woohoo! Thanks Shane.

Do you know if queued PR builds will automatically be picked up? Or do we
have to ping the Jenkinmensch manually from each PR?

Nick



"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 14:49:57 -0700",Re: amplab jenkins is down,amp-infra <amp-infra@googlegroups.com>,"i'd ping the Jenkinsmench...  the master was completely offline, so any new
jobs wouldn't have reached it.  any jobs that were queued when power was
lost probably started up, but jobs that were running would fail.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 4 Sep 2014 19:21:14 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"It appears that our main man is having trouble
<https://amplab.cs.berkeley.edu/jenkins/view/Pull%20Request%20Builders/job/SparkPullRequestBuilder/>
 hearing new requests
<https://github.com/apache/spark/pull/2277#issuecomment-54549106>.

Do we need some smelling salts?



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 4 Sep 2014 16:30:05 -0700",Re: amplab jenkins is down,amp-infra@googlegroups.com,"Hm yeah it seems that it hasn't been polling since 3:45.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 16:56:12 -0700",Re: amplab jenkins is down,Nicholas Chammas <nicholas.chammas@gmail.com>,"looking



"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 16:58:52 -0700",Re: amplab jenkins is down,Nicholas Chammas <nicholas.chammas@gmail.com>,"i'm going to restart jenkins and see if that fixes things.



"
Kan Zhang <kzhang@apache.org>,"Thu, 4 Sep 2014 18:08:32 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

Compiled, ran newly-introduced PySpark Hadoop input/output examples.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 4 Sep 2014 22:10:20 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"Looks like during the last build
<https://amplab.cs.berkeley.edu/jenkins/view/Pull%20Request%20Builders/job/SparkPullRequestBuilder/19797/console>
Jenkins was unable to execute a git fetch?



"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Sep 2014 22:02:55 -0700",Re: amplab jenkins is down,Nicholas Chammas <nicholas.chammas@gmail.com>,"yep.  that's exactly the behavior i saw earlier, and will be figuring out
first thing tomorrow morning.  i bet it's an environment issues on the
slaves.



"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Fri, 5 Sep 2014 02:26:59 -0700 (PDT)",[mllib] Add multiplying large scale matrices,dev@spark.incubator.apache.org,"Hi all, 

It seems that there is a method to multiply a RowMatrix and a (local)
Matrix. 
However, there is not a method to multiply a large scale matrix and another
one in Spark.
It would be helpful. Does anyone have a plan to add multiplying large scale
matrices? 
Or shouldn't  we support it in Spark?

thanks,



--

---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Fri, 5 Sep 2014 11:05:18 -0400",Re: [mllib] Add multiplying large scale matrices,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"I think it would be interesting to have a variety of matrix operations
(multiplication, addition / subtraction, powers, scalar multiply, etc.)
available in Spark.

Diagonalization may be more difficult but iterative approximation
approaches may be quite amenable.





-- 
em rnowling@gmail.com
c 954.496.2314
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Fri, 5 Sep 2014 08:18:00 -0700 (PDT)",Re: [mllib] Add multiplying large scale matrices,dev@spark.incubator.apache.org,"Hi RJ,

Thank you for your comment. I am interested in to have other matrix
operations too.
I will create a JIRA issue in the first place.

thanks,



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 5 Sep 2014 11:18:00 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"Hmm, looks like at least some builds
<https://amplab.cs.berkeley.edu/jenkins/view/Pull%20Request%20Builders/job/SparkPullRequestBuilder/19804/consoleFull>
are working now, though this last one was from ~5 hours ago.



"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Fri, 5 Sep 2014 09:12:20 -0700",Re: [mllib] Add multiplying large scale matrices,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"There's some work on this going on in the AMP Lab. Create a ticket and we
can update with our progress so that we don't duplicate effort.



"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Fri, 5 Sep 2014 09:18:23 -0700 (PDT)",Re: [mllib] Add multiplying large scale matrices,dev@spark.incubator.apache.org,"Hi Evan, 

That's sounds interesting. 

Here is the ticket which I created.
https://issues.apache.org/jira/browse/SPARK-3416

thanks,



--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 5 Sep 2014 09:23:03 -0700",Re: [mllib] Add multiplying large scale matrices,"""Evan R. Sparks"" <evan.sparks@gmail.com>","FWIW matrix multiplication is extremely communication intensive when
you have two row partitioned matrices and there are often other ways
to solve problems. Regardless, it would be good to have a more
complete matrix library and it would be good to contribute some of the
stuff we have done in the AMPLab to MLLib.

Shivaram


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 5 Sep 2014 09:23:22 -0700",Re: [mllib] Add multiplying large scale matrices,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Hey There,

I believe this is on the roadmap for the 1.2 next release. But
Xiangrui can comment on this.

- Patrick


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 5 Sep 2014 09:51:55 -0700",Re: How to kill a Spark job running in local mode programmatically ?,randomuser54 <talktorohit54@gmail.com>,"I don't think that's possible at the moment, mainly because
SparkSubmit expects it to be run from the command line, and not
programatically, so it doesn't return anything that can be used to
control what's going on. You may try to interrupt the thread calling
into SparkSubmit, but that might not work - especially if the app
doesn't handle it correctly.

Another thing to consider is that Spark itself doesn't play well with
multiple contexts running in the same JVM, so that would have to be
fixed before having SparkSubmit support that kind of use case.

Have you thought about spawning a child process to run SparkSubmit?
Then you can kill the underlying process if you need to.


e:
to
ch
ne suggest
.n3.nabble.com/How-to-kill-a-Spark-job-running-in-local-mode-programmatically-tp8279.html
.com.



-- 
Marcelo

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 5 Sep 2014 10:23:14 -0700",Re: amplab jenkins is down,Nicholas Chammas <nicholas.chammas@gmail.com>,"it's looking like everything except the pull request builders are working.
 i'm going to be working on getting this resolved today.



"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Fri, 5 Sep 2014 13:29:18 -0400",Re: [mllib] Add multiplying large scale matrices,Patrick Wendell <pwendell@gmail.com>,"Hey all, 

Definitely agreed this would be nice! In our own work we've done element-wise addition, subtraction, and scalar multiplication of similarly partitioned matrices very efficiently with zipping. We've also done matrix-matrix multiplication with zipping, but that only works in certain circumstances, and it's otherwise very communication intensive (as Shivaram says). Another tricky thing with addition / subtraction is how to handle sparse vs. dense arrays.

Would be happy to contribute anything we did, but definitely first worth knowing what progress has been made from the AMPLab.

-- Jeremy

---------------------
jeremy freeman, phd
neuroscientist
@thefreemanlab


http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Add-multiplying-large-scale-matrices-tp8291p8296.html
Nabble.com.

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 5 Sep 2014 16:15:28 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"How's it going?

It looks like during the last build
<https://amplab.cs.berkeley.edu/jenkins/view/Pull%20Request%20Builders/job/SparkPullRequestBuilder/lastBuild/console>
from about 30 min ago Jenkins was still having trouble fetching from
GitHub. It also looks like not all requests for testing are triggering
builds.



"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 5 Sep 2014 15:14:36 -0700",Re: Dependency hell in Spark applications,Koert Kuipers <koert@tresata.com>,"If httpClient dependency is coming from Hive, you could build Spark without
Hive. Alternatively, have you tried excluding httpclient from
spark-streaming dependency in your sbt/maven project?

TD




"
Ted Yu <yuzhihong@gmail.com>,"Fri, 5 Sep 2014 16:14:28 -0700",Re: Dependency hell in Spark applications,Tathagata Das <tathagata.das1565@gmail.com>,"
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @
spark-streaming_2.10 ---
[INFO] org.apache.spark:spark-streaming_2.10:jar:1.1.0-SNAPSHOT
INFO] +- org.apache.spark:spark-core_2.10:jar:1.1.0-SNAPSHOT:compile
[INFO] |  +- org.apache.hadoop:hadoop-client:jar:2.4.0:compile
...
[INFO] |  +- net.java.dev.jets3t:jets3t:jar:0.9.0:compile
[INFO] |  |  +- commons-codec:commons-codec:jar:1.5:compile
[INFO] |  |  +- org.apache.httpcomponents:httpclient:jar:4.1.2:compile
[INFO] |  |  +- org.apache.httpcomponents:httpcore:jar:4.1.2:compile

bq. excluding httpclient from spark-streaming dependency in your sbt/maven
project

This should work.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 5 Sep 2014 20:33:02 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"Looks like Jenkins is back!

lol The poor guy has like a million builds
<https://amplab.cs.berkeley.edu/jenkins/view/Pull%20Request%20Builders/job/SparkPullRequestBuilder/>
to catch up on.



"
shane knapp <sknapp@berkeley.edu>,"Fri, 5 Sep 2014 17:35:41 -0700",Re: amplab jenkins is down,Nicholas Chammas <nicholas.chammas@gmail.com>,"yeah, it was a problem w/the PRB's OAuth key.  josh rosen added a new key,
and magique!

we're about to clear the queue of all builds as most aren't wanted/needed.



"
Josh Rosen <rosenville@gmail.com>,"Fri, 5 Sep 2014 17:49:31 -0700",Re: amplab jenkins is down,"shane knapp <sknapp@berkeley.edu>, Nicholas Chammas
 <nicholas.chammas@gmail.com>","We have successfully purged Jenkins‚Äô build queue. ¬†If you want a PR to be re-tested, please ask Jenkins again.

rote:

yeah, it was a problem w/the PRB's OAuth key. josh rosen added a new key,  
and magique!  

we're about to clear the queue of all builds as most aren't wanted/needed.  


ail.com  

job/SparkPullRequestBuilder/>  
/job/SparkPullRequestBuilder/lastBuild/console>  
  
rs/job/SparkPullRequestBuilder/19804/consoleFull>  
g  
on the  
ders/job/SparkPullRequestBuilder/19797/console>  
u>  
uilders/job/SparkPullRequestBuilder/>  
106>.  
edu>  
,  
 
l.  
  
ed  
n  
  
lan  
  
re  
up  
  
  
l  
 
m  
.  
"
=?UTF-8?B?6aG+6I2j?= <gurongwalker@gmail.com>,"Sat, 6 Sep 2014 11:24:51 +0800",Re: [mllib] Add multiplying large scale matrices,"Patrick Wendell <pwendell@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, 
	Haoyuan Li <haoyuan.li@gmail.com>, =?UTF-8?B?5bC557uq5qOu?= <yinxusen@gmail.com>, 
	Reynold Xin <rxin@databricks.com>","Missed the dev-list last email. Resent it again. Please ignore the
duplicated one.

2014-09-06 11:22 GMT+08:00 È°æËç£ <gurongwalker@gmail.com>:

e
mputing process.
n
multiply
ly
n
ram
iplying-large-scale-matrices-tp8291p8296.html



-- 
------------------
Rong Gu
Department of Computer Science and Technology
State Key Laboratory for Novel Software Technology
Nanjing University
Phone: +86 15850682791
Email: gurongwalker@gmail.com
Homepage: http://pasa-bigdata.nju.edu.cn/people/ronggu/
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sat, 6 Sep 2014 01:17:07 -0700 (PDT)",Re: [mllib] Add multiplying large scale matrices,dev@spark.incubator.apache.org,"Hi  Jeremy, 

Great work!

I'm interested in your work. If there is your code on github, could you let
me know?

-- Yu Ishikawa



--

---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sat, 6 Sep 2014 01:28:06 -0700 (PDT)",Re: [mllib] Add multiplying large scale matrices,dev@spark.incubator.apache.org,"Hi Rong, 

Great job! Thank you for let me know your work.
I will read the source code of saury later.

Although AMPLab is working to implement them, would you like to merge it
into Spark?

Best,

-- Yu Ishikawa




--

---------------------------------------------------------------------


"
"""Aaron Babcock"" <aaron.babcock@gmail.com>","Fri, 6 Sep 2014 11:33:09 +0000",Your Weekly GPGold Offer Is Waiting Inside,"""Adan Perez"" <nadazerep@gmail.com>,
 ""dev help"" <dev-help@spark.incubator.apache.org>,
 ""Brian Maddy"" <brian@brianmaddy.com>, ""Aaron Yancy"" <ayancy@gopro.com>,
 ""Melissa Gilitzer"" <melbell54@hotmail.com>,
 ""Aaron Kardell"" <aaron@mobilerealtyapps.com>,
 ""Eric Caron"" <eric@idea-ignition.com>,
 ""dev"" <dev@spark.incubator.apache.org>",http://maxigas.rrzconsulting.com/ssnfxezj/fpxihvkqhbkjlwgz.upbuevikyiirz
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 6 Sep 2014 11:12:39 -0400",trimming unnecessary test output,dev <dev@spark.apache.org>,"Continuing the discussion started here
<https://github.com/apache/spark/pull/2279>, I‚Äôm wondering if people
already know that certain test output is unnecessary and should be trimmed.

For example
<https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19917/consoleFull>,
I see a bunch of lines like this:

14/09/06 07:54:13 INFO GenerateProjection: Code generated expression
List(IS NOT NULL 1) in 128.33733 ms

Can/should this type of output be suppressed? Is there any other test
output that is obviously more noise than signal?

Nick
‚Äã
"
Erik Erlandson <eje@redhat.com>,"Sat, 6 Sep 2014 11:27:23 -0400 (EDT)","PSA: SI-8835 (Iterator 'drop' method has a complexity bug causing
 quadratic behavior)",dev <dev@spark.apache.org>,"I tripped over this recently while preparing a solution for SPARK-3250 (efficient sampling):

Iterator 'drop' method has a complexity bug causing quadratic behavior
https://issues.scala-lang.org/browse/SI-8835

It's something of a corner case, as the impact is serious only if one is repeatedly invoking 'drop' on Iterator[T], and it doesn't apply to all iterator subclasses (e.g. Array().iterator).   It's actually a bug in 'slice', and so invocations of 'drop', 'take' and 'slice' are potential vulnerabilities.

Not something I'd expect to show up frequently, but it seemed worth mentioning, as RDD partitions are ubiquitously presented as Iterator[T] in the compute model, and if it does happen it turns a linear algorithm into something having quadratic behavior.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 6 Sep 2014 12:14:57 -0400",Scala's Jenkins setup looks neat,dev <dev@spark.apache.org>,"After reading Erik's email, I found this Scala PR
<https://github.com/scala/scala/pull/3963> and immediately noticed a few
cool things:

   - Jenkins is hooked directly into GitHub somehow, so you get the ""All is
   well"" message in the merge status window, presumably based on the last test
   status
   - Jenkins is also tagging the PR based on its test status or need for
   review
   - Jenkins is also tagging the PR for a specific milestone

Do any of these things make sense to add to our setup? Or perhaps something
inspired by these features?

Nick
"
Reynold Xin <rxin@databricks.com>,"Sat, 6 Sep 2014 10:10:56 -0700",Re: Scala's Jenkins setup looks neat,Nicholas Chammas <nicholas.chammas@gmail.com>,"that would require github hooks permission and unfortunately asf infra
wouldn't allow that.

Maybe they will change their mind one day, but so far we asked about this
and the answer has been no for security reasons.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 6 Sep 2014 13:29:53 -0400",Re: Scala's Jenkins setup looks neat,Reynold Xin <rxin@databricks.com>,"Aww, that's a bummer...



"
Josh Rosen <rosenville@gmail.com>,"Sat, 6 Sep 2014 12:48:25 -0700",Re: amplab jenkins is down,"shane knapp <sknapp@berkeley.edu>, Nicholas Chammas
 <nicholas.chammas@gmail.com>","It looks like Jenkins is up and running, but there seems to be a delay in responding to requests to re-test patches. ¬†It seems like Jenkins is promptly testing new PRs, or new commits as they‚Äôre added to existing PRs, but taking a very¬†long time to respond to requests to re-test PRs.

I‚Äôm going to continue monitoring this today. ¬†I‚Äôm considering creating my own fork of the Jenkins pull request builder plugin so that we can add extra logging in order to diagnose what‚Äôs causing this lag.

- Josh
rote:

We have successfully purged Jenkins‚Äô build queue. ¬†If you want a PR to be re-tested, please ask Jenkins again.

rote:

yeah, it was a problem w/the PRB's OAuth key. josh rosen added a new key,
and magique!

we're about to clear the queue of all builds as most aren't wanted/needed.


ail.com

job/SparkPullRequestBuilder/>
/job/SparkPullRequestBuilder/lastBuild/console>

rs/job/SparkPullRequestBuilder/19804/consoleFull>
g
on the
ders/job/SparkPullRequestBuilder/19797/console>
u>
uilders/job/SparkPullRequestBuilder/>
106>.
edu>
,
l.

ed
n

lan

re
up


l
m
.
"
Tathagata Das <tathagata.das1565@gmail.com>,"Sat, 6 Sep 2014 14:35:43 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

Tested streaming integration with flume on a local test bed.



"
Sean Owen <sowen@cloudera.com>,"Sat, 6 Sep 2014 22:50:05 +0100",Re: trimming unnecessary test output,Nicholas Chammas <nicholas.chammas@gmail.com>,"This is just a line logging that one test succeeded right? I don't find
that noise. Recently I wanted to search test run logs for a test case
success and it was important that the individual test case was logged.

ple
d.
consoleFull
"
Reza Zadeh <reza@databricks.com>,"Sat, 6 Sep 2014 17:12:47 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1
Tested recently merged mllib matrix multiplication bugfix
<https://github.com/apache/spark/pull/2224>



"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 7 Sep 2014 15:39:42 -0400",jenkins failed all tests?,dev@spark.apache.org,"Hi, all 

I just modified some document, 

but still failed to pass tests?

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19950/consoleFull

Anyone can look at the problem?

Best, 

-- 
Nan Zhu

"
Michael Armbrust <michael@databricks.com>,"Sun, 7 Sep 2014 13:29:50 -0700",Re: trimming unnecessary test output,Sean Owen <sowen@cloudera.com>,"Feel free to submit a PR to add a log4j.properies file to
sql/catalyst/src/test/resources similar to what we do in core/hive.



eople
consoleFull
"
Sean Owen <sowen@cloudera.com>,"Sun, 7 Sep 2014 21:30:43 +0100",Re: jenkins failed all tests?,Nan Zhu <zhunanmcgill@gmail.com>,"It would help to point to your change. Are you sure it was only docs
and are you sure you're rebased, submitting against the right branch?
Jenkins is saying you are changing public APIs; it's not reporting
test failures. But it could well be a test/Jenkins problem.


---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 7 Sep 2014 16:52:50 -0400",Re: jenkins failed all tests?,Sean Owen <sowen@cloudera.com>,"Hi, Sean, 

Thanks for the reply

Here are the updated files:

https://github.com/apache/spark/pull/2312/files 

just two md files...

Best, 

-- 
Nan Zhu





"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 7 Sep 2014 16:54:16 -0400",Re: jenkins failed all tests?,Sean Owen <sowen@cloudera.com>,"It seems that I‚Äôm not the only one   

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/

Best,  

--  
Nan Zhu




19950/consoleFull

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 7 Sep 2014 18:26:53 -0400",Re: jenkins failed all tests?,Nan Zhu <zhunanmcgill@gmail.com>,"Yeah, it feels like Jenkins has become a lot more flaky recently. Or maybe
it‚Äôs just our tests.

Here are some more examples:

   - https://github.com/apache/spark/pull/2310#issuecomment-54741169
   - https://github.com/apache/spark/pull/2313#issuecomment-54752766

Nick
‚Äã


consoleFull
"
cjermaine <cmj4@rice.edu>,"Sun, 7 Sep 2014 14:25:34 -0700 (PDT)","Re: A Comparison of Platforms for Implementing and Running Very
 Large Scale Machine Learning Algorithms",dev@spark.incubator.apache.org,"
I‚Äôm out of the authors of this paper, and I just came across this thread.
I‚Äôm glad that Ignacio Zendejas noticed our paper!

First off, let me post link to the published version of the paper, which is
likely slightly different than the version linked above:

http://cmj4.web.rice.edu/performance.pdf
<http://cmj4.web.rice.edu/performance.pdf>  

Next, I just want to quickly address a couple of comments made here.

rxin says:

 

It‚Äôs a bit misleading to say that we just tried a ‚Äúcouple‚Äù of algorithms;
the paper describes five different algorithms, along with multiple
implementations on each; we tried a LOT of variants of each algorithm, as we
detail in the paper.

Also, it is true that we did use our own implementations; the point was to
compare each platform as a programming and execution platform. The paper is
clear that the benchmark was directed towards ‚Äúa user who wants to run a
specific ML inference algorithm over a large data set, but cannot find an
existing implementation and thus must 'roll her own' ML code.‚Äù We
specifically state that we are not interested in comparing canned libraries,
which is a very different task. Both ease-of-use and speed were considered
as being equally important. If you read the paper, you‚Äôll see that we
generally gave PySpark high marks as a programming platform

Several of the posts here imply that all Spark experiments were using
PySpark. This is not true. Matei Zaharia says:


And Ignacio Zendajas says:


In reality, the paper also describes pure Java implementations that ran on
Spark, with no Python, for two models: a Gaussian mixture model and LDA. 

For the GMM, Java ran in 40-50% of the time compared to Python (though to be
fair, a lot of that is due to the fact that GMM inference is
linear-algebra-intensive; it‚Äôs not easy to do linear algebra in the JVM for
reasons I‚Äôll not get into here‚Ä¶ it‚Äôs possible someone else could do a lot
faster).

Matai Zaharia also says:

s 
. 

It‚Äôs certainly possible our implementations were sub-optimal. All I can say
is that again, the goal of the paper was to chronicle our experiences using
each platform as both a programming and execution platform. While doubtless
Spark‚Äôs developers could have done a better job of writing code for Spark, I
hope we didn‚Äôt do too badly! And again, evaluating ease-of-programming was
at least half of our goal.

That said, I doubt that sending out the model should be much of a bottleneck
as Matai Zaharia implies‚Äîat least in the cases we tested.  And even if it
was a bottleneck, one could argue that it probably shouldn't be. In the very
worst case (LDA) the model is 100 components X 10^4 dictionary size X 8
bytes per FP number, or 8 MB in all. Not too large. The smallest model is
the GMM @10 dimension. In this case, the model has 10 components X (10 X 10
covariance matrix + 10 dim mean vector) X 8 bytes, or roughly 10KB.  Tiny
even!

Matai Zaharia also says:

 

I couldn‚Äôt agree more. But again, the idea of our benchmark was specifically
to consider the case of an expert who is facing just such an implementation
challenge: he/she needs a model for which a canned implementation does not
exist. 

Finally, let me say that we‚Äôd absolutely LOVE it if someone who is an active
Spark developer would take the time to implement one or more of these
algorithms and replicate our experiments (I‚Äôd be happy to help anyone out
who wants to do this‚Äîsend me a message). It‚Äôs already been a year since we
did all of this, and for that reason alone the results might be quite
different.



--
3.nabble.com/A-Comparison-of-Platforms-for-Implementing-and-Running-Very-Large-Scale-Machine-Learning-Algorithms-tp7823p8326.html
om.

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sun, 7 Sep 2014 16:07:58 -0700",Re: amplab jenkins is down,"shane knapp <sknapp@berkeley.edu>, Nicholas Chammas
 <nicholas.chammas@gmail.com>","Does anyone know why some of the MiMa tests have started failing?

See¬†https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19948/consoleFull¬†for an example.


It looks like Jenkins is up and running, but there seems to be a delay in responding to requests to re-test patches. ¬†It seems like Jenkins is promptly testing new PRs, or new commits as they‚Äôre added to existing PRs, but taking a very¬†long time to respond to requests to re-test PRs.

I‚Äôm going to continue monitoring this today. ¬†I‚Äôm considering creating my own fork of the Jenkins pull request builder plugin so that we can add extra logging in order to diagnose what‚Äôs causing this lag.

- Josh
rote:

We have successfully purged Jenkins‚Äô build queue. ¬†If you want a PR to be re-tested, please ask Jenkins again.

rote:

yeah, it was a problem w/the PRB's OAuth key. josh rosen added a new key,
and magique!

we're about to clear the queue of all builds as most aren't wanted/needed.


ail.com

job/SparkPullRequestBuilder/>
/job/SparkPullRequestBuilder/lastBuild/console>

rs/job/SparkPullRequestBuilder/19804/consoleFull>
g
on the
ders/job/SparkPullRequestBuilder/19797/console>
u>
uilders/job/SparkPullRequestBuilder/>
106>.
edu>
,
l.

ed
n

lan

re
up


l
m
.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 7 Sep 2014 20:28:51 -0400",Re: Unit tests in < 5 minutes,Reynold Xin <rxin@databricks.com>,"

SPARK-3431 <https://issues.apache.org/jira/browse/SPARK-3431>: Parallelize
execution of tests
Fix logging of unit test execution time

Nick
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 7 Sep 2014 17:50:56 -0700",[RESULT] [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote passes with 8 binding +1 votes and no -1 votes. I'll post
the final release in the next 48 hours... just finishing the release
notes and packaging (which now takes a long time given the number of
contributors!).

+1:
Reynold Xin*
Michael Armbrust*
Xiangrui Meng*
Andrew Or*
Sean Owen
Matthew Farrellee
Marcelo Vanzin
Josh Rosen*
Cheng Lian
Mubarak Seyed
Matei Zaharia*
Nan Zhu
Jeremy Freeman
Denny Lee
Tom Graves*
Henry Saputra
Egor Pahomov
Rohit Sinha
Kan Zhang
Tathagata Das*
Reza Zadeh

-1:

0:

* = binding

---------------------------------------------------------------------


"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 8 Sep 2014 10:38:25 +0530",Re: amplab jenkins is down,Josh Rosen <rosenville@gmail.com>,"Looks like this is already taken care of ?

Prashant Sharma




consoleFull for
ting PRs,
ring creating my
PR to be
.
/SparkPullRequestBuilder/
/SparkPullRequestBuilder/lastBuild/console
/SparkPullRequestBuilder/19804/consoleFull
g
/SparkPullRequestBuilder/19797/console
/SparkPullRequestBuilder/
6
u
,
l.
ed
n
re
up
l
m
"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 8 Sep 2014 00:31:08 -0700",Re: [mllib] Add multiplying large scale matrices,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sorry for my late reply! I'm also very interested in the
implementation of distributed matrix multiplication. As Shivaram
mentioned, the communication is the concern here. But maybe we can
start with a reasonable implementation and then iterate on its
performance. It would be great if eventually we can implement an
algorithm close to the 2.5D algorithm
(http://www.netlib.org/lapack/lawnspdf/lawn248.pdf).

I created two JIRAs for this topic:

1. Distributed block matrix: https://issues.apache.org/jira/browse/SPARK-3434
2. Distributed matrix multiplication:
https://issues.apache.org/jira/browse/SPARK-3435

We can move our discussion there.

Rong, I'm really happy to see the Saury project. It would be great if
you can share your design and experience (maybe on the JIRA page so it
is easier to track). I will read the reports on CSDN and ping you if I
ran into problems. Thanks!

Best,
Xiangrui


---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Mon, 8 Sep 2014 00:41:44 -0700 (PDT)",Re: [mllib] Add multiplying large scale matrices,dev@spark.incubator.apache.org,"Hi Xiangrui Meng,

Thank you for your comment and creating tickets.

The ticket which I created would be moved to your tickets.
I will close my ticket, and then will link it to yours later.

Best,
Yu Ishikawa



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 8 Sep 2014 01:13:10 -0700",Re: amplab jenkins is down,Prashant Sharma <scrapcodes@gmail.com>,"Yeah, I think https://github.com/apache/spark/pull/2315 should have fixed
the Mima issue.  We're still seeing some intermittent failures due to
DriverSuite and SparkSubmitSuite tests failing, so I'd appreciate any help
in diagnosing that issue.


/consoleFull for
n
sting PRs,
ering creating my
 PR to be
,
d.
b/SparkPullRequestBuilder/
b/SparkPullRequestBuilder/lastBuild/console
g
b/SparkPullRequestBuilder/19804/consoleFull
ng
b/SparkPullRequestBuilder/19797/console
b/SparkPullRequestBuilder/
e,
d
en
a
r
s
s
ll
om
"
shane knapp <sknapp@berkeley.edu>,"Mon, 8 Sep 2014 08:41:43 -0700",Re: amplab jenkins is down,Josh Rosen <rosenville@gmail.com>,"i'll take a look at this today.



p
8/consoleFull for
isting PRs,
dering creating
dd
a PR to
y,
ob/SparkPullRequestBuilder/
ob/SparkPullRequestBuilder/lastBuild/console
ng
ob/SparkPullRequestBuilder/19804/consoleFull
s
ob/SparkPullRequestBuilder/19797/console
u
ob/SparkPullRequestBuilder/
n
ed
 a
er
g
as
ks
e
.
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 9 Sep 2014 07:32:35 -0700",Re: Lost executor on YARN ALS iterations,Xiangrui Meng <mengxr@gmail.com>,"Hi Sandy,

Any resolution for YARN failures ? It's a blocker for running spark on top
of YARN.

Thanks.
Deb


"
=?UTF-8?B?6aG+6I2j?= <gurongwalker@gmail.com>,"Wed, 10 Sep 2014 00:05:22 +0800",Re: [mllib] Add multiplying large scale matrices,Xiangrui Meng <mengxr@gmail.com>,"Hi All,

Sorry for my late reply!

Yu Ishikawa,Thanks for your interests in Saury project. You are welcomed to
try that out. If you have questions about that, please email me. We are
keeping improving performance/adding features for the project.

Xiangrui, thanks for your encouragement. If you have any problems with my
CSDN reports, please feel free to contact me. We had some design for Saury
on our lab's private JIRA which is in Chinese. I will translate into
English then share it to you these days. Acutally, I also have surveyed the
related algorithms/systems before we started the Saury project. The survey
is attached in this email, not on CSDN report. We also had considered the
2.5D algorithm for reducing communication. However, at that time, MLlib did
not have a distributed block matrix representation. So, we decided to
firstly implement the distributed matrix multiplication on the
IndexRowMatrix as time is limited for the Summer Code project. Also, as far
as we know, nobody had tried that at that time. Actually, adopting 2.5D
algorithm to reduce network communication is on our roadmap. We are also
planning to do that in the next days.

Best,
Rong


2014-09-08 15:31 GMT+08:00 Xiangrui Meng <mengxr@gmail.com>:



-- 
------------------
Rong Gu
Department of Computer Science and Technology
State Key Laboratory for Novel Software Technology
Nanjing University
Phone: +86 15850682791
Email: gurongwalker@gmail.com
Homepage: http://pasa-bigdata.nju.edu.cn/people/ronggu/

---------------------------------------------------------------------"
Patrick Wendell <pwendell@gmail.com>,"Tue, 9 Sep 2014 09:16:21 -0700",RFC: Deprecating YARN-alpha API's,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Everyone,

This is a call to the community for comments on SPARK-3445 [1]. In a
nutshell, we are trying to figure out timelines for deprecation of the
YARN-alpha API's as Yahoo is now moving off of them. It's helpful for
us to have a sense of whether anyone else uses these.

Please comment on the JIRA if you have feeback, thanks!

[1] https://issues.apache.org/jira/browse/SPARK-3445

- Patrick

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Tue, 9 Sep 2014 12:17:37 -0500",parquet predicate / projection pushdown into unionAll,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've been looking at performance differences between spark sql queries
against single parquet tables, vs a unionAll of two tables.  It's a
significant difference, like 5 to 10x

Is there a reason in general not to push projections and predicates down
into the individual ParquetTableScans in a union?

Here's an example of what I'm talking about:


scala> p.printSchema
root
 |-- name: string (nullable = true)
 |-- age: integer (nullable = false)
 |-- phones: array (nullable = true)
 |    |-- element: string (containsNull = true)


scala> p2.printSchema
root
 |-- name: string (nullable = true)
 |-- age: integer (nullable = false)
 |-- phones: array (nullable = true)
 |    |-- element: string (containsNull = true)


scala> val b = p.unionAll(p2)


// single table, pushdown
scala> p.where('age < 40).select('name)
res36: org.apache.spark.sql.SchemaRDD =
SchemaRDD[97] at RDD at SchemaRDD.scala:103
== Query Plan ==
== Physical Plan ==
Project [name#3]
 ParquetTableScan [name#3,age#4], (ParquetRelation /var/tmp/people,
Some(Configuration: core-default.xml, core-site.xml, mapred-default.xml,
mapred-site.xml), org.apache.spark.sql.SQLContext@6d7e79f6, []), [(age#4 <
40)]


// union of 2 tables, no pushdown
scala> b.where('age < 40).select('name)
res37: org.apache.spark.sql.SchemaRDD =
SchemaRDD[99] at RDD at SchemaRDD.scala:103
== Query Plan ==
== Physical Plan ==
Project [name#3]
 Filter (age#4 < 40)
  Union [ParquetTableScan [name#3,age#4,phones#5], (ParquetRelation
/var/tmp/people, Some(Configuration: core-default.xml, core-site.xml,
mapred-default.xml, mapred-site.xml),
org.apache.spark.sql.SQLContext@6d7e79f6, []), []
,ParquetTableScan [name#0,age#1,phones#2], (ParquetRelation
/var/tmp/people2, Some(Configuration: core-default.xml, core-site.xml,
mapred-default.xml, mapred-site.xml),
org.apache.spark.sql.SQLContext@6d7e79f6, []), []
]
   ParquetTableScan [name#3,age#4,phones#5], (ParquetRelation
/var/tmp/people, Some(Configuration: core-default.xml, core-site.xml,
mapred-default.xml, mapred-site.xml), org.apache.spark.sql...
"
Michael Armbrust <michael@databricks.com>,"Tue, 9 Sep 2014 10:46:48 -0700",Re: parquet predicate / projection pushdown into unionAll,Cody Koeninger <cody@koeninger.org>,"
This would be a great case to add to ColumnPruning.  Would be awesome if
you could open a JIRA or even a PR :)
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 9 Sep 2014 10:49:15 -0700",Re: Lost executor on YARN ALS iterations,Debasish Das <debasish.das83@gmail.com>,"Hi Deb,

The current state of the art is to increase
spark.yarn.executor.memoryOverhead until the job stops failing.  We do have
plans to try to automatically scale this based on the amount of memory
requested, but it will still just be a heuristic.

-Sandy


"
Sean Owen <sowen@cloudera.com>,"Tue, 9 Sep 2014 18:59:52 +0100",Re: RFC: Deprecating YARN-alpha API's,Patrick Wendell <pwendell@gmail.com>,"FWIW consensus from Cloudera folk seems to be that there's no need or
demand on this end for YARN alpha. It wouldn't have an impact if it
were removed sooner even.

It will be a small positive to reduce complexity by removing this
support, making it a little easier to develop for current YARN APIs.


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Tue, 9 Sep 2014 13:07:12 -0500",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"Opened

https://issues.apache.org/jira/browse/SPARK-3462

I'll take a look at ColumnPruning and see what I can do


"
Michael Armbrust <michael@databricks.com>,"Tue, 9 Sep 2014 11:08:48 -0700",Re: parquet predicate / projection pushdown into unionAll,Cody Koeninger <cody@koeninger.org>,"Thanks!


"
Gary Malouf <malouf.gary@gmail.com>,"Tue, 9 Sep 2014 14:26:09 -0400",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"I'm kind of surprised this was not run into before.  Do people not
segregate their data by day/week in the HDFS directory structure?



"
Michael Armbrust <michael@databricks.com>,"Tue, 9 Sep 2014 11:29:07 -0700",Re: parquet predicate / projection pushdown into unionAll,Gary Malouf <malouf.gary@gmail.com>,"I think usually people add these directories as multiple partitions of the
same table instead of union.  This actually allows us to efficiently prune
directories when reading in addition to standard column pruning.


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 9 Sep 2014 11:58:24 -0700",Re: Lost executor on YARN ALS iterations,Sandy Ryza <sandy.ryza@cloudera.com>,"Hmm...I did try it increase to few gb but did not get a successful run
yet...

Any idea if I am using say 40 executors, each running 16GB, what's the
typical spark.yarn.executor.memoryOverhead for say 100M x 10 M large
matrices with say few billion ratings...


"
Cody Koeninger <cody@koeninger.org>,"Tue, 9 Sep 2014 14:01:11 -0500",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"Maybe I'm missing something, I thought parquet was generally a write-once
format and the sqlContext interface to it seems that way as well.

d1.saveAsParquetFile(""/foo/d1"")

// another day, another table, with same schema
d2.saveAsParquetFile(""/foo/d2"")

Will give a directory structure like

/foo/d1/_metadata
/foo/d1/part-r-1.parquet
/foo/d1/part-r-2.parquet
/foo/d1/_SUCCESS

/foo/d2/_metadata
/foo/d2/part-r-1.parquet
/foo/d2/part-r-2.parquet
/foo/d2/_SUCCESS

// ParquetFileReader will fail, because /foo/d1 is a directory, not a
parquet partition
sqlContext.parquetFile(""/foo"")

// works, but has the noted lack of pushdown
sqlContext.parquetFile(""/foo/d1"").unionAll(sqlContext.parquetFile(""/foo/d2""))


Is there another alternative?




"
Patrick Wendell <pwendell@gmail.com>,"Tue, 9 Sep 2014 12:09:19 -0700",Re: parquet predicate / projection pushdown into unionAll,Cody Koeninger <cody@koeninger.org>,"I think what Michael means is people often use this to read existing
partitioned Parquet tables that are defined in a Hive metastore rather
than data generated directly from within Spark and then reading it
back as a table. I'd expect the latter case to become more common, but
for now most users connect to an existing metastore.

I think you could go this route by creating a partitioned external
table based on the on-disk layout you create. The downside is that
you'd have to go through a hive metastore whereas what you are doing
now doesn't need hive at all.

We should also just fix the case you are mentioning where a union is
used directly from within spark. But that's the context.

- Patrick


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 9 Sep 2014 12:15:35 -0700",Re: Lost executor on YARN ALS iterations,Debasish Das <debasish.das83@gmail.com>,"I would expect 2 GB would be enough or more than enough for 16 GB executors
(unless ALS is using a bunch of off-heap memory?).  You mentioned earlier
in this thread that the property wasn't showing up in the Environment tab.
 Are you sure it's making it in?

-Sandy


"
Michael Armbrust <michael@databricks.com>,"Tue, 9 Sep 2014 13:02:08 -0700",Re: parquet predicate / projection pushdown into unionAll,Patrick Wendell <pwendell@gmail.com>,"What Patrick said is correct.  Two other points:
 - In the 1.2 release we are hoping to beef up the support for working with
partitioned parquet independent of the metastore.
 - You can actually do operations like INSERT INTO for parquet tables to
add data.  This creates new parquet files for each insertion.  This will
break if there are multiple concurrent writers to the same table.


"
shane knapp <sknapp@berkeley.edu>,"Tue, 9 Sep 2014 14:00:30 -0700","yet another jenkins restart early thursday morning -- 730am PDT (and
 a brief update on our new jenkins infra)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","since the power incident last thursday, the github pull request builder
plugin is still not really working 100%.  i found an open issue
w/jenkins[1] that could definitely be affecting us, i will be pausing
builds early thursday morning and then restarting jenkins.
i'll send out a reminder tomorrow, and if this causes any problems for you,
please let me know and we can work out a better time.

but, now for some good news!  yesterday morning, we racked and stacked the
systems for the new jenkins instance in the berkeley datacenter.  tomorrow
i should be able to log in to them and start getting them set up and
configured.  this is a major step in getting us in to a much more
'production' style environment!

anyways:  thanks for your patience, and i think we've all learned that hard
powering down your build system is a definite recipe for disaster.  :)

shane

[1] -- https://issues.jenkins-ci.org/browse/JENKINS-22509
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 9 Sep 2014 14:04:34 -0700",Re: Lost executor on YARN ALS iterations,Sandy Ryza <sandy.ryza@cloudera.com>,"Last time it did not show up on environment tab but I will give it another
shot...Expected behavior is that this env variable will show up right ?


"
Chester Chen <chester@alpinenow.com>,"Tue, 9 Sep 2014 14:39:26 -0700",Re: RFC: Deprecating YARN-alpha API's,Sean Owen <sowen@cloudera.com>,"We were using it until recently, we are talking to our customers and see if
we can get off it.

Chester
Alpine Data Labs




"
Cody Koeninger <cody@koeninger.org>,"Tue, 9 Sep 2014 18:09:48 -0500",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"Ok, so looking at the optimizer code for the first time and trying the
simplest rule that could possibly work,

object UnionPushdown extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    // Push down filter into
union
    case f @ Filter(condition, u @ Union(left, right)) =>

      u.copy(left = f.copy(child = left), right = f.copy(child =
right))


    // Push down projection into
union
    case p @ Project(projectList, u @ Union(left, right)) =>
      u.copy(left = p.copy(child = left), right = p.copy(child =
right))

}

}


If I try manually applying that rule to a logical plan in the repl, it
produces the query shape I'd expect, and executing that plan results in
parquet pushdowns as I'd expect.

But adding those cases to ColumnPruning results in a runtime exception
(below)

I can keep digging, but it seems like I'm missing some obvious initial
context around naming of attributes.  If you can provide any pointers to
speed me on my way I'd appreciate it.


java.lang.AssertionError: assertion failed: ArrayBuffer() + ArrayBuffer()
!= WrappedArray(name#6, age#7), List(name#9, age#10, phones#11)
        at scala.Predef$.assert(Predef.scala:179)
        at
org.apache.spark.sql.parquet.ParquetTableScan.<init>(ParquetTableOperations.scala:75)
        at
org.apache.spark.sql.execution.SparkStrategies$ParquetOperations$$anonfun$9.apply(SparkStrategies.scala:234)
        at
org.apache.spark.sql.execution.SparkStrategies$ParquetOperations$$anonfun$9.apply(SparkStrategies.scala:234)
        at
org.apache.spark.sql.SQLContext$SparkPlanner.pruneFilterProject(SQLContext.scala:367)
        at
org.apache.spark.sql.execution.SparkStrategies$ParquetOperations$.apply(SparkStrategies.scala:230)
        at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
        at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at
org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
        at
org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)
        at
org.apache.spark.sql.execution.SparkStrategies$BasicOperators$$anonfun$12.apply(SparkStrategies.scala:282)
        at
org.apache.spark.sql.execution.SparkStrategies$BasicOperators$$anonfun$12.apply(SparkStrategies.scala:282)
        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at
scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at
org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:282)
        at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
        at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at
org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
        at
org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:402)
        at
org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:400)
        at
org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:406)
        at
org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:406)
        at
org.apache.spark.sql.SQLContext$QueryExecution.toString(SQLContext.scala:431)





"
Sudershan Malpani <sudershan.malpani@gmail.com>,"Tue, 9 Sep 2014 17:14:01 -0700",Junit spark tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I am calling an object which in turn is calling a method inside a map RDD in spark. While writing the tests how can I mock that object's call? Currently I did doNothing().when(class).method() is called but it is giving task not serializable exception. I tried making the class both spy and mock. 

Sudershan Malpani
Sent from my iPhone
---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 9 Sep 2014 17:16:43 -0700",Re: Junit spark tests,Sudershan Malpani <sudershan.malpani@gmail.com>,"Can you be a little bit more specific, maybe give a code snippet?



"
Sudershan Malpani <sudershan.malpani@gmail.com>,"Tue, 9 Sep 2014 17:38:57 -0700",Re: Junit spark tests,Reynold Xin <rxin@databricks.com>,"Class1.java

@Autowired
Private ClassX cx;

Public list method1(JavaPairRDD data){
     List list1 = new ArrayList();
     List list2 = new ArrayList();
     JavaPairRDD computed = data.map(
            new Function<Tuple2<object, list>>() {
                Public List call(object obj) throws exception {
              cx.method2(list2);
           Return list1;
}});
    Return computed.flatMap(new FlatMapfunc() { do something }}).collect();
  }
}

Class1Test.java

@Mock
Private ClassX cx;

@InjectMocks
Private Class1 c1;

@Before
Public void init() {
  Super.init();
   Sc = getSparkContext();
   doNothing().when(cx).method2(Collections.<object>emptyList());
}

@Test
Public void testMethod1() {
   JavaRDD alldata = Sc.paraalelize("""");
   JavaPairRDD data = createrdd(alldata);
   c1.method1(rdd);
}
   


Sudershan Malpani
Sent from my iPhone

 in spark. While writing the tests how can I mock that object's call? Currently I did doNothing().when(class).method() is called but it is giving task not serializable exception. I tried making the class both spy and mock.
"
Cody Koeninger <cody@koeninger.org>,"Wed, 10 Sep 2014 08:19:05 -0500",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"So the obvious thing I was missing is that the analyzer has already
resolved attributes by the time the optimizer runs, so the references in
the filter / projection need to be fixed up to match the children.

Created a PR, let me know if there's a better way to do it.  I'll see about
testing performance against some actual data sets.


"
Cody Koeninger <cody@koeninger.org>,"Wed, 10 Sep 2014 11:31:37 -0500",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"Tested the patch against a cluster with some real data.  Initial results
seem like going from one table to a union of 2 tables is now closer to a
doubling of query time as expected, instead of 5 to 10x.

Let me know if you see any issues with that PR.


"
Michael Armbrust <michael@databricks.com>,"Wed, 10 Sep 2014 09:39:14 -0700",Re: parquet predicate / projection pushdown into unionAll,Cody Koeninger <cody@koeninger.org>,"Hey Cody,

Thanks for doing this!  Will look at your PR later today.

Michael


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 10 Sep 2014 09:58:35 -0700",Re: Lost executor on YARN ALS iterations,Debasish Das <debasish.das83@gmail.com>,"That's right


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 10 Sep 2014 17:46:39 -0400","Re: yet another jenkins restart early thursday morning -- 730am PDT
 (and a brief update on our new jenkins infra)",shane knapp <sknapp@berkeley.edu>,"I'm looking forward to this. :)

Looks like Jenkins is having trouble triggering builds for new commits or
after user requests (e.g.
<https://github.com/apache/spark/pull/2339#issuecomment-55165937>).
Hopefully that will be resolved tomorrow.

Nick


"
shane knapp <sknapp@berkeley.edu>,"Wed, 10 Sep 2014 15:44:24 -0700","Re: yet another jenkins restart early thursday morning -- 730am PDT
 (and a brief update on our new jenkins infra)",Nicholas Chammas <nicholas.chammas@gmail.com>,"that's kinda what we're hoping as well.  :)


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 10 Sep 2014 23:23:21 -0700",Re: [RESULT] [VOTE] Release Apache Spark 1.1.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey just a heads up to everyone - running a bit behind on getting the
final artifacts and notes up. Finalizing this release was much more
complicated than previous ones due to new binary formats (we need to
redesign the download page a bit for this to work) and the large
increase in contributor count. Next time we can pipeline this work to
avoid a delay.

I did cut the v1.1.0 tag today. We should be able to do the full
announce tomorrow.

Thanks,
Patrick


---------------------------------------------------------------------


"
Jun Feng Liu <liujunf@cn.ibm.com>,"Thu, 11 Sep 2014 17:14:45 +0800",Spark authenticate enablement,dev@spark.apache.org,"Hi, there

I am trying to enable the authentication on spark on standealone model. 
Seems like only SparkSubmit load the properties from spark-defaults.conf. 
org.apache.spark.deploy.master.Master dose not really load the default 
setting from spark-defaults.conf. 

Dose it mean the spark authentication only work for like YARN model? Or I 
missed something with standalone model.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 "
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Thu, 11 Sep 2014 15:39:12 +0530",Re: Dependency hell in Spark applications,Ted Yu <yuzhihong@gmail.com>,"Thanks everyone for weighing in on this.

I had backported kinesis module from master to spark 1.0.2 so just to
confirm if I am not missing anything, I did a dependency graph compare of
my spark build with spark-master
and org.apache.httpcomponents:httpclient:jar does seem to resolve to 4.1.2
dependency.

I need Hive so, I can't really do a build without it. Even if I
exclude httpclient
dependency from my project's build, it will not solve the problem because
AWS SDK has been compiled with a greater version of http client. My spark
stream project does not uses http client directly. AWS SDK will look for
 class org.apache.http.impl.conn.DefaultClientConnectionOperator and it
will be loaded from spark-assembly jar regardless of how I package my
project (unless I am missing something?). I enabled verbosed classloading
to confirm that the class is indeed loading from spark-assembly jar.

spark.files.userClassPathFirst option doesn't seem to be working on my
spark 1.0.2 build (not sure why).

I was only left custom building spark and forcingly introduce latest
httpclient's latest version as dependency.

Finally, I tested this on 1.1.0-RC4 today and it has the same issue. Has
anyone ever been able to get the Kinesis example work with spark-hadoop2.4
(with hive and yarn) build? I feel like this is a bug that exists even in
1.1.0.

I still believe we need a better solution to address the dependency hell
problem. If OSGi is deemed too over the top, what are the solutions being
investigated?


"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 11 Sep 2014 10:42:00 -0400","Re: Some Serious Issue with Spark Streaming ? Blocks Getting
 Removed and Jobs have Failed..",Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Hi,   

Can you attach more logs to see if there is some entry from ContextCleaner?

I met very similar issue before‚Ä¶but haven‚Äôt get resolved  

Best,  

--  
Nan Zhu




stand what is happening.  
m/dibbhatt/kafka-spark-consumer) which basically a low level Kafka Consumer implemented custom Receivers for every Kafka topic partitions and pulling data in parallel. Individual streams from all topic partitions are then merged to create Union stream which used for further processing.
n I tested this with huge amount of backlog messages from Kafka ( 50 million + messages), I see couple of major issue in Spark Streaming. Wanted to get some opinion on this....
 in Amazon EMR , 3 m1.xlarge Node Spark cluster running in Standalone Mode.
er with a huge backlog in Kafka ( around 50 Million), Spark is completely busy performing the Receiving task and hardly schedule any processing task. Can you let me if this is expected ? If there is large backlog, Spark will take long time pulling them . Why Spark not doing any processing ? Is it because of resource limitation ( say all cores are busy puling ) or it is by design ? I am setting the executor-memory to 10G and driver-memory to 4G .
e with this email. What I can see very frequently Block are selected to be Removed...This kind of entries are all over the place. But when a Block is removed , below problem happen.... May be this issue cause the issue 1 that no Jobs are getting processed ..
ropping
-1410443074600 from memory
4600 of size 12651900 dropped from memory (free 21220667)
10443074600 on ip-10-252-5-113.asskickery.us:53752 (http://ip-10-252-5-113.asskickery.us:53752) in memory (size: 12.1 MB, free: 100.6 MB)
10443074600 on ip-10-252-5-62.asskickery.us:37033 (http://ip-10-252-5-62.asskickery.us:37033) in memory (size: 12.1 MB, free: 154.6 MB)
ge 7.0 (TID 118, ip-10-252-5-62.asskickery.us (http://ip-10-252-5-62.asskickery.us)): java.lang.Exception: Could not compute split, block input-0-1410443074600 not found
tage 7.0 (TID 126) on executor ip-10-252-5-62.asskickery.us (http://ip-10-252-5-62.asskickery.us): java.lang.Exception (Could not compute split, block input-0-1410443074600 not found) [duplicate 1]
 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 139, ip-10-252-5-62.asskickery.us (http://ip-10-252-5-62.asskickery.us)): java.lang.Exception: Could not compute split, block input-0-1410443074600 not found


1)
62)
ala:177)
cutor.java:1145)
Executor.java:615)
r-unsubscribe@spark.apache.org)
:user-help@spark.apache.org)


"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 11 Sep 2014 10:43:32 -0400","Re: Some Serious Issue with Spark Streaming ? Blocks Getting
 Removed and Jobs have Failed..",Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"This is my case about broadcast variable:  

14/07/21 19:49:13 INFO Executor: Running task ID 4 14/07/21 19:49:13 INFO DAGScheduler: Completed ResultTask(0, 2) 14/07/21 19:49:13 INFO TaskSetManager: Finished TID 2 in 95 ms on localhost (progress: 3/106) 14/07/21 19:49:13 INFO TableOutputFormat: Created table instance for hdfstest_customers 14/07/21 19:49:13 INFO Executor: Serialized size of result for 3 is 596 14/07/21 19:49:13 INFO Executor: Sending result for 3 directly to driver 14/07/21 19:49:13 INFO BlockManager: Found block broadcast_0 locally 14/07/21 19:49:13 INFO Executor: Finished task ID 3 14/07/21 19:49:13 INFO TaskSetManager: Starting task 0.0:5 as TID 5 on executor localhost: localhost (PROCESS_LOCAL) 14/07/21 19:49:13 INFO TaskSetManager: Serialized task 0.0:5 as 11885 bytes in 0 ms 14/07/21 19:49:13 INFO Executor: Running task ID 5 14/07/21 19:49:13 INFO BlockManager: Removing broadcast 0 14/07/21 19:49:13 INFO DAGScheduler: Completed ResultTask(0, 3) 14/07/21 19:49:13 INFO ContextCleaner: Cleaned broadcast 0 14/07/21 19:49:13 INFO TaskSetManager: Finished TID 3 in 97 ms on localhost (progress: 4/106) 14/07/21 19:49:13 INFO BlockManager: Found block broadcast_0 locally 14/07/21 19:49:13 INFO BlockManager: Removing block broadcast_0 14/07/21 19:49:13 INFO MemoryStore: Block broadcast_0 of size 202564 dropped from memory (free 886623436) 14/07/21 19:49:13 INFO ContextCleaner: Cleaned shuffle 0 14/07/21 19:49:13 INFO ShuffleBlockManager: Deleted all files for shuffle 0 14/07/21 19:49:13 INFO HadoopRDD: Input split: hdfs://172.31.34.184:9000/etltest/hdfsData/customer.csv:25+5 14/07/21 19:49:13 INFO HadoopRDD: Input split: hdfs://172.31.34.184:9000/etltest/hdfsData/customer.csv:20+5 14/07/21 19:49:13 INFO TableOutputFormat: Created table instance for hdfstest_customers 14/07/21 19:49:13 INFO Executor: Serialized size of result for 4 is 596 14/07/21 19:49:13 INFO Executor: Sending result for 4 directly to driver 14/07/21 19:49:13 INFO Executor: Finished task ID 4 14/07/21 19:49:13 INFO TaskSetManager: Starting task 0.0:6 as TID 6 on executor localhost: localhost (PROCESS_LOCAL) 14/07/21 19:49:13 INFO TaskSetManager: Serialized task 0.0:6 as 11885 bytes in 0 ms 14/07/21 19:49:13 INFO Executor: Running task ID 6 14/07/21 19:49:13 INFO DAGScheduler: Completed ResultTask(0, 4) 14/07/21 19:49:13 INFO TaskSetManager: Finished TID 4 in 80 ms on localhost (progress: 5/106) 14/07/21 19:49:13 INFO TableOutputFormat: Created table instance for hdfstest_customers 14/07/21 19:49:13 INFO Executor: Serialized size of result for 5 is 596 14/07/21 19:49:13 INFO Executor: Sending result for 5 directly to driver 14/07/21 19:49:13 INFO Executor: Finished task ID 5 14/07/21 19:49:13 INFO TaskSetManager: Starting task 0.0:7 as TID 7 on executor localhost: localhost (PROCESS_LOCAL) 14/07/21 19:49:13 INFO TaskSetManager: Serialized task 0.0:7 as 11885 bytes in 0 ms 14/07/21 19:49:13 INFO Executor: Running task ID 7 14/07/21 19:49:13 INFO DAGScheduler: Completed ResultTask(0, 5) 14/07/21 19:49:13 INFO TaskSetManager: Finished TID 5 in 77 ms on localhost (progress: 6/106) 14/07/21 19:49:13 INFO HttpBroadcast: Started reading broadcast variable 0 14/07/21 19:49:13 INFO HttpBroadcast: Started reading broadcast variable 0 14/07/21 19:49:13 ERROR Executor: Exception in task ID 6 java.io.FileNotFoundException: http://172.31.34.174:52070/broadcast_0 at sun.net.www.protocol.http.HttpURLConnection.getInputStream (http://www.protocol.http.HttpURLConnection.getInputStream)(HttpURLConnection.java:1624) at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:196) at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:89) at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63) at org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61) at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141) at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)



--  
Nan Zhu



ner?
d  
e:
erstand what is happening.  
com/dibbhatt/kafka-spark-consumer) which basically a low level Kafka Consumer implemented custom Receivers for every Kafka topic partitions and pulling data in parallel. Individual streams from all topic partitions are then merged to create Union stream which used for further processing.
hen I tested this with huge amount of backlog messages from Kafka ( 50 million + messages), I see couple of major issue in Spark Streaming. Wanted to get some opinion on this....
ng in Amazon EMR , 3 m1.xlarge Node Spark cluster running in Standalone Mode.
umer with a huge backlog in Kafka ( around 50 Million), Spark is completely busy performing the Receiving task and hardly schedule any processing task. Can you let me if this is expected ? If there is large backlog, Spark will take long time pulling them . Why Spark not doing any processing ? Is it because of resource limitation ( say all cores are busy puling ) or it is by design ? I am setting the executor-memory to 10G and driver-memory to 4G .
ace with this email. What I can see very frequently Block are selected to be Removed...This kind of entries are all over the place. But when a Block is removed , below problem happen.... May be this issue cause the issue 1 that no Jobs are getting processed ..
 dropping
-0-1410443074600 from memory
074600 of size 12651900 dropped from memory (free 21220667)
1410443074600 on ip-10-252-5-113.asskickery.us:53752 (http://ip-10-252-5-113.asskickery.us:53752) in memory (size: 12.1 MB, free: 100.6 MB)
1410443074600 on ip-10-252-5-62.asskickery.us:37033 (http://ip-10-252-5-62.asskickery.us:37033) in memory (size: 12.1 MB, free: 154.6 MB)
tage 7.0 (TID 118, ip-10-252-5-62.asskickery.us (http://ip-10-252-5-62.asskickery.us)): java.lang.Exception: Could not compute split, block input-0-1410443074600 not found
 stage 7.0 (TID 126) on executor ip-10-252-5-62.asskickery.us (http://ip-10-252-5-62.asskickery.us): java.lang.Exception (Could not compute split, block input-0-1410443074600 not found) [duplicate 1]
sk 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 139, ip-10-252-5-62.asskickery.us (http://ip-10-252-5-62.asskickery.us)): java.lang.Exception: Could not compute split, block input-0-1410443074600 not found
2)
2)
:61)
a:62)
scala:177)
xecutor.java:1145)
olExecutor.java:615)

ser-unsubscribe@spark.apache.org)
to:user-help@spark.apache.org)

"
shane knapp <sknapp@berkeley.edu>,"Thu, 11 Sep 2014 07:38:19 -0700","Re: yet another jenkins restart early thursday morning -- 730am PDT
 (and a brief update on our new jenkins infra)",Nicholas Chammas <nicholas.chammas@gmail.com>,"jenkins is now in quiet mode, and a restart is happening soon.


"
shane knapp <sknapp@berkeley.edu>,"Thu, 11 Sep 2014 08:29:42 -0700","Re: yet another jenkins restart early thursday morning -- 730am PDT
 (and a brief update on our new jenkins infra)",Nicholas Chammas <nicholas.chammas@gmail.com>,"...and the restart is done.


"
Matthew Farrellee <matt@redhat.com>,"Thu, 11 Sep 2014 12:15:11 -0400","Re: yet another jenkins restart early thursday morning -- 730am PDT
 (and a brief update on our new jenkins infra)",shane knapp <sknapp@berkeley.edu>,"shane,

is there anything we should do for pull requests that failed, but for 
unrelated issues?

best,


matt



---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 11 Sep 2014 09:18:25 -0700","Re: yet another jenkins restart early thursday morning -- 730am PDT
 (and a brief update on our new jenkins infra)",Matthew Farrellee <matt@redhat.com>,"you can just click on 'rebuild', if you'd like.  what project specifically?
 (i had forgotten that i'd killed
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/557/,
which i just started a rebuild on)


"
Matthew Farrellee <matt@redhat.com>,"Thu, 11 Sep 2014 14:41:05 -0400","Re: yet another jenkins restart early thursday morning -- 730am PDT
 (and a brief update on our new jenkins infra)",shane knapp <sknapp@berkeley.edu>,"it was part of the review queue, but it looks like the runs have been 
gc'd. oh well!

best,


matt



---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Thu, 11 Sep 2014 11:42:59 -0700","Re: yet another jenkins restart early thursday morning -- 730am
 PDT (and a brief update on our new jenkins infra)","Matthew Farrellee <matt@redhat.com>, shane knapp
 <sknapp@berkeley.edu>","I purposely disabled the Maven master tests while we‚Äôre debugging why they‚Äôre always failing the Hive tests. ¬†We‚Äôll post an update later when we learn more; in the meantime, please don‚Äôt trigger those tests, since we don‚Äôt want them to clobber some of the work that we‚Äôre doing by hand in those workspaces.

Thanks,
Josh


it was part of the review queue, but it looks like the runs have been  
gc'd. oh well!  

best,  


matt  

/557/,  
  
5165937  
 


---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Tom <thubregtsen@gmail.com>,"Thu, 11 Sep 2014 14:17:59 -0700 (PDT)",Questions regarding memory usage,dev@spark.incubator.apache.org,"Hi All, I currently have 3 questions regarding memory usage:

1)
Regarding overall memory usage:
If I set SPARK_DRIVER_MEMORY to x GB, Spark reports
/14/09/11 15:36:41 INFO MemoryStore: MemoryStore started with capacity
~0.55*x GB/
*Question:*
Does this relate to spark.storage.memoryFraction (default 0.6), and is the
other 0.4 used by spark.shulffle.memoryFraction (default 0.2) and spark
general usage (0,2?).

2)
Regarding RDD‚Äôs and intermediate storage:
Say I have two different programs:
a)
JavaPairRDD<String, String> a = file.flatMapToPair(some class());
JavaPairRDD<String, String> b = a.reduceByKey(some class());
JavaPairRDD<String, String> c = b.mapValues(some class());

b)
file.flatMapToPair(some class()).reduceByKey(some class()).mapValues(some
class());

I am now wondering which RDD's are actually created, and if they are the
same in both situations:
I could see a scenario in a) in which lazy evaluation has a similar
situation too
Int a, b, c;
a = 0;
b = a;
c = b;
Were the compiler removes a and b, and only stores c. 

I could also see a scenario in b) in which Spark has a need to create
intermediate RDD‚Äôs to pass the data to the next function, without them being
explicitly mentioned, but still being there. In this case, there are RDD's
created, and potentially removed.

Now when I look into the output, I see
/MappedRDD[37]/
But I only defined 18 RDD's in my code with JavaPairRDD.

*Question:*
When are RDD's actually created? Can I trace these one-on-one in the output? 

3)
Regarding the size of an RDD:
When I run a program, I see the following lines:
/14/09/11 15:36:44 INFO MemoryStore: ensureFreeSpace(3760) called with
curMem=360852, maxMem=2899102924
14/09/11 15:36:44 INFO MemoryStore: Block broadcast_9 stored as values in
memory (estimated size 3.7 KB, free 2.7 GB)/
But also
y
map of 493 MB to disk (7 times so far)
map of 493 MB to disk (8 times so far)/

I could see a scenario in which a shuffle uses more than an actual RDD store
needs, but this seems disproportional to me. 
*Question:*
Where can I see the actual size of an individual RDD? Or is there a way to
calculate it?

Thanks a lot for any help!!

Tom








--
3.nabble.com/Questions-regarding-memory-usage-tp8376.html
om.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 11 Sep 2014 17:12:38 -0700",Announcing Spark 1.1.0!,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","I am happy to announce the availability of Spark 1.1.0! Spark 1.1.0 is
the second release on the API-compatible 1.X line. It is Spark's
largest release ever, with contributions from 171 developers!

This release brings operational and performance improvements in Spark
core including a new implementation of the Spark shuffle designed for
very large scale workloads. Spark 1.1 adds significant extensions to
the newest Spark modules, MLlib and Spark SQL. Spark SQL introduces a
JDBC server, byte code generation for fast expression evaluation, a
public types API, JSON support, and other features and optimizations.
MLlib introduces a new statistics library along with several new
algorithms and optimizations. Spark 1.1 also builds out Spark's Python
support and adds new components to the Spark Streaming module.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

[1] http://spark.eu.apache.org/releases/spark-release-1-1-0.html
[2] http://spark.eu.apache.org/downloads.html

NOTE: SOME ASF DOWNLOAD MIRRORS WILL NOT CONTAIN THE RELEASE FOR SEVERAL HOURS.

Please e-mail me directly for any type-o's in the release notes or name listing.

Thanks, and congratulations!
- Patrick

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 11 Sep 2014 20:26:34 -0400",Re: Announcing Spark 1.1.0!,Patrick Wendell <pwendell@gmail.com>,"Nice work everybody! I'm looking forward to trying out this release!


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 11 Sep 2014 18:02:31 -0700",Reporting serialized task size after task broadcast change?,"""dev@spark.apache.org"" <dev@spark.apache.org>","After the change to broadcast all task data, is there any easy way to
discover the serialized size of the data getting sent down for a task?

thanks,
-Sandy
"
Reynold Xin <rxin@databricks.com>,"Thu, 11 Sep 2014 18:26:24 -0700",Re: Reporting serialized task size after task broadcast change?,Sandy Ryza <sandy.ryza@cloudera.com>,"I don't think so. We should probably add a line to log it.


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 11 Sep 2014 18:29:51 -0700",Re: Reporting serialized task size after task broadcast change?,Reynold Xin <rxin@databricks.com>,"It used to be available on the UI, no?


"
Reynold Xin <rxin@databricks.com>,"Thu, 11 Sep 2014 18:33:24 -0700",Re: Reporting serialized task size after task broadcast change?,Sandy Ryza <sandy.ryza@cloudera.com>,"I didn't know about that ....


"
Denny Lee <denny.g.lee@gmail.com>,"Thu, 11 Sep 2014 18:35:04 -0700",RE: Announcing Spark 1.1.0!,"user@spark.apache.org, Haopu Wang <hwang@qilinsoft.com>, 
 dev@spark.apache.org, Patrick Wendell <pwendell@gmail.com>","I‚Äôm not sure if I‚Äôm completely answering your question here but I‚Äôm currently working (on OSX) with Hadoop 2.5 and I used the Spark 1.1 with Hadoop 2.4 without any issues.


te:

I see the binary packages include hadoop 1, 2.3 and 2.4.  
Does Spark 1.1.0 support hadoop 2.5.0 at below address?  

http://hadoop.apache.org/releases.html#11+August%2C+2014%3A+Release+2.5.0+available  

 
the second release on the API-compatible 1.X line. It is Spark's  
largest release ever, with contributions from 171 developers!  

This release brings operational and performance improvements in Spark  
core including a new implementation of the Spark shuffle designed for  
very large scale workloads. Spark 1.1 adds significant extensions to  
the newest Spark modules, MLlib and Spark SQL. Spark SQL introduces a  
JDBC server, byte code generation for fast expression evaluation, a  
public types API, JSON support, and other features and optimizations.  
MLlib introduces a new statistics library along with several new  
algorithms and optimizations. Spark 1.1 also builds out Spark's Python  
support and adds new components to the Spark Streaming module.  

Visit the release notes [1] to read about the new features, or  
download [2] the release today.  

[1] http://spark.eu.apache.org/releases/spark-release-1-1-0.html  
[2] http://spark.eu.apache.org/downloads.html  

NOTE: SOME ASF DOWNLOAD MIRRORS WILL NOT CONTAIN THE RELEASE FOR SEVERAL HOURS.  

Please e-mail me directly for any type-o's in the release notes or name listing.  

Thanks, and congratulations!  
- Patrick  

---------------------------------------------------------------------  
For additional commands, e-mail: user-help@spark.apache.org  

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 11 Sep 2014 18:47:59 -0700",Re: Reporting serialized task size after task broadcast change?,Reynold Xin <rxin@databricks.com>,"Hmm, well I can't find it now, must have been hallucinating.  Do you know
off the top of your head where I'd be able to find the size to log it?


"
Reynold Xin <rxin@databricks.com>,"Thu, 11 Sep 2014 18:59:45 -0700",Re: Reporting serialized task size after task broadcast change?,Sandy Ryza <sandy.ryza@cloudera.com>,"It is in the dag scheduler. Look for broadcast.


"
Denny Lee <denny.g.lee@gmail.com>,"Thu, 11 Sep 2014 19:00:10 -0700",RE: Announcing Spark 1.1.0!,"Patrick Wendell <pwendell@gmail.com>, Haopu Wang
 <hwang@qilinsoft.com>, dev@spark.apache.org, user@spark.apache.org","Please correct me if I‚Äôm wrong but I was under the impression as per the maven repositories that it was just to stay more in sync with the various version of Hadoop. ¬†Looking at the latest documentation (https://spark.apache.org/docs/latest/building-with-maven.html), there are multiple Hadoop versions called out.

As for the potential differences in Spark, this is more about ensuring the various jars and library dependencies of the correct version of Hadoop are included so there can be proper connectivity to Hadoop from Spark vs. any differences in Spark itself. ¬† Another good reference on this topic is call out for Hadoop versions within github:¬†https://github.com/apache/spark

HTH!


te:

Danny, thanks for the response.

¬†

I raise the question because in Spark 1.0.2, I saw one binary package for hadoop2, but in Spark 1.1.0, there are separate packages for hadoop 2.3 and 2.4.

That implies some difference in Spark according to hadoop version.

¬†

From:Denny Lee [mailto:denny.g.lee@gmail.com]
Sent: Friday, September 12, 2014 9:35 AM
To: user@spark.apache.org; Haopu Wang; dev@spark.apache.org; Patrick Wendell
Subject: RE: Announcing Spark 1.1.0!

¬†

I‚Äôm not sure if I‚Äôm completely answering your question here but I‚Äôm currently working (on OSX) with Hadoop 2.5 and I used the Spark 1.1 with Hadoop 2.4 without any issues.

¬†

¬†

te:

I see the binary packages include hadoop 1, 2.3 and 2.4.
Does Spark 1.1.0 support hadoop 2.5.0 at below address?

http://hadoop.apache.org/releases.html#11+August%2C+2014%3A+Release+2.5.0+available

the second release on the API-compatible 1.X line. It is Spark's
largest release ever, with contributions from 171 developers!

This release brings operational and performance improvements in Spark
core including a new implementation of the Spark shuffle designed for
very large scale workloads. Spark 1.1 adds significant extensions to
the newest Spark modules, MLlib and Spark SQL. Spark SQL introduces a
JDBC server, byte code generation for fast expression evaluation, a
public types API, JSON support, and other features and optimizations.
MLlib introduces a new statistics library along with several new
algorithms and optimizations. Spark 1.1 also builds out Spark's Python
support and adds new components to the Spark Streaming module.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

[1] http://spark.eu.apache.org/releases/spark-release-1-1-0.html
[2] http://spark.eu.apache.org/downloads.html

NOTE: SOME ASF DOWNLOAD MIRRORS WILL NOT CONTAIN THE RELEASE FOR SEVERAL HOURS.

Please e-mail me directly for any type-o's in the release notes or name listing.

Thanks, and congratulations!
- Patrick

---------------------------------------------------------------------
For additional commands, e-mail: user-help@spark.apache.org"
"""wyphao.2007"" <wyphao.2007@163.com>","Fri, 12 Sep 2014 10:05:48 +0800 (CST)",How to use jdbcRDD in JAVA,"dev@spark.apache.org, user@spark.apache.org","Hi,I want to know how to use jdbcRDD in JAVA not scala, trying to figure out the last parameter in the constructor of jdbcRDD


thanks"
Denny Lee <denny.g.lee@gmail.com>,"Thu, 11 Sep 2014 20:03:30 -0700",RE: Announcing Spark 1.1.0!,"user@spark.apache.org, Haopu Wang <hwang@qilinsoft.com>, 
 dev@spark.apache.org, Patrick Wendell <pwendell@gmail.com>","Yes, atleast for my query scenarios, I have been able to use Spark 1.1 with Hadoop 2.4 against Hadoop 2.5. ¬†Note, Hadoop 2.5 is considered a relatively minor release (http://hadoop.apache.org/releases.html#11+August%2C+2014%3A+Release+2.5.0+available) where Hadoop 2.4 and 2.3 were considered more significant releases.



te:

From the web page (https://spark.apache.org/docs/latest/building-with-maven.html) which is pointed out by you, it‚Äôs saying ‚ÄúBecause HDFS is not protocol-compatible across versions, if you want to read from HDFS, you‚Äôll need to build Spark against the specific HDFS version in your environment.‚Äù

¬†

Did you try to read a hadoop 2.5.0 file using Spark 1.1 with hadoop 2.4?

¬†

Thanks!

¬†

From:Denny Lee [mailto:denny.g.lee@gmail.com]
Sent: Friday, September 12, 2014 10:00 AM
To: Patrick Wendell; Haopu Wang; dev@spark.apache.org; user@spark.apache.org
Subject: RE: Announcing Spark 1.1.0!

¬†

Please correct me if I‚Äôm wrong but I was under the impression as per the maven repositories that it was just to stay more in sync with the various version of Hadoop. ¬†Looking at the latest documentation (https://spark.apache.org/docs/latest/building-with-maven.html), there are multiple Hadoop versions called out.

¬†

As for the potential differences in Spark, this is more about ensuring the various jars and library dependencies of the correct version of Hadoop are included so there can be proper connectivity to Hadoop from Spark vs. any differences in Spark itself. ¬† Another good reference on this topic is call out for Hadoop versions within github:¬†https://github.com/apache/spark

¬†

HTH!

¬†

¬†

te:

Danny, thanks for the response.

¬†

I raise the question because in Spark 1.0.2, I saw one binary package for hadoop2, but in Spark 1.1.0, there are separate packages for hadoop 2.3 and 2.4.

That implies some difference in Spark according to hadoop version.

¬†

From:Denny Lee [mailto:denny.g.lee@gmail.com]
Sent: Friday, September 12, 2014 9:35 AM
To: user@spark.apache.org; Haopu Wang; dev@spark.apache.org; Patrick Wendell
Subject: RE: Announcing Spark 1.1.0!

¬†

I‚Äôm not sure if I‚Äôm completely answering your question here but I‚Äôm currently working (on OSX) with Hadoop 2.5 and I used the Spark 1.1 with Hadoop 2.4 without any issues.

¬†

¬†

te:

I see the binary packages include hadoop 1, 2.3 and 2.4.
Does Spark 1.1.0 support hadoop 2.5.0 at below address?

http://hadoop.apache.org/releases.html#11+August%2C+2014%3A+Release+2.5.0+available

the second release on the API-compatible 1.X line. It is Spark's
largest release ever, with contributions from 171 developers!

This release brings operational and performance improvements in Spark
core including a new implementation of the Spark shuffle designed for
very large scale workloads. Spark 1.1 adds significant extensions to
the newest Spark modules, MLlib and Spark SQL. Spark SQL introduces a
JDBC server, byte code generation for fast expression evaluation, a
public types API, JSON support, and other features and optimizations.
MLlib introduces a new statistics library along with several new
algorithms and optimizations. Spark 1.1 also builds out Spark's Python
support and adds new components to the Spark Streaming module.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

[1] http://spark.eu.apache.org/releases/spark-release-1-1-0.html
[2] http://spark.eu.apache.org/downloads.html

NOTE: SOME ASF DOWNLOAD MIRRORS WILL NOT CONTAIN THE RELEASE FOR SEVERAL HOURS.

Please e-mail me directly for any type-o's in the release notes or name listing.

Thanks, and congratulations!
- Patrick

---------------------------------------------------------------------
For additional commands, e-mail: user-help@spark.apache.org"
Sean Owen <sowen@cloudera.com>,"Fri, 12 Sep 2014 09:03:17 +0100",Re: Questions regarding memory usage,Tom <thubregtsen@gmail.com>,"
Yes, that matches my understanding.



These programs create the same RDDs. The difference in how references
to JavaPairRDD are saved or moved around are immaterial. This is not
equivalent to the assignments you mention since these aren't
assignments, but method calls, and not least of which because they
have side effects.



I would not necessarily take the [37] to be a count of RDDs created so
far, not necessarily. But they are not 1-1 with method calls,
necessarily, either. It shouldn't really matter. They are handles on
stages of computations, and are materialized or computed as needed for
you.



Look at the ""Storage"" tab in the UI to see persisted RDDs. For RDDs
that aren't persisted, I'm not sure you can meaningfully say what
their actual resource consumption is. It could be nothing if it has
not yet been computed for example. Memory used for shuffling is not
the same as for persisting RDDs. Yes you can imagine situations in
which shuffles are really large.

I think the answers may depend more specifically on what you're getting at.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 12 Sep 2014 01:17:26 -0700","Re: PSA: SI-8835 (Iterator 'drop' method has a complexity bug causing
 quadratic behavior)",Erik Erlandson <eje@redhat.com>,"Thanks for the email, Erik.

The Scala collection library implementation is a complicated beast ...



"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Fri, 12 Sep 2014 14:14:11 +0530","Re: Some Serious Issue with Spark Streaming ? Blocks Getting Removed
 and Jobs have Failed..",Nan Zhu <zhunanmcgill@gmail.com>,"Dear all,

I am sorry. This was a false alarm

There was some issue in the RDD processing logic which leads to large
messages being pulled nicely without any Block Removed error. I need to
tune certain configurations in my Kafka Consumer to modify the data rate
and also the batch size.

Sorry again.


Regards,
Dibyendu


st (progress: 3/106)
test_customers
ecutor localhost: localhost (PROCESS_LOCAL)
es in 0 ms
19:49:13 INFO ContextCleaner: Cleaned broadcast 0*
st (progress: 4/106)
19:49:13 INFO MemoryStore: Block broadcast_0 of size 202564 dropped from memory (free 886623436)*
 0
etltest/hdfsData/customer.csv:25+5
07/21> 19:49:13 INFO HadoopRDD: Input split: hdfs://172.31.34.184:9000/etltest/hdfsData/customer.csv:20+5
07/21> 19:49:13 INFO TableOutputFormat: Created table instance for hdfstest_customers
ecutor localhost: localhost (PROCESS_LOCAL)
es in 0 ms
st (progress: 5/106)
test_customers
ecutor localhost: localhost (PROCESS_LOCAL)
es in 0 ms
st (progress: 6/106)
0
0
nection.java:1624)
6)
la:89)
sorImpl.java:43)
7)
798)
90)
798)
90)
798)
sorImpl.java:43)
7)
798)
90)
798)
90)
798)
sorImpl.java:43)
7)
798)
90)
798)
Serializer.scala:63)
la:61)
41)
7)
796)
Serializer.scala:63)
rializer.scala:85)
java:1145)
.java:615)
r?
c
I
+
n
sy
n
ke
se
gn
to
k
 of
e
)
:1145)
a:615)
"
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 12 Sep 2014 13:11:48 +0400",Adding abstraction in MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Here in Yandex, during implementation of gradient boosting in spark and
creating our ML tool for internal use, we found next serious problems in
MLLib:


   - There is no Regression/Classification model abstraction. We were
   building abstract data processing pipelines, which should work just with
   some regression - exact algorithm specified outside this code. There is no
   abstraction, which will allow me to do that. *(It's main reason for all
   further problems) *
   - There is no common practice among MLlib for testing algorithms: every
   model generates it's own random test data. There is no easy extractable
   test cases applible to another algorithm. There is no benchmarks for
   comparing algorithms. After implementing new algorithm it's very hard to
   understand how it should be tested.
   - Lack of serialization testing: MLlib algorithms don't contain tests
   which test that model work after serialization.
   - During implementation of new algorithm it's hard to understand what
   API you should create and which interface to implement.

Start for solving all these problems must be done in creating common
interface for typical algorithms/models - regression, classification,
clustering, collaborative filtering.

All main tests should be written against these interfaces, so when new
algorithm implemented - all it should do is passed already written tests.
It allow us to have managble quality among all lib.

There should be couple benchmarks which allow new spark user to get feeling
about which algorithm to use.

Test set against these abstractions should contain serialization test. In
production most time there is no need in model, which can't be stored.

As the first step of this roadmap I'd like to create trait RegressionModel,
*ADD* methods to current algorithms to implement this trait and create some
tests against it. Planning of doing it next week.

Purpose of this letter is to collect any objections to this approach on
early stage: please give any feedback. Second reason is to set lock on this
activity so we wouldn't do the same thing twice: I'll create pull request
by the end of the next week and any parallalizm in development we can start
from there.



-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Christoph Sawade <christoph.sawade@googlemail.com>,"Fri, 12 Sep 2014 13:53:26 +0200",Re: Adding abstraction in MLlib,Egor Pahomov <pahomov.egor@gmail.com>,"I totally agree, and we discovered also some drawbacks with the
classification models implementation that are based on GLMs:

- There is no distinction between predicting scores, classes, and
calibrated scores (probabilities). For these models it is common to have
access to all of them and the prediction function ``predict``should be
consistent and stateless. Currently, the score is only available after
removing the threshold from the model.
- There is no distinction between multinomial and binomial classification.
For multinomial problems, it is necessary to handle multiple weight vectors
and multiple confidences.
- Models are not serialisable, which makes it hard to use them in practise.

I started a pull request [1] some time ago. I would be happy to continue
the discussion and clarify the interfaces, too!

Cheers, Christoph

[1] https://github.com/apache/spark/pull/2137/

2014-09-12 11:11 GMT+02:00 Egor Pahomov <pahomov.egor@gmail.com>:

"
Guru Medasani <gdmeda@outlook.com>,"Fri, 12 Sep 2014 07:17:43 -0500",Re: Reporting serialized task size after task broadcast change?,Sandy Ryza <sandy.ryza@cloudera.com>,"I thought we could see this on the Spark Web UI storage tab. May be I was looking at something else too.


know
<sandy.ryza@cloudera.com>
to
task?


---------------------------------------------------------------------


"
Archit Thakur <archit279thakur@gmail.com>,"Fri, 12 Sep 2014 17:58:05 +0530",Use Case of mutable RDD - any ideas around will help.,dev@spark.incubator.apache.org,"Hi,

We have a use case where we are planning to keep sparkcontext alive in a
server and run queries on it. But the issue is we have  a continuous
flowing data the comes in batches of constant duration(say, 1hour). Now we
want to exploit the schemaRDD and its benefits of columnar caching and
compression. Is there a way I can append the new batch (uncached) to the
older(cached) batch without losing the older data from cache and caching
the whole dataset.

Thanks and Regards,


Archit Thakur.
Sr Software Developer,
Guavus, Inc.
"
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 12 Sep 2014 16:38:52 +0400",Re: Adding abstraction in MLlib,Christoph Sawade <christoph.sawade@googlemail.com>,"Sorry, I misswrote  - I meant learners part of framework - models already
exists.

2014-09-12 15:53 GMT+04:00 Christoph Sawade <christoph.sawade@googlemail.com



-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 12 Sep 2014 19:20:13 +0400",Re: Adding abstraction in MLlib,Christoph Sawade <christoph.sawade@googlemail.com>,"Some architect suggestions on this matter -
https://github.com/apache/spark/pull/2371

2014-09-12 16:38 GMT+04:00 Egor Pahomov <pahomov.egor@gmail.com>:




-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Rajiv Abraham <rajiv.abraham@gmail.com>,"Fri, 12 Sep 2014 11:48:47 -0400",Re: Junit spark tests,Sudershan Malpani <sudershan.malpani@gmail.com>,"Hi Sudershan,

That's interesting. I don't have an answer to your question but considering
the functional nature of Spark, I have hardly had to use mock objects(maybe
you could inform us of your use case). Mock object 'expectations' are in
'most' cases implementation of 'Tell, Don't ask' principle which deals with
stateful object interactions unlike the 'value object/type' (POJO objects,
I think , in Java parlance) kind of design expected by data driven
computation or functional programming(and Spark , I guess).

Maybe you could map a value object from your stateful domain object and
then feed that to Spark? And then it may become easier.

Another question to ask is are the tests testing multiple responsibilities
which sometimes is a pain to maintain. Should one separate out the mocking
into a unit test just for that domain object and have a separate unit test
just to test the data computation done by spark(which I suggested above).



2014-09-09 20:38 GMT-04:00 Sudershan Malpani <sudershan.malpani@gmail.com>:




-- 
Take care,
Rajiv
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 12 Sep 2014 09:07:06 -0700",Re: Use Case of mutable RDD - any ideas around will help.,Archit Thakur <archit279thakur@gmail.com>,"[moving to user@]

This would typically be accomplished with a union() operation. You
can't mutate an RDD in-place, but you can create a new RDD with a
union() which is an inexpensive operator.


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 12 Sep 2014 11:27:12 -0500",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"Cool, thanks for your help on this.  Any chance of adding it to the 1.1.1
point release, assuming there ends up being one?


"
Reynold Xin <rxin@databricks.com>,"Fri, 12 Sep 2014 10:45:07 -0700",Re: Adding abstraction in MLlib,Egor Pahomov <pahomov.egor@gmail.com>,"Xiangrui can comment more, but I believe Joseph and him are actually
working on standardize interface and pipeline feature for 1.2 release.


"
Hansu GU <guhansu@gmail.com>,"Fri, 12 Sep 2014 12:45:27 -0600",A Spark Compilation Question,dev@spark.apache.org,"I downloaded the source and imported it into IntelliJ 13.1 as a Maven project.

When I used IntelliJ Build -> make Project, I encountered:

Error:(44, 66) not found: type SparkFlumeProtocol val
transactionTimeout: Int, val backOffInterval: Int) extends
SparkFlumeProtocol with Logging {

I think there are some avro generated files missing but I am not sure.
Could anyone help me understand this in order to successfully compile
the source?

Thanks,
Hansu

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 12 Sep 2014 11:47:53 -0700",Re: Spark authenticate enablement,Jun Feng Liu <liujunf@cn.ibm.com>,"Hi Jun,

I believe that's correct that Spark authentication only works against YARN.

-Sandy


"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 12 Sep 2014 11:51:53 -0700",Re: Adding abstraction in MLlib,Reynold Xin <rxin@databricks.com>,"Hi Egor,

Thanks for the feedback! We are aware of some of the issues you
mentioned and there are JIRAs created for them. Specifically, I'm
pushing out the design on pipeline features and algorithm/model
parameters this week. We can move our discussion to
https://issues.apache.org/jira/browse/SPARK-1856 .

It would be nice to make tests against interfaces. But it definitely
needs more discussion before making PRs. For example, we discussed the
learning interfaces in Christoph's PR
(https://github.com/apache/spark/pull/2137/) but it takes time to
reach a consensus, especially on interfaces. Hopefully all of us could
benefit from the discussion. The best practice is to break down the
proposal into small independent piece and discuss them on the JIRA
before submitting PRs.

For performance tests, there is a spark-perf package
(https://github.com/databricks/spark-perf) and we added performance
tests for MLlib in v1.1. But definitely more work needs to be done.

The dev-list may not be a good place for discussion on the design,
could you create JIRAs for each of the issues you pointed out, and we
track the discussion on JIRA? Thanks!

Best,
Xiangrui


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 12 Sep 2014 12:07:56 -0700",Re: parquet predicate / projection pushdown into unionAll,Cody Koeninger <cody@koeninger.org>,"Yeah, thanks for implementing it!

Since Spark SQL is an alpha component and moving quickly the plan is to
backport all of master into the next point release in the 1.1 series.


"
Erik Erlandson <eje@redhat.com>,"Fri, 12 Sep 2014 15:10:21 -0400 (EDT)",Re: Adding abstraction in MLlib,Xiangrui Meng <mengxr@gmail.com>,"
Are interface designs being captured anywhere as documents that the community can follow along with as the proposals evolve?

I've worked on other open source projects where design docs were published as ""living documents"" (e.g. on google docs, or etherpad, but the particular mechanism isn't crucial).   FWIW, I found that to be a good way to work in a community environment.


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6469B11720
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 12 Sep 2014 19:49:20 +0000 (UTC)
Received: (qmail 78327 invoked by uid 500); 12 Sep 2014 19:49:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78250 invoked by uid 500); 12 Sep 2014 19:49:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78237 invoked by uid 99); 12 Sep 2014 19:49:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Sep 2014 19:49:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 12 Sep 2014 19:49:14 +0000
Received: by mail-we0-f171.google.com with SMTP id p10so1275965wes.2
        for <dev@spark.apache.org>; Fri, 12 Sep 2014 12:48:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=PrkPqg9XpggW5e8qn4+ew0ZcXWrYunqhp6zSOObTabk=;
        b=V95fE+8LFiYc87uZkpWRKAHUAEVfjJ20sbEdZnj/nZw/lWq+uUOmFmTX3NuPjlaoZL
         xQ7KLp6rlJpduFzJpu57mn1J5pJ8ZJ56orinuTBCibmJ4/i0ioRWBVrVx6z8pNCiVSsB
         CPQL0Y7xPLr/UhkGjdqn+EOlZ0xwn9eMC2gHQXtW02HyeIa7wjeyJ4uTQeOWdp2JXTfH
         JqFtPsjDCaiKg7ml2XRkcGe0fz7OGDdJAhsN7D9Cfr7mvcp1jdE7trw7HtE+XmRPLmk2
         4b0IMGnjicELQNedcYoJNL6LzECEuHkSoeh8D8gealm3bJh0Vy69cX3G1GWUfNbU6kB/
         7wNQ==
X-Received: by 10.195.11.200 with SMTP id ek8mr13971785wjd.85.1410551333599;
 Fri, 12 Sep 2014 12:48:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 12 Sep 2014 12:48:13 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 12 Sep 2014 15:48:13 -0400
Message-ID: <CAOhmDzc1=pp6ce4y7Uj4zSq4+9WrfTbypMkjW6P_TZyyAsz0aQ@mail.gmail.com>
Subject: don't trigger tests when only .md files are changed
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b8737622d45010502e393ea
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b8737622d45010502e393ea
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Would it make sense to have Jenkins *not* trigger tests when the only files
that have changed are .md files (example
<https://github.com/apache/spark/pull/2367>)? Those don=E2=80=99t even need=
 RAT
checks, right?

I can make this change if it makes sense.

Nick
=E2=80=8B

--047d7b8737622d45010502e393ea--

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 12 Sep 2014 15:51:36 -0400",Re: don't trigger tests when only .md files are changed,dev <dev@spark.apache.org>,"We could still have Jenkins post a message to the effect of ‚Äúthis patch
only modifies .md files; no tests will be run‚Äù.
‚Äã


ed RAT
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 12 Sep 2014 13:00:21 -0700",Re: Adding abstraction in MLlib,Erik Erlandson <eje@redhat.com>,"We typically post design docs on JIRA's before major work starts. For
instance, pretty sure SPARk-1856 will have a design doc posted
shortly.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 12 Sep 2014 13:00:41 -0700",Re: don't trigger tests when only .md files are changed,Nicholas Chammas <nicholas.chammas@gmail.com>,"I like that idea, but the load on Jenkins isn't very high. The more
complexity we add to the test script, the easier it is to screw it up (at
some point we would need to add unit tests for the build scripts).

Maybe we can just add the message part, so it becomes clear that a pull
request does not change anything other than markdown files, but still run
through the regular tests?




 patch
need RAT
"
Rajiv Abraham <rajiv.abraham@gmail.com>,"Fri, 12 Sep 2014 17:19:47 -0400",Response to archived question 'Spark and Scala Worksheet',"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
This is a response to an archived email about how to run Spark in a Scala
worksheet in the Scala IDE.

http://mail-archives.apache.org/mod_mbox/spark-user/201401.mbox/%3CCAAUywg8A+mJQwhtgYtz0LUMntLGFwA-NOxTOPEYaDEQ+GwSLYg@mail.gmail.com%3E

I know it's a bit late :) but here is how I do it.

https://gist.github.com/RAbraham/585939e5390d46a7d6f8



-- 
Take care,
Rajiv
"
Du Li <lidu@yahoo-inc.com.INVALID>,"Sat, 13 Sep 2014 00:47:44 +0000",NullWritable not serializable,"""user@spark.apache.org"" <user@spark.apache.org>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi,

I was trying the following on spark-shell (built with apache master and hadoop 2.4.0). Both calling rdd2.collect and calling rdd3.collect threw java.io.NotSerializableException: org.apache.hadoop.io.NullWritable.

I got the same problem in similar code of my app which uses the newly released Spark 1.1.0 under hadoop 2.4.0. Previously it worked fine with spark 1.0.2 under either hadoop 2.40 and 0.23.10.

Anybody knows what caused the problem?

Thanks,
Du

----
import org.apache.hadoop.io.{NullWritable, Text}
val rdd = sc.textFile(""README.md"")
val res = rdd.map(x => (NullWritable.get(), new Text(x)))
res.saveAsSequenceFile(""./test_data"")
val rdd2 = sc.sequenceFile(""./test_data"", classOf[NullWritable], classOf[Text])
rdd2.collect
val rdd3 = sc.sequenceFile[NullWritable,Text](""./test_data"")
rdd3.collect


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 13 Sep 2014 00:10:11 -0400",Re: NullWritable not serializable,"Du Li <lidu@yahoo-inc.com.invalid>, 
 ""=?utf-8?Q?user=40spark.apache.org?="" <user@spark.apache.org>, 
 ""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","Hi Du,

I don't think NullWritable has ever been serializable, so you must be doing something differently from your previous program. In this case though, just use a map() to turn your Writables to serializable types (e.g. null and String).

Matie


Hi,

I was trying the following on spark-shell (built with apache master and hadoop 2.4.0). Both calling rdd2.collect and calling rdd3.collect threw¬†java.io.NotSerializableException: org.apache.hadoop.io.NullWritable.¬†

I got the same problem in similar code of my app which uses the newly released Spark 1.1.0 under hadoop 2.4.0. Previously it worked fine with spark 1.0.2 under either hadoop 2.40 and 0.23.10.

Anybody knows what caused the problem?

Thanks,
Du

----
import org.apache.hadoop.io.{NullWritable, Text}
val rdd = sc.textFile(""README.md"")
val res = rdd.map(x => (NullWritable.get(), new Text(x)))
res.saveAsSequenceFile(""./test_data"")
val rdd2 = sc.sequenceFile(""./test_data"", classOf[NullWritable], classOf[Text])
rdd2.collect
val rdd3 = sc.sequenceFile[NullWritable,Text](""./test_data"")
rdd3.collect


"
Yanbo Liang <yanbohappy@gmail.com>,"Sat, 13 Sep 2014 17:12:21 +0800",[mllib] LogisticRegressionWithLBFGS interface is not consistent with LogisticRegressionWithSGD,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org, mengxr@gmail.com, 
	dbtsai@dbtsai.com","Hi All,

I found that LogisticRegressionWithLBFGS interface is not consistent
with LogisticRegressionWithSGD in master and 1.1 release.

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/LogisticRegression.scala#L199

In the above code snippet, users can only construct a
LogisticRegressionWithLBFGS without any user specified parameters.
Although users can specific training parameters calling corresponding
function of class LBFGS.
But this behave different with LogisticRegressionWithSGD.
Could anyone can tell me why we did not refactor the code to keep
consistent interface?

Thank you
Yanbo
"
Yanbo Liang <yanbohappy@gmail.com>,"Sat, 13 Sep 2014 17:15:17 +0800","Re: [mllib] LogisticRegressionWithLBFGS interface is not consistent
 with LogisticRegressionWithSGD","""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org, mengxr@gmail.com, 
	dbtsai@dbtsai.com","I also found
https://github.com/apache/spark/commit/8f6e2e9df41e7de22b1d1cbd524e20881f861dd0
had resolve this issue but it seems that right code snippet not occurs in
master or 1.1 release.

2014-09-13 17:12 GMT+08:00 Yanbo Liang <yanbohappy@gmail.com>:

"
DB Tsai <dbtsai@dbtsai.com>,"Sat, 13 Sep 2014 02:25:34 -0700","Re: [mllib] LogisticRegressionWithLBFGS interface is not consistent
 with LogisticRegressionWithSGD",Yanbo Liang <yanbohappy@gmail.com>,"Hi Yanbo,

We made the change here
https://github.com/apache/spark/commit/5d25c0b74f6397d78164b96afb8b8cbb1b15cfbd

Those apis to set the parameters are very difficult to maintain, so we
decide not to provide them. In next release, Spark 1.2, we will have a
better api design for parameter setting.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Sun, 14 Sep 2014 09:23:02 +0800",Workload for spark testing,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi All:

We know some memory of spark are used for computing (e.g.,
spark.shuffle.memoryFraction) and some are used for caching RDD for future
use (e.g., spark.storage.memoryFraction).

Is there any existing workload which can utilize both of them during the
running left cycle? I want to do some performance by adjusting the ratio of
them.

Thanks.

-- 
*Regards,*
*Zhaojie*
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 13 Sep 2014 23:07:33 -0700",Tests and Test Infrastructure,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Wanted to send a quick update about test infrastructure. With the
number of contributors we have and the rate of development,
maintaining a well-oiled test infra is really important.

Every time a flaky test fails a legitimate pull request, it wastes
developer time and effort.

1. Master build: Spark's master builds are back to green again in
Maven and SBT after a long time of instability. Big thanks to Josh
Rosen, Andrew Or, Nick Chammas, Shane Knapp, Sean Owen, and many
others who were involved in pinpointing and fixing fairly convoluted
test failure issues.

2. Jenkins PRB: The Jenkins Pull Request Builder is mostly functioning
again. However, we are working on a simpler technical pipeline for
testing patches, as this plug-in has been a constant source of
downtime and issues for us, and is very hard to debug.

3. Reverting flaky patches: Going forward - we may revert patches that
seem to be the root cause of flaky or failing tests. This is necessary
as these days, the test infra being down will block something like
10-30 in-flight patches on a given day. This puts the onus back on the
test writer to try and figure out what's going on - we'll of course
help debug the issue!

4. Time of tests: With hundreds (thousands?) of tests, we will have a
very high bar for tests which take several seconds or longer. Things
like Thread.sleep() bloat test time when proper synchronization
mechanisms should be used. Expect reviewers to push back on any
long-running tests, in many cases they can be re-written to be both
shorter and better.

Thanks again to everyone putting in effort on this, we've made a ton
of progress in the last few weeks. A solid test infra will help us
scale and move quickly as Spark development continues to accelerate.

- Patrick

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sat, 13 Sep 2014 23:20:17 -0700",Re: Tests and Test Infrastructure,"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>, Patrick
 Wendell <pwendell@gmail.com>","Also, huge thanks to Cheng Lian, who tracked down and fixed the final issue that was causing the Maven master build‚Äôs Spark SQL tests to fail!

Hey All,  

Wanted to send a quick update about test infrastructure. With the  
number of contributors we have and the rate of development,  
maintaining a well-oiled test infra is really important.  

Every time a flaky test fails a legitimate pull request, it wastes  
developer time and effort.  

1. Master build: Spark's master builds are back to green again in  
Maven and SBT after a long time of instability. Big thanks to Josh  
Rosen, Andrew Or, Nick Chammas, Shane Knapp, Sean Owen, and many  
others who were involved in pinpointing and fixing fairly convoluted  
test failure issues.  

2. Jenkins PRB: The Jenkins Pull Request Builder is mostly functioning  
again. However, we are working on a simpler technical pipeline for  
testing patches, as this plug-in has been a constant source of  
downtime and issues for us, and is very hard to debug.  

3. Reverting flaky patches: Going forward - we may revert patches that  
seem to be the root cause of flaky or failing tests. This is necessary  
as these days, the test infra being down will block something like  
10-30 in-flight patches on a given day. This puts the onus back on the  
test writer to try and figure out what's going on - we'll of course  
help debug the issue!  

4. Time of tests: With hundreds (thousands?) of tests, we will have a  
very high bar for tests which take several seconds or longer. Things  
like Thread.sleep() bloat test time when proper synchronization  
mechanisms should be used. Expect reviewers to push back on any  
long-running tests, in many cases they can be re-written to be both  
shorter and better.  

Thanks again to everyone putting in effort on this, we've made a ton  
of progress in the last few weeks. A solid test infra will help us  
scale and move quickly as Spark development continues to accelerate.  

- Patrick  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 14 Sep 2014 18:04:20 -0400",Re: Tests and Test Infrastructure,Josh Rosen <rosenville@gmail.com>,"I fully support this. A smoothly running test infrastructure helps
everybody‚Äôs work just flow better.

The Jenkins Pull Request Builder is mostly functioning
again. However, we are working on a simpler technical pipeline for
testing patches, as this plug-in has been a constant source of
downtime and issues for us, and is very hard to debug.

fetch from git. Hopefully a new pipeline will be able to fetch more
reliably.

flaky tests

Dunno if these were some of the ones recently fixed, but the flakiest tests
seem to be the Kafka and Flume tests in Spark Streaming, based purely on my
subjective experience. It would be great if we could stabilize them!

Time of tests

PSA: Here are some related JIRA issues for those interested in working on
our testing setup:

   - SPARK-3431: Parallelize execution of tests
   <https://issues.apache.org/jira/browse/SPARK-3431>
   - SPARK-3432: Fix logging of unit test execution time
   <https://issues.apache.org/jira/browse/SPARK-3432>

Nick
‚Äã


o fail!
)
"
Cody Koeninger <cody@koeninger.org>,"Sun, 14 Sep 2014 17:10:01 -0500",Support for Hive buckets,"""dev@spark.apache.org"" <dev@spark.apache.org>","I noticed that the release notes for 1.1.0 said that spark doesn't support
Hive buckets ""yet"".  I didn't notice any jira issues related to adding
support.

Broadly speaking, what would be involved in supporting buckets, especially
the bucketmapjoin and sortedmerge optimizations?
"
David Tung <dtung@touchtunes.com>,"Sun, 14 Sep 2014 18:41:23 -0400",Source code for mining big data with Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I watched am impressed spark demo video by Reynold Xin and Aaron Davidson
in youtube ( https://www.youtube.com/watch?v=FjhRkfAuU7I ). Can someone
let me know where can I find the source codes for the demo? I canπt see
the source codes from video clearly.

Thanks in advance

CONFIDENTIALITY CAUTION 
This e-mail and any attachments may be confidential or legally privileged. If you received this message in error or are not the intended recipient, you should destroy the e-mail message and any attachments or copies, and you are prohibited from retaining, distributing, disclosing or using any information contained herein. Please inform us of the erroneous delivery by return e-mail. Thank you for your cooperation.
DOCUMENT CONFIDENTIEL 
Le prÈsent courriel et tout fichier joint ‡ celui-ci peuvent contenir des renseignements confidentiels ou privilÈgiÈs. Si cet envoi ne s'adresse pas ‡ vous ou si vous l'avez reÁu par erreur, vous devez l'effacer. Vous ne pouvez conserver, distribuer, communiquer ou utiliser les renseignements qu'il contient. Nous vous prions de nous signaler l'erreur par courriel. Merci de votre collaboration.

---------------------------------------------------------------------


"
Egor Pahomov <pahomov.egor@gmail.com>,"Mon, 15 Sep 2014 10:26:05 +0400",Re: Adding abstraction in MLlib,Patrick Wendell <pwendell@gmail.com>,"It's good, that databricks working on this issue! However current process
of working on that is not very clear for outsider.

   - Last update on this ticket is August 5. If all this time was active
   development, I have concerns that without feedback from community for such
   long time development can fall in wrong way.
   - Even if it would be great big patch as soon as you introduce new
   interfaces to community it would allow us to start working on our pipeline
   code. It would allow us write algorithm in new paradigm instead of in lack
   of any paradigms like it was before. It would allow us to help you transfer
   old code to new paradigm.

My main point - shorter iterations with more transparency.

I think it would be good idea to create some pull request with code, which
you have so far, even if it doesn't pass tests, so just we can comment on
it before formulating it in design doc.


2014-09-13 0:00 GMT+04:00 Patrick Wendell <pwendell@gmail.com>:




-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Reynold Xin <rxin@databricks.com>,"Mon, 15 Sep 2014 00:44:38 -0700",Re: Adding abstraction in MLlib,Egor Pahomov <pahomov.egor@gmail.com>,"Hi Egor,

Thanks for the suggestion. It is definitely our intention and practice to
post design docs as soon as they are ready, and short iteration cycles. As
a matter of fact, we encourage design docs for major features posted before
implementation starts, and WIP pull requests before they are fully baked
for large features.

That said, no, not 100% of a committer's time is on a specific ticket.
There are lots of tickets that are open for a long time before somebody
starts actively working on it. So no, it is not true that ""all this time
was active development"". Xiangrui should post the design doc as soon as it
is ready for feedback.




"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Mon, 15 Sep 2014 06:44:24 -0700",Re: Spark authenticate enablement,"Jun Feng Liu <liujunf@cn.ibm.com>,
  ""dev@spark.apache.org"" <dev@spark.apache.org>","Spark authentication does work in standalone mode (atleast it did, I haven't tested it in a while). The same shared secret has to be set on all the daemons (master and workers) and then also in the configs of any applications submitted.  Since everyone shares the same secret its by no means ideal or
I am trying to enable the authentication
on spark on standealone model. Seems like only SparkSubmit load the properties
from spark-defaults.conf.  org.apache.spark.deploy.master.Master dose
not really load the default setting from spark-defaults.conf.  

Dose it mean the spark authentication
only work for like YARN model? Or I missed something with standalone model.
 
Best Regards 
  
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing 

________________________________
 
  Phone: 86-10-82452683 
E-mail:liujunf@cn.ibm.com  

BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China  "
Andrew Ash <andrew@andrewash.com>,"Mon, 15 Sep 2014 08:59:48 -0700",Re: PARSING_ERROR from kryo,npanj <nitinpanj@gmail.com>,"Hi npanj,

I'm seeing the same exception now on the Spark 1.1.0 release.  Did you ever
get this figured out?

Andrew


"
Ankur Dave <ankurdave@gmail.com>,"Mon, 15 Sep 2014 14:10:52 -0700",Re: PARSING_ERROR from kryo,"Andrew Ash <andrew@andrewash.com>, npanj <nitinpanj@gmail.com>","
The error is because I merged a GraphX PR that introduced a nondeterministic bug [1]. I reverted the faulty PR, but it was too late for the 1.1.0 release. The problem should go away if you use branch-1.1 or master. Sorry about that...

Ankur

[1] https://issues.apache.org/jira/browse/SPARK-3400

---------------------------------------------------------------------


"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Mon, 15 Sep 2014 15:21:53 -0700",Re: CoHadoop Papers,Gary Malouf <malouf.gary@gmail.com>,"This feature is called ""block affinity groups"" and it's been under
discussion for a while, but isn't fully implemented yet.  HDFS-2576 is
not a complete solution because it doesn't change the way the balancer
storage management (HDFS-2832) is implemented, you will be able to get
a similar effect through using separate storages, at the cost of
fragmenting the backing store somewhat.

Of course, ""co-locating related data blocks"" is often bad, not good,
because it reduces the amount of parallelism a single job can exploit,
and can increase the chance of losing an entire dataset due to node
failures.  That's one reason why the current semi-random placement
strategy has lasted so long.  In other words, this is
workload-dependent.

best,
Colin


---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Mon, 15 Sep 2014 15:31:13 -0700",Re: PARSING_ERROR from kryo,Ankur Dave <ankurdave@gmail.com>,"I should clarify: I'm not using GraphX, it's a different
application-specific Kryo registrator that causes the same stacktrace
ending in PARSING_ERROR:

com.esotericsoftware.kryo.KryoException: java.io.IOException: failed to
uncompress the chunk: PARSING_ERROR(2)
com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
com.esotericsoftware.kryo.io.Input.require(Input.java:169)
com.esotericsoftware.kryo.io.Input.readInt(Input.java:325)
com.esotericsoftware.kryo.io.Input.readFloat(Input.java:624)
com.esotericsoftware.kryo.serializers.DefaultSerializers$FloatSerializer.read(DefaultSerializers.java:127)
com.esotericsoftware.kryo.serializers.DefaultSerializers$FloatSerializer.read(DefaultSerializers.java:117)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)
com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
... my registrator


Ankur from my read of the ticket there's not a root cause identified for
those PARSING_ERROR exceptions in GraphX yet?


Andrew


"
Erik Erlandson <eje@redhat.com>,"Mon, 15 Sep 2014 19:23:19 -0400 (EDT)",why does BernoulliSampler class use a lower and upper bound?,dev <dev@spark.apache.org>,"I'm climbing under the hood in there for SPARK-3250, and I see this:

override def sample(items: Iterator[T]): Iterator[T] = {
  items.filter { item =>
    val x = rng.nextDouble()
    (x >= lb && x < ub) ^ complement
  }
}


The clause (x >= lb && x < ub) is equivalent to (x < ub-lb), which is faster, and requires only one parameter (sampling fraction).   Any caller asking for BernoulliSampler(a, b) can equally well ask for BernoulliSampler(b-a).

Is there some angle I'm missing?

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 15 Sep 2014 17:14:20 -0700",Re: why does BernoulliSampler class use a lower and upper bound?,Erik Erlandson <eje@redhat.com>,"It is also used in RDD.randomSplit. -Xiangrui


---------------------------------------------------------------------


"
Otis Gospodnetic <otis.gospodnetic@gmail.com>,"Mon, 15 Sep 2014 21:49:02 -0400",Wiki page for Operations/Monitoring tools?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I'm looking for a suitable place on the Wiki to add some info about a Spark
monitoring we've built.  The Wiki looks nice and orderly, so I didn't want
to go in and mess it up without asking where to put such info.  I don't see
an existing ""Operations"" or ""Monitoring"" or similar pages.  Should I just
create a Child page under https://cwiki.apache.org/confluence/display/SPARK
?

Thanks,
Otis
--
Monitoring * Alerting * Anomaly Detection * Centralized Log Management
Solr & Elasticsearch Support * http://sematext.com/
"
npanj <nitinpanj@gmail.com>,"Mon, 15 Sep 2014 18:57:02 -0700 (PDT)",Re: PARSING_ERROR from kryo,dev@spark.incubator.apache.org,"Hi Andrew,

No I could not figure out the root cause. This seems to be non-deterministic
error... I didn't see same error after rerunning same program. But I noticed
same error on a different program. 

First I thought that this may be related to SPARK-2878, but @Graham replied
that this looks irrelevant.






--

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 15 Sep 2014 22:52:18 -0700",Re: NullWritable not serializable,Du Li <lidu@yahoo-inc.com>,"Can you post the exact code for the test that worked in 1.0? I can't think of much that could've changed. The one possibility is if ¬†we had some operations that were computed locally on the driver (this happens with things like first() and take(), which will try to do the first partition locally). But generally speaking these operations should *not* work over a network, so you'll have to make sure that you only send serializable types through shuffles or collects, or use a serialization framework like Kryo that might be okay with Writables.

Matei


Hi Matei,

Thanks for your reply.¬†

The Writable classes have never been serializable and this is why it is weird. I did try as you suggested to map the Writables to integers and strings. It didn‚Äôt pass, either. Similar exceptions were thrown except that the messages became IntWritable, Text are not serializable. The reason is in the implicits defined in the SparkContext object that convert those values into their corresponding Writable classes before saving the data in sequence file.

My original code was actual some test cases to try out SequenceFile related APIs. The tests all passed when the spark version was specified as 1.0.2. But this one failed after I changed the spark version to 1.1.0 the new release, nothing else changed. In addition, it failed when I called rdd2.collect(), take(1), and first(). But it worked fine when calling rdd2.count(). As you can see, count() does not need to serialize and ship data while the other three methods do.

Do you recall any difference between spark 1.0 and 1.1 that might cause this problem?

Thanks,
Du


From: Matei Zaharia <matei.zaharia@gmail.com>
Date: Friday, September 12, 2014 at 9:10 PM
To: Du Li <lidu@yahoo-inc.com.invalid>, ""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>
Subject: Re: NullWritable not serializable

Hi Du,

I don't think NullWritable has ever been serializable, so you must be doing something differently from your previous program. In this case though, just use a map() to turn your Writables to serializable types (e.g. null and String).

Matie


Hi,

I was trying the following on spark-shell (built with apache master and hadoop 2.4.0). Both calling rdd2.collect and calling rdd3.collect threw¬†java.io.NotSerializableException: org.apache.hadoop.io.NullWritable.¬†

I got the same problem in similar code of my app which uses the newly released Spark 1.1.0 under hadoop 2.4.0. Previously it worked fine with spark 1.0.2 under either hadoop 2.40 and 0.23.10.

Anybody knows what caused the problem?

Thanks,
Du

----
import org.apache.hadoop.io.{NullWritable, Text}
val rdd = sc.textFile(""README.md"")
val res = rdd.map(x => (NullWritable.get(), new Text(x)))
res.saveAsSequenceFile(""./test_data"")
val rdd2 = sc.sequenceFile(""./test_data"", classOf[NullWritable], classOf[Text])
rdd2.collect
val rdd3 = sc.sequenceFile[NullWritable,Text](""./test_data"")
rdd3.collect


"
rapelly kartheek <kartheek.mbms@gmail.com>,"Tue, 16 Sep 2014 12:04:52 +0530","how does replicate() method in BlockManager.scala aquires resources
 for rdd replication",dev@spark.apache.org,"HI,

I was tracing the flow of replicate method in BlockManager.scala. I am
trying to find out as to where exactly in the code, the resources are
aquired for rdd replication.

I find that the BlockManagerMaster.getPeers() method returns only one
BlockManagerId for all the rdd partitions.

But, the storage details of the rdd in WebUI depicts that all the
partitions are replicated over multiple nodes.

I just want to find out where do we get the resources allocated for rdd
replication.

Can someone please help me out in this regard!!

Thank you
Karthik
"
sochi <sochiseung@gmail.com>,"Tue, 16 Sep 2014 00:07:34 -0700 (PDT)",GraphX: some vertex with specific edge,dev@spark.incubator.apache.org,"Hi. Im ChiSeung. 

1. How to know each vertices connecting some edges? 

I want to know how I find edges connected some vertices. 

And 
  
2. 
example, there are vertex a, b, c, d and edge e1. 
      a and b is connected by e1 
      b and c is connected by e1 
      c and d is also connected by e1 

so, above example is like  a ---(e1)---> b ---(e1)---> c ---(e1)---> d 

In this case, can I find b,c and d when I have just src vertex, a and edge,
e1?  



--

---------------------------------------------------------------------


"
Ankur Dave <ankurdave@gmail.com>,"Tue, 16 Sep 2014 02:29:47 -0700",Re: GraphX: some vertex with specific edge,"sochi <sochiseung@gmail.com>, dev@spark.incubator.apache.org","
First, to clarify: the three edges in your example are all distinct, since they have different source and destination vertices. Therefore I assume you're using e1 to refer to the edge property that they have in common.

In that case, this problem is equivalent to finding the connected component containing vertex a in the subgraph where edges have property e1. Here is how to do that in the Spark shell: https://gist.github.com/ankurdave/25732a493bc8c8541c97

Ankur

---------------------------------------------------------------------


"
"""wyphao.2007"" <wyphao.2007@163.com>","Tue, 16 Sep 2014 17:59:45 +0800 (CST)",Building Spark source error with maven,"dev@spark.incubator.apache.org, ""Reynold Xin"" <rxin@databricks.com>","Hi, When I building spark with maven, but  failed, the error message is as following.  I didn't found the satisfactory solution by google.  Anyone can help me? Thank you!


INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM .......................... SUCCESS [2.173s]
[INFO] Spark Project Core ................................ SUCCESS [3:13.638s]
[INFO] Spark Project Bagel ............................... SUCCESS [22.405s]
[INFO] Spark Project GraphX .............................. FAILURE [1.719s]
[INFO] Spark Project Streaming ........................... SKIPPED
[INFO] Spark Project ML Library .......................... SKIPPED
[INFO] Spark Project Tools ............................... SKIPPED
[INFO] Spark Project Catalyst ............................ SKIPPED
[INFO] Spark Project SQL ................................. SKIPPED
[INFO] Spark Project Hive ................................ SKIPPED
[INFO] Spark Project REPL ................................ SKIPPED
[INFO] Spark Project YARN Parent POM ..................... SKIPPED
[INFO] Spark Project YARN Stable API ..................... SKIPPED
[INFO] Spark Project Assembly ............................ SKIPPED
[INFO] Spark Project External Twitter .................... SKIPPED
[INFO] Spark Project External Kafka ...................... SKIPPED
[INFO] Spark Project External Flume Sink ................. SKIPPED
[INFO] Spark Project External Flume ...................... SKIPPED
[INFO] Spark Project External ZeroMQ ..................... SKIPPED
[INFO] Spark Project External MQTT ....................... SKIPPED
[INFO] Spark Project Examples ............................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3:40.545s
[INFO] Finished at: Tue Sep 16 17:53:02 CST 2014
[INFO] Final Memory: 59M/949M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile (scala-compile-first) on project spark-graphx_2.10: wrap: scala.reflect.internal.MissingRequirementError: object scala.runtime in compiler mirror not found. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-graphx_2.10"
Andrew Or <andrew@databricks.com>,"Tue, 16 Sep 2014 11:06:14 -0700",Re: Spark authenticate enablement,Tom Graves <tgraves_cs@yahoo.com>,"Hi Jun,

You can still set the authentication variables through `spark-env.sh`, by
exporting SPARK_MASTER_OPTS, SPARK_WORKER_OPTS, SPARK_HISTORY_OPTS etc to
include ""-Dspark.auth.{...}"". There is an open pull request that allows
these processes to also read from spark-defaults.conf, but this is not
merged into master yet.

Andrew

2014-09-15 6:44 GMT-07:00 Tom Graves <tgraves_cs@yahoo.com.invalid>:

"
rapelly kartheek <kartheek.mbms@gmail.com>,"Tue, 16 Sep 2014 22:35:47 +0530","Re: how does replicate() method in BlockManager.scala aquires
 resources for rdd replication",dev@spark.apache.org,"I could resolve the conflict between my method trace and the details from
the webUI.

I was modifying and compiling only on the master node. So, I found only
node in the print trace. Now, I incorporated the prints in all the nodes
and compiled them individually. Then started all the processes and ran a
job. Each node was dealing with one node for partition replication. That
way, storage details from the webUI and the print traces from all the nodes
are both in sink.

But, Where do I get all the replication nodes at a time? The getPeers()
method calls askDriverWithReply(), which returns only one BlockManagerId in
each node.

Can someone please help me where to look for obtaining all the nodes chosen
for replication.

Thank you


"
Du Li <lidu@yahoo-inc.com.INVALID>,"Tue, 16 Sep 2014 19:25:56 +0000",Re: NullWritable not serializable,Matei Zaharia <matei.zaharia@gmail.com>,"Hi,

The test case is separated out as follows. The call to rdd2.first() breaks when spark version is changed to 1.1.0, reporting exception NullWritable not serializable. However, the same test passed with spark 1.0.2. The pom.xml file is attached. The test data README.md was copied from spark.

Thanks,
Du
-----

package com.company.project.test

import org.scalatest._

class WritableTestSuite extends FunSuite {
  test(""generated sequence file should be readable from spark"") {
    import org.apache.hadoop.io.{NullWritable, Text}
    import org.apache.spark.{SparkContext, SparkConf}
    import org.apache.spark.SparkContext._

    val conf = new SparkConf(false).setMaster(""local"").setAppName(""test data exchange with spark"")
    val sc = new SparkContext(conf)

    val rdd = sc.textFile(""README.md"")
    val res = rdd.map(x => (NullWritable.get(), new Text(x)))
    res.saveAsSequenceFile(""./test_data"")

    val rdd2 = sc.sequenceFile(""./test_data"", classOf[NullWritable], classOf[Text])

    assert(rdd.first == rdd2.first._2.toString)
  }
}



From: Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.comDate: Monday, September 15, 2014 at 10:52 PM
To: Du Li <lidu@yahoo-inc.com<mailto:lidu@yahoo-inc.com>>
Cc: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: NullWritable not serializable

Can you post the exact code for the test that worked in 1.0? I can't think of much that could've changed. The one possibility is if  we had some operations that were computed locally on the driver (this happens with things like first() and take(), which will try to do the first partition locally). But generally speaking these operations should *not* work over a network, so you'll have to make sure that you only send serializable types through shuffles or collects, or use a serialization framework like Kryo that might be okay with Writables.

Matei



Hi Matei,

Thanks for your reply.

The Writable classes have never been serializable and this is why it is weird. I did try as you suggested to map the Writables to integers and strings. It didnít pass, either. Similar exceptions were thrown except that the messages became IntWritable, Text are not serializable. The reason is in the implicits defined in the SparkContext object that convert those values into their corresponding Writable classes before saving the data in sequence file.

My original code was actual some test cases to try out SequenceFile related APIs. The tests all passed when the spark version was specified as 1.0.2. But this one failed after I changed the spark version to 1.1.0 the new release, nothing else changed. In addition, it failed when I called rdd2.collect(), take(1), and first(). But it worked fine when calling rdd2.count(). As you can see, count() does not need to serialize and ship data while the other three methods do.

Do you recall any difference between spark 1.0 and 1.1 that might cause this problem?

Thanks,
Du


From: Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.comDate: Friday, September 12, 2014 at 9:10 PM
To: Du Li <lidu@yahoo-inc.com.invalid<mailto:lidu@yahoo-inc.com.invalid>>, ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: NullWritable not serializable

Hi Du,

I don't think NullWritable has ever been serializable, so you must be doing something differently from your previous program. In this case though, just use a map() to turn your Writables to serializable types (e.g. null and String).

Matie



Hi,

I was trying the following on spark-shell (built with apache master and hadoop 2.4.0). Both calling rdd2.collect and calling rdd3.collect threw java.io.NotSerializableException: org.apache.hadoop.io.NullWritable.

I got the same problem in similar code of my app which uses the newly released Spark 1.1.0 under hadoop 2.4.0. Previously it worked fine with spark 1.0.2 under either hadoop 2.40 and 0.23.10.

Anybody knows what caused the problem?

Thanks,
Du

----
import org.apache.hadoop.io.{NullWritable, Text}
val rdd = sc.textFile(""README.md"")
val res = rdd.map(x => (NullWritable.get(), new Text(x)))
res.saveAsSequenceFile(""./test_data"")
val rdd2 = sc.sequenceFile(""./test_data"", classOf[NullWritable], classOf[Text])
rdd2.collect
val rdd3 = sc.sequenceFile[NullWritable,Text](""./test_data"")
rdd3.collect



---------------------------------------------------------------------"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 16 Sep 2014 19:42:18 +0000",RE: NullWritable not serializable,"Du Li <lidu@yahoo-inc.com.INVALID>,
        Matei Zaharia
	<matei.zaharia@gmail.com>","There appears to be a newly added Boolean in DAGScheduler default to ""False"":

private val localExecutionEnabled = sc.getConf.getBoolean(""spark.localExecution.enabled"", false)

Then

val shouldRunLocally =
        localExecutionEnabled && allowLocal && finalStage.parents.isEmpty && partitions.length == 1


I'm wondering whether by default ""running locally"" is disabled.

Yan

From: Du Li [mailto:lidu@yahoo-inc.com.INVALID]
Sent: Tuesday, September 16, 2014 12:26 PM
To: Matei Zaharia
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: NullWritable not serializable

Hi,

The test case is separated out as follows. The call to rdd2.first() breaks when spark version is changed to 1.1.0, reporting exception NullWritable not serializable. However, the same test passed with spark 1.0.2. The pom.xml file is attached. The test data README.md was copied from spark.

Thanks,
Du
-----

package com.company.project.test

import org.scalatest._

class WritableTestSuite extends FunSuite {
  test(""generated sequence file should be readable from spark"") {
    import org.apache.hadoop.io.{NullWritable, Text}
    import org.apache.spark.{SparkContext, SparkConf}
    import org.apache.spark.SparkContext._

    val conf = new SparkConf(false).setMaster(""local"").setAppName(""test data exchange with spark"")
    val sc = new SparkContext(conf)

    val rdd = sc.textFile(""README.md"")
    val res = rdd.map(x => (NullWritable.get(), new Text(x)))
    res.saveAsSequenceFile(""./test_data"")

    val rdd2 = sc.sequenceFile(""./test_data"", classOf[NullWritable], classOf[Text])

    assert(rdd.first == rdd2.first._2.toString)
  }
}



From: Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.comDate: Monday, September 15, 2014 at 10:52 PM
To: Du Li <lidu@yahoo-inc.com<mailto:lidu@yahoo-inc.com>>
Cc: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: NullWritable not serializable

Can you post the exact code for the test that worked in 1.0? I can't think of much that could've changed. The one possibility is if  we had some operations that were computed locally on the driver (this happens with things like first() and take(), which will try to do the first partition locally). But generally speaking these operations should *not* work over a network, so you'll have to make sure that you only send serializable types through shuffles or collects, or use a serialization framework like Kryo that might be okay with Writables.

Matei


Hi Matei,

Thanks for your reply.

The Writable classes have never been serializable and this is why it is weird. I did try as you suggested to map the Writables to integers and strings. It didn't pass, either. Similar exceptions were thrown except that the messages became IntWritable, Text are not serializable. The reason is in the implicits defined in the SparkContext object that convert those values into their corresponding Writable classes before saving the data in sequence file.

My original code was actual some test cases to try out SequenceFile related APIs. The tests all passed when the spark version was specified as 1.0.2. But this one failed after I changed the spark version to 1.1.0 the new release, nothing else changed. In addition, it failed when I called rdd2.collect(), take(1), and first(). But it worked fine when calling rdd2.count(). As you can see, count() does not need to serialize and ship data while the other three methods do.

Do you recall any difference between spark 1.0 and 1.1 that might cause this problem?

Thanks,
Du


From: Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.comDate: Friday, September 12, 2014 at 9:10 PM
To: Du Li <lidu@yahoo-inc.com.invalid<mailto:lidu@yahoo-inc.com.invalid>>, ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: NullWritable not serializable


Hi Du,

I don't think NullWritable has ever been serializable, so you must be doing something differently from your previous program. In this case though, just use a map() to turn your Writables to serializable types (e.g. null and String).

Matie


Hi,

I was trying the following on spark-shell (built with apache master and hadoop 2.4.0). Both calling rdd2.collect and calling rdd3.collect threw java.io.NotSerializableException: org.apache.hadoop.io.NullWritable.

I got the same problem in similar code of my app which uses the newly released Spark 1.1.0 under hadoop 2.4.0. Previously it worked fine with spark 1.0.2 under either hadoop 2.40 and 0.23.10.

Anybody knows what caused the problem?

Thanks,
Du

----
import org.apache.hadoop.io.{NullWritable, Text}
val rdd = sc.textFile(""README.md"")
val res = rdd.map(x => (NullWritable.get(), new Text(x)))
res.saveAsSequenceFile(""./test_data"")
val rdd2 = sc.sequenceFile(""./test_data"", classOf[NullWritable], classOf[Text])
rdd2.collect
val rdd3 = sc.sequenceFile[NullWritable,Text](""./test_data"")
rdd3.collect


"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Tue, 16 Sep 2014 15:21:53 -0700",[mllib] State of Multi-Model training,dev@spark.apache.org,"I'm curious about the state of development Multi-Model learning in MLlib
(training sets of models during the same training session, rather then one
at a time). The JIRA lists it as in progress targeting Spark 1.2.0 (
https://issues.apache.org/jira/browse/SPARK-1486 ). But there hasn't been
any notes on it in over a month.
I submitted a pull request for a possible method to do this work a little
over two months ago (https://github.com/apache/spark/pull/1292), but
haven't yet received any feedback on the patch yet.
Is anybody else working on multi-model training?

Kyle
"
Burak Yavuz <byavuz@stanford.edu>,"Tue, 16 Sep 2014 16:09:27 -0700 (PDT)",Re: [mllib] State of Multi-Model training,Kyle Ellrott <kellrott@soe.ucsc.edu>,"Hi Kyle,

I'm actively working on it now. It's pretty close to completion, I'm just trying to figure out bottlenecks and optimize as much as possible.
As Phase 1, I implemented multi model training on Gradient Descent. Instead of performing Vector-Vector operations on rows (examples) and weights,
I've batched them into matrices so that we can use Level 3 BLAS to speed things up. I've also added support for Sparse Matrices (https://github.com/apache/spark/pull/2294) as making use of sparsity will allow you to train more models at once.

Best,
Burak

(training sets of models during the same training session, rather then one
at a time). The JIRA lists it as in progress targeting Spark 1.2.0 (
https://issues.apache.org/jira/browse/SPARK-1486 ). But there hasn't been
any notes on it in over a month.
I submitted a pull request for a possible method to do this work a little
over two months ago (https://github.com/apache/spark/pull/1292), but
haven't yet received any feedback on the patch yet.
Is anybody else working on multi-model training?

Kyle


---------------------------------------------------------------------


"
"""=?ISO-8859-1?B?VHJpZGVudA==?="" <cwk32@vip.qq.com>","Wed, 17 Sep 2014 07:22:40 +0800",Network Communication - Akka or more?,"""=?ISO-8859-1?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?="" <dev@spark.apache.org>","Thank you for reading this mail.

I'm trying to change the underlying network connection system of Spark to support Infiniteband. 

1. I doubt whether ConnectionManager and netty is under construction. It seems that they are not usually used.

2. How much connection payload is carried by akka?

3. When running ./bin/run-example SparkPi   I noticed that the jar file has been sent from server to client. It is scary because the jar is big. Is it common?

                                    Trident"
Patrick Wendell <pwendell@gmail.com>,"Tue, 16 Sep 2014 17:10:55 -0700",Re: Wiki page for Operations/Monitoring tools?,Otis Gospodnetic <otis.gospodnetic@gmail.com>,"Hey Otis,

Could you describe a bit more about what your program is. Is it an
open source project? A product? This would help understand a bit where
it should go.

- Patrick


---------------------------------------------------------------------


"
Otis Gospodnetic <otis.gospodnetic@gmail.com>,"Tue, 16 Sep 2014 21:37:00 -0400",Re: Wiki page for Operations/Monitoring tools?,,"Hi Patrick,

In our case we have a performance monitoring, alerting, and anomaly
monitoring to it.  My thinking was Spark, like any similar project really,
needs a page/section/something listing various operational tools or
services, both open-source and non-open-source.

Thanks,
Otis
--
Monitoring * Alerting * Anomaly Detection * Centralized Log Management
Solr & Elasticsearch Support * http://sematext.com/



"
Mohit Jaggi <mohitjaggi@gmail.com>,"Tue, 16 Sep 2014 21:16:58 -0700",greeting from new member and jira 3489,dev@spark.apache.org,"https://issues.apache.org/jira/browse/SPARK-3489

Folks,
I am Mohit Jaggi and I work for Ayasdi Inc. After experimenting with Spark
for  a while and discovering its awesomeness(!) I made an attempt to
provide a wrapper API that looks like R and/or pandas dataframe.

https://github.com/AyasdiOpenSource/df

""df"" uses a collection of RDDs, each element in the collection being a
column in a dataframe. To make rows from the columns I used zip() in a loop
but that is not very efficient. I created JIRA 3489 requesting a zip()
variant that zips a sequence of RDDs. I noticed that it was easy to write
that code so I wrote that code and it seems to work. I attached the diff to
the jira. I believe that this API would be useful in general and is not
specific to ""df"". Please take a look at the request and the proposed
solution and let me know what you think.

Cheers,
Mohit
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 16 Sep 2014 21:36:11 -0700",Re: greeting from new member and jira 3489,Mohit Jaggi <mohitjaggi@gmail.com>,"Hi Mohit,

Welcome to the Spark community! We normally look at feature proposals
using github pull requests mind submitting one? The contribution
process is covered here:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


---------------------------------------------------------------------


"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Tue, 16 Sep 2014 21:41:45 -0700",Re: [mllib] State of Multi-Model training,Burak Yavuz <byavuz@stanford.edu>,"I'd be interested in helping to test your code as soon as its available.
The version I wrote used a paired RDD and combined by key, it worked best
if it used a custom partitioner that put all the samples in the same area.
Running things in batched matrices would probably speed things up greatly.
You probably won't need my training code, but I did write some stuff
related to calculating Binary classifications metric (
https://github.com/apache/spark/pull/1292/files#diff-6) and AUC (
https://github.com/apache/spark/pull/1292/files#diff-5) for multiple models
that you might be able to use.

Kyle



"
Burak Yavuz <byavuz@stanford.edu>,"Tue, 16 Sep 2014 22:22:07 -0700 (PDT)",Re: [mllib] State of Multi-Model training,Kyle Ellrott <kellrott@soe.ucsc.edu>,"Hi Kyle,

Thank you for the code examples. We may be able to use some of the ideas there. I think initially the goal is to have the optimizers ready (SGD, LBFGS), 
and then the evaluation metrics will come next. It might take some time, however as MLlib is going to have a significant API ""face-lift"" (e.g. https://issues.apache.org/jira/browse/SPARK-3530). Evaluation metrics will be significant in the new ""pipeline""s and the ability to evaluate multiple models
efficiently is very important. We encourage you to read through the design docs, and we would appreciate any feedback from you and the rest of the community!

Best,
Burak

I'd be interested in helping to test your code as soon as its available.
The version I wrote used a paired RDD and combined by key, it worked best
if it used a custom partitioner that put all the samples in the same area.
Running things in batched matrices would probably speed things up greatly.
You probably won't need my training code, but I did write some stuff
related to calculating Binary classifications metric (
https://github.com/apache/spark/pull/1292/files#diff-6) and AUC (
https://github.com/apache/spark/pull/1292/files#diff-5) for multiple models
that you might be able to use.

Kyle





---------------------------------------------------------------------


"
Jun Feng Liu <liujunf@cn.ibm.com>,"Wed, 17 Sep 2014 13:32:22 +0800",Re: Spark authenticate enablement,Andrew Or <andrew@databricks.com>,"I see. Thank you, it works for me. It looks confusing to have two ways 
expose configuration though.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Andrew Or <andrew@databricks.com> 
2014/09/17 02:06

To
Tom Graves <tgraves_cs@yahoo.com>, 
cc
Jun Feng Liu/China/IBM@IBMCN, ""dev@spark.apache.org"" 
<dev@spark.apache.org>
Subject
Re: Spark authenticate enablement






Hi Jun,

You can still set the authentication variables through `spark-env.sh`, by
exporting SPARK_MASTER_OPTS, SPARK_WORKER_OPTS, SPARK_HISTORY_OPTS etc to
include ""-Dspark.auth.{...}"". There is an open pull request that allows
these processes to also read from spark-defaults.conf, but this is not
merged into master yet.

Andrew

2014-09-15 6:44 GMT-07:00 Tom Graves <tgraves_cs@yahoo.com.invalid>:

all
<liujunf@cn.ibm.com>
model.

"
Egor Pahomov <pahomov.egor@gmail.com>,"Wed, 17 Sep 2014 10:45:11 +0400",Workflow Scheduler for Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","There are two things we(Yandex) miss in Spark: MLlib good abstractions and
good workflow job scheduler. From threads ""Adding abstraction in MlLib"" and
""[mllib] State of Multi-Model training"" I got the idea, that databricks
working on it and we should wait until first post doc, which would lead us.
What about workflow scheduler? Is there anyone already working on it? Does
anyone have a plan on doing it?

P.S. We thought that MLlib abstractions about multiple algorithms run with
same data would need such scheduler, which would rerun algorithm in case of
failure. I understand, that spark provide fault tolerance out of the box,
but we found some ""Ooozie-like"" scheduler more reliable for such long
living workflows.

-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 16 Sep 2014 23:53:27 -0700",Re: Adding abstraction in MLlib,Reynold Xin <rxin@databricks.com>,"Hi Egor,

I posted the design doc for pipeline and parameters on the JIRA, now
I'm trying to work out some details of ML datasets, which I will post
it later this week. You feedback is welcome!

Best,
Xiangrui


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 17 Sep 2014 00:06:16 -0700",Re: Network Communication - Akka or more?,Trident <cwk32@vip.qq.com>,"I'm not familiar with Infiniband, but I can chime in on the Spark part.

There are two kinds of communications in Spark: control plane and data
plane.  Task scheduling / dispatching is control, whereas fetching a block
(e.g. shuffle) is data.




They are used for data plane communication. Broadcast, shuffle, all use
them.




Akka is mainly responsible for control, i.e. dispatching tasks, reporting a
block being put into memory to the driver etc.




How are you going to distribute the jar file if you don't send it? The
workers need to bytecode for those classes you are going to execute.
"
Reynold Xin <rxin@databricks.com>,"Wed, 17 Sep 2014 00:21:01 -0700",Re: Workflow Scheduler for Spark,Egor Pahomov <pahomov.egor@gmail.com>,"Hi Egor,

I think the design doc for the pipeline feature has been posted.

For the workflow, I believe Oozie actually works fine with Spark if you
want some external workflow system. Do you have any trouble using that?



"
Egor Pahomov <pahomov.egor@gmail.com>,"Wed, 17 Sep 2014 13:00:06 +0400",Re: Workflow Scheduler for Spark,Reynold Xin <rxin@databricks.com>,"I have problems using Oozie. For example it doesn't sustain spark context
like ooyola job server does. Other than GUI interfaces like HUE it's hard
to work with - scoozie stopped in development year ago(I spoke with
creator) and oozie xml very hard to write.
Oozie still have all documentation and code in MR model rather than in yarn
model. And based on it's current speed of development I can't expect
radical changes in nearest future. There is no ""Databricks"" for oozie,
which would have people on salary to develop this kind of radical changes.
It's dinosaur.

Reunold, can you help finding this doc? Do you mean just pipelining spark
code or additional logic of persistence tasks, job server, task retry, data
availability and extra?


2014-09-17 11:21 GMT+04:00 Reynold Xin <rxin@databricks.com>:



-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 17 Sep 2014 02:08:34 -0700",Re: Workflow Scheduler for Spark,Egor Pahomov <pahomov.egor@gmail.com>,"See https://issues.apache.org/jira/browse/SPARK-3530 and this doc,
referenced in that JIRA:

https://docs.google.com/document/d/1rVwXRjWKfIb-7PI6b86ipytwbUH7irSNLF1_6dLmh8o/edit?usp=sharing


"
Egor Pahomov <pahomov.egor@gmail.com>,"Wed, 17 Sep 2014 13:47:11 +0400",Re: Workflow Scheduler for Spark,Mark Hamstra <mark@clearstorydata.com>,"It's doc about MLLib pipeline functionality. What about oozie-like
workflow?

2014-09-17 13:08 GMT+04:00 Mark Hamstra <mark@clearstorydata.com>:



-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
"""wyphao.2007"" <wyphao.2007@163.com>","Wed, 17 Sep 2014 18:43:14 +0800 (CST)",network.ConnectionManager error,"dev@spark.apache.org, ""Reynold Xin"" <rxin@databricks.com>","Hi,  When I run spark job on yarn,and the job finished success,but I found there are some error logs in the logfile as follow(the red color text):


14/09/17 18:25:03 INFO ui.SparkUI: Stopped Spark web UI at http://sparkserver2.cn:63937
14/09/17 18:25:03 INFO scheduler.DAGScheduler: Stopping DAGScheduler
14/09/17 18:25:03 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
14/09/17 18:25:03 INFO cluster.YarnClusterSchedulerBackend: Asking each executor to shut down
14/09/17 18:25:03 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(sparkserver2.cn,9072)
14/09/17 18:25:03 INFO network.ConnectionManager: Removing ReceivingConnection to ConnectionManagerId(sparkserver2.cn,9072)
14/09/17 18:25:03 ERROR network.ConnectionManager: Corresponding SendingConnection to ConnectionManagerId(sparkserver2.cn,9072) not found
14/09/17 18:25:03 INFO network.ConnectionManager: Removing ReceivingConnection to ConnectionManagerId(sparkserver2.cn,14474)
14/09/17 18:25:03 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(sparkserver2.cn,14474)
14/09/17 18:25:03 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(sparkserver2.cn,14474)
14/09/17 18:25:04 INFO spark.MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
14/09/17 18:25:04 INFO network.ConnectionManager: Selector thread was interrupted!
14/09/17 18:25:04 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(sparkserver2.cn,9072)
14/09/17 18:25:04 INFO network.ConnectionManager: Removing SendingConnection to ConnectionManagerId(sparkserver2.cn,14474)
14/09/17 18:25:04 INFO network.ConnectionManager: Removing ReceivingConnection to ConnectionManagerId(sparkserver2.cn,9072)
14/09/17 18:25:04 ERROR network.ConnectionManager: Corresponding SendingConnection to ConnectionManagerId(sparkserver2.cn,9072) not found
14/09/17 18:25:04 INFO network.ConnectionManager: Removing ReceivingConnection to ConnectionManagerId(sparkserver2.cn,14474)
14/09/17 18:25:04 ERROR network.ConnectionManager: Corresponding SendingConnection to ConnectionManagerId(sparkserver2.cn,14474) not found
14/09/17 18:25:04 WARN network.ConnectionManager: All connections not cleaned up
14/09/17 18:25:04 INFO network.ConnectionManager: ConnectionManager stopped
14/09/17 18:25:04 INFO storage.MemoryStore: MemoryStore cleared
14/09/17 18:25:04 INFO storage.BlockManager: BlockManager stopped
14/09/17 18:25:04 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
14/09/17 18:25:04 INFO spark.SparkContext: Successfully stopped SparkContext
14/09/17 18:25:04 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
14/09/17 18:25:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
14/09/17 18:25:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
14/09/17 18:25:04 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
14/09/17 18:25:04 INFO Remoting: Remoting shut down
14/09/17 18:25:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.


What is the cause of this error? My spark version is 1.1.0 &  hadoop version is 2.2.0.
Thank you."
Christian Chua <cc8678@icloud.com>,"Wed, 17 Sep 2014 08:44:26 -0700",Re: network.ConnectionManager error,"""wyphao.2007"" <wyphao.2007@163.com>","I see the same thing. 

A workaround is to put a Thread.sleep(5000) statement before sc.stop()

Let us know how it goes. 



 there are some error logs in the logfile as follow(the red color text):
ver2.cn:63937
ll executors
ecutor to shut down
on to ConnectionManagerId(sparkserver2.cn,9072)
tion to ConnectionManagerId(sparkserver2.cn,9072)
nnection to ConnectionManagerId(sparkserver2.cn,9072) not found
tion to ConnectionManagerId(sparkserver2.cn,14474)
on to ConnectionManagerId(sparkserver2.cn,14474)
on to ConnectionManagerId(sparkserver2.cn,14474)
Actor stopped!
rrupted!
on to ConnectionManagerId(sparkserver2.cn,9072)
on to ConnectionManagerId(sparkserver2.cn,14474)
tion to ConnectionManagerId(sparkserver2.cn,9072)
nnection to ConnectionManagerId(sparkserver2.cn,9072) not found
tion to ConnectionManagerId(sparkserver2.cn,14474)
nnection to ConnectionManagerId(sparkserver2.cn,14474) not found
ned up
d
ped
xt
ster with SUCCEEDED
hutting down remote daemon.
emote daemon shut down; proceeding with flushing remote transports.
uccessfully unregistered.
emoting shut down.
on is 2.2.0.

---------------------------------------------------------------------


"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Wed, 17 Sep 2014 09:48:54 -0700",Re: [mllib] State of Multi-Model training,Burak Yavuz <byavuz@stanford.edu>,"This sounds like a pretty major re-write of the system. Is it going to live
in an different repo during development? Or will we be able to track
progress in the main Spark repo?

Kyle


"
Reynold Xin <rxin@databricks.com>,"Wed, 17 Sep 2014 09:54:01 -0700",Re: network.ConnectionManager error,"""wyphao.2007"" <wyphao.2007@163.com>","This is during shutdown right? Looks ok to me since connections are being
closed. We could've handle this more gracefully, but the logs look
harmless.


"
Reynold Xin <rxin@databricks.com>,"Wed, 17 Sep 2014 11:28:15 -0700",Re: Workflow Scheduler for Spark,Egor Pahomov <pahomov.egor@gmail.com>,"There might've been some misunderstanding. I was referring to the MLlib
pipeline design doc when I said the design doc was posted, in response to
the first paragraph of your original email.



"
Burak Yavuz <byavuz@stanford.edu>,"Wed, 17 Sep 2014 13:15:44 -0700 (PDT)",Re: [mllib] State of Multi-Model training,Kyle Ellrott <kellrott@soe.ucsc.edu>,"I believe it will be in the main repo.

Burak

This sounds like a pretty major re-write of the system. Is it going to live
in an different repo during development? Or will we be able to track
progress in the main Spark repo?

Kyle




---------------------------------------------------------------------


"
Du Li <lidu@yahoo-inc.com.INVALID>,"Wed, 17 Sep 2014 23:50:49 +0000",problem with HiveContext inside Actor,"""user@spark.apache.org"" <user@spark.apache.org>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi,

Wonder anybody had similar experience or any suggestion here.

I have an akka Actor that processes database requests in high-level messages. Inside this Actor, it creates a HiveContext object that does the actual db work. The main thread creates the needed SparkContext and passes in to the Actor to create the HiveContext.

When a message is sent to the Actor, it is processed properly except that, when the message triggers the HiveContext to create a database, it throws a NullPointerException in hive.ql.Driver.java which suggests that its conf variable is not initialized.

Ironically, it works fine if my main thread directly calls actor.hiveContext to create the database. The spark version is 1.1.0.

Thanks,
Du
"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Thu, 18 Sep 2014 09:51:53 +0800",Re: GraphX graph partitioning strategy,ankurdave@gmail.com,"Hi Ankur, all,

I've implemented few graph partitioning algorithms, and done some 
evaluation.
The goal is to lower replication factor and produce better balanced 
graph, so to make work load more balance.
Detailed description and result: 
https://issues.apache.org/jira/browse/SPARK-3523

Can you help take a look?
Thank you!

Larry



---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 18 Sep 2014 02:21:29 +0000",RE: problem with HiveContext inside Actor,"Du Li <lidu@yahoo-inc.com.INVALID>, ""user@spark.apache.org""
	<user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, Du
I am not sure what you mean ""triggers the HiveContext to create a database"", do you create the sub class of HiveContext? Just be sure you call the ""HiveContext.sessionState"" eagerly, since it will set the proper ""hiveconf"" into the SessionState, otherwise the HiveDriver will always get the null value when retrieving HiveConf.

Cheng Hao

From: Du Li [mailto:lidu@yahoo-inc.com.INVALID]
Sent: Thursday, September 18, 2014 7:51 AM
To: user@spark.apache.org; dev@spark.apache.org
Subject: problem with HiveContext inside Actor

Hi,

Wonder anybody had similar experience or any suggestion here.

I have an akka Actor that processes database requests in high-level messages. Inside this Actor, it creates a HiveContext object that does the actual db work. The main thread creates the needed SparkContext and passes in to the Actor to create the HiveContext.

When a message is sent to the Actor, it is processed properly except that, when the message triggers the HiveContext to create a database, it throws a NullPointerException in hive.ql.Driver.java which suggests that its conf variable is not initialized.

Ironically, it works fine if my main thread directly calls actor.hiveContext to create the database. The spark version is 1.1.0.

Thanks,
Du
"
Michael Armbrust <michael@databricks.com>,"Wed, 17 Sep 2014 19:40:45 -0700",Re: problem with HiveContext inside Actor,"""Cheng, Hao"" <hao.cheng@intel.com>","- dev

Is it possible that you are constructing more than one HiveContext in a
single JVM?  Due to global state in Hive code this is not allowed.

Michael


re you
l set the proper
r will always get
e
s
,
 a
"
Tom Hubregtsen <thubregtsen@gmail.com>,"Thu, 18 Sep 2014 07:57:14 -0700 (PDT)",Spark spilling location,dev@spark.incubator.apache.org,"Hi all,

Just one line of context, since last post mentioned this would help:
I'm currently writing my masters thesis (Computer Engineering) on storage
and memory in both Spark and Hadoop.

Right now I'm trying to analyze the spilling behavior of Spark, and I do not
see what I expect. Therefor, I want to be sure that I am looking at the
correct location.

If I set spark.local.dir and SPARK_LOCAL_DIRS to, for instance, ~/temp
instead of /tmp. Will this be the location where all data will be spilled
to? I assume it is, based on the description of spark.local.dir at
https://spark.apache.org/docs/latest/configuration.html:
""Directory to use for ""scratch"" space in Spark, including map output files
and RDDs that get stored on disk.""

Thanks!



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 18 Sep 2014 10:08:09 -0700",Re: Spark spilling location,Tom Hubregtsen <thubregtsen@gmail.com>,"Yes - I believe we use the local dirs for spilling as well.


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Thu, 18 Sep 2014 10:20:44 -0700",Re: Spark authenticate enablement,Jun Feng Liu <liujunf@cn.ibm.com>,"2014-09-16 22:32 GMT-07:00 Jun Feng Liu <liujunf@cn.ibm.com>:


I agree. We're working on it. :)






"
Meethu Mathew <meethu.mathew@flytxt.com>,"Fri, 19 Sep 2014 10:55:53 +0530",Gaussian Mixture Model clustering,dev@spark.apache.org,"Hi all,

We have come up with an initial distributed implementation of Gaussian 
Mixture Model in pyspark where the parameters are estimated using the 
Expectation-Maximization algorithm.Our current implementation considers 
diagonal covariance matrix for each component.
We did an initial benchmark study on a 2 node Spark standalone cluster 
setup where each node config is 8 Cores,8 GB RAM, the spark version used 
is 1.0.0. We also evaluated python version of k-means available in spark 
on the same datasets.Below are the results from this benchmark study. 
The reported stats are average from 10 runs.Tests were done on multiple 
datasets with varying number of features and instances.


          Dataset 	      Gaussian mixture model
	               Kmeans(Python)

Instances 	Dimensions 	Avg time per iteration 	Time for 100 iterations
	Avg time per iteration 	Time for 100 iterations
0.7million 	13
	7s
	12min
	  13s 	26min
1.8million 	11
	17s
	 29min 	   33s
	 53min
10 million 	16
	1.6min 	2.7hr
	  1.2min 	2 hr


We are interested in contributing this implementation as a patch to 
SPARK. Does MLLib accept python implementations? If not, can we 
contribute to the pyspark component
I have created a JIRA for the same 
https://issues.apache.org/jira/browse/SPARK-3588 .How do I get the 
ticket assigned to myself?

Please review and suggest how to take this forward.



-- 

Regards,


*Meethu Mathew*

*Engineer*

*Flytxt*

F: +91 471.2700202

www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us 
<http://www.twitter.com/flytxt> | _Connect on Linkedin 
<http://www.linkedin.com/home?trk=hb_tab_home_top>_

"
Meethu Mathew <meethu.mathew@flytxt.com>,"Fri, 19 Sep 2014 11:08:12 +0530",Re: Gaussian Mixture Model clustering,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
Please find attached the image of benchmark results. The table in the 
previous mail got messed up. Thanks.




-- 

Regards,

*Meethu Mathew*

*Engineer*

*Flytxt*

Skype: meethu.mathew7

  F: +91 471.2700202

www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us 
<http://www.twitter.com/flytxt> | _Connect on Linkedin 
<http://www.linkedin.com/home?trk=hb_tab_home_top>_


---------------------------------------------------------------------"
Debasish Das <debasish.das83@gmail.com>,"Fri, 19 Sep 2014 09:19:03 -0700","Re: I want to contribute MLlib two quality measures(ARHR and HR) for
 top N recommendation system. Is this meaningful?",Xiangrui Meng <mengxr@gmail.com>,"Hi Xiangrui,

Could you please point to some reference for calculating prec@k and ndcg@k ?

prec is precision I suppose but ndcg I have no idea about...

Thanks.
Deb



f
d
is
*.
§ *N*), then the
"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Fri, 19 Sep 2014 10:54:35 -0700",Re: Gaussian Mixture Model clustering,Meethu Mathew <meethu.mathew@flytxt.com>,"Hey Meethu - what are you setting ""K"" to in the benchmarks you show? This
can greatly affect the runtime.


"
Christoph Sawade <christoph.sawade@googlemail.com>,"Fri, 19 Sep 2014 22:16:08 +0200","Re: I want to contribute MLlib two quality measures(ARHR and HR) for
 top N recommendation system. Is this meaningful?",Debasish Das <debasish.das83@gmail.com>,"Hey Deb,

NDCG is the ""Normalized Discounted Cumulative Gain"" [1]. Another popular
measure is ""Expected Reciprocal Rank"" (ERR) [2]; it is based on a
probabilistic user model, where the user scans the presented list of search
results or recommendations and chooses the first that is sufficiently
relevant. ERR is the expectation of the reciprocal rank under this model.
Precision@k is the precision at a cut-off k. Often this is used as an
overage over the first n k's; a starting point might by [3].

Hope that helps.

Cheers, Christoph

[1] K. J√§rvelin and J. Kek√§l√§inen. Cumulated gain-based evaluation of IR
techniques. ACM Transactions on Information Systems, 20(4):422‚Äì446, 2002.
[2] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. Expected reciprocal
rank for graded rel- evance. In Proceeding of the Conference on Information
and Knowledge Management, 2009.
[3] http://en.wikipedia.org/wiki/IR_evaluation
"
Debasish Das <debasish.das83@gmail.com>,"Fri, 19 Sep 2014 13:29:35 -0700","Re: I want to contribute MLlib two quality measures(ARHR and HR) for
 top N recommendation system. Is this meaningful?",Christoph Sawade <christoph.sawade@googlemail.com>,"Thanks Christoph.

Are these numbers for mllib als implicit and explicit feedback on
movielens/netflix datasets documented on JIRA ?

ch
aluation of IR
6, 2002.
"
Cody Koeninger <cody@koeninger.org>,"Sat, 20 Sep 2014 00:30:14 -0500",guava version conflicts,"""dev@spark.apache.org"" <dev@spark.apache.org>","After the recent spark project changes to guava shading, I'm seeing issues
with the datastax spark cassandra connector (which depends on guava 15.0)
and the datastax cql driver (which depends on guava 16.0.1)

Building an assembly for a job (with spark marked as provided) that
includes either guava 15.0 or 16.0.1, results in errors like the following:

scala> session.close

scala> s[14/09/20 04:56:35 ERROR Futures$CombinedFuture: input future
failed.
java.lang.IllegalAccessError: tried to access class
org.spark-project.guava.common.base.Absent from class
com.google.common.base.Optional
        at com.google.common.base.Optional.absent(Optional.java:79)
        at com.google.common.base.Optional.fromNullable(Optional.java:94)
        at
        at
com.google.common.util.concurrent.Futures$CombinedFuture.access$400(Futures.java:1470)
        at
com.google.common.util.concurrent.Futures$CombinedFuture$2.run(Futures.java:1548)
        at
com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
        at
com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
        at
com.google.common.util.concurrent.ExecutionList.add(ExecutionList.java:101)
        at
com.google.common.util.concurrent.AbstractFuture.addListener(AbstractFuture.java:170)
        at
com.google.common.util.concurrent.Futures$CombinedFuture.init(Futures.java:1545)
        at
com.google.common.util.concurrent.Futures$CombinedFuture.<init>(Futures.java:1491)
        at
com.google.common.util.concurrent.Futures.listFuture(Futures.java:1640)
        at
com.google.common.util.concurrent.Futures.allAsList(Futures.java:983)
        at
com.datastax.driver.core.CloseFuture$Forwarding.<init>(CloseFuture.java:73)
        at
com.datastax.driver.core.HostConnectionPool.closeAsync(HostConnectionPool.java:398)
        at
com.datastax.driver.core.SessionManager.closeAsync(SessionManager.java:157)
        at
com.datastax.driver.core.SessionManager.close(SessionManager.java:172)
        at
com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$destroySession(CassandraConnector.scala:180)
        at
com.datastax.spark.connector.cql.CassandraConnector$$anonfun$5.apply(CassandraConnector.scala:151)
        at
com.datastax.spark.connector.cql.CassandraConnector$$anonfun$5.apply(CassandraConnector.scala:151)
        at com.datastax.spark.connector.cql.RefCountedCache.com
$datastax$spark$connector$cql$RefCountedCache$$releaseImmediately(RefCountedCache.scala:86)
        at
com.datastax.spark.connector.cql.RefCountedCache$ReleaseTask.run(RefCountedCache.scala:26)
        at
com.datastax.spark.connector.cql.RefCountedCache$$anonfun$com$datastax$spark$connector$cql$RefCountedCache$$processPendingReleases$2.apply(RefCountedCache.scala:150)
        at
com.datastax.spark.connector.cql.RefCountedCache$$anonfun$com$datastax$spark$connector$cql$RefCountedCache$$processPendingReleases$2.apply(RefCountedCache.scala:147)
        at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at
scala.collection.concurrent.TrieMapIterator.foreach(TrieMap.scala:922)
        at
scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.concurrent.TrieMap.foreach(TrieMap.scala:632)
        at
scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at com.datastax.spark.connector.cql.RefCountedCache.com
$datastax$spark$connector$cql$RefCountedCache$$processPendingReleases(RefCountedCache.scala:147)
        at
com.datastax.spark.connector.cql.RefCountedCache$$anon$1.run(RefCountedCache.scala:157)
        at
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at
java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
"
tian zhang <tzhang101@yahoo.com.INVALID>,"Fri, 19 Sep 2014 22:38:44 -0700",spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2,"""dev@spark.apache.org"" <dev@spark.apache.org>","

Hi, Spark experts,

I have the following issue when using aws java sdk in my spark application. Here I narrowed down the following steps to reproduce the problem

1) I have Spark 1.1.0 with hadoop 2.4 installed on 3 nodes cluster
2) from the master node, I did the following steps.
spark-shell --jars  ws-java-sdk-1.7.2.jar 
import com.amazonaws.{Protocol, ClientConfiguration}
import com.amazonaws.auth.BasicAWSCredentials
import com.amazonaws.services.s3.AmazonS3Client
val clientConfiguration = new ClientConfiguration()
val s3accessKey=""X""
val s3secretKey=""Y""
val credentials = new BasicAWSCredentials(s3accessKey,s3secretKey)
println(""CLASSPATH=""+System.getenv(""CLASSPATH""))
CLASSPATH=::/home/hadoop/spark/conf:/home/hadoop/spark/lib/spark-assembly-1.1.0-hadoop2.4.0.jar:/home/hadoop/conf:/home/hadoop/conf
println(""java.class.path=""+System.getProperty(""java.class.path""))
java.class.path=::/home/hadoop/spark/conf:/home/hadoop/spark/lib/spark-assembly-1.1.0-hadoop2.4.0.jar:/home/hadoop/conf:/home/hadoop/conf

So far all look good and normal. But then the following step will fail and it looks like the class loader can't resolve to the right class. Any suggestion
for Spark application that requires aws sdk?

scala> val s3Client = new AmazonS3Client(credentials, clientConfiguration)
java.lang.NoClassDefFoundError: org/apache/http/impl/conn/PoolingClientConnectionManager
at com.amazonaws.http.ConnectionManagerFactory.createPoolingClientConnManager(ConnectionManagerFactory.java:26)
at com.amazonaws.http.HttpClientFactory.createHttpClient(HttpClientFactory.java:96)
at com.amazonaws.http.AmazonHttpClient.<init>(AmazonHttpClient.java:155)
at com.amazonaws.AmazonWebServiceClient.<init>(AmazonWebServiceClient.java:119)
at com.amazonaws.AmazonWebServiceClient.<init>(AmazonWebServiceClient.java:103)
at com.amazonaws.services.s3.AmazonS3Client.<init>(AmazonS3Client.java:334)
at $iwC$$iwC$$iwC$$iwC.<init>(<console>:21)
at $iwC$$iwC$$iwC.<init>(<console>:26)
at $iwC$$iwC.<init>(<console>:28)
at $iwC.<init>(<console>:30)
at <init>(<console>:32)
at .<init>(<console>:36)
at .<clinit>(<console>)
at .<init>(<console>:7)
at .<clinit>(<console>)
at $print(<console>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789)
at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062)
at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)
at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:859)
at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:771)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:616)
at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:624)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:629)
at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:954)
at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:902)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:997)
at org.apache.spark.repl.Main$.main(Main.scala:31)
at org.apache.spark.repl.Main.main(Main.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:328)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.conn.PoolingClientConnectionManager
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
... 46 more

Thanks.

Tian"
Aniket <aniket.bhatnagar@gmail.com>,"Fri, 19 Sep 2014 23:17:18 -0700 (PDT)",Re: spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2,dev@spark.incubator.apache.org,"Looks like the same issue as
http://mail-archives.apache.org/mod_mbox/spark-dev/201409.mbox/%3CCAJOb8btdXks-7-spJJ5jMNw0XsnrjwDpCQqtjht1hUn6j4zb_g@mail.gmail.com%3E





--"
Reynold Xin <rxin@databricks.com>,"Sat, 20 Sep 2014 01:24:18 -0700",Re: Eliminate copy while sending data : any Akka experts here ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","BTW - a partial solution here: https://github.com/apache/spark/pull/2470

This doesn't address the 0 size block problem yet, but makes my large job
on hundreds of terabytes of data much more reliable.



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 20 Sep 2014 11:50:26 -0400",A couple questions about shared variables,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

A couple questions came up about shared variables recently, and I wanted to
confirm my understanding and update the doc to be a little more clear.

*Broadcast variables*
Now that tasks data is automatically broadcast, the only occasions where it
makes sense to explicitly broadcast are:
* You want to use a variable from tasks in multiple stages.
* You want to have the variable stored on the executors in deserialized
form.
* You want tasks to be able to modify the variable and have those
modifications take effect for other tasks running on the same executor
(usually a very bad idea).

Is that right?

*Accumulators*
Values are only counted for successful tasks.  Is that right?  KMeans seems
to use it in this way.  What happens if a node goes away and successful
tasks need to be resubmitted?  Or the stage runs again because a different
job needed it.

thanks,
Sandy
"
Seraph <lsyurd@gmail.com>,"Sat, 20 Sep 2014 15:05:02 -0700 (PDT)","Re: A Comparison of Platforms for Implementing and Running Very
 Large Scale Machine Learning Algorithms",dev@spark.incubator.apache.org,"I‚Äôm also one of the authors of this paper and I am responsible for the Spark
experiments in this paper. Thank you for your guys discussion!

(1)

Ignacio Zendejas wrote
 
 
o 
 


Davies Liu wrote

""scala & python"", or ""spark & pyspark"". Simple examples may not be enough
for us to draw a conclusion on such comparison. We may need more complex
models for testing to obtain more comprehensive ideas. It is possible that
spark is fast in some applications, but it is slower than pyspark in others.
So the speed issue should be application specific. It is also one of the
purpose for our paper: shed some light such benchmarks for
platform/performance comparison.

(2)

Matei Zaharia wrote
s
.

We have tried our best to write several implementation methods for each
model, in order to pick up the optimal one. Some functions may seem
promising, but they fail when we did our experiments. Broadcast() is a good
idea, and we can try it for our models to see if it can bring much
difference. But as cjermaine said, ""broadcast models"" should not be a
bottleneck because the models are small int all experiments. Also, we may
change the parameters of models in each iteration, so the ""one time
broadcast"" may not provide so much help as expected. Moreover, we are really
careful when we use collect() or collectAsMap(). In our experiments, we do
not collect large sets of data by collectAsMap(), and it does not consume
much time either. 

Overall, I have no doubt that Spark developers can write more efficient code
for our models, and it is very welcome that some Spark developers can
provide better implementations for our experiments.

Thanks!




--
3.nabble.com/A-Comparison-of-Platforms-for-Implementing-and-Running-Very-Large-Scale-Machine-Learning-Algorithms-tp7823p8485.html
om.

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Sat, 20 Sep 2014 18:38:29 -0700",Re: guava version conflicts,Cody Koeninger <cody@koeninger.org>,"Hmm, looks like the hack to maintain backwards compatibility in the
Java API didn't work that well. I'll take a closer look at this when I
get to work on Monday.




-- 
Marcelo

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 20 Sep 2014 22:10:05 -0700",Re: A couple questions about shared variables,"Sandy Ryza <sandy.ryza@cloudera.com>, 
 ""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","Hey Sandy,


Hey All,¬†

A couple questions came up about shared variables recently, and I wanted to¬†
confirm my understanding and update the doc to be a little more clear.¬†

*Broadcast variables*¬†
Now that tasks data is automatically broadcast, the only occasions where it¬†
makes sense to explicitly broadcast are:¬†
* You want to use a variable from tasks in multiple stages.¬†
* You want to have the variable stored on the executors in deserialized¬†
form.¬†
* You want tasks to be able to modify the variable and have those¬†
modifications take effect for other tasks running on the same executor¬†
(usually a very bad idea).¬†

Is that right?¬†
Yeah, pretty much. Reason 1 above is probably the biggest, but 2 also matters. (We might later factor tasks in a different way to avoid 2, but it's hard due to things like Hadoop JobConf objects in the tasks).


*Accumulators*¬†
Values are only counted for successful tasks. Is that right? KMeans seems¬†
to use it in this way. What happens if a node goes away and successful¬†
tasks need to be resubmitted? Or the stage runs again because a different¬†
job needed it.¬†
Accumulators are guaranteed to give a deterministic result if you only increment them in actions. For each result stage, the accumulator's update from each task is only applied once, even if that task runs multiple times. If you use accumulators in transformations (i.e. in a stage that may be part of multiple jobs), then you may see multiple updates, from each run. This is kind of confusing but it was useful for people who wanted to use these for debugging.

Matei





thanks,¬†
Sandy¬†
"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 21 Sep 2014 16:25:33 -0400",Re: A couple questions about shared variables,Matei Zaharia <matei.zaharia@gmail.com>,"Hi, Matei,   

Can you give some hint on how the current implementation guarantee the accumulator is only applied for once?

There is a pending PR trying to achieving this (https://github.com/apache/spark/pull/228/files), but from the current implementation, I didn‚Äôt see this has been done? (maybe I missed something)

Best,  

--  
Nan Zhu



d to  
 
e it  
  
 
atters. (We might later factor tasks in a different way to avoid 2, but it's hard due to things like Hadoop JobConf objects in the tasks).
eems  
 
rent  
increment them in actions. For each result stage, the accumulator's update from each task is only applied once, even if that task runs multiple times. If you use accumulators in transformations (i.e. in a stage that may be part of multiple jobs), then you may see multiple updates, from each run. This is kind of confusing but it was useful for people who wanted to use these for debugging.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 21 Sep 2014 15:35:42 -0700",Re: A couple questions about shared variables,Nan Zhu <zhunanmcgill@gmail.com>,"Hmm, good point, this seems to have been broken by refactorings of the scheduler, but it worked in the past. Basically the solution is simple -- in a result stage, we should not apply the update for each task ID more than once -- the same way we don't call job.listener.taskSucceeded more than once. Your PR also tried to avoid this for resubmitted shuffle stages, but I don't think we need to do that necessarily (though we could).

Matei

rote:

Hi, Matei,¬†

Can you give some hint on how the current implementation guarantee the accumulator is only¬†applied for once?

There is a pending PR trying to achieving this (https://github.com/apache/spark/pull/228/files), but from the current implementation, I didn‚Äôt see this has been done? (maybe I missed something)

Best,

--¬†
Nan Zhu

Hey Sandy,


Hey All,¬†

A couple questions came up about shared variables recently, and I wanted to¬†
confirm my understanding and update the doc to be a little more clear.¬†

*Broadcast variables*¬†
Now that tasks data is automatically broadcast, the only occasions where it¬†
makes sense to explicitly broadcast are:¬†
* You want to use a variable from tasks in multiple stages.¬†
* You want to have the variable stored on the executors in deserialized¬†
form.¬†
* You want tasks to be able to modify the variable and have those¬†
modifications take effect for other tasks running on the same executor¬†
(usually a very bad idea).¬†

Is that right?¬†
Yeah, pretty much. Reason 1 above is probably the biggest, but 2 also matters. (We might later factor tasks in a different way to avoid 2, but it's hard due to things like Hadoop JobConf objects in the tasks).


*Accumulators*¬†
Values are only counted for successful tasks. Is that right? KMeans seems¬†
to use it in this way. What happens if a node goes away and successful¬†
tasks need to be resubmitted? Or the stage runs again because a different¬†
job needed it.¬†
Accumulators are guaranteed to give a deterministic result if you only increment them in actions. For each result stage, the accumulator's update from each task is only applied once, even if that task runs multiple times. If you use accumulators in transformations (i.e. in a stage that may be part of multiple jobs), then you may see multiple updates, from each run. This is kind of confusing but it was useful for people who wanted to use these for debugging.

Matei





thanks,¬†
Sandy¬†

"
Nishkam Ravi <nravi@cloudera.com>,"Sun, 21 Sep 2014 20:41:06 -0700",BlockManager issues,dev <dev@spark.apache.org>,"Recently upgraded to 1.1.0. Saw a bunch of fetch failures for one of the
workloads. Tried tracing the problem through change set analysis. Looks
like the offending commit is 4fde28c from Aug 4th for PR1707. Please see
SPARK-3633 for more details.

Thanks,
Nishkam
"
Reynold Xin <rxin@databricks.com>,"Sun, 21 Sep 2014 20:45:41 -0700",Re: BlockManager issues,Nishkam Ravi <nravi@cloudera.com>,"It seems like you just need to raise the ulimit?



"
Meethu Mathew <meethu.mathew@flytxt.com>,"Mon, 22 Sep 2014 10:39:05 +0530",Re: Gaussian Mixture Model clustering,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Hi Evan,
Sorry that I forgot to mention about it. I set the value of K as 10 for 
the benchmark study.


-- 

Regards,

*Meethu Mathew*

*Engineer*

*Flytxt*

www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us 
<http://www.twitter.com/flytxt> | _Connect on Linkedin 
<http://www.linkedin.com/home?trk=hb_tab_home_top>_

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 21 Sep 2014 22:29:38 -0700",Re: BlockManager issues,Reynold Xin <rxin@databricks.com>,"Hey the numbers you mentioned don't quite line up - did you mean PR 2711?


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 21 Sep 2014 22:32:25 -0700",Re: BlockManager issues,Reynold Xin <rxin@databricks.com>,"Ah I see it was SPARK-2711 (and PR1707). In that case, it's possible
that you are just having more spilling as a result of the patch and so
the filesystem is opening more files. I would try increasing the
ulimit.

How much memory do your executors have?

- Patrick


---------------------------------------------------------------------


"
Nishkam Ravi <nravi@cloudera.com>,"Sun, 21 Sep 2014 22:56:47 -0700",Re: BlockManager issues,Patrick Wendell <pwendell@gmail.com>,"Thanks for the quick follow up Reynold and Patrick. Tried a run with
significantly higher ulimit, doesn't seem to help. The executors have 35GB
each. Btw, with a recent version of the branch, the error message is ""fetch
failures"" as opposed to ""too many open files"". Not sure if they are
related.  Please note that the workload runs fine with head set to 066765d.
In case you want to reproduce the problem: I'm running slightly modified
ScalaPageRank (with KryoSerializer and persistence level
memory_and_disk_ser) on a 30GB input dataset and a 6-node cluster.

Thanks,
Nishkam


"
Hortonworks <zzhang@hortonworks.com>,"Mon, 22 Sep 2014 00:24:19 -0700",Re: BlockManager issues,Nishkam Ravi <nravi@cloudera.com>,"Actually I met similar issue when doing groupByKey and then count if the shuffle size is big e.g. 1tb.

Thanks.

Zhan Zhang

Sent from my iPhone


-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
=?UTF-8?B?7J207J246recKGluUSk=?= <gofiri@gmail.com>,"Mon, 22 Sep 2014 18:30:28 +0900",Re: Dependency hell in Spark applications,Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Hello,

In my case, I manually deleted org/apache/http directory in the
spark-assembly jar file..
I think if we use the latest version of httpclient (httpcore) library, we
can resolve the problem.
How about upgrading httpclient? (or jets3t?)

2014-09-11 19:09 GMT+09:00 Aniket Bhatnagar <aniket.bhatnagar@gmail.com>:

"
Christoph Sawade <christoph.sawade@googlemail.com>,"Mon, 22 Sep 2014 12:02:31 +0200",Re: BlockManager issues,Hortonworks <zzhang@hortonworks.com>,"Hey all. We had also the same problem described by Nishkam almost in the
same big data setting. We fixed the fetch failure by increasing the timeout
for acks in the driver:

set(""spark.core.connection.ack.wait.timeout"", ""600"") // 10 minutes timeout
for acks between nodes

Cheers, Christoph

2014-09-22 9:24 GMT+02:00 Hortonworks <zzhang@hortonworks.com>:

"
David Rowe <davidrowe@gmail.com>,"Mon, 22 Sep 2014 20:36:31 +1000",Re: BlockManager issues,Christoph Sawade <christoph.sawade@googlemail.com>,"I've run into this with large shuffles - I assumed that there was
contention between the shuffle output files and the JVM for memory.
Whenever we start getting these fetch failures, it corresponds with high
load on the machines the blocks are being fetched from, and in some cases
complete unresponsiveness (no ssh etc). Setting the timeout higher, or the
JVM heap lower (as a percentage of total machine memory) seemed to help..




"
"""Haopu Wang"" <HWang@qilinsoft.com>","Mon, 22 Sep 2014 18:40:29 +0800",FW: Spark SQL 1.1.0: NPE when join two cached table,<dev@spark.apache.org>,"FWD to dev mail list for helps

 

________________________________

From: Haopu Wang 
Sent: 2014ƒÍ9‘¬22»’ 16:35
To: user@spark.apache.org
Subject: Spark SQL 1.1.0: NPE when join two cached table

 

I have two data sets and want to join them on each first field. Sample data are below:

 

data set 1:

  id2,name1,2,300.0

 

data set 2:

  id1,aaaaaaaaaaaa

 

The code is something like below:

 

    val sparkConf = new SparkConf().setAppName(""JoinInScala"")

    val sc = new SparkContext(sparkConf)

    val sqlContext = new SQLContext(sc)

    sqlContext.setConf(""spark.sql.inMemoryColumnarStorage.compressed"", ""true"")

    import org.apache.spark.sql._   

    

    val testdata = sc.textFile(args(0) + ""data.txt"").map(_.split("",""))

      .map(p => Row(p(0), p(1).trim, p(2).trim.toLong, p(3).trim.toDouble))

      

    val fields = new Array[StructField](4)

    fields(0) = StructField(""id"", StringType, false);

    fields(1) = StructField(""name"", StringType, false);

    fields(2) = StructField(""agg1"", LongType, false);

    fields(3) = StructField(""agg2"", DoubleType, false);    

    val schema = StructType(fields);

 

    val data = sqlContext.applySchema(testdata, schema)

    

    data.registerTempTable(""datatable"")

    sqlContext.cacheTable(""datatable"")

 

    val refdata = sc.textFile(args(0) + ""ref.txt"").map(_.split("",""))

      .map(p => Row(p(0), p(1).trim))

      

    val reffields = new Array[StructField](2)

    reffields(0) = StructField(""id"", StringType, false);

    reffields(1) = StructField(""data"", StringType, true);

    val refschema = StructType(reffields);

 

    val refschemardd = sqlContext.applySchema(refdata, refschema)

    refschemardd.registerTempTable(""ref"")

    sqlContext.cacheTable(""ref"")

    

   val results = sqlContext.sql(""SELECT d.id,d.name,d.agg1,d.agg2,ref.data FROM datatable as d join ref on d.id=ref.id"")

    results.foreach(T => Unit);

 

But I got below NullPointerException. If I comment out the two ""cacheTable()"" calls, the program run well. Please shed some lights, thank you!

 

Exception in thread ""main"" java.lang.NullPointerException

        at org.apache.spark.sql.columnar.InMemoryRelation.statistics$lzycompute(InMemoryColumnarTableScan.scala:43)

        at org.apache.spark.sql.columnar.InMemoryRelation.statistics(InMemoryColumnarTableScan.scala:42)

        at org.apache.spark.sql.execution.SparkStrategies$HashJoin$.apply(SparkStrategies.scala:83)

        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)

        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)

        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)

        at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)

        at org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)

        at org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:268)

        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)

        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)

        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)

        at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)

        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:402)

        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:400)

        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:406)

        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:406)

        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:409)

        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:409)

        at org.apache.spark.sql.SchemaRDD.getDependencies(SchemaRDD.scala:120)

        at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:191)

        at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:189)

        at scala.Option.getOrElse(Option.scala:120)

        at org.apache.spark.rdd.RDD.dependencies(RDD.scala:189)

        at org.apache.spark.rdd.RDD.firstParent(RDD.scala:1233)

        at org.apache.spark.sql.SchemaRDD.getPartitions(SchemaRDD.scala:117)

        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)

        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)

        at scala.Option.getOrElse(Option.scala:120)

        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)

        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1135)

        at org.apache.spark.rdd.RDD.foreach(RDD.scala:759)

        at Join$$anonfun$main$1.apply$mcVI$sp(Join.scala:44)

        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)

        at Join$.main(Join.scala:42)

        at Join.main(Join.scala)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:606)

        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:328)

        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)

        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

 

 

 

 

 

"
Andrew Ash <andrew@andrewash.com>,"Mon, 22 Sep 2014 04:07:46 -0700",Re: BlockManager issues,David Rowe <davidrowe@gmail.com>,"Another data point on the 1.1.0 FetchFailures:

Running this SQL command works on 1.0.2 but fails on 1.1.0 due to the
exceptions mentioned earlier in this thread: ""SELECT stringCol,
SUM(doubleCol) FROM parquetTable GROUP BY stringCol""

The FetchFailure exception has the remote block manager that failed to
produce the shuffle.  I enabled GC logging and repeated, and the
CoarseGrainedExecutorBackend JVM is just pounding in full GCs:

943.047: [Full GC [PSYoungGen: 5708288K->5536188K(8105472K)] [ParOldGen:
20971043K->20971202K(20971520K)] 26679331K->26507390K(29076992K)
[PSPermGen: 52897K->52897K(57344K)], 48.4514680 secs] [Times: user=602.38
sys=4.43, real=48.44 secs]
991.591: [Full GC [PSYoungGen: 5708288K->5591884K(8105472K)] [ParOldGen:
20971202K->20971044K(20971520K)] 26679490K->26562928K(29076992K)
[PSPermGen: 52897K->52897K(56832K)], 51.8109380 secs] [Times: user=645.44
sys=5.03, real=51.81 secs]
1043.431: [Full GC [PSYoungGen: 5708288K->5606238K(8105472K)] [ParOldGen:
20971044K->20971100K(20971520K)] 26679332K->26577339K(29076992K)
[PSPermGen: 52908K->52908K(56320K)], 85.9367800 secs] [Times: user=1074.29
sys=9.49, real=85.92 secs]
1129.419: [Full GC [PSYoungGen: 5708288K->5634246K(8105472K)] [ParOldGen:
20971100K->20971471K(20971520K)] 26679388K->26605717K(29076992K)
[PSPermGen: 52912K->52912K(55808K)], 52.2114100 secs] [Times: user=652.29
sys=4.94, real=52.21 secs]
1181.671: [Full GC [PSYoungGen: 5708288K->5656389K(8105472K)] [ParOldGen:
20971471K->20971125K(20971520K)] 26679759K->26627514K(29076992K)
[PSPermGen: 52961K->52961K(55296K)], 65.3284620 secs] [Times: user=818.58
sys=6.71, real=65.31 secs]
1247.034: [Full GC [PSYoungGen: 5708288K->5672356K(8105472K)] [ParOldGen:
20971125K->20971417K(20971520K)] 26679413K->26643774K(29076992K)
[PSPermGen: 52982K->52982K(54784K)], 91.2656940 secs] [Times: user=1146.94
sys=9.83, real=91.25 secs]
1338.318: [Full GC [PSYoungGen: 5708288K->5683177K(8105472K)] [ParOldGen:
20971417K->20971364K(20971520K)] 26679705K->26654541K(29076992K)
[PSPermGen: 52982K->52982K(54784K)], 68.9840690 secs] [Times: user=866.72
sys=7.31, real=68.97 secs]
1407.319: [Full GC [PSYoungGen: 5708288K->5691352K(8105472K)] [ParOldGen:
20971364K->20971041K(20971520K)] 26679652K->26662394K(29076992K)
[PSPermGen: 52985K->52985K(54272K)], 58.2522860 secs] [Times: user=724.33
sys=5.74, real=58.24 secs]
1465.572: [Full GC [PSYoungGen: 5708288K->5691382K(8105472K)] [ParOldGen:
20971041K->20971041K(20971520K)] 26679329K->26662424K(29076992K)
[PSPermGen: 52986K->52986K(54272K)], 17.8034740 secs] [Times: user=221.43
sys=0.72, real=17.80 secs]
1483.377: [Full GC [PSYoungGen: 5708288K->5691383K(8105472K)] [ParOldGen:
20971041K->20971041K(20971520K)] 26679329K->26662424K(29076992K)
[PSPermGen: 52987K->52987K(54272K)], 64.3194300 secs] [Times: user=800.32
sys=6.65, real=64.31 secs]
1547.700: [Full GC [PSYoungGen: 5708288K->5692228K(8105472K)] [ParOldGen:
20971041K->20971029K(20971520K)] 26679329K->26663257K(29076992K)
[PSPermGen: 52991K->52991K(53760K)], 54.8107170 secs] [Times: user=681.07
sys=5.41, real=54.80 secs]
1602.519: [Full GC [PSYoungGen: 5708288K->5695801K(8105472K)] [ParOldGen:
20971029K->20971401K(20971520K)] 26679317K->26667203K(29076992K)
[PSPermGen: 52993K->52993K(53760K)], 71.7970690 secs] [Times: user=896.22
sys=7.61, real=71.79 secs]



I repeated the job, this time taking jmap -histos as it went along.  The
last histo I was able to get before the JVM locked up (getting a histo on a
JVM in GC storms is very difficult) is here:

 num     #instances         #bytes  class name
----------------------------------------------
   1:      31437598     2779681704  [B
   2:      62794123     1507058952  scala.collection.immutable.$colon$colon
   3:      31387645     1506606960
 org.apache.spark.sql.catalyst.expressions.Cast
   4:      31387645     1506606960
 org.apache.spark.sql.catalyst.expressions.SumFunction
   5:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.Literal
   6:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.Coalesce
   7:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.MutableLiteral
   8:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.Add
   9:      31391224     1004519168  java.util.HashMap$Entry
  10:      31402978      756090664  [Ljava.lang.Object;
  11:      31395785      753498840  java.lang.Double
  12:      31387645      753303480
 [Lorg.apache.spark.sql.catalyst.expressions.AggregateFunction;
  13:      31395808      502332928
 org.apache.spark.sql.catalyst.expressions.GenericRow
  14:      31387645      502202320
 org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToDouble$5
  15:           772      234947960  [Ljava.util.HashMap$Entry;
  16:           711      106309792  [I
  17:        106747       13673936  <methodKlass>
  18:        106747       12942856  <constMethodKlass>
  19:          8186        9037880  <constantPoolKlass>
  20:          8186        8085000  <instanceKlassKlass>
  21:        222494        5339856  scala.Tuple2
  22:        213731        5129544  java.lang.Long
  23:          6609        4754144  <constantPoolCacheKlass>
  24:         36254        3348992  [C
  25:            73        2393232
 [Lscala.concurrent.forkjoin.ForkJoinTask;
  26:         88787        2130888  org.apache.spark.storage.ShuffleBlockId
  27:          4826        1812216  <methodDataKlass>
  28:          8688        1020352  java.lang.Class
  29:         15957         869016  [[I


For me at least a symptom is large GC storms on the executors.  Is anyone
observing these FetchFailures on a consistent basis that doesn't also see
heavy GC?

Hope this helps with debugging.

Andrew



"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Mon, 22 Sep 2014 16:45:08 +0530",Re: Dependency hell in Spark applications,=?UTF-8?B?7J207J246recKGluUSk=?= <gofiri@gmail.com>,"I have submitted a defect in JIRA for this:
https://issues.apache.org/jira/browse/SPARK-3638 and have submitted a PR (
https://github.com/apache/spark/pull/2489) that temporarily fixes the
issue. Users would have to build spark with kinesis-asl to get the
compatible httpclient added to spark assembly jar.


f
.2
e
k
g
.4
n
g
ing-aws-s3-client-on-spark-while-javap-shows-otherwi/25488955#25488955
n
ge
ache/http/conn/scheme/SchemeRegistry;Lorg/apache/http/conn/DnsResolver;)V
nOperator(PoolingClientConnectionManager.java:140)
lientConnectionManager.java:114)
lientConnectionManager.java:99)
er(ConnectionManagerFactory.java:29)
java:97)
119)
103)
lient.java:136)
lient.java:117)
esisAsyncClient.java:132)
d
d
he
m
s
"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 22 Sep 2014 07:55:31 -0400",Re: A couple questions about shared variables,Matei Zaharia <matei.zaharia@gmail.com>,"If you think it as necessary to fix, I would like to resubmit that PR (seems to have some conflicts with the current DAGScheduler)  

My suggestion is to make it as an option in accumulator, e.g. some algorithms utilizing accumulator for result calculation, it needs a deterministic accumulator, while others implementing something like Hadoop counters may need the current implementation (count everything happened, including the duplications)

Your thoughts?  

--  
Nan Zhu



scheduler, but it worked in the past. Basically the solution is simple -- in a result stage, we should not apply the update for each task ID more than once -- the same way we don't call job.listener.taskSucceeded more than once. Your PR also tried to avoid this for resubmitted shuffle stages, but I don't think we need to do that necessarily (though we could).
e accumulator is only applied for once?  
ache/spark/pull/228/files), but from the current implementation, I didn‚Äôt see this has been done? (maybe I missed something)  
anted to   
ar.  
where it  
ized  
tor  
so matters. (We might later factor tasks in a different way to avoid 2, but it's hard due to things like Hadoop JobConf objects in the tasks).
ns seems  
ful  
ifferent  
nly increment them in actions. For each result stage, the accumulator's update from each task is only applied once, even if that task runs multiple times. If you use accumulators in transformations (i.e. in a stage that may be part of multiple jobs), then you may see multiple updates, from each run. This is kind of confusing but it was useful for people who wanted to use these for debugging.

"
Cody Koeninger <cody@koeninger.org>,"Mon, 22 Sep 2014 09:16:06 -0500",hash vs sort shuffle,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just as a heads up, we deployed 471e6a3a of master (in order to get some
sql fixes), and were seeing jobs fail until we set

spark.shuffle.manager=HASH

I'd be reluctant to change the default to sort for the 1.1.1 release
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 22 Sep 2014 08:54:01 -0700",Re: hash vs sort shuffle,Cody Koeninger <cody@koeninger.org>,"Thanks for the heads up Cody.  Any indication of what was going wrong?


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 22 Sep 2014 09:08:03 -0700",Re: A couple questions about shared variables,Nan Zhu <zhunanmcgill@gmail.com>,"MapReduce counters do not count duplications.  In MapReduce, if a task
needs to be re-run, the value of the counter from the second task
overwrites the value from the first task.

-Sandy


p
s,
something)
e
s.
"
Cody Koeninger <cody@koeninger.org>,"Mon, 22 Sep 2014 11:08:13 -0500",Re: hash vs sort shuffle,Sandy Ryza <sandy.ryza@cloudera.com>,"Unfortunately we were somewhat rushed to get things working again and did
not keep the exact stacktraces, but one of the issues we saw was similar to
that reported in

https://issues.apache.org/jira/browse/SPARK-3032

We also saw FAILED_TO_UNCOMPRESS errors from snappy when reading the
shuffle file.




"
Gary Malouf <malouf.gary@gmail.com>,"Mon, 22 Sep 2014 12:15:32 -0400",Re: guava version conflicts,Marcelo Vanzin <vanzin@cloudera.com>,"Hi Marcelo,

Interested to hear the approach to be taken.  Shading guava itself seems
extreme, but that might make sense.

Gary


"
Ye Xianjin <advancedxy@gmail.com>,"Tue, 23 Sep 2014 00:56:43 +0800",spark_classpath in core/pom.xml and yarn/porm.xml,"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","Hi:
    I notice the scalatest-maven-plugin set SPARK_CLASSPATH environment variable for testing. But in the SparkConf.scala, this is deprecated in Spark 1.0+.
    So what this variable for? should we just remove this variable?
    

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)

"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 22 Sep 2014 13:36:05 -0400",Re: A couple questions about shared variables,Sandy Ryza <sandy.ryza@cloudera.com>,"I see, thanks for pointing this out  


--  
Nan Zhu



needs to be re-run, the value of the counter from the second task overwrites the value from the first task.
 (seems to have some conflicts with the current DAGScheduler)  
gorithms utilizing accumulator for result calculation, it needs a deterministic accumulator, while others implementing something like Hadoop counters may need the current implementation (count everything happened, including the duplications)
the scheduler, but it worked in the past. Basically the solution is simple -- in a result stage, we should not apply the update for each task ID more than once -- the same way we don't call job.listener.taskSucceeded more than once. Your PR also tried to avoid this for resubmitted shuffle stages, but I don't think we need to do that necessarily (though we could).
e the accumulator is only applied for once?  
m/apache/spark/pull/228/files), but from the current implementation, I didn‚Äôt see this has been done? (maybe I missed something)  
 I wanted to   
 clear.  
ons where it  
rialized  
se  
xecutor  
2 also matters. (We might later factor tasks in a different way to avoid 2, but it's hard due to things like Hadoop JobConf objects in the tasks).
KMeans seems  
cessful  
 a different  
ou only increment them in actions. For each result stage, the accumulator's update from each task is only applied once, even if that task runs multiple times. If you use accumulators in transformations (i.e. in a stage that may be part of multiple jobs), then you may see multiple updates, from each run. This is kind of confusing but it was useful for people who wanted to use these for debugging.

"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 22 Sep 2014 11:42:55 -0700",Re: guava version conflicts,Cody Koeninger <cody@koeninger.org>,"Hi Cody,

I'm still writing a test to make sure I understood exactly what's
going on here, but from looking at the stack trace, it seems like the
newer Guava library is picking up the ""Optional"" class from the Spark
assembly.

Could you try one of the options that put the user's classpath before
the Spark assembly? (spark.files.userClassPathFirst or
spark.yarn.user.classpath.first depending on which master you're
running)

People seem to have run into issues with those options in the past,
but if they work for you, then Guava should pick its own Optional
class (instead of the one shipped with Spark) and things should then
work.

I'll investigate a way to fix it in Spark in the meantime.





-- 
Marcelo

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 22 Sep 2014 11:55:43 -0700",Re: Support for Hive buckets,Cody Koeninger <cody@koeninger.org>,"Hi Cody,

There are currently no concrete plans for adding buckets to Spark SQL, but
thats mostly due to lack of resources / demand for this feature.  Adding
full support is probably a fair amount of work since you'd have to make
changes throughout parsing/optimization/execution.  That said, there are
probably some smaller tasks that could be easier (for example, you might be
able to avoid a shuffle when doing joins on tables that are already
bucketed by exposing more metastore information to the planner).

Michael


"
Cody Koeninger <cody@koeninger.org>,"Mon, 22 Sep 2014 14:46:07 -0500",Re: guava version conflicts,Marcelo Vanzin <vanzin@cloudera.com>,"We're using Mesos, is there a reasonable expectation that
spark.files.userClassPathFirst will actually work?


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 22 Sep 2014 13:13:30 -0700",Re: guava version conflicts,Cody Koeninger <cody@koeninger.org>,"Hmmm, a quick look at the code indicates this should work for
executors, but not for the driver... (maybe this deserves a bug being
filed, if there isn't one already?)

If it's feasible for you, you could remove the Optional.class file
from the Spark assembly you're using.




-- 
Marcelo

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 22 Sep 2014 15:28:07 -0500",Re: guava version conflicts,Marcelo Vanzin <vanzin@cloudera.com>,"We've worked around it for the meantime by excluding guava from transitive
dependencies in the job assembly and specifying the same version of guava
14 that spark is using.  Obviously things break whenever a guava 15 / 16
feature is used at runtime, so a long term solution is needed.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 22 Sep 2014 14:10:28 -0700",Re: guava version conflicts,Cody Koeninger <cody@koeninger.org>,"FYI I filed SPARK-3647 to track the fix (some people internally have
bumped into this also).




-- 
Marcelo

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 22 Sep 2014 16:34:45 -0500",OutOfMemoryError on parquet SnappyDecompressor,"""dev@spark.apache.org"" <dev@spark.apache.org>","After commit 8856c3d8 switched from gzip to snappy as default parquet
compression codec, I'm seeing the following when trying to read parquet
files saved using the new default (same schema and roughly same size as
files that were previously working):

java.lang.OutOfMemoryError: Direct buffer memory
        java.nio.Bits.reserveMemory(Bits.java:658)
        java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
        java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306)

parquet.hadoop.codec.SnappyDecompressor.setInput(SnappyDecompressor.java:99)

parquet.hadoop.codec.NonBlockedDecompressorStream.read(NonBlockedDecompressorStream.java:43)
        java.io.DataInputStream.readFully(DataInputStream.java:195)
        java.io.DataInputStream.readFully(DataInputStream.java:169)

parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:201)

parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:521)

parquet.column.impl.ColumnReaderImpl.checkRead(ColumnReaderImpl.java:493)

parquet.column.impl.ColumnReaderImpl.consume(ColumnReaderImpl.java:546)

parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:339)

parquet.column.impl.ColumnReadStoreImpl.newMemColumnReader(ColumnReadStoreImpl.java:63)

parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:58)

parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:265)
        parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:60)
        parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:74)

parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:110)

parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:172)

parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:130)

org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:139)

org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$class.isEmpty(Iterator.scala:256)
        scala.collection.AbstractIterator.isEmpty(Iterator.scala:1157)

org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:220)

org.apache.spark.sql.execution.ExistingRdd$$anonfun$productToRowRdd$1.apply(basicOperators.scala:219)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)

org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)

org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:722)
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 22 Sep 2014 16:30:27 -0700",Re: hash vs sort shuffle,Cody Koeninger <cody@koeninger.org>,"Hey Cody,

In terms of Spark 1.1.1 - we wouldn't change a default value in a spot
release. Changing this to default is slotted for 1.2.0:

https://issues.apache.org/jira/browse/SPARK-3280

- Patrick


---------------------------------------------------------------------


"
Yi Tian <tianyi.asiainfo@gmail.com>,"Tue, 23 Sep 2014 15:47:17 +0800",Question about SparkSQL and Hive-on-Spark,dev@spark.apache.org,"Hi all,

I have some questions about the SparkSQL and Hive-on-Spark

Will SparkSQL support all the hive feature in the future? or just making hive as a datasource of Spark?

From Spark 1.1.0 , we have thrift-server support running hql on spark. Will this feature be replaced by Hive on Spark?

The reason for asking these questions is that we found some hive functions are not  running well on SparkSQL ( like window function, cube and rollup function)

Is it worth for making effort on implement these functions with SparkSQL? Could you guys give some advices ? 

thank you.


Best Regards,

Yi Tian
tianyi.asiainfo@gmail.com





---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 23 Sep 2014 00:49:49 -0700",Re: Question about SparkSQL and Hive-on-Spark,Yi Tian <tianyi.asiainfo@gmail.com>,"

Most likely not *ALL* Hive features, but almost all common features.



No.





Yes absolutely.


"
rapelly kartheek <kartheek.mbms@gmail.com>,"Tue, 23 Sep 2014 13:33:06 +0530",resources allocated for an application,dev@spark.apache.org,"Hi,

I am trying to find out where exactly in the spark code are the resources
getting allocated for a newly submitted spark application.

I have a stand-alone spark cluster. Can someone please direct me to the
right part of the code.

regards
"
"""Shao, Saisai"" <saisai.shao@intel.com>","Tue, 23 Sep 2014 12:25:54 +0000",RE: spark.local.dir and spark.worker.dir not used,Priya Ch <learnings.chitturi@gmail.com>,"Hi,

Spark.local.dir is the one used to write map output data and persistent RDD blocks, but the path of  file has been hashed, so you cannot directly find the persistent rdd block files, but definitely it will be in this folders on your worker node.

Thanks
Jerry

From: Priya Ch [mailto:learnings.chitturi@gmail.com]
Sent: Tuesday, September 23, 2014 6:31 PM
To: user@spark.apache.org; dev@spark.apache.org
Subject: spark.local.dir and spark.worker.dir not used

Hi,

I am using spark 1.0.0. In my spark code i m trying to persist an rdd to disk as rrd.persist(DISK_ONLY). But unfortunately couldn't find the location where the rdd has been written to disk. I specified SPARK_LOCAL_DIRS and SPARK_WORKER_DIR to some other location rather than using the default /tmp directory, but still couldnt see anything in worker directory andspark ocal directory.

I also tried specifying the local dir and worker dir from the spark code while defining the SparkConf as conf.set(""spark.local.dir"", ""/home/padma/sparkdir"") but the directories are not used.


In general which directories spark would be using for map output files, intermediate writes and persisting rdd to disk ?


Thanks,
Padma Ch
"
Cody Koeninger <cody@koeninger.org>,"Tue, 23 Sep 2014 09:39:35 -0500",Re: OutOfMemoryError on parquet SnappyDecompressor,"""dev@spark.apache.org"" <dev@spark.apache.org>","So as a related question, is there any reason the settings in SQLConf
aren't read from the spark context's conf?  I understand why the sql conf
is mutable, but it's not particularly user friendly to have most spark
configuration set via e.g. defaults.conf or --properties-file, but for
spark sql to ignore those.


"
Will Benton <willb@redhat.com>,"Tue, 23 Sep 2014 11:39:27 -0400 (EDT)",Re: Question about SparkSQL and Hive-on-Spark,Yi Tian <tianyi.asiainfo@gmail.com>,"Hi Yi,

I've had some interest in implementing windowing and rollup in particular for some of my applications but haven't had them on the front of my plate yet.  If you need them as well, I'm happy to start taking a look this week.


best,
wb


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 18ED6115B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 23 Sep 2014 16:05:18 +0000 (UTC)
Received: (qmail 52472 invoked by uid 500); 23 Sep 2014 16:05:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52410 invoked by uid 500); 23 Sep 2014 16:05:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52398 invoked by uid 99); 23 Sep 2014 16:05:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Sep 2014 16:05:16 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 98.139.212.189 as permitted sender)
Received: from [98.139.212.189] (HELO nm30.bullet.mail.bf1.yahoo.com) (98.139.212.189)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 23 Sep 2014 16:05:11 +0000
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=s2048; d=yahoo.com;
	b=rqk3q3cG7wTKCcgXt8nbdnnFQ4kNbbE6/4MJv4Tffz0Hm8mZDTWoQeMd20c5/XS3tg6JOcdR6PZHq4E0TqtL1ePWVYkxwaiFEsIZe/PLfiA5PAxcxPtpfVSes6vxjrrvkv4kgr7QlJ2KlLaMdtbm6TuP1d7GlMnwGjf3tNtqNyu7FWsWdythBIxklpykBRt7Mmx4Ra8W5voheQqzjDqQhvIwbHSBVnTnSe0z/IH6tdlKb9FlDYX666LP2mfP3R2e1LGo/kRmA3ptQDpPYsiDBgRX7PmOvRUcWfoKwbqWSGe0D3nn/bwxZmXvC7cAzclpNqhSq1FKzbEnasmxauK23g==;
Received: from [98.139.215.141] by nm30.bullet.mail.bf1.yahoo.com with NNFMP; 23 Sep 2014 16:04:50 -0000
Received: from [98.139.212.196] by tm12.bullet.mail.bf1.yahoo.com with NNFMP; 23 Sep 2014 16:04:50 -0000
Received: from [127.0.0.1] by omp1005.mail.bf1.yahoo.com with NNFMP; 23 Sep 2014 16:04:50 -0000
X-Yahoo-Newman-Property: ymail-5
X-Yahoo-Newman-Id: 259349.23730.bm@omp1005.mail.bf1.yahoo.com
Date: Tue, 23 Sep 2014 16:04:48 +0000 (UTC)
From: Tom Graves <tgraves_cs@yahoo.com.INVALID>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
To: Chester Chen <chester@alpinenow.com>, Sean Owen <sowen@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>
Message-ID: <27889487.1262.1411488289082.JavaMail.yahoo@jws10601b.mail.bf1.yahoo.com>
In-Reply-To: <CAPYnQ0UW2H3MgvPQR6cLzEpJmWPr69pKoLtaLtq6-jJEmDdRLg@mail.gmail.com>
References: <CAPYnQ0UW2H3MgvPQR6cLzEpJmWPr69pKoLtaLtq6-jJEmDdRLg@mail.gmail.com>
Subject: Re: RFC: Deprecating YARN-alpha API's
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary=""----=_Part_1261_875487588.1411488289073""
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_1261_875487588.1411488289073
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Any other comments or objections on this?
Thanks,Tom 

   

 We were using it until recently, we are talking to our customers and see if
we can get off it.

Chester
Alpine Data Labs






    
------=_Part_1261_875487588.1411488289073--

"
Michael Armbrust <michael@databricks.com>,"Tue, 23 Sep 2014 09:18:32 -0700",Re: OutOfMemoryError on parquet SnappyDecompressor,Cody Koeninger <cody@koeninger.org>,"I actually submitted a patch to do this yesterday:
https://github.com/apache/spark/pull/2493

Can you tell us more about your configuration.  In particular how much
memory/cores do the executors have and what does the schema of your data
look like?


"
Cody Koeninger <cody@koeninger.org>,"Tue, 23 Sep 2014 12:13:04 -0500",Re: OutOfMemoryError on parquet SnappyDecompressor,Michael Armbrust <michael@databricks.com>,"Cool, that's pretty much what I was thinking as far as configuration goes.

Running on Mesos.  Worker nodes are amazon xlarge, so 4 core / 15g.  I've
tried executor memory sizes as high as 6G
Default hdfs block size 64m, about 25G of total data written by a job with
128 partitions.  The exception comes when trying to read the data (all
columns).

Schema looks like this:

case class A(
  a: Long,
  b: Long,
  c: Byte,
  d: Option[Long],
  e: Option[Long],
  f: Option[Long],
  g: Option[Long],
  h: Option[Int],
  i: Long,
  j: Option[Int],
  k: Seq[Int],
  l: Seq[Int],
  m: Seq[Int]
)

We're just going back to gzip for now, but might be nice to help someone
else avoid running into this.


"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 23 Sep 2014 11:03:08 -0700",Re: OutOfMemoryError on parquet SnappyDecompressor,Cody Koeninger <cody@koeninger.org>,"This may be related: https://github.com/Parquet/parquet-mr/issues/211

Perhaps if we change our configuration settings for Parquet it would get
better, but the performance characteristics of Snappy are pretty bad here
under some circumstances.


"
Priya Ch <learnings.chitturi@gmail.com>,"Tue, 23 Sep 2014 16:01:15 +0530",spark.local.dir and spark.worker.dir not used,"user@spark.apache.org, dev@spark.apache.org","Hi,

I am using spark 1.0.0. In my spark code i m trying to persist an rdd to
disk as rrd.persist(DISK_ONLY). But unfortunately couldn't find the
location where the rdd has been written to disk. I specified
SPARK_LOCAL_DIRS and SPARK_WORKER_DIR to some other location rather than
using the default /tmp directory, but still couldnt see anything in worker
directory andspark ocal directory.

I also tried specifying the local dir and worker dir from the spark code
while defining the SparkConf as conf.set(""spark.local.dir"",
""/home/padma/sparkdir"") but the directories are not used.


In general which directories spark would be using for map output files,
intermediate writes and persisting rdd to disk ?


Thanks,
Padma Ch
"
DB Tsai <dbtsai@dbtsai.com>,"Tue, 23 Sep 2014 13:24:44 -0700",Re: Question about SparkSQL and Hive-on-Spark,Will Benton <willb@redhat.com>,"Hi Will,

We're also very interested in windowing support in SparkSQL. Let's us
know once this is available for testing. Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
Soumitra Kumar <kumar.soumitra@gmail.com>,"Tue, 23 Sep 2014 13:28:21 -0700 (PDT)",SPARK-3660 : Initial RDD for updateStateByKey transformation,dev@spark.apache.org,"Hello fellow developers,

Thanks TD for relevant pointers.

I have created an issue :
https://issues.apache.org/jira/browse/SPARK-3660

Copying the description from JIRA:
""
How to initialize state tranformation updateStateByKey?

I have word counts from previous spark-submit run, and want to load that in next spark-submit job to start counting over that.

initial : Option [RDD [(K, S)]] = None

This will maintain the backward compatibility as well.

I have a working code as well.

This thread started on spark-user list at:
http://apache-spark-user-list.1001560.n3.nabble.com/How-to-initialize-updateStateByKey-operation-td14772.html
""

Please let me know if I shall add a parameter ""initial : Option [RDD [(K, S)]] = None"" to all updateStateByKey methods or create new ones?

Thanks,
-Soumitra.

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 23 Sep 2014 18:06:30 -0700",Re: A couple questions about shared variables,Nan Zhu <zhunanmcgill@gmail.com>,"Filed https://issues.apache.org/jira/browse/SPARK-3642 for documenting
these nuances.

-Sandy


p
s,
something)
e
s.
"
Yi Tian <tianyi.asiainfo@gmail.com>,"Wed, 24 Sep 2014 12:01:57 +0800",Re: Question about SparkSQL and Hive-on-Spark,Will Benton <willb@redhat.com>,"Hi, Will

We are planning to start implementing these functions.

We hope that we could make a general design in following week.



Best Regards,

Yi Tian
tianyi.asiainfo@gmail.com





particular for some of my applications but haven't had them on the front of my plate yet.  If you need them as well, I'm happy to start taking a look this week.
making hive
spark. Will
functions
rollup
SparkSQL?


---------------------------------------------------------------------


"
Yi Tian <tianyi.asiainfo@gmail.com>,"Wed, 24 Sep 2014 15:27:44 +0800",Re: Question about SparkSQL and Hive-on-Spark,Reynold Xin <rxin@databricks.com>,"Hi Reynold!

Will sparkSQL strictly obey the HQL syntax ?

For example, the cube function.

In other words, the hiveContext of sparkSQL should only implement the subset of HQL features?


Best Regards,

Yi Tian
tianyi.asiainfo@gmail.com





making hive as a datasource of Spark?
spark. Will this feature be replaced by Hive on Spark?
functions are not  running well on SparkSQL ( like window function, cube and rollup function) 
SparkSQL? Could you guys give some advices ?

"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 24 Sep 2014 17:14:39 +0800",Re: Question about SparkSQL and Hive-on-Spark,Yi Tian <tianyi.asiainfo@gmail.com>,"I don‚Äôt think so. For example, we‚Äôve already added extended syntax like CACHE
TABLE.
‚Äã


g
"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Wed, 24 Sep 2014 15:53:04 +0530",Re: All-time stream re-processing,"Tobias Pfeiffer <tgp@preferred.jp>, ""dev@spark.apache.org"" <dev@spark.apache.org>","So you have a single Kafka topic which has very high retention period (
that decides the storage capacity of a given Kafka topic) and you want to
process all historical data first using Camus and then start the streaming
process ?

The challenge is, Camus and Spark are two different consumer for Kafka
topic and both maintains their own consumed offset different way. Camus
stores offset in HDFS, and Spark Consumer in ZK. What I understand, you
need something which identify till which point Camus pulled ( for a given
partitions of topic) and want to start Spark receiver from there ?


Dib


"
Will Benton <willb@redhat.com>,"Wed, 24 Sep 2014 09:37:08 -0400 (EDT)",Re: Question about SparkSQL and Hive-on-Spark,Yi Tian <tianyi.asiainfo@gmail.com>,"Hi Yi,

So I've been thinking about implementing windowing for some time and started working on it in earnest yesterday.  There is already a PR for ROLLUP and CUBE; you may want to look at it and see if you can help the author out or provide some test cases:  https://github.com/apache/spark/pull/1567

As far as windowing, I'll be developing my own test cases but would appreciate it if you could also share some kinds of queries you're interested in so that I can incorporate them as well.


best,
wb


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7290C11298
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 24 Sep 2014 18:50:54 +0000 (UTC)
Received: (qmail 370 invoked by uid 500); 24 Sep 2014 18:50:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 299 invoked by uid 500); 24 Sep 2014 18:50:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 287 invoked by uid 99); 24 Sep 2014 18:50:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Sep 2014 18:50:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 24 Sep 2014 18:50:47 +0000
Received: by mail-ig0-f175.google.com with SMTP id h18so7106010igc.14
        for <dev@spark.apache.org>; Wed, 24 Sep 2014 11:50:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=jrKWyDmB7XdsKmNJxKSys/GhItO+1YwyFhth/Cy4O6o=;
        b=sRNAeHc+5H1tArep7kwIp5k8lKqlYIJVgL37CcI7sVS/fzGVOkB7iCE4AM/HBFQu+Z
         2nQR8f1Y7cq+UvL4Wb0fdlY8wmJXJWXgWipn9EXHQPMq62wadLnO1eQP6MpF5S+s9jLd
         BTpRJAw1UIP2rKKg6XvvtStj4WU+T1V9KwelJzlRkWGKUiWoBk14FBq1YNb9T2DhENZU
         3bP1NryuqNwEUfjpMwwyrzRKJOyVYrKa+9yaCq8vSielAPEslSTd9t+PRDc1LcOFXnXD
         hBe+BplcM+bXeq1sVkcZxr67IvqR4BorZ/0fFEQbnDZUl5846orIf/1MfZ1RLwE3XWcL
         gWhQ==
X-Received: by 10.43.155.13 with SMTP id lg13mr13371856icc.15.1411584627020;
        Wed, 24 Sep 2014 11:50:27 -0700 (PDT)
Received: from [142.157.43.132] (wpa043132.Wireless.McGill.CA. [142.157.43.132])
        by mx.google.com with ESMTPSA id e16sm478768igz.8.2014.09.24.11.50.23
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Wed, 24 Sep 2014 11:50:26 -0700 (PDT)
Date: Wed, 24 Sep 2014 15:04:55 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, 
 ""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>
Message-ID: <734ACEFF578D4EACB7841717736AD4EE@gmail.com>
In-Reply-To: <CACBYxKKTMz45txvUUjvoYetxuE0qugO9smyUDpvhG7tqSd4peA@mail.gmail.com>
References: <CACBYxK+35mxEL+De_ig5-JwLUz1pP6bjgZGDNrSavMTGCOUvsw@mail.gmail.com>
 <etPan.541e5dad.4db127f8.c101@mbp-3.local>
 <94DE5E900B054CF487A51DAA4451291C@gmail.com>
 <etPan.541f52be.ded7263.c101@mbp-3>
 <C278CAAAD2FA4864B5124450DDB94A6D@gmail.com>
 <CACBYxKJUHW0271LPD1g+wsa2mw_7w+5Q4-OtMdXh1EmBnM9PeA@mail.gmail.com>
 <3B040748EC1A4B788E19D44230A80671@gmail.com>
 <CACBYxKKTMz45txvUUjvoYetxuE0qugO9smyUDpvhG7tqSd4peA@mail.gmail.com>
Subject: Re: A couple questions about shared variables
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary=""542315d7_769a091f_214""
X-Virus-Checked: Checked by ClamAV on apache.org

--542315d7_769a091f_214
Content-Type: text/plain; charset=""utf-8""
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

I proposed a fix https://github.com/apache/spark/pull/2524 =20

Glad to receive feedbacks =20

-- =20
Nan Zhu



g these nuances.
ask needs to be re-run, the value of the counter from the second task ove=
rwrites the value from the first task.
t PR (seems to have some conflicts with the current DAGScheduler) =20
e algorithms utilizing accumulator for result calculation, it needs a det=
erministic accumulator, while others implementing something like Hadoop c=
ounters may need the current implementation (count everything happened, i=
ncluding the duplications)
 of the scheduler, but it worked in the past. Basically the solution is s=
imple -- in a result stage, we should not apply the update for each task =
ID more than once -- the same way we don't call job.listener.taskSucceede=
d more than once. Your PR also tried to avoid this for resubmitted shuffl=
e stages, but I don't think we need to do that necessarily (though we cou=
ld).
antee the accumulator is only applied for once=3F =20
b.com/apache/spark/pull/228/files), but from the current implementation, =
I didn=E2=80=99t see this has been done=3F (maybe I missed something) =20
:
 and I wanted to  =20
more clear. =20
casions where it =20
 =20
deserialized =20
 those =20
me executor =20
but 2 also matters. (We might later factor tasks in a different way to av=
oid 2, but it's hard due to things like Hadoop JobConf objects in the tas=
ks).
=3F KMeans seems =20
 successful =20
ause a different =20
if you only increment them in actions. =46or each result stage, the accum=
ulator's update from each task is only applied once, even if that task ru=
ns multiple times. If you use accumulators in transformations (i.e. in a =
stage that may be part of multiple jobs), then you may see multiple updat=
es, from each run. This is kind of confusing but it was useful for people=
 who wanted to use these for debugging.


--542315d7_769a091f_214--


"
Du Li <lidu@yahoo-inc.com.INVALID>,"Thu, 25 Sep 2014 00:58:34 +0000",Spark SQL use of alias in where clause,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

The following query does not work in Shark nor in the new Spark SQLContext or HiveContext.
SELECT key, value, concat(key, value) as combined from src where combined like í11%í;

The following tweak of syntax works fine although a bit ugly.
SELECT key, value, concat(key, value) as combined from src where concat(key,value) like í11%í order by combined;

Are you going to support alias in where clause soon?

Thanks,
Du
"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 24 Sep 2014 21:44:31 -0400",do MIMA checking before all test cases start?,dev@spark.apache.org,"Hi, all  

It seems that, currently, Jenkins makes MIMA checking after all test cases have finished, IIRC, during the first months we introduced MIMA, we do the MIMA checking before running test cases

What‚Äôs the motivation to adjust this behaviour?

In my opinion, if you have some binary compatibility issues, you just need to do some minor changes, but in the current environment, you can only get if your change works after all test cases finished (1 hour later‚Ä¶)

Best,  

--  
Nan Zhu

"
Yanbo Liang <yanbohappy@gmail.com>,"Thu, 25 Sep 2014 11:32:37 +0800",Re: Spark SQL use of alias in where clause,Du Li <lidu@yahoo-inc.com.invalid>,"Maybe it's the way SQL works.
The select part is executed after the where filter is applied, so you
cannot use alias declared in select part in where clause.
Hive and Oracle behavior the same as Spark SQL.

2014-09-25 8:58 GMT+08:00 Du Li <lidu@yahoo-inc.com.invalid>:

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 24 Sep 2014 21:04:33 -0700",Re: do MIMA checking before all test cases start?,Nan Zhu <zhunanmcgill@gmail.com>,"Have you considered running the mima checks locally? We prefer people
not use Jenkins for very frequent checks since it takes resources away
from other people trying to run tests.


---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 25 Sep 2014 00:25:52 -0400",Re: do MIMA checking before all test cases start?,Patrick Wendell <pwendell@gmail.com>,"yeah, I tried that, but there is always an issue when I ran dev/mima,  

it always gives me some binary compatibility error on Java API part‚Ä¶.

so I have to wait for Jenkins‚Äô result when fixing MIMA issues

--  
Nan Zhu




cases have finished, IIRC, during the first months we introduced MIMA, we do the MIMA checking before running test cases
 need to do some minor changes, but in the current environment, you can only get if your change works after all test cases finished (1 hour later...)


"
Ye Xianjin <advancedxy@gmail.com>,"Thu, 25 Sep 2014 16:59:08 +0800",Re: spark_classpath in core/pom.xml and yarn/porm.xml,"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","hi, Sandy Ryza:
     I believe It's you originally added the SPARK_CLASSPATH in core/pom.xml in the org.scalatest section. Does this still needed in 1.1?
     I noticed this setting because when I looked into the unit-tests.log, It shows something below:

However I didn't set SPARK_CLASSPATH env variable. And looked into the SparkConf.scala, If user actually set extraClassPath,  the SparkConf will throw SparkException.
-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)




"
Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"Thu, 25 Sep 2014 15:09:32 +0200",MLlib enable extension of the LabeledPoint class,dev@spark.apache.org,"Hi Spark developers,

I try to implement a framework with Spark and MLlib to do duplicate
detection. I'm not familiar with Spark and Scala so please be patient
with me. In order to enrich the LabeledPoint class with some information
I tried to extend it and added some properties.
But the ML algorithms (in my case DecisionTree) don't accept my new
ExtendedLabeledPoint class. They just accept the type RDD[LabeledPoint].
Would it be possible to extract a LabeledPoint interface / trait or to
change the type to something covariant like
def train[A <: LabeledPoint]( RDD[A], ... )

I hope it's a usefull idea and it's possible.

Thanks in advance,
Niklas

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 25 Sep 2014 09:39:35 -0400",Re: do MIMA checking before all test cases start?,Nan Zhu <zhunanmcgill@gmail.com>,"It might still make sense to make this change if MIMA checks are always
relatively quick, for the same reason we do style checks first.


¶.
ly
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 25 Sep 2014 09:43:21 -0400",Re: Spark SQL use of alias in where clause,Yanbo Liang <yanbohappy@gmail.com>,"That is correct. Aliases in the SELECT clause can only be referenced in the
ORDER BY and HAVING clauses. Otherwise, you'll have to just repeat the
statement, like concat() in this case.

A more elegant alternative, which is probably not available in Spark SQL
yet, is to use Common Table Expressions
<http://technet.microsoft.com/en-us/library/ms190766(v=sql.105).aspx>.


d
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 25 Sep 2014 07:02:12 -0700 (PDT)",Re: MLlib enable extension of the LabeledPoint class,dev@spark.incubator.apache.org,"Hi Niklas Wilcke,

As you said, it is difficult to extend LabeledPoint class in
mllib.regression.
Do you want to extend LabeledPoint class in order to use any other type
exclude Double type?
If you have your code on Github, could you show us it? I want to know what
you want to do.

By the way, I think LabeledPoint class is very useful exclude
mllib.regression package.
Especially, some estimation algorithms should use a type for the labels
exclude Double type, 
such as String type. The common generics labeled-point class would be useful
in MLlib.
I'd like to get your thoughts on it.

For example,
```
abstract class LabeledPoint[T](label: T, features: Vector)
```

thanks






-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Egor Pahomov <pahomov.egor@gmail.com>,"Thu, 25 Sep 2014 18:22:50 +0400",Re: MLlib enable extension of the LabeledPoint class,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"@Yu Ishikawa,

*I think the right place for such discussion -
 https://issues.apache.org/jira/browse/SPARK-3573
<https://issues.apache.org/jira/browse/SPARK-3573>*


2014-09-25 18:02 GMT+04:00 Yu Ishikawa <yuu.ishikawa+spark@gmail.com>:



-- 



*Sincere"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 25 Sep 2014 07:25:53 -0700 (PDT)",Re: MLlib enable extension of the LabeledPoint class,dev@spark.incubator.apache.org,"Hi Egor Pahomov, 

Thank you for your comment!



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Egor Pahomov <pahomov.egor@gmail.com>,"Thu, 25 Sep 2014 18:27:29 +0400",Re: MLlib enable extension of the LabeledPoint class,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"I agree with Yu, that you should tell more about your intentions, but
possible dirty workaround is create wrapper class for LabeledPoint with all
additional information you need and unwrap values before train, and wrap
them again after. (look at zipWithIndex - it helps match back additional
information after unwrapping)

But I would rather patch my spark with method signature chagnes you
suggested.

2014-09-25 18:22 GMT+04:00 Egor Pahomov <pahomov.egor@gmail.com>:




-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"Thu, 25 Sep 2014 16:52:56 +0200",Re: MLlib enable extension of the LabeledPoint class,dev@spark.apache.org,"Hi Yu Ishikawa,

I'm sorry but I can't share my code via github at the moment. Hopefully
in some months I can.
I don't want to change the type of the label but that would be also a
very nice improvement.

Making LabeledPoint abstract is exactly what I need. That enables me to
create a class like

    LabeledPointTuplePair(label: Double, features: Vector, tuplePair: (Tuple, Tuple)) extends LabeledPoint

and use it in combination of the existing ML algorithms.

In my limited understanding of the MLlib I agree with your proposal of
the LabeledPoint interface

    abstract class LabeledPoint[T](label: T, features: Vector)

In my opinion making LabeledPoint abstract is necessary and introducing
a generic label would be nice to have.
Just to clarify my priorities.

Kind Regards,
Niklas Wilcke




---------------------------------------------------------------------


"
Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"Thu, 25 Sep 2014 17:04:27 +0200",Re: MLlib enable extension of the LabeledPoint class,dev@spark.apache.org,"Hi Egor Pahomov,

thanks for your suggestions. I think I will do the dirty workaround
because I don't want to maintain my own version of spark for now. Maybe
I will do later when I feel ready to contribute to the project.

Kind Regards,
Niklas Wilcke



---------------------------------------------------------------------


"
Du Li <lidu@yahoo-inc.com.INVALID>,"Thu, 25 Sep 2014 15:59:28 +0000",Re: Spark SQL use of alias in where clause,"Nicholas Chammas <nicholas.chammas@gmail.com>,
        Yanbo Liang
	<yanbohappy@gmail.com>","Thanks, Yanbo and Nicholas. Now it makes more sense ó query optimization is the answer. /Du

From: Nicholas Chammas <nicholas.chammas@gmail.com<mailto:nicholas.chammas@gmail.com>>
Date: Thursday, September 25, 2014 at 6:43 AM
To: Yanbo Liang <yanbohappy@gmail.com<mailto:yanbohappy@gmail.com>>
Cc: Du Li <lidu@yahoo-inc.com.invalid<mailto:lidu@yahoo-inc.com.invalid>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
Subject: Re: Spark SQL use of alias in where clause

That is correct. Aliases in the SELECT clause can only be referenced in the ORDER BY and HAVING clauses. Otherwise, you'll have to just repeat the statement, like concat() in this case.

A more elegant alternative, which is probably not available in Spark SQL yet, is to use Common Table Expressions<http://technet.microsoft.com/en-us/library/ms190766(v=sql.105).aspx>.

Maybe it's the way SQL works.
The select part is executed after the where filter is applied, so you cannot use alias declared in select part in where clause.
Hive and Oracle behavior the same as Spark SQL.

2014-09-25 8:58 GMT+08:00 Du Li <lidu@yahoo-inc.com.invalid<mailto:lidu@yahoo-inc.com.invalid>>:
Hi,

The following query does not work in Shark nor in the new Spark SQLContext or HiveContext.
SELECT key, value, concat(key, value) as combined from src where combined like í11%í;

The following tweak of syntax works fine although a bit ugly.
SELECT key, value, concat(key, value) as combined from src where concat(key,value) like í11%í order by combined;

Are you going to support alias in where clause soon?

Thanks,
Du


"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Fri, 26 Sep 2014 00:41:12 +0800",VertexRDD partition imbalance,"dev@spark.apache.org, user@spark.apache.org","Hi all

VertexRDD is partitioned with HashPartitioner, and it exhibits some 
imbalance of tasks.
For example, Connected Components with partition strategy Edge2D:


        Aggregated Metrics by Executor

Executor ID 	Task Time 	Total Tasks 	Failed Tasks 	Succeeded Tasks 
Input 	Shuffle Read 	Shuffle Write 	Shuffle Spill (Memory) 	Shuffle 
Spill (Disk)
1 	10 s 	10 	0 	10 	234.6 MB 	0.0 B 	43.2 MB 	0.0 B 	0.0 B
2 	3 s 	3 	0 	3 	70.4 MB 	0.0 B 	13.0 MB 	0.0 B 	0.0 B
3 	6 s 	6 	0 	6 	140.7 MB 	0.0 B 	25.9 MB 	0.0 B 	0.0 B
4 	9 s 	8 	0 	8 	187.9 MB 	0.0 B 	34.6 MB 	0.0 B 	0.0 B
5 	10 s 	9 	0 	9 	211.4 MB 	0.0 B 	38.9 MB 	0.0 B 	0.0 B

For a stage on mapPartitions at VertexRDD.scala:347
343
344   /** Generates an RDD of vertex attributes suitable for shipping to 
the edge partitions. */
345   private[graphx] def shipVertexAttributes(
346       shipSrc: Boolean, shipDst: Boolean): RDD[(PartitionID, 
VertexAttributeBlock[VD])] = {
347 
partitionsRDD.mapPartitions(_.flatMap(_.shipVertexAttributes(shipSrc, 
shipDst)))
348   }
349

This is executed for every iteration in Pregel, so the imbalance is bad 
for performance.

However, when run PageRank with Edge2D, the tasks are even across 
executors. (all finish 6 tasks)
Our configuration is 6 node, 36 partitions.

My questions is:

    What decides the number of tasks for different executors? And how to
    make it balance?

Thanks!
Larry

"
"""Mozumder, Monir"" <Monir.Mozumder@amd.com>","Thu, 25 Sep 2014 17:54:49 +0000",Code reading tips Spark source,"""dev@spark.apache.org"" <dev@spark.apache.org>","Folks,

I am starting to explore Spark framework and hopefully contribute to it in future. I was wondering if you have any documentation or tips to get understanding the inner workings of the code quickly. 

I am new to both Spark and Scala and am taking a look at the *Rdd*.scala files in the source tree.

My ultimate goal is to offload some of the compute done on a partition to the GPU cores available on the node. Any prior attempt or design discussion done on that aspect?

Bests,
-Monir


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 25 Sep 2014 15:06:47 -0700",Re: do MIMA checking before all test cases start?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yeah we can also move it first. Wouldn't hurt.


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 25 Sep 2014 15:29:37 -0700",Re: spark_classpath in core/pom.xml and yarn/porm.xml,Ye Xianjin <advancedxy@gmail.com>,"Hi Ye,

I think git blame shows me because I fixed the formatting in core/pom.xml,
but I don't actually know the original reason for setting SPARK_CLASSPATH
there.

Do the tests run OK if you take it out?

-Sandy



"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 25 Sep 2014 15:36:29 -0700",Re: spark_classpath in core/pom.xml and yarn/porm.xml,Sandy Ryza <sandy.ryza@cloudera.com>,"BTW I removed it from the yarn pom since it was not used (and actually
interfered with a test I was writing).

I did not touch the core pom, but I wouldn't be surprised if it's not
needed there either.




-- 
Marcelo

---------------------------------------------------------------------


"
Ye Xianjin <advancedxy@gmail.com>,"Fri, 26 Sep 2014 10:07:21 +0800",Re: spark_classpath in core/pom.xml and yarn/pom.xml,Sandy Ryza <sandy.ryza@cloudera.com>,"Hi Sandy, 

Sorry for the bothering. 

The tests run ok even the SPARK_CLASS setting is there now, but It gives a config warning and will potential interfere other settings like Marcelo said. The warning goes away if I remove it out.

And Marcelo, I believe the setting in core/pom should not be used any more. But I don't think it's worthy to file a JIRA for such small change. Maybe put it into other related JIRA. It's a pity that your pr
already got merged.
     

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)




"
Yanbo Liang <yanbohappy@gmail.com>,"Fri, 26 Sep 2014 18:09:01 +0800",Re: A Spark Compilation Question,Hansu GU <guhansu@gmail.com>,"Hi Hansu,

I have encountered the same problem. Maven compiled avro file and generated
corresponding Java file in new directory which is not source file directory
of the project.

I have modified pom.xml file and it can be work.
The line marked as red is added, you can add them to your
spark-*.*.*/external/flume-sink/pom.xml.

    <plugin>
        <groupId>org.apache.avro</groupId>
        <artifactId>avro-maven-plugin</artifactId>
        <version>${avro.version}</version>
        <configuration>
          <!-- Generate the output in the same directory as the
sbt-avro-plugin -->

<outputDirectory>${project.basedir}/target/scala-${scala.binary.version}/src_managed/main/compiled_avro</outputDirectory>
  <outputDirectory>${project.basedir}/src/main/java</outputDirectory>
        </configuration>
        <executions>
          <execution>
            <phase>generate-sources</phase>
            <goals>
              <goal>idl-protocol</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

    <plugin>
    <groupId>org.codehaus.mojo</groupId>
<artifactId>build-helper-maven-plugin</artifactId>
<version>1.9.1</version>
<executions>
  <execution>
<id>add-source</id>
<phase>generate-sources</phase>
<goals>
  <goal>add-source</goal>
</goals>
    <configuration>
      <sources>
  <source>${project.basedir}/src/main/java</source>
  </sources>
    </configuration>
  </execution>
    </executions>
  </plugin>




2014-09-13 2:45 GMT+08:00 Hansu GU <guhansu@gmail.com>:

"
praveen seluka <praveen.seluka@gmail.com>,"Fri, 26 Sep 2014 17:32:01 +0530",executorAdded event to DAGScheduler,"dev@spark.apache.org, user@spark.apache.org","Can someone explain the motivation behind passing executorAdded event to
DAGScheduler ? *DAGScheduler *does *submitWaitingStages *when *executorAdded
*method is called by *TaskSchedulerImpl*. I see some issue in the below
code,

*TaskSchedulerImpl.scala code*
if (!executorsByHost.contains(o.host)) {
        executorsByHost(o.host) = new HashSet[String]()
        executorAdded(o.executorId, o.host)
        newExecAvail = true
      }

Note that executorAdded is called only when there is a new host and not for
every new executor. For instance, there can be two executors in the same
host and in this case. (But DAGScheduler executorAdded is notified only for
new host - so only once in this case). If this is indeed an issue, I would
like to submit a patch for this quickly. [cc Andrew Or]

- Praveen
"
praveen seluka <praveen.seluka@gmail.com>,"Fri, 26 Sep 2014 17:33:17 +0530",Re: executorAdded event to DAGScheduler,"dev@spark.apache.org, user@spark.apache.org","Some corrections.


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 26 Sep 2014 08:35:42 -0400",Re: executorAdded event to DAGScheduler,praveen seluka <praveen.seluka@gmail.com>,"just a quick reply, we cannot start two executors in the same host for a single application in the standard deployment (one worker per machine)  

I‚Äôm not sure if it will create an issue when you have multiple workers in the same host, as submitWaitingStages is called everywhere and I never try such a deployment mode

Best,  

--  
Nan Zhu



o DAGScheduler ? DAGScheduler does submitWaitingStages when executorAdded method is called by TaskSchedulerImpl. I see some issue in the below code,
 for every new executor. For instance, there can be two executors in the same host and in this case. (But DAGScheduler executorAdded is notified only for new host - so only once in this case). If this is indeed an issue, I would like to submit a patch for this quickly. [cc Andrew Or]

"
praveen seluka <praveen.seluka@gmail.com>,"Fri, 26 Sep 2014 17:56:01 +0530",Re: executorAdded event to DAGScheduler,Nan Zhu <zhunanmcgill@gmail.com>,"In Yarn, we can easily  have multiple containers allocated in the same node.


rkers in
ded
e,
"
Arun Ahuja <aahuja11@gmail.com>,"Fri, 26 Sep 2014 15:58:56 -0400",Re: PARSING_ERROR from kryo,npanj <nitinpanj@gmail.com>,"I am seeing the same error as well since upgrading to Spark1.1:

14/09/26 15:35:05 ERROR executor.Executor: Exception in task 1032.0 in
stage 5.1 (TID 22449)
com.esotericsoftware.kryo.KryoException: java.io.IOException: failed to
uncompress the chunk: PARSING_ERROR(2)
        at com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
        at com.esotericsoftware.kryo.io.Input.require(Input.java:155)
        at com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
        at
com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:109)
        at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:610)
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:721)
        at
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
        at
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        at
org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1082)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        at
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)

Out of 6000 tasks 5000 something finish fine, so I don't believe there are
any issues with the serialization and some other datasets everything works
fine. Also the same code, same dataset worked fine with Spark 1.0.2


"
shane knapp <sknapp@berkeley.edu>,"Fri, 26 Sep 2014 13:19:47 -0700",FYI: jenkins systems patched to fix bash exploit,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","all of our systems were affected by the shellshock bug, and i've just
patched everything w/the latest fix from redhat:

https://access.redhat.com/articles/1200223

we're not running bash.x86_64 0:4.1.2-15.el6_5.2 on all of our systems.

shane
"
shane knapp <sknapp@berkeley.edu>,"Fri, 26 Sep 2014 13:35:40 -0700",Re: FYI: jenkins systems patched to fix bash exploit,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","
:)
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 26 Sep 2014 16:50:38 -0400",thank you for reviewing our patches,dev <dev@spark.apache.org>,"I recently came across this mailing list post by Linus Torvalds
<https://lkml.org/lkml/2004/12/20/255> about the value of reviewing even
‚Äútrivial‚Äù patches. The following passages stood out to me:

I think that much more important than the patch is the fact that people get
used to the notion that they can change the kernel

‚Ä¶

So please don‚Äôt stop. Yes, those trivial patches *are* a bother. Damn, they
are *horrible*. But at the same time, the devil is in the detail, and they
are needed in the long run. Both the patches themselves, and the people
that grew up on them.

Spark is the first (and currently only) open source project I contribute
regularly to. My first several PRs against the project, as simple as they
were, were definitely patches that I ‚Äúgrew up on‚Äù.

I appreciate the time and effort all the reviewers I‚Äôve interacted with
have taken to work with me on my PRs, even when they are ‚Äútrivial‚Äù. And I‚Äôm
sure that as I continue to contribute to this project there will be many
more patches that I will ‚Äúgrow up on‚Äù.

Thank you Patrick, Reynold, Josh, Davies, Michael, and everyone else who‚Äôs
taken time to review one of my patches. I appreciate it!

Nick
‚Äã
"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Sep 2014 13:54:41 -0700",Re: thank you for reviewing our patches,Nicholas Chammas <nicholas.chammas@gmail.com>,"Keep the patches coming :)



:
et
Damn, they
y
d with
‚Äù. And I‚Äôm
‚Äôs
"
Du Li <lidu@yahoo-inc.com.INVALID>,"Fri, 26 Sep 2014 23:48:34 +0000",SparkSQL: map type MatchError when inserting into Hive table,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I was loading data into a partitioned table on Spark 1.1.0
beeline-thriftserver. The table has complex data types such as map<string,
string> and array<map<string,string>>. The query is like ≥insert overwrite
table a partition (ä) select ä≤ and the select clause worked if run
separately. However, when running the insert query, there was an error as
follows.

The source code of Cast.scala seems to only handle the primitive data
types, which is perhaps why the MatchError was thrown.

I just wonder if this is still work in progress, or I should do it
differently.

Thanks,
Du


----
scala.MatchError: MapType(StringType,StringType,true) (of class
org.apache.spark.sql.catalyst.types.MapType)
        
org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:2
47)
        org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:247)
        org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:263)
        
org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala
:84)
        
org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.appl
y(Projection.scala:66)
        
org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.appl
y(Projection.scala:50)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sq
l$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.sca
la:149)
        
org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHive
File$1.apply(InsertIntoHiveTable.scala:158)
        
org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHive
File$1.apply(InsertIntoHiveTable.scala:158)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1
145)
        
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:
615)
        java.lang.Thread.run(Thread.java:722)






---------------------------------------------------------------------


"
Du Li <lidu@yahoo-inc.com.INVALID>,"Sat, 27 Sep 2014 01:24:38 +0000",Re: SparkSQL: map type MatchError when inserting into Hive table,"""dev@spark.apache.org"" <dev@spark.apache.org>","
It might be a problem when inserting into a partitioned table. It worked
fine to when the target table was unpartitioned.

Can you confirm this?

Thanks,
Du



On 9/26/14, 4:48 PM, ""Du Li"" <lidu@yahoo-inc.com.INVALID> wrote:

>Hi,
>
>I was loading data into a partitioned table on Spark 1.1.0
>beeline-thriftserver. The table has complex data types such as map<string,
>string> and array<map<string,string>>. The query is like ¬≥insert overwrite
>table a partition (≈†) select ≈†¬≤ and the select clause worked if run
>separately. However, when running the insert query, there was an error as
>follows.
>
>The source code of Cast.scala seems to only handle the primitive data
>types, which is perhaps why the MatchError was thrown.
>
>I just wonder if this is still work in progress, or I should do it
>differently.
>
>Thanks,
>Du
>
>
>----
>scala.MatchError: MapType(StringType,StringType,true) (of class
>org.apache.spark.sql.catalyst.types.MapType)
>        
>org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:
>2
>47)
>        
>org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:247)
>        
>org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:263)
>        
>org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scal
>a
>:84)
>        
>org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.app
>l
>y(Projection.scala:66)
>        
>org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.app
>l
>y(Projection.scala:50)
>        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>        
>org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$s
>q
>l$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.sc
>a
>la:149)
>        
>org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiv
>e
>File$1.apply(InsertIntoHiveTable.scala:158)
>        
>org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiv
>e
>File$1.apply(InsertIntoHiveTable.scala:158)
>        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
>        org.apache.spark.scheduler.Task.run(Task.scala:54)
>        
>org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
>        
>java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:
>1
>145)
>        
>java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java
>:
>615)
>        java.lang.Thread.run(Thread.java:722)
>
>
>
>
>
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>For additional commands, e-mail: user-help@spark.apache.org
>

"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 27 Sep 2014 11:06:16 +0800",Re: SparkSQL: map type MatchError when inserting into Hive table,"Du Li <lidu@yahoo-inc.com.INVALID>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Would you mind to provide the DDL of this partitioned table together 
with the query you tried? The stacktrace suggests that the query was 
trying to cast a map into something else, which is not supported in 
Spark SQL. And I doubt whether Hive support casting a complex type to 
some other type.

) select ä≤ and the select clause worked if run


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 27 Sep 2014 11:08:24 +0800",Re: SparkSQL: map type MatchError when inserting into Hive table,"Du Li <lidu@yahoo-inc.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Would you mind to provide the DDL of this partitioned table together 
with the query you tried? The stacktrace suggests that the query was 
trying to cast a map into something else, which is not supported in 
Spark SQL. And I doubt whether Hive support casting a complex type to 
some other type.

) select ä≤ and the select clause worked if run


---------------------------------------------------------------------


"
Ye Xianjin <advancedxy@gmail.com>,"Sat, 27 Sep 2014 14:49:50 +0800",Re: thank you for reviewing our patches,Nicholas Chammas <nicholas.chammas@gmail.com>,"



e
ey

 Yeah, me too. I think I will keep patching.  
ted with
al‚Äù. And I‚Äôm
y
o‚Äôs

And yes, the reviewer are very nice and responsive. It's a pleasure communicating with   
the reviewers, Thank you for your time.  
"
Tom Hubregtsen <thubregtsen@gmail.com>,"Sat, 27 Sep 2014 14:27:05 -0700 (PDT)",Spark memory regions,dev@spark.incubator.apache.org,"As I've told before, I am currently writing my master's thesis on storage and
memory usage in Spark. I am currently specifically looking at the different
fractions of memory:

I was able to find 3 memory regions, but it seems to leave some unaccounted
for:
1. spark.shuffle.memoryFraction: 20%
2. spark.storage.memoryFraction: 60%
3. spark.storage.unrollFraction: 20% of spark.storage.memoryFraction = 12%

4a. Unaccounted: 100-(20+60+12)=8%
or, if unrollFraction is not only proportional to, but also resides in
storage.memoryFraction:
4b. Unaccounted: 100-(20+0.8*60+0.2*60)=20%

Question 1: How big is the unaccounted fraction, and what is this used for? 
(Expected answer: Spark environment)

Question 2: What is stored into spark.storage.memoryFraction?
14/09/23 10:56:56 INFO MemoryStore: Block broadcast_0 stored as values in
memory (estimated size 184.7 KB, free 47.1 GB)
14/09/23 13:13:11 INFO MemoryStore: Block rdd_1_1 stored as values in memory
(estimated size 1458.0 MB, free 47.1 GB)
Expected answer: broadcast variables, cached RDDs, potentially unrolled
blocks (although not seen in the messages, and not noticeable in the size
reduction in these logmessages)
Remark: If there is nothing else that resides in this area, then in the case
that the user would not use .cache() or .persist(MEMORY), a lot of memory is
kept unused, since the broadcast is connectible small, and unroll, if stored
here, takes a max of 20% of the 60%, right?

Question 3: Which RDDs are not only instantiated, but also actually filled
with data?
I am trying to estimate the dataset I have. I know that because of lazy
evaluation, we can never be certain, but it should be possible to estimate a
minimum. Is it safe to assume that at least the RDDs that are the output of
a sort/shuffle stage, and the ones that the user calls
{cache(),persist(MEMORY),collect()} on, are not only instantiated, but also
filled with data? And are there any other assumptions we can make, for
instance about the other RDDs?

Question 4a: Where is intermediate data between stages stored? 
Question 4b: Where is intermediate data during stages stored? 
When I do not use rdd.cache(), I do not see the memory in
storage.memoryFraction go up. Therefore, I think we can eliminate this
fraction.
The intermediate data from a sort/shuffle uses the ExternalSorter or the
moved & removed at the end of the stage, or does the next stage retrieve it
from here?
Is there any more intermediate data?
If only RDDs that relate to a sort/shuffle are filled, then I expect it to
be in this area, but it might also be possible that these are moved once the
particular shuffle finishes?

Question 5: If I have sufficient memory (256G), will there be a difference
in execution time between caching no RDDs and caching all RDDs?
I did not expect it, but my intermediate results show a 1.5 to 2x
difference. 



--

---------------------------------------------------------------------


"
Tom Hubregtsen <thubregtsen@gmail.com>,"Sat, 27 Sep 2014 15:00:03 -0700 (PDT)",RE: spark.local.dir and spark.worker.dir not used,dev@spark.incubator.apache.org,"Also, if I am not mistaken, this data is automatically removed after your
run. Be sure to check it while running your program.



--

---------------------------------------------------------------------


"
Tom Hubregtsen <thubregtsen@gmail.com>,"Sat, 27 Sep 2014 15:07:36 -0700 (PDT)",Re: memory size for caching RDD,dev@spark.incubator.apache.org,"Use unpersist(), even when not persisted before. 



--

---------------------------------------------------------------------


"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Sun, 28 Sep 2014 11:25:39 +0800","PageRank execution imbalance, might hurt performance by 6x","user@spark.apache.org, dev@spark.apache.org","Hi all!

I'm running PageRank on GraphX, and I find on some tasks on one machine 
can spend 5~6 times more time than on others, others are perfectly 
balance (around 1 second to finish).
And since time for a stage (iteration) is determined by the slowest 
task, the performance is undesirable.

I don't know if there's any internals that might make execution 
unstable? Like scheduling, garbage collection ‚Ä¶

A stage for mapPartitions at GraphImpl.scala:409
looks like this:


        Tasks

Index 	ID 	Attempt 	Status 	Locality Level 	Executor 	Launch Time 
Duration ‚ñ¥ 	GC Time 	Accumulators 	Input 	Shuffle Read 	Write Time 
Shuffle Write 	Errors
21 	787 	0 	SUCCESS 	PROCESS_LOCAL 	brick0 	2014/09/28 03:04:42 	7 s 	
	
	333.3 MB (memory) 	4.9 MB 	1 ms 	652.3 KB 	
0 	768 	0 	SUCCESS 	PROCESS_LOCAL 	brick2 	2014/09/28 03:04:42 	7 s 	
	
	531.5 MB (memory) 	8.0 MB 	2 ms 	1321.5 KB 	
9 	775 	0 	SUCCESS 	PROCESS_LOCAL 	brick0 	2014/09/28 03:04:42 	6 s 	
	
	270.4 MB (memory) 	4.1 MB 	1 ms 	659.3 KB 	
15 	781 	0 	SUCCESS 	PROCESS_LOCAL 	brick0 	2014/09/28 03:04:42 	6 s 	
	
	272.7 MB (memory) 	4.3 MB 	1 ms 	658.9 KB 	
3 	769 	0 	SUCCESS 	PROCESS_LOCAL 	brick0 	2014/09/28 03:04:42 	6 s 	
	
	285.5 MB (memory) 	4.4 MB 	1 ms 	658.5 KB 	
6 	774 	0 	SUCCESS 	PROCESS_LOCAL 	brick2 	2014/09/28 03:04:42 	6 s 	
	
	346.8 MB (memory) 	4.6 MB 	1 ms 	657.0 KB 	
12 	780 	0 	SUCCESS 	PROCESS_LOCAL 	brick2 	2014/09/28 03:04:42 	6 s 	
	
	313.2 MB (memory) 	4.4 MB 	1 ms 	645.5 KB 	
18 	786 	0 	SUCCESS 	PROCESS_LOCAL 	brick2 	2014/09/28 03:04:42 	6 s 	
	
	281.7 MB (memory) 	4.2 MB 	1 ms 	660.1 KB 	
1 	771 	0 	SUCCESS 	PROCESS_LOCAL 	brick3 	2014/09/28 03:04:42 	2 s 	
	
	339.1 MB (memory) 	5.1 MB 	1 ms 	657.4 KB 	
7 	777 	0 	SUCCESS 	PROCESS_LOCAL 	brick3 	2014/09/28 03:04:42 	2 s 	
	
	322.8 MB (memory) 	4.9 MB 	1 ms 	654.5 KB 	
13 	783 	0 	SUCCESS 	PROCESS_LOCAL 	brick3 	2014/09/28 03:04:42 	2 s 	
	
	279.8 MB (memory) 	4.6 MB 	1 ms 	655.4 KB 	
19 	789 	0 	SUCCESS 	PROCESS_LOCAL 	brick3 	2014/09/28 03:04:42 	2 s 	
	
	268.4 MB (memory) 	4.4 MB 	1 ms 	658.5 KB 	
16 	784 	0 	SUCCESS 	PROCESS_LOCAL 	brick4 	2014/09/28 03:04:42 	1 s 	
	
	339.1 MB (memory) 	5.1 MB 	1 ms 	660.1 KB 	
11 	776 	0 	SUCCESS 	PROCESS_LOCAL 	brick1 	2014/09/28 03:04:42 	1 s 	
	
	341.0 MB (memory) 	5.3 MB 	1 ms 	655.4 KB 	
2 	773 	0 	SUCCESS 	PROCESS_LOCAL 	brick5 	2014/09/28 03:04:42 	1 s 	
	
	320.9 MB (memory) 	4.9 MB 	1 ms 	655.3 KB 	
22 	790 	0 	SUCCESS 	PROCESS_LOCAL 	brick4 	2014/09/28 03:04:42 	1 s 	
	
	301.7 MB (memory) 	4.9 MB 	1 ms 	659.5 KB 	
17 	782 	0 	SUCCESS 	PROCESS_LOCAL 	brick1 	2014/09/28 03:04:42 	1 s 	
	
	317.1 MB (memory) 	5.2 MB 	1 ms 	653.7 KB 	
23 	788 	0 	SUCCESS 	PROCESS_LOCAL 	brick1 	2014/09/28 03:04:42 	1 s 	
	
	268.3 MB (memory) 	4.9 MB 	1 ms 	664.3 KB 	
8 	779 	0 	SUCCESS 	PROCESS_LOCAL 	brick5 	2014/09/28 03:04:42 	1 s 	
	
	291.3 MB (memory) 	4.6 MB 	1 ms 	660.6 KB 	
20 	791 	0 	SUCCESS 	PROCESS_LOCAL 	brick5 	2014/09/28 03:04:42 	1 s 	
	
	272.7 MB (memory) 	4.5 MB 	1 ms 	661.7 KB 	
10 	778 	0 	SUCCESS 	PROCESS_LOCAL 	brick4 	2014/09/28 03:04:42 	1 s 	
	
	276.5 MB (memory) 	4.4 MB 	1 ms 	656.4 KB 	
4 	772 	0 	SUCCESS 	PROCESS_LOCAL 	brick4 	2014/09/28 03:04:42 	1 s 	
	
	260.8 MB (memory) 	4.4 MB 	1 ms 	661.7 KB 	
14 	785 	0 	SUCCESS 	PROCESS_LOCAL 	brick5 	2014/09/28 03:04:42 	1 s 	
	
	262.7 MB (memory) 	4.3 MB 	1 ms 	651.7 KB 	
5 	770 	0 	SUCCESS 	PROCESS_LOCAL 	brick1 	2014/09/28 03:04:42 	1 s 	
	
	276.5 MB (memory) 	4.7 MB 	1 ms 	655.1 KB 	


Thanks!
Larry
"
Yi Tian <tianyi.asiainfo@gmail.com>,"Sun, 28 Sep 2014 17:24:28 +0800",Re: A Spark Compilation Question,Yanbo Liang <yanbohappy@gmail.com>,"I think you should modify the module settings in IDEA instead of pom.xml


Best Regards,

Yi Tian
tianyi.asiainfo@gmail.com





generated
directory
<outputDirectory>${project.basedir}/target/scala-${scala.binary.version}/src_managed/main/compiled_avro</outputDirectory>
sure.


---------------------------------------------------------------------


"
Yanbo Liang <yanbohappy@gmail.com>,"Sun, 28 Sep 2014 17:48:39 +0800","[MLlib] LogisticRegressionWithSGD and LogisticRegressionWithLBFGS
 converge with different weights.","""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>, 
	Xiangrui Meng <mengxr@gmail.com>, DB Tsai <dbtsai@dbtsai.com>","Hi

We have used LogisticRegression with two different optimization method SGD
and LBFGS in MLlib.
With the same dataset and the same training and test split, but get
different weights vector.

For example, we use
spark-1.1.0/data/mllib/sample_binary_classification_data.txt
as our training and test dataset.
With LogisticRegressionWithSGD and LogisticRegressionWithLBFGS as training
method and the same other parameters.

The precisions of these two methods almost near 100% and AUCs are also near
1.0.
As far as I know, the convex optimization problem will converge to the
global minimum value. (We use SGD with mini batch fraction as 1.0)
But I got two different weights vector? Is this expectation or make sense?
"
myasuka <myasuka@live.com>,"Sun, 28 Sep 2014 02:59:15 -0700 (PDT)",How to use multi thread in RDD map function ?,dev@spark.incubator.apache.org,"Hi, everyone
    I come across with a problem about increasing the concurency. In a
program, after shuffle write, each node should fetch 16 pair matrices to do
matrix multiplication. such as:

*import breeze.linalg.{DenseMatrix => BDM}

pairs.map(t => {
        val b1 = t._2._1.asInstanceOf[BDM[Double]]
        val b2 = t._2._2.asInstanceOf[BDM[Double]]
     
        val c = (b1 * b2).asInstanceOf[BDM[Double]]

        (new BlockID(t._1.row, t._1.column), c)
      })*
 
    Each node has 16 cores. However, no matter I set 16 tasks or more on
each node, the concurrency cannot be higher than 60%, which means not every
core on the node is computing. Then I check the running log on the WebUI,
according to the amount of shuffle read and write in every task, I see some
task do once matrix multiplication, some do twice while some do none.

    Thus, I think of using java multi thread to increase the concurrency. I
wrote a program in scala which calls java multi thread without Spark on a
single node, by watch the 'top' monitor, I find this program can use CPU up
to 1500% ( means nearly every core are computing). But I have no idea how to
use Java multi thread in RDD transformation.

    Is there any one can provide some example code to use Java multi thread
in RDD transformation, or give any idea to increase the concurrency ?

Thanks for all




--

---------------------------------------------------------------------


"
Yi Tian <tianyi.asiainfo@gmail.com>,"Sun, 28 Sep 2014 19:11:41 +0800",Re: How to use multi thread in RDD map function ?,myasuka <myasuka@live.com>,"for yarn-client mode:
 
SPARK_EXECUTOR_CORES * SPARK_EXECUTOR_INSTANCES = 2(or 3) * 
for standlone mode:

SPARK_WORKER_INSTANCES * SPARK_WORKER_CORES = 2(or 3) * 


Best Regards,

Yi Tian
tianyi.asiainfo@gmail.com





to do
on
every
WebUI,
some
concurrency. I
on a
CPU up
how to
thread
http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-use-multi-thread-in-RDD-map-function-tp8583.html
Nabble.com.

"
Egor Pahomov <pahomov.egor@gmail.com>,"Sun, 28 Sep 2014 19:44:38 +0400",Re: Workflow Scheduler for Spark,Reynold Xin <rxin@databricks.com>,"I created Jira <https://issues.apache.org/jira/browse/SPARK-3714> and design
doc
<https://docs.google.com/document/d/1q2Q8Ux-6uAkH7wtLJpc3jz-GfrDEjlbWlXtf20hvguk/edit?usp=sharing>
on
this matter.

2014-09-17 22:28 GMT+04:00 Reynold Xin <rxin@databricks.com>:




-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Du Li <lidu@yahoo-inc.com.INVALID>,"Sun, 28 Sep 2014 17:34:02 +0000",Re: SparkSQL: map type MatchError when inserting into Hive table,Cheng Lian <lian.cs.zju@gmail.com>,"It turned out a bug in my code. In the select clause the list of fields is
misaligned with the schema of the target table. As a consequence the map
data couldn‚Äôt be cast to some other type in the schema.

Thanks anyway.


On 9/26/14, 8:08 PM, ""Cheng Lian"" <lian.cs.zju@gmail.com> wrote:

>Would you mind to provide the DDL of this partitioned table together
>with the query you tried? The stacktrace suggests that the query was
>trying to cast a map into something else, which is not supported in
>Spark SQL. And I doubt whether Hive support casting a complex type to
>some other type.
>
>On 9/27/14 7:48 AM, Du Li wrote:
>> Hi,
>>
>> I was loading data into a partitioned table on Spark 1.1.0
>> beeline-thriftserver. The table has complex data types such as
>>map<string,
>> string> and array<map<string,string>>. The query is like ¬≥insert
>>overwrite
>> table a partition (≈†) select ≈†¬≤ and the select clause worked if run
>> separately. However, when running the insert query, there was an error
>>as
>> follows.
>>
>> The source code of Cast.scala seems to only handle the primitive data
>> types, which is perhaps why the MatchError was thrown.
>>
>> I just wonder if this is still work in progress, or I should do it
>> differently.
>>
>> Thanks,
>> Du
>>
>>
>> ----
>> scala.MatchError: MapType(StringType,StringType,true) (of class
>> org.apache.spark.sql.catalyst.types.MapType)
>>
>> 
>>org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala
>>:2
>> 47)
>>          
>>org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:247)
>>          
>>org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:263)
>>
>> 
>>org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.sca
>>la
>> :84)
>>
>> 
>>org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.ap
>>pl
>> y(Projection.scala:66)
>>
>> 
>>org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.ap
>>pl
>> y(Projection.scala:50)
>>          scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>          scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>
>> 
>>org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$
>>sq
>> 
>>l$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.s
>>ca
>> la:149)
>>
>> 
>>org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHi
>>ve
>> File$1.apply(InsertIntoHiveTable.scala:158)
>>
>> 
>>org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHi
>>ve
>> File$1.apply(InsertIntoHiveTable.scala:158)
>>          
>>org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
>>          org.apache.spark.scheduler.Task.run(Task.scala:54)
>>
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
>>
>> 
>>java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java
>>:1
>> 145)
>>
>> 
>>java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.jav
>>a:
>> 615)
>>          java.lang.Thread.run(Thread.java:722)
>>
>>
>>
>>
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>

"
Du Li <lidu@yahoo-inc.com.INVALID>,"Sun, 28 Sep 2014 18:59:10 +0000",view not supported in spark thrift server?,"""dev@spark.apache.org"" <dev@spark.apache.org>","
Can anybody confirm whether or not view is currently supported in spark? I found ìcreate view translateî in the blacklist of HiveCompatibilitySuite.scala and also the following scenario threw NullPointerException on beeline/thriftserver (1.1.0). Any plan to support it soon?



kv1.txt' into table src;



Error: java.lang.NullPointerException (state=,code=0)
"
Michael Armbrust <michael@databricks.com>,"Sun, 28 Sep 2014 12:13:42 -0700",Re: view not supported in spark thrift server?,Du Li <lidu@yahoo-inc.com.invalid>,"Views are not supported yet.  Its not currently on the near term roadmap,
but that can change if there is sufficient demand or someone in the
community is interested in implementing them.  I do not think it would be
very hard.

Michael


;
"
Du Li <lidu@yahoo-inc.com.INVALID>,"Sun, 28 Sep 2014 19:49:20 +0000",Re: view not supported in spark thrift server?,Michael Armbrust <michael@databricks.com>,"Thanks, Michael, for your quick response.

View is critical for my project that is migrating from shark to spark SQL. I have implemented and tested everything else. It would be perfect if view could be implemented soon.

Du


From: Michael Armbrust <michael@databricks.com<mailto:michael@databricks.com>>
Date: Sunday, September 28, 2014 at 12:13 PM
To: Du Li <lidu@yahoo-inc.com.invalid<mailto:lidu@yahoo-inc.com.invalid>>
Cc: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
Subject: Re: view not supported in spark thrift server?

Views are not supported yet.  Its not currently on the near term roadmap, but that can change if there is sufficient demand or someone in the community is interested in implementing them.  I do not think it would be very hard.

Michael


Can anybody confirm whether or not view is currently supported in spark? I found ìcreate view translateî in the blacklist of HiveCompatibilitySuite.scala and also the following scenario threw NullPointerException on beeline/thriftserver (1.1.0). Any plan to support it soon?



kv1.txt' into table src;



Error: java.lang.NullPointerException (state=,code=0)

"
Reynold Xin <rxin@databricks.com>,"Sun, 28 Sep 2014 12:56:35 -0700",Spark meetup on Oct 15 in NYC,"""dev@spark.apache.org"" <dev@spark.apache.org>, user@spark.apache.org","Hi Spark users and developers,

Some of the most active Spark developers (including Matei Zaharia, Michael
Armbrust, Joseph Bradley, TD, Paco Nathan, and me) will be in NYC for
Strata NYC. We are working with the Spark NYC meetup group and Bloomberg to
host a meetup event. This might be the event with the highest committer to
user ratio in the history of user meetups. Look forward to meeting more
users in NYC.

You can sign up for that here:
http://www.meetup.com/Spark-NYC/events/209271842/

Cheers.
"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 29 Sep 2014 00:21:22 -0700","Re: [MLlib] LogisticRegressionWithSGD and LogisticRegressionWithLBFGS
 converge with different weights.",Yanbo Liang <yanbohappy@gmail.com>,"The test accuracy doesn't mean the total loss. All points between (-1,
1) can separate points -1 and +1 and give you 1.0 accuracy, but their
coressponding loss are different. -Xiangrui


---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 29 Sep 2014 10:05:34 +0200","Re: [MLlib] LogisticRegressionWithSGD and LogisticRegressionWithLBFGS
 converge with different weights.",Yanbo Liang <yanbohappy@gmail.com>,"reason maybe SGD doesn't converge well and you can see that by
label of your training data is totally separable, so you can always
increase the log-likelihood by multiply a constant to the weights.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
Yanbo Liang <yanbohappy@gmail.com>,"Mon, 29 Sep 2014 17:45:20 +0800","Re: [MLlib] LogisticRegressionWithSGD and LogisticRegressionWithLBFGS
 converge with different weights.",DB Tsai <dbtsai@dbtsai.com>,"Thank you for all your patient response.

I can conclude that if the data is totally separable or over-fit occurs,
weights may be different.
And it also consistent with my experiment.

I have evaluate two different dataset and the result as followed:
Loss function: LogisticGradient
Regularizer: L2
regParam: 1.0
numIterations: 10000 (SGD)

Dataset 1: spark-1.1.0/data/mllib/sample_binary_classification_data.txt
# of classes: 2
# of samples: 100
# of features: 692
areaUnderROC of both SGD and LBFGS can reach nearly 1.0
Loss function of both optimization method converge
nearly 1.7147811767900675E-5 (very very small)
Weights of each optimization method is different but looks like multiple
relationship (not very strict) just as what DB Tsai mention above.  It
might be the dataset is totally separable.

Dataset 2:
http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#german.numer
# of classes: 2
# of samples: 1000
# of features: 24
areaUnderROC of both SGD and LBFGS both are nearly 0.8
Loss function of both optimization method converge nearly 0.5367041390107519
Weights of each optimization method is just the same.



2014-09-29 16:05 GMT+08:00 DB Tsai <dbtsai@dbtsai.com>:

"
myasuka <myasuka@live.com>,"Mon, 29 Sep 2014 06:06:31 -0700 (PDT)",Re: How to use multi thread in RDD map function ?,dev@spark.incubator.apache.org,"Our cluster is a standalone cluster with 16 computing nodes, each node has 16
cores. I set SPARK_WORKER_INSTANCES to 1, and set SPARK_WORKER_CORES to 32,
we give 512 tasks all together, this situation can help increase the
concurrency. But if I  set SPARK_WORKER_INSTANCES to 2, SPARK_WORKER_CORES
to 16, this dosen't work well.

Thank you for your reply.


Yi Tian wrote













--

---------------------------------------------------------------------


"
Yi Tian <tianyi.asiainfo@gmail.com>,"Mon, 29 Sep 2014 21:44:01 +0800",Re: How to use multi thread in RDD map function ?,myasuka <myasuka@live.com>,"Hi, myasuka

Have you checked the jvm gc time of each executor? 

I think you should increase the SPARK_EXECUTOR_CORES or SPARK_EXECUTOR_INSTANCES until you get the enough concurrency.

Here is my recommend config:

SPARK_EXECUTOR_CORES=8
SPARK_EXECUTOR_INSTANCES=4
SPARK_WORKER_MEMORY=8G

note: make sure you got enough memory on each node, more than SPARK_EXECUTOR_INSTANCES * SPARK_WORKER_MEMORY

Best Regards,

Yi Tian
tianyi.asiainfo@gmail.com





has 16
to 32,
SPARK_WORKER_CORES
matrices to
on
not
WebUI,
see
none.
concurrency.
on a
CPU
idea how
?
http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-use-multi-thread-in-RDD-map-function-tp8583.html
---------------------------------------------------------------------
http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-use-multi-thread-in-RDD-map-function-tp8583p8594.html
Nabble.com.

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 29 Sep 2014 11:31:56 -0700",BasicOperationsSuite failing ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
Running test suite in trunk, I got:

^[[32mBasicOperationsSuite:^[[0m
^[[32m- map^[[0m
^[[32m- flatMap^[[0m
^[[32m- filter^[[0m
^[[32m- glom^[[0m
^[[32m- mapPartitions^[[0m
^[[32m- repartition (more partitions)^[[0m
^[[32m- repartition (fewer partitions)^[[0m
^[[32m- groupByKey^[[0m
^[[32m- reduceByKey^[[0m
^[[32m- reduce^[[0m
^[[32m- count^[[0m
^[[32m- countByValue^[[0m
^[[32m- mapValues^[[0m
^[[32m- flatMapValues^[[0m
^[[32m- union^[[0m
^[[32m- StreamingContext.union^[[0m
^[[32m- transform^[[0m
^[[32m- transformWith^[[0m
^[[32m- StreamingContext.transform^[[0m
^[[32m- cogroup^[[0m
^[[32m- join^[[0m
^[[32m- leftOuterJoin^[[0m
^[[32m- rightOuterJoin^[[0m
^[[32m- fullOuterJoin^[[0m
^[[32m- updateStateByKey^[[0m
^[[32m- updateStateByKey - object lifecycle^[[0m
^[[32m- slice^[[0m
^[[32m- slice - has not been initialized^[[0m
^[[32m- rdd cleanup - map and window^[[0m
^[[32m- rdd cleanup - updateStateByKey^[[0m
^[[31m- rdd cleanup - input blocks and persisted RDDs *** FAILED ***^[[0m
^[[31m  org.scalatest.exceptions.TestFailedException was thrown.
(BasicOperationsSuite.scala:528)^[[0m

However, using sbt for this testsuite, it seemed to pass:

[info] - slice - has not been initialized
[info] - rdd cleanup - map and window
[info] - rdd cleanup - updateStateByKey
Exception in thread ""Thread-561"" org.apache.spark.SparkException: Job
cancelled because SparkContext was shut down
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:701)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:700)
at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
at
org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:700)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor.postStop(DAGScheduler.scala:1406)
at
akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:201)
at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:163)
at akka.actor.ActorCell.terminate(ActorCell.scala:338)
at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431)
at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
at akka.dispatch.Mailbox.run(Mailbox.scala:218)
at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[info] - rdd cleanup - input blocks and persisted RDDs
[info] ScalaTest
[info] Run completed in 1 minute, 1 second.
[info] Total number of tests run: 31
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 31, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 31, Failed 0, Errors 0, Passed 31
java.lang.AssertionError: assertion failed: List(object package$DebugNode,
object package$DebugNode)
at scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)
at
scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:2988)
at
scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:2991)
at
scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenASM.scala:1371)
at scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:120)
at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$mcV$sp(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:166)
at
sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.scala:98)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)

Any comment ?

Thanks
"
shane knapp <sknapp@berkeley.edu>,"Mon, 29 Sep 2014 13:43:44 -0700","jenkins downtime/system upgrade wednesday morning, 730am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","happy monday, everyone!

remember a few weeks back when i upgraded jenkins, and unwittingly began
DOSing our system due to massive log spam?

well, that bug has been fixed w/the current release and i'd like to get our
logging levels back to something more verbose that we have now.

downtime will be from 730am-1000am PDT (i do expect this to be done well
before 1000am)

the update will be from 1.578 -> 1.582

changelog here:  http://jenkins-ci.org/changelog

please let me know if there are any questions or concerns.  thanks!

shane, your friendly devops engineer
"
shane knapp <sknapp@berkeley.edu>,"Mon, 29 Sep 2014 14:20:09 -0700",FYI: i've doubled the jenkins executors for every build node,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","we were running at 8 executors per node, and BARELY even stressing the
machines (32 cores, ~230G RAM).

in the interest of actually using system resources, and giving ourselves
some headroom, i upped the executors to 16 per node.  i'll be keeping an
eye on ganglia for the rest of the week to make sure everything's cool.

i hope you all enjoy your freshly allocated capacity!  :)

shane
"
Reynold Xin <rxin@databricks.com>,"Mon, 29 Sep 2014 14:25:15 -0700",Re: FYI: i've doubled the jenkins executors for every build node,shane knapp <sknapp@berkeley.edu>,"Thanks. We might see more failures due to contention on resources. Fingers
acrossed ... At some point it might make sense to run the tests in a VM or
container.



"
shane knapp <sknapp@berkeley.edu>,"Mon, 29 Sep 2014 14:31:46 -0700",Re: FYI: i've doubled the jenkins executors for every build node,Reynold Xin <rxin@databricks.com>,"yeah, this is why i'm gonna keep a close eye on things this week...

as for VMs vs containers, please do the latter more than the former.  one
of our longer-term plans here at the lab is to move most of our jenkins
infra to VMs, and running tests w/nested VMs is Bad[tm].


"
Lochana Menikarachchi <lochanac@gmail.com>,"Tue, 30 Sep 2014 08:15:58 +0530",Hyper Parameter Optimization Algorithms,dev@spark.apache.org,"Hi,

Is there anyone who works on hyper parameter optimization algorithms? If 
not, is there any interest on the subject. We are thinking about 
implementing some of these algorithms and contributing to spark? thoughts?

Lochana

---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 29 Sep 2014 23:00:59 -0400","Re: jenkins downtime/system upgrade wednesday morning, 730am
 PDT",shane knapp <sknapp@berkeley.edu>,"Just noticed these lines in the jenkins log 

========================================================================= Running Apache RAT checks ========================================================================= Attempting to fetch rat Launching rat from /home/jenkins/workspace/SparkPullRequestBuilder/lib/apache-rat-0.10.jar Error: Invalid or corrupt jarfile /home/jenkins/workspace/SparkPullRequestBuilder/lib/apache-rat-0.10.jar RAT checks passed.

Something wrong?

Best, 

-- 
Nan Zhu




"
Debasish Das <debasish.das83@gmail.com>,"Mon, 29 Sep 2014 19:48:30 -0700",Re: Hyper Parameter Optimization Algorithms,Lochana Menikarachchi <lochanac@gmail.com>,"You should look into Evan Spark's talk from Spark Summit 2014

http://spark-summit.org/2014/talk/model-search-at-scale

I am not sure if some of it is already open sourced through MLBase...


"
"""Haopu Wang"" <HWang@qilinsoft.com>","Tue, 30 Sep 2014 11:36:55 +0800",Spark SQL question: why build hashtable for both sides in HashOuterJoin?,<dev@spark.apache.org>,"I take a look at HashOuterJoin and it's building a Hashtable for both
sides.

This consumes quite a lot of memory when the partition is big. And it
doesn't reduce the iteration on streamed relation, right?

Thanks!

---------------------------------------------------------------------


"
Liquan Pei <liquanpei@gmail.com>,"Mon, 29 Sep 2014 21:31:23 -0700",Re: Spark SQL question: why build hashtable for both sides in HashOuterJoin?,Haopu Wang <HWang@qilinsoft.com>,"Hi Haopu,

My understanding is that the hashtable on both left and right side is used
for including null values in result in an efficient manner. If hash table
is only built on one side, let's say left side and we perform a left outer
join, for each row in left side, a scan over the right side is needed to
make sure that no matching tuples for that row on left side.

Hope this helps!
Liquan




-- 
Liquan Pei
Department of Physics
University of Massachusetts Amherst
"
"""Haopu Wang"" <HWang@qilinsoft.com>","Tue, 30 Sep 2014 14:47:37 +0800",RE: Spark SQL question: why build hashtable for both sides in HashOuterJoin?,"""Liquan Pei"" <liquanpei@gmail.com>","Hi, Liquan, thanks for the response.

 

In your example, I think the hash table should be built on the ""right"" side, so Spark can iterate through the left side and find matches in the right side from the hash table efficiently. Please comment and suggest, thanks again!

 

________________________________

From: Liquan Pei [mailto:liquanpei@gmail.com] 
Sent: 2014ƒÍ9‘¬30»’ 12:31
To: Haopu Wang
Cc: dev@spark.apache.org; user
Subject: Re: Spark SQL question: why build hashtable for both sides in HashOuterJoin?

 

Hi Haopu,

 

My understanding is that the hashtable on both left and right side is used for including null values in result in an efficient manner. If hash table is only built on one side, let's say left side and we perform a left outer join, for each row in left side, a scan over the right side is needed to make sure that no matching tuples for that row on left side. 

 

Hope this helps!

Liquan

 


I take a look at HashOuterJoin and it's building a Hashtable for both
sides.

This consumes quite a lot of memory when the partition is big. And it
doesn't reduce the iteration on streamed relation, right?

Thanks!

---------------------------------------------------------------------





 

-- 
Liquan Pei 
Department of Physics 
University of Massachusetts Amherst 

"
Debasish Das <debasish.das83@gmail.com>,"Tue, 30 Sep 2014 01:25:01 -0700",Cluster tests failing,dev <dev@spark.apache.org>,"Hi,

Inside mllib I am running tests using:

mvn -Dhadoop.version=2.3.0-cdh5.1.0 -Phadoop-2.3 -Pyarn install

The locat tests run fine but cluster tests are failing..

LBFGSClusterSuite:

- task size should be small *** FAILED ***

  org.apache.spark.SparkException: Job aborted due to stage failure: Master
removed our application: FAILED

Do I need to start a localhost spark cluster first before running these
tests that has ClusterSuite in them ?

Thanks.

Deb
"
scwf <wangfei1@huawei.com>,"Tue, 30 Sep 2014 17:37:53 +0800",Re: Cluster tests failing,<dev@spark.apache.org>,"first run cmd mvn clean, then try again




---------------------------------------------------------------------


"
Liquan Pei <liquanpei@gmail.com>,"Tue, 30 Sep 2014 03:33:59 -0700",Re: Spark SQL question: why build hashtable for both sides in HashOuterJoin?,Haopu Wang <HWang@qilinsoft.com>,"Hi Haopu,

case.

Liquan


d
r



-- 
Liquan Pei
Department of Physics
University of Massachusetts Amherst
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 30 Sep 2014 08:23:32 -0700",Re: Cluster tests failing,scwf <wangfei1@huawei.com>,"I have done mvn clean several times...

Consistently all the mllib tests that are using
LocalClusterSparkContext.scala, they fail !
"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 30 Sep 2014 10:11:35 -0700",Re: Cluster tests failing,Debasish Das <debasish.das83@gmail.com>,"Try to build the assembly jar first. ClusterSuite uses local-cluster
mode, which requires the assembly jar. -Xiangrui


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 30 Sep 2014 10:22:31 -0700","Re: jenkins downtime/system upgrade wednesday morning, 730am PDT",Nan Zhu <zhunanmcgill@gmail.com>,"(this time, reply to all)

nice catch.  there's a bug in spark/dev/check-license, which i've confirmed
from the CLI.  i'll open a bug and PR to fix it.


"
shane knapp <sknapp@berkeley.edu>,"Tue, 30 Sep 2014 10:34:40 -0700","Re: jenkins downtime/system upgrade wednesday morning, 730am PDT",Nan Zhu <zhunanmcgill@gmail.com>,"https://issues.apache.org/jira/browse/SPARK-3745


"
Ameet Talwalkar <atalwalkar@gmail.com>,"Tue, 30 Sep 2014 14:35:48 -0700",Re: Hyper Parameter Optimization Algorithms,Debasish Das <debasish.das83@gmail.com>,"Hi Lochana,

We are indeed working on hyperparameter optimization as part of the MLbase
<http://www.mlbase.org/> project.  We are writing a paper about this work
right now, and also plan to eventually open-source our code.

-Ameet


"
shane knapp <sknapp@berkeley.edu>,"Tue, 30 Sep 2014 14:40:52 -0700","Re: jenkins downtime/system upgrade wednesday morning, 730am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:  this is happening tomorrow morning.  i will be putting jenkins
in to quiet mode at ~7am, and then doing the upgrade once any stray builds
finish.


"
