Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 01 Nov 2015 03:17:52 +0000",Downloading Hadoop from s3://spark-related-packages/,Spark dev list <dev@spark.apache.org>,"https://s3.amazonaws.com/spark-related-packages/

spark-ec2 uses this bucket to download and install HDFS on clusters. Is it
owned by the Spark project or by the AMPLab?

Anyway, it looks like the latest Hadoop install available on there is
Hadoop 2.4.0.

Are there plans to add newer versions of Hadoop for use by spark-ec2 and
similar tools, or should we just be getting that stuff via an Apache mirror
<http://hadoop.apache.org/releases.html>? The latest version is 2.7.1, by
the way.

The problem with the Apache mirrors, if I am not mistaken, is that you
cannot use a single URL that automatically redirects you to a working
mirror to download Hadoop. You have to pick a specific mirror and pray it
doesn't disappear tomorrow.

Nick
"
Reynold Xin <rxin@databricks.com>,"Sun, 1 Nov 2015 08:22:39 +0100",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Sjoerd Mulder <sjoerdmulder@gmail.com>,"Thanks for reporting it, Sjoerd. You might have a different version of
Janino brought in from somewhere else.

This should fix your problem: https://github.com/apache/spark/pull/9372

Can you give it a try?




"
Chenxi Li <spiritlcx@gmail.com>,"Sun, 1 Nov 2015 09:09:58 +0100",unscribe,dev@spark.apache.org,"unscribe
"
Steve Loughran <stevel@hortonworks.com>,"Sun, 1 Nov 2015 10:30:00 +0000",Re: Downloading Hadoop from s3://spark-related-packages/,Nicholas Chammas <nicholas.chammas@gmail.com>,"

https://s3.amazonaws.com/spark-related-packages/

spark-ec2 uses this bucket to download and install HDFS on clusters. Is it owned by the Spark project or by the AMPLab?

Anyway, it looks like the latest Hadoop install available on there is Hadoop 2.4.0.

Are there plans to add newer versions of Hadoop for use by spark-ec2 and similar tools, or should we just be getting that stuff via an Apache mirror<http://hadoop.apache.org/releases.html>? The latest version is 2.7.1, by the way.


you should be grabbing the artifacts off the ASF and then verifying their SHA1 checksums as published on the ASF HTTPS web site


The problem with the Apache mirrors, if I am not mistaken, is that you cannot use a single URL that automatically redirects you to a working mirror to download Hadoop. You have to pick a specific mirror and pray it doesn't disappear tomorrow.


They don't go away, especially http://mirror.ox.ac.uk , and in the us the apache.osuosl.org<http://apache.osuosl.org>, osu being a where a lot of the ASF servers are kept.

full list with availability stats

http://www.apache.org/mirrors/


"
Sean Owen <sowen@cloudera.com>,"Sun, 1 Nov 2015 12:16:49 +0000",Re: Spark 1.6 Release Schedule,Michael Armbrust <michael@databricks.com>,"I like the idea, but I think there's already a lot of triage backlog. Can
we more concretely address this now and during the next two weeks?

1.6.0 stats from JIRA:

344 issues targeted at 1.6.0, of which
  253 are from committers, of which
    215 are improvements/other, of which
       5 are blockers
    38 are bugs, of which
       4 are blockers
      11 are critical

Tip: It's really easy to manage saved queries for this and other things
with the free JIRA Client (http://almworks.com/jiraclient/overview.html)
that now works with Java 8.

It still looks like a lot for a point where 1.6.0 is supposed to be being
tested in theory. Lots of (most?) things that were said to be done for
1.6.0 for several months aren't going to be, and that still surprises me as
a software development practice.

Well, life is busy and chaotic out here in OSS land. I'd still like to push
even more on lightweight triage and release planning, centering around
Target Version, if only to make visible what's happening with intention and
reality:

1. Any JIRAs that seem to have been targeted at 1.6.0 by a non-committer
are untargeted, as they shouldn't be to begin with

2. This week, maintainers and interested parties review all JIRAs targeted
at 1.6.0 and untarget/retarget accordingly

3. Start of next week (the final days before an RC), non-Blocker non-bugs
untargeted, or in a few cases pushed to 1.6.1 or beyond

4. After next week, non-Blocker and non-Critical bugs are pushed, as the RC
is then late.

5. No release candidate until no Blockers are open.

6. (Repeat 1 and 2 more regularly through the development period for 1.7
instead of at the end.)


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 1 Nov 2015 06:11:00 -0800",Re: unscribe,Chenxi Li <spiritlcx@gmail.com>,"Please take a look at first section of spark.apache.org/community

FYI


"
Romi Kuntsman <romi@totango.com>,"Sun, 1 Nov 2015 18:08:23 +0200","Some spark apps fail with ""All masters are unresponsive"", while
 others pass normally","""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","[adding dev list since it's probably a bug, but i'm not sure how to
reproduce so I can open a bug about it]

Hi,

I have a standalone Spark 1.4.0 cluster with 100s of applications running
every day.

below)
But at the same time (and also after that), other applications are running,
so I can safely assume the master and workers are working.

1. why is there a NullPointerException? (i can't track the scala stack
trace to the code, but anyway NPE is usually a obvious bug even if there's
actually a network error...)
2. why can't it connect to the master? (if it's a network timeout, how to
increase it? i see the values are hardcoded inside AppClient)
3. how to recover from this error?


  ERROR 01-11 15:32:54,991    SparkDeploySchedulerBackend - Application has
been killed. Reason: All masters are unresponsive! Giving up. ERROR
logs/error.log
  java.lang.NullPointerException NullPointerException
      at
org.apache.spark.deploy.client.AppClient$ClientActor$$anonfun$receiveWithLogging$1.applyOrElse(AppClient.scala:160)
      at
scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
      at
scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
      at
scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
      at
org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:59)
      at
org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:42)
      at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
      at
org.apache.spark.util.ActorLogReceive$$anon$1.applyOrElse(ActorLogReceive.scala:42)
      at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
      at
org.apache.spark.deploy.client.AppClient$ClientActor.aroundReceive(AppClient.scala:61)
      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
      at akka.actor.ActorCell.invoke(ActorCell.scala:487)
      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
      at akka.dispatch.Mailbox.run(Mailbox.scala:220)
      at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
      at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
      at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
      at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
      at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
  ERROR 01-11 15:32:55,603                   SparkContext - Error
initializing SparkContext. ERROR
  java.lang.IllegalStateException: Cannot call methods on a stopped
SparkContext
      at org.apache.spark.SparkContext.org
$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:103)
      at
org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:1501)
      at
org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2005)
      at org.apache.spark.SparkContext.<init>(SparkContext.scala:543)
      at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)


Thanks!

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sun, 1 Nov 2015 09:25:15 -0800",Re: Downloading Hadoop from s3://spark-related-packages/,Steve Loughran <stevel@hortonworks.com>,"I think that getting them from the ASF mirrors is a better strategy in
general as it'll remove the overhead of keeping the S3 bucket up to
date. It works in the spark-ec2 case because we only support a limited
number of Hadoop versions from the tool. FWIW I don't have write
access to the bucket and also haven't heard of any plans to support
newer versions in spark-ec2.

Thanks
Shivaram


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 01 Nov 2015 22:16:39 +0000",Re: Downloading Hadoop from s3://spark-related-packages/,"shivaram@eecs.berkeley.edu, Steve Loughran <stevel@hortonworks.com>","OK, I’ll focus on the Apache mirrors going forward.

The problem with the Apache mirrors, if I am not mistaken, is that you
cannot use a single URL that automatically redirects you to a working
mirror to download Hadoop. You have to pick a specific mirror and pray it
doesn’t disappear tomorrow.

They don’t go away, especially http://mirror.ox.ac.uk , and in the us the
apache.osuosl.org, osu being a where a lot of the ASF servers are kept.

So does Apache offer no way to query a URL and automatically get the
closest working mirror? If I’m installing HDFS onto servers in various EC2
regions, the best mirror will vary depending on my location.

Nick
​


d
ir
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sun, 1 Nov 2015 14:32:30 -0800",Re: Downloading Hadoop from s3://spark-related-packages/,Nicholas Chammas <nicholas.chammas@gmail.com>,"ror
’t
e us the
est
2 regions,
Not sure if this is officially documented somewhere but if you pass
'&asjson=1' you will get back a JSON which has a 'preferred' field set
to the closest mirror.

Shivaram
s
nd
.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 01 Nov 2015 23:18:26 +0000",Re: Downloading Hadoop from s3://spark-related-packages/,shivaram@eecs.berkeley.edu,"Oh, sweet! For example:

http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz?asjson=1

Thanks for sharing that tip. Looks like you can also use as_json
<https://svn.apache.org/repos/asf/infrastructure/site/trunk/content/dyn/mirrors/mirrors.cgi>
(vs. asjson).

Nick
​


the us
EC2
m
s
ou
g
s
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sun, 1 Nov 2015 15:40:23 -0800",Re: Downloading Hadoop from s3://spark-related-packages/,Nicholas Chammas <nicholas.chammas@gmail.com>,"I think the lua one at
https://svn.apache.org/repos/asf/infrastructure/site/trunk/content/dyn/closer.lua
has replaced the cgi one from before. Also it looks like the lua one
also supports `action=download` with a filename argument. So you could
just do something like

wget http://www.apache.org/dyn/closer.lua?filename=hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz&action=download

Thanks
Shivaram

7.1.tar.gz?asjson=1
 the us
.
 EC2
n
d
m>
.
is
2
e
ng
us

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 02 Nov 2015 00:08:06 +0000",Re: Downloading Hadoop from s3://spark-related-packages/,shivaram@eecs.berkeley.edu,"Hmm, yeah, some Googling confirms this, though there isn't any clear
documentation about this.

Strangely, if I click on the link from your email the download works, but
curl and wget somehow don't get redirected correctly...

Nick


oser.lua
1/hadoop-2.7.1.tar.gz&action=download
7.1.tar.gz?asjson=1
ou
g
in the us
us EC2
et
e
ng
t
t
"
DB Tsai <dbtsai@dbtsai.com>,"Sun, 1 Nov 2015 19:12:14 -0800",Re: [Spark MLlib] about linear regression issue,Zhiliang Zhu <zchl.jump@yahoo.com>,"For the constrains like all weights >=0, people do LBFGS-B which is
supported in our optimization library, Breeze.
https://github.com/scalanlp/breeze/issues/323

However, in Spark's LiR, our implementation doesn't have constrain
implementation. I do see this is useful given we're experimenting
SLIM: Sparse Linear Methods for recommendation,
http://www-users.cs.umn.edu/~xning/papers/Ning2011c.pdf which requires
all the weights to be positive (Eq. 3) to represent positive relations
between items.

In summary, it's possible and not difficult to add this constrain to
our current linear regression, but currently, there is no open source
implementation in Spark.

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D



---------------------------------------------------------------------


"
Disha Shrivastava <dishu.905@gmail.com>,"Mon, 2 Nov 2015 10:29:34 +0530",Implementation of RNN/LSTM in Spark,dev@spark.apache.org,"Hi,

I wanted to know if someone is working on implementing RNN/LSTM in Spark or
has already done. I am also willing to contribute to it and get some
guidance on how to go about it.

Thanks and Regards
Disha
Masters Student, IIT Delhi
"
Sasaki Kai <lewuathe@me.com>,"Mon, 02 Nov 2015 14:22:34 +0900",Re: Implementation of RNN/LSTM in Spark,Disha Shrivastava <dishu.905@gmail.com>,"Hi, Disha

There seems to be no JIRA on RNN/LSTM directly. But there were several tickets about other type of networks regarding deep learning.

Stacked Auto Encoder
https://issues.apache.org/jira/browse/SPARK-2623 <https://issues.apache.org/jira/browse/SPARK-2623>
CNN
https://issues.apache.org/jira/browse/SPARK-9129 <https://issues.apache.org/jira/browse/SPARK-9129>
https://issues.apache.org/jira/browse/SPARK-9273 <https://issues.apache.org/jira/browse/SPARK-9273>

Roadmap of MLlib deep learning
https://issues.apache.org/jira/browse/SPARK-5575 <https://issues.apache.org/jira/browse/SPARK-5575>

I think it may be good to join the discussion on SPARK-5575. 
Best

Kai Sasaki


Spark or has already done. I am also willing to contribute to it and get some guidance on how to go about it.

"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 2 Nov 2015 13:18:59 +0530",Re: Unable to run applications on spark in standalone cluster mode,Rohith P <rparameshwara@couponsinc.com>,"Can you paste the contents of your spark-env.sh file? Also would be good to
have a look at the /etc/hosts file. Cannot bind to the given ip address can
be resolved if you put the hostname instead of the ip address. Also make
sure the configuration (conf directory) across your cluster have the same
contents.

Thanks
Best Regards


"
Romi Kuntsman <romi@totango.com>,"Mon, 2 Nov 2015 10:12:40 +0200",Re: Getting Started,Saurabh Shah <shahsaurabh0103@gmail.com>,"https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Romi Kuntsman <romi@totango.com>,"Mon, 2 Nov 2015 10:14:30 +0200",Re: Getting Started,dev <dev@spark.apache.org>,"wait, this is an identical email like was from ""Aadi Thakar <
thakkar.aadi2@gmail.com>"" a day before
could it be a spambot?

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Rohith P <rparameshwara@couponsinc.com>,"Mon, 2 Nov 2015 01:47:56 -0700 (MST)",Re: Unable to run applications on spark in standalone cluster mode,dev@spark.apache.org,"The contents of spark-env.sh is :
SPARK_MASTER_IP=marvin.spark.ins-01
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_WORKER_WEBUI_PORT=8081
SPARK_WORKER_INSTANCES=1
SPARK_LOCAL_IP=marvin.spark.ins-01



The contents of etc/hosts is 
172.28.161.33           marvin.base.ins-01              ip-172-28-161-33

172.28.161.200          marvin.cassandra.ins-01         ip-172-28-161-200
172.28.161.201          marvin.cassandra.ins-02         ip-172-28-161-201

172.28.161.138          marvin.spark.ins-01             ip-172-28-161-138
172.28.161.139          marvin.spark.ins-02             ip-172-28-161-139
172.28.161.140          marvin.spark.ins-03             ip-172-28-161-140
172.28.161.141          marvin.spark.ins-04             ip-172-28-161-141





--

---------------------------------------------------------------------


"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Mon, 2 Nov 2015 16:03:56 +0530",Lead operator not working as aggregation operator,dev@spark.apache.org,"Hi! I was trying out window functions in SparkSql (using hive context) and
I noticed that while this
<https://issues.apache.org/jira/browse/TAJO-919?jql=text%20~%20%22lag%20window%22>
mentions that *lead* is implemented as an aggregate operator, it seems not
to be the case.

I am using the following configuration:

Query : SELECT lead(max(`expenses`)) FROM `table` GROUP BY `customerId`
Spark Version: 10.4
SparkSql Version: 1.5.1

I am using the standard example of (`customerId`, `expenses`) scheme where
each customer has multiple values for expenses (though I am setting age as
Double and not Int as I am trying out maths functions).


*java.lang.NullPointerException at
org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.evaluate(GenericUDFLeadLag.java:57)*

The entire error stack can be found here <http://pastebin.com/jTRR4Ubx>.

Can someone confirm if this is an actual issue or some oversight on my part?

Thanks!
"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Mon, 2 Nov 2015 16:08:10 +0530",Re: Lead operator not working as aggregation operator,dev@spark.apache.org,"I was referring to this jira issue :
https://issues.apache.org/jira/browse/TAJO-919


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Mon, 2 Nov 2015 11:45:58 +0100",Re: Lead operator not working as aggregation operator,Shagun Sodhani <sshagunsodhani@gmail.com>,"Hi,

This is more a question for the User list.

Lead and Lag imply ordering of the whole dataset, and this is not
supported. You can use Lead/Lag in an ordered window function and you'll be
fine:

*select lead(max(expenses)) over (order by customerId) from tbl group by
customerId*

HTH

Met vriendelijke groet/Kind regards,

Herman van Hövell tot Westerflier

QuestTec B.V.
Torenwacht 98
2353 DC Leiderdorp
hvanhovell@questtec.nl
+31 6 420 590 27


2015-11-02 11:33 GMT+01:00 Shagun Sodhani <sshagunsodhani@gmail.com>:

0window%22>
e
s
DFLeadLag.java:57)*
"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Mon, 2 Nov 2015 16:20:59 +0530",Re: Lead operator not working as aggregation operator,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"I get the part about using it with window, but most other window operators
also work as aggregator operator and in this case, it is specifically
mentioned in the jira issue as well. I asked on dev list and not user list
as it was already mentioned in the issue.


be
20window%22>
g
UDFLeadLag.java:57)*
"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 2 Nov 2015 10:00:15 -0300",Re: Downloading Hadoop from s3://spark-related-packages/,shivaram@eecs.berkeley.edu,"I am getting the same results using closer.lua versus close.cgi, which
seems to be downloading a page where the user can choose the closest
mirror. I tried to add parameters to follow redirect without much success.
There seems to be already a jira for a similar request with infra:
https://issues.apache.org/jira/browse/INFRA-10240.

A workaround is to use a url pointing to the mirror directly.

curl -O -L
http://ftp.unicamp.br/pub/apache/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz

I second the lack of documentation on what is available with these scripts,
I'll see if I can find the source and try to see other options.



oser.lua
1/hadoop-2.7.1.tar.gz&action=download
7.1.tar.gz?asjson=1
ou
g
in the us
us EC2
et
e
ng
t
t


-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
YiZhi Liu <javelinjs@gmail.com>,"Mon, 2 Nov 2015 23:32:59 +0800",Re: Ability to offer initial coefficients in ml.LogisticRegression,"DB Tsai <dbtsai@dbtsai.com>, Holden Karau <holden@pigscanfly.ca>","Hi Tsai,

Is it proper if I create a jira and try to work on it?

2015-10-23 10:40 GMT+08:00 YiZhi Liu <javelinjs@gmail.com>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 2 Nov 2015 07:42:52 -0800",Re: test failed due to OOME,Mridul Muralidharan <mridul@gmail.com>,"Looks like SparkListenerSuite doesn't OOM on QA runs compared to Jenkins
builds.

I wonder if this is due to difference between machines running QA tests vs
machines running Jenkins builds.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Nov 2015 09:53:25 -0800",Re: test failed due to OOME,Ted Yu <yuzhihong@gmail.com>,"I believe this is some bug in our tests. For some reason we are using way
more memory than necessary. We'll probably need to log into Jenkins and
heap dump some running tests and figure out what is going on.


"
shane knapp <sknapp@berkeley.edu>,"Mon, 2 Nov 2015 09:55:11 -0800","[BUILD SYSTEM] quick jenkins downtime, november 5th 7am","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i'd like to take jenkins down briefly thursday morning to install some
plugin updates.

this will hopefully be short (~1hr), but could easily become longer as
the jenkins plugin ecosystem is fragile and updates like this are
known to cause things to explode.  the only reason why i'm
contemplating this, is i'm having some issues with the git plugin on
new github pull request builder builds.

i'll send updates as things progress.

thanks,

shane

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 2 Nov 2015 09:56:23 -0800",Re: test failed due to OOME,Patrick Wendell <pwendell@gmail.com>,"I have a PR which tries to address this issue:
https://github.com/apache/spark/pull/9384

Comment is welcome.


"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 2 Nov 2015 11:30:41 -0800",Re: Ability to offer initial coefficients in ml.LogisticRegression,YiZhi Liu <javelinjs@gmail.com>,"Hi YiZhi,

Sure. I think Holden already created a JIRA for this. Please
coordinate with Holden, and keep me in the loop. Thanks.

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D



---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 2 Nov 2015 19:14:25 -0800",Ability to offer initial coefficients in ml.LogisticRegression,DB Tsai <dbtsai@dbtsai.com>,"Hi YiZhi,

I've been waiting on the shared param to go in (I think it was kmeans) so
we could have a common API. I think the issue is SPARK-7852 but I am on
mobile right now.

Cheers,

Holden :)




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
YiZhi Liu <javelinjs@gmail.com>,"Tue, 3 Nov 2015 12:04:21 +0800",Re: Ability to offer initial coefficients in ml.LogisticRegression,Holden Karau <holden@pigscanfly.ca>,"Hi Holden,

Yep the issue id is correct. It seems that you're waiting for
SPARK-11136 which Jayant is working on?

Best,
Yizhi

2015-11-03 11:14 GMT+08:00 Holden Karau <holden@pigscanfly.ca>:



-- 
Yizhi Liu
Senior Software Engineer / Data Mining
www.mvad.com, Shanghai, China

---------------------------------------------------------------------


"
Renjie Liu <liurenjie2008@gmail.com>,"Tue, 03 Nov 2015 07:06:29 +0000",Re: Guaranteed processing orders of each batch in Spark Streaming,dev@spark.apache.org,"Hi, all:
I have given a detailed description of my proposal in this jira
<https://issues.apache.org/jira/browse/SPARK-11308>.


Liu, Renjie
Software Engineer, MVAD
"
canan chen <ccnfdu@gmail.com>,"Tue, 3 Nov 2015 16:25:07 +0800","Anyone has perfect solution for spark source code compilation issue
 on intellij",dev@spark.apache.org,"Hi folks,

I often meet the spark compilation issue on intellij. It wastes me lots of
time. I googled it and found someone else also meet similar issue, but
seems no perfect solution for now. but still wondering anyone here has
perfect solution for that. The issue happens sometimes, I don't know what
cause this. It even happens sometimes when I import a new copy of clean
spark source code.

I have tried lots of ways (like sbt clean, clean intellij files and
reimport etc ), none of them can resolve this issue.

Here's some error message in intellij.

Error:scala:
     while compiling:
/Users/hadoop/github/spark_2/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala
        during phase: jvm
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args: -nobootcp -javabootclasspath : -deprecation -feature
-classpath
"
Holden Karau <holden@pigscanfly.ca>,"Tue, 3 Nov 2015 00:25:55 -0800",Re: Ability to offer initial coefficients in ml.LogisticRegression,YiZhi Liu <javelinjs@gmail.com>,"Thats correct :)





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Stefano Baghino <stefano.baghino@radicalbit.io>,"Tue, 3 Nov 2015 09:27:24 +0100",Running individual test classes,dev@spark.apache.org,"Hi all,

I'm new to contributing to Spark (and Apache projects in general); I've
started working on SPARK-7425
<https://issues.apache.org/jira/browse/SPARK-7425> and have implemented
what looks like a viable solution. Now I'd like to test it, however I'm
having some trouble running an individual test class to quickly iterate
over it; I tried running

mvn -Dtest=org.apache.spark.ml.ProbabilisticClassifierSuite test

and (without the fully qualified class name)

mvn -Dtest=ProbabilisticClassifierSuite test

but both commands resulted in running all tests, both when launching Maven
from the project root and from the MLlib module root. I've tried to look
this up in the mailing list archives but haven't had luck so far.

How can I run a single test suite? Thanks in advance!

-- 
BR,
Stefano Baghino
"
Michael Armbrust <michael@databricks.com>,"Tue, 3 Nov 2015 10:12:03 +0100",Re: Running individual test classes,Stefano Baghino <stefano.baghino@radicalbit.io>,"In SBT:

build/sbt ""mllib/test-only *ProbabilisticClassifierSuite""


"
Nitin Goyal <nitin2goyal@gmail.com>,"Tue, 3 Nov 2015 14:54:53 +0530",Re: Running individual test classes,Michael Armbrust <michael@databricks.com>,"In maven, you might want to try following :-

-DwildcardSuites=org.apache.spark.ml.ProbabilisticClassifierSuite




-- 
Regards
Nitin Goyal
"
Stefano Baghino <stefano.baghino@radicalbit.io>,"Tue, 3 Nov 2015 11:36:47 +0100",Re: Running individual test classes,Nitin Goyal <nitin2goyal@gmail.com>,"Oh, I saw POMs and thought I was supposed to use Maven. Thank you so much
for the help, I'll try it as soon as possible.





-- 
BR,
Stefano Baghino

Software Engineer @ Radicalbit
"
=?UTF-8?Q?Sergio_Ram=c3=adrez?= <sramirezga@ugr.es>,"Tue, 3 Nov 2015 11:49:33 +0100",Unchecked contribution (JIRA and PR),dev <dev@spark.apache.org>,"Hello all:

I developed two packages for MLlib in March. These have been also upload 
to the spark-packages repository. Associated to these packages, I 
created two JIRA's threads and the correspondent pull requests, which 
are listed below:

https://github.com/apache/spark/pull/5184
https://github.com/apache/spark/pull/5170

https://issues.apache.org/jira/browse/SPARK-6531
https://issues.apache.org/jira/browse/SPARK-6509

These remain unassigned in JIRA and unverified in GitHub.

Could anyone explain why are they in this state yet? Is it normal?

Thanks!

Sergio R.

-- 

Sergio Ramírez Gallego
Research group on Soft Computing and Intelligent Information Systems,
Dept. Computer Science and Artificial Intelligence,
University of Granada, Granada, Spain.
Email: sramirez@decsai.ugr.es
Research Group URL: http://sci2s.ugr.es/

-------------------------------------------------------------------------

Este correo electrónico y, en su caso, cualquier fichero anexo al mismo,
contiene información de carácter confidencial exclusivamente dirigida a
su destinatario o destinatarios. Si no es vd. el destinatario indicado,
queda notificado que la lectura, utilización, divulgación y/o copia sin
autorización está prohibida en virtud de la legislación vigente. En el
caso de haber recibido este correo electrónico por error, se ruega
notificar inmediatamente esta circunstancia mediante reenvío a la
dirección electrónica del remitente.
Evite imprimir este mensaje si no es estrictamente necesario.

This email and any file attached to it (when applicable) contain(s)
confidential information that is exclusively addressed to its
recipient(s). If you are not the indicated recipient, you are informed
that reading, using, disseminating and/or copying it without
authorisation is forbidden in accordance with the legislation in effect.
If you have received this email by mistake, please immediately notify
the sender of the situation by resending it to their email address.
Avoid printing this message if it is not absolutely necessary.


---------------------------------------------------------------------


"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Tue, 3 Nov 2015 16:20:00 +0530",Extracting RDD of values per key from PairRDD,dev@spark.apache.org,"Hello,

I have a use case where I need to get *an RDD of values per key *from
a PairRDD. Below is my PairRDD.

JavaPairRDD<Double, Iterable<Vector>> classifiedSampleRdd =
sampleRDD.groupByKey();

I want a separate RDD for the vectors per double entry in the key. *I
would now want a RDD of values for each key.* Which will be

JavaRDD<Vector> = classifiedSampleRdd. ???


Can someone tell me how I can extract that ?

Thanks
Deepak
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 3 Nov 2015 18:50:50 +0800",Master build fails ?,dev@spark.apache.org,"Looks like it's due to guava version conflicts, I see both guava 14.0.1 and
16.0.1 under lib_managed/bundles. Anyone meet this issue too ?

[error]
/Users/jzhang/github/spark_apache/core/src/main/scala/org/apache/spark/SecurityManager.scala:26:
object HashCodes is not a member of package com.google.common.hash
[error] import com.google.common.hash.HashCodes
[error]        ^
[info] Resolving org.apache.commons#commons-math;2.2 ...
[error]
/Users/jzhang/github/spark_apache/core/src/main/scala/org/apache/spark/SecurityManager.scala:384:
not found: value HashCodes
[error]         val cookie = HashCodes.fromBytes(secret).toString()
[error]                      ^




-- 
Best Regards

Jeff Zhang
"
Michael Armbrust <michael@databricks.com>,"Tue, 3 Nov 2015 12:02:27 +0100",Re: Running individual test classes,Stefano Baghino <stefano.baghino@radicalbit.io>,"We support both build systems.  We use maven to publish the canonical
distributions as it interoperates better with downstream consumers.  Most
of the developers that I know, however, use SBT for day to day development.


"
,"Tue, 3 Nov 2015 12:52:07 +0100",Re: Master build fails ?,dev@spark.apache.org,"Hi Jeff,

it works for me (with skipping the tests).

Let me try again, just to be sure.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 3 Nov 2015 19:55:47 +0800",Re: Master build fails ?,,"I found it is due to SPARK-11073.

Here's the command I used to build

build/sbt clean compile -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver
-Psparkr


ecurityManager.scala:26:
ecurityManager.scala:384:


-- 
Best Regards

Jeff Zhang
"
Stefano Baghino <stefano.baghino@radicalbit.io>,"Tue, 3 Nov 2015 13:03:20 +0100",Re: Running individual test classes,Michael Armbrust <michael@databricks.com>,"Good to know, thank you very much. :)




-- 
BR,
Stefano Baghino

Software Engineer @ Radicalbit
"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 3 Nov 2015 20:21:24 +0800",Re: Master build fails ?,"Jeff Zhang <zjffdu@gmail.com>
	dev <dev@spark.apache.org>","Yeah, I also met this problem, just curious why jenkins test is OK.


t>
SecurityManager.scala:26:
SecurityManager.scala:384:
"
gus <gustavo.arocena@gmail.com>,"Tue, 3 Nov 2015 05:25:28 -0700 (MST)","Re: SparkLauncher#setJavaHome does not set JAVA_HOME in child
 process",dev@spark.apache.org,"Thanks, Ted.
The SparkLauncher test suite runs fine for me, with or without the change.
Do you agree this is a bug? If so, should I open a JIRA?



--

---------------------------------------------------------------------


"
,"Tue, 3 Nov 2015 13:37:37 +0100",Re: Master build fails ?,dev@spark.apache.org,"Thanks for the update, I used mvn to build but without hive profile.

Let me try with mvn with the same options as you and sbt also.

I keep you posted.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 3 Nov 2015 12:40:59 +0000",Re: Unchecked contribution (JIRA and PR),=?UTF-8?Q?Sergio_Ram=C3=ADrez?= <sramirezga@ugr.es>,"Generally speaking, the default disposition of any PR or JIRA is
""won't merge"" until proven otherwise. This is especially true of
large, stand-alone features like a new ML algorithm. I believe the
lack of traction means there is not interest in adding this to Spark
and so these issues should be closed. It is no judgment on the quality
or usefulness of the idea; it's an indication that this should live
outside Spark for now.

ote:
to
o
w:
smo,
rigida a
opia sin
gente. En el

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 3 Nov 2015 04:57:16 -0800",Re: Running individual test classes,Nitin Goyal <nitin2goyal@gmail.com>,"My experience is that going through tests in each module takes some time before reaching the test specified by the wildcard. 

Some test, such as SparkLauncherSuite, would run even if not in wildcard. 

FYI 

tarted working on SPARK-7425 and have implemented what looks like a viable solution. Now I'd like to test it, however I'm having some trouble running an individual test class to quickly iterate over it; I tried running
en from the project root and from the MLlib module root. I've tried to look this up in the mailing list archives but haven't had luck so far.
"
Stefano Baghino <stefano.baghino@radicalbit.io>,"Tue, 3 Nov 2015 14:00:25 +0100",Re: Running individual test classes,Ted Yu <yuzhihong@gmail.com>,"Thank you for the tip, I'll keep that in mind.




-- 
BR,
Stefano Baghino

Software Engineer @ Radicalbit
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 3 Nov 2015 14:44:06 +0100",Re: Master build fails ?,dev@spark.apache.org,"Hi,

Just built the sources using the following command and it worked fine.

➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver
-DskipTests clean install
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 14:15 min
[INFO] Finished at: 2015-11-03T14:40:40+01:00
[INFO] Final Memory: 438M/1972M
[INFO] ------------------------------------------------------------------------

➜  spark git:(master) ✗ java -version
java version ""1.8.0_66""
Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)

I'm on Mac OS.

Pozdrawiam,
Jacek

--
Jacek Laskowski | http://blog.japila.pl | http://blog.jaceklaskowski.pl
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski


et
 ?
ecurityManager.scala:26:
sh
ecurityManager.scala:384:
-

---------------------------------------------------------------------


"
,"Tue, 3 Nov 2015 14:58:01 +0100",Re: Master build fails ?,dev@spark.apache.org,"Hi Jacek,

it works fine with mvn: the problem is with sbt.

I suspect a different reactor order in sbt compare to mvn.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 3 Nov 2015 06:23:34 -0800",Re: SparkLauncher#setJavaHome does not set JAVA_HOME in child process,gus <gustavo.arocena@gmail.com>,"Opening JIRA is fine.

Thanks


"
Vivekananda Venkateswaran <svvenkat@gmail.com>,"Tue, 3 Nov 2015 20:00:37 +0530",Re: Extracting RDD of values per key from PairRDD,Deepak Gopalakrishnan <dgkris@gmail.com>,"Hi Deepak,

AFAIK, such nested RDDs are not allowed.

Thanks,
-Venkat.


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 3 Nov 2015 06:31:11 -0800",Re: Master build fails ?,,"Interesting, Sbt builds were not all failing:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/

FYI


et>
.net
/SecurityManager.scala:26:
/SecurityManager.scala:384:
"
Disha Shrivastava <dishu.905@gmail.com>,"Tue, 3 Nov 2015 20:30:59 +0530",Re: Implementation of RNN/LSTM in Spark,Sasaki Kai <lewuathe@me.com>,"I would love to work on this and ask for ideas on how it can be done or can
suggest some papers as starting point. Also, I wanted to know if Spark
would be an ideal platform to have a distributive implementation for
RNN/LSTM


"
,"Tue, 3 Nov 2015 16:20:17 +0100",Re: Master build fails ?,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

thanks for the update. The build with sbt is in progress on my box.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Tue, 3 Nov 2015 16:22:34 +0100",Re: Implementation of RNN/LSTM in Spark,Disha Shrivastava <dishu.905@gmail.com>,"Hi,
Is my understanding that little research has been done yet on distributed computation (without access to shared memory) in RNN. I also look forward to contributing in this respect.

ió:
n suggest some papers as starting point. Also, I wanted to know if Spark would be an ideal platform to have a distributive implementation for RNN/LSTM
ckets about other type of networks regarding deep learning.
e:
 or has already done. I am also willing to contribute to it and get some guidance on how to go about it.
"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Nov 2015 07:24:28 -0800",Re: Unchecked contribution (JIRA and PR),=?UTF-8?Q?Sergio_Ram=C3=ADrez?= <sramirezga@ugr.es>,"Sergio,

Usually it takes a lot of effort to get something merged into Spark itself,
especially for relatively new algorithms that might not have established
itself yet. I will leave it to mllib maintainers to comment on the
specifics of the individual algorithms proposed here.

Just another general comment: we have been working on making packages be as
easy to use as possible for Spark users. Right now it only requires a
simple flag to pass to the spark-submit script to include a package.


te:

smo,
rigida a
opia sin
gente. En el
"
Disha Shrivastava <dishu.905@gmail.com>,"Tue, 3 Nov 2015 20:55:13 +0530",Re: Implementation of RNN/LSTM in Spark,Julio Antonio Soto de Vicente <julio@esbet.es>,"Hi Julio,

Can you please cite references based on the distributed implementation?


k
"
Jerry Lam <chilinglam@gmail.com>,"Tue, 3 Nov 2015 10:30:11 -0500",Re: Unchecked contribution (JIRA and PR),Reynold Xin <rxin@databricks.com>,"Sergio, you are not alone for sure. Check the RowSimilarity implementation
[SPARK-4823]. It has been there for 6 months. It is very likely those which
don't merge in the version of spark that it was developed will never merged
because spark changes quite significantly from version to version if the
algorithm depends a lot of internal api.


n
rote:
d
-
ismo,
irigida a
copia sin
igente. En el
"
Ryan Williams <ryan.blake.williams@gmail.com>,"Tue, 03 Nov 2015 15:57:41 +0000",Re: Off-heap storage and dynamic allocation,"Justin Uang <justin.uang@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","fwiw, I think that having cached RDD partitions prevents executors from
being removed under dynamic allocation by default; see SPARK-8958
<https://issues.apache.org/jira/browse/SPARK-8958>. The
""spark.dynamicAllocation.cachedExecutorIdleTimeout"" config
<http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation>
controls this.


"
Justin Uang <justin.uang@gmail.com>,"Tue, 03 Nov 2015 15:59:26 +0000",Re: Off-heap storage and dynamic allocation,"Ryan Williams <ryan.blake.williams@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Yup, but I'm wondering what happens when an executor does get removed, but
when we're using tachyon. Will the cached data still be available, since
we're using off-heap storage, so the data isn't stored in the executor?


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Nov 2015 08:53:16 -0800",Re: Off-heap storage and dynamic allocation,Justin Uang <justin.uang@gmail.com>,"I don't think there is any special handling w.r.t. Tachyon vs in-heap
caching. As a matter of fact, I think the current offheap caching
implementation is pretty bad, because:

1. There is no namespace sharing in offheap mode
2. Similar to 1, you cannot recover the offheap memory once Spark driver or
executor crashes
3. It requires expensive serialization to go offheap

It would've been simpler to just treat Tachyon as a normal file system, and
use it that way to at least satisfy 1 and 2, and also substantially
simplify the internals.





"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Tue, 3 Nov 2015 18:52:29 +0000","Frozen exception while dynamically creating classes inside Spark
 using JavaAssist API","""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I am trying to dynamically create a new class in Spark using javaassist API. The code seems very simple just invoking makeClass API on a hardcoded class name. The code works find outside Spark environment but getting this chedkNotFrozen exception when I am running the code inside Spark
Code Excerpt:
ClassPool pool = ClassPool.getDefault()
CtClass regExClass= pool.makeClass(""TestClass14"",baseFeatureProcessor)
Exception Details:
###################### Exception make Class
java.lang.RuntimeException: TestClass14: frozen class (cannot edit)
    at javassist.ClassPool.checkNotFrozen(ClassPool.java:617)
    at javassist.ClassPool.makeClass(ClassPool.java:859)

"
Justin Uang <justin.uang@gmail.com>,"Tue, 03 Nov 2015 21:13:12 +0000",Re: Off-heap storage and dynamic allocation,Reynold Xin <rxin@databricks.com>,"Thanks for your response. I was worried about #3, vs being able to use the
objects directly. #2 seems to be the dealbreaker for my use case right?
Even if it I am using tachyon for caching, if an executor is lost, then
that partition is lost for the purposes of spark?


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Nov 2015 13:14:43 -0800",Re: Off-heap storage and dynamic allocation,Justin Uang <justin.uang@gmail.com>,"It is lost unfortunately (although can be recomputed automatically).



"
Justin Uang <justin.uang@gmail.com>,"Tue, 03 Nov 2015 21:17:49 +0000",Re: Pickle Spark DataFrame,"agg212 <agg@cs.brown.edu>, dev@spark.apache.org","Is the Manager a python multiprocessing manager? Why are you using
parallelism on python when theoretically most of the heavy lifting is done
via spark?


"
Justin Uang <justin.uang@gmail.com>,"Tue, 03 Nov 2015 21:20:09 +0000",Re: Off-heap storage and dynamic allocation,Reynold Xin <rxin@databricks.com>,"Alright, we'll just stick with normal caching then.

Just for future reference, how much work would it be to get it to retain
the partitions in tachyon. This is especially helpful in a multitenant
situation, where many users each have their own persistent spark contexts,
but where the notebooks can be idle for long periods of time while holding
onto cached rdds.


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Nov 2015 13:25:35 -0800",Re: Off-heap storage and dynamic allocation,Justin Uang <justin.uang@gmail.com>,"It is quite a bit of work. Again, I think going through the file system API
is more ideal in the long run. In the long run, I don't even think the
current offheap API makes much sense, and we should consider just removing
it to simplify things.


"
Justin Uang <justin.uang@gmail.com>,"Tue, 03 Nov 2015 21:27:05 +0000",Re: Off-heap storage and dynamic allocation,Reynold Xin <rxin@databricks.com>,"Cool, thanks for the dev insight into what parts of the codebase are
worthwhile, and which are not =)


"
Justin Uang <justin.uang@gmail.com>,"Tue, 03 Nov 2015 21:41:05 +0000",Info about Dataset,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I was looking through some of the PRs slated for 1.6.0 and I noted
something called a Dataset, which looks like a new concept based off of the
scaladoc for the class. Can anyone point me to some references/design_docs
regarding the choice to introduce the new concept? I presume it is probably
something to do with performance optimizations?

Thanks!

Justin
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 3 Nov 2015 13:42:07 -0800",Re: Info about Dataset,Justin Uang <justin.uang@gmail.com>,"Hi Justin,

The Dataset API proposal is available here:
https://issues.apache.org/jira/browse/SPARK-9999.

-Sandy


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Nov 2015 15:22:28 -0800",[VOTE] Release Apache Spark 1.5.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.5.2. The vote is open until Sat Nov 7, 2015 at 00:00 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5.2
[ ] -1 Do not release this package because ...


The release fixes 59 known issues in Spark 1.5.1, listed here:
http://s.apache.org/spark-1.5.2

The tag to be voted on is v1.5.2-rc2:
https://github.com/apache/spark/releases/tag/v1.5.2-rc2

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
- as version 1.5.2-rc2:
https://repository.apache.org/content/repositories/orgapachespark-1153
- as version 1.5.2:
https://repository.apache.org/content/repositories/orgapachespark-1152

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc2-docs/


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.1. Bugs already present
in 1.5.1 will not block this release.
"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Nov 2015 15:54:06 -0800",Please reply if you use Mesos fine grained mode,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","If you are using Spark with Mesos fine grained mode, can you please respond
to this email explaining why you use it over the coarse grained mode?

Thanks.
"
Soren Macbeth <soren@yieldbot.com>,"Tue, 3 Nov 2015 16:22:35 -0800",Re: Please reply if you use Mesos fine grained mode,Reynold Xin <rxin@databricks.com>,"we use fine-grained mode. coarse-grained mode keeps JVMs around which often
leads to OOMs, which in turn kill the entire executor, causing entire
stages to be retried. In fine-grained mode, only the task fails and
subsequently gets retried without taking out an entire stage or worse.


"
Sasaki Kai <lewuathe@me.com>,"Wed, 04 Nov 2015 10:06:55 +0900",Re: Implementation of RNN/LSTM in Spark,Disha Shrivastava <dishu.905@gmail.com>,"Hi, Disha

deeplearning4j seems to implement distributed RNN on core and scalaout packages.
http://deeplearning4j.org/recurrentnetwork.html <http://deeplearning4j.org/recurrentnetwork.html>
https://github.com/deeplearning4j/deeplearning4j <https://github.com/deeplearning4j/deeplearning4j>
It might be helpful to refer the implementation of distributed RNN.

And also I found that there is a resources about efficient RNN and implementation.
https://karpathy.github.io/2015/05/21/rnn-effectiveness/ <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>

Best

Kai

implementation?
distributed computation (without access to shared memory) in RNN. I also look forward to contributing in this respect.
<mailto:dishu.905@gmail.com>> escribió:
or can suggest some papers as starting point. Also, I wanted to know if  Spark would be an ideal platform to have a distributive implementation for RNN/LSTM
several tickets about other type of networks regarding deep learning.
<https://issues.apache.org/jira/browse/SPARK-2623>
<https://issues.apache.org/jira/browse/SPARK-9129>
<https://issues.apache.org/jira/browse/SPARK-9273>
<https://issues.apache.org/jira/browse/SPARK-5575>
Spark or has already done. I am also willing to contribute to it and get some guidance on how to go about it.

"
Jerry Lam <chilinglam@gmail.com>,"Tue, 3 Nov 2015 20:48:34 -0500",Re: Please reply if you use Mesos fine grained mode,Soren Macbeth <soren@yieldbot.com>,"We ""used"" Spark on Mesos to build interactive data analysis platform
because the interactive session could be long and might not use Spark for
the entire session. It is very wasteful of resources if we used the
coarse-grained mode because it keeps resource for the entire session.
Therefore, fine-grained mode was used.

Knowing that Spark now supports dynamic resource allocation with coarse
grained mode, we were thinking about using it. However, we decided to
switch to Yarn because in addition to dynamic allocation, it has better
supports on security.


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Nov 2015 18:00:36 -0800",Re: Please reply if you use Mesos fine grained mode,Soren Macbeth <soren@yieldbot.com>,"Soren,

If I understand how Mesos works correctly, even the fine grained mode keeps
the JVMs around?



"
MEETHU MATHEW <meethu2006@yahoo.co.in>,"Wed, 4 Nov 2015 04:29:20 +0000 (UTC)",Re: Please reply if you use Mesos fine grained mode,"Reynold Xin <rxin@databricks.com>, user <user@spark.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
We are using Mesos fine grained mode because we can have multiple instances of spark to share machines and each application get resources dynamically allocated.  Thanks & Regards,  Meethu M 


   

 If you are using Spark with Mesos fine grained mode, can you please respond to this email explaining why you use it over the coarse grained mode?
Thanks.


  "
Timothy Chen <tnachen@gmail.com>,"Tue, 3 Nov 2015 22:12:01 -0800",Re: Please reply if you use Mesos fine grained mode,Reynold Xin <rxin@databricks.com>,"Fine grain mode does reuse the same JVM but perhaps different placement or different allocated cores comparing to the same total memory allocation.

Tim

Sent from my iPhone

s the JVMs around?

en leads to OOMs, which in turn kill the entire executor, causing entire stages to be retried. In fine-grained mode, only the task fails and subsequently gets retried without taking out an entire stage or worse. 

ond to this email explaining why you use it over the coarse grained mode?
"
Charles Yeh <charles@eactiv.com>,"Tue, 3 Nov 2015 22:24:39 -0800",Getting new metrics into /api/v1,dev@spark.apache.org,"Hello,

I'm trying to get maxCores and memoryPerExecutorMB into /api/v1 for this
ticket: https://issues.apache.org/jira/browse/SPARK-10565

I can't figure out which *getApplicationInfoList *is used by
*ApiRootResource.scala.
*It's attached in SparkUI but SparkUI's doesn't have start / end times
and /api/v1/applications does.

It looks like:

   - *MasterWebUI.scala* has these fields, since it has the applications
   themselves
   - *HistoryServer.scala *doesn't have these fields, since it infers them
   from logs
   - *SparkUI.scala *looks like a mock since it doesn't have end time /
   user / attempt id either
"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Wed, 4 Nov 2015 16:21:36 +0800",Codegen In Shuffle,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Dear all:

Tungsten project has mentioned that they are applying code generation is to
speed up the conversion of data from in-memory binary format to
wire-protocol for shuffle.

Where can I find the related implementation in spark code-based ?

-- 
*Regards,*
*Zhaojie*
"
Reynold Xin <rxin@databricks.com>,"Wed, 4 Nov 2015 00:25:11 -0800",Re: Codegen In Shuffle,=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"GenerateUnsafeProjection -- projects any internal row data structure
directly into bytes (UnsafeRow).



"
Charles Yeh <charles@eactiv.com>,"Wed, 4 Nov 2015 01:21:17 -0800",Re: Getting new metrics into /api/v1,dev@spark.apache.org,"Never mind, I didn't realize building the core module separately doesn't
update the main jar that the scripts use.

Will look into making the history server consistent.


"
,"Wed, 4 Nov 2015 10:26:43 +0100",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),dev@spark.apache.org,"+1 (non binding)

Just tested with some snippets on my side.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
gsvic <victorasgs@gmail.com>,"Wed, 4 Nov 2015 04:27:49 -0700 (MST)",Build a specific module only,dev@spark.apache.org,"Is it possible to build a specific spark module without building the whole
project?

For example, I am trying to build sql-core project by

/build/mvn -pl sql/core install -DskipTests/

and I am getting the following error:

/[error] bad symbolic reference. A signature in WebUI.class refers to term
eclipse
[error] in package org which is not available.
[error] It may be completely missing from the current classpath, or the
version on
[error] the classpath might be incompatible with the version used when
compiling WebUI.class.
[error] bad symbolic reference. A signature in WebUI.class refers to term
jetty
[error] in value org.eclipse which is not available./

You can see the whole trace here http://pastebin.com/qt4f6JGu



--

---------------------------------------------------------------------


"
Fazlan Nazeem <fazlann@wso2.com>,"Wed, 4 Nov 2015 17:12:05 +0530",Re: PMML version in MLLib,dev@spark.apache.org,"[adding dev]





-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
Sean Owen <sowen@cloudera.com>,"Wed, 4 Nov 2015 11:52:28 +0000",Re: PMML version in MLLib,Fazlan Nazeem <fazlann@wso2.com>,"I'm pretty sure that attribute is required. I am not sure what PMML
version the code has been written for but would assume 4.2.1. Feel
free to open a PR to add this version to all the output.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 4 Nov 2015 13:10:27 +0100",Re: Master build fails ?,dev@spark.apache.org,"Hi,

It appears it's time to switch to my lovely sbt then!

Pozdrawiam,
Jacek

--
Jacek Laskowski | http://blog.japila.pl | http://blog.jaceklaskowski.pl
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski


et>
.net
/SecurityManager.scala:26:
/SecurityManager.scala:384:

---------------------------------------------------------------------


"
Fazlan Nazeem <fazlann@wso2.com>,"Wed, 4 Nov 2015 17:44:36 +0530",Re: PMML version in MLLib,Sean Owen <sowen@cloudera.com>,"Thanks Owen. Will do it





-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
"""Heller, Chris"" <cheller@akamai.com>","Wed, 4 Nov 2015 12:21:37 +0000",Re: Please reply if you use Mesos fine grained mode,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","We’ve been making use of both. Fine-grain mode makes sense for more ad-hoc work loads, and coarse-grained for more job like loads on a common data set. My preference is the fine-grain mode in all cases, but the overhead associated with its startup and the possibility that an overloaded cluster would be starved for resources makes coarse grain mode a reality at the moment.

On Wednesday, 4 November 2015 5:24 AM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:


If you are using Spark with Mesos fine grained mode, can you please respond to this email explaining why you use it over the coarse grained mode?

Thanks.



"
=?UTF-8?B?VMOzdGggWm9sdMOhbg==?= <tz@looper.hu>,"Wed, 4 Nov 2015 14:11:43 +0100",Looking for the method executors uses to write to HDFS,"dev@spark.apache.org, user@spark.apache.org","Hi,

I'd like to write a parquet file from the driver. I could use the HDFS API
but I am worried that it won't work on a secure cluster. I assume that the
method the executors use to write to HDFS takes care of managing Hadoop
security. However, I can't find the place where HDFS write happens in the
spark source.

Please help me:
1.How to write parquet from the driver using the Spark API?
2. If this wouldn't possible, where can I find the method executors use to
write to HDFS?

Thanks,
Zoltan
"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Wed, 4 Nov 2015 21:36:52 +0800",Re: Codegen In Shuffle,Reynold Xin <rxin@databricks.com>,"I see. Thanks very much.

2015-11-04 16:25 GMT+08:00 Reynold Xin <rxin@databricks.com>:



-- 
*Regards,*
*Zhaojie*
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 4 Nov 2015 06:15:27 -0800",Re: Build a specific module only,gsvic <victorasgs@gmail.com>,"Please take a look at
https://issues.apache.org/jira/browse/SPARK-10883



n3.nabble.com/Build-a-specific-module-only-tp14943.html
com.

---------------------------------------------------------------------


"
Alex Nastetsky <alex.nastetsky@vervemobile.com>,"Wed, 4 Nov 2015 09:37:53 -0500",Sort Merge Join from the filesystem,<dev@spark.apache.org>,"(this is kind of a cross-post from the user list)

Does Spark support doing a sort merge join on two datasets on the file
system that have already been partitioned the same with the same number of
partitions and sorted within each partition, without needing to
repartition/sort them again?

This functionality exists in
- Hive (hive.optimize.bucketmapjoin.sortedmerge)
- Pig (USING 'merge')
- MapReduce (CompositeInputFormat)

If this is not supported in Spark, is a ticket already open for it? Does
the Spark architecture present unique difficulties to having this feature?

It is very useful to have this ability, as you can prepare dataset A to be
joined with dataset B before B even exists, by pre-processing A with a
partition/sort.

Thanks.
"
gsvic <victorasgs@gmail.com>,"Wed, 4 Nov 2015 07:56:34 -0700 (MST)",Re: Build a specific module only,dev@spark.apache.org,"Ok, thank you very much Ted.



--

---------------------------------------------------------------------


"
=?UTF-8?Q?Sergio_Ram=c3=adrez?= <sramirezga@ugr.es>,"Wed, 4 Nov 2015 16:23:24 +0100",Re: Unchecked contribution (JIRA and PR),"Jerry Lam <chilinglam@gmail.com>, Reynold Xin <rxin@databricks.com>","OK, for me, time is not a problem. I was just worried about there was no 
movement in those issues. I think they are good contributions. For 
example, I have found no complex discretization algorithm in MLlib, 
which is rare. My algorithm, a Spark implementation of the well-know 
discretizer developed by Fayyad and Irani, could be considered a good 
starting point for the discretization part. Furthermore, this is also 
supported by two scientific articles.

Anyway, I uploaded these two algorithms as two different packages to 
spark-packages.org, but I would like to contribute directly to MLlib. I 
understand you have a lot of requests, and it is not possible to include 
all the contributions made by the Spark community.

I'll be patient and ready to collaborate.

Thanks again



-- 

Sergio Ramírez Gallego
Research group on Soft Computing and Intelligent Information Systems,
Dept. Computer Science and Artificial Intelligence,
University of Granada, Granada, Spain.
Email: sramirez@decsai.ugr.es
Research Group URL: http://sci2s.ugr.es/

-------------------------------------------------------------------------

Este correo electrónico y, en su caso, cualquier fichero anexo al mismo,
contiene información de carácter confidencial exclusivamente dirigida a
su destinatario o destinatarios. Si no es vd. el destinatario indicado,
queda notificado que la lectura, utilización, divulgación y/o copia sin
autorización está prohibida en virtud de la legislación vigente. En el
caso de haber recibido este correo electrónico por error, se ruega
notificar inmediatamente esta circunstancia mediante reenvío a la
dirección electrónica del remitente.
Evite imprimir este mensaje si no es estrictamente necesario.

This email and any file attached to it (when applicable) contain(s)
confidential information that is exclusively addressed to its
recipient(s). If you are not the indicated recipient, you are informed
that reading, using, disseminating and/or copying it without
authorisation is forbidden in accordance with the legislation in effect.
If you have received this email by mistake, please immediately notify
the sender of the situation by resending it to their email address.
Avoid printing this message if it is not absolutely necessary.

"
Timothy Chen <tnachen@gmail.com>,"Wed, 4 Nov 2015 08:05:25 -0800",Re: Please reply if you use Mesos fine grained mode,"""Heller, Chris"" <cheller@akamai.com>","Hi Chris,

How does coarse grain mode gives you less starvation in your overloaded cluster? Is it just because it allocates all resources at once (which I think in a overloaded cluster allows less things to run at once).

Tim


d-hoc work loads, and coarse-grained for more job like loads on a common data set. My preference is the fine-grain mode in all cases, but the overhead associated with its startup and the possibility that an overloaded cluster would be starved for resources makes coarse grain mode a reality at the moment. 
rote:
d to this email explaining why you use it over the coarse grained mode?
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Wed, 4 Nov 2015 17:54:30 +0100",Re: Please reply if you use Mesos fine grained mode,Timothy Chen <tnachen@gmail.com>,"Probably because only coarse-grained mode respects `spark.cores.max` right
now. See (and maybe review ;-)) #9027
<https://github.com/apache/spark/pull/9027> (sorry for the shameless plug).

iulian


re ad-hoc
r


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Sean Owen <sowen@cloudera.com>,"Wed, 4 Nov 2015 17:03:40 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Reynold Xin <rxin@databricks.com>,"As usual the signatures and licenses and so on look fine. I continue
to get the same test failures on Ubuntu in Java 7/8:

- Unpersisting HttpBroadcast on executors only in distributed mode ***
FAILED ***

But I continue to assume that's specific to tests and/or Ubuntu and/or
the build profile, since I don't see any evidence of this in other
builds on Jenkins. It's not a change from previous behavior, though it
doesn't always happen either.


---------------------------------------------------------------------


"
"""Heller, Chris"" <cheller@akamai.com>","Wed, 4 Nov 2015 17:06:36 +0000",Re: Please reply if you use Mesos fine grained mode,Timothy Chen <tnachen@gmail.com>,"Correct. Its just that with coarse mode we grab the resources up front, so its either available or not. But using resources on demand, as with a fine grained mode, just means the potential to starve out an individual job. There is also the sharing of RDDs that coarse gives you which would need something like Tachyon to achieve in fine grain mode.


From: Timothy Chen <tnachen@gmail.com<mailto:tnachen@gmail.com>>
Date: Wednesday, November 4, 2015 at 11:05 AM
To: ""Heller, Chris"" <cheller@akamai.com<mailto:cheller@akamai.com>>
Cc: Reynold Xin <rxin@databricks.cilto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Please reply if you use Mesos fine grained mode

Hi Chris,

How does coarse grain mode gives you less starvation in your overloaded cluster? Is it just because it allocates all resources at once (which I think in a overloaded cluster allows less things to run at once).

Tim


On Nov 4, 2015, at 4:21 AM, Heller, Chris <cheller@akamai.com<mailto:cheller@akamai.com>> wrote:

We’ve been making use of both. Fine-grain mode makes sense for more ad-hoc work loads, and coarse-grained for more job like loads on a common data set. My preference is the fine-grain mode in all cases, but the overhead associated with its startup and the possibility that an overloaded cluster would be starved for resources makes coarse grain mode a reality at the moment.

On Wednesday, 4 November 2015 5:24 AM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:


If you are using Spark with Mesos fine grained mode, can you please respond to this email explaining why you use it over the coarse grained mode?

Thanks.



"
Reynold Xin <rxin@databricks.com>,"Wed, 4 Nov 2015 09:35:32 -0800",Re: Sort Merge Join from the filesystem,Alex Nastetsky <alex.nastetsky@vervemobile.com>,"It's not supported yet, and not sure if there is a ticket for it. I don't
think there is anything fundamentally hard here either.



"
Charmee Patel <charmeep@gmail.com>,"Wed, 04 Nov 2015 18:19:27 +0000",How to force statistics calculation of Dataframe?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

If I have a hive table, analyze table compute statistics will ensure Spark
SQL has statistics of that table. When I have a dataframe, is there a way
to force spark to collect statistics?

I have a large lookup file and I am trying to avoid a broadcast join by
applying a filter before hand. This filtered RDD does not have statistics
and so catalyst does not force a broadcast join. Unfortunately I have to
use spark sql and cannot use dataframe api so cannot give a broadcast hint
in the join.

Example is this -
If filtered RDD is saved as a table and compute stats is run, statistics are

test.queryExecution.analyzed.statistics
org.apache.spark.sql.catalyst.plans.logical.Statistics =
Statistics(38851747)


filtered RDD as is gives
org.apache.spark.sql.catalyst.plans.logical.Statistics =
Statistics(58403444019505585)

filtered RDD forced to be materialized (cache/count), causes a different
issue. Executors goes in a deadlock type state where not a single thread
runs - for hours. I suspect cache a dataframe + broadcast join on same
dataframe does this. As soon as cache is removed, the job moves forward.

If there was a way for me to force statistics collection without caching a
dataframe so Spark SQL would use it in a broadcast join?

Thanks,
Charmee
"
Reynold Xin <rxin@databricks.com>,"Wed, 4 Nov 2015 15:49:15 -0800",Re: How to force statistics calculation of Dataframe?,Charmee Patel <charmeep@gmail.com>,"Can you use the broadcast hint?

e.g.

df1.join(broadcast(df2))

the broadcast function is in org.apache.spark.sql.functions




"
Charmee Patel <charmeep@gmail.com>,"Wed, 04 Nov 2015 23:51:55 +0000",Re: How to force statistics calculation of Dataframe?,Reynold Xin <rxin@databricks.com>,"Due to other reasons we are using spark sql, not dataframe api. I saw that
broadcast hint is only available on dataframe api.

"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 5 Nov 2015 00:51:59 +0000",RE: Sort Merge Join from the filesystem,"Reynold Xin <rxin@databricks.com>, Alex Nastetsky
	<alex.nastetsky@vervemobile.com>","Yes, we probably need more change for the data source API if we need to implement it in a generic way.
BTW, I create the JIRA by copy most of words from Alex. ☺

https://issues.apache.org/jira/browse/SPARK-11512


From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Thursday, November 5, 2015 1:36 AM
To: Alex Nastetsky
Cc: dev@spark.apache.org
Subject: Re: Sort Merge Join from the filesystem

It's not supported yet, and not sure if there is a ticket for it. I don't think there is anything fundamentally hard here either.


On Wed, Nov 4, 2015 at 6:37 AM, Alex Nastetsky <alex.nastetsky@vervemobile.com<mailto:alex.nastetsky@vervemobile.com>> wrote:
(this is kind of a cross-post from the user list)

Does Spark support doing a sort merge join on two datasets on the file system that have already been partitioned the same with the same number of partitions and sorted within each partition, without needing to repartition/sort them again?

This functionality exists in
- Hive (hive.optimize.bucketmapjoin.sortedmerge)
- Pig (USING 'merge')
- MapReduce (CompositeInputFormat)

If this is not supported in Spark, is a ticket already open for it? Does the Spark architecture present unique difficulties to having this feature?

It is very useful to have this ability, as you can prepare dataset A to be joined with dataset B before B even exists, by pre-processing A with a partition/sort.

Thanks.

"
Egor Pahomov <pahomov.egor@gmail.com>,"Wed, 4 Nov 2015 17:20:08 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Sean Owen <sowen@cloudera.com>,"+1

Things, which our infrastructure use and I checked:

Dynamic allocation
Spark ODBC server
Reading json
Writing parquet
SQL quires (hive context)
Running on CDH


2015-11-04 9:03 GMT-08:00 Sean Owen <sowen@cloudera.com>:



-- 

*Sincerely yoursEgor Pa"
Jeff Zhang <zjffdu@gmail.com>,"Thu, 5 Nov 2015 10:30:51 +0800",Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?,dev@spark.apache.org,"Not sure the reason,  it seems LibSVMRelation and CsvRelation can extends
HadoopFsRelation and leverage the features from HadoopFsRelation.  Any
other consideration for that ?


-- 
Best Regards

Jeff Zhang
"
gen tang <gen.tang86@gmail.com>,"Thu, 5 Nov 2015 12:43:00 +0800",Fwd: dataframe slow down with tungsten turn on,dev@spark.apache.org,"Hi,

In fact, I tested the same code with spark 1.5 with tungsten turning off.
The result is quite the same as tungsten turning on.
It seems that it is not the problem of tungsten, it is simply that spark
1.5 is slower than spark 1.4.

Is there any idea about why it happens?
Thanks a lot in advance

Cheers
Gen


---------- Forwarded message ----------
From: gen tang <gen.tang86@gmail.com>
Date: Wed, Nov 4, 2015 at 3:54 PM
Subject: dataframe slow down with tungsten turn on
To: ""user@spark.apache.org"" <user@spark.apache.org>


Hi sparkers,

I am using dataframe to do some large ETL jobs.
More precisely, I create dataframe from HIVE table and do some operations.
And then I save it as json.

When I used spark-1.4.1, the whole process is quite fast, about 1 mins.
However, when I use the same code with spark-1.5.1(with tungsten turn on),
it takes a about 2 hours to finish the same job.

I checked the detail of tasks, almost all the time is consumed by
computation.

Any idea about why this happens?

Thanks a lot in advance for your help.

Cheers
Gen
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 5 Nov 2015 04:55:35 +0000",RE: dataframe slow down with tungsten turn on,"gen tang <gen.tang86@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","1.5 has critical performance / bug issues, you’d better try 1.5.1 or 1.5.2rc version.

From: gen tang [mailto:gen.tang86@gmail.com]
Sent: Thursday, November 5, 2015 12:43 PM
To: dev@spark.apache.org
Subject: Fwd: dataframe slow down with tungsten turn on

Hi,

In fact, I tested the same code with spark 1.5 with tungsten turning off. The result is quite the same as tungsten turning on.
It seems that it is not the problem of tungsten, it is simply that spark 1.5 is slower than spark 1.4.

Is there any idea about why it happens?
Thanks a lot in advance

Cheers
Gen


---------- Forwarded message ----------
From: gen tang <gen.tang86@gmail.com<mailto:gen.tang86@gmail.com>>
Date: Wed, Nov 4, 2015 at 3:54 PM
Subject: dataframe slow down with tungsten turn on
To: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>

Hi sparkers,

I am using dataframe to do some large ETL jobs.
More precisely, I create dataframe from HIVE table and do some operations. And then I save it as json.

When I used spark-1.4.1, the whole process is quite fast, about 1 mins. However, when I use the same code with spark-1.5.1(with tungsten turn on), it takes a about 2 hours to finish the same job.

I checked the detail of tasks, almost all the time is consumed by computation.
[https://owa.gf.com.cn/owa/service.svc/s/GetFileAttachment?id=AAMkAGEzNGJiN2Q4LTI2ODYtNGIyYS1hYWIyLTMzMTYxOGQzYTViNABGAAAAAACPuqp5iM6mRqg7wmvE6c8KBwBKGW%2B6dpgjRb4BfC%2BACXJIAAAAAAEPAABKGW%2B6dpgjRb4BfC%2BACXJIAAAAQcF3AAABEgAQAIeCeL7UEe9GhqECpYfXhDI%3D&X-OWA-CANARY=7U3OIyan90CkQzeCMSlDnFM6WrDs5NIIksHvCIBBNwcmtRNW4tO1_1WPFeb51C1IsASUo1jqj_A.]
Any idea about why this happens?

Thanks a lot in advance for your help.

Cheers
Gen


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 5 Nov 2015 05:01:12 +0000",RE: dataframe slow down with tungsten turn on,"""Cheng, Hao"" <hao.cheng@intel.com>, gen tang <gen.tang86@gmail.com>,
	""dev@spark.apache.org"" <dev@spark.apache.org>","BTW, 1 min V.S. 2 Hours, seems quite weird, can you provide more information on the ETL work?

From: Cheng, Hao [mailto:hao.cheng@intel.com]
Sent: Thursday, November 5, 2015 12:56 PM
To: gen tang; dev@spark.apache.org
Subject: RE: dataframe slow down with tungsten turn on

1.5 has critical performance / bug issues, you’d better try 1.5.1 or 1.5.2rc version.

From: gen tang [mailto:gen.tang86@gmail.com]
Sent: Thursday, November 5, 2015 12:43 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Fwd: dataframe slow down with tungsten turn on

Hi,

In fact, I tested the same code with spark 1.5 with tungsten turning off. The result is quite the same as tungsten turning on.
It seems that it is not the problem of tungsten, it is simply that spark 1.5 is slower than spark 1.4.

Is there any idea about why it happens?
Thanks a lot in advance

Cheers
Gen


---------- Forwarded message ----------
From: gen tang <gen.tang86@gmail.com<mailto:gen.tang86@gmail.com>>
Date: Wed, Nov 4, 2015 at 3:54 PM
Subject: dataframe slow down with tungsten turn on
To: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
Hi sparkers,

I am using dataframe to do some large ETL jobs.
More precisely, I create dataframe from HIVE table and do some operations. And then I save it as json.

When I used spark-1.4.1, the whole process is quite fast, about 1 mins. However, when I use the same code with spark-1.5.1(with tungsten turn on), it takes a about 2 hours to finish the same job.

I checked the detail of tasks, almost all the time is consumed by computation.
[https://owa.gf.com.cn/owa/service.svc/s/GetFileAttachment?id=AAMkAGEzNGJiN2Q4LTI2ODYtNGIyYS1hYWIyLTMzMTYxOGQzYTViNABGAAAAAACPuqp5iM6mRqg7wmvE6c8KBwBKGW%2B6dpgjRb4BfC%2BACXJIAAAAAAEPAABKGW%2B6dpgjRb4BfC%2BACXJIAAAAQcF3AAABEgAQAIeCeL7UEe9GhqECpYfXhDI%3D&X-OWA-CANARY=7U3OIyan90CkQzeCMSlDnFM6WrDs5NIIksHvCIBBNwcmtRNW4tO1_1WPFeb51C1IsASUo1jqj_A.]
Any idea about why this happens?

Thanks a lot in advance for your help.

Cheers
Gen


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 5 Nov 2015 05:16:42 +0000","RE: Why LibSVMRelation and CsvRelation don't extends
 HadoopFsRelation ?","Jeff Zhang <zjffdu@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Probably 2 reasons:

1.      HadoopFsRelation was introduced since 1.4, but seems CsvRelation was created based on 1.3

2.      HadoopFsRelation introduces the concept of Partition, which probably not necessary for LibSVMRelation.

But I think it will be easy to change as extending from HadoopFsRelation.

Hao

From: Jeff Zhang [mailto:zjffdu@gmail.com]
Sent: Thursday, November 5, 2015 10:31 AM
To: dev@spark.apache.org
Subject: Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?


Not sure the reason,  it seems LibSVMRelation and CsvRelation can extends HadoopFsRelation and leverage the features from HadoopFsRelation.  Any other consideration for that ?


--
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Thu, 5 Nov 2015 13:54:39 +0800",Re: Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?,"""Cheng, Hao"" <hao.cheng@intel.com>","Thanks Hao. I have ready made it extends HadoopFsRelation and it works.
Will create a jira for that.

Besides that, I noticed that in DataSourceStrategy, spark build physical
plan based on the trait of the BaseRelation in pattern matching (e.g.
CatalystScan, TableScan, HadoopFsRelation). That means the order matters. I
think it is risky because that means one BaseRelation can't extends more
than 2 of these traits. And seems there's no place to restrict to extends
more than 2 traits. Maybe needs to clean and reorganize these traits
otherwise user may meets some weird issue when developing new DataSource.







-- 
Best Regards

Jeff Zhang
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 5 Nov 2015 06:39:19 +0000","RE: Why LibSVMRelation and CsvRelation don't extends
 HadoopFsRelation ?",Jeff Zhang <zjffdu@gmail.com>,"I think you’re right, we do offer the opportunity for developers to make mistakes while implementing the new Data Source.

Here we assume that the new relation MUST NOT extends more than one trait of the CatalystScan, TableScan, PrunedScan, PrunedFilteredScan , etc. otherwise it will causes problem as you described, probably we can add additional checking / reporting rule for the abuse.


y, November 5, 2015 1:55 PM
To: Cheng, Hao
Cc: dev@spark.apache.org
Subject: Re: Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?

Thanks Hao. I have ready made it extends HadoopFsRelation and it works. Will create a jira for that.

Besides that, I noticed that in DataSourceStrategy, spark build physical plan based on the trait of the BaseRelation in pattern matching (e.g. CatalystScan, TableScan, HadoopFsRelation). That means the order matters. I think it is risky because that means one BaseRelation can't extends more than 2 of these traits. And seems there's no place to restrict to extends more than 2 traits. Maybe needs to clean and reorganize these traits otherwise user may meets some weird issue when developing new DataSource.



On Thu, Nov 5, 2015 at 1:16 PM, Cheng, Hao <hao.cheng@intel.com<mailto:hao.cheng@intel.com>> wrote:
Probably 2 reasons:

1.      HadoopFsRelation was introduced since 1.4, but seems CsvRelation was created based on 1.3

2.      HadoopFsRelation introduces the concept of Partition, which probably not necessary for LibSVMRelation.

But I think it will be easy to change as extending from HadoopFsRelation.

Hao

From: Jeff Zhang [mailto:zjffdu@gmail.com<mailto:zjffdu@gmail.com>]
Sent: Thursday, November 5, 2015 10:31 AM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?


Not sure the reason,  it seems LibSVMRelation and CsvRelation can extends HadoopFsRelation and leverage the features from HadoopFsRelation.  Any other consideration for that ?


--
Best Regards

Jeff Zhang



--
Best Regards

Jeff Zhang
"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Thu, 5 Nov 2015 15:56:30 +0800",pyspark with pypy not work for spark-1.5.1,dev@spark.apache.org,"Hi all,

I am trying to run pyspark with pypy, and it is work when using spark-1.3.1
but failed when using spark-1.4.1 and spark-1.5.1

my pypy version:

$ /usr/bin/pypy --version
Python 2.7.3 (2.2.1+dfsg-1ubuntu0.3, Sep 30 2015, 15:18:40)
[PyPy 2.2.1 with GCC 4.8.4]

works with spark-1.3.1

$ PYSPARK_PYTHON=/usr/bin/pypy ~/Tool/spark-1.3.1-bin-hadoop2.6/bin/pyspark
Python 2.7.3 (2.2.1+dfsg-1ubuntu0.3, Sep 30 2015, 15:18:40)
[PyPy 2.2.1 with GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
15/11/05 15:50:30 WARN Utils: Your hostname, xxxxxx resolves to a loopback
address: 127.0.1.1; using xxx.xxx.xxx.xxx instead (on interface eth0)
15/11/05 15:50:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to
another address
15/11/05 15:50:31 WARN NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.1
      /_/

Using Python version 2.7.3 (2.2.1+dfsg-1ubuntu0.3, Sep 30 2015)
SparkContext available as sc, HiveContext available as sqlContext.
And now for something completely different: ``Armin: ""Prolog is a mess."",
CF:
""No, it's very cool!"", Armin: ""Isn't this what I said?""''

error message for 1.5.1

$ PYSPARK_PYTHON=/usr/bin/pypy ~/Tool/spark-1.5.1-bin-hadoop2.6/bin/pyspark
Python 2.7.3 (2.2.1+dfsg-1ubuntu0.3, Sep 30 2015, 15:18:40)
[PyPy 2.2.1 with GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File ""app_main.py"", line 72, in run_toplevel
  File ""app_main.py"", line 614, in run_it
  File
""/home/yahsuan/Tool/spark-1.5.1-bin-hadoop2.6/python/pyspark/shell.py"",
line 30, in <module>
    import pyspark
  File
""/home/yahsuan/Tool/spark-1.5.1-bin-hadoop2.6/python/pyspark/__init__.py"",
line 41, in <module>
    from pyspark.context import SparkContext
  File
""/home/yahsuan/Tool/spark-1.5.1-bin-hadoop2.6/python/pyspark/context.py"",
line 26, in <module>
    from pyspark import accumulators
  File
""/home/yahsuan/Tool/spark-1.5.1-bin-hadoop2.6/python/pyspark/accumulators.py"",
line 98, in <module>
    from pyspark.serializers import read_int, PickleSerializer
  File
""/home/yahsuan/Tool/spark-1.5.1-bin-hadoop2.6/python/pyspark/serializers.py"",
line 400, in <module>
    _hijack_namedtuple()
  File
""/home/yahsuan/Tool/spark-1.5.1-bin-hadoop2.6/python/pyspark/serializers.py"",
line 378, in _hijack_namedtuple
    _old_namedtuple = _copy_func(collections.namedtuple)
  File
""/home/yahsuan/Tool/spark-1.5.1-bin-hadoop2.6/python/pyspark/serializers.py"",
line 376, in _copy_func
    f.__defaults__, f.__closure__)
AttributeError: 'function' object has no attribute '__closure__'
And now for something completely different: ``the traces don't lie''

is this a known issue? any suggestion to resolve it? or how can I help to
fix this problem?

Thanks.
"
Josh Rosen <rosenville@gmail.com>,"Thu, 5 Nov 2015 00:06:04 -0800",Re: pyspark with pypy not work for spark-1.5.1,Chang Ya-Hsuan <sumtiogo@gmail.com>,"I noticed that you're using PyPy 2.2.1, but it looks like Spark 1.5.1's
docs say that we only support PyPy 2.3+. Could you try using a newer PyPy
version to see if that works?

I just checked and it looks like our Jenkins tests are running against PyPy
2.5.1, so that version is known to work. I'm not sure what the actual
minimum supported PyPy version is. Would you be interested in helping to
investigate so that we can update the documentation or produce a fix to
restore compatibility with earlier PyPy builds?


"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Thu, 5 Nov 2015 16:14:45 +0800",Re: pyspark with pypy not work for spark-1.5.1,Josh Rosen <rosenville@gmail.com>,"Thanks for your quickly reply.

I will test several pypy versions and report the result later.


l
,
"",
,
s.py"",
.py"",
.py"",
.py"",
o


-- 
-- 張雅軒
"
Shouheng Yi <shouhengyi@gmail.com>,"Thu, 5 Nov 2015 00:19:37 -0800",A New Global Numerical Optimization Algo,dev@spark.apache.org,"Hi all,

I hacked a new global optimizer that is based on Differential Evolution and
a local search algorithm. I can make it to solve 1000+ dimensional problems
on a cluster within a very reasonable error and time. It's using Akka's
actor system, so it scales very well and very easily just by tweaking
configuration files. I'm wondering if it is good enough to make it to
Spark's ecosystem.

Need some feedbacks from Spark/Scala lovers :)

You can find it here:
https://github.com/EstusDev/Estus/tree/master/estus-optimization

Details is documented in its wiki:
https://github.com/EstusDev/Estus/wiki/SolverMOS

Best,
Shouheng
"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Thu, 5 Nov 2015 16:30:20 +0800",Re: pyspark with pypy not work for spark-1.5.1,Josh Rosen <rosenville@gmail.com>,"I've test on following pypy version against to spark-1.5.1

  pypy-2.2.1
  pypy-2.3
  pypy-2.3.1
  pypy-2.4.0
  pypy-2.5.0
  pypy-2.5.1
  pypy-2.6.0
  pypy-2.6.1

I run

    $ PYSPARK_PYTHON=/path/to/pypy-xx.xx/bin/pypy
/path/to/spark-1.5.1/bin/pyspark

and only pypy-2.2.1 failed.

Any suggestion to run advanced test?


y
al
e
e
y"",
"",
rs.py"",
s.py"",
s.py"",
s.py"",



-- 
-- 張雅軒
"
Sjoerd Mulder <sjoerdmulder@gmail.com>,"Thu, 5 Nov 2015 15:37:51 +0100",Re: If you use Spark 1.5 and disabled Tungsten mode ...,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

I had version 2.6.1 in my project which was provided by the fine folks
from spring-boot-dependencies.

Now have overridden it to 2.7.8 :)

Sjoerd

2015-11-01 8:22 GMT+01:00 Reynold Xin <rxin@databricks.com>:

"
Christian <engrean@gmail.com>,"Thu, 5 Nov 2015 09:25:17 -0700",Recommended change to core-site.xml template,dev@spark.apache.org,"We ended up reading and writing to S3 a ton in our Spark jobs.
For this to work, we ended up having to add s3a, and s3 key/secret pairs.
We also had to add fs.hdfs.impl to get these things to work.

I thought maybe I'd share what we did and it might be worth adding these to
the spark conf for out of the box functionality with S3.

We created:
ec2/deploy.generic/root/spark-ec2/templates/root/spark/conf/core-site.xml

We changed the contents form the original, adding in the following:

  <property>
    <name>fs.file.impl</name>
    <value>org.apache.hadoop.fs.LocalFileSystem</value>
  </property>

  <property>
    <name>fs.hdfs.impl</name>
    <value>org.apache.hadoop.hdfs.DistributedFileSystem</value>
  </property>

  <property>
    <name>fs.s3.impl</name>
    <value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value>
  </property>

  <property>
    <name>fs.s3.awsAccessKeyId</name>
    <value>{{aws_access_key_id}}</value>
  </property>

  <property>
    <name>fs.s3.awsSecretAccessKey</name>
    <value>{{aws_secret_access_key}}</value>
  </property>

  <property>
    <name>fs.s3n.awsAccessKeyId</name>
    <value>{{aws_access_key_id}}</value>
  </property>

  <property>
    <name>fs.s3n.awsSecretAccessKey</name>
    <value>{{aws_secret_access_key}}</value>
  </property>

  <property>
    <name>fs.s3a.awsAccessKeyId</name>
    <value>{{aws_access_key_id}}</value>
  </property>

  <property>
    <name>fs.s3a.awsSecretAccessKey</name>
    <value>{{aws_secret_access_key}}</value>
  </property>

This change makes spark on ec2 work out of the box for us. It took us
several days to figure this out. It works for 1.4.1 and 1.5.1 on Hadoop
version 2.

Best Regards,
Christian
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 05 Nov 2015 16:35:56 +0000",Re: Recommended change to core-site.xml template,"Christian <engrean@gmail.com>, dev@spark.apache.org","Thanks for sharing this, Christian.

What build of Spark are you using? If I understand correctly, if you are
using Spark built against Hadoop 2.6+ then additional configs alone won't
help because additional libraries also need to be installed
<https://issues.apache.org/jira/browse/SPARK-7481>.

Nick


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 5 Nov 2015 08:36:53 -0800",Re: Recommended change to core-site.xml template,Christian <engrean@gmail.com>,"Thanks for investigating this. The right place to add these is the
core-site.xml template we have at
https://github.com/amplab/spark-ec2/blob/branch-1.5/templates/root/spark/conf/core-site.xml
and/or https://github.com/amplab/spark-ec2/blob/branch-1.5/templates/root/ephemeral-hdfs/conf/core-site.xml

Feel free to open a PR against the amplab/spark-ec2 repository for this.

Thanks
Shivaram


---------------------------------------------------------------------


"
Christian <engrean@gmail.com>,"Thu, 5 Nov 2015 10:17:22 -0700",Re: Recommended change to core-site.xml template,Nicholas Chammas <nicholas.chammas@gmail.com>,"I am using both 1.4.1 and 1.5.1. In the end, we used 1.5.1 because of the
new feature for instance-profile which greatly helps with this as well.
Without the instance-profile, we got it working by copying a
.aws/credentials file up to each node. We could easily automate that
through the templates.

I don't need any additional libraries. We just need to change the
core-site.xml

-Christian


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 05 Nov 2015 17:28:15 +0000",Re: Recommended change to core-site.xml template,Christian <engrean@gmail.com>,"
That's the Spark version. I'm wondering what version of Hadoop your Spark
is built against.

For example, when you download Spark
<http://spark.apache.org/downloads.html> you have to select from a number
of packages (under ""Choose a package type""), and each is built against a
different version of Hadoop. When Spark is built against Hadoop 2.6+, from
my understanding, you need to install additional libraries
<https://issues.apache.org/jira/browse/SPARK-7481> to access S3. When Spark
is built against Hadoop 2.4 or earlier, you don't need to do this.

I'm confirming that this is what is happening in your case.

Nick


"
Reynold Xin <rxin@databricks.com>,"Thu, 5 Nov 2015 10:07:52 -0800",Re: How to force statistics calculation of Dataframe?,Charmee Patel <charmeep@gmail.com>,"If your data came from RDDs (i.e. not a file system based data source), and
you don't want to cache, then no ....



"
Josh Rosen <joshrosen@databricks.com>,"Thu, 05 Nov 2015 18:23:14 +0000",Re: pyspark with pypy not work for spark-1.5.1,"Chang Ya-Hsuan <sumtiogo@gmail.com>, Josh Rosen <rosenville@gmail.com>","You could try running PySpark's own unit tests. Try ./python/run-tests
--help for instructions.


:
Py
ual
o
ce
o
le
,
py"",
y"",
ors.py"",
rs.py"",
rs.py"",
rs.py"",
"
"""Dilip Biswal"" <dbiswal@us.ibm.com>","Thu, 5 Nov 2015 10:21:23 -0800",Re: Master build fails ?,,"Hello,

I am getting the same build error about not being able to find 
com.google.common.hash.HashCodes.

Is there a solution to this ?

Regards,
Dilip Biswal
Tel: 408-463-4980
dbiswal@us.ibm.com



From:   Jean-Baptiste Onofré <jb@nanthrax.net>
To:     Ted Yu <yuzhihong@gmail.com>
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   11/03/2015 07:20 AM
Subject:        Re: Master build fails ?



Hi Ted,

thanks for the update. The build with sbt is in progress on my box.

Regards
JB

On 11/03/2015 03:31 PM, Ted Yu wrote:
> Interesting, Sbt builds were not all failing:
>
> https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/
>
> FYI
>
> On Tue, Nov 3, 2015 at 5:58 AM, Jean-Baptiste Onofré <jb@nanthrax.net
> <mailto:jb@nanthrax.net>> wrote:
>
>     Hi Jacek,
>
>     it works fine with mvn: the problem is with sbt.
>
>     I suspect a different reactor order in sbt compare to mvn.
>
>     Regards
>     JB
>
>     On 11/03/2015 02:44 PM, Jacek Laskowski wrote:
>
>         Hi,
>
>         Just built the sources using the following command and it worked
>         fine.
>
>         ➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6
>         -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver
>         -DskipTests clean install
>         ...
>         [INFO]
> ------------------------------------------------------------------------
>         [INFO] BUILD SUCCESS
>         [INFO]
> ------------------------------------------------------------------------
>         [INFO] Total time: 14:15 min
>         [INFO] Finished at: 2015-11-03T14:40:40+01:00
>         [INFO] Final Memory: 438M/1972M
>         [INFO]
> ------------------------------------------------------------------------
>
>         ➜  spark git:(master) ✗ java -version
>         java version ""1.8.0_66""
>         Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
>         Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
>
>         I'm on Mac OS.
>
>         Pozdrawiam,
>         Jacek
>
>         --
>         Jacek Laskowski | http://blog.japila.pl |
>         http://blog.jaceklaskowski.pl
>         Follow me at https://twitter.com/jaceklaskowski
>         Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski
>
>
>         On Tue, Nov 3, 2015 at 1:37 PM, Jean-Baptiste Onofré
>         <jb@nanthrax.net <mailto:jb@nanthrax.net>> wrote:
>
>             Thanks for the update, I used mvn to build but without hive
>             profile.
>
>             Let me try with mvn with the same options as you and sbt 
also.
>
>             I keep you posted.
>
>             Regards
>             JB
>
>             On 11/03/2015 12:55 PM, Jeff Zhang wrote:
>
>
>                 I found it is due to SPARK-11073.
>
>                 Here's the command I used to build
>
>                 build/sbt clean compile -Pyarn -Phadoop-2.6 -Phive
>                 -Phive-thriftserver
>                 -Psparkr
>
>                 On Tue, Nov 3, 2015 at 7:52 PM, Jean-Baptiste Onofré
>                 <jb@nanthrax.net <mailto:jb@nanthrax.net>
>                 <mailto:jb@nanthrax.net <mailto:jb@nanthrax.net>>> 
wrote:
>
>                       Hi Jeff,
>
>                       it works for me (with skipping the tests).
>
>                       Let me try again, just to be sure.
>
>                       Regards
>                       JB
>
>
>                       On 11/03/2015 11:50 AM, Jeff Zhang wrote:
>
>                           Looks like it's due to guava version
>                 conflicts, I see both guava
>                           14.0.1
>                           and 16.0.1 under lib_managed/bundles. Anyone
>                 meet this issue too ?
>
>                           [error]
>
> 
/Users/jzhang/github/spark_apache/core/src/main/scala/org/apache/spark/SecurityManager.scala:26:
>                           object HashCodes is not a member of package
>                 com.google.common.hash
>                           [error] import 
com.google.common.hash.HashCodes
>                           [error]        ^
>                           [info] Resolving
>                 org.apache.commons#commons-math;2.2 ...
>                           [error]
>
> 
/Users/jzhang/github/spark_apache/core/src/main/scala/org/apache/spark/SecurityManager.scala:384:
>                           not found: value HashCodes
>                           [error]         val cookie =
>                 HashCodes.fromBytes(secret).toString()
>                           [error]                      ^
>
>
>
>
>                           --
>                           Best Regards
>
>                           Jeff Zhang
>
>
>                       --
>                       Jean-Baptiste Onofré
>                 jbonofre@apache.org <mailto:jbonofre@apache.org>
>                 <mailto:jbonofre@apache.org <mailto:jbonofre@apache.org
>>
>                 http://blog.nanthrax.net
>                       Talend - http://www.talend.com
>
>
> ---------------------------------------------------------------------
>                       To unsubscribe, e-mail:
>                 dev-unsubscribe@spark.apache.org
>                 <mailto:dev-unsubscribe@spark.apache.org>
>                       <mailto:dev-unsubscribe@spark.apache.org
>                 <mailto:dev-unsubscribe@spark.apache.org>>
>                       For additional commands, e-mail:
>                 dev-help@spark.apache.org <
mailto:dev-help@spark.apache.org>
>                       <mailto:dev-help@spark.apache.org
>                 <mailto:dev-help@spark.apache.org>>
>
>
>
>
>                 --
>                 Best Regards
>
>                 Jeff Zhang
>
>
>
>             --
>             Jean-Baptiste Onofré
>             jbonofre@apache.org <mailto:jbonofre@apache.org>
>             http://blog.nanthrax.net
>             Talend - http://www.talend.com
>
> ---------------------------------------------------------------------
>             To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>             <mailto:dev-unsubscribe@spark.apache.org>
>             For additional commands, e-mail: dev-help@spark.apache.org
>             <mailto:dev-help@spark.apache.org>
>
>
> ---------------------------------------------------------------------
>         To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>         <mailto:dev-unsubscribe@spark.apache.org>
>         For additional commands, e-mail: dev-help@spark.apache.org
>         <mailto:dev-help@spark.apache.org>
>
>
>     --
>     Jean-Baptiste Onofré
>     jbonofre@apache.org <mailto:jbonofre@apache.org>
>     http://blog.nanthrax.net
>     Talend - http://www.talend.com
>
> ---------------------------------------------------------------------
>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org>
>
>

-- 
Jean-Baptiste Onofré
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org




"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Nov 2015 10:44:51 -0800",Re: Master build fails ?,Jeff Zhang <zjffdu@gmail.com>,"Hi Jeff,


What command line are you using to build? Can you run ""mvn
dependency:tree"" (with all the other options you're using) to figure
out where guava 16 is coming from? Locally I only see version 14,
compiling against hadoop 2.5.0.

-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 5 Nov 2015 10:46:18 -0800",Re: Master build fails ?,"Dilip Biswal <dbiswal@us.ibm.com>
	""dev@spark.apache.org"" <dev@spark.apache.org>","Dilip:
Can you give the command you used ?

Which release were you building ?
What OS did you build on ?

Cheers


net
d
doop-2.6
r
i

curityManager.scala:26:
es
curityManager.scala:384:
--
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Nov 2015 10:53:47 -0800",Re: Master build fails ?,Jeff Zhang <zjffdu@gmail.com>,"Seems like it's an sbt issue, not a maven one, so ""dependency:tree""
might not help. Still, the command line would be helpful. I use sbt
and don't see this.




-- 
Marcelo

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 5 Nov 2015 19:04:13 +0000",Re: Master build fails ?,Marcelo Vanzin <vanzin@cloudera.com>,"SBT/ivy pulls in the most recent version of a JAR in, whereas maven pulls in the ""closest"", where closest is lowest distance/depth from the root.


te:
 and


---------------------------------------------------------------------


"
"""Dilip Biswal"" <dbiswal@us.ibm.com>","Thu, 5 Nov 2015 11:07:09 -0800",Re: Master build fails ?,Ted Yu <yuzhihong@gmail.com>,"Hello Ted,

Thanks for your response.

Here is the command i used :

build/sbt clean
build/sbt -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver 
-Dhadoop.version=2.6.0 -DskipTests assembly

I am building on CentOS and on master branch.

One other thing, i was able to build fine with the above command up until 
recently. I think i have stared
to have problem after SPARK-11073 where the HashCodes import was added.

Regards,
Dilip Biswal
Tel: 408-463-4980
dbiswal@us.ibm.com



From:   Ted Yu <yuzhihong@gmail.com>
To:     Dilip Biswal/Oakland/IBM@IBMUS
Cc:     Jean-Baptiste Onofré <jb@nanthrax.net>, ""dev@spark.apache.org"" 
<dev@spark.apache.org>
Date:   11/05/2015 10:46 AM
Subject:        Re: Master build fails ?



Dilip:
Can you give the command you used ?

Which release were you building ?
What OS did you build on ?

Cheers

On Thu, Nov 5, 2015 at 10:21 AM, Dilip Biswal <dbiswal@us.ibm.com> wrote:
Hello,

I am getting the same build error about not being able to find 
com.google.common.hash.HashCodes.

Is there a solution to this ?

Regards,
Dilip Biswal
Tel: 408-463-4980
dbiswal@us.ibm.com



From:        Jean-Baptiste Onofré <jb@nanthrax.net>
To:        Ted Yu <yuzhihong@gmail.com>
Cc:        ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:        11/03/2015 07:20 AM
Subject:        Re: Master build fails ?



Hi Ted,

thanks for the update. The build with sbt is in progress on my box.

Regards
JB

On 11/03/2015 03:31 PM, Ted Yu wrote:
> Interesting, Sbt builds were not all failing:
>
> https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/
>
> FYI
>
> On Tue, Nov 3, 2015 at 5:58 AM, Jean-Baptiste Onofré <jb@nanthrax.net
> <mailto:jb@nanthrax.net>> wrote:

>
>     Hi Jacek,
>
>     it works fine with mvn: the problem is with sbt.
>
>     I suspect a different reactor order in sbt compare to mvn.
>
>     Regards
>     JB
>
>     On 11/03/2015 02:44 PM, Jacek Laskowski wrote:
>
>         Hi,
>
>         Just built the sources using the following command and it worked
>         fine.
>
>         ➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6
>         -Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver
>         -DskipTests clean install
>         ...
>         [INFO]
>         
------------------------------------------------------------------------
>         [INFO] BUILD SUCCESS
>         [INFO]
>         
------------------------------------------------------------------------
>         [INFO] Total time: 14:15 min
>         [INFO] Finished at: 2015-11-03T14:40:40+01:00
>         [INFO] Final Memory: 438M/1972M
>         [INFO]
>         
------------------------------------------------------------------------
>
>         ➜  spark git:(master) ✗ java -version
>         java version ""1.8.0_66""
>         Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
>         Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
>
>         I'm on Mac OS.
>
>         Pozdrawiam,
>         Jacek
>
>         --
>         Jacek Laskowski | 
http://blog.japila.pl|
>         http://blog.jaceklaskowski.pl

>         Follow me at https://twitter.com/jaceklaskowski
>         Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski

>
>
>         On Tue, Nov 3, 2015 at 1:37 PM, Jean-Baptiste Onofré
>         <jb@nanthrax.net <mailto:jb@nanthrax.net>> wrote:
>
>             Thanks for the update, I used mvn to build but without hive
>             profile.
>
>             Let me try with mvn with the same options as you and sbt 
also.
>
>             I keep you posted.
>
>             Regards
>             JB
>
>             On 11/03/2015 12:55 PM, Jeff Zhang wrote:
>
>
>                 I found it is due to SPARK-11073.
>
>                 Here's the command I used to build
>
>                 build/sbt clean compile -Pyarn -Phadoop-2.6 -Phive
>                 -Phive-thriftserver
>                 -Psparkr
>
>                 On Tue, Nov 3, 2015 at 7:52 PM, Jean-Baptiste Onofré
>                 <jb@nanthrax.net <mailto:jb@nanthrax.net>
>                 <mailto:jb@nanthrax.net<mailto:jb@nanthrax.net>>> wrote:

>
>                       Hi Jeff,
>
>                       it works for me (with skipping the tests).
>
>                       Let me try again, just to be sure.
>
>                       Regards
>                       JB
>
>
>                       On 11/03/2015 11:50 AM, Jeff Zhang wrote:
>
>                           Looks like it's due to guava version
>                 conflicts, I see both guava
>                           14.0.1
>                           and 16.0.1 under lib_managed/bundles. Anyone
>                 meet this issue too ?
>
>                           [error]
>
>                 
/Users/jzhang/github/spark_apache/core/src/main/scala/org/apache/spark/SecurityManager.scala:26:
>                           object HashCodes is not a member of package
>                 com.google.common.hash
>                           [error] import 
com.google.common.hash.HashCodes
>                           [error]        ^
>                           [info] Resolving
>                 org.apache.commons#commons-math;2.2 ...
>                           [error]
>
>                 
/Users/jzhang/github/spark_apache/core/src/main/scala/org/apache/spark/SecurityManager.scala:384:
>                           not found: value HashCodes
>                           [error]         val cookie =
>                 HashCodes.fromBytes(secret).toString()
>                           [error]                      ^
>
>
>
>
>                           --
>                           Best Regards
>
>                           Jeff Zhang
>
>
>                       --
>                       Jean-Baptiste Onofré
>                 jbonofre@apache.org <
mailto:jbonofre@apache.org>
>                 <mailto:jbonofre@apache.org<mailto:jbonofre@apache.org>>
>                 http://blog.nanthrax.net
>                       Talend - http://www.talend.com
>
>
>                   
---------------------------------------------------------------------
>                       To unsubscribe, e-mail:
>                 dev-unsubscribe@spark.apache.org
>                 <mailto:dev-unsubscribe@spark.apache.org>
>                       <mailto:dev-unsubscribe@spark.apache.org
>                 <mailto:dev-unsubscribe@spark.apache.org>>
>                       For additional commands, e-mail:
>                 dev-help@spark.apache.org <
mailto:dev-help@spark.apache.org>
>                       <mailto:dev-help@spark.apache.org

>                 <mailto:dev-help@spark.apache.org>>
>
>
>
>
>                 --
>                 Best Regards
>
>                 Jeff Zhang
>
>
>
>             --
>             Jean-Baptiste Onofré
>             jbonofre@apache.org <mailto:jbonofre@apache.org>
>             http://blog.nanthrax.net
>             Talend - http://www.talend.com
>
>             
---------------------------------------------------------------------
>             To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>             <mailto:dev-unsubscribe@spark.apache.org>
>             For additional commands, e-mail: dev-help@spark.apache.org
>             <mailto:dev-help@spark.apache.org>
>
>
>         
---------------------------------------------------------------------
>         To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>         <mailto:dev-unsubscribe@spark.apache.org>
>         For additional commands, e-mail: dev-help@spark.apache.org
>         <mailto:dev-help@spark.apache.org>
>
>
>     --
>     Jean-Baptiste Onofré
>     jbonofre@apache.org <mailto:jbonofre@apache.org>
>     http://blog.nanthrax.net
>     Talend - http://www.talend.com
>
>     
---------------------------------------------------------------------
>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org>
>
>

-- 
Jean-Baptiste Onofré
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org






"
shane knapp <sknapp@berkeley.edu>,"Thu, 5 Nov 2015 11:08:59 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","well, i forgot to put this on my calendar and didn't get around to
getting it done this morning.  :)

anyways, i'll be shooting for tomorrow (friday) morning instead.

shane


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 5 Nov 2015 11:37:47 -0800",Re: Master build fails ?,Dilip Biswal <dbiswal@us.ibm.com>,"build/sbt -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver
-Dhadoop.version=2.6.0 -DskipTests assembly

The above command fails on Mac.

build/sbt -Pyarn -Phadoop-2.2 -Phive -Phive-thriftserver -Pkinesis-asl
-DskipTests assembly

The above command, used by Jenkins, passes.
That's why the build error wasn't caught.

FYI


.org""
t>>
.net*
d
doop-2.6
r

curityManager.scala:26:
es
curityManager.scala:384:
<
*
--
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Nov 2015 11:44:34 -0800",Re: Master build fails ?,Ted Yu <yuzhihong@gmail.com>,"Does anyone know how to get something similar to ""mvn dependency:tree"" from sbt?

mvn dependency:tree with hadoop 2.6.0 does not show any instances of guava 16...

l
e.org""
:
.net
ed
adoop-2.6
er
--
--
--
)
ki
e

e:
/SecurityManager.scala:26:
/SecurityManager.scala:384:



-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Nov 2015 11:55:49 -0800",Re: Master build fails ?,Ted Yu <yuzhihong@gmail.com>,"Answering my own question: ""dependency-graph""

:
om sbt?
a 16...
:
il
he.org""
e:
x.net
ked
hadoop-2.6
ver
---
---
---
e)
ski
ve
é
te:
e
k/SecurityManager.scala:26:
k/SecurityManager.scala:384:
g>>
g



-- 
Marcelo

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 05 Nov 2015 20:01:39 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),"Egor Pahomov <pahomov.egor@gmail.com>, Sean Owen <sowen@cloudera.com>","-0

The spark-ec2 version is still set to 1.5.1
<https://github.com/apache/spark/blob/v1.5.2-rc2/ec2/spark_ec2.py#L54>.

Nick


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Nov 2015 12:07:08 -0800",Re: Master build fails ?,Ted Yu <yuzhihong@gmail.com>,"Man that command is slow. Anyway, it seems guava 16 is being brought
transitively by curator 2.6.0 which should have been overridden by the
explicit dependency on curator 2.4.0, but apparently, as Steve
mentioned, sbt/ivy decided to break things, so I'll be adding some
exclusions.

:
te:
rom sbt?
va 16...
e:
til
.
che.org""
te:
ax.net
rked
Phadoop-2.6
rver
----
----
----
de)
wski
ive
é
ote:
ne
rk/SecurityManager.scala:26:
e
rk/SecurityManager.scala:384:
rg>>
-
-
rg
-
-



-- 
Marcelo

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 5 Nov 2015 12:20:12 -0800",Re: Spark 1.6 Release Schedule,Sean Owen <sowen@cloudera.com>,"Sorry for the delay due to traveling...

The branch has been cut.  At this point anything that we want to go into
Spark 1.6 will need to be cherry-picked.  Please be cautious when doing so,
and contact me if you are uncertain.

Michael


"
Christian <engrean@gmail.com>,"Thu, 5 Nov 2015 13:28:53 -0700",Re: Recommended change to core-site.xml template,Nicholas Chammas <nicholas.chammas@gmail.com>,"Spark 1.5.1-hadoop1


"
Christian <engrean@gmail.com>,"Thu, 5 Nov 2015 13:53:49 -0700",Re: Recommended change to core-site.xml template,Nicholas Chammas <nicholas.chammas@gmail.com>,"I created the cluster with the following:

--hadoop-major-version=2
--spark-version=1.4.1

from: spark-1.5.1-bin-hadoop1

Are you saying there might be different behavior if I download
spark-1.5.1-hadoop-2.6 and create my cluster?


"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Thu, 5 Nov 2015 17:34:01 -0500",Need advice on hooking into Sql query plan,dev <dev@spark.apache.org>,"Hi folks, not sure if this belongs to dev or user list..sending to dev as
it seems a bit convoluted.

I have a UI in which we allow users to write ad-hoc queries against a (very
large, partitioned) table. I would like to analyze the queries prior to
execution for two purposes:

1. Reject under-constrained queries (i.e. there is a field predicate that I
want to make sure is always present)
2. Augment the query with additional predicates (e.g if the user asks for a
student_id I also want to push a constraint on another field)

I could parse the sql string before passing to spark but obviously spark
already does this anyway. Can someone give me general direction on how to
do this (if possible).

Something like

myDF = sql(""user_sql_query"")
myDF.queryExecution.logical  //here examine the filters provided by user,
reject if underconstrained, push new filters as needed (via
withNewChildren?)

at this point with some luck I'd have a new LogicalPlan -- what is the
proper way to create an execution plan on top of this new Plan? Im looking
at this
https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L329
but this method is restricted to the package. I'd really prefer to hook
into this as early as possible and still let spark run the plan
optimizations as usual.

Any guidance or pointers much appreciated.
"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Thu, 5 Nov 2015 23:50:09 +0100",Re: Need advice on hooking into Sql query plan,yana.kadiyska@gmail.com,"Would it be possible to use views to address some of your requirements?

Alternatively it might be better to parse it yourself. There are open source libraries for it, if you need really a complete sql parser. Do you want to do it on sub queries?

t seems a bit convoluted.
y large, partitioned) table. I would like to analyze the queries prior to execution for two purposes:
 want to make sure is always present)
 student_id I also want to push a constraint on another field)
lready does this anyway. Can someone give me general direction on how to do this (if possible).
eject if underconstrained, push new filters as needed (via withNewChildren?)
per way to create an execution plan on top of this new Plan? Im looking at this https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L329 but this method is restricted to the package. I'd really prefer to hook into this as early as possible and still let spark run the plan optimizations as usual.
"
Jakob Odersky <jodersky@gmail.com>,"Thu, 5 Nov 2015 15:38:49 -0800",State of the Build,dev@spark.apache.org,"Hi everyone,
in the process of learning Spark, I wanted to get an overview of the
interaction between all of its sub-projects. I therefore decided to have a
look at the build setup and its dependency management.
Since I am alot more comfortable using sbt than maven, I decided to try to
port the maven configuration to sbt (with the help of automated tools).
This led me to a couple of observations and questions on the build system
design:

First, currently, there are two build systems, maven and sbt. Is there a
preferred tool (or future direction to one)?

Second, the sbt build also uses maven ""profiles"" requiring the use of
specific commandline parameters when starting sbt. Furthermore, since it
relies on maven poms, dependencies to the scala binary version (_2.xx) are
hardcoded and require running an external script when switching versions.
Sbt could leverage built-in constructs to support cross-compilation and
emulate profiles with configurations and new build targets. This would
remove external state from the build (in that no extra steps need to be
performed in a particular order to generate artifacts for a new
configuration) and therefore improve stability and build reproducibility
(maybe even build performance). I was wondering if implementing such
functionality for the sbt build would be welcome?

thanks,
--Jakob
"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Thu, 5 Nov 2015 19:18:51 -0500",Re: Need advice on hooking into Sql query plan,=?UTF-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"I don't think a view would help -- in the case of under-constraining, I
want to make sure that the user is constraining a column (e.g. I want to
restrict them to querying a single partition at a time but I don't care
which one)...a view per partition value is not practical due to the fairly
high cardinality...

In the case of predicate augmentation, the additional predicate depends on
the value the user is providing e.g. my data is partitioned under
teacherName but the end users don't have this information...So if they ask
for student_id=""1234"" I'd like to add ""teacherName='Smith'"" based on a
mapping that is not surfaced to the user (sorry for the contrived
example)...But I don't think I can do this with a view. A join will produce
the right answer but is counter-productive as my goal is to minimize the
partitions being processed.

I can parse the query myself -- I was not fond of this solution as I'd go
sql string to parse tree back to augmented sql string only to have spark
repeat the first part of the exercise....but will do if need be. And yes,
I'd have to be able to process sub-queries too...

te:

r
g
pache/spark/sql/hive/HiveContext.scala#L329
"
Reynold Xin <rxin@databricks.com>,"Thu, 5 Nov 2015 16:20:19 -0800",Re: Need advice on hooking into Sql query plan,Yana Kadiyska <yana.kadiyska@gmail.com>,"You can hack around this by constructing logical plans yourself and then
creating a DataFrame in order to execute them. Note that this is all
depending on internals of the framework and can break when Spark upgrades.



y
n
k
a
ce
rote:
u
s
or
t
r
o
ng
apache/spark/sql/hive/HiveContext.scala#L329
"
Stephen Boesch <javadba@gmail.com>,"Thu, 5 Nov 2015 16:30:17 -0800",Re: State of the Build,Jakob Odersky <jodersky@gmail.com>,"Yes. The current dev/change-scala-version.sh mutates (/pollutes) the build
environment by updating the pom.xml in each of the subprojects. If you were
able to come up with a structure that avoids that approach it would be an
improvement.

2015-11-05 15:38 GMT-08:00 Jakob Odersky <jodersky@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Thu, 5 Nov 2015 16:44:31 -0800",Re: State of the Build,Stephen Boesch <javadba@gmail.com>,"See previous discussion:
http://search-hadoop.com/m/q3RTtPnPnzwOhBr

FYI


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 5 Nov 2015 17:03:26 -0800",Re: State of the Build,"""dev@spark.apache.org"" <dev@spark.apache.org>","There was a lot of discussion that preceded our arriving at this statement
in the Spark documentation: ""Maven is the official build tool recommended
for packaging Spark, and is the build of reference.""
https://spark.apache.org/docs/latest/building-spark.html#building-with-sbt

I'm not aware of anything new in the way of SBT tooling or your post,
Jakob, that would lead us to reconsider the choice of Maven over SBT for
the reference build of Spark.  Of course, I'm by no means the sole and
final authority on the matter, but at least I am not seeing anything in
your suggested approach that hasn't already been considered.  You're
welcome to review the prior discussion and try to convince us that we've
made the wrong choice, but I wouldn't expect that to be a quick and easy
process.



"
Koert Kuipers <koert@tresata.com>,"Thu, 5 Nov 2015 20:48:59 -0500",Re: State of the Build,Jakob Odersky <jodersky@gmail.com>,"People who do upstream builds of spark (think bigtop and hadoop distros)
are used to legacy systems like maven, so maven is the default build. I
don't think it will change.

Any improvements for the sbt build are of course welcome (it is still used
by many developers), but i would not do anything that increases the burden
of maintaining two build systems.

"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Nov 2015 18:07:04 -0800",Re: Master build fails ?,Ted Yu <yuzhihong@gmail.com>,"FYI I pushed a fix for this to github; so if you pull everything
should work now.

:
te:
ote:
from sbt?
ava 16...
te:
ntil
d.
ache.org""
ote:
rax.net
orked
-Phadoop-2.6
erver
-----
-----
-----
ode)
owski
hive
t
é
rote:
one
ark/SecurityManager.scala:26:
ge
ark/SecurityManager.scala:384:
org>>
--
--
g
org
--
--



-- 
Marcelo

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 06 Nov 2015 03:10:22 +0000",Re: Recommended change to core-site.xml template,Christian <engrean@gmail.com>,"Yep, I think if you try spark-1.5.1-hadoop-2.6 you will find that you
cannot access S3, unfortunately.


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 6 Nov 2015 03:02:05 +0000",RE: dataframe slow down with tungsten turn on,"gen tang <gen.tang86@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","What’s the big size of the raw data and the result data? Is that any other changes like HDFS, Spark configuration, your own code etc. besides the Spark binary? Can you monitor the IO/CPU state while executing the final stage, and it will be great if you can paste the call stack if you observe the high CPU utilization.

And can you try not to cache anything and repeat the same step? Just be sure it’s not caused by the memory stuff.

From: gen tang [mailto:gen.tang86@gmail.com]
Sent: Friday, November 6, 2015 12:18 AM
To: dev@spark.apache.org
Subject: Fwd: dataframe slow down with tungsten turn on


---------- Forwarded message ----------
From: gen tang <gen.tang86@gmail.com<mailto:gen.tang86@gmail.com>>
Date: Fri, Nov 6, 2015 at 12:14 AM
Subject: Re: dataframe slow down with tungsten turn on
To: ""Cheng, Hao"" <hao.cheng@intel.com<mailto:hao.cheng@intel.com>>

Hi,

My application is as follows:
1. create dataframe from hive table
2. transform dataframe to rdd of json and do some aggregations on json (in fact, I use pyspark, so it is rdd of dict)
3. retransform rdd of json to dataframe and cache it (triggered by count)
4. join several dataframe which is created by the above steps.
5. save final dataframe as json.(by dataframe write api)

There are a lot of stages, other stage is quite the same under two version of spark. However, the final step (save as json) is 1 min vs. 2 hour. In my opinion, I think it is writing to hdfs cause the slowness of final stage. However, I don't know why...

In fact, I make a mistake about the version of spark that I used. The spark which runs faster is build on source code of spark 1.4.1. The spark which runs slower is build on source code of spark 1.5.2, 2 days ago.

Any idea? Thanks a lot.

Cheers
Gen


On Thu, Nov 5, 2015 at 1:01 PM, Cheng, Hao <hao.cheng@intel.com<mailto:hao.cheng@intel.com>> wrote:
BTW, 1 min V.S. 2 Hours, seems quite weird, can you provide more information on the ETL work?

From: Cheng, Hao [mailto:hao.cheng@intel.com<mailto:hao.cheng@intel.com>]
Sent: Thursday, November 5, 2015 12:56 PM
To: gen tang; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: RE: dataframe slow down with tungsten turn on

1.5 has critical performance / bug issues, you’d better try 1.5.1 or 1.5.2rc version.

From: gen tang [mailto:gen.tang86@gmail.com]
Sent: Thursday, November 5, 2015 12:43 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Fwd: dataframe slow down with tungsten turn on

Hi,

In fact, I tested the same code with spark 1.5 with tungsten turning off. The result is quite the same as tungsten turning on.
It seems that it is not the problem of tungsten, it is simply that spark 1.5 is slower than spark 1.4.

Is there any idea about why it happens?
Thanks a lot in advance

Cheers
Gen


---------- Forwarded message ----------
From: gen tang <gen.tang86@gmail.com<mailto:gen.tang86@gmail.com>>
Date: Wed, Nov 4, 2015 at 3:54 PM
Subject: dataframe slow down with tungsten turn on
To: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
Hi sparkers,

I am using dataframe to do some large ETL jobs.
More precisely, I create dataframe from HIVE table and do some operations. And then I save it as json.

When I used spark-1.4.1, the whole process is quite fast, about 1 mins. However, when I use the same code with spark-1.5.1(with tungsten turn on), it takes a about 2 hours to finish the same job.

I checked the detail of tasks, almost all the time is consumed by computation.
[https://owa.gf.com.cn/owa/service.svc/s/GetFileAttachment?id=AAMkAGEzNGJiN2Q4LTI2ODYtNGIyYS1hYWIyLTMzMTYxOGQzYTViNABGAAAAAACPuqp5iM6mRqg7wmvE6c8KBwBKGW%2B6dpgjRb4BfC%2BACXJIAAAAAAEPAABKGW%2B6dpgjRb4BfC%2BACXJIAAAAQcF3AAABEgAQAIeCeL7UEe9GhqECpYfXhDI%3D&X-OWA-CANARY=7U3OIyan90CkQzeCMSlDnFM6WrDs5NIIksHvCIBBNwcmtRNW4tO1_1WPFeb51C1IsASUo1jqj_A.]
Any idea about why this happens?

Thanks a lot in advance for your help.

Cheers
Gen




"
Christian <engrean@gmail.com>,"Fri, 06 Nov 2015 05:22:10 +0000",Re: Recommended change to core-site.xml template,Nicholas Chammas <nicholas.chammas@gmail.com>,"Even with the changes I mentioned above?

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 06 Nov 2015 05:35:23 +0000",Re: Recommended change to core-site.xml template,Christian <engrean@gmail.com>,"I might be mistaken, but yes, even with the changes you mentioned you will
not be able to access S3 if Spark is built against Hadoop 2.6+ unless you
install additional libraries. The issue is explained in SPARK-7481
<https://issues.apache.org/jira/browse/SPARK-7481> and SPARK-7442
<https://issues.apache.org/jira/browse/SPARK-7442>.


"
Christian <engrean@gmail.com>,"Fri, 06 Nov 2015 05:36:25 +0000",Re: Recommended change to core-site.xml template,Nicholas Chammas <nicholas.chammas@gmail.com>,"Oh right. I forgot about the libraries being removed.

"
Sean Owen <sowen@cloudera.com>,"Fri, 6 Nov 2015 06:53:24 +0000",Re: State of the Build,Koert Kuipers <koert@tresata.com>,"Maven isn't 'legacy', or supported for the benefit of third parties.
SBT had some behaviors / problems that Maven didn't relative to what
Spark needs. SBT is a development-time alternative only, and partly
generated from the Maven build.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 5 Nov 2015 23:23:59 -0800",Re: State of the Build,Sean Owen <sowen@cloudera.com>,"Hey Jakob,

The builds in Spark are largely maintained by me, Sean, and Michael
Armbrust (for SBT). For historical reasons, Spark supports both a Maven and
SBT build. Maven is the build of reference for packaging Spark and is used
by many downstream packagers and to build all Spark releases. SBT is more
often used by developers. Both builds inherit from the same pom files (and
rely on the same profiles) to minimize maintenance complexity of Spark's
very complex dependency graph.

If you are looking to make contributions that help with the build, I am
happy to point you towards some things that are consistent maintenance
headaches. There are two major pain points right now that I'd be thrilled
to see fixes for:

1. SBT relies on a different dependency conflict resolution strategy than
maven - causing all kinds of headaches for us. I have heard that newer
versions of SBT can (maybe?) use Maven as a dependency resolver instead of
Ivy. This would make our life so much better if it were possible, either by
virtue of upgrading SBT or somehow doing this ourselves.

2. We don't have a great way of auditing the net effect of dependency
changes when people make them in the build. I am working on a fairly clunky
patch to do this here:

https://github.com/apache/spark/pull/8531

It could be done much more nicely using SBT, but only provided (1) is
solved.

Doing a major overhaul of the sbt build to decouple it from pom files, I'm
not sure that's the best place to start, given that we need to continue to
support maven - the coupling is intentional. But getting involved in the
build in general would be completely welcome.

- Patrick


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 6 Nov 2015 10:21:19 +0000",Re: Master build fails ?,Marcelo Vanzin <vanzin@cloudera.com>,"


It's not that ivy is bad per-se, only that it has a different policy, one which holds *provided all later versions of JARs are backwards compatible*

Maven's closest-first policy has a different flaw, namely that its not always obvious why a guava 14.0 that is two hops of transitiveness should take priority over a 16.0 version three hops away. Especially when that 0.14 version should have come

If you look at the the diffs for the hive 1.2.1 update patch, the final week was pretty much frittered away trying to get the two builds to have consistent versions of things.

1. I should have historical commit rights to ivy, so, transitively to SBT's dependency logic. If someone writes a resolver with the same behaviour as maven2 I'll see about getting it in.

2. Hadoop 2.6 is on curator 2.7.1; HADOOP-11492. To verify it worked against guava 11.02, I ended up compiling curator against that version to see what broke. curator-x-discovery is the only module which doesn't compile against older guava versions (HADOOP-11102)

-Steve

---------------------------------------------------------------------


"
Chang Ya-Hsuan <sumtiogo@gmail.com>,"Fri, 6 Nov 2015 18:27:16 +0800",Re: pyspark with pypy not work for spark-1.5.1,Josh Rosen <joshrosen@databricks.com>,"Hi I run ./python/ru-tests to test following modules of spark-1.5.1:

[pyspark-core', 'pyspark-ml', 'pyspark-mllib', 'pyspark-sql',
'pyspark-streaming]

against to following pypy versions:

pypy-2.2.1  pypy-2.3  pypy-2.3.1  pypy-2.4.0  pypy-2.5.0  pypy-2.5.1
 pypy-2.6.0  pypy-2.6.1  pypy-4.0.0

except pypy-2.2.1, all others pass the test.

the error message of pypy-2.2.1 is:

Traceback (most recent call last):
  File ""app_main.py"", line 72, in run_toplevel
  File ""/home/yahsuan/.pyenv/versions/pypy-2.2.1/lib-python/2.7/runpy.py"",
line 151, in _run_module_as_main
    mod_name, loader, code, fname = _get_module_details(mod_name)
  File ""/home/yahsuan/.pyenv/versions/pypy-2.2.1/lib-python/2.7/runpy.py"",
line 101, in _get_module_details
    loader = get_loader(mod_name)
  File
""/home/yahsuan/.pyenv/versions/pypy-2.2.1/lib-python/2.7/pkgutil.py"", line
465, in get_loader
    return find_loader(fullname)
  File
""/home/yahsuan/.pyenv/versions/pypy-2.2.1/lib-python/2.7/pkgutil.py"", line
475, in find_loader
    for importer in iter_importers(fullname):
  File
""/home/yahsuan/.pyenv/versions/pypy-2.2.1/lib-python/2.7/pkgutil.py"", line
431, in iter_importers
    __import__(pkg)
  File ""pyspark/__init__.py"", line 41, in <module>
    from pyspark.context import SparkContext
  File ""pyspark/context.py"", line 26, in <module>
    from pyspark import accumulators
  File ""pyspark/accumulators.py"", line 98, in <module>
    from pyspark.serializers import read_int, PickleSerializer
  File ""pyspark/serializers.py"", line 400, in <module>
    _hijack_namedtuple()
  File ""pyspark/serializers.py"", line 378, in _hijack_namedtuple
    _old_namedtuple = _copy_func(collections.namedtuple)
  File ""pyspark/serializers.py"", line 376, in _copy_func
    f.__defaults__, f.__closure__)
AttributeError: 'function' object has no attribute '__closure__'

p.s. would you want to test different pypy versions on your Jenkins? maybe
I could help

:

:
:
s
yPy
tual
to
o
.
ace
ble
.
"",
.py"",
py"",
tors.py"",
ers.py"",
ers.py"",
ers.py"",
p


-- 
-- 張雅軒
"
Daniel Margo <dmargo@eecs.harvard.edu>,"Fri, 6 Nov 2015 04:35:01 -0700 (MST)",GraphX EdgePartition format,dev@spark.apache.org,"I was looking through the GraphX source and noticed that the topology of an
EdgePartition is a triplet of source, destination, and data columns --
essentially a COO sparse matrix -- sorted by source, and equipped with an
index from each (global) vertex ID to the start of its (local) source
cluster. This index provides efficient local neighborhood lookup.

Given that the columns are source-sorted, is there a reason that the
duplicate values in the source column are not efficiently packed, as in e.g.
a CSR sparse matrix? That is, replace every source cluster with a single
source value plus a length. Furthermore, these source values would duplicate
the existing global2local index, so they can be removed entirely.

This is a common optimization in sparse matrix systems and I recall (perhaps
incorrectly) that GraphLab used this format -- is there a reason that GraphX
does not?
-dwm




--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 6 Nov 2015 12:44:15 +0000",Ready to talk about Spark 2.0?,dev <dev@spark.apache.org>,"Since branch-1.6 is cut, I was going to make version 1.7.0 in JIRA. However
I've had a few side conversations recently about Spark 2.0, and I know I
and others have a number of ideas about it already.

I'll go ahead and make 1.7.0, but thought I'd ask, how much other interest
is there in starting to plan Spark 2.0? is that even on the table as the
next release after 1.6?

Sean
"
,"Fri, 6 Nov 2015 13:53:20 +0100",Re: Ready to talk about Spark 2.0?,dev@spark.apache.org,"Hi Sean,

Happy to see this discussion.

I'm working on PoC to run Camel on Spark Streaming. The purpose is to 
have an ingestion and integration platform directly running on Spark 
Streaming.

Basically, we would be able to use a Camel Spark DSL like:

from(""jms:queue:foo"").choice().when(predicate).to(""job:bar"").when(predicate).to(""hdfs:path"").otherwise(""file:path"")....

Before a formal proposal (I have to do more work there), I'm just 
wondering if such framework can be a new Spark module (Spark Integration 
for instance, like Spark ML, Spark Stream, etc).

Maybe it could be a good candidate for an addition in a ""major"" release 
like Spark 2.0.

Just my $0.01 ;)

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Fri, 6 Nov 2015 09:25:50 -0500",Re: Master build fails ?,Steve Loughran <stevel@hortonworks.com>,"if there is no strong preference for one dependencies policy over another,
but consistency between the 2 systems is desired, then i believe maven can
be made to behave like ivy pretty easily with a setting in the pom


"
Luc Bourlier <luc.bourlier@typesafe.com>,"Fri, 06 Nov 2015 14:43:02 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),"Nicholas Chammas <nicholas.chammas@gmail.com>, Egor Pahomov <pahomov.egor@gmail.com>, 
	Sean Owen <sowen@cloudera.com>","+1 (non binding)

Tested the integration with Mesos in the different configurations.

Luc

Le jeu. 5 nov. 2015 à 21:02, Nicholas Chammas <nicholas.chammas@gmail.com>
a écrit :

t:
/
3
2
================
================
an
=========================
===="
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 6 Nov 2015 14:51:36 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),"Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Egor Pahomov <pahomov.egor@gmail.com>, 
	Sean Owen <sowen@cloudera.com>"," While running our regression tests I found https://issues.apache.org/jira/browse/SPARK-11555.  It is a break in backwards compatibility but its using the old spark-class and --num-workers interface which I hope no one is still using.  
I'm a +0 as it doesn't seem super critical but I hate to break backwards compatibility unless we explicitly decide to.

Tom 


   

 -0
The spark-ec2 version is still set to 1.5.1.
Nick

+1 
Things, which our infrastructure use and I checked:
Dynamic allocationSpark ODBC serverReading jsonWriting parquetSQL quires (hive context)Running on CDH 

2015-11-04 9:03 GMT-08:00 Sean Owen <sowen@cloudera.com>:

As usual the signatures and licenses and so on look fine. I continue
to get the same test failures on Ubuntu in Java 7/8:

- Unpersisting HttpBroadcast on executors only in distributed mode ***
FAILED ***

But I continue to assume that's specific to tests and/or Ubuntu and/or
the build profile, since I don't see any evidence of this in other
builds on Jenkins. It's not a change from previous behavior, though it
doesn't always happen either.

a
===============
===============
========================
========================
nt

---------------------------------------------------------------------





-- 
Sincerely yours
Egor Pakhomov, AnchorFree




  "
Ted Yu <yuzhihong@gmail.com>,"Fri, 6 Nov 2015 06:58:04 -0800",Re: Master build fails ?,Koert Kuipers <koert@tresata.com>,"Since maven is the preferred build vehicle, ivy style dependencies policy
would produce surprising results compared to today's behavior.

I would suggest staying with current dependencies policy.

My two cents.


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 07:39:23 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 08:04:49 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","and we're back!


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 6 Nov 2015 08:42:21 -0800",Re: Looking for the method executors uses to write to HDFS,tz@looper.hu,"Are you looking for this?

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRelation.scala#L69



I
e
o
"
Chester Chen <chester@alpinenow.com>,"Fri, 6 Nov 2015 09:27:41 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Reynold Xin <rxin@databricks.com>,"+1
Test against CDH5.4.2 with hadoop 2.6.0 version using yesterday's code,
build locally.

Regression running in Yarn Cluster mode against few internal ML ( logistic
regression, linear regression, random forest and statistic summary) as well
Mlib KMeans. "
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 6 Nov 2015 09:35:32 -0800",Re: Master build fails ?,Steve Loughran <stevel@hortonworks.com>,"
But that's not the case here; guava is a direct dependency of spark,
not a transitive one, and the root pom explicitly sets its version to
14. sbt is just choosing to ignore that and pick whatever latest
version exists from transitive analysis.

Maven would behave similarly if Spark did not declare a direct
dependency on guava, but it does.

-- 
Marcelo

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 6 Nov 2015 19:09:30 +0000",Re: Master build fails ?,Marcelo Vanzin <vanzin@cloudera.com>,"
rote:
lways obvious why a guava 14.0 that is two hops of transitiveness should take priority over a 16.0 version three hops away. Especially when that 0.14 version should have come

I agree, that's wrong

I think if you have an indirect dependency, it picks one on the shortest path, so if you aren't explicit, you can still lose control of what's going on...

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 6 Nov 2015 12:06:40 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Chester Chen <chester@alpinenow.com>,1
Sean Owen <sowen@cloudera.com>,"Fri, 6 Nov 2015 20:10:35 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Tom Graves <tgraves_cs@yahoo.com>,"Hm, if I read that right, looks like --num-executors doesn't work at
all on YARN unless dynamic allocation is on? the fix is easy, but
sounds like it could be a Blocker.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 6 Nov 2015 12:14:24 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Sean Owen <sowen@cloudera.com>,"The way I read Tom's report, it just affects a long-deprecated command
line option (--num-workers). I wouldn't block the release for it.




-- 
Marcelo

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 6 Nov 2015 20:16:15 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),"Marcelo Vanzin <vanzin@cloudera.com>, Sean Owen <sowen@cloudera.com>","Its either --num-workers or --num-executors when using the spark-class interface directly.  If you use spark-submit with --num-executors it ends up setting spark.executor.instances which works around the issue.
Tom 


   

 The way I read Tom's report, it just affects a long-deprecated command
line option (--num-workers). I wouldn't block the release for it.

ers



-- 
Marcelo

---------------------------------------------------------------------



  "
Michael Armbrust <michael@databricks.com>,"Fri, 6 Nov 2015 12:28:33 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",shane knapp <sknapp@berkeley.edu>,"I'm noticing several problems with Jenkins since the upgrade.

PR comments say: ""Build started sha1 is merged."" instead of actually
printing the hash

Also:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/45246/console

GitHub pull request #9527 of commit
0e0959efada849a56430d30305eb4e9a88ecafad, no merge conflicts.
Setting status of 0e0959efada849a56430d30305eb4e9a88ecafad to PENDING
with url  and message: 'Build started sha1 is merged.'
FileNotFoundException means that the credentials Jenkins is using is
probably wrong. Or the user account does not have write access to the
repo.
java.io.FileNotFoundException: {""message"":""Not
Found"",""documentation_url"":""https://developer.github.com/v3""}
	at org.kohsuke.github.Requester.handleApiError(Requester.java:501)
	at org.kohsuke.github.Requester._to(Requester.java:248)
	at org.kohsuke.github.Requester.to(Requester.java:194)
	at org.kohsuke.github.GHRepository.createCommitStatus(GHRepository.java:829)
	at org.jenkinsci.plugins.ghprb.extensions.status.GhprbSimpleStatus.createCommitStatus(GhprbSimpleStatus.java:208)
	at org.jenkinsci.plugins.ghprb.extensions.status.GhprbSimpleStatus.onBuildStart(GhprbSimpleStatus.java:147)
	at org.jenkinsci.plugins.ghprb.GhprbBuilds.onStarted(GhprbBuilds.java:122)
	at org.jenkinsci.plugins.ghprb.GhprbBuildListener.onStarted(GhprbBuildListener.java:24)
	at org.jenkinsci.plugins.ghprb.GhprbBuildListener.onStarted(GhprbBuildListener.java:17)
	at hudson.model.listeners.RunListener.fireStarted(RunListener.java:215)
	at hudson.model.Run.execute(Run.java:1737)
	at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)
	at hudson.model.ResourceController.execute(ResourceController.java:98)
	at hudson.model.Executor.run(Executor.java:410)
Caused by: java.io.FileNotFoundException:
https://api.github.com/repos/apache/spark/statuses/0e0959efada849a56430d30305eb4e9a88ecafad
	at sun.reflect.GeneratedConstructorAccessor174.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1675)
	at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1673)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.net.www.protocol.http.HttpURLConnection.getChainedException(HttpURLConnection.java:1671)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1244)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:254)
	at org.kohsuke.github.Requester.parse(Requester.java:458)
	at org.kohsuke.github.Requester._to(Requester.java:227)
	... 12 more
Caused by: java.io.FileNotFoundException:
https://api.github.com/repos/apache/spark/statuses/0e0959efada849a56430d30305eb4e9a88ecafad
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338)
	at org.kohsuke.github.Requester.parse(Requester.java:454)
	... 13 more





"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 12:30:30 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",Michael Armbrust <michael@databricks.com>,"looking in to this now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 12:35:30 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",Michael Armbrust <michael@databricks.com>,"a pox on the github pull request builder...  the update wiped out the
github auth creds.  :\


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 12:54:47 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",Michael Armbrust <michael@databricks.com>,"alright, i'm downgrading our ghprb plugin back to the last known
working version.  this will require a jenkins restart, which i will do
immediately.

sorry about this!  :(


---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Fri, 6 Nov 2015 13:04:24 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",shane knapp <sknapp@berkeley.edu>,"Are you sure that the credentials are missing? Also: did you enable GitHub
commit status updating by accident / configuration loss? That might explain
the errors here, since our keys don't have permissions to use that API.


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 13:11:32 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",Josh Rosen <joshrosen@databricks.com>,"i (stupidly) updated the ghprb plugin as the version we're using is
really, really old.  this re-wrote the config and broke stuff.

so, i just downgraded the plugin back to the last known working
version, and noticed that some of the fields in the xml are missing.

thankfully i have a backup config from ~5 days ago....  i'm working on
munging them together right now.

and thankfully our new hardware has just showed up so i'll have a
jenkins staging instance up really, really soon and be able to test
stuff like this in the future.

shane


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 13:13:34 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",Josh Rosen <joshrosen@databricks.com>,"gonna have to kick jenkins again, folks.  sorry!


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Nov 2015 13:30:57 -0800","Re: [BUILD SYSTEM] quick jenkins downtime, november 5th 7am",Josh Rosen <joshrosen@databricks.com>,"ok, i think i've kicked jenkins enough that it's now working again w/o
spamming tracebacks.

sorry for the interruption...  i should have realized that touching
the house of cards (aka ghprb plugin) would cause it to fall down no
matter what i did.  :)

shane

ps - did i mention the hardware for our staging instance just showed up?  :)


---------------------------------------------------------------------


"
Jakob Odersky <jodersky@gmail.com>,"Fri, 6 Nov 2015 14:28:49 -0800",Re: State of the Build,Patrick Wendell <pwendell@gmail.com>,"Reposting to the list...

Thanks for all the feedback everyone, I get a clearer picture of the
reasoning and implications now.

Koert, according to your post in this thread
http://apache-spark-developers-list.1001551.n3.nabble.com/Master-build-fails-tt14895.html#a15023,
it is apparently very easy to change the maven resolution mechanism to the
ivy one.
Patrick, would this not help with the problems you described?


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Nov 2015 14:35:02 -0800",Re: State of the Build,Jakob Odersky <jodersky@gmail.com>,"I think we'd have to standardize on Maven-style resolution, or I'd at least
like to see that path explored first. The issue is if we switch the
standard now, it could cause compatibility breaks for applications on top
of Spark.


"
Koert Kuipers <koert@tresata.com>,"Fri, 6 Nov 2015 18:04:31 -0500",Re: State of the Build,Patrick Wendell <pwendell@gmail.com>,"thats interesting...

if i understand it correctly it would cause compatibility breaks for
applications on top of spark, because those applications use the exact same
current resolution logic (so basically they are maven apps), and the change
would make them inconsistent?

by that logic all existing applications on top of spark that do not use
maven are already in danger of incompatibly breaks?
or am i completely misunderstanding?

this makes the implications of spark switching to maven a while back
somewhat more serious than i realized.

on the other hand we use sbt for our spark apps and this has never been an
issue for us, so i am not sure how real/serious this compatibility issue is.





, assuming those applications

because those applications are maven applications that currently use the
same exact logic,


because you publish a pom

because those applications build on top of spark currently assume


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 6 Nov 2015 15:07:36 -0800",Re: State of the Build,Koert Kuipers <koert@tresata.com>,"
I think Patrick said it could cause compatibility breaks because
switching to sbt's version resolution means Spark's dependency tree
would change. Just to cite the recent example, you'd get Guava 16
instead of 14 (let's ignore that Guava is currently mostly shaded in
Spark), so if your app depended transitively on Guava and used APIs
from 14 that are not on 16, it would break.

-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Nov 2015 15:29:50 -0800",Re: State of the Build,Marcelo Vanzin <vanzin@cloudera.com>,"I think there are a few minor differences in the dependency graph that
arise from this. For a given user, the probability it affects them is low -
it needs to conflict with a library a user application is using. However
the probability it affects *some users* is very high and we do see small
changes crop up fairly frequently.

My feeling is mostly pragmatic... if we can get things working to
standardize on Maven-style resolution by upgrading SBT, let's do it. If
that's not tenable, we can evaluate alternatives.

- Patrick


"
Koert Kuipers <koert@tresata.com>,"Fri, 6 Nov 2015 18:42:50 -0500",Re: State of the Build,Marcelo Vanzin <vanzin@cloudera.com>,"oh ok i think i got it... i hope! since the app runs with the spark
assembly jar on its classpath, the exact version as resolved by spark's
build process is actually on the apps classpath.

sorry didnt mean the pollute this thread with my own dependency resolution
confusion.



"
Jakob Odersky <jodersky@gmail.com>,"Fri, 6 Nov 2015 16:25:49 -0800",Re: State of the Build,Patrick Wendell <pwendell@gmail.com>,"[Reposting to the list again, I really should double-check that
reply-to-all button]

in the mean-time, as a light Friday-afternoon patch I was thinking about
splitting the ~600loc-single-build sbt file into something more manageable
like the Akka build (without changing any dependencies or settings). I know
its pretty trivial and not very important, but it might make things easier
to add/refactor in the future.

Also, why do we include an sbt jar in the source repo, especially if it is
used only as an internal development tool?


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 6 Nov 2015 16:43:02 -0800",Re: State of the Build,Jakob Odersky <jodersky@gmail.com>,"bq. include an sbt jar in the source repo

Can you clarify which sbt jar (by path) ?

I tried 'git log' on the following files but didn't see commit history:

./build/sbt-launch-0.13.7.jar
./build/zinc-0.3.5.3/lib/sbt-interface.jar
./sbt/sbt-launch-0.13.2.jar
./sbt/sbt-launch-0.13.5.jar


"
Jakob Odersky <jodersky@gmail.com>,"Fri, 6 Nov 2015 16:49:41 -0800",Re: State of the Build,Ted Yu <yuzhihong@gmail.com>,"Any of them.
Sbt is a build tool, and I don't understand why it is included in a source
repository. It would be like including make in a project.


"
Michael Armbrust <michael@databricks.com>,"Fri, 6 Nov 2015 16:53:12 -0800",Re: State of the Build,Jakob Odersky <jodersky@gmail.com>,"Its not included, it is downloaded on demand.

That said I think the fact that we can download the jar is a huge feature
of SBT, no installation needed, build the project as long as you have a JVM.


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 6 Nov 2015 23:39:51 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Reynold Xin <rxin@databricks.com>,"+1 (non-binding, of course) (Hope I made it in time. ~T-20 !)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 25:52 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib (iPython 4.0, FYI, notebook install is separate
“con"
Jacek Laskowski <jacek@japila.pl>,"Sat, 7 Nov 2015 13:41:04 +0100","Build fails due to...multiple overloaded alternatives of constructor
 RDDInfo define default arguments?",dev@spark.apache.org,"Hi,

Checked out the latest sources and the build failed:

[error] /Users/jacek/dev/oss/spark/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala:25:
in class RDDInfo, multiple overloaded alternatives of constructor
RDDInfo define default arguments.
[error] class RDDInfo(
[error]       ^

The build commands:

➜  spark git:(master) ✗ git rev-parse --short HEAD
2ff0e79

➜  spark git:(master) ./dev/change-scala-version.sh 2.11

➜  spark git:(master) ✗ ./build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver
-DskipTests clean install

Pozdrawiam,
Jacek

--
Jacek Laskowski | http://blog.japila.pl | http://blog.jaceklaskowski.pl
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 7 Nov 2015 04:56:52 -0800","Re: Build fails due to...multiple overloaded alternatives of
 constructor RDDInfo define default arguments?",Jacek Laskowski <jacek@japila.pl>,"Created a PR for the compilation error:
https://github.com/apache/spark/pull/9538

Cheers


DDInfo.scala:25:
"
Jacek Laskowski <jacek@japila.pl>,"Sat, 7 Nov 2015 15:08:09 +0100","Re: Build fails due to...multiple overloaded alternatives of
 constructor RDDInfo define default arguments?",Ted Yu <yuzhihong@gmail.com>,"Worked for me. Thanks!

Pozdrawiam,
Jacek

--
Jacek Laskowski | http://blog.japila.pl | http://blog.jaceklaskowski.pl
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski


RDDInfo.scala:25:

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 7 Nov 2015 10:25:31 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Robin East <robin.east@xense.co.uk>,"+1 myself too


"
Joseph Bradley <joseph@databricks.com>,"Sat, 7 Nov 2015 11:55:31 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 tested on OS X


"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 7 Nov 2015 12:01:19 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Reynold Xin <rxin@databricks.com>,1
vonnagy <ivan@vadio.com>,"Sat, 7 Nov 2015 13:17:58 -0700 (MST)",Calling stop on StreamingContext locks up,dev@spark.apache.org,"If I have a streaming job (Spark 1.5.1) and attempt to stop the stream after
the first batch, the system locks up and never completes. The pseudo code
below shows that after the batch complete notification is called the stream
is stopped. I have traced the lockup to the call `listener.stop()`in
JobScheduler (line 114) which attempts to join the thread in
AsynchronousListenerBus. That thread never ends because it is still getting
messages `SparkListenerExecutorMetricsUpdate` from the DAGScheduler. The
thread never ends because the events continue to come in.

Any thoughts/ideas on how I can effectively stop the stream after the first
batch would greatly appreciated.

Psuedo Example:

class SomeJob {

    val ssc = createStreamingContext()
    val listener = new MyListener(ssc)
    ssc.addStreamingListener(listener)

    val stream = getStream

    stream.foreachRDD { rdd =>
        // Do something with the data
    }
}

class MyListener(ctx: StreamingContext) extends StreamingListener {
    override def onBatchCompleted(batchCompleted:
StreamingListenerBatchCompleted) = synchronized {
        ctx.stop(false, false)
        // NOTE: I get the same results with ctx.stop(), ctx.stop(true),
ctx.stop(true, true), or ctx.stop(false, false)
    }
}



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 7 Nov 2015 13:35:06 -0800",Re: Calling stop on StreamingContext locks up,vonnagy <ivan@vadio.com>,"Would the following change work for you ?

diff --git
a/core/src/main/scala/org/apache/spark/util/AsynchronousListenerBus.scala
b/core/src/main/scala/org/apache/spark/util/AsynchronousListenerBus.scala
index 61b5a4c..c330d25 100644
---
a/core/src/main/scala/org/apache/spark/util/AsynchronousListenerBus.scala
+++
b/core/src/main/scala/org/apache/spark/util/AsynchronousListenerBus.scala
@@ -66,6 +66,7 @@ private[spark] abstract class AsynchronousListenerBus[L
<: AnyRef, E](name: Stri
         self.synchronized {
           processingEvent = true
         }
+        if (stopped.get()) return
         try {
           val event = eventQueue.poll
           if (event == null) {


"
Denny Lee <denny.g.lee@gmail.com>,"Sun, 08 Nov 2015 00:35:52 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),"Mark Hamstra <mark@clearstorydata.com>, Reynold Xin <rxin@databricks.com>",1
Vincenzo Selvaggio <vselvaggio@gmail.com>,"Sun, 8 Nov 2015 09:08:49 +0000",Re: PMML version in MLLib,Fazlan Nazeem <fazlann@wso2.com>,"Hi,

I confirm the models are exported for PMML version 4.2, in fact you can see
in the generated xml
PMML xmlns=""http://www.dmg.org/PMML-4_2""
This is the default version when using
https://github.com/jpmml/jpmml-model/tree/1.1.X.

I didn't realize the attribute version of the PMML root element was
required, this can be easily added.
Please, as Owen suggested, add a PR and link it to
https://issues.apache.org/jira/browse/SPARK-8545.

Thanks,
Vincenzo


"
Romi Kuntsman <romi@totango.com>,"Sun, 8 Nov 2015 12:53:01 +0200",Re: Ready to talk about Spark 2.0?,,"A major release usually means giving up on some API backward compatibility?
Can this be used as a chance to merge efforts with Apache Flink (
https://flink.apache.org/) and create the one ultimate open source big data
processing system?
Spark currently feels like it was made for interactive use (like Python and
R), and when used others (batch/streaming), it feels like scripted
interactive instead of really a standalone complete app. Maybe some base
concepts may be adapted?

(I'm not currently a committer, but as a heavy Spark user I'd love to
participate in the discussion of what can/should be in Spark 2.0)

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


e
.
te).to(""hdfs:path"").otherwise(""file:path"")....
"
Sean Owen <sowen@cloudera.com>,"Sun, 8 Nov 2015 11:33:09 +0000",Re: Ready to talk about Spark 2.0?,Romi Kuntsman <romi@totango.com>,"Major releases can change APIs, yes. Although Flink is pretty similar
in broad design and goals, the APIs are quite different in
particulars. Speaking for myself, I can't imagine merging them, as it
would either mean significantly changing Spark APIs, or making Flink
use Spark APIs. It would mean effectively removing one project which
seems infeasible.

I am not sure of what you're saying the difference is, but I would not
describe Spark as primarily for interactive use.

niche. It's actually valuable to have many takes on important
problems. Hence any problem worth solving gets solved 10 times. Just
look at all those SQL engines and logging frameworks...

y?
ata
nd
t>
ve
g.
ate).to(""hdfs:path"").otherwise(""file:path"")....
 for

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Sun, 8 Nov 2015 10:59:51 -0500",Re: Ready to talk about Spark 2.0?,Sean Owen <sowen@cloudera.com>,"romi,
unless am i misunderstanding your suggestion you might be interested in
projects like the new mahout where they try to abstract out the engine with
bindings, so that they can support multiple engines within a single
platform. I guess cascading is heading in a similar direction (although no
spark or flink yet there, just mr1 and tez).


e
net>
te).to(""hdfs:path"").otherwise(""file:path"")....
e
nd
"
Romi Kuntsman <romi@totango.com>,"Sun, 8 Nov 2015 18:14:20 +0200",Re: Ready to talk about Spark 2.0?,Koert Kuipers <koert@tresata.com>,"Hi, thanks for the feedback
I'll try to explain better what I meant.

First we had RDDs, then we had DataFrames, so could the next step be
something like stored procedures over DataFrames?
So I define the whole calculation flow, even if it includes any ""actions""
in between, and the whole thing is planned and executed in a super
optimized way once I tell it ""go!""

What I mean by ""feels like scripted"" is that actions come back to the
driver, like they would if you were in front of a command prompt.
But often the flow contains many steps with actions in between - multiple
levels of aggregations, iterative machine learning algorithms etc.
Sending the whole ""workplan"" to the Spark framework would be, as I see it,
the next step of it's evolution, like stored procedures send a logic with
many SQL queries to the database.

Was it more clear this time? :)


*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


th
o
n
se
.net>
ate).to(""hdfs:path"").otherwise(""file:path"")....
se
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 8 Nov 2015 09:13:58 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Denny Lee <denny.g.lee@gmail.com>,1
Sean McNamara <Sean.McNamara@Webtrends.com>,"Sun, 8 Nov 2015 17:33:51 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Reynold Xin <rxin@databricks.com>,"+1

Sean



Please vote on releasing the following candidate as Apache Spark version 1.5.2. The vote is open until Sat Nov 7, 2015 at 00:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5."
vonnagy <ivan@vadio.com>,"Sun, 8 Nov 2015 11:03:13 -0700 (MST)",Re: Calling stop on StreamingContext locks up,dev@spark.apache.org,"Hi Ted,

Your fix addresses the issue for me. Thanks again for your help and I saw
the PR you submitted to Master.

Ivan



--

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Nov 2015 12:10:37 -0800",Re: Ready to talk about Spark 2.0?,Romi Kuntsman <romi@totango.com>,"Yes, that's clearer -- at least to me.

But before going any further, let me note that we are already sliding past
Sean's opening question of ""Should we start talking about Spark 2.0?"" to
actually start talking about Spark 2.0.  I'll try to keep the rest of this
post at a higher- or meta-level in order to attempt to avoid a somewhat
premature discussion of detailed 2.0 proposals, since I think that we do
still need to answer Sean's question and a couple of related questions
before really diving into the details of 2.0 planning.  The related
questions that I am talking about are: Is Spark 1.x done except for
bug-fixing? and What would definitely make us say that we must begin
working on Spark 2.0?

I'm not going to try to answer my own two questions even though I'm really
interested in how others will answer them, but I will answer Sean's by
saying that it is a good time to start talking about Spark 2.0 -- which is
quite different from saying that we are close to an understanding of what
will differentiate Spark 2.0 or when we want to deliver it.

will be different in 2.0"" into some distinct categories.  I see at least
three such categories for openers, although the third will probably need to
be broken down further.

The first is the simplest, would take almost no time to complete, and would
have minimal impact on current Spark users.  This is simply getting rid of
everything that is already marked deprecated in Spark 1.x but that we
haven't already gotten rid of because of our commitment to maintaining API
stability within major versions.  There should be no need for discussion or
apology before getting rid of what is already deprecated -- it's just gone
and it's time to move on.  Kind of a category-1.1 are parts of the the
current public API that are now marked as Experimental or Developer that
should become part of the fully-supported public API in 2.0 -- and there is
room for debate here.

The next category of things that will be different in 2.0 isn't a lot
harder to implement, shouldn't take a lot of time to complete, but will
have some impact on current Spark users.  I'm talking about areas in the
current code that we know don't work the way we want them to and don't have
the public API that we would like, but for which there aren't or can't be
recommended alternatives yet, so the code isn't formally marked as
deprecated.  Again, these are things that we haven't already changed mostly
because of the need to maintain API stability in 1.x.  But because these
haven't already been marked as deprecated, there is potential to catch
existing Spark users by surprise when the API changes.  We don't guarantee
API stability across major version number changes, so there isn't any
reason why we can't make the changes we want, but we should start building
up a comprehensive list of API changes that will occur in Spark 2.0 to at
least minimize the amount of surprise for current Spark users.

I don't already have anything like such a comprehensive list, but one
example of the kind of thing that I am talking about is something that I've
personally been looking at and regretting of late, and that's the
complicated relationships among SparkListener, SQLListener, onJobEnd and
onExecutionEnd.  A lot of this complication is because of the need to
maintain the public API, so we end up with comments like this (
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala#L58):
""Ideally, we need to make sure onExecutionEnd happens after onJobStart and
onJobEnd.  However, onJobStart and onJobEnd run in the listener thread.
Because we cannot add new SQL event types to SparkListener since it's a
public API, we cannot guarantee that.""  I think it should be pretty obvious
that we should be making these kinds of architectural and API changes in
2.0 -- they are currently causing Spark developers and often Spark users to
deal with complications that are needed mostly or entirely just to maintain
the public API.  I know that there are other (and larger) examples of this
kind of refactoring that others are itching to start doing, but I'll let
them speak to those specifics as we build up the list of API changes.

The third category of things that will be different in 2.0 is the category
that could get us seriously off track or badly behind schedule.  This is
the category that includes fundamental architectural changes or significant
new features and functionality.  Before we go too far afield in exploring
our wish lists for Spark 2.0, I think we need to try very hard to identify
which architectural changes are really needed to achieve a minimum viable
platform that can meet our goals for the complete Spark 2.0 cycle.  This
starts to get back to my questions of whether Spark 1.x is done and whether
we really need to start working on Spark 2.0.  If we look back at the total
Spark ecosystem in the 0.9 timeframe vs. where it is now on the verge of
1.6.0, it should be clear that an amazing number of additions and
refinements have been made to Spark itself, Spark packages, third-party
tools and applications, etc. -- and all of that was done without requiring
fundamental changes to Spark's architecture.  What I think that implies is
that as items are added to our collective wish list for Spark 2.0, we need
to be asking of each one at least two things: 1) Whether it really requires
a fundamental change in Spark before this new feature or functionality can
be implemented; and 2) If it does require a fundamental change, is that
change (but not necessarily all the new features that need that change)
something that we are willing to commit to completing before Spark 2.0.0
can be released?  Or alternatively, is that a fundamental change that we
can and should put off making for potentially years while the Spark 2.x
development cycle runs its course?  If wish list items don't require
fundamental changes, then we shouldn't feel bad about needing to say for
many of them that they look like good and/or interesting ideas, and things
that we may very well want to include in Spark, but that they may end up in
Spark 2.x instead of 2.0.

To finally get back to your posts, Romi, what I think you are talking about
is the ability to compose things like Spark Jobs, RDD actions and SQL
Executions without needing to explicitly coordinate the collection of
intermediate results to the Driver and the redistribution of data to the
Executors.  This is the kind of thing that is already done in some respects
in transformations like RDD#sortByKey, but that actually breaks Spark's
claim that transformations are lazy.  Wanting to be able to compose things
in Spark in a manner more in line with what functional programmers expect
and doing so without breaking other expectations of Spark users is
something that has been on several others' wish lists for awhile now.  A
few attempts have been made to address the issue within the Spark 1.x
architecture, and some of the recent additions that Matei has made in
regard to realizing adaptive DAG scheduling may allow us to push things
further within Spark 1.x, but this may also be the kind of thing that will
prompt us to make deeper changes in Spark 2.0.

Where I thought you going at first is another category three item: Whether
Spark should be fundamentally changed to allow streams to be handled at the
event level instead of (or in addition to) micro-batches.

So, from my perspective that is a meta-framework that I think is useful to
shape the Spark 2.0 discussion, a couple of category three wish list items,
and bunch of questions that I'm not even going to try to answer on my own
-- but looking forward to the Spark 2.0 discussion.


,
ith
no
:
x.net>
o
cate).to(""hdfs:path"").otherwise(""file:path"")....
e
-
"
Romi Kuntsman <romi@totango.com>,"Sun, 8 Nov 2015 23:41:38 +0200",Re: Ready to talk about Spark 2.0?,Mark Hamstra <mark@clearstorydata.com>,"Since it seems we do have so much to talk about Spark 2.0, then the answer
to the question ""ready to talk about spark 2"" is yes.
But that doesn't mean the development of the 1.x branch is ready to stop or
that there shouldn't be a 1.7 release.

Regarding what should go into the next major version - obviously on the
technical level it's breaking API changes and perhaps some long-awaited
architectural refactoring.

But what I think should be the major change is on the conceptual side - the
realization that the way interactive, batch and streaming data flows work
are fundamentally different, and building the framework around that will
benefit each of those flows (like events instead of microbatches in
streaming, worker-side intermediate processing in batch, etc).

So where is the best way to have a full Spark 2.0 discussion?

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


t
s
y
s
s
0
ve
ly
e
g
ve
pache/spark/sql/execution/SQLExecution.scala#L58):
d
us
to
in
s
y
nt
y
er
al
g
s
d
es
n
s
in
QL
ts
s
l
r
he
o
s,
""
e
with
 no
o
ax.net
icate).to(""hdfs:path"").otherwise(""file:path"")....
.
,
he
--
"
shane knapp <sknapp@berkeley.edu>,"Sun, 8 Nov 2015 14:53:07 -0800","[build system] emergency restart to temporarily patch a massive java
 security hole","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","hey everyone!

i'm about to shut down jenkins to deploy a temporary fix for a massive
security hole i found out about late friday:

http://foxglovesecurity.com/2015/11/06/what-do-weblogic-websphere-jboss-jenkins-opennms-and-your-application-have-in-common-this-vulnerability/

read the whole thing.  it's kinda nuts.

anyways, jenkins/cloudbees is on top of it and they've released a
quick patch, which i will be deploying right now:

https://jenkins-ci.org/content/mitigating-unauthenticated-remote-code-execution-0-day-jenkins-cli

downtime should be minimal, and i'll let every know when we're back up.

i'm shutting jenkins down immediately.

shane

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Sun, 8 Nov 2015 14:58:37 -0800","Re: [build system] emergency restart to temporarily patch a massive
 java security hole","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","ok, we're good to go.

https://amplab.cs.berkeley.edu/jenkins/cli/  returns a 404, as it should.

thanks for your patience...

shane


---------------------------------------------------------------------


"
"""=?gb18030?B?xbfI8Q==?="" <494165115@qq.com>","Mon, 9 Nov 2015 11:28:57 +0800","=?gb18030?B?u9i4tKO6W1ZPVEVdIFJlbGVhc2UgQXBhY2hlIFNw?=
 =?gb18030?B?YXJrIDEuNS4yIChSQzIp?=","""=?gb18030?B?RGVubnkgTGVl?="" <denny.g.lee@gmail.com>, ""=?gb18030?B?TWFyayBIYW1zdHJh?="" <mark@clearstorydata.com>, ""=?gb18030?B?UmV5bm9sZCBYaW4=?="" <rxin@databricks.com>","build spark-streaming-mqtt_2.10 failed!


nohup mvn -X -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package -rf :spark-streaming-mqtt_2.10 & 

[DEBUG] org.scala-tools.te.activemq:activemq-core:jar:5.7.0:test 
[DEBUG] org.apache.geronimo.specs:geronimo-jms_1.1_spec:jar:1.1.1:st 
[DEBUG] org.apache.activemq.protobuf:activemq-protobuf:jar:1.1:test 
[DEBUG] org.fusesource.mqtt-clienUG] org.fusesource.hawtdispatch:hawtdispatch:jar:1.11:test 
[DEBUG] org.fusesource.hawtbuf:hawtbuf:jar:1.9:test 
[DEBUG] org.apache.geronimo.specs:geronimo-j2eengframework:spring-context:jar:3.0.7.RELEASE:test 
[DEBUG] org.springframework:spring-aop:jar:3.0.7.RELEASE
[DEBUG] org.springframework:spring-beans:jar:3.0.7.RELEASE:test 
[DEBUG] org.springframework:spring-core:jar:3.0.7.RELEASE:test 
[DEBUG] commons-logging:commons-logging:jar:1.1.1:test 
[DEBUG] org.springframeworkg.springframework:spring-asm:jar:3.0.7.RELEASE:test 
[DEBUG] org.jasypt:jasypt:jar:1.9.0:test 
[DEBUG] org.spark-project.spark:unused:jar:1.0.0:compile 
[DEBUGNFO] ------------------------------------------------------------------------ 
[INFO] Reactor Summary: 
[INFO] 
[INFO] Spark Project External MQTT ........................ FAILURE [ 2.403 s] 
[INFO] Spark Project External MQTT Assembly ............... SKIPPED 
[INFO] Spark Project External ZeroMQ ...................... SKIPPED 
[INFO] Spark Project External Kafka ....................... SKIPPED 
[INFO] Spark Project Examples ............................. SKIPPED 
[INFO] Spark Project External Kafka Assembly .............. SKIPPED 
[INFO] ------------------------------------------------------------------------ 
[INFO] BUILD FAILURE 
[INFO] ------------------------------------------------------------------------ 
[INFO] Total time: 4.471 s 
[INFO] Finished at: 2015-11-09T11:10:57+08:00 
[INFO] Final Memory: 31M/173M 
[INFO] ------------------------------------------------------------------------ 
[WARNING] The requested profile ""hive"" could not be activated because it does not exist. 
[ERROR] Failed to execute goal on project spark-streaming-mqtt_2.10: Could not resolve dependencies for project org.apa: The following artifacts could not be resolved: org.apache.spark:spark-streaming_2.10:jar:1.5.0-SNAPSHOT, og.apache.spark:spark-core_2.10:jar:tests:1.5.0-SNAPSHOT, org.apache.spark:spark-launcher_2.10:jar:1.5.0-SNAPSHOT, org.apache.spark:spark-network-common_2.10:jar:1.5.0-SNAPSHOT, org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1: Failure to find org.apache.spark:spark-streaming_2.10:jar:1.5.0-20150818.023902-334 in http://maven.cnsuning.com/content/groups/public/ was cached in the local repository, resolution will not be reattempted until the update interval of suning_maven_repo has elapsed or updates are forced -> [Help 1] 
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal on project spark-streaming-mqtt_2.10: Could not resolve dependencies for project org.HOT: The following artifacts could not be resolved: org.apache.spark:spark-streaming_2.10:jar:1.5.0-SNAPSHOT org.apache.spark:spark-core_2.10:jar:tests:1.5.0-SNAPSHOT, org.apache.spark:spark-launcher_2.10:jar:1.5.0-SNAPSHOT, org.apache.spark:spark-network-common_2.10:jar:1.5.0-SNAPSHOT, org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1: Failure to find org.apache.spark:spark-streaming_2.10:jar:1.5.0-20150818.023902-334 in http://maven.cnsuning.com/content/groups/public/ was cached in the local repository, resolution will not be reattempted until the update interval of suning_maven_repo has elapsed or updates are forced
ҵiPhone

------------------ ԭʼʼ ------------------
: Denny Lee <denny.g.lee@gmail.com>
ʱ: 2015118 08:36
ռ: Mark Hamstra <mark@clearstorydata.com>, Reynold Xin <rxin@databricks.com>
: dev@spark.apache.org <dev@spark.apache.org>
: Re: [VOTE] Release Apache Spark 1.5.2 (RC2)



+1




On Sat, Nov 7, 2015 at 12:01 PM Mark Hamstra <mark@clearstorydata.com> wrote:

+1



On Tue, Nov 3, 2015 at 3:22 PM, Reynold Xin <rxin@databricks.com> wrote:


Please vote on releasing the following candidate as Apache Spark version 1.5.2. The vote is open until Sat Nov 7, 2015 at 00:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast.


[ ] +1 Release this package as Apache Spark 1.5.2
[ ] -1 Do not release this package because ...




The release fixes 59 known issues in Spark 1.5.1, listed here:
http://s.apache.org/spark-1.5.2


The tag to be voted on is v1.5.2-rc2:
https://github.com/apache/spark/releases/tag/v1.5.2-rc2


The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc2-bin/


Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc


The staging repository for this release can be found at:
- as version 1.5.2-rc2: https://repository.apache.org/content/repositories/orgapachespark-1153
- as version 1.5.2: https://repository.apache.org/content/repositories/orgapachespark-1152


The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc2-docs/




=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.


================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.1. Bugs already present in 1.5.1 will not block this release."
Ted Yu <yuzhihong@gmail.com>,"Sun, 8 Nov 2015 19:54:13 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),=?UTF-8?B?5qyn6ZSQ?= <494165115@qq.com>,"Why did you directly jump to spark-streaming-mqtt module ?

Can you drop 'spark-streaming-mqtt' and try again ?

Not sure why 1.5.0-SNAPSHOT showed up.
Were you using RC2 source ?

Cheers

:

t
d
d
of
:
d
of
-
 08:36
ynold Xin <
n
f a
================
================
=========================
=========================
"
Krishna Sankar <ksankar42@gmail.com>,"Sun, 8 Nov 2015 20:43:39 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),Ted Yu <yuzhihong@gmail.com>,"In addition to the wrong entry point, I suspect there is a cache problem as
well. I have seen strange errors that disappear completely once the ivy
cache is deleted.
Cheers
<k/>


te:
st
nd
 of
e
s
T:
nd
 of
--
 08:36
eynold Xin <
:
:
/
================
================
n
=========================
=========================
"
Reynold Xin <rxin@databricks.com>,"Sun, 8 Nov 2015 21:10:28 -0800",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks everybody for voting. I'm going to close the vote now. The vote
passes with 14 +1 votes and no -1 vote. I will work on packaging this asap.

+1:
Egor Pahomov
Luc Bourlier
Tom Graves*
Chester Chen
Michael Armbrust*
Krishna Sankar
Robin East
Reynold Xin*
Joseph Bradley
Mark Hamstra
Denny Lee
Ted Yu
Sean McNamara

-0:
Nicholas Chammas (spark-ec2 issue)

-1:
none




a
===============
===============
========================
========================
"
"""fightfate@163.com"" <fightfate@163.com>","Mon, 9 Nov 2015 14:02:47 +0800",OLAP query using spark dataframe with cassandra,"user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","Hi, community

We are specially interested about this featural integration according to some slides from [1]. The SMACK(Spark+Mesos+Akka+Cassandra+Kafka)

seems good implementation for lambda architecure in the open-source world, especially non-hadoop based cluster environment. As we can see, 

the advantages obviously consist of :

1 the feasibility and scalability of spark datafram api, which can also make a perfect complement for Apache Cassandra native cql feature.

2 both streaming and batch process availability using the ALL-STACK thing, cool.

3 we can both achieve compacity and usability for spark with cassandra, including seemlessly integrating with job scheduling and resource management.

Only one concern goes to the OLAP query performance issue, which mainly caused by frequent aggregation work between daily increased large tables, for 

both spark sql and cassandra. I can see that the [1] use case facilitates FiloDB to achieve columnar storage and query performance, but we had nothing more 

knowledge. 

Question is : Any guy had such use case for now, especially using in your production environment ? Would be interested in your architeture for designing this 

OLAP engine using spark +  cassandra. What do you think the comparison between the scenario with traditional OLAP cube design? Like Apache Kylin or 

pentaho mondrian ? 

Best Regards,

Sun.


[1]  http://www.slideshare.net/planetcassandra/cassandra-summit-2014-interactive-olap-queries-using-apache-cassandra-and-spark



fightfate@163.com
"
Fazlan Nazeem <fazlann@wso2.com>,"Mon, 9 Nov 2015 11:57:08 +0530",Re: PMML version in MLLib,Vincenzo Selvaggio <vselvaggio@gmail.com>,"Hi Vincenzo/Owen,

I have sent a pull request[1] with necessary changes to add the pmml
version attribute to the root node. I have also linked the issue under the
PMML improvement umbrella[2] as you suggested.

[1] https://github.com/apache/spark/pull/9558
[2]  https://issues.apache.org/jira/browse/SPARK-8545.




-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
Sean Owen <sowen@cloudera.com>,"Mon, 9 Nov 2015 06:39:44 +0000",Re: [VOTE] Release Apache Spark 1.5.2 (RC2),=?UTF-8?B?5qyn6ZSQ?= <494165115@qq.com>,"Looks like you are building a module without install-ing other
modules. That won't work in general in Maven. Also, it looks like you
are building a snapshot, not the release we are talking about.

:
.10
t
03
d
ing
d
of
:
d
of

---------------------------------------------------------------------


"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Mon, 9 Nov 2015 07:40:04 +0100",Re: OLAP query using spark dataframe with cassandra,"""fightfate@163.com"" <fightfate@163.com>","
Is there any distributor supporting these software components in combination? If no and your core business is not software then you may want to look for something else, because it might not make sense to build up internal know-how in all of these areas.

In any case - it depends all highly on your data and queries. You will have to do your own experiments.

ome slides from [1]. The SMACK(Spark+Mesos+Akka+Cassandra+Kafka)
 especially non-hadoop based cluster environment. As we can see, 
ke a perfect complement for Apache Cassandra native cql feature.
 cool.
cluding seemlessly integrating with job scheduling and resource management.
used by frequent aggregation work between daily increased large tables, for 
FiloDB to achieve columnar storage and query performance, but we had nothing more 
roduction environment ? Would be interested in your architeture for designing this 
ween the scenario with traditional OLAP cube design? Like Apache Kylin or 
active-olap-queries-using-apache-cassandra-and-spark
"
"""fightfate@163.com"" <fightfate@163.com>","Mon, 9 Nov 2015 14:46:05 +0800",Re: Re: OLAP query using spark dataframe with cassandra,=?utf-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"Hi, 

Thanks for suggesting. Actually we are now evaluating and stressing the spark sql on cassandra, while

trying to define business models. FWIW, the solution mentioned here is different from traditional OLAP

cube engine, right ? So we are hesitating on the common sense or direction choice of olap architecture. 

And we are happy to hear more use case from this community. 

Best,
Sun. 



fightfate@163.com
 
From: Jörn Franke
Date: 2015-11-09 14:40
To: fightfate@163.com
CC: user; dev
Subject: Re: OLAP query using spark dataframe with cassandra

Is there any distributor supporting these software components in combination? If no and your core business is not software then you may want to look for something else, because it might not make sense to build up internal know-how in all of these areas.

In any case - it depends all highly on your data and queries. You will have to do your own experiments.

On 09 Nov 2015, at 07:02, ""fightfate@163.com"" <fightfate@163.com> wrote:

Hi, community

We are specially interested about this featural integration according to some slides from [1]. The SMACK(Spark+Mesos+Akka+Cassandra+Kafka)

seems good implementation for lambda architecure in the open-source world, especially non-hadoop based cluster environment. As we can see, 

the advantages obviously consist of :

1 the feasibility and scalability of spark datafram api, which can also make a perfect complement for Apache Cassandra native cql feature.

2 both streaming and batch process availability using the ALL-STACK thing, cool.

3 we can both achieve compacity and usability for spark with cassandra, including seemlessly integrating with job scheduling and resource management.

Only one concern goes to the OLAP query performance issue, which mainly caused by frequent aggregation work between daily increased large tables, for 

both spark sql and cassandra. I can see that the [1] use case facilitates FiloDB to achieve columnar storage and query performance, but we had nothing more 

knowledge. 

Question is : Any guy had such use case for now, especially using in your production environment ? Would be interested in your architeture for designing this 

OLAP engine using spark +  cassandra. What do you think the comparison between the scenario with traditional OLAP cube design? Like Apache Kylin or 

pentaho mondrian ? 

Best Regards,

Sun.


[1]  http://www.slideshare.net/planetcassandra/cassandra-summit-2014-interactive-olap-queries-using-apache-cassandra-and-spark



fightfate@163.com
"
Muhammad Haseeb Javed <11besemjaved@seecs.edu.pk>,"Mon, 9 Nov 2015 12:41:48 +0500",Wrap an RDD with a ShuffledRDD,"user <user@spark.apache.org>, dev <dev@spark.apache.org>","I am working on a modified Spark core and have a Broadcast variable which I
deserialize to obtain an RDD along with its set of dependencies, as is done
in ShuffleMapTask, as following:

val taskBinary: Broadcast[Array[Byte]]var (rdd, dep) =
ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](
      ByteBuffer.wrap(taskBinary.value),
Thread.currentThread.getContextClassLoader)

However, I want to wrap this rdd by a ShuffledRDD because I need to apply a
custom partitioner to it ,and I am doing this by:

var wrappedRDD = new ShuffledRDD[_ ,_, _](rdd[_ <: Product2[Any,
Any]], context.getCustomPartitioner())

but it results in an error:

Error:unbound wildcard type rdd = new ShuffledRDD[_ ,_, _ ](rdd[_ <:
Product2[Any, Any]], context.getCustomPartitioner())
..................................^

The problem is that I don't know how to replace these wildcards with any
inferred type as I its supposed to be dynamic and I have no idea what would
be the inferred type of the original rdd. Any idea how I could resolved
this?
"
Tim Preece <tepreece@mail.com>,"Mon, 9 Nov 2015 05:12:33 -0700 (MST)","Re: Anyone has perfect solution for spark source code compilation
 issue on intellij",dev@spark.apache.org,"I've had success building with maven  ( 3.3.3 ) with:
Intellij 14.1.5
scala 2.10.4
openjdk 7  (1.7.0_79)

What OS/Platform are you on ?



--

---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 9 Nov 2015 19:01:21 +0530",Re: sample or takeSample or ??,=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?= <zzq98736@alibaba-inc.com>,"You can't create a new RDD by selecting few elements. A rdd.take(n),
takeSample etc are actions and it will trigger your entire pipeline to be
executed.
You can although do something like this i guess:

val sample_data = rdd.take(10)

val sample_rdd = sc.parallelize(sample_data)



Thanks
Best Regards


st?
"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 9 Nov 2015 19:03:42 +0530",Re: Guidance to get started,Aaska Shah <aaskashah1996@gmail.com>,"You can read the installation details from here
http://spark.apache.org/docs/latest/

You can read about contributing to spark from here
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Thanks
Best Regards


"
"""=?gb18030?B?Umlja3k=?="" <494165115@qq.com>","Mon, 9 Nov 2015 22:57:02 +0800","=?gb18030?B?u9i4tKO6IFtWT1RFXSBSZWxlYXNlIEFwYWNoZSBT?=
 =?gb18030?B?cGFyayAxLjUuMiAoUkMyKQ==?=","""=?gb18030?B?U2VhbiBPd2Vu?="" <sowen@cloudera.com>, ""=?gb18030?B?VGVkIFl1?="" <yuzhihong@gmail.com>, ""=?gb18030?B?S3Jpc2huYSBTYW5rYXI=?="" <ksankar42@gmail.com>","Now I try the spark-1.5.2-rc2.zip from githup ,the result also has  errors .


[root@ouyangshourui spark-1.5.2-rc2]# pwd
/SparkCode/spark-1.5.2-rc2
[root@ouyangshourui spark-1.5.2-rc2]# nohup   mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package &



The error log as following:


[INFO] Building jar: /SparkCode/spark-1.5.2-rc2/external/flume-assembly/target/spark-streaming-flume-assembly_2.10-1.5.2-test-sources.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project External MQTT 1.5.2
[INFO] ------------------------------------------------------------------------
Downloading: http://maven.oschina.net/content/groups/public/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.0.1/org.eclipse.paho.client.mqttv3-1.0.1.pom
[WARNING] The POM for org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1 is missing, no dependency information available
Downloading: http://maven.oschina.net/content/groups/public/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.0.1/org.eclipse.paho.client.mqttv3-1.0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [ 13.576 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 19.966 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 11.279 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  5.353 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 20.199 s]
[INFO] Spark Project Core ................................. SUCCESS [04:18 min]
[INFO] Spark Project Bagel ................................ SUCCESS [ 27.070 s]
[INFO] Spark Project GraphX ............................... SUCCESS [01:09 min]
[INFO] Spark Project Streaming ............................ SUCCESS [01:57 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [02:21 min]
[INFO] Spark Project SQL .................................. SUCCESS [02:50 min]
[INFO] Spark Project ML Library ........................... SUCCESS [03:01 min]
[INFO] Spark Project Tools ................................ SUCCESS [ 13.731 s]
[INFO] Spark Project Hive ................................. SUCCESS [02:06 min]
[INFO] Spark Project REPL ................................. SUCCESS [ 42.023 s]
[INFO] Spark Project YARN ................................. SUCCESS [ 56.501 s]
[INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 53.986 s]
[INFO] Spark Project Assembly ............................. SUCCESS [01:58 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [ 18.626 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 34.569 s]
[INFO] Spark Project External Flume ....................... SUCCESS [ 29.643 s]
[INFO] Spark Project External Flume Assembly .............. SUCCESS [  4.430 s]
[INFO] Spark Project External MQTT ........................ FAILURE [  5.822 s]
[INFO] Spark Project External MQTT Assembly ............... SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project External Kafka Assembly .............. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 25:41 min
[INFO] Finished at: 2015-11-09T09:45:27-05:00
[INFO] Final Memory: 73M/1041M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project spark-streaming-mqtt_2.10: Could not resolve dependencies for project org.apache.spark:spark-streaming-mqtt_2.10:jar:1.5.2: Could not find artifact org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1 in nexus-osc (http://maven.oschina.net/content/groups/public/) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-streaming-mqtt_2.10






Best Regards 

Ricky Yang(ŷ)







 





------------------ ԭʼʼ ------------------
: ""Sean Owen"";<sowen@cloudera.com>;
ʱ: 2015119(һ) 2:39
ռ: ""Ricky""<494165115@qq.com>; 
: ""Denny Lee""<denny.g.lee@gmail.com>; ""Mark Hamstra""<mark@clearstorydata.com>; ""Reynold Xin""<rxin@databricks.com>; ""dev@spark.apache.org""<dev@spark.apache.org>; 
: Re: [VOTE] Release Apache Spark 1.5.2 (RC2)



Looks like you are building a module without install-ing other
modules. That won't work in general in Maven. Also, it looks like you
are building a snapshot, not the release we are talking about.

On Mon, Nov 9, 2015 at 3:28 AM, ŷ <494165115@qq.com> wrote:
>
> build spark-streaming-mqtt_2.10 failed!
>
> nohup mvn -X -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive
> -Phive-thriftserver -DskipTests clean t
> [DEBUG] org.apache.activemq:activemq-core:jar:5.7.0:test
> [DEBUG] org.apache.geronimo.specs:geronimo-jms_1.1_spec:jar:1.1.1:test
> [DEBUG] org.apache.actiemq.protobuf:activemq-protobuf:jar:1.1:test
> [DEBUG] org.fusesource.mqtt-client:mqtt-client:jar:1.3:test
> [DEBUG] org.fusesource.hawtdispatch:hawtdispatch-traurce.hawtbuf:hawtbuf:jar:1.9:test
> [DEBUG]
> org.apache.geronimo.specs:geronimo-j2ee-management_1.1_spec:palliance:aopalliance:jar:1.0:test
> [DEBUG] org.spri.1.1:test
> [DEBUG] org.springframework:spring-expression:jar:3.0.7.RELEASE:test
> [DEBUG] org.springframejasypt:jasypt:jar:1.9.0:test
> [DEBUG] org.spark-projla-lang:scala-reflect:jar:2.10.4:provided
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project External MQTT ........................ FAILURE [ 2.403
> s]
> [INFO] Spark Project External MQTT Assembly ............... SKIPPED
> [INFO] Spark Project External ZeroMQ ...................... SKIPPED
> [INFO] Spark Project External Kafka ....................... SKIPPED
> [INFO] Spark Project Examples ............................. SKIPPED
> [INFO] Spark Project External Kafka Assembly .............. SKIPPED
> [INFO]
> ------------------------------------------------------------------------
> [INFO] BUILD FAILURE
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Total time: 4.471 s
> [INFO] Finished at: 2015-11-09T11:10:57+08:00
> [INFO] Final Memory: 31M/173M
> [INFO]
> ------------------------------------------------------------------------
> [WARNING] The requested profile ""hive"" could not be activated because it
> does not exist.
> [ERROR] Failed to execute goal on project spark-streaming-mqtt_2.10: Could
> not resolve dependencies for project
> org.apache.spark:spark-streaming-mqtt_2.10:jar:1.5.0-SNAPSHOT: The following
> aark-streaming_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.spark:spark-core_2.10:jar:1.5.0-SNAPSHOT,
> org.apacheg.apache.spark:spark-launcher_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.spark:spark-network-common_2.10:jar:1.5.0-SNAPSHOT,
> org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1: Failure to find
> org.apache.spark:spark-streaming_2.10:jar:1.5.0-20150818.023902-334 in
> http://maven.cnsuning.com/content/groups/public/ was cached in the local
> repository, resolution will not be reattempted until the update interval of
> suning_maven_repo has elapsed or updates are forced -> [Help 1]
> org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
> goal on project spark-streaming-mqtt_2.10: Could not resolve dependencies
> for project org.apache.spark:spark-streaming-mqtt_2.10:jar:1.5.0-SNAPSHOT:
> The following artifacts could not be resolved:
> org.apache.spark:spark-streaming_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.spark:spark-core_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.spark:spar,
> org.eclipse.paho:org.eclipse.paho.client.mqttv3:jreaming_2.10:jar:1.5.0-20150818.023902-334 in
> http://maven.cnsuning.com/content/groups/public/ was cached in the local
> repository, resolution will not be reattempted until the update interval of
> suning_maven_repo has elapsed or updates are forced
> ҵiPhone
>
>"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 9 Nov 2015 20:29:23 +0530","Re: Some spark apps fail with ""All masters are unresponsive"", while
 others pass normally",Romi Kuntsman <romi@totango.com>,"Is that all you have in the executor logs? I suspect some of those jobs are
having a hard time managing  the memory.

Thanks
Best Regards


"
Romi Kuntsman <romi@totango.com>,"Mon, 9 Nov 2015 17:14:25 +0200","Re: Some spark apps fail with ""All masters are unresponsive"", while
 others pass normally",Akhil Das <akhil@sigmoidanalytics.com>,"If they have a problem managing memory, wouldn't there should be a OOM?
Why does AppClient throw a NPE?

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 9 Nov 2015 21:30:15 +0530","Re: Some spark apps fail with ""All masters are unresponsive"", while
 others pass normally",Romi Kuntsman <romi@totango.com>,"Did you find anything regarding the OOM in the executor logs?

Thanks
Best Regards


"
Romi Kuntsman <romi@totango.com>,"Mon, 9 Nov 2015 18:30:10 +0200","Re: Some spark apps fail with ""All masters are unresponsive"", while
 others pass normally",Akhil Das <akhil@sigmoidanalytics.com>,"I didn't see anything about a OOM.
This happens sometimes before anything in the application happened, and
happens to a few applications at the same time - so I guess it's a
communication failure, but the problem is that the error shown doesn't
represent the actual problem (which may be a network timeout etc)

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Tim Preece <tepreece@mail.com>,"Mon, 9 Nov 2015 09:40:00 -0700 (MST)","Re: Some spark apps fail with ""All masters are unresponsive"", while
 others pass normally",dev@spark.apache.org,"Searching shows several people hit this same NPE in AppClient.scala line 160
( perhaps because appID was null - could  application had be stopped before
registered ?) 



--

---------------------------------------------------------------------


"
"""=?gb18030?B?Umlja3k=?="" <494165115@qq.com>","Tue, 10 Nov 2015 01:20:04 +0800","=?gb18030?B?u9i4tKO6IFtWT1RFXSBSZWxlYXNlIEFwYWNoZSBT?=
 =?gb18030?B?cGFyayAxLjUuMiAoUkMyKQ==?=","""=?gb18030?B?U2VhbiBPd2Vu?="" <sowen@cloudera.com>","thank for your help,do as you said,the problem is firewall issues,when changing to  maven default repo (http://repo1.maven.org) from  http://maven.oschina.net, spark-streaming-mqtt_2.10 module  compiled Successfully.



------------------

Best Regards 











 




------------------ ԭʼʼ ------------------
: ""ŷ"";<494165115@qq.com>;
ʱ: 2015119(һ) 10:57
ռ: ""Sean Owen""<sowen@cloudera.com>; ""Ted Yu""<yuzhihong@gmail.com>; ""Krishna Sankar""<ksankar42@gmail.com>; 
: ""Denny Lee""<denny.g.lee@gmail.com>; ""Mark Hamstra""<mark@clearstorydata.com>; ""Reynold Xin""<rxin@databricks.com>; ""dev@spark.apache.org""<dev@spark.apache.org>; 
: ظ [VOTE] Release Apache Spark 1.5.2 (RC2)



Now I try the spark-1.5.2-rc2.zip from githup ,the result also has  errors .


[root@ouyangshourui spark-1.5.2-rc2]# pwd
/SparkCode/spark-1.5.2-rc2
[root@ouyangshourui spark-1.5.2-rc2]# nohup   mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package &



The error log as following:


[INFO] Building jar: /SparkCode/spark-1.5.2-rc2/external/flume-assembly/target/spark-streaming-flume-assembly_2.10-1.5.2-test-sources.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project External MQTT 1.5.2
[INFO] ------------------------------------------------------------------------
Downloading: http://maven.oschina.net/content/groups/public/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.0.1/org.eclipse.paho.client.mqttv3-1.0.1.pom
[WARNING] The POM for org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1 is missing, no dependency information available
Downloading: http://maven.oschina.net/content/groups/public/org/eclipse/paho/org.eclipse.paho.client.mqttv3/1.0.1/org.eclipse.paho.client.mqttv3-1.0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [ 13.576 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 19.966 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 11.279 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  5.353 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 20.199 s]
[INFO] Spark Project Core ................................. SUCCESS [04:18 min]
[INFO] Spark Project Bagel ................................ SUCCESS [ 27.070 s]
[INFO] Spark Project GraphX ............................... SUCCESS [01:09 min]
[INFO] Spark Project Streaming ............................ SUCCESS [01:57 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [02:21 min]
[INFO] Spark Project SQL .................................. SUCCESS [02:50 min]
[INFO] Spark Project ML Library ........................... SUCCESS [03:01 min]
[INFO] Spark Project Tools ................................ SUCCESS [ 13.731 s]
[INFO] Spark Project Hive ................................. SUCCESS [02:06 min]
[INFO] Spark Project REPL ................................. SUCCESS [ 42.023 s]
[INFO] Spark Project YARN ................................. SUCCESS [ 56.501 s]
[INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 53.986 s]
[INFO] Spark Project Assembly ............................. SUCCESS [01:58 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [ 18.626 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 34.569 s]
[INFO] Spark Project External Flume ....................... SUCCESS [ 29.643 s]
[INFO] Spark Project External Flume Assembly .............. SUCCESS [  4.430 s]
[INFO] Spark Project External MQTT ........................ FAILURE [  5.822 s]
[INFO] Spark Project External MQTT Assembly ............... SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project External Kafka Assembly .............. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 25:41 min
[INFO] Finished at: 2015-11-09T09:45:27-05:00
[INFO] Final Memory: 73M/1041M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project spark-streaming-mqtt_2.10: Could not resolve dependencies for project org.apache.spark:spark-streaming-mqtt_2.10:jar:1.5.2: Could not find artifact org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1 in nexus-osc (http://maven.oschina.net/content/groups/public/) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -s 

Ricky Yang(ŷ)







 





------------------ ԭʼʼ ------------------
: ""Sean Owen"";<sowen@cloudera.com>;
ʱ: 2015119(һ) 2:39
ռ: ""Ricky""<494165115@qq.com>; 
: ""Denny Lee""<denny.g.lee@gmail.com>; ""Mark Hamstra""<mark@clearstorydata.com>; ""Reynold Xin""<rxin@databricks.com>; ""dev@spark.apache.org""<dev@spark.apache.org>; 
: Re: [VOTE] Release Apache Spark 1.5.2 (RC2)



Looks like you are building a module without install-ing other
modules. That won't work in general in Maven. Also, it looks like you
are building a snapshot, not the release we are talking about.

On Mon, Nov 9, 2015 at 3:28 AM, ŷ <494165115@qq.com> wrote:
>
> build spark-streaming-mqtt_2.10 failed!
>
> nohup mvn -X -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive
> -Phive-thriftserver -DskipTests clean package -rf :spark-streaming-mqtt_2.10
> &
>
> [DEBUG] org.scala-tools.testing:test-interface:jar:0.5:test
> [DEBUG] org.apache.activemq:activemq-core:jar:5.7.0:test
> [DEBUG] org.apache.geronimo.specs:gero.activemq:kahadb:jar:5.7.0:test
> [DEBUG] org.apache.activemq.protobuf:activemq-protobuf:jar:1.1:test
> [DEBUG] org.fusesource.mqtt-client:mqtt-client:jar:1.3:test
> [DEBUG] org.fusesource.hawtdispatch:hawtdispatch-transport:jar:1.11:test
> [DEBUG] org.fusesource.hawtdispatch:hawtdispatch:jar:1.11:test
> [DEBUG] org.frg.apache.geronimo.specs:geronimo-j2ee-management_1.1_spec:jar:1.0.1:test
> [DEBUG] org.springframework:spring-context:jar:3.0.7.RELEASE:test
> [DEBUG] org.springframework:spring-aop:jar:3.0.7.RELEASE:test
> [DEBUG] aopalliance:aopalliance:jar:1.0:test
> [DEBUG] org.springframework:spring-beans:jar:3.0.7.RELEASE:test
> [DEBUG] org.springframework:spring-core:jar:3.0.7.RELEASE:test
> [DEBUG] commons-logging:commons-logging: org.jasypt:jasypt:jar:1.9.0:test
> [DEBUG] org.spark-project.spark:unused:jar:1.0.0:compile
> [DEBUG] org.scalatest:scalatest_2.10:jar:2.2.1:test
> [DEBUG] orO]
> ------------------------------------------------------------------------
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project External MQTT ........................ FAILURE [ 2.403
> s]
> [INFO] Spark Project External MQTT Assembly ............... SKIPPED
> [INFO] Spark Project External ZeroMQ ...................... SKIPPED
> [INFO] Spark Project External Kafka ....................... SKIPPED
> [INFO] Spark Project Examples ............................. SKIPPED
> [INFO] Spark Project External Kafka Assembly .............. SKIPPED
> [INFO]
> ------------------------------------------------------------------------
> [INFO] BUILD FAILURE
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Total time: 4.471 s
> [INFO] Finished at: 2015-11-09T11:10:57+08:00
> [INFO] Final Memory: 31M/173M
> [INFO]
> ------------------------------------------------------------------------
> [WARNING] The requested profile ""hive"" could not be activated because it
> does not exist.
> [ERROR] Failed to execute goal on project spark-streaming-mqtt_2.10: Could
> not reso-streaming-mqtt_2.10:jar:1.5.0-SNAPSHOT: The following
> artifacts could not be resolved:
> org.apache.spark:spark-streaming_2.10:jar:1.5.0-SNAPSHOT,
> org.apapache.spark:spark-core_2.10:jar:tests:1.5.0-SNAPSHOT,
> org.apache.spark:spark-launcher_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.spark:spark-network-common_2.10:jar:1.5.0-SNAPSHOT,
> org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1: Failure to find
> org.apache.spark:spark-streaming_2.10:jar:1.5.0-20150818.023902-334 in
> http://maven.cnsuning.com/content/groups/public/ was cached in the local
> repository, resolution will not be reattempted until the update interval of
> suning_maven_repo has elapsed or updates are forced -> [Help 1]
> org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
> goal on project spark-streaming-mqtt_2.10: Could not resolve dependencies
> for project org.apache.spark:spark-streaming-mqtt_2.10:jar:1.5.0-SNAPSHOT:
> The following artifacreaming_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.spark:spark-core_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.sparkhe.spark:spark-launcher_2.10:jar:1.5.0-SNAPSHOT,
> org.apache.spark:spark-network-common_2.10:jar:1.5.0-SNAPSHOT,
> org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.0.1: Failure to find
> org.apache.spark:spark-streaming_2.10:jar:1.5.0-20150818.023902-334 in
> http://maven.cnsuning.com/content/groups/public/ was cached in the local
> repository, resolution will not be reattempted until the update interval of
> suning_maven_repo has elapsed or updates are forced
> ҵiPhone
>
>"
turp1twin <turp1twin@gmail.com>,"Mon, 9 Nov 2015 11:32:27 -0700 (MST)",Re: Block Transfer Service encryption support,dev@spark.apache.org,"I created a pull request for issue  SPARK-6373
<https://issues.apache.org/jira/browse/SPARK-6373>  .... Any feedback would
be appreciated... https://github.com/apache/spark/pull/9416


Jeff




--

---------------------------------------------------------------------


"
tsh <tsh@timshenkao.su>,"Mon, 9 Nov 2015 21:56:39 +0300",Re: OLAP query using spark dataframe with cassandra,"""fightfate@163.com"" <fightfate@163.com>, user@spark.apache.org,
 dev@spark.apache.org","Hi,

I'm in the same position right now: we are going to implement something 
like OLAP BI + Machine Learning explorations on the same cluster.
Well, the question is quite ambivalent: from one hand, we have terabytes 
of versatile data and the necessity to make something like cubes (Hive 
and Hive on HBase are unsatisfactory). From the other, our users get 
accustomed to Tableau + Vertica.
So, right now I consider the following choices:
1) Platfora (not free, I don't know price right now) + Spark
2) AtScale + Tableau(not free, I don't know price right now) + Spark
3) Apache Kylin (young project?) + Spark on YARN + Kafka + Flume + some 
storage
4) Apache Phoenix + Apache HBase + Mondrian + Spark on YARN + Kafka + 
Flume (has somebody use it in production?)
5) Spark + Tableau  (cubes?)

For myself, I decided not to dive into Mesos. Cassandra is hardly 
configurable, you'll have to dedicate special employee to support it.

I'll be glad to hear other ideas & propositions as we are at the 
beginning of the process too.

Sincerely yours, Tim Shenkao


"
Alex Nastetsky <alex.nastetsky@vervemobile.com>,"Mon, 9 Nov 2015 14:02:47 -0500",Re: Sort Merge Join from the filesystem,"""Cheng, Hao"" <hao.cheng@intel.com>","Thanks for creating that ticket.

Another thing I was thinking of, is doing this type of join between dataset
A which is already partitioned/sorted on disk and dataset B, which gets
generated during the run of the application.

Dataset B would need something like repartitionAndSortWithinPartitions to
be performed on it, using the same partitioner that was used with dataset
A. Then dataset B could be joined with dataset A without needing to write
it to disk first (unless it's too big to fit in memory, then it would need
to be [partially] spilled).


"
shane knapp <sknapp@berkeley.edu>,"Mon, 9 Nov 2015 14:36:28 -0800","[build system] shane OOO until monday, nov 16","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Josh Rosen <joshrosen@databricks.com>, Jon Kuroda <jkuroda@eecs.berkeley.edu>","i'll be at the USENIX LISA conference in DC, so josh and jon will be
keeping an eye on jenkins and making sure it doesn't misbehave.

since attending every session of every day will drive one insane, i
will be sporadically checking in and making sure things are humming
along...  but for emergencies, feel free to reach out to either josh
rosen or jon kuroda (CCed on this mail).

danke shane  :)

---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Tue, 10 Nov 2015 00:23:37 +0000",RE: Sort Merge Join from the filesystem,Alex Nastetsky <alex.nastetsky@vervemobile.com>,"Yes, we definitely need to think how to handle this case, probably even more common than both sorted/partitioned tables case, can you jump to the jira and leave comment there?

From: Alex Nastetsky [mailto:alex.nastetsky@vervemobile.com]
Sent: Tuesday, November 10, 2015 3:03 AM
To: Cheng, Hao
Cc: Reynold Xin; dev@spark.apache.org
Subject: Re: Sort Merge Join from the filesystem

Thanks for creating that ticket.

Another thing I was thinking of, is doing this type of join between dataset A which is already partitioned/sorted on disk and dataset B, which gets generated during the run of the application.

Dataset B would need something like repartitionAndSortWithinPartitions to be performed on it, using the same partitioner that was used with dataset A. Then dataset B could be joined with dataset A without needing to write it to disk first (unless it's too big to fit in memory, then it would need to be [partially] spilled).

On Wed, Nov 4, 2015 at 7:51 PM, Cheng, Hao <hao.cheng@intel.com<mailto:hao.cheng@intel.com>> wrote:
Yes, we probably need more change for the data source API if we need to implement it in a generic way.
BTW, I create the JIRA by copy most of words from Alex. ☺

https://issues.apache.org/jira/browse/SPARK-11512


From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
Sent: Thursday, November 5, 2015 1:36 AM
To: Alex Nastetsky
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Sort Merge Join from the filesystem

It's not supported yet, and not sure if there is a ticket for it. I don't think there is anything fundamentally hard here either.


On Wed, Nov 4, 2015 at 6:37 AM, Alex Nastetsky <alex.nastetsky@vervemobile.com<mailto:alex.nastetsky@vervemobile.com>> wrote:
(this is kind of a cross-post from the user list)

Does Spark support doing a sort merge join on two datasets on the file system that have already been partitioned the same with the same number of partitions and sorted within each partition, without needing to repartition/sort them again?

This functionality exists in
- Hive (hive.optimize.bucketmapjoin.sortedmerge)
- Pig (USING 'merge')
- MapReduce (CompositeInputFormat)

If this is not supported in Spark, is a ticket already open for it? Does the Spark architecture present unique difficulties to having this feature?

It is very useful to have this ability, as you can prepare dataset A to be joined with dataset B before B even exists, by pre-processing A with a partition/sort.

Thanks.


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 9 Nov 2015 17:22:07 -0800",Re: OLAP query using spark dataframe with cassandra,=?utf-8?Q?Andr=C3=A9s_Ivaldi?= <iaivaldi@gmail.com>,"Please consider using NoSQL engine such as hbase. 

Cheers


e, a possible solution is use Spark as Multiple Source connector and basic transformation layer, then persist the information (actually is a RDBM), after that with our engine we build a kind of Cube queries, and the result is processed again by Spark adding Machine Learning.
alable than RDBM, dont care about pre processing information if after pre processing the queries are fast.
ike OLAP BI + Machine Learning explorations on the same cluster.
tes of versatile data and the necessity to make something like cubes (Hive and Hive on HBase are unsatisfactory). From the other, our users get accustomed to Tableau + Vertica. 
torage
me (has somebody use it in production?)
rable, you'll have to dedicate special employee to support it.
g of the process too.
park sql on cassandra, while
ifferent from traditional OLAP
on choice of olap architecture. 
tion? If no and your core business is not software then you may want to look for something else, because it might not make sense to build up internal know-how in all of these areas.
ave to do your own experiments.

o some slides from [1]. The SMACK(Spark+Mesos+Akka+Cassandra+Kafka)
ld, especially non-hadoop based cluster environment. As we can see, 
 make a perfect complement for Apache Cassandra native cql feature.
ng, cool.
 including seemlessly integrating with job scheduling and resource management.
 caused by frequent aggregation work between daily increased large tables, for 
es FiloDB to achieve columnar storage and query performance, but we had nothing more 
ur production environment ? Would be interested in your architeture for designing this 
etween the scenario with traditional OLAP cube design? Like Apache Kylin or 
teractive-olap-queries-using-apache-cassandra-and-spark
"
"""fightfate@163.com"" <fightfate@163.com>","Tue, 10 Nov 2015 10:01:50 +0800",Re: Re: OLAP query using spark dataframe with cassandra,"tsh <tsh@timshenkao.su>, 
	user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","Hi,

According to my experience, I would recommend option 3) using Apache Kylin for your requirements. 

This is a suggestion based on the open-source world. 

For the per cassandra thing, I accept your advice for the special support thing. But the community is very

open and convinient for prompt response. 



fightfate@163.com
 
From: tsh
Date: 2015-11-10 02:56
To: fightfate@163.com; user; dev
Subject: Re: OLAP query using spark dataframe with cassandra
Hi,

I'm in the same position right now: we are going to implement something like OLAP BI + Machine Learning explorations on the same cluster.
Well, the question is quite ambivalent: from one hand, we have terabytes of versatile data and the necessity to make something like cubes (Hive and Hive on HBase are unsatisfactory). From the other, our users get accustomed to Tableau + Vertica. 
So, right now I consider the following choices:
1) Platfora (not free, I don't know price right now) + Spark
2) AtScale + Tableau(not free, I don't know price right now) + Spark
3) Apache Kylin (young project?) + Spark on YARN + Kafka + Flume + some storage
4) Apache Phoenix + Apache HBase + Mondrian + Spark on YARN + Kafka + Flume (has somebody use it in production?)
5) Spark + Tableau  (cubes?)

For myself, I decided not to dive into Mesos. Cassandra is hardly configurable, you'll have to dedicate special employee to support it.

I'll be glad to hear other ideas & propositions as we are at the beginning of the process too.

Sincerely yours, Tim Shenkao

On 11/09/2015 09:46 AM, fightfate@163.com wrote:
Hi, 

Thanks for suggesting. Actually we are now evaluating and stressing the spark sql on cassandra, while

trying to define business models. FWIW, the solution mentioned here is different from traditional OLAP

cube engine, right ? So we are hesitating on the common sense or direction choice of olap architecture. 

And we are happy to hear more use case from this community. 

Best,
Sun. 



fightfate@163.com
 
From: Jörn Franke
Date: 2015-11-09 14:40
To: fightfate@163.com
CC: user; dev
Subject: Re: OLAP query using spark dataframe with cassandra

Is there any distributor supporting these software components in combination? If no and your core business is not software then you may want to look for something else, because it might not make sense to build up internal know-how in all of these areas.

In any case - it depends all highly on your data and queries. You will have to do your own experiments.

On 09 Nov 2015, at 07:02, ""fightfate@163.com"" <fightfate@163.com> wrote:

Hi, community

We are specially interested about this featural integration according to some slides from [1]. The SMACK(Spark+Mesos+Akka+Cassandra+Kafka)

seems good implementation for lambda architecure in the open-source world, especially non-hadoop based cluster environment. As we can see, 

the advantages obviously consist of :

1 the feasibility and scalability of spark datafram api, which can also make a perfect complement for Apache Cassandra native cql feature.

2 both streaming and batch process availability using the ALL-STACK thing, cool.

3 we can both achieve compacity and usability for spark with cassandra, including seemlessly integrating with job scheduling and resource management.

Only one concern goes to the OLAP query performance issue, which mainly caused by frequent aggregation work between daily increased large tables, for 

both spark sql and cassandra. I can see that the [1] use case facilitates FiloDB to achieve columnar storage and query performance, but we had nothing more 

knowledge. 

Question is : Any guy had such use case for now, especially using in your production environment ? Would be interested in your architeture for designing this 

OLAP engine using spark +  cassandra. What do you think the comparison between the scenario with traditional OLAP cube design? Like Apache Kylin or 

pentaho mondrian ? 

Best Regards,

Sun.


[1]  http://www.slideshare.net/planetcassandra/cassandra-summit-2014-interactive-olap-queries-using-apache-cassandra-and-spark



fightfate@163.com

"
"""fightfate@163.com"" <fightfate@163.com>","Tue, 10 Nov 2015 10:11:49 +0800",Re: Re: OLAP query using spark dataframe with cassandra,"=?UTF-8?B?QW5kcsOpcyBJdmFsZGk=?= <iaivaldi@gmail.com>, 
	tsh <tsh@timshenkao.su>","Hi, 

Have you ever considered cassandra as a replacement ? We are now almost the seem usage as your engine, e.g. using mysql to store 

initial aggregated data. Can you share more about your kind of Cube queries ? We are very interested in that arch too : )

Best,
Sun.


fightfate@163.com
 
From: Andrés Ivaldi
Date: 2015-11-10 07:03
To: tsh
CC: fightfate@163.com; user; dev
Subject: Re: OLAP query using spark dataframe with cassandra
Hi,
I'm also considering something similar, Spark plain is too slow for my case, a possible solution is use Spark as Multiple Source connector and basic transformation layer, then persist the information (actually is a RDBM), after that with our engine we build a kind of Cube queries, and the result is processed again by Spark adding Machine Learning.
Our Missing part is reemplace the RDBM with something more suitable and scalable than RDBM, dont care about pre processing information if after pre processing the queries are fast.

Regards

On Mon, Nov 9, 2015 at 3:56 PM, tsh <tsh@timshenkao.su> wrote:
Hi,

I'm in the same position right now: we are going to implement something like OLAP BI + Machine Learning explorations on the same cluster.
Well, the question is quite ambivalent: from one hand, we have terabytes of versatile data and the necessity to make something like cubes (Hive and Hive on HBase are unsatisfactory). From the other, our users get accustomed to Tableau + Vertica. 
So, right now I consider the following choices:
1) Platfora (not free, I don't know price right now) + Spark
2) AtScale + Tableau(not free, I don't know price right now) + Spark
3) Apache Kylin (young project?) + Spark on YARN + Kafka + Flume + some storage
4) Apache Phoenix + Apache HBase + Mondrian + Spark on YARN + Kafka + Flume (has somebody use it in production?)
5) Spark + Tableau  (cubes?)

For myself, I decided not to dive into Mesos. Cassandra is hardly configurable, you'll have to dedicate special employee to support it.

I'll be glad to hear other ideas & propositions as we are at the beginning of the process too.

Sincerely yours, Tim Shenkao


On 11/09/2015 09:46 AM, fightfate@163.com wrote:
Hi, 

Thanks for suggesting. Actually we are now evaluating and stressing the spark sql on cassandra, while

trying to define business models. FWIW, the solution mentioned here is different from traditional OLAP

cube engine, right ? So we are hesitating on the common sense or direction choice of olap architecture. 

And we are happy to hear more use case from this community. 

Best,
Sun. 



fightfate@163.com
 
From: Jörn Franke
Date: 2015-11-09 14:40
To: fightfate@163.com
CC: user; dev
Subject: Re: OLAP query using spark dataframe with cassandra

Is there any distributor supporting these software components in combination? If no and your core business is not software then you may want to look for something else, because it might not make sense to build up internal know-how in all of these areas.

In any case - it depends all highly on your data and queries. You will have to do your own experiments.

On 09 Nov 2015, at 07:02, ""fightfate@163.com"" <fightfate@163.com> wrote:

Hi, community

We are specially interested about this featural integration according to some slides from [1]. The SMACK(Spark+Mesos+Akka+Cassandra+Kafka)

seems good implementation for lambda architecure in the open-source world, especially non-hadoop based cluster environment. As we can see, 

the advantages obviously consist of :

1 the feasibility and scalability of spark datafram api, which can also make a perfect complement for Apache Cassandra native cql feature.

2 both streaming and batch process availability using the ALL-STACK thing, cool.

3 we can both achieve compacity and usability for spark with cassandra, including seemlessly integrating with job scheduling and resource management.

Only one concern goes to the OLAP query performance issue, which mainly caused by frequent aggregation work between daily increased large tables, for 

both spark sql and cassandra. I can see that the [1] use case facilitates FiloDB to achieve columnar storage and query performance, but we had nothing more 

knowledge. 

Question is : Any guy had such use case for now, especially using in your production environment ? Would be interested in your architeture for designing this 

OLAP engine using spark +  cassandra. What do you think the comparison between the scenario with traditional OLAP cube design? Like Apache Kylin or 

pentaho mondrian ? 

Best Regards,

Sun.


[1]  http://www.slideshare.net/planetcassandra/cassandra-summit-2014-interactive-olap-queries-using-apache-cassandra-and-spark



fightfate@163.com




-- 
Ing. Ivaldi Andres
"
Yuming Wang <q79969786@gmail.com>,"Tue, 10 Nov 2015 13:08:30 +0800",ml.feature.Word2Vec.transform() very slow issue,dev@spark.apache.org,"Hi



I found org.apache.spark.ml.feature.Word2Vec.transform() very slow.

I think we should not read broadcast every sentence, so I fixed on my forked.



https://github.com/979969786/spark/commit/a9f894df3671bb8df2f342de1820dab3185598f3



I have use 20000 number rows test it. Original version consume *5 minutes*,


​

and my version just consume *22 seconds* on same data.


​




If I'm right, I will pull request.



Thanks
"
Sudhir Menon <smenon@pivotal.io>,"Mon, 9 Nov 2015 21:34:24 -0800",Support for views/ virtual tables in SparkSQL,dev@spark.apache.org,"Team:

Do we plan to add support for views/ virtual tables in SparkSQL anytime
soon?
Trying to run the TPC-H workload and failing on queries that assumes
support for views in the underlying database

Thanks in advance

Suds
"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 10 Nov 2015 06:24:34 +0000",Re: Support for views/ virtual tables in SparkSQL,Sudhir Menon <smenon@pivotal.io>,"I think you can rewrite those TPC-H queries not using view, for example registerTempTable

Thanks.

Zhan Zhang


oon?
ort for views in the underlying database


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 10 Nov 2015 06:56:37 +0000",Re: ml.feature.Word2Vec.transform() very slow issue,Yuming Wang <q79969786@gmail.com>,"Since it's a fairly expensive operation to build the Map, I tend to agree
it should not happen in the loop.


ked.
3185598f3
*,
"
Luke Han <luke.hq@gmail.com>,"Tue, 10 Nov 2015 15:16:18 +0800",Re: OLAP query using spark dataframe with cassandra,tsh <tsh@timshenkao.su>,"Some friends refer me this thread about OLAP/Kylin and Spark...

Here's my 2 cents..

If you are trying to setup OLAP, Apache Kylin should be one good idea for
you to evaluate.

The project has developed more than 2 years and going to graduate to Apache
Top Level Project [1].
There are many deployments on production already include eBay, Exponential,
JD.com, VIP.com and others, refer to powered by page [2].

Apache Kylin's spark engine also on the way, there's discussion about
turning the performance [3].

There are variety clients are available to interactive with Kylin with ANSI
SQL, including Tableau, Zeppelin, Pentaho/mondrian, Saiku/mondrian, and the
Excel/PowerBI support will roll out this week.

Apache Kylin is young but mature with huge case validation (one biggest
cube in eBay contains 85+B rows, 90%ile production platform's query latency
in few seconds).

StreamingOLAP is coming in Kylin v2.0 with plug-able architecture, there's
already one real case on production inside eBay, please refer to our design
deck [4]

We are really welcome everyone to join and contribute to Kylin as OLAP
engine for Big Data:-)

Please feel free to contact our community or me for any question.

Thanks.

1. http://s.apache.org/bah
2. http://kylin.incubator.apache.org/community/poweredby.html
3. http://s.apache.org/lHA
4.
http://www.slideshare.net/lukehan/1-apache-kylin-deep-dive-streaming-and-plugin-architecture-apache-kylin-meetup-shanghai
5. http://kylin.io


Best Regards!
---------------------

Luke Han


d
ed
g
n
nt
,
,
ive-olap-queries-using-apache-cassandra-and-spark>
ve-olap-queries-using-apache-cassandra-and-spark
"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 10 Nov 2015 09:41:15 +0200",Re: ml.feature.Word2Vec.transform() very slow issue,dev <dev@spark.apache.org>,"Seems a straightforward change that purely enhances efficiency, so yes
please submit a JIRA and PR for this


rked.
b3185598f3
s*,
"
selvinsource <vselvaggio@gmail.com>,"Tue, 10 Nov 2015 02:54:08 -0700 (MST)",Re: PMML version in MLLib,dev@spark.apache.org,"Thank you Fazlan, looks good!



--

---------------------------------------------------------------------


"
danielcsant <dcarroza@stratio.com>,"Tue, 10 Nov 2015 03:58:13 -0700 (MST)",Re: OLAP query using spark dataframe with cassandra,dev@spark.apache.org,"You can also evaluate Stratio Sparkta. It is a real time aggregation tool
based on Spark Streaming. 
It is able to write in Cassandra and in other databases like MongoDB,
Elasticsearch,... It is prepared to deploy this aggregations in Mesos so
maybe it fits your necessities.

There is no a query layer that could abstract the analytics part in OLAP but
it is on the roadmap.

Disclaimer: I work in this product 



--

---------------------------------------------------------------------


"
Tim Preece <tepreece@mail.com>,"Tue, 10 Nov 2015 06:54:15 -0700 (MST)",Re: Block Transfer Service encryption support,dev@spark.apache.org,"Nb. I did notice some test failures when I ran a quick test on the pull
request ( not sure if it is related - I haven't looked in any detail at the
cause ).

Failed tests: 
 
SslChunkFetchIntegrationSuite>ChunkFetchIntegrationSuite.fetchBothChunks:201
expected:<[]> but was:<[0, 1]>
 
SslChunkFetchIntegrationSuite>ChunkFetchIntegrationSuite.fetchBufferChunk:175
expected:<[]> but was:<[0]>
 
SslChunkFetchIntegrationSuite>ChunkFetchIntegrationSuite.fetchChunkAndNonExistent:210
expected:<[]> but was:<[0]>
 
SslChunkFetchIntegrationSuite>ChunkFetchIntegrationSuite.fetchFileChunk:184
expected:<[]> but was:<[1]>
 
SslTransportClientFactorySuite>TransportClientFactorySuite.neverReturnInactiveClients:165
null
 
SslTransportClientFactorySuite>TransportClientFactorySuite.returnDifferentClientsForDifferentServers:145
null

Tim




--

---------------------------------------------------------------------


"
Tim Preece <tepreece@mail.com>,"Tue, 10 Nov 2015 08:32:47 -0700 (MST)",Re: Block Transfer Service encryption support,dev@spark.apache.org,"So it appears the tests fail because of an SSLHandshakeException. 

Tracing the failure I see:
3,0001,Using SSLEngineImpl.\0A
3,0001,\0AIs initial handshake: true\0A
3,0001,Ignoring unsupported cipher suite: SSL_RSA_WITH_DES_CBC_SHA for
TLSv1.2\0A
3,0001,No available cipher suite for TLSv1.2\0A
3,0001,shuffle-client-4\2C fatal error: 40: Couldn't kickstart
handshaking\0Ajavax.net.ssl.SSLHandshakeException: No appropriate
protocol\2C may be no appropriate cipher suite specified or protocols are
deactivated\0A
3,0001,shuffle-client-4
3,0001,\2C SEND TLSv1.2 ALERT:  
3,0001,fatal\2C 
3,0001,description = handshake_failure\0A
3,0001,shuffle-client-4\2C WRITE: TLSv1.2 Alert\2C length = 2\0A
3,0001,Using SSLEngineImpl.\0A
3,0001,shuffle-client-4\2C called closeOutbound()\0A
3,0001,shuffle-client-4\2C closeOutboundInternal()\0A
3,0001,[Raw write]: length = 7\0A
3,0001,0000: 15 03 03 00 02 02 28                              
.......\0A\0A
3,0001,\0AIs initial handshake: true\0A
3,0001,Ignoring unsupported cipher suite: SSL_RSA_WITH_DES_CBC_SHA for
TLSv1.2\0A
3,0001,No available cipher suite for TLSv1.2\0A
3,0001,shuffle-server-5\2C fatal error: 80: problem unwrapping net
record\0Ajavax.net.ssl.SSLHandshakeException: No appropriate protocol\2C may
be no appropriate cipher suite specified or protocols are deactivated\0A
3,0001,shuffle-server-5
3,0001,\2C SEND TLSv1.2 ALERT:  
3,0001,fatal\2C 
3,0001,description = internal_error\0A
3,0001,shuffle-server-5\2C WRITE: TLSv1.2 Alert\2C length = 2\0A
3,0001,shuffle-server-5\2C called closeOutbound()\0A
3,0001,shuffle-server-5\2C closeOutboundInternal()\0A
3,0001,shuffle-server-5\2C called closeInbound()\0A
3,0001,shuffle-server-5\2C closeInboundInternal()\0A
3,0001,shuffle-client-4\2C called closeOutbound()\0A
3,0001,shuffle-client-4\2C closeOutboundInternal()\0A
3,0001,shuffle-client-4\2C called closeInbound()\0A
3,0001,shuffle-client-4\2C closeInboundInternal()\0A
3,0001,shuffle-server-5\2C called closeOutbound()\0A
3,0001,shuffle-server-5\2C closeOutboundInternal()\0A
3,0001,shuffle-server-5\2C called closeInbound()\0A
3,0001,shuffle-server-5\2C closeInboundInternal()\0A

So this fails because of the use of DES. From
https://www-01.ibm.com/support/knowledgecenter/SSYKE2_7.0.0/com.ibm.java.security.component.71.doc/security-component/jsse2Docs/ciphersuites.html
I see: 2 RFC 5246 TLS 1.2 forbids the use of these suites. These can be used
in the SSLv3/TLS1.0/TLS1.1 protocols, but cannot be used in TLS 1.2 and
later.

Note. I'm using the IBM Java SDK.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Nov 2015 08:49:31 -0800",[ANNOUNCE] Announcing Spark 1.5.2,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi All,

Spark 1.5.2 is a maintenance release containing stability fixes. This
release is based on the branch-1.5 maintenance branch of Spark. We
*strongly recommend* all 1.5.x users to upgrade to this release.

The full list of bug fixes is here: http://s.apache.org/spark-1.5.2

http://spark.apache.org/releases/spark-release-1-5-2.html
"
Michael Armbrust <michael@databricks.com>,"Tue, 10 Nov 2015 11:22:47 -0800",Re: Support for views/ virtual tables in SparkSQL,Zhan Zhang <zzhang@hortonworks.com>,"We do support hive style views, though all tables have to be visible to
Hive.  You can also turn on the experimental native view support (but it
does not canonicalize the query).

set spark.sql.nativeView = true



"
Sudhir Menon <smenon@pivotal.io>,"Tue, 10 Nov 2015 13:51:29 -0800",Re: Support for views/ virtual tables in SparkSQL,Michael Armbrust <michael@databricks.com>,"Thanks Zhan, thanks Michael. I was already going down the temp table path,
will check out the experimental native view support

Suds


"
Rad Gruchalski <radek@gruchalski.com>,"Tue, 10 Nov 2015 23:52:56 +0100","SPARK-11638: Run Spark on Mesos, in Docker with Bridge
 networking",dev@spark.apache.org,"Dear Team,  

We, Virdata, would like to present the result of the last few months of our work with Mesos and Spark. Our requirement was to run Spark on Mesos in Docker for multi-tenant.
This required adapting Spark to run in Docker with Bridge networking.

The result (and patches) of our work is presented in the following JIRA ticket: https://issues.apache.org/jira/browse/SPARK-11638. The PR is: https://github.com/apache/spark/pull/9608.

The Summary

Provides spark.driver.advertisedPort, spark.fileserver.advertisedPort, spark.broadcast.advertisedPort and spark.replClassServer.advertisedPort settings to enable running Spark in Mesos on Docker with Bridge networking. Provides patches for Akka Remote to enable Spark driver advertisement using alternative host and port.
With these settings, it is possible to run Spark Master in a Docker container and have the executors running on Mesos talk back correctly to such Master.

The problem is discussed on the Mesos mailing list here: https://mail-archives.apache.org/mod_mbox/mesos-user/201510.mbox/%3CCACTd3c9vjAMXk=bFOtj5LJZFRH5u7ix-ghppFqKnVg9mkKctjg@mail.gmail.com%3E

We would like to contribute this to Apache Spark.

Happy to provide any further information.










Kind regards, 
Radek Gruchalski
 radek@gruchalski.com (mailto:radek@gruchalski.com)  (mailto:radek@gruchalski.com)
de.linkedin.com/in/radgruchalski/ (http://de.linkedin.com/in/radgruchalski/)

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.


"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Nov 2015 15:10:55 -0800",A proposal for Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","I’m starting a new thread since the other one got intermixed with feature
requests. Please refrain from making feature request in this thread. Not
that we shouldn’t be adding features, but we can always add features in
1.7, 2.1, 2.2, ...

First - I want to propose a premise for how to think about Spark 2.0 and
major releases in Spark, based on discussion with several members of the
community: a major release should be low overhead and minimally disruptive
to the Spark community. A major release should not be very different from a
minor release and should not be gated based on new features. The main
purpose of a major release is an opportunity to fix things that are broken
in the current API and remove certain deprecated APIs (examples follow).

For this reason, I would *not* propose doing major releases to break
substantial API's or perform large re-architecting that prevent users from
upgrading. Spark has always had a culture of evolving architecture
incrementally and making changes - and I don't think we want to change this
model. In fact, we’ve released many architectural changes on the 1.X line.

If the community likes the above model, then to me it seems reasonable to
do Spark 2.0 either after Spark 1.6 (in lieu of Spark 1.7) or immediately
after Spark 1.7. It will be 18 or 21 months since Spark 1.0. A cadence of
major releases every 2 years seems doable within the above model.

Under this model, here is a list of example things I would propose doing in
Spark 2.0, separated into APIs and Operation/Deployment:


APIs

1. Remove interfaces, configs, and modules (e.g. Bagel) deprecated in Spark
1.x.

2. Remove Akka from Spark’s API dependency (in streaming), so user
applications can use Akka (SPARK-5293). We have gotten a lot of complaints
about user applications being unable to use Akka due to Spark’s dependency
on Akka.

3. Remove Guava from Spark’s public API (JavaRDD Optional).

4. Better class package structure for low level developer API’s. In
particular, we have some DeveloperApi (mostly various listener-related
classes) added over the years. Some packages include only one or two public
classes but a lot of private classes. A better structure is to have public
classes isolated to a few public packages, and these public packages should
have minimal private classes for low level developer APIs.

5. Consolidate task metric and accumulator API. Although having some subtle
differences, these two are very similar but have completely different code
path.

6. Possibly making Catalyst, Dataset, and DataFrame more general by moving
them to other package(s). They are already used beyond SQL, e.g. in ML
pipelines, and will be used by streaming also.


Operation/Deployment

1. Scala 2.11 as the default build. We should still support Scala 2.10, but
it has been end-of-life.

2. Remove Hadoop 1 support.

3. Assembly-free distribution of Spark: don’t require building an enormous
assembly jar in order to run Spark.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 10 Nov 2015 23:35:06 +0000",Re: A proposal for Spark 2.0,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","substantial API's or perform large re-architecting that prevent users from
upgrading. Spark has always had a culture of evolving architecture
incrementally and making changes - and I don't think we want to change this
model.

+1 for this. The Python community went through a lot of turmoil over the
Python 2 -> Python 3 transition because the upgrade process was too painful
for too long. The Spark community will benefit greatly from our explicitly
looking to avoid a similar situation.

enormous assembly jar in order to run Spark.

Could you elaborate a bit on this? I'm not sure what an assembly-free
distribution means.

Nick


 feature
res in
e
 a
n
m
is
1.X line.
r
s
ependency
In
ic
c
ld
g
 enormous
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 10 Nov 2015 15:41:55 -0800",Re: A proposal for Spark 2.0,Nicholas Chammas <nicholas.chammas@gmail.com>,"I also feel the same as Reynold. I agree we should minimize API breaks and
focus on fixing things around the edge that were mistakes (e.g. exposing
Guava and Akka) rather than any overhaul that could fragment the community.
Ideally a major release is a lightweight process we can do every couple of
years, with minimal impact for users.

- Patrick


m
is
ul
y
an
h feature
ures in
ve
m a
en
om
his
 1.X line.
o
y
f
er
ts
dependency
 In
lic
ic
uld
in
n
"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Nov 2015 15:53:29 -0800",Re: A proposal for Spark 2.0,Nicholas Chammas <nicholas.chammas@gmail.com>,"
an
Right now we ship Spark using a single assembly jar, which causes a few
different problems:

- total number of classes are limited on some configurations

- dependency swapping is harder


The proposal is to just avoid a single fat jar.
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 10 Nov 2015 15:56:41 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"There's a proposal / discussion of the assembly-less distributions at
https://github.com/vanzin/spark/pull/2/files /
https://issues.apache.org/jira/browse/SPARK-11157.


 an
"
Kostas Sakellis <kostas@cloudera.com>,"Tue, 10 Nov 2015 16:02:04 -0800",Re: A proposal for Spark 2.0,Patrick Wendell <pwendell@gmail.com>,"+1 on a lightweight 2.0

What is the thinking around the 1.x line after Spark 2.0 is released? If
not terminated, how will we determine what goes into each major version
line? Will 1.x only be for stability fixes?

Thanks,
Kostas

:

d
y.
f
om
his
ful
ly
"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 10 Nov 2015 16:22:35 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"Would be also good to fix api breakages introduced as part of 1.0
(where there is missing functionality now), overhaul & remove all
deprecated config/features/combinations, api changes that we need to
make to public api which has been deferred for minor releases.

Regards,
Mridul

 feature
res in 1.7,
e
 a
n
m
is
1.X line.
 do
ter
in
rk
r
s
ependency
In
ic
c
ld
le
e
g
ut
 enormous

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 10 Nov 2015 16:23:49 -0800",Re: A proposal for Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"+1

stay on the current release schedule and don't unnecessarily delay 2.0
to wait for new features / big architectural changes.

In terms of fixes to 1.x, I think our current policy of back-porting
fixes to older releases would still apply. I don't think"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Nov 2015 16:28:11 -0800",Re: A proposal for Spark 2.0,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Echoing Shivaram here. I don't think it makes a lot of sense to add more
features to the 1.x line. We should still do critical bug fixes though.



f
e?
ng
e
k
ing an
 with
add
n
s
 the 1.X
le
n
o user
s
s. In
ed
ng an
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 10 Nov 2015 16:53:42 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"Another +1 to Reynold's proposal.

Maybe this is obvious, but I'd like to advocate against a blanket removal
of deprecated / developer APIs.  Many APIs can likely be removed without
material impact (e.g. the SparkContext constructor that takes preferred
node location data), while others likely see heavier usage (e.g. I wouldn't
be surprised if mapPartitionsWithContext was baked into a number of apps)
and merit a little extra consideration.

Maybe also obvious, but I think a migration guide with API equivlents and
the like would be incredibly useful in easing the transition.

-Sandy


s
ak
ding an
e
d with
s
 add
0
f
t
in
k
n the 1.X
in
so user
s
s. In
o
s
e
ing an
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 10 Nov 2015 16:54:33 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"Oh and another question - should Spark 2.0 support Java 7?

:

't
e
r
lding an
ee
ed with
s add
e
ak
on the
 so user
s
.
s. In
e
me
y
ding an
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 10 Nov 2015 16:59:15 -0800",Re: A proposal for Spark 2.0,Sandy Ryza <sandy.ryza@cloudera.com>,"Really, Sandy?  ""Extra consideration"" even for already-deprecated API?  If
we're not going to remove these with a major version change, then just when
will we remove them?

:

't
e
r
lding an
ee
ed with
s add
e
ak
on the
 so user
s
.
s. In
e
me
y
ding an
"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Nov 2015 17:03:33 -0800",Re: A proposal for Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"Maybe a better idea is to un-deprecate an API if it is too important to not
be removed.

I don't think we can drop Java 7 support. It's way too soon.




f
en
l
n't
)
d
:
e
?
re
o
ilding an
xed with
ys add
e
 on the
e
d
, so user
s
).
s. In
y
by
,
lding an
"
Sasaki Kai <lewuathe@me.com>,"Wed, 11 Nov 2015 10:10:51 +0900",Re: Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?,Jeff Zhang <zjffdu@gmail.com>,"Did you indicate CsvRelation in spark-csv package? LibSVMRelation is included in spark core package, but CsvRelation(spark-csv) is not.
Is it necessary for us to modify also spark-csv as you proposed in SPARK-11622?

Regards

Kai 

extends HadoopFsRelation and leverage the features from HadoopFsRelation.  Any other consideration for that ?


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 10 Nov 2015 17:19:49 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"I'm liking the way this is shaping up, and I'd summarize it this way (let
me know if I'm misunderstanding or misrepresenting anything):

   - New features are not at all the focus of Spark 2.0 -- in fact, a
   release with no new features might even be best.
   - Remove deprecated API that we agree really should be deprecated.
   - Fix/change publicly-visible things that anyone who has spent any time
   looking at already knows are mistakes or should be done better, but that
   can't be changed within 1.x.

Do we want to attempt anticipatory changes at all?  In other words, are
there things we want to do in 2.x for which we already know that we'll want
to make publicly-visible changes or that, if we don't add or change it now,
will fall into the ""everybody knows it shouldn't be that way"" category when
it comes time to discuss the Spark 3.0 release?  I'd be fine if we don't
try at all to anticipate what is needed -- working from the premise that
being forced into a 3.x release earlier than we expect would be less
painful than trying to back out a mistake made at the outset of 2.0 while
trying to guess what we'll need.


 feature
res in
e
 a
n
m
is
1.X line.
r
s
ependency
In
ic
c
ld
g
 enormous
"
Fengdong Yu <fengdongy@everstring.com>,"Wed, 11 Nov 2015 10:12:53 +0800",Re: [ANNOUNCE] Announcing Spark 1.5.2,Reynold Xin <rxin@databricks.com>,"This is the most simplest announcement I saw.



release is based on the branch-1.5 maintenance branch of Spark. We *strongly recommend* all 1.5.x users to upgrade to this release.
<http://s.apache.org/spark-1.5.2>
<http://spark.apache.org/releases/spark-release-1-5-2.html>

"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 11 Nov 2015 10:41:40 +0800",Re: Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?,Sasaki Kai <lewuathe@me.com>,"Yes Kai, I also to plan to do for CsvRelation, will create PR for spark-csv




-- 
Best Regards

Jeff Zhang
"
Sudhir Menon <smenon@pivotal.io>,"Tue, 10 Nov 2015 18:48:41 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"Agree. If it is deprecated, get rid of it in 2.0
If the deprecation was a mistake, let's fix that.

Suds
Sent from my iPhone


Maybe a better idea is to un-deprecate an API if it is too important to not
be removed.

I don't think we can drop Java 7 support. It's way too soon.




f
en
l
n't
)
d
:
e
?
re
o
ilding an
xed with
ys add
e
 on the
e
d
, so user
s
).
s. In
y
by
,
lding an
"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Nov 2015 18:51:38 -0800",Re: A proposal for Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"Mark,

I think we are in agreement, although I wouldn't go to the extreme and say
""a release with no new features might even be best.""

Can you elaborate ""anticipatory changes""? A concrete example or so would be
helpful.


ut
nt
w,
en
h feature
ures in
ve
m a
en
om
his
 1.X line.
o
y
f
er
ts
dependency
 In
lic
ic
uld
in
n
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 10 Nov 2015 18:57:27 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"
I don't know if that's what Mark had in mind, but I'd count the
""remove Guava Optional from Java API"" in that category. It would be
nice to have an alternative before that API is removed, although I
have no idea how you'd do it nicely, given that they're all in return
types (so overloading doesn't really work).

---------------------------------------------------------------------


"
Sasaki Kai <lewuathe@me.com>,"Wed, 11 Nov 2015 11:59:51 +0900",Re: Why LibSVMRelation and CsvRelation don't extends HadoopFsRelation ?,Jeff Zhang <zjffdu@gmail.com>,"Great, thank you!

spark-csv
included in spark core package, but CsvRelation(spark-csv) is not.
SPARK-11622?
extends HadoopFsRelation and leverage the features from HadoopFsRelation.  Any other consideration for that ?

"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 10 Nov 2015 19:04:45 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"Heh... ok, I was intentionally pushing those bullet points to be extreme to
find where people would start pushing back, and I'll agree that we do
probably want some new features in 2.0 -- but I think we've got good
agreement that new features aren't really the main point of doing a 2.0
release.

I don't really have a concrete example of an anticipatory change, and
that's actually kind of the problem with trying to anticipate what we'll
need in the way of new public API and the like: Until what we already have
is clearly inadequate, it hard to concretely imagine how things really
should be.  At this point I don't have anything specific where I can say ""I
really want to do __ with Spark in the future, and I think it should be
changed in this way in 2.0 to allow me to do that.""  I'm just wondering
whether we want to even entertain those kinds of change requests if people
have them, or whether we can just delay making those kinds of decisions
until it is really obvious that what we have does't work and that there is
clearly something better that should be done.


y
t
but
ant
ow,
hen
e
:
th
ways add
d
e
ive
om a
ken
.
rom
this
e 1.X line.
. A
odel.
g
ser
nts
 dependency
. In
blic
lic
ould
 in
an
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 10 Nov 2015 19:26:43 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"To take a stab at an example of something concrete and anticipatory I can
go back to something I mentioned previously.  It's not really a good
example because I don't mean to imply that I believe that its premises are
true, but try to go with it.... If we were to decide that real-time,
event-based streaming is something that we really think we'll want to do in
the 2.x cycle and that the current API (after having deprecations removed
and clear mistakes/inadequacies remedied) isn't adequate to support that,
would we want to ""take our best shot"" at defining a new API at the outset
of 2.0?  Another way of looking at it is whether API changes in 2.0 should
be entirely backward-looking, trying to fix problems that we've already
identified or whether there is room for some forward-looking changes that
are intended to open new directions for Spark development.


e
""I
e
s
 but
want
now,
when
t
t
le
ith
lways add
of
d
from
 this
he 1.X line.
0. A
model.
user
ints
s dependency
s. In
ublic
blic
hould
. in
,
 an
"
,"Wed, 11 Nov 2015 05:41:04 +0100",Re: A proposal for Spark 2.0,dev@spark.apache.org,"Agree, it makes sense.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
,"Wed, 11 Nov 2015 05:45:53 +0100",Re: A proposal for Spark 2.0,dev@spark.apache.org,"Hi,

I fully agree that. Actually, I'm working on PR to add ""client"" and 
""exploded"" profiles in Maven build.

The client profile create a spark-client-assembly jar, largely more 
lightweight that the spark-assembly. In our case, we construct jobs that 
don't require all the spark server side. It means that the minimal size 
of the generated jar is about 120MB, and it's painful in spark-submit 
submission time. That's why I started to remove unecessary dependencies 
in spark-assembly.

using a fat monolithic spark-assembly jar file, I'm working on a 
exploded mode, allowing users to view/change the dependencies.

For the client profile, I've already something ready, I will propose the 
PR very soon (by the end of this week hopefully). For the exploded 
profile, I need more time.

My $0.02

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 11 Nov 2015 17:20:48 +0800","Why there's no api for SparkContext#textFiles to support multiple
 inputs ?","""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Although user can use the hdfs glob syntax to support multiple inputs. But
sometimes, it is not convenient to do that. Not sure why there's no api
of SparkContext#textFiles. It should be easy to implement that. I'd love to
create a ticket and contribute for that if there's no other consideration
that I don't know.

-- 
Best Regards

Jeff Zhang
"
Sean Owen <sowen@cloudera.com>,"Wed, 11 Nov 2015 10:58:56 +0100",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"
Agree with this stance. Generally, a major release might also be a
time to replace some big old API or implementation with a new one, but
I don't see obvious candidates.

I wouldn't mind turning attention to 2.x sooner than later, unless
there's a fairly good reason to continue adding features in 1.x to a
1.7 release. The scope as of 1.6 is already pretty darned big.



By the time 2.x rolls around, 2.12 will be the main version, 2.11 will
be quite stable, and 2.10 will have been EOL for a while. I'd propose
dropping 2.10. Otherwise it's supported for 2 more years.



I'd go further to drop support for <2.2 for sure (2.0 and 2.1 were
sort of 'alpha' and 'beta' releases) and even <2.6.

I'm sure we'll think of a number of other small things -- shading a
bunch of stuff? reviewing and updating dependencies in light of
simpler, more recent dependencies to support from Hadoop etc?

Farming out Tachyon to a module? (I felt like someone proposed this?)
Pop out any Docker stuff to another repo?
Continue that same effort for EC2?
Farming out some of the ""external"" integrations to another repo (?
controversial)

See also anything marked version ""2+"" in JIRA.

---------------------------------------------------------------------


"
Tim Preece <tepreece@mail.com>,"Wed, 11 Nov 2015 04:48:05 -0700 (MST)",Re: A proposal for Spark 2.0,dev@spark.apache.org,"Considering Spark 2.x will run for 2 years, would moving up to Scala 2.12 (
pencilled in for Jan 2016 ) make any sense ? - although that would then
pre-req Java 8.



--

---------------------------------------------------------------------


"
gsvic <victorasgs@gmail.com>,"Wed, 11 Nov 2015 05:35:07 -0700 (MST)",Map Tasks - Disk I/O,dev@spark.apache.org,"According to  this paper
<http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf>  
Spak's map tasks writes the results to disk. 

My actual question is, in  BroadcastHashJoin
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoin.scala#L100>  
doExecute() method at line  109 the mapPartitions
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoin.scala#L109>  
method is called. At this step, Spark will schedule a number of tasks for
execution in order to perform the hash join operation. The results of these
tasks will be written to each worker's disk?



--

---------------------------------------------------------------------


"
Cristian O <cristian.b.opris@googlemail.com>,"Wed, 11 Nov 2015 12:59:35 +0000",Support for local disk columnar storage for DataFrames,dev@spark.apache.org,"Hi,

I was wondering if there's any planned support for local disk columnar
storage.

This could be an extension of the in-memory columnar store, or possibly
something similar to the recently added local checkpointing for RDDs

This could also have the added benefit of enabling iterative usage for
DataFrames by pruning the query plan through local checkpoints.

A further enhancement would be to add update support to the columnar format
(in the immutable copy-on-write sense of course), by maintaining references
to unchanged row blocks and only copying and mutating the ones that have
changed.

A use case here is streaming and merging updates in a large dataset that
can be efficiently stored internally in a columnar format, rather than
accessing a more inefficient external  data store like HDFS or Cassandra.

Thanks,
Cristian
"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Wed, 11 Nov 2015 17:19:19 +0000",Re: A proposal for Spark 2.0,"Tim Preece <tepreece@mail.com>, dev@spark.apache.org","Hi,

Reconsidering the execution model behind Streaming would be a good
candidate here, as Spark will not be able to provide the low latency and
sophisticated windowing semantics that more and more use-cases will
require. Maybe relaxing the strict batch model would help a lot. (Mainly
this would hit the shuffling, but the shuffle package suffers from
overlapping functionalities, lack of good modularity anyway. Look at how
coalesce implemented for example - inefficiency also kicks in there.)


"
Koert Kuipers <koert@tresata.com>,"Wed, 11 Nov 2015 12:29:18 -0500",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"i would drop scala 2.10, but definitely keep java 7

cross build for scala 2.12 is great, but i dont know how that works with
java 8 requirement. dont want to make java 8 mandatory.

and probably stating the obvious, but a lot of apis got polluted due to
binary compatibility requirement. cleaning that up assuming only source
compatibility would be a good idea, right?


 feature
res in
e
 a
n
m
is
1.X line.
r
s
ependency
In
ic
c
ld
g
 enormous
"
Koert Kuipers <koert@tresata.com>,"Wed, 11 Nov 2015 12:33:42 -0500",Re: A proposal for Spark 2.0,Sean Owen <sowen@cloudera.com>,"good point about dropping <2.2 for hadoop. you dont want to deal with
protobuf 2.4 for example



"
Jonathan Kelly <jonathakamzn@gmail.com>,"Wed, 11 Nov 2015 10:03:23 -0800",Re: A proposal for Spark 2.0,Koert Kuipers <koert@tresata.com>,"If Scala 2.12 will require Java 8 and we want to enable cross-compiling
Spark against Scala 2.11 and 2.12, couldn't we just make Java 8 a
requirement if you want to use Scala 2.12?


h feature
ures in
ve
m a
en
om
his
 1.X line.
o
y
f
er
ts
dependency
 In
lic
ic
uld
in
n
"
Jakob Odersky <jodersky@gmail.com>,"Wed, 11 Nov 2015 10:21:07 -0800","Re: Why there's no api for SparkContext#textFiles to support multiple
 inputs ?",Jeff Zhang <zjffdu@gmail.com>,"Hey Jeff,
Do you mean reading from multiple text files? In that case, as a
workaround, you can use the RDD#union() (or ++) method to concatenate
multiple rdds. For example:

val lines1 = sc.textFile(""file1"")
val lines2 = sc.textFile(""file2"")

val rdd = lines1 union lines2

regards,
--Jakob


"
Shixiong Zhu <zsxwing@gmail.com>,"Wed, 11 Nov 2015 10:22:38 -0800","Re: Why there's no api for SparkContext#textFiles to support multiple
 inputs ?",Jakob Odersky <jodersky@gmail.com>,"In addition, if you have more than two text files, you can just put them
into a Seq and use ""reduce(_ ++ _)"".

Best Regards,
Shixiong Zhu

2015-11-11 10:21 GMT-08:00 Jakob Odersky <jodersky@gmail.com>:

"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 11 Nov 2015 10:27:51 -0800","Re: Why there's no api for SparkContext#textFiles to support multiple
 inputs ?",Jakob Odersky <jodersky@gmail.com>,"For more than a small number of files, you'd be better off using
SparkContext#union instead of RDD#union.  That will avoid building up a
lengthy lineage.


"
hitoshi <ozawa_h@worksap.co.jp>,"Wed, 11 Nov 2015 13:10:06 -0700 (MST)",Re: A proposal for Spark 2.0,dev@spark.apache.org,"It looks like Chill is willing to upgrade their Kryo to 3.x if Spark and Hive
will. As it is now Spark, Chill, and Hive have Kryo jar but it really can't
be used because Kryo 2 can't serdes some classes. Since Spark 2.0 is a major
release, it really would be nice if we can resolve the Kryo issue.
 
https://github.com/twitter/chill/pull/230#issuecomment-155845959



--

---------------------------------------------------------------------


"
hitoshi <ozawa_h@worksap.co.jp>,"Wed, 11 Nov 2015 13:37:00 -0700 (MST)",Re: A proposal for Spark 2.0,dev@spark.apache.org,"Resending my earlier message because it wasn't accepted.

Would like to add a proposal to upgrade jars when they do not break APIs and
fixes a bug. 
To be more specific, I would like to see Kryo to be upgraded from 2.21 to
3.x. Kryo 2.x has a bug (e.g.SPARK-7708) that is blocking it usage in
production environment. 
Other projects like Chill is also wanting to upgrade Kryo to 3.x but being
blocked because Spark won't upgrade. I think OSS community at large will
benefit if we can coordinate to upgrade to Kryo 3.x 



--

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 11 Nov 2015 16:32:22 -0500",Re: A proposal for Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","I like the idea of popping out Tachyon to an optional component too to reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.

Regarding Scala 2.12, we should definitely support it eventually, but I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.

Needless to say, but I'm all for the idea of making ""major"" releases as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.

Matei

from a
broken
follow).
2.10, but


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Wed, 11 Nov 2015 22:01:40 +0000",Choreographing a Kryo update,Spark dev list <dev@spark.apache.org>,"

Spark is currently on a fairly dated version of Kryo 2.x; it's trailing on the fixes in Hive and, as the APIs are incompatible, resulted in that mutant spark-project/hive JAR needed for the Hive 1.2.1 support

But: updating it hasn't been an option, because Spark needs to be in sync with Twitter's Chill library.

There's now an offer from Twitter to help coordinate a kryo update across Chill, Scalding and other things they use

https://github.com/twitter/chill/pull/230

Given kryo is ""The guava jar of serialization"", I doubt anyone is jumping uo it, all the hive spark integration is probably going to break again; getting in sync with hive (see SPARK-10793) would reduce the traumaticness of hive updates
"
Zhan Zhang <zzhang@hortonworks.com>,"Wed, 11 Nov 2015 22:45:30 +0000",Proposal for SQL join optimization,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Folks,

I did some performance measurement based on TPC-H recently, and want to bring up some performance issue I observed. Both are related to cartesian join.

1. CartesianProduct implementation.

Currently CartesianProduct relies on RDD.cartesian, in which the computation is realized as follows

  override def compute(split: Partition, context: TaskContext): Iterator[(T, U)] = {
    val currSplit = split.asInstanceOf[CartesianPartition]
    for (x <- rdd1.iterator(currSplit.s1, context);
         y <- rdd2.iterator(currSplit.s2, context)) yield (x, y)
  }

s. Which is really heavy and may never finished if n is large, especially when rdd2 is coming from ShuffleRDD.

We should have some optimization on CartesianProduct by caching rightResults. The problem is that we dont have cleanup hook to unpersist rightResults AFAIK. I think we should have some cleanup hook after query execution.
With the hook available, we can easily optimize such Cartesian join. I believe such cleanup hook may also benefit other query optimizations.


2. Unnecessary CartesianProduct join.

When we have some queries similar to following (dont remember the exact form):
select * from a, b, c, d where a.key1 = c.key1 and b.key2 = c.key2 and c.key3 = d.key3

There will be a cartesian join between a and b. But if we just simply change the table order, for example from a, c, b, d, such cartesian join are eliminated.
Without such manual tuning, the query will never finish if a, c are big. But we should not relies on such manual optimization.


Please provide your inputs. If they are both valid, I will open liras for each.

Thanks.

Zhan Zhang

---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Thu, 12 Nov 2015 08:30:06 +0800","Re: Why there's no api for SparkContext#textFiles to support multiple
 inputs ?",Mark Hamstra <mark@clearstorydata.com>,"I know these workaround, but wouldn't it be more convenient and
straightforward to use SparkContext#textFiles ?




-- 
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Thu, 12 Nov 2015 08:49:41 +0800","Re: Why there's no api for SparkContext#textFiles to support multiple
 inputs ?",Pradeep Gollakota <pradeepg26@gmail.com>,"Yes, that's what I suggest. TextInputFormat support multiple inputs. So in
spark side, we just need to provide API to for that.




-- 
Best Regards

Jeff Zhang
"
Xiao Li <gatorsmile@gmail.com>,"Wed, 11 Nov 2015 18:16:33 -0800",Re: Proposal for SQL join optimization,Zhan Zhang <zzhang@hortonworks.com>,"Hi, Zhan,

That sounds really interesting! Please at me when you submit the PR. If
possible, please also posted the performance difference.

Thanks,

Xiao Li


2015-11-11 14:45 GMT-08:00 Zhan Zhang <zzhang@hortonworks.com>:

npersist
e exact
d
"
Jeff Zhang <zjffdu@gmail.com>,"Thu, 12 Nov 2015 10:24:00 +0800","Re: Why there's no api for SparkContext#textFiles to support multiple
 inputs ?",Pradeep Gollakota <pradeepg26@gmail.com>,"Hi Pradeep

≥≥≥ Looks like what I was suggesting doesn't work. :/
I guess you mean put comma separated path into one string and pass it
to existing API (SparkContext#textFile). It should not work. I suggest to
create new api SparkContext#textFiles to accept an array of string. I have
already implemented a simple patch and it works.





 a
e
y
ement
s no


-- 
Best Regards

Jeff Zhang
"
Reynold Xin <rxin@databricks.com>,"Wed, 11 Nov 2015 19:30:59 -0800",Re: Choreographing a Kryo update,Steve Loughran <stevel@hortonworks.com>,"We should consider this for Spark 2.0.



"
Reynold Xin <rxin@databricks.com>,"Wed, 11 Nov 2015 19:31:33 -0800",Re: Support for local disk columnar storage for DataFrames,Cristian O <cristian.b.opris@googlemail.com>,"Thanks for the email. Can you explain what the difference is between this
and existing formats such as Parquet/ORC?



"
Andrew Duffy <andreweduffy@gmail.com>,"Thu, 12 Nov 2015 00:05:11 -0800",Re: Support for local disk columnar storage for DataFrames,Reynold Xin <rxin@databricks.com>,"Relevant link:
http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files


"
witgo@qq.com,"Thu, 12 Nov 2015 22:49:12 +0800",Re: A proposal for Spark 2.0,dev@spark.apache.org,"Who has the idea of machine learning? Spark missing some features for machine learning, For example, the parameter server.


<matei.zaharia@gmail.com> д
reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.
I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.
as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.
from a
main
broken
follow).
but
2.10, but
will




---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 12 Nov 2015 10:27:42 -0500",Re: A proposal for Spark 2.0,witgo@qq.com,"Being specific to Parameter Server, I think the current agreement is that PS shall exist as a third-party library instead of a component of the core code base, isn’t?

Best,  

--  
Nan Zhu
http://codingcat.me



 machine learning, For example, the parameter server.
atei Zaharia <matei.zaharia@gmail.com (mailto:matei.zaharia@gmail.com)> 写道：
o reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.
 I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.
ses as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.
ent from a
main
re broken
ollow).
but
a
 2.10, but
ill
se

this?)
po (?
--
dev-unsubscribe@spark.apache.org)
lto:dev-help@spark.apache.org)

v-unsubscribe@spark.apache.org)
o:dev-help@spark.apache.org)
unsubscribe@spark.apache.org)
dev-help@spark.apache.org)


"
Cristian O <cristian.b.opris@googlemail.com>,"Thu, 12 Nov 2015 16:57:55 +0000",Re: Support for local disk columnar storage for DataFrames,"Reynold Xin <rxin@databricks.com>, dev@spark.apache.org","Sorry, apparently only replied to Reynold, meant to copy the list as well,
so I'm self replying and taking the opportunity to illustrate with an
example.

Basically I want to conceptually do this:

val bigDf = sqlContext.sparkContext.parallelize((1 to 1000000)).map(i
=> (i, 1)).toDF(""k"", ""v"")
val deltaDf = sqlContext.sparkContext.parallelize(Array(1,
50000)).map(i => (i, 1)).toDF(""k"", ""v"")

bigDf.cache()

bigDf.registerTempTable(""big"")
deltaDf.registerTempTable(""delta"")

val newBigDf = sqlContext.sql(""SELECT big.k, big.v + IF(delta.v is
null, 0, delta.v) FROM big LEFT JOIN delta on big.k = delta.k"")

newBigDf.cache()
bigDf.unpersist()


This is essentially an update of keys ""1"" and ""50000"" only, in a dataset of
1 million keys.

This can be achieved efficiently if the join would preserve the cached
blocks that have been unaffected, and only copy and mutate the 2 affected
blocks corresponding to the matching join keys.

Statistics can determine which blocks actually need mutating. Note also
that shuffling is not required assuming both dataframes are pre-partitioned
by the same key K.

In SQL this could actually be expressed as an UPDATE statement or for a
more generalized use as a MERGE UPDATE:
https://technet.microsoft.com/en-us/library/bb522522(v=sql.105).aspx

While this may seem like a very special case optimization, it would
effectively implement UPDATE support for cached DataFrames, for both
optimal and non-optimal usage.

I appreciate there's quite a lot here, so thank you for taking the time to
consider it.

Cristian




"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 12 Nov 2015 17:44:43 +0000",RE: A proposal for Spark 2.0,"Nan Zhu <zhunanmcgill@gmail.com>, ""witgo@qq.com"" <witgo@qq.com>","Parameter Server is a new feature and thus does not match the goal of 2.0 is “to fix things that are broken in the current API and remove certain deprecated APIs”. At the same time I would be happy to have that feature.

With regards to Machine learning, it would be great to move useful features from MLlib to ML and deprecate the former. Current structure of two separate machine learning packages seems to be somewhat confusing.
With regards to GraphX, it would be great to deprecate the use of RDD in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.

Best regards, Alexander

From: Nan Zhu [mailto:zhunanmcgill@gmail.com]
Sent: Thursday, November 12, 2015 7:28 AM
To: witgo@qq.com
Cc: dev@spark.apache.org
Subject: Re: A proposal for Spark 2.0

Being specific to Parameter Server, I think the current agreement is that PS shall exist as a third-party library instead of a component of the core code base, isn’t?

Best,

--
Nan Zhu
http://codingcat.me


On Thursday, November 12, 2015 at 9:49 AM, witgo@qq.com<mailto:witgo@qq.com> wrote:
Who has the idea of machine learning? Spark missing some features for machine learning, For example, the parameter server.


在 2015年11月12日，05:32，Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.com>> 写道：

I like the idea of popping out Tachyon to an optional component too to reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.

Regarding Scala 2.12, we should definitely support it eventually, but I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.

Needless to say, but I'm all for the idea of making ""major"" releases as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.

Matei

On Nov 11, 2015, at 4:58 AM, Sean Owen
On Wed, Nov 11, 2015 at 12:10 AM, Reynold Xin <rxin@datpark community. A major release should not be very different from a
minor release and should not be gated based on new features. The main
purpose of a major release is an opportunity to fix things that are broken
in the current API and remove certain deprecated APIs (examples follow).

Agree with this stance. Generally, a major release might also be a
time to replace some big old API or implementation with a new one, but
I don't see obvious candidates.

I wouldn't mind turning attention to 2.x sooner than later, unless
there's a fairly good reason to continue adding features in 1.x to a
1.7 release. The scope as of 1.6 is already pretty darned big.


1. Scala 2.11 as the default build. We should still support Scala 2.10, but
it has been end-of-life.

By the time 2.x rolls around, 2.12 will be the main version, 2.11 will
be quite stable, and 2.10 will have been EOL for a while. I'd propose
dropping 2.10. Otherwise it's supported for 2 more years.


2. Remove Hadoop 1 support.

I'd go further to drop support for <2.2 for sure (2.0 and 2.1 were
sort of 'alpha' and 'beta' releases) and even <2.6.

I'm sure we'll think of a number of other small things -- shading a
bunch of stuff? reviewing and updating dependencies in light of
simpler, more recent dependencies to support from Hadoop etc?

Farming out Tachyon to a module? (I felt like someone proposed this?)
Pop out any Docker stuff to another repo?
Continue that same effort for EC2?
Farming out some of the ""external"" integrations to another repo (?
controversial)

See also anything marked version ""2+"" in JIRA.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>




---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 12 Nov 2015 17:56:12 +0000",Re: A proposal for Spark 2.0,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>, Nan Zhu <zhunanmcgill@gmail.com>, 
	""witgo@qq.com"" <witgo@qq.com>","With regards to Machine learning, it would be great to move useful features
from MLlib to ML and deprecate the former. Current structure of two
separate machine learning packages seems to be somewhat confusing.

With regards to GraphX, it would be great to deprecate the use of RDD in
GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.

things in 2.0 without removing or replacing them immediately. That way 2.0
doesn’t have to wait for everything that we want to deprecate to be
replaced all at once.

Nick
​

m>

ertain
feature.
en.
e
 Zaharia <matei.zaharia@gmail.com> 写道：
o.
:
 a
n
ut
"
Zhan Zhang <zzhang@hortonworks.com>,"Thu, 12 Nov 2015 18:26:26 +0000",Re: Proposal for SQL join optimization,Xiao Li <gatorsmile@gmail.com>,"Hi Xiao,

Performance-wise, without the manual tuning, the query cannot be finished, and with the tuning the query can finish in minutes in TPCH 100G data.

I have created https://issues.apache.org/jira/browse/SPARK-11704 and https://issues.apache.org/jira/browse/SPARK-11705 for these two issues, and we can move the discussion there.

Thanks.

Zhan Zhang


Hi, Zhan,

That sounds really interesting! Please at me when you submit the PR. If possible, please also posted the performance difference.

Thanks,

Xiao Li


2015-11-11 14:45 GMT-08:00 Zhan Zhang <zzhang@hortonworks.com<mailto:zzhang@hortonworks.com>>:
Hi Folks,

I did some performance measurement based on TPC-H recently, and want to bring up some performance issue I observed. Both are related to cartesian join.

1. CartesianProduct implementation.

Currently CartesianProduct relies on RDD.cartesian, in which the computation is realized as follows

  override def compute(split: Partition, context: TaskContext): Iterator[(T, U)] = {
    val currSplit = split.asInstanceOf[CartesianPartition]
    for (x <- rdd1.iterator(currSplit.s1, context);
         y <- rdd2.iterator(currSplit.s2, context)) yield (x, y)
  }

s. Which is really heavy and may never finished if n is large, especially when rdd2 is coming from ShuffleRDD.

We should have some optimization on CartesianProduct by caching rightResults. The problem is that we dont have cleanup hook to unpersist rightResults AFAIK. I think we should have some cleanup hook after query execution.
With the hook available, we can easily optimize such Cartesian join. I believe such cleanup hook may also benefit other query optimizations.


2. Unnecessary CartesianProduct join.

When we have some queries similar to following (dont remember the exact form):
select * from a, b, c, d where a.key1 = c.key1 and b.key2 = c.key2 and c.key3 = d.key3

There will be a cartesian join between a and b. But if we just simply change the table order, for example from a, c, b, d, such cartesian join are eliminated.
Without such manual tuning, the query will never finish if a, c are big. But we should not relies on such manual optimization.


Please provide your inputs. If they are both valid, I will open liras for each.

Thanks.

Zhan Zhang

---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>



"
shane knapp <sknapp@berkeley.edu>,"Thu, 12 Nov 2015 12:14:48 -0800","[build system] short jenkins downtime tomorrow morning, 11-13-2015 @
 7am PST","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","i will admit that it does seem like a bad idea to poke jenkins on
friday the 13th, but there's a release that fixes a lot of security
issues:

https://wiki.jenkins-ci.org/display/SECURITY/Jenkins+Security+Advisory+2015-11-11

i'll set jenkins to stop kicking off any new builds around 5am PST,
and will upgrade and restart jenkins around 7am PST.  barring anything
horrible happening, we should be back up and building by 730am.

...and this time, i promise not to touch any of the plugins.  :)

shane

---------------------------------------------------------------------


"
Kostas Sakellis <kostas@cloudera.com>,"Thu, 12 Nov 2015 13:27:15 -0800",Re: A proposal for Spark 2.0,Nicholas Chammas <nicholas.chammas@gmail.com>,"I know we want to keep breaking changes to a minimum but I'm hoping that
with Spark 2.0 we can also look at better classpath isolation with user
programs. I propose we build on spark.{driver|executor}.userClassPathFirst,
setting it true by default, and not allow any spark transitive dependencies
to leak into user code. For backwards compatibility we can have a whitelist
if we want but I'd be good if we start requiring user apps to explicitly
pull in all their dependencies. From what I can tell, Hadoop 3 is also
moving in this direction.

Kostas


en.
0
be
0
certain
 feature.
ten.
t
re
i Zaharia <matei.zaharia@gmail.com> 写道：
l
oo.
m
en
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 13 Nov 2015 01:17:23 +0000",RE: A proposal for Spark 2.0,"Kostas Sakellis <kostas@cloudera.com>, Nicholas Chammas
	<nicholas.chammas@gmail.com>","I am not sure what the best practice for this specific problem, but it’s really worth to think about it in 2.0, as it is a painful issue for lots of users.

By the way, is it also an opportunity to deprecate the RDD API (or internal API only?)? As lots of its functionality overlapping with DataFrame or DataSet.

Hao

From: Kostas Sakellis [mailto:kostas@cloudera.com]
Sent: Friday, November 13, 2015 5:27 AM
To: Nicholas Chammas
Cc: Ulanov, Alexander; Nan Zhu; witgo@qq.com; dev@spark.apache.org; Reynold Xin
Subject: Re: A proposal for Spark 2.0

I know we want to keep breaking changes to a minimum but I'm hoping that with Spark 2.0 we can also look at better classpath isolation with user programs. I propose we build on spark.{driver|executor}.userClassPathFirst, setting it true by default, and not allow any spark transitive dependencies to leak into user code. For backwards compatibility we can have a whitelist if we want but I'd be good if we start requiring user apps to explicitly pull in all their dependencies. From what I can tell, Hadoop 3 is also moving in this direction.

Kostas

On Thu, Nov 12, 2015 at 9:56 AM, Nicholas Chammas <nicholas.chammas@gmail.com<mailto:nicholas.chammas@gmail.com>> wrote:

With regards to Machine learning, it would be great to move useful features from MLlib to ML and deprecate the former. Current structure of two separate machine learning packages seems to be somewhat confusing.

With regards to GraphX, it would be great to deprecate the use of RDD in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.

On that note of deprecating stuff, it might be good to deprecate some things in 2.0 without removing or replacing them immediately. That way 2.0 doesn’t have to wait for everything that we want to deprecate to be replaced all at once.

Nick
​

On Thu, Nov 12, 2015 at 12:45 PM Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Parameter Server is a new feature and thus does not match the goal of 2.0 is “to fix things that are broken in the current API and remove certain deprecated APIs”. At the same time I would be happy to have that feature.

With regards to Machine learning, it would be great to move useful features from MLlib to ML and deprecate the former. Current structure of two separate machine learning packages seems to be somewhat confusing.
With regards to GraphX, it would be great to deprecate the use of RDD in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.

Best regards, Alexander

From: Nan Zhu [mailto:zhunanmcgill@gmail.com<mailto:zhunanmcgill@gmail.com>]
Sent: Thursday, November 12, 2015 7:28 AM
To: witgo@qq.com<mailto:witgo@qq.com>
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: A proposal for Spark 2.0

Being specific to Parameter Server, I think the current agreement is that PS shall exist as a third-party library instead of a component of the core code base, isn’t?

Best,

--
Nan Zhu
http://codingcat.me


On Thursday, November 12, 2015 at 9:49 AM, witgo@qq.com<mailto:witgo@qq.com> wrote:
Who has the idea of machine learning? Spark missing some features for machine learning, For example, the parameter server.


在 2015年11月12日，05:32，Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.com>> 写道：

I like the idea of popping out Tachyon to an optional component too to reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.

Regarding Scala 2.12, we should definitely support it eventually, but I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.

Needless to say, but I'm all for the idea of making ""major"" releases as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.

Matei

On Nov 11, 2015, at 4:58 Am>> wrote:

On Wed, Nov 11, 2015 at 12:10 AM, Reynold Xe:
to the Spark community. A major release should not be very different from a
minor release and should not be gated based on new features. The main
purpose of a major release is an opportunity to fix things that are broken
in the current API and remove certain deprecated APIs (examples follow).

Agree with this stance. Generally, a major release might also be a
time to replace some big old API or implementation with a new one, but
I don't see obvious candidates.

I wouldn't mind turning attention to 2.x sooner than later, unless
there's a fairly good reason to continue adding features in 1.x to a
1.7 release. The scope as of 1.6 is already pretty darned big.


1. Scala 2.11 as the default build. We should still support Scala 2.10, but
it has been end-of-life.

By the time 2.x rolls around, 2.12 will be the main version, 2.11 will
be quite stable, and 2.10 will have been EOL for a while. I'd propose
dropping 2.10. Otherwise it's supported for 2 more years.


2. Remove Hadoop 1 support.

I'd go further to drop support for <2.2 for sure (2.0 and 2.1 were
sort of 'alpha' and 'beta' releases) and even <2.6.

I'm sure we'll think of a number of other small things -- shading a
bunch of stuff? reviewing and updating dependencies in light of
simpler, more recent dependencies to support from Hadoop etc?

Farming out Tachyon to a module? (I felt like someone proposed this?)
Pop out any Docker stuff to another repo?
Continue that same effort for EC2?
Farming out some of the ""external"" integrations to another repo (?
controversial)

See also anything marked version ""2+"" in JIRA.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>




---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>


"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 13 Nov 2015 09:28:28 +0800","Re: Why there's no api for SparkContext#textFiles to support multiple
 inputs ?",Pradeep Gollakota <pradeepg26@gmail.com>,"Didn't notice that I can pass comma separated path in the existing API
(SparkContext#textFile). So no necessary for new api. Thanks all.




. :/
e
m
m
p a
te
hy
lement
's no



-- 
Best Regards

Jeff Zhang
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 12 Nov 2015 17:42:32 -0800",Re: A proposal for Spark 2.0,"""Cheng, Hao"" <hao.cheng@intel.com>","The place of the RDD API in 2.0 is also something I've been wondering
about.  I think it may be going too far to deprecate it, but changing
emphasis is something that we might consider.  The RDD API came well before
DataFrames and DataSets, so programming guides, introductory how-to
articles and the like have, to this point, also tended to emphasize RDDs --
or at least to deal with them early.  What I'm thinking is that with 2.0
maybe we should overhaul all the documentation to de-emphasize and
reposition RDDs.  In this scheme, DataFrames and DataSets would be
introduced and fully addressed before RDDs.  They would be presented as the
normal/default/standard way to do things in Spark.  RDDs, in contrast,
would be presented later as a kind of lower-level, closer-to-the-metal API
that can be used in atypical, more specialized contexts where DataFrames or
DataSets don't fully fit.


s
of
t,
es
st
en.
0
be
ertain
feature.
en.
e
 Zaharia <matei.zaharia@gmail.com> 写道：
o.
:
 a
n
ut
"
Yin Huai <yhuai@databricks.com>,"Thu, 12 Nov 2015 18:21:54 -0800",Seems jenkins is down (or very slow)?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Guys,

Seems Jenkins is down or very slow? Does anyone else experience it or just
me?

Thanks,

Yin
"
Yin Huai <yhuai@databricks.com>,"Thu, 12 Nov 2015 18:27:51 -0800",Re: Seems jenkins is down (or very slow)?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Seems it is back.


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 12 Nov 2015 18:28:17 -0800",Re: Seems jenkins is down (or very slow)?,Yin Huai <yhuai@databricks.com>,"I was able to access the following where response was fast:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/45806/

Cheers


"
Fengdong Yu <fengdongy@everstring.com>,"Fri, 13 Nov 2015 10:46:54 +0800",Re: Seems jenkins is down (or very slow)?,Ted Yu <yuzhihong@gmail.com>,"I can assess directly in China



https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN <https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN>
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/45806/ <https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/45806/just me?

"
Stephen Boesch <javadba@gmail.com>,"Thu, 12 Nov 2015 19:05:25 -0800",Re: A proposal for Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"My understanding is that  the RDD's presently have more support for
complete control of partitioning which is a key consideration at scale.
While partitioning control is still piecemeal in  DF/DS  it would seem
premature to make RDD's a second-tier approach to spark dev.

An example is the use of groupBy when we know that the source relation
(/RDD) is already partitioned on the grouping expressions.  AFAIK the spark
sql still does not allow that knowledge to be applied to the optimizer - so
a full shuffle will be performed. However in the native RDD we can use
preservesPartitioning=true.

2015-11-12 17:42 GMT-08:00 Mark Hamstra <mark@clearstorydata.com>:

re
--
he
I
or
’s
 of
st,
ies
ist
ten.
.0
 be
0
certain
 feature.
ten.
t
re
i Zaharia <matei.zaharia@gmail.com> 写道：
l
oo.
m
en
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 12 Nov 2015 19:22:59 -0800",Re: A proposal for Spark 2.0,Stephen Boesch <javadba@gmail.com>,"Hmmm... to me, that seems like precisely the kind of thing that argues for
retaining the RDD API but not as the first thing presented to new Spark
developers: ""Here's how to use groupBy with DataFrames.... Until the
optimizer is more fully developed, that won't always get you the best
performance that can be obtained.  In these particular circumstances, ...,
you may want to use the low-level RDD API while setting
preservesPartitioning to true.  Like this....""


rk
so
ore
 --
the
PI
 or
’s
s of
t
rst,
cies
list
y
f
n
sten.
2.0
o be
move certain
t feature.
f
n
sten.
the
ei Zaharia <matei.zaharia@gmail.com> 写道：
ul
too.
e
e
.
"
"""=?gb18030?B?R3VvcWlhbmcgTGk=?="" <witgo@qq.com>","Fri, 13 Nov 2015 11:53:46 +0800",Re: RE: A proposal for Spark 2.0,"""=?gb18030?B?VWxhbm92LCBBbGV4YW5kZXI=?="" <alexander.ulanov@hpe.com>, ""=?gb18030?B?TmFuIFpodQ==?="" <zhunanmcgill@gmail.com>","Yes, I agree with  Nan Zhu. I recommend these projects:
https://github.com/dmlc/ps-lite (Apache License 2)
https://github.com/Microsoft/multiverso (MIT License)


Alexander, You may also be interested in the demo(graph on parameter Server) 


https://github.com/witgo/zen/tree/ps_graphx/graphx/src/main/scala/com/github/cloudml/zen/graphx







------------------ Original ------------------
From:  ""Ulanov, Alexander"";<alexander.ulanov@hpe.com>;
Date:  Fri, Nov 13, 2015 01:44 AM
To:  ""Nan Zhu""<zhunanmcgill@gmail.com>; ""Guoqiang Li""<witgo@qq.com>; 
Cc:  ""dev@spark.apache.org""<dev@spark.apache.org>; ""Reynold Xin""<rxin@databricks.com>; 
Subject:  RE: A proposal for Spark 2.0



  
Parameter Server is a new feature and thus does not match the goal of 2.0 is to fix things that are broken in the current API and remove certain deprecated APIs.  At the same time I would be happy to have that feature.
 
 
 
With regards to Machine learning, it would be great to move useful features from MLlib to ML and deprecate the former. Current structure of two separate machine  learning packages seems to be somewhat confusing.
 
With regards to GraphX, it would be great to deprecate the use of RDD in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.
 
 
 
Best regards, Alexander
 
 
 
From: Nan Zhu [mailto:zhunanmcgill@gmail.com] 
 Sent: Thursday, November 12, 2015 7:28 AM
 To: witgo@qq.com
 Cc: dev@spark.apache.org
 Subject: Re: A proposal for Spark 2.0
 
 
  
Being specific to Parameter Server, I think the current agreement is that PS shall exist as a third-party library instead of a component of the core code base, isnt?
 
  
 
 
  
Best,
 
   
 
 
  
-- 
 
  
Nan Zhu
 
  
http://codingcat.me
 
  
 
 
 
 
On Thursday, November 12, 2015 at 9:49 AM,  witgo@qq.com wrote:
     
Who has the idea of machine learning? Spark missing some features for machine learning, For example, the parameter server.
 
  
 
 
  
 
 
    
 20151112գ05:32Matei  Zaharia <matei.zaharia@gmail.com>  д
 
  
 
 
  
I like the idea of popping out Tachyon to an optional component too to reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.
 
  
 
 
  
Regarding Scala 2.12, we should definitely support it eventually, but I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many  code changes, just maybe some REPL stuff.
 
  
 
 
  
Needless to say, but I'm all for the idea of making ""major"" releases as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.
 
  
 
 
  
Matei
 
  
 
 
    
On Nov 11, 2015, at 4:58 AM, Sean Owen <sowen@cloudera.com> wrote:
 
  
 
 
  
On Wed, Nov 11, 2015 at 12:10 AM, Reynold Xin <rxin@databricks.com> wrote:
 
    
to the Spark community. A major release should not be very different from a
 
  
minor release and should not be gated based on new features. The main
 
  
purpose of a major release is an opportunity to fix things that are broken
 
  
in the current API and remove certain deprecated APIs (examples follow).
 
 
   
 
 
  
Agree with this stance. Generally, a major release might also be a
 
  
time to replace some big old API or implementation with a new one, but
 
  
I don't see obvious candidates.
 
  
 
 
  
I wouldn't mind turning attention to 2.x sooner than later, unless
 
  
there's a fairly good reason to continue adding features in 1.x to a
 
  
1.7 release. The scope as of 1.6 is already pretty darned big.
 
  
 
 
  
 
 
    
1. Scala 2.11 as the default build. We should still support Scala 2.10, but
 
  
it has been end-of-life.
 
 
   
 
 
  
By the time 2.x rolls around, 2.12 will be the main version, 2.11 will
 
  
be quite stable, and 2.10 will have been EOL for a while. I'd propose
 
  
dropping 2.10. Otherwise it's supported for 2 more years.
 
  
 
 
  
 
 
   
2. Remove Hadoop 1 support.
 
   
 
 
  
I'd go further to drop support for <2.2 for sure (2.0 and 2.1 were
 
  
sort of 'alpha' and 'beta' releases) and even <2.6.
 
  
 
 
  
I'm sure we'll think of a number of other small things -- shading a
 
  
bunch of stuff? reviewing and updating dependencies in light of
 
  
simpler, more recent dependencies to support from Hadoop etc?
 
  
 
 
  
Farming out Tachyon to a module? (I felt like someone proposed this?)
 
  
Pop out any Docker stuff to another repo?
 
  
Continue that same effort for EC2?
 
  
Farming out some of the ""external"" integrations to another repo (?
 
  
controversial)
 
  
 
 
  
See also anything marked version ""2+"" in JIRA.
 
  
 
 
  
---------------------------------------------------------------------
 
  
To unsubscribe, e-mail:  dev-unsubscribe@spark.apache.org
 
  
For additional commands, e-mail:  dev-help@spark.apache.org
 
 
   
 
 
  
 
 
  
---------------------------------------------------------------------
 
  
To unsubscribe, e-mail:  dev-unsubscribe@spark.apache.org
 
  
For additional commands, e-mail:  dev-help@spark.apache.org
 
 
   
 
 
  
 
 
  
 
 
  
 
 
  
---------------------------------------------------------------------
 
  
To unsubscribe, e-mail:  dev-unsubscribe@spark.apache.org
 
  
For additional commands, e-mail:  dev-help@spark.apache.org"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 13 Nov 2015 04:39:48 +0000",RE: A proposal for Spark 2.0,"Mark Hamstra <mark@clearstorydata.com>, Stephen Boesch <javadba@gmail.com>","Agree, more features/apis/optimization need to be added in DF/DS.

I mean, we need to think about what kind of RDD APIs we have to provide to developer, maybe the fundamental API is enough, like, the ShuffledRDD etc..  But PairRDDFunctions probably not in this category, as we can do the same thing easily with DF/DS, even better performance.

From: Mark Hamstra [mailto:mark@clearstorydata.com]
Sent: Friday, November 13, 2015 11:23 AM
To: Stephen Boesch
Cc: dev@spark.apache.org
Subject: Re: A proposal for Spark 2.0

Hmmm... to me, that seems like precisely the kind of thing that argues for retaining the RDD API but not as the first thing presented to new Spark developers: ""Here's how to use groupBy with DataFrames.... Until the optimizer is more fully developed, that won't always get you the best performance that can be obtained.  In these particular circumstances, ..., you may want to use the low-level RDD API while setting preservesPartitioning to true.  Like this....""

On Thu, Nov 12, 2015 at 7:05 PM, Stephen Boesch <javadba@gmail.com<mailto:javadba@gmail.com>> wrote:
My understanding is that  the RDD's presently have more support for complete control of partitioning which is a key consideration at scale.  While partitioning control is still piecemeal in  DF/DS  it would seem premature to make RDD's a second-tier approach to spark dev.

An example is the use of groupBy when we know that the source relation (/RDD) is already partitioned on the grouping expressions.  AFAIK the spark sql still does not allow that knowledge to be applied to the optimizer - so a full shuffle will be performed. However in the native RDD we can use preservesPartitioning=true.

2015-11-12 17:42 GMT-08:00 Mark Hamstra <mark@clearstorydata.com<mailto:mark@clearstorydata.com>>:
The place of the RDD API in 2.0 is also something I've been wondering about.  I think it may be going too far to deprecate it, but changing emphasis is something that we might consider.  The RDD API came well before DataFrames and DataSets, so programming guides, introductory how-to articles and the like have, to this point, also tended to emphasize RDDs -- or at least to deal with them early.  What I'm thinking is that with 2.0 maybe we should overhaul all the documentation to de-emphasize and reposition RDDs.  In this scheme, DataFrames and DataSets would be introduced and fully addressed before RDDs.  They would be presented as the normal/default/standard way to do things in Spark.  RDDs, in contrast, would be presented later as a kind of lower-level, closer-to-the-metal API that can be used in atypical, more specialized contexts where DataFrames or DataSets don't fully fit.

On Thu, Nov 12, 2015 at 5:17 PM, Cheng, Hao <hao.cheng@intel.com<mailto:hao.cheng@intel.com>> wrote:
I am not sure what the best practice for this specific problem, but it’s really worth to think about it in 2.0, as it is a painful issue for lots of users.

By the way, is it also an opportunity to deprecate the RDD API (or internal API only?)? As lots of its functionality overlapping with DataFrame or DataSet.

Hao

From: Kostas Sakellis [mailto:kostas@cloudera.com<mailto:kostas@cloudera.com>]
Sent: Friday, November 13, 2015 5:27 AM
To: Nicholas Chammas
Cc: Ulanov, Alexander; Nan Zhu; witgo@qq.com<mailto:witgo@qq.com>; dev@spark.apache.org<mailto:dev@spark.apache.org>; Reynold Xin

Subject: Re: A proposal for Spark 2.0

I know we want to keep breaking changes to a minimum but I'm hoping that with Spark 2.0 we can also look at better classpath isolation with user programs. I propose we build on spark.{driver|executor}.userClassPathFirst, setting it true by default, and not allow any spark transitive dependencies to leak into user code. For backwards compatibility we can have a whitelist if we want but I'd be good if we start requiring user apps to explicitly pull in all their dependencies. From what I can tell, Hadoop 3 is also moving in this direction.

Kostas

On Thu, Nov 12, 2015 at 9:56 AM, Nicholas Chammas <nicholas.chammas@gmail.com<mailto:nicholas.chammas@gmail.com>> wrote:

With regards to Machine learning, it would be great to move useful features from MLlib to ML and deprecate the former. Current structure of two separate machine learning packages seems to be somewhat confusing.

With regards to GraphX, it would be great to deprecate the use of RDD in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.

On that note of deprecating stuff, it might be good to deprecate some things in 2.0 without removing or replacing them immediately. That way 2.0 doesn’t have to wait for everything that we want to deprecate to be replaced all at once.

Nick
​

On Thu, Nov 12, 2015 at 12:45 PM Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Parameter Server is a new feature and thus does not match the goal of 2.0 is “to fix things that are broken in the current API and remove certain deprecated APIs”. At the same time I would be happy to have that feature.

With regards to Machine learning, it would be great to move useful features from MLlib to ML and deprecate the former. Current structure of two separate machine learning packages seems to be somewhat confusing.
With regards to GraphX, it would be great to deprecate the use of RDD in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.

Best regards, Alexandev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: A proposal for Spark 2.0

Being specific to Parameter Server, I think the current agreement is that PS shall exist as a third-party library instead of a component of the core code base, isn’t?

Best,

--
Nan Zhu
http://codingcat.me


On Thursday, November 12, 2015 ahas the idea of machine learning? Spark missing some features for machine learning, For example, the parameter server.


在 2015年11月12日，05:32，Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.com>> 写道：

I like the idea of popping out Tachyon to an optional component too to reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.

Regarding Scala 2.12, we should definitely support it eventually, but I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.

Needless to say, but I'm all for the idea of making ""major"" releases as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.

Matei

On Nov 11, 2015, at 4:58 AM, Sean Owen <sowen@cloudera.com<mailto:sowen@cloudera.com>> wrote:

On Wed, Nov 11, 2015 at 12:10 AM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:
to the Spark community. A major release should not be very different from a
minor release and should not be gated based on new features. The main
purpose of a major release is an opportunity to fix things that are broken
in the current API and remove certain deprecated APIs (examples follow).

Agree with this stance. Generally, a major release might also be a
time to replace some big old API or implementation with a new one, but
I don't see obvious candidates.

I wouldn't mind turning attention to 2.x sooner than later, unless
there's a fairly good reason to continue adding features in 1.x to a
1.7 release. The scope as of 1.6 is already pretty darned big.


1. Scala 2.11 as the default build. We should still support Scala 2.10, but
it has been end-of-life.

By the time 2.x rolls around, 2.12 will be the main version, 2.11 will
be quite stable, and 2.10 will have been EOL for a while. I'd propose
dropping 2.10. Otherwise it's supported for 2 more years.


2. Remove Hadoop 1 support.

I'd go further to drop support for <2.2 for sure (2.0 and 2.1 were
sort of 'alpha' and 'beta' releases) and even <2.6.

I'm sure we'll think of a number of other small things -- shading a
bunch of stuff? reviewing and updating dependencies in light of
simpler, more recent dependencies to support from Hadoop etc?

Farming out Tachyon to a module? (I felt like someone proposed this?)
Pop out any Docker stuff to another repo?
Continue that same effort for EC2?
Farming out some of the ""external"" integrations to another repo (?
controversial)

See also anything marked version ""2+"" in JIRA.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>




---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>





"
shane knapp <sknapp@berkeley.edu>,"Fri, 13 Nov 2015 06:36:19 -0800","Re: [build system] short jenkins downtime tomorrow morning,
 11-13-2015 @ 7am PST","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 13 Nov 2015 07:16:42 -0800","Re: [build system] short jenkins downtime tomorrow morning,
 11-13-2015 @ 7am PST","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","this is still ongoing.  the update is running 'chown -R jenkins' on
the jenkins root directory, which is a hair under 3T.

this might take a while...  :\

shane


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 13 Nov 2015 08:33:12 -0800","Re: [build system] short jenkins downtime tomorrow morning,
 11-13-2015 @ 7am PST","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","phew.  this is finally done...  jenkins is up and building.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 13 Nov 2015 09:16:11 -0800",Re: Seems jenkins is down (or very slow)?,Yin Huai <yhuai@databricks.com>,"were you hitting any particular URL when you noticed this, or was it
generally slow?


---------------------------------------------------------------------


"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 13 Nov 2015 10:52:39 -0800",let spark streaming sample come to stop,dev@spark.apache.org,"Hi,

I try to run the following 1.4.1 sample by putting a words.txt under
localdir

bin\run-example org.apache.spark.examples.streaming.HdfsWordCount localdir

2 questions

1. it does not pick up words.txt because it's 'old' I guess - any option to
let it picked up?
2. I managed to put a 'new' file on the fly which got picked up, but after
processing, the program doesn't stop (keeps generating empty RDDs instead),
any option to let it stop when no new files come in (otherwise it blocks
others when I want to run multiple samples?)

Thanks,
Renyi.
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 13 Nov 2015 11:17:52 -0800",SparkPullRequestBuilder coverage,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I noticed that SparkPullRequestBuilder completes much faster than maven
Jenkins build.

From
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/45871/consoleFull
, I couldn't get exact time the builder started but looks like the duration
was around 20 minutes.

From
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=spark-test/4099/console
:

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:42 h


Can someone enlighten me on the sets of tests executed by
SparkPullRequestBuilder ?


BTW I noticed that recent Jenkins builds were not in good shape:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=spark-test/


Comments are welcome.
"
Reynold Xin <rxin@databricks.com>,"Fri, 13 Nov 2015 11:20:34 -0800",Re: SparkPullRequestBuilder coverage,Ted Yu <yuzhihong@gmail.com>,"It only runs tests that are impacted by the change. E.g. if you only modify
SQL, it won't run the core or streaming tests.



"
Kostas Sakellis <kostas@cloudera.com>,"Fri, 13 Nov 2015 11:40:40 -0800",Re: A proposal for Spark 2.0,"""Cheng, Hao"" <hao.cheng@intel.com>","We have veered off the topic of Spark 2.0 a little bit here - yes we can
talk about RDD vs. DS/DF more but lets refocus on Spark 2.0. I'd like to
propose we have one more 1.x release after Spark 1.6. This will allow us to
stabilize a few of the new features that were added in 1.6:

1) the experimental Datasets API
2) the new unified memory manager.

I understand our goal for Spark 2.0 is to offer an easy transition but
there will be users that won't be able to seamlessly upgrade given what we
have discussed as in scope for 2.0. For these users, having a 1.x release
with these new features/APIs stabilized will be very beneficial. This might
make Spark 1.7 a lighter release but that is not necessarily a bad thing.

Any thoughts on this timeline?

Kostas Sakellis




o
he
r
,
:
rk
so
re
--
he
I
or
s
of
t,
es
st
en.
0
be
ertain
feature.
en.
e
 Zaharia <matei.zaharia@gmail.com> 写道：
o.
:
 a
n
ut
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 13 Nov 2015 12:09:30 -0800",Re: SparkPullRequestBuilder coverage,Reynold Xin <rxin@databricks.com>,"Can I assume that for any particular test, if it passes reliably on
SparkPullRequestBuilder,
it should pass on maven Jenkins ?

If so, should flaky test(s) be disabled, strengthened and enabled again ?

Cheers


"
Reynold Xin <rxin@databricks.com>,"Fri, 13 Nov 2015 12:10:15 -0800",Re: SparkPullRequestBuilder coverage,Ted Yu <yuzhihong@gmail.com>,"Yes. And those have been happening too.



"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 13 Nov 2015 12:26:04 -0800",Re: A proposal for Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"Why does stabilization of those two features require a 1.7 release instead
of 1.6.1?


to
e
ht
the
ark
.,
ark
 so
ore
 --
the
PI
 or
’s
 of
st,
ies
ist
ten.
.0
 be
0
certain
 feature.
ten.
t
re
i Zaharia <matei.zaharia@gmail.com> 写道：
l
oo.
m
en
"
Yin Huai <yhuai@databricks.com>,"Fri, 13 Nov 2015 12:37:46 -0800",Re: Seems jenkins is down (or very slow)?,shane knapp <sknapp@berkeley.edu>,"It was generally slow. But, after 5 or 10 minutes, it's all good.


"
Andrew Lee <alee526@hotmail.com>,"Fri, 13 Nov 2015 21:00:51 +0000",Spark 1.4.2 release and votes conversation? ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,


I'm wondering if Spark 1.4.2 had been voted by any chance or if I have overlooked and we are targeting 1.4.3?


By looking at the JIRA

https://issues.apache.org/jira/browse/SPARK/fixforversion/12332833/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel


All issues were resolved and no blockers. Anyone knows what happened to this release?



or was there any recommendation to skip that and ask users to use Spark 1.5.2 instead?
"
Reynold Xin <rxin@databricks.com>,"Fri, 13 Nov 2015 13:30:24 -0800",Re: Spark 1.4.2 release and votes conversation?,Andrew Lee <alee526@hotmail.com>,"I actually tried to build a binary for 1.4.2 and wanted to start voting,
but there was an issue with the release script that failed the jenkins job.
Would be great to kick off a 1.4.2 release.



"
Reynold Xin <rxin@databricks.com>,"Fri, 13 Nov 2015 13:31:19 -0800",Re: Spark 1.4.2 release and votes conversation?,Andrew Lee <alee526@hotmail.com>,"In the interim, you can just build it off branch-1.4 if you want.



"
DavidFallside <david@fallside.com>,"Fri, 13 Nov 2015 14:58:21 -0700 (MST)",Incubator Proposal for Spark-Kernel,dev@spark.apache.org,"Hello, I wanted to make known a new Apache Incubator proposal for
""Spark-Kernel"", https://wiki.apache.org/incubator/SparkKernelProposal, which
provides applications with a mechanism to interactively and remotely access
Spark. The proposal is just starting to be discussed on the general
incubator list and the Spark community's input would be very valuable.
Thanks,
David




--

---------------------------------------------------------------------


"
Federico Bertola <federico.bertola88@gmail.com>,"Fri, 13 Nov 2015 23:30:44 +0100",Problem with Breadcast variable not deserialized,dev@spark.apache.org,"Hi,
  I've a simple Spark job that tries to broadcast an *Externalizable* 
object across workers. A simple System.out confirm that the object is 
only serialized with writeExternal and than deserialized without 
readExternal. Personally I think this is a bug. I'm using the default 
JavaSerializer with Spark 1.5.*

Kind regards,

Federico.

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Fri, 13 Nov 2015 14:33:36 -0800",Re: pyspark with pypy not work for spark-1.5.1,Chang Ya-Hsuan <sumtiogo@gmail.com>,"We already test CPython 2.6, CPython 3.4 and PyPy 2.5, it took more
than 30 min to run (without parallelization),
I think it should be enough.

PyPy 2.2 is too old that we have not enough resource to support that.

,
,
y"",
y"",
y"",
e I
te:
e:
e:
's
PyPy
t
ctual
 to
to
n.
face
p
able
n.
y"", line
_.py"",
.py"",
ators.py"",
zers.py"",
zers.py"",
zers.py"",
lp

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 14 Nov 2015 11:40:31 +0000",Re: A proposal for Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"
Producing new x.0 releases of open source projects is a recurrent problem: too radical a change means the old version gets updated anyway (Python 3) and an incompatible version stops takeup (example, Log4Js dropping support for log4j.properties files),

Similarly, any radical new feature does tend to push out release times longer than you think (Hadoop 2).

I think the lessons I'd draw from those and others is: keep an x.0 version as compatible as possible so that everyone can move, and ship fast. You want to be able to retire the 1.x line.

And how to ship fast? Keep those features down.

For anyone planning anything radical —a branch with a clear plan/schedule to be merged in is probably the best strategy. I actually think the firefox process is the best here, and that it should have been adopted more in Hadoop; ongoing work is going in in branches for some things (erasure coding, IPv6), but there's still pressure to define the release schedule on feature completeness.

https://wiki.mozilla.org/Release_Management/Release_Process

see also JDD's article on evolution vs revolution in OSS; 15 years old but still valid. At the time, the Jakarta project was the equivalent of the ASF hadoop/big data stack, and indeed, its traces run through the code and the build & test process if you know what to look for

http://incubator.apache.org/learn/rules-for-revolutionaries.html



-Steve
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 15 Nov 2015 02:17:16 -0800",Re: spark 1.4 GC issue,Renu Yadav <yrenu21@gmail.com>,"Please take a look at http://www.infoq.com/articles/tuning-tips-G1-GC

Cheers


"
gsvic <victorasgs@gmail.com>,"Sun, 15 Nov 2015 11:52:29 -0700 (MST)",Are map tasks spilling data to disk?,dev@spark.apache.org,"According to  this paper
<http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf>  
Spak's map tasks writes the results to disk. 

My actual question is, in  BroadcastHashJoin
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoin.scala#L100>  
doExecute() method at line  109 the mapPartitions
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoin.scala#L109>  
method is called. At this step, Spark will schedule a number of tasks for
execution in order to perform the hash join operation. The results of these
tasks will be written to each worker's disk?



--

---------------------------------------------------------------------


"
gsvic <victorasgs@gmail.com>,"Sun, 15 Nov 2015 11:53:25 -0700 (MST)",Map Tasks - Disk Spill (?),dev@spark.apache.org,"According to  this paper
<http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf>  
Spak's map tasks writes the results to disk. 

My actual question is, in  BroadcastHashJoin
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoin.scala#L100>  
doExecute() method at line  109 the mapPartitions
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoin.scala#L109>  
method is called. At this step, Spark will schedule a number of tasks for
execution in order to perform the hash join operation. The results of these
tasks will be written to each worker's disk?



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Nov 2015 10:57:31 -0800",Re: Are map tasks spilling data to disk?,gsvic <victorasgs@gmail.com>,"It depends on what the next operator is. If the next operator is just an
aggregation, then no, the hash join won't write anything to disk. It will
just stream the data through to the next operator. If the next operator is
shuffle (exchange), then yes.


"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 16 Nov 2015 10:47:27 +0530",Re: A proposal for Spark 2.0,"Matei Zaharia <matei.zaharia@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Matei,




Our REPL specific changes were merged in scala/scala and are available as
part of 2.11.7 and hopefully be part of 2.12 too. If I am not wrong, REPL
stuff is taken care of, we don;t need to keep upgrading REPL code for every
scala release now. http://www.scala-lang.org/news/2.11.7

I am +1 on the proposal for Spark 2.0.

Thanks,


Prashant Sharma




"
Charmee Patel <charmeep@gmail.com>,"Mon, 16 Nov 2015 00:40:10 -0500",Hive Context incompatible with Sentry enabled Cluster,dev@spark.apache.org,"Hi,

We have recently run into this issue:
https://issues.apache.org/jira/browse/SPARK-9042

My organization's application reads raw data from files, processes/cleanses
it and pushes the results to Hive tables. To keep reads efficient, we have
partitioned our tables. In a Sentry enabled cluster, our writes to Hive
tables fail as Hive Context tries to edit partitions in meta store directly
and Sentry has disabled direct edits in Hive Meta Store.

After discussing our options with Cloudera Support, current workaround for
us is to generate bunch of files at the end of Spark process and open a
separate connection to HiveServer2 to load those files. We can change our
tables to be external tables to reduce data movement. Regardless, it's a
stop gap measure as we need to open separate connection to HiveServer2 to
manage the partitions.

This also affects all Hive CTAS + DDLs supported from within Hive Context.
We'd like to know where Hive Support within Spark is headed with Security
products like Sentry or Ranger in place.

Thanks,
Charmee
"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Nov 2015 21:52:09 -0800",Re: Support for local disk columnar storage for DataFrames,Cristian O <cristian.b.opris@googlemail.com>,"This (updates) is something we are going to think about in the next release
or two.


"
kiran lonikar <lonikar@gmail.com>,"Mon, 16 Nov 2015 12:07:17 +0530",Hive on Spark Vs Spark SQL,"dev@spark.apache.org, user <user@spark.apache.org>","I would like to know if Hive on Spark uses or shares the execution code
with Spark SQL or DataFrames?

More specifically, does Hive on Spark benefit from the changes made to
Spark SQL, project Tungsten? Or is it completely different execution path
where it creates its own plan and executes on RDD?

-Kiran
"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Nov 2015 22:37:41 -0800",Re: Hive on Spark Vs Spark SQL,kiran lonikar <lonikar@gmail.com>,"It's a completely different path.



"
kiran lonikar <lonikar@gmail.com>,"Mon, 16 Nov 2015 12:15:02 +0530",Re: Hive on Spark Vs Spark SQL,Reynold Xin <rxin@databricks.com>,"So does not benefit from Project Tungsten right?



"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Nov 2015 22:50:29 -0800",Re: Hive on Spark Vs Spark SQL,kiran lonikar <lonikar@gmail.com>,"No it does not -- although it'd benefit from some of the work to make
shuffle more robust.



"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 16 Nov 2015 12:23:47 +0530",releasing Spark 1.4.2,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am wondering when spark 1.4.2 will be released?

is it in the voting stage at the moment?

rgds

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 16 Nov 2015 16:03:56 +0800","Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",dev <dev@spark.apache.org>,"Sometimes, the jars under lib_managed is missing. And after I rebuild the
spark, the jars under lib_managed is still not downloaded. This would cause
the spark-shell fail due to jars missing. Anyone has hit this weird issue ?



-- 
Best Regards

Jeff Zhang
"
Josh Rosen <joshrosen@databricks.com>,"Mon, 16 Nov 2015 00:10:08 -0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Jeff Zhang <zjffdu@gmail.com>,"As of https://github.com/apache/spark/pull/9575, Spark's build will no
longer place every dependency JAR into lib_managed. Can you say more about
how this affected spark-shell for you (maybe share a stacktrace)?


"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 16 Nov 2015 10:24:40 +0200",Re: Support for local disk columnar storage for DataFrames,"""dev@spark.apache.org"" <dev@spark.apache.org>","Cloudera's Kudu also looks interesting here (getkudu.io) - Hadoop
input/output format support:
https://github.com/cloudera/kudu/blob/master/java/kudu-mapreduce/src/main/java/org/kududb/mapreduce/KuduTableInputFormat.java


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 16 Nov 2015 00:30:54 -0800",Re: Support for local disk columnar storage for DataFrames,Nick Pentreath <nick.pentreath@gmail.com>,"FiloDB is also closely reated.  https://github.com/tuplejump/FiloDB


"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 16 Nov 2015 16:47:55 +0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Josh Rosen <joshrosen@databricks.com>,"It's about the datanucleus related jars which is needed by spark sql.
Without these jars, I could not call data frame related api ( I make
HiveContext enabled)






-- 
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 16 Nov 2015 16:51:25 +0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Josh Rosen <joshrosen@databricks.com>,"This is the exception I got

15/11/16 16:50:48 WARN metastore.HiveMetaStore: Retrying creating default
database after error: Class
org.datanucleus.api.jdo.JDOPersistenceManagerFactory was not found.
javax.jdo.JDOFatalUserException: Class
org.datanucleus.api.jdo.JDOPersistenceManagerFactory was not found.
at
at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
at
org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
at
org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
at
org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
at
org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
at
org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
at
org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
at
org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
at
org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
at
org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
at
org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
at
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)





-- 
Best Regards

Jeff Zhang
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 16 Nov 2015 00:51:40 -0800",Re: releasing Spark 1.4.2,Niranda Perera <niranda.perera@gmail.com>,"See this thread:

http://search-hadoop.com/m/q3RTtLKc2ctNPcq&subj=Re+Spark+1+4+2+release+and+votes+conversation+

ote:
"
Nick Evans <me@nicolasevans.org>,"Mon, 16 Nov 2015 16:53:06 +0100",Streaming Receiverless Kafka API + Offset Management,dev@spark.apache.org,"I really like the Streaming receiverless API for Kafka streaming jobs, but
I'm finding the manual offset management adds a fair bit of complexity. I'm
sure that others feel the same way, so I'm proposing that we add the
ability to have consumer offsets managed via an easy-to-use API. This would
be done similarly to how it is done in the receiver API.

I haven't written any code yet, but I've looked at the current version of
the codebase and have an idea of how it could be done.

To keep the size of the pull requests small, I propose that the following
distinct features are added in order:

   1. If a group ID is set in the Kafka params, and also if fromOffsets is
   not passed in to createDirectStream, then attempt to resume from the
   remembered offsets for that group ID.
   2. Add a method on KafkaRDDs that commits the offsets for that KafkaRDD
   to Zookeeper.
   3. Update the Python API with any necessary changes.

My goal is to not break the existing API while adding the new functionality.

whether it's a better idea to set the group ID as mentioned through Kafka
params, or to define a new overload of createDirectStream that expects the
group ID in place of the fromOffsets param. I think the latter is a cleaner
interface, but I'm not sure whether adding a new param is a good idea.

If anyone has any feedback on this general approach, I'd be very grateful.
I'm going to open a JIRA in the next couple days and begin working on the
first point, but I think comments from the community would be very helpful
on building a good API here.
"
Cody Koeninger <cody@koeninger.org>,"Mon, 16 Nov 2015 10:28:55 -0600",Re: Streaming Receiverless Kafka API + Offset Management,Nick Evans <me@nicolasevans.org>,"There are already private methods in the code for interacting with Kafka's
offset management api.

There's a jira for making those methods public, but TD has been reluctant
to merge it

https://issues.apache.org/jira/browse/SPARK-10963

I think adding any ZK specific behavior to spark is a bad idea, since ZK
may no longer be the preferred storage location for Kafka offsets within
the next year.




"
Alex Nastetsky <alex.nastetsky@vervemobile.com>,"Mon, 16 Nov 2015 12:40:57 -0500",Re: Sort Merge Join from the filesystem,"""Cheng, Hao"" <hao.cheng@intel.com>","Done, thanks.


"
"""Fernando O."" <fotero@gmail.com>","Mon, 16 Nov 2015 15:38:15 -0300",Persisting DStreams,dev@spark.apache.org,"Hi all,
   I was wondering if someone could give me a brief explanation or point me
in the right direction in the code for where DStream persistence is donde.
I'm looking at DStream.java but all it does is setting the StorageLevel,
and neither WindowedDStream or ReducedWindowedDStream seem to change that
behaviour a lot:
WindowedDStream delegates on the parent and
ReducedWindowedDStream calls super and delegates on an aggregated DStream

So it seems like persist will only change the StorageLevel and then you
need to compute the DStream to persist the inner RDDs

So basically what I'm trying to verify is: persist is just lazy persistence
method that sets the Storage level and the actual persistence takes place
when you compute the DStream right?
"
,"Mon, 16 Nov 2015 20:23:06 +0100",Re: Persisting DStreams,dev@spark.apache.org,"Hi Fernando,

the ""persistence"" of a DStream is defined depending of the StorageLevel.

Window is not related to persistence: it's the processing of multiple 
DStream in one, a kind of ""gather of DStreams"". The transformation is 
applied on a ""slide window"". For instance, you define a window of 3 
DStreams, and you apply the transformation on this window.

I hope it helps.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Mon, 16 Nov 2015 15:43:54 -0800",Re: slightly more informative error message in MLUtils.loadLibSVMFile,Robert Dodier <robert.dodier@gmail.com>,"That sounds useful; would you mind submitting a JIRA (and a PR if you're
willing)?
Thanks,
Joseph


"
Joseph Bradley <joseph@databricks.com>,"Mon, 16 Nov 2015 15:54:02 -0800",Re: Spark Implementation of XGBoost,Meihua Wu <rotationsymmetry14@gmail.com>,"""""""
1) I agree the sorting method you suggested is a very efficient way to
handle the unordered categorical variables in binary classification
and regression. I propose we have a Spark ML Transformer to do the
sorting and encoding, bringing the benefits to many tree based
methods. How about I open a jira for this?
""""""

--> MLlib trees do this currently, so you could check out that code as an
example.
I'm not sure how this would work as a generic transformer, though; it seems
more like an internal part of space-partitioning algorithms.




"
Joseph Bradley <joseph@databricks.com>,"Mon, 16 Nov 2015 16:36:02 -0800",Re: Unchecked contribution (JIRA and PR),=?UTF-8?Q?Sergio_Ram=C3=ADrez?= <sramirezga@ugr.es>,"Hi Sergio,

Apart from apologies about limited review bandwidth (from me too!), I
wanted to add: It would be interesting to hear what feedback you've gotten
from users of your package.  Perhaps you could collect feedback by (a)
emailing the user list and (b) adding a note in the Spark Packages pointing
to the JIRA, and encouraging users to add their comments directly to the
JIRA.  That'd be a nice way to get a sense of use cases and priority.

Thanks for your patience,
Joseph

te:

e,
ed
c
n
ch
ed
on
d
ed
d
--
mismo,
dirigida a
 copia sin
vigente. En el
.
smo,
rigida a
opia sin
gente. En el
"
Andrew Lee <alee526@hotmail.com>,"Tue, 17 Nov 2015 00:49:42 +0000",Re: Spark 1.4.2 release and votes conversation?,Reynold Xin <rxin@databricks.com>,"I did, and it passes all of our test case, so I'm wondering what did I miss. I know there is the memory leak spill JIRA SPARK-11293, but not sure if that will go in 1.4.2 or 1.4.3, etc.



________________________________
From: Reynold Xin <rxin@databricks.com>
Sent: Friday, November 13, 2015 1:31 PM
To: Andrew Lee
Cc: dev@spark.apache.org
Subject: Re: Spark 1.4.2 release and votes conversation?

In the interim, you can just build it off branch-1.4 if you want.


I actually tried to build a binary for 1.4.2 and wanted to start voting, but there was an issue with the release script that failed the jenkins job. Would be great to kick off a 1.4.2 release.



Hi All,


I'm wondering if Spark 1.4.2 had been voted by any chance or if I have overlooked and we are targeting 1.4.3?


By looking at the JIRA

https://issues.apache.org/jira/browse/SPARK/fixforversion/12332833/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel


All issues were resolved and no blockers. Anyone knows what happened to this release?



or was there any recommendation to skip that and ask users to use Spark 1.5.2 instead?


"
Nick Evans <me@nicolasevans.org>,"Tue, 17 Nov 2015 02:27:15 +0100",Re: Streaming Receiverless Kafka API + Offset Management,Cody Koeninger <cody@koeninger.org>,"The only dependancy on Zookeeper I see is here:
https://github.com/apache/spark/blob/1c5475f1401d2233f4c61f213d1e2c2ee9673067/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/ReliableKafkaReceiver.scala#L244-L247

If that's the only line that depends on Zookeeper, we could probably try to
implement an abstract offset manager that could be switched out in favour
of the new offset management system, yes? I know kafka.consumer.Consumer
currently depends on Zookeeper, but I'm guessing this library will
eventually be updated to use the new method.




-- 
*Nick Evans* <me@nicolasevans.org>
P. (613) 793-5565
LinkedIn <http://linkd.in/nZpN6w> | Website <http://bit.ly/14XTBtj>
"
Jo Voordeckers <jo.voordeckers@gmail.com>,"Mon, 16 Nov 2015 17:46:36 -0800",Mesos cluster dispatcher doesn't respect most args from the submit req,"dev <dev@spark.apache.org>, user@spark.apache.org","Hi all,

I'm running the mesos cluster dispatcher, however when I submit jobs with
things like jvm args, classpath order and UI port aren't added to the
commandline executed by the mesos scheduler. In fact it only cares about
the class, jar and num cores/mem.

https://github.com/jayv/spark/blob/mesos_cluster_params/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L412-L424

I've made an attempt at adding a few of the args that I believe are useful
to the MesosClusterScheduler class, which seems to solve my problem.

Please have a look:

https://github.com/apache/spark/pull/9752

Thanks

- Jo Voordeckers
"
Bryan Cutler <cutlerb@gmail.com>,"Mon, 16 Nov 2015 18:29:26 -0800",Re: let spark streaming sample come to stop,Renyi Xiong <renyixiong0@gmail.com>,"Hi Renyi,

This is the intended behavior of the streaming HdfsWordCount example.  It
makes use of a 'textFileStream' which will monitor a hdfs directory for any
newly created files and push them into a dstream.  It is meant to be run
indefinitely, unless interrupted by ctrl-c, for example.

-bryan

"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 17 Nov 2015 11:35:08 +0800",Re: Streaming Receiverless Kafka API + Offset Management,Nick Evans <me@nicolasevans.org>,"Kafka now build-in supports managing metadata itself besides ZK, it is easy
to use and change from current ZK implementation. I think here the problem
is do we need to manage offset in Spark Streaming level or leave this
question to user.

If you want to manage offset in user level, letting Spark to offer a
convenient API, I think Cody's patch (
https://issues.apache.org/jira/browse/SPARK-10963) could satisfy your needs.

If you hope to let Spark Streaming to manage offsets for you (transparent
to the user level), I think I had a PR before but the community inclines to
leave this to user level.


"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 17 Nov 2015 14:46:04 +0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Josh Rosen <joshrosen@databricks.com>,"Hi Josh,

I notice the comments in https://github.com/apache/spark/pull/9575 said
that Datanucleus related jars will still be copied to lib_managed/jars. But
I don't see any jars under lib_managed/jars. The weird thing is that I see
the jars on another machine, but could not see jars on my laptop even after
I delete the whole spark project and start from scratch. Does it related
with environments ? I try to add the following code in SparkBuild.scala to
track the issue, it shows that the jars is empty. Any thoughts on that ?


deployDatanucleusJars := {
      val jars: Seq[File] = (fullClasspath in assembly).value.map(_.data)
        .filter(_.getPath.contains(""org.datanucleus""))
      // this is what I added
      println(""*********************************************"")
      println(""fullClasspath:""+fullClasspath)
      println(""assembly:""+assembly)
      println(""jars:""+jars.map(_.getAbsolutePath()).mkString("",""))
      //






-- 
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 17 Nov 2015 15:21:15 +0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Josh Rosen <joshrosen@databricks.com>,"BTW, After I revert  SPARK-784, I can see all the jars under
lib_managed/jars





-- 
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 17 Nov 2015 18:27:11 +0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Josh Rosen <joshrosen@databricks.com>,"BTW, After I revert  SPARK-7841, I can see all the jars under
lib_managed/jars





-- 
Best Regards

Jeff Zhang
"
gsvic <victorasgs@gmail.com>,"Tue, 17 Nov 2015 04:22:02 -0700 (MST)",Re: Are map tasks spilling data to disk?,dev@spark.apache.org,"So, in case of Sort Merge join in which a shuffle (exchange) will be
performed, I have the following questions (Please correct me if my
understanding is not correct):

Let's say that relation A is a JSONRelation (640 MB) on the HDFS where the
block size is 64MB. This will produce a Scan JSONRelation() of 10 partitions
( 640 / 64 ) where each of these partitions will contain |A| / 10 rows.

The second step will be a hashPartitioning(#key, 200) where #key is the
equi-join condition and 200 the default number of shuffles
(spark.sql.shuffle.partitions). Each partition will be computed in an
individual task, in which every row will be hashed on the #key and then will
be written in the corresponing chunk (of 200 resulting chunks) directly on
disk. 

Q1: What happens if a resulting hashed row in the executor A must be written
in a chunk which is stored in the executor B? Does it use the
HashShuffleManager to transfer it over the network?

Q2: After the Sort (3rd) step there will be 200, 200 resulting
partitions/chunks for relations A and B respectively which will be
concatenated into 200 SortMergeJoin tasks where each of them will contain
(|A|/200 + |B|/200) rows. For each pair (chunkOfA, chunkOfB) will chunkOfA
and chunkOfB contain rows for the same hash key ?



--

---------------------------------------------------------------------


"
yuming wang <wgyumg@gmail.com>,"Tue, 17 Nov 2015 20:10:44 +0800",Add a function to support Google's Word2Vec,dev@spark.apache.org,"Hi:



I have a function to load Google’s Word2Vec generated binary file and spark
can use this model. If it is convenient, I'm going to open a JIRA and Pull
Request.



My code is:

https://github.com/wangyum/spark/commit/7c80d311722d67ed4b9746537e0a21c2dc1a9670



Thanks
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 17 Nov 2015 14:16:39 +0100","Re: Mesos cluster dispatcher doesn't respect most args from the
 submit req",Jo Voordeckers <jo.voordeckers@gmail.com>,"Hi Jo,

I agree that there's something fishy with the cluster dispatcher, I've seen
some issues like that.

I think it actually tries to send all properties as part of
`SPARK_EXECUTOR_OPTS`, which may not be everything that's needed:

https://github.com/jayv/spark/blob/mesos_cluster_params/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L375-L377

Can you please open a Jira ticket and describe also the symptoms? This
might be related, or the same issue: SPARK-11280
<https://issues.apache.org/jira/browse/SPARK-11280> and also SPARK-11327
<https://issues.apache.org/jira/browse/SPARK-11327>

thanks,
iulian







-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
"""Robin East (hotmail)"" <robin_east@hotmail.com>","Tue, 17 Nov 2015 13:28:55 +0000",Re: Add a function to support Google's Word2Vec,yuming wang <wgyumg@gmail.com>,"Have a look at SPARK-9484, JIRA is already there. Pull request would be good.

Robin


nd spark can use this model. If it is convenient, I'm going to open a JIRA and Pull Request.
1a9670
"
Jo Voordeckers <jo.voordeckers@gmail.com>,"Tue, 17 Nov 2015 10:28:49 -0800","Re: Mesos cluster dispatcher doesn't respect most args from the
 submit req",=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,".com>

la/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L375-L377
Aha that's interesting, I overlooked that line, I'll debug some more today
because I know for sure that those options don't make it onto the
commandline when I was running it in my debugger.



SPARK-11327 <https://issues.apache.org/jira/browse/SPARK-11327> is exactly
my problem, but I don't run docker.

 - Jo

h
ala/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L412-L424
em.
"
Timothy Chen <tnachen@gmail.com>,"Tue, 17 Nov 2015 12:01:17 -0800","Re: Mesos cluster dispatcher doesn't respect most args from the
 submit req",Jo Voordeckers <jo.voordeckers@gmail.com>,"Hi Jo,

Thanks for the links, I would expected the properties to be in
scheduler properties but I need to double check.

I'll be looking into these problems this week.

Tim

fe.com>
ala/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L375-L377
y
m>
th
t the
cala/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L412-L424
lem.

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Tue, 17 Nov 2015 22:12:19 +0000","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Jeff Zhang <zjffdu@gmail.com>,"Is the Hive profile enabled? I think it may need to be turned on in order
for those JARs to be deployed.

"
Jo Voordeckers <jo.voordeckers@gmail.com>,"Tue, 17 Nov 2015 15:38:27 -0800","Re: Mesos cluster dispatcher doesn't respect most args from the
 submit req",Timothy Chen <tnachen@gmail.com>,"Hi Tim,

I've done more forensics on this bug, see my comment here:

https://issues.apache.org/jira/browse/SPARK-11327?focusedCommentId=15009843&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15009843


- Jo Voordeckers



la/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L375-L377
la/org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler.scala#L412-L424
"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 18 Nov 2015 08:08:37 +0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Josh Rosen <joshrosen@databricks.com>,"Sure, hive profile is enabled.




-- 
Best Regards

Jeff Zhang
"
Josh Rosen <joshrosen@databricks.com>,"Wed, 18 Nov 2015 01:42:19 +0000","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Jeff Zhang <zjffdu@gmail.com>,"Can you file a JIRA issue to help me triage this further? Thanks!


"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 18 Nov 2015 10:09:12 +0800","Re: Does anyone meet the issue that jars under lib_managed is never
 downloaded ?",Josh Rosen <joshrosen@databricks.com>,"Created https://issues.apache.org/jira/browse/SPARK-11798






-- 
Best Regards

Jeff Zhang
"
=?UTF-8?B?7J6E7KCV7YOd?= <kabhwan@gmail.com>,"Wed, 18 Nov 2015 13:26:15 +0900",Fwd: zeppelin (or spark-shell) with HBase fails on executor level,dev@spark.apache.org,"HI,

First of all, I'm sorry if you received mail before (apache spark user
mailing group).
I posted this mail to user mailing group but I didn't receive any
informations about resolving so I'd like to post this to dev mailing group.

This link points to the thread I'm forwarding, so if you feel convenient
referring to mail archive, please use this link.

https://mail-archives.apache.org/mod_mbox/spark-user/201511.mbox/%3CCAF5108jMXyOjiGmCgr%3Ds%2BNvTMcyKWMBVM1GsrH7Pz4xUj48LfA%40mail.gmail.com%3E

This behavior is a bit odd for me, so I'd like to get any hints to resolve,
or report bug if it is.

Thanks!
Jungtaek Lim (HeartSaVioR)


---------- Forwarded message ----------
From: 임정택 <kabhwan@gmail.com>
Date: 2015-11-17 18:01 GMT+09:00
Subject: zeppelin (or spark-shell) with HBase fails on executor level
To: user@spark.apache.org


Hi all,

I'm evaluating zeppelin to run driver which interacts with HBase.
I use fat jar to include HBase dependencies, and see failures on executor
level.
I thought it is zeppelin's issue, but it fails on spark-shell, too.

I loaded fat jar via --jars option,


and run driver code using provided SparkContext instance, and see failures
from spark-shell console and executor logs.

below is stack traces,

org.apache.spark.SparkException: Job aborted due to stage failure:
Task 55 in stage 0.0 failed 4 times, most recent failure: Lost task
55.3 in stage 0.0 (TID 281, <svr hostname>):
java.lang.NoClassDefFoundError: Could not initialize class
org.apache.hadoop.hbase.client.HConnectionManager
    at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:197)
    at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:159)
    at org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(TableInputFormat.java:101)
    at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:128)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:70)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)


15/11/16 18:59:57 ERROR Executor: Exception in task 14.0 in stage 0.0 (TID 14)
java.lang.ExceptionInInitializerError
    at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:197)
    at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:159)
    at org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(TableInputFormat.java:101)
    at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:128)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:70)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: hbase-default.xml file seems to
be for and old version of HBase (null), this version is
0.98.6-cdh5.2.0
    at org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:73)
    at org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:105)
    at org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:116)
    at org.apache.hadoop.hbase.client.HConnectionManager.<clinit>(HConnectionManager.java:222)
    ... 18 more


Please note that it runs smoothly on spark-submit.

Btw, if issue is that hbase-default.xml is not properly loaded (maybe
because of classloader), it seems to run properly on driver level.

import org.apache.hadoop.hbase.HBaseConfiguration
val conf = HBaseConfiguration.create()
println(conf.get(""hbase.defaults.for.version""))

It prints ""0.98.6-cdh5.2.0"".

I'm using Spark-1.4.1-hadoop-2.4-bin, and zeppelin 0.5.5, and HBase
0.98.6-CDH5.2.0.

Thanks in advance!

Best,
Jungtaek Lim (HeartSaVioR)



-- 
Name : 임 정택
Blog : http://www.heartsavior.net / http://dev.heartsavior.net
Twitter : http://twitter.com/heartsavior
LinkedIn : http://www.linkedin.com/in/heartsavior
"
ddcd <zeeroo32@gmail.com>,"Wed, 18 Nov 2015 05:22:01 -0700 (MST)",How to Add builtin geometry type to SparkSQL?,dev@spark.apache.org,"Hi all,

I'm considering adding geometry type to SparkSQL. 

I know that there is a project named  sparkGIS
<https://github.com/drubbo/SparkGIS>  which is an add-on of sparkSQL. The
project uses user defined types and user defined functions. 

But I think a built-in type will be better in that we can make more
optimization. I'm reading the code of sparkSQL but it's really difficult for
me. Is there any document that helps?

Thank you!




--

---------------------------------------------------------------------


"
Cristian O <cristian.b.opris@googlemail.com>,"Wed, 18 Nov 2015 16:31:33 +0000",Re: Support for local disk columnar storage for DataFrames,Mark Hamstra <mark@clearstorydata.com>,"Hi,

While these OSS efforts are interesting, they're for now quite unproven.
Personally would be much more interested in seeing Spark incrementally
moving towards supporting updating DataFrames on various storage
substrates, and first of all locally, perhaps as an extension of cached
DataFrames.

However before we get full blown update support, I would suggest two
enhancements that are fairly straightforward with the current design. If
they make sense please let me know and I'll add them as Jiras:

1. Checkpoint support for DataFrames - as mentioned this can be as simple
as saving to a parquet file or some other format, but would not require
re-reading the file to alter the lineage, and would also prune the logical
plan. Alternatively checkpointing a cached DataFrame can delegate to
checkpointing the underlying RDD but again needs to prune the logical plan.

2. Efficient transformation of cached DataFrames to cached DataFrames - an
efficient copy-on-write mechanism can be used to avoid unpacking
CachedBatches (row groups) into InternalRows when building a cached
DataFrame out of a source cached DataFrame through transformations (like an
outer join) that only affect a small subset of rows. Statistics and
partitioning information can be used to determine which row groups are
affected and which can be copied *by reference* unchanged. This would
effectively allow performing immutable updates of cached DataFrames in
scenarios like Streaming or other iterative use cases like ML.

Thanks,
Cristian




"
Reynold Xin <rxin@databricks.com>,"Wed, 18 Nov 2015 09:19:36 -0800",Re: orc read issue n spark,Renu Yadav <yrenu21@gmail.com>,"What do you mean by starts delay scheduling? Are you saying it is no longer
doing local reads?

If that's the case you can increase the spark.locality.read timeout.


"
Reynold Xin <rxin@databricks.com>,"Wed, 18 Nov 2015 09:20:54 -0800",Re: How to Add builtin geometry type to SparkSQL?,ddcd <zeeroo32@gmail.com>,"Have you looked into https://github.com/harsha2010/magellan ?


"
Scott walent <scottwalent@gmail.com>,"Wed, 18 Nov 2015 18:58:45 +0000",Spark Summit East 2016 CFP - Closing in 5 days,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi Spark Devs and Users,

The CFP for Spark Summit East 2016 (https://spark-summit.org) is closing
this weekend. As the leading event for Apache Spark, this is the chance to
both share key learnings and to gain insights from the creators of Spark,
developers, vendors and peers who are using Spark.

We are looking for presenters who would like to showcase how Spark and its
related technologies are used in a variety of ways, including Applications,
Developer, Research, Data Science and our new in 2016 track, Enterprise.

Don’t wait! The call for presentations for Spark Summit East closes in less
than a week, November 22nd at 11:59 pm PST. Please visit our submission
page (https://spark-summit.org/east-2016/) for additional details.

Regards,
Spark Summit Organizers
"
Kostas Sakellis <kostas@cloudera.com>,"Wed, 18 Nov 2015 15:39:38 -0800",Re: A proposal for Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"A 1.6.x release will only fix bugs - we typically don't change APIs in z
releases. The Dataset API is experimental and so we might be changing the
APIs before we declare it stable. This is why I think it is important to
first stabilize the Dataset API with a Spark 1.7 release before moving to
Spark 2.0. This will benefit users that would like to use the new Dataset
APIs but can't move to Spark 2.0 because of the backwards incompatible
changes, like removal of deprecated APIs, Scala 2.11 etc.

Kostas



d
 to
we
e
ght
.
D
 the
park
..,
park
- so
fore
s --
0
 the
API
s or
:
’s
s of
t
rst,
cies
list
y
f
n
sten.
2.0
o be
move certain
t feature.
f
n
sten.
the
ei Zaharia <matei.zaharia@gmail.com> 写道：
ul
too.
e
e
.
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 18 Nov 2015 15:43:17 -0800",Re: A proposal for Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"Ah, got it; by ""stabilize"" you meant changing the API, not just bug
fixing.  We're on the same page now.


n
o
s to
 we
se
ight
g.
:
e
DD
o the
Spark
...,
.
spark
 - so
efore
Ds --
.0
s the
 API
es or
 issue for
h
, and
r
good
of
 2.0
to be
emove certain
at feature.
of
 the
tei Zaharia <matei.zaharia@gmail.com> 写道：
ful
h
I
 too.
we
s
ne
).
,
"
jeff saremi <jeffsaremi@hotmail.com>,"Wed, 18 Nov 2015 23:04:09 -0500",FW: SequenceFile and object reuse,"""dev@spark.apache.org"" <dev@spark.apache.org>","I sent this to the user forum. I got no responses. Could someone here please help? thanks
jeff

From: jeffsaremi@hotmail.com
To: user@spark.apache.org
Subject: SequenceFile and object reuse
Date: Fri, 13 Nov 2015 13:29:58 -0500




So we tried reading a sequencefile in Spark and realized that all our records have ended up becoming the same.
THen one of us found this:

Note: Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD or directly passing it to an aggregation or shuffle operation will create many references to the same object. If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first copy them using a map function.

Is there anyone that can shed some light on this bizzare behavior and the decisions behind it?
And I also would like to know if anyone's able to read a binary file and not to incur the additional map() as suggested by the above? What format did you use?

thanksJeff 		 	   		   		 	   		  "
Jeff Zhang <zjffdu@gmail.com>,"Thu, 19 Nov 2015 13:26:31 +0800",Re: FW: SequenceFile and object reuse,jeff saremi <jeffsaremi@hotmail.com>,"Would this be an issue on the raw data ? I use the following simple code,
and don't hit the issue you mentioned. Or it would be better to share your
code.

val rdd =sc.sequenceFile(""/Users/hadoop/Temp/Seq"",
classOf[IntWritable], classOf[Text])
rdd.map{case (k,v) => (k.get(), v.toString)}.collect() foreach println






-- 
Best Regards

Jeff Zhang
"
jeff saremi <jeffsaremi@hotmail.com>,"Thu, 19 Nov 2015 00:35:08 -0500",RE: SequenceFile and object reuse,Jeff Zhang <zjffdu@gmail.com>,"You're not seeing the issue because you perform one additional ""map"". 
map{case (k,v) => (k.get(), v.toString)}Instead of being able to use the read Text you had to create a tuple (single) out of the string of the text.

That is exactly why I asked this question.Why do we have t do this additional processing? What is the rationale behind it?
Is there other ways of reading a hadoop file (or any other file) that would not incur this additional step?thanks

Date: Thu, 19 Nov 2015 13:26:31 +0800
Subject: Re: FW: SequenceFile and object reuse
From: zjffdu@gmail.com
To: jeffsaremi@hotmail.com
CC: dev@spark.apache.org

Would this be an issue on the raw data ? I use the following simple code, and don't hit the issue you mentioned. Or it would be better to share your code. 
val rdd =sc.sequenceFile(""/Users/hadoop/Temp/Seq"", classOf[IntWritable], classOf[Text])
rdd.map{case (k,v) => (k.get(), v.toString)}.collect() foreach println



I sent this to the user forum. I got no responses. Could someone here please help? thanks
jeff

From: jeffsaremi@hotmail.com
To: user@spark.apache.org
Subject: SequenceFile and object reuse
Date: Fri, 13 Nov 2015 13:29:58 -0500




So we tried reading a sequencefile in Spark and realized that all our records have ended up becoming the same.
THen one of us found this:

Note: Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD or directly passing it to an aggregation or shuffle operation will create many references to the same object. If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first copy them using a map function.

Is there anyone that can shed some light on this bizzare behavior and the decisions behind it?
And I also would like to know if anyone's able to read a binary file and not to incur the additional map() as suggested by the above? What format did you use?

thanksJeff 		 	   		   		 	   		  


-- 
Best Regards

Jeff Zhang
 		 	   		  "
gsvic <victorasgs@gmail.com>,"Thu, 19 Nov 2015 00:02:39 -0700 (MST)",Hash Partitioning & Sort Merge Join,dev@spark.apache.org,"In case of Sort Merge join in which a shuffle (exchange) will be performed, I
have the following questions (Please correct me if my understanding is not
correct): 

Let's say that relation A is a JSONRelation (640 MB) on the HDFS where the
block size is 64MB. This will produce a Scan JSONRelation() of 10 partitions
( 640 / 64 ) where each of these partitions will contain |A| / 10 rows. 

The second step will be a hashPartitioning(#key, 200) where #key is the
equi-join condition and 200 the default number of shuffles
(spark.sql.shuffle.partitions). Each partition will be computed in an
individual task, in which every row will be hashed on the #key and then will
be written in the corresponing chunk (of 200 resulting chunks) directly on
disk. 

Q1: What happens if a resulting hashed row in the executor A must be written
in a chunk which is stored in the executor B? Does it use the
HashShuffleManager to transfer it over the network? 

Q2: After the Sort (3rd) step there will be 200, 200 resulting
partitions/chunks for relations A and B respectively which will be
concatenated into 200 SortMergeJoin tasks where each of them will contain
(|A|/200 + |B|/200) rows. For each pair (chunkOfA, chunkOfB) will chunkOfA
and chunkOfB contain rows for the same hash key ? 

Q3: In the SortMergeJoin of Q2, I suppose that each of the 200 SortMergeJoin
tasks joins two partitions/chunks with the same hash key. So, if a task
corresponds to a hash key X, does it use ShuffleBlockFetchIterator to fetch
the two Shuffles/Chunks (of relations A and B) with hash key X?

Q4: Which sorting algorithm is being used?



--

---------------------------------------------------------------------


"
<andrew.rowson@thomsonreuters.com>,"Thu, 19 Nov 2015 09:14:27 +0000",RE: SequenceFile and object reuse,"<jeffsaremi@hotmail.com>, <zjffdu@gmail.com>","As I understand it, it's down to how Hadoop FileInputFormats work, and
questions of mutability. If you were to read a file from Hadoop via an
InputFormat with a simple Java program, the InputFormat's RecordReader
creates a single, mutable instance of the Writable key class and a single,
mutable instance of the Writable value. When you loop through the records,
the RecordReader reuses those Writable instances by deserializing the
underlying bytes from the file into the instances 1 record at a time. It's
up to the application to then copy whatever's needed out of those Writable
instances into something else if it wants to do something with them.

 

It's exactly the same when using Spark as the application. When you create
an RDD of Writable objects by calling .sequenceFile, the RDD contains many
identical references to the exact same object instance. Therefore, when
Spark does a sort, cache or shuffle, (I believe) it optimizes because it
assumes that objects are immutable. Therefore, the map step is necessary,
because it creates a distinct, immutable copy of each record.

 

This is just an issue with the Hadoop InputFormat class. If you can write a
way of reading files from HDFS that don't use Hadoop's classes (though I'm
not sure why you would, a simple map is far easier), then the map would
potentially be unnecessary.

 

Andrew

 

From: jeff saremi [mailto:jeffsaremi@hotmail.com] 
Sent: 19 November 2015 05:35
To: Jeff Zhang <zjffdu@gmail.com>
Cc: dev@spark.apache.org
Subject: RE: SequenceFile and object reuse

 

You're not seeing the issue because you perform one additional ""map"". 

map{case (k,v) => (k.get(), v.toString)}

Instead of being able to use the read Text you had to create a tuple
(single) out of the string of the text.

That is exactly why I asked this question.

Why do we have t do this additional processing? What is the rationale behind
it?
Is there other ways of reading a hadoop file (or any other file) that would
not incur this additional step?

thanks

 

 

  _____  

Date: Thu, 19 Nov 2015 13:26:31 +0800
Subject: Re: FW: SequenceFile and object reuse
From: zjffdu@gmail.com <mailto:zjffdu@gmail.com> 
To: jeffsaremi@hotmail.com <mailto:jeffsaremi@hotmail.com> 
CC: dev@spark.apache.org <mailto:dev@spark.apache.org> 

Would this be an issue on the raw data ? I use the following simple code,
and don't hit the issue you mentioned. Or it would be better to share your
code. 

 

val rdd =sc.sequenceFile(""/Users/hadoop/Temp/Seq"", classOf[IntWritable],
classOf[Text])
rdd.map{case (k,v) => (k.get(), v.toString)}.collect() foreach println

 


I sent this to the user forum. I got no responses. Could someone here please
help? thanks
jeff

 


  _____  


From: jeffsaremi@hotmail.com <mailto:jeffsaremi@hotmail.com> 
To: user@spark.apache.org <mailto:user@spark.apache.org> 
Subject: SequenceFile and object reuse
Date: Fri, 13 Nov 2015 13:29:58 -0500

 

So we tried reading a sequencefile in Spark and realized that all our
records have ended up becoming the same.
THen one of us found this:

Note: Because Hadoop's RecordReader class re-uses the same Writable object
for each record, directly caching the returned RDD or directly passing it to
an aggregation or shuffle operation will create many references to the same
object. If you plan to directly cache, sort, or aggregate Hadoop writable
objects, you should first copy them using a map function.

Is there anyone that can shed some light on this bizzare behavior and the
decisions behind it?
And I also would like to know if anyone's able to read a binary file and not
to incur the additional map() as suggested by the above? What format did you
use?

thanks

Jeff





 

-- 

Best Regards

Jeff Zhang

"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 19 Nov 2015 12:42:47 +0100",Removing the Mesos fine-grained mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Mesos is the only cluster manager that has a fine-grained mode, but it's
more often than not problematic, and it's a maintenance burden. I'd like to
suggest removing it in the 2.0 release.

A few reasons:

- code/maintenance complexity. The two modes duplicate a lot of
functionality (and sometimes code) that leads to subtle differences or
bugs. See SPARK-10444 <https://issues.apache.org/jira/browse/SPARK-10444> and
also this thread
<https://mail-archives.apache.org/mod_mbox/spark-user/201510.mbox/%3CCALxMP-A+aygNwSiyTM8ff20-MGWHykbhct94a2hwZTh1jWHp_g@mail.gmail.com%3E>
 and MESOS-3202 <https://issues.apache.org/jira/browse/MESOS-3202>
- it's not widely used (Reynold's previous thread
<http://apache-spark-developers-list.1001551.n3.nabble.com/Please-reply-if-you-use-Mesos-fine-grained-mode-td14930.html>
got very few responses from people relying on it)
- similar functionality can be achieved with dynamic allocation +
coarse-grained mode

I suggest that Spark 1.6 already issues a warning if it detects
fine-grained use, with removal in the 2.0 release.

Thoughts?

iulian
"
Dean Wampler <deanwampler@gmail.com>,"Thu, 19 Nov 2015 07:25:09 -0600",Re: Removing the Mesos fine-grained mode,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Sounds like the right move. Simplifies things in important ways.

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com

.com>

to
 and
MP-A+aygNwSiyTM8ff20-MGWHykbhct94a2hwZTh1jWHp_g@mail.gmail.com%3E>
f-you-use-Mesos-fine-grained-mode-td14930.html>
"
"""Heller, Chris"" <cheller@akamai.com>","Thu, 19 Nov 2015 13:52:01 +0000",Re: Removing the Mesos fine-grained mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","I was one that argued for fine-grain mode, and there is something I still appreciate about how fine-grain mode operates in terms of the way one would define a Mesos framework. That said, with dyn-allocation and Mesos support for both resource reservation, oversubscription and revocation, I think the direction is clear that the coarse mode is the proper way forward, and having the two code paths is just noise.

-Chris

From: Iulian Dragoș <iulian.dragos@typesafe.com<mailto:iulian.dragos@typesafe.com>>
Date: Thursday, November 19, 2015 at 6:42 AM
To: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Removing the Mesos fine-grained mode

Hi all,

Mesos is the only cluster manager that has a fine-grained mode, but it's more often than not problematic, and it's a maintenance burden. I'd like to suggest removing it in the 2.0 release.

A few reasons:

- code/maintenance complexity. The two modes duplicate a lot of functionality (and sometimes code) that leads to subtle differences or bugs. See SPARK-10444<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D10444&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=4_2dJBDiLqTcfXfX1LZluOo1U6tRKR2wKGGzfwiKdVY&e=> and also this thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__mail-2Darchives.apache.org_mod-5Fmbox_spark-2Duser_201510.mbox_-253CCALxMP-2DA-2BaygNwSiyTM8ff20-2DMGWHykbhct94a2hwZTh1jWHp-5Fg-40mail.gmail.com-253E&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=SNFPzodGw7sgp3km9NKYM46gZHLguvxVNzCIeUlJzOw&e=> and MESOS-3202<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_MESOS-2D3202&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=d-U4CohYsiZc0Zmj4KETn2dT_2ZFe5s3_IIbMm2tjJo&e=>
- it's not widely used (Reynold's previous thread<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_Please-2Dreply-2Dif-2Dyou-2Duse-2DMesos-2Dfine-2Dgrained-2Dmode-2Dtd14930.html&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=HGMiKyzxFDhpbomduKVIIRHWk9RDGDCk7tneJVQqTwo&e=> got very few responses from people relying on it)
- similar functionality can be achieved with dynamic allocation + coarse-grained mode

I suggest that Spark 1.6 already issues a warning if it detects fine-grained use, with removal in the 2.0 release.

Thoughts?

iulian

"
"""james.green9@baesystems.com"" <james.green9@baesystems.com>","Thu, 19 Nov 2015 15:14:28 +0000",new datasource,"""dev@spark.apache.org"" <dev@spark.apache.org>","

We have written a new Spark DataSource that uses both Parquet and ElasticSearch.  It is based on the existing Parquet DataSource.   When I look at the filters being pushed down to buildScan I don’t get anything representing any filters based on UDFs – or for any fields generated by an explode – I had thought if I made it a CatalystScan I would get everything I needed.



This is fine from the Parquet point of view – but we are using ElasticSearch to index/filter the data we are searching and I need to be able to capture the UDF conditions – or have access to the Plan AST in order that I can construct a query for ElasticSearch.



I am thinking I might just need to patch Spark to do this – but I’d prefer not too if there is a way of getting round this without hacking the core code.  Any ideas?



Thanks



James



Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 19 Nov 2015 15:29:44 +0000",RE: new datasource,"""james.green9@baesystems.com"" <james.green9@baesystems.com>,
	""dev@spark.apache.org"" <dev@spark.apache.org>","I think you probably need to write some code as you need to support the ES, there are 2 options per my understanding:

Create a new Data Source from scratch, but you probably need to overwrite the interface at:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala#L751

Or you can reuse most of code in ParquetRelation in the new DataSource, but also need to modify your own logic, see
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRelation.scala#L285

Hope it helpful.

Hao
From: james.green9@baesystems.com [mailto:james.green9@baesystems.com]
Sent: Thursday, November 19, 2015 11:14 PM
To: dev@spark.apache.org
Subject: new datasource



We have written a new Spark DataSource that uses both Parquet and ElasticSearch.  It is based on the existing Parquet DataSource.   When I look at the filters being pushed down to buildScan I don’t get anything representing any filters based on UDFs – or for any fields generated by an explode – I had thought if I made it a CatalystScan I would get everything I needed.



This is fine from the Parquet point of view – but we are using ElasticSearch to index/filter the data we are searching and I need to be able to capture the UDF conditions – or have access to the Plan AST in order that I can construct a query for ElasticSearch.



I am thinking I might just need to patch Spark to do this – but I’d prefer not too if there is a way of getting round this without hacking the core code.  Any ideas?



Thanks



James


Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.
"
"""james.green9@baesystems.com"" <james.green9@baesystems.com>","Thu, 19 Nov 2015 16:55:52 +0000",RE: new datasource,"""Cheng, Hao"" <hao.cheng@intel.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Thanks Hao

I have written a new Data Source based on ParquetRelation and I have just retested what I had said about not getting anything extra when I change it over to a CatalystScan instead of PrunedFilteredScan and ooops it seems to work fine.





From: Cheng, Hao [mailto:hao.cheng@intel.com]
Sent: 19 November 2015 15:30
To: Green, James (UK Guildford); dev@spark.apache.org
Subject: RE: new datasource

I think you probably need to write some code as you need to support the ES, there are 2 options per my understanding:

Create a new Data Source from scratch, but you probably need to overwrite the interface at:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala#L751

Or you can reuse most of code in ParquetRelation in the new DataSource, but also need to modify your own logic, see
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRelation.scala#L285

Hope it helpful.

Hao
From: james.green9@baesystems.com<mailto:james.green9@baesystems.com> [mailto:james.green9@baesystems.com]
Sent: Thursday, November 19, 2015 11:14 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: new datasource



We have written a new Spark DataSource that uses both Parquet and ElasticSearch.  It is based on the existing Parquet DataSource.   When I look at the filters being pushed down to buildScan I don’t get anything representing any filters based on UDFs – or for any fields generated by an explode – I had thought if I made it a CatalystScan I would get everything I needed.



This is fine from the Parquet point of view – but we are using ElasticSearch to index/filter the data we are searching and I need to be able to capture the UDF conditions – or have access to the Plan AST in order that I can construct a query for ElasticSearch.



I am thinking I might just need to patch Spark to do this – but I’d prefer not too if there is a way of getting round this without hacking the core code.  Any ideas?



Thanks



James


Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.
Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.
"
Michael Armbrust <michael@databricks.com>,"Thu, 19 Nov 2015 10:52:24 -0800",Re: new datasource,"""james.green9@baesystems.com"" <james.green9@baesystems.com>","Yeah, CatalystScan should give you everything we can possibly push down in
raw form.  Note that this is not compatible across different spark versions.


t
o
pache/spark/sql/sources/interfaces.scala#L751
pache/spark/sql/execution/datasources/parquet/ParquetRelation.scala#L285
Search.  It is based on the existing Parquet DataSource.   When I look at the filters being pushed down to buildScan I don’t get anything representing any filters based on UDFs – or for any fields generated by an explode – I had thought if I made it a CatalystScan I would get everything I needed.
asticSearch to index/filter the data we are searching and I need to be able to capture the UDF conditions – or have access to the Plan AST in order that I can construct a query for ElasticSearch.
’d prefer not too if there is a way of getting round this without hacking the core code.  Any ideas?
e
e
"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Thu, 19 Nov 2015 21:21:35 +0000","spark-submit is throwing NPE when trying to submit a random forest
 model","""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Issue:
I have a random forest model that am trying to load during streaming using following code.  The code is working fine when I am running the code from Eclipse but getting NPE when running the code using spark-submit.

JavaStreamingContext jssc = new JavaStreamingContext(jsc, Durations.seconds(duration));
System.out.println(""&&&&&&&&&&&&&&&&&&&&& trying to get the context &&&&&&&&&&&&&&&&&&& "" );
final RandomForestModel model = RandomForestModel.load(jssc.sparkContext().sc(), MODEL_DIRECTORY);//line 116 causing the issue.
System.out.println(""&&&&&&&&&&&&&&&&&&&&& model debug &&&&&&&&&&&&&&&&&&&&&&& "" + model.toDebugString());


Exception Details:
INFO : org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool
Exception in thread ""main"" java.lang.NullPointerException
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$SplitData.toSplit(DecisionTreeModel.scala:144)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$$anonfun$16.apply(DecisionTreeModel.scala:291)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$$anonfun$16.apply(DecisionTreeModel.scala:291)
                        at scala.Option.map(Option.scala:145)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$.constructNode(DecisionTreeModel.scala:291)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$.constructNode(DecisionTreeModel.scala:286)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$.constructNode(DecisionTreeModel.scala:287)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$.constructNode(DecisionTreeModel.scala:286)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$.constructTree(DecisionTreeModel.scala:268)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$$anonfun$12.apply(DecisionTreeModel.scala:251)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$$anonfun$12.apply(DecisionTreeModel.scala:250)
                        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
                        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
                        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
                        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
                        at org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$.constructTrees(DecisionTreeModel.scala:250)
                        at org.apache.spark.mllib.tree.model.TreeEnsembleModel$SaveLoadV1_0$.loadTrees(treeEnsembleModels.scala:340)
                        at org.apache.spark.mllib.tree.model.RandomForestModel$.load(treeEnsembleModels.scala:72)
                        at org.apache.spark.mllib.tree.model.RandomForestModel.load(treeEnsembleModels.scala)
                        at com.markmonitor.antifraud.ce.KafkaURLStreaming.main(KafkaURLStreaming.java:116)
                        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
                        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                        at java.lang.reflect.Method.invoke(Method.java:606)
                        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
                        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
                        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
                        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
                        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Nov 19, 2015 1:10:56 PM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl

Spark Source Code:
case class PredictData(predict: Double, prob: Double) {
      def toPredict: Predict = new Predict(predict, prob)
}

Thanks,

Rachana


"
Reynold Xin <rxin@databricks.com>,"Thu, 19 Nov 2015 14:14:44 -0800",Dropping support for earlier Hadoop versions in Spark 2.0?,"""dev@spark.apache.org"" <dev@spark.apache.org>, Sean Owen <srowen@gmail.com>, 
	Thomas Graves <tgraves@apache.org>","I proposed dropping support for Hadoop 1.x in the Spark 2.0 email, and I
think everybody is for that.

https://issues.apache.org/jira/browse/SPARK-11807

Sean suggested also dropping support for Hadoop 2.2, 2.3, and 2.4. That is
to say, keep only Hadoop 2.6 and greater.

What are the community's thoughts on that?
"
Jo Voordeckers <jo.voordeckers@gmail.com>,"Thu, 19 Nov 2015 16:55:22 -0800",Re: Removing the Mesos fine-grained mode,"""Heller, Chris"" <cheller@akamai.com>","As a recent fine-grained mode adopter I'm now confused after reading this
and other resources from spark-summit, the docs, ...  so can someone please
advise me for our use-case?

We'll have 1 or 2 streaming jobs and an will run scheduled batch jobs which
should take resources away from the streaming jobs and give 'em back upon
completion.

Can someone point me at the docs or a guide to set this up?

Thanks!

- Jo Voordeckers



ld
t
he
to
_jira_browse_SPARK-2D10444&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=4_2dJBDiLqTcfXfX1LZluOo1U6tRKR2wKGGzfwiKdVY&e=> and
pache.org_mod-5Fmbox_spark-2Duser_201510.mbox_-253CCALxMP-2DA-2BaygNwSiyTM8ff20-2DMGWHykbhct94a2hwZTh1jWHp-5Fg-40mail.gmail.com-253E&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=SNFPzodGw7sgp3km9NKYM46gZHLguvxVNzCIeUlJzOw&e=>
_jira_browse_MESOS-2D3202&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=d-U4CohYsiZc0Zmj4KETn2dT_2ZFe5s3_IIbMm2tjJo&e=>
evelopers-2Dlist.1001551.n3.nabble.com_Please-2Dreply-2Dif-2Dyou-2Duse-2DMesos-2Dfine-2Dgrained-2Dmode-2Dtd14930.html&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=HGMiKyzxFDhpbomduKVIIRHWk9RDGDCk7tneJVQqTwo&e=>
"
Joseph Bradley <joseph@databricks.com>,"Thu, 19 Nov 2015 16:56:46 -0800","Re: spark-submit is throwing NPE when trying to submit a random
 forest model",Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Hi,
Could you please submit this via JIRA as a bug report?  It will be very
helpful if you include the Spark version, system details, and other info
too.
Thanks!
Joseph


"
"""=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?="" <zzq98736@alibaba-inc.com>","Fri, 20 Nov 2015 09:16:36 +0800","=?UTF-8?B?5Zue5aSNOkRyb3BwaW5nIHN1cHBvcnQgZm9yIGVhcmxpZXIgSGFkb29wIHZlcnNpb25zIGlu?=
  =?UTF-8?B?IFNwYXJrIDIuMD8=?=","""Reynold Xin"" <rxin@databricks.com>,
  ""dev@spark.apache.org"" <dev@spark.apache.org>,
  ""Sean Owen"" <srowen@gmail.com>,
  ""Thomas Graves"" <tgraves@apache.org>","I agreed
+1------------------------------------------------------------------发件人：Reynold Xin<rxin@databricks.com>日　期：2015年11月20日 06:14:44收件人：dev@spark.apache.org<dev@spark.apache.org>; Sean Owen<srowen@gmail.com>; Thomas Graves<tgraves@apache.org>主　题：Dropping support for earlier Hadoop versions in Spark 2.0?I proposed dropping support for Hadoop 1.x in the Spark 2.0 email, and I think everybody is for that.
https://issues.apache.org/jira/browse/SPARK-11807

Sean suggested also dropping support for Hadoop 2.2, 2.3, and 2.4. That is to say, keep only Hadoop 2.6 and greater.
What are the community's thoughts on that?


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 19 Nov 2015 17:23:16 -0800",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,=?UTF-8?B?5byg5b+X5by6KOaXuui9qSk=?= <zzq98736@alibaba-inc.com>,"Should a new job be setup under Spark-Master-Maven-with-YARN for hadoop
2.6.x ?

Cheers


.org>; Sean Owen<srowen@gmail.com>;
in Spark 2.0?
s
"
Henri Dubois-Ferriere <henridf@gmail.com>,"Thu, 19 Nov 2015 18:19:43 -0800",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Reynold Xin <rxin@databricks.com>,1
,"Fri, 20 Nov 2015 08:58:03 +0100",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,dev@spark.apache.org,"+1

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 20 Nov 2015 16:48:42 +0800",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,dev <dev@spark.apache.org>,"+1.

Hadoop 2.6 would be a good choice with many features added (like supporting
long running service, label based scheduling). Currently there's lot of
reflection codes to support multiple version of Yarn, so upgrading to a
newer version will really ease"
Steve Loughran <stevel@hortonworks.com>,"Fri, 20 Nov 2015 10:36:37 +0000",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Reynold Xin <rxin@databricks.com>,"

I proposed dropping support for Hadoop 1.x in the Spark 2.0 email, and I think everybody is for that.

https://issues.apache.org/jira/browse/SPARK-11807

Sean suggested also dropping support for Hadoop 2.2, 2.3, and 2.4. That is to say, keep only Hadoop 2.6 and greater.

What are the community's thoughts on that?


+1

It's the common APIs under pretty much shipping; EMR, CDH & HDP, and there's no significant API changes between it and 2.7. [There's a couple of extra records in job submissions in 2.7 which you can get at with reflection for AM failure reset window and rolling log capture patterns]. It's also getting some ongoing maintenance (2.6.3 being planned for dec).

It's not perfect; if I were to list troublespots to me they are : s3a isn't ready for use; there's better logging and tracing in later versions. But those aren't at the API level.
"
Cristian O <cristian.b.opris@googlemail.com>,"Fri, 20 Nov 2015 12:33:51 +0000",Re: Support for local disk columnar storage for DataFrames,dev@spark.apache.org,"Raised this for checkpointing, hopefully it gets some priority as it's very
useful and relatively straightforward to implement ?

https://issues.apache.org/jira/browse/SPARK-11879


"
chester@alpinenow.com,"Fri, 20 Nov 2015 06:28:30 -0800",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Reynold Xin <rxin@databricks.com>,"Assuming we have 1.6 and 1.7 releases, then spark 2.0 is about 9 months away. 

customer will need to upgrade the new Hadoop clusters to Apache 2.6 or later to leverage new spark 2.0 in one year. I think this possible as latest release on cdh5.x,  HDP 2.x are both on Apache 2.6.0 already. Company will have enough time to upgrade cluster.

+1 for me as well

Chester




Sent from my iPad

hink everybody is for that.
 to say, keep only Hadoop 2.6 and greater.
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 20 Nov 2015 15:52:00 +0000",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,"""chester@alpinenow.com"" <chester@alpinenow.com>","

Assuming we have 1.6 and 1.7 releases, then spark 2.0 is about 9 months away.

customer will need to upgrade the new Hadoop clusters to Apache 2.6 or later to leverage new spark 2.0 in one year. I think this possible as latest release on cdh5.x,  HDP 2.x are both on Apache 2.6.0 already. Company will have enough time to upgrade cluster.

+1 for me as well

Chester


now, if you are looking that far ahead, the other big issue is ""when to retire Java 7 support"".?

That's a tough decision for all projects. Hadoop 3.x will be Java 8 only, but nobody has committed the patch to the trunk codebase to force a java 8 build; + most of *todays* hadoop clusters are Java 7. But as you can't even download a Java 7 JDK for the desktop from oracle any more today, 2016 is a time to look at the language support and decide what is the baseline version

Commentary from Twitter here -as they point out, it's not just the server farm that matters, it's all the apps that talk to it


http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201503.mbox/%3CCAB7mwtE+kefcxsR6n46-ZTcS19ED7cWc9voBtR1jQEWDkye07g@mail.gmail.com%3E<http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201503.mbox/<CAB7mwtE+kefcxsR6n46-ZTcS19ED7cWc9voBtR1jQEWDkye07g@mail.gmail.com>>

-Steve
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Fri, 20 Nov 2015 16:53:41 +0100",Re: Removing the Mesos fine-grained mode,Jo Voordeckers <jo.voordeckers@gmail.com>,"This is a good point. We should probably document this better in the
migration notes. In the mean time:

http://spark.apache.org/docs/latest/running-on-mesos.html#dynamic-resource-allocation-with-mesos

Roughly, dynamic allocation lets Spark add and kill executors based on the
scheduling delay. The min and max number of executors can be configured.
Would this fit your use-case?

iulian



se
k
:
l
uld
rt
the
 to
g_jira_browse_SPARK-2D10444&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=4_2dJBDiLqTcfXfX1LZluOo1U6tRKR2wKGGzfwiKdVY&e=> and
apache.org_mod-5Fmbox_spark-2Duser_201510.mbox_-253CCALxMP-2DA-2BaygNwSiyTM8ff20-2DMGWHykbhct94a2hwZTh1jWHp-5Fg-40mail.gmail.com-253E&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=SNFPzodGw7sgp3km9NKYM46gZHLguvxVNzCIeUlJzOw&e=>
g_jira_browse_MESOS-2D3202&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=d-U4CohYsiZc0Zmj4KETn2dT_2ZFe5s3_IIbMm2tjJo&e=>
developers-2Dlist.1001551.n3.nabble.com_Please-2Dreply-2Dif-2Dyou-2Duse-2DMesos-2Dfine-2Dgrained-2Dmode-2Dtd14930.html&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=HGMiKyzxFDhpbomduKVIIRHWk9RDGDCk7tneJVQqTwo&e=>


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Reynold Xin <rxin@databricks.com>,"Fri, 20 Nov 2015 13:39:23 -0800",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Steve Loughran <stevel@hortonworks.com>,"OK I'm not exactly asking for a vote here :)

I don't think we should look at it from only maintenance point of view --
because in that case the answer is clearly supporting as few versions as
possible (or just rm -rf spark source code and call it a day). It is a
tradeoff between the number of users impacted and the maintenance burden.

So a few questions for those more familiar with Hadoop:

1. Can Hadoop 2.6 client read Hadoop 2.4 / 2.3?

2. If the answer to 1 is yes, are there known, major issues with backward
compatibility?

3. Can Hadoop 2.6+ YARN work on older versions of YARN clusters?

4. (for Hadoop vendors) When did/will support for Hadoop 2.4 and below
stop? To what extent do you care about running Spark on older Hadoop
clusters.




"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 20 Nov 2015 13:59:08 -0800",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Reynold Xin <rxin@databricks.com>,"To answer your fourth question from Cloudera's perspective, we would never
support a customer running Spark 2.0 on a Hadoop version < 2.6.

-Sandy


"
BenFradet <benjamin.fradet@gmail.com>,"Fri, 20 Nov 2015 15:39:33 -0700 (MST)",Unhandled case in VectorAssembler,dev@spark.apache.org,"Hey there,

I noticed that there is an unhandled case in the transform method of
VectorAssembler if one of the input columns doesn't have one of the
supported type DoubleType, NumericType, BooleanType or VectorUDT.

So, if you try to transform a column of StringType you get a cryptic
""scala.MatchError: StringType"".
I was wondering if we shouldn't throw a custom exception indicating that
this is not a supported type.

I can submit a jira and pr if needed.

Best regards,
Ben.




--

---------------------------------------------------------------------


"
Chester Chen <chester@alpinenow.com>,"Fri, 20 Nov 2015 15:02:17 -0800",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Sandy Ryza <sandy.ryza@cloudera.com>,"for #1-3, the answer is likely No.

  Recently we upgrade to Spark 1.5.1, with CDH5.3, CDH5.4 and HDP2.2  and
others.

  We were using CDH5.3 client to talk to CDH5.4. We were doing this to see
if we support many different hadoop cluster versions without changing the
build. This was ok for yarn-cluster spark 1.3.1, but could not get spark
1.5.1 started. We upgrade the client to CDH5.4, then everything works.

  There are API changes between Apache 2.4 and 2.6, not sure you can mix
match them.

Chester



"
Joseph Bradley <joseph@databricks.com>,"Fri, 20 Nov 2015 17:42:11 -0800",Re: Unhandled case in VectorAssembler,BenFradet <benjamin.fradet@gmail.com>,"Yes, please, could you send a JIRA (and PR)?  A custom error message would
be better.
Thank you!
Joseph


"
Adam McElwee <adam@mcelwee.me>,"Fri, 20 Nov 2015 20:37:20 -0600",Re: Removing the Mesos fine-grained mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've used fine-grained mode on our mesos spark clusters until this week,
mostly because it was the default. I started trying coarse-grained because
of the recent chatter on the mailing list about wanting to move the mesos
execution path to coarse-grained only. The odd things is, coarse-grained vs
fine-grained seems to yield drastic cluster utilization metrics for any of
our jobs that I've tried out this week.

If this is best as a new thread, please let me know, and I'll try not to
derail this conversation. Otherwise, details below:

We monitor our spark clusters with ganglia, and historically, we maintain
at least 90% cpu utilization across the cluster. Making a single
configuration change to use coarse-grained execution instead of
fine-grained consistently yields a cpu utilization pattern that starts
around 90% at the beginning of the job, and then it slowly decreases over
the next 1-1.5 hours to level out around 65% cpu utilization on the
cluster. Does anyone have a clue why I'd be seeing such a negative effect
of switching to coarse-grained mode? GC activity is comparable in both
cases. I've tried 1.5.2, as well as the 1.6.0 preview tag that's on github.

Thanks,
-Adam

.com>

e-allocation-with-mesos
e
s
ase
ck
 one
s
 I
s
e to
rg_jira_browse_SPARK-2D10444&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=4_2dJBDiLqTcfXfX1LZluOo1U6tRKR2wKGGzfwiKdVY&e=> and
.apache.org_mod-5Fmbox_spark-2Duser_201510.mbox_-253CCALxMP-2DA-2BaygNwSiyTM8ff20-2DMGWHykbhct94a2hwZTh1jWHp-5Fg-40mail.gmail.com-253E&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=SNFPzodGw7sgp3km9NKYM46gZHLguvxVNzCIeUlJzOw&e=>
rg_jira_browse_MESOS-2D3202&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=d-U4CohYsiZc0Zmj4KETn2dT_2ZFe5s3_IIbMm2tjJo&e=>
Ddevelopers-2Dlist.1001551.n3.nabble.com_Please-2Dreply-2Dif-2Dyou-2Duse-2DMesos-2Dfine-2Dgrained-2Dmode-2Dtd14930.html&d=CwMFaQ&c=96ZbZZcaMF4w0F4jpN6LZg&r=ylcFa5bBSUyTQqbx1Aqz47ec5BJJc7uk0YQ4EQKh-DY&m=36NeiiniCnBgPZ3AKAvvSJYBLQNxvpOcLoAi-VwXbtc&s=HGMiKyzxFDhpbomduKVIIRHWk9RDGDCk7tneJVQqTwo&e=>
"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Sat, 21 Nov 2015 11:13:11 +0100",Re: Unhandled case in VectorAssembler,Joseph Bradley <joseph@databricks.com>,"Will do, thanks for your input.

"
Sean Owen <sowen@cloudera.com>,"Sat, 21 Nov 2015 14:48:17 +0100",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Reynold Xin <rxin@databricks.com>,"
The upside to supporting only newer versions is less maintenance (no
small thing given how sprawling the build is), but also more ability
to use newer functionality. The downside is of course not letting
older Hadoop users use the latest Spark.



If the question is about HDFS, really, then I think the answer is
""yes"". The big compatibility problem has been protobuf but all of 2.2+
is on 2.5.



Same client/server question? This is where I'm not as clear. I think
the answer is 'yes' to the extent you're using functionality that
existed in the older YARN. Of course, using some newer API vs old
clusters doesn't work.



CDH 5.3 = Hadoop 2.6, FWIW, which was out about a year ago. Support
continues for a long time in the sense that CDH 5 will be supported
for years. However, Spark 2 would never be shipped / supported in CDH
5. So, it's not an issue for Spark 2; Spark 2 will be ""supported""
probably only vs Hadoop 3 or at least something later in 2.x than 2.6.

The question is here is really about whether Spark should specially
support, say, Spark 2 + CDH 5.0 or something. My experience so far is
that Spark has not really supported older vendor versions it claims
to, and I'd rather not pretend it does. So this doesn't strike me as a
great reason either.

This is roughly why supporting, say, 2.6 as a pretty safely recent
version seems like an OK place to draw the line 6-8 months from now.

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 21 Nov 2015 19:47:53 +0000",Re: Dropping support for earlier Hadoop versions in Spark 2.0?,Reynold Xin <rxin@databricks.com>,"
> On 20 Nov 2015, at 21:39, Reynold Xin <rxin@databricks.com> wrote:
> 
> OK I'm not exactly asking for a vote here :)
> 
> I don't think we should look at it from only maintenance point of view -- because in that case the answer is clearly supporting as few versions as possible (or just rm -rf spark source code and call it a day). It is a tradeoff between the number of users impacted and the maintenance burden.
> 
> So a few questions for those more familiar with Hadoop:
> 
> 1. Can Hadoop 2.6 client read Hadoop 2.4 / 2.3? 
> 

yes, at HDFS 

There's some special cases with HDFS stopping a 2.2-2.5 client talking to Hadoop 2.6


-HDFS at rest encryption needs a client that can decode it (2.6.x+)
-HDFS erasure code will need a later version (2.8?)

If you turn SASL on in your datanodes, your DNs don't need to come up on a port < 1024, but Hadoop  < 2.6 clients stop being able to work with HDFS at that point



> 2. If the answer to 1 is yes, are there known, major issues with backward compatibility?
> 

hadoop native libs, every time. Guava, jackson and protobuf can be managed with shading, but hadoop.{so,dll} is a real problem. A hadoop-2.6 JAR will use native methods in hadoop.lib which, if not loaded, will break the app.  This is a pain as nobody includes that native lib with their java binaries —who can even predict which one they have to do. As a consequence, I'd really advise against trying to run an app built with the 2.6 JARS inside a YARN cluster  < 2.6. You can certainly talk to HDFS and the YARN services, but there's a risk a codepath will hit a native method that isn't there.


It's trouble the other way too.  -even though we try not break existing code by moving/renaming native methods it can happen.

The last time someone did this in a big way, I was the first to find it in HADOOP-11064; the changes where reverted/altered but there was no official declaration that compatibility at the JNI layer will be maintained. Apparently you can't guarantee it over JVM versions either.

We really need a lib versioning story, which is what HADOOP-11127 covers.

> 3. Can Hadoop 2.6+ YARN work on older versions of YARN clusters?
> 

I'd say no, with classpath and hadoop native being the failure points.

There's also feature completeness; Hadoop 2.6 was the first version with all the YARN-896 work for long-lived services


> 4. (for Hadoop vendors) When did/will support for Hadoop 2.4 and below stop? To what extent do you care about running Spark on older Hadoop clusters.
> 
> 

I don't know. And I probably don't want to make any forward looking statements anyway. But I don't even know how well supported 2.4 is today; 2.6 is the one that still gets bug fixes out from the ASF. I can see it lasting a while.


What essentially happens is that we provide bug fixes to the existing releases, but for anything new: upgrade.

Assuming that policy continues (disclaimer: personal opinions, etc), then any Spark 2.0 release would be rebuilt against all the JARs which the rest of that version of HDP would use, and that's the only version we'd recommend using.



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
bowen zhang <bowenzhangusa@yahoo.com.INVALID>,"Sat, 21 Nov 2015 23:38:30 +0000 (UTC)",Using spark MLlib without installing Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi folks,I am a big fan of Spark's Mllib package. I have a java web app where I want to run some ml jobs inside the web app. My question is: is there a way to just import spark-core and spark-mllib jars to invoke my ML jobs without installing the entire Spark package? All the tutorials related Spark seems to indicate installing Spark is a pre-condition for this.
Thanks,Bowen
"
Rad Gruchalski <radek@gruchalski.com>,"Sun, 22 Nov 2015 00:43:22 +0100",Re: Using spark MLlib without installing Spark,bowen zhang <bowenzhangusa@yahoo.com>,"Bowen,  

trella/spark-notebook
It uses Spark you in the way you intend to use it.










Kind regards, 
Radek Gruchalski
 radek@gruchalski.com (mailto:radek@gruchalski.com)  (mailto:radek@gruchalski.com)
de.linkedin.com/in/radgruchalski/ (http://de.linkedin.com/in/radgruchalski/)

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.




want to run some ml jobs inside the web app. My question is: is there a way to just import spark-core and spark-mllib jars to invoke my ML jobs without installing the entire Spark package? All the tutorials related Spark seems to indicate installing Spark is a pre-condition for this.

"
bowen zhang <bowenzhangusa@yahoo.com.INVALID>,"Sun, 22 Nov 2015 00:01:08 +0000 (UTC)",Re: Using spark MLlib without installing Spark,Rad Gruchalski <radek@gruchalski.com>,"Thanks Rad for info. I looked into the repo and see some .snb file using spark mllib. Can you give me a more specific place to look for when invoking the mllib functions? What if I just want to invoke some of the ML functions in my HelloWorld.java?

      From: Rad Gruchalski <radek@gruchalski.com>
 To: bowen zhang <bowenzhangusa@yahoo.com> 
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>
 Sent: Saturday, November 21, 2015 3:43 PM
 Subject: Re: Using spark MLlib without installing Spark
   
Bowen, 
ypetrella/spark-notebookIt uses Spark you in the way you intend to use it.    Kindregards, 
RadekGruchalski
 radek@gruchalski.com 
de.linkedin.com/in/radgruchalski/

Confidentiality:
Thiscommunication is intended for the above-named person and may beconfidential and/or legally privileged.
If it has come to you inerror you must take no action based on it, nor must you copy or showit to anyone; please delete/destroy and inform the senderimmediately. 

 Hi folks,I am a big fan of Spark's Mllib package. I have a java web app where I want to run some ml jobs inside the web app. My question is: is there a way to just import spark-core and spark-mllib jars to invoke my ML jobs without installing the entire Spark package? All the tutorials related Spark seems to indicate installing Spark is a pre-condition for this.
Thanks,Bowen
 
  
 

  "
Rad Gruchalski <radek@gruchalski.com>,"Sun, 22 Nov 2015 01:05:39 +0100",Re: Using spark MLlib without installing Spark,bowen zhang <bowenzhangusa@yahoo.com>,"Bowen,  

What Andy is doing in the notebook is a slightly different thing. He’s using sbt to bring all spark jars (core, mllib, repl, what have you). You could use maven for that. He then creates a repl and submits all the spark code into it.
Pretty sure spark unit tests cover similar uses cases. Maybe not mllib per se but this kind of submission.










Kind regards, 
Radek Gruchalski
 radek@gruchalski.com (mailto:radek@gruchalski.com)  (mailto:radek@gruchalski.com)
de.linkedin.com/in/radgruchalski/ (http://de.linkedin.com/in/radgruchalski/)

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.




g spark mllib. Can you give me a more specific place to look for when invoking the mllib functions? What if I just want to invoke some of the ML functions in my HelloWorld.java?
ki.com)>
o.com)>  
spark.apache.org (mailto:dev@spark.apache.org)>
petrella/spark-notebook
 (mailto:radek@gruchalski.com)
ski/)
nfidential and/or legally privileged.
 must you copy or show it to anyone; please delete/destroy and inform the sender immediately.  
I want to run some ml jobs inside the web app. My question is: is there a way to just import spark-core and spark-mllib jars to invoke my ML jobs without installing the entire Spark package? All the tutorials related Spark seems to indicate installing Spark is a pre-condition for this.

"
Reynold Xin <rxin@databricks.com>,"Sat, 21 Nov 2015 20:03:46 -0800",Re: Using spark MLlib without installing Spark,Rad Gruchalski <radek@gruchalski.com>,"You can use MLlib and Spark directly without ""installing anything"". Just
run Spark in local mode.



s
k
r
L
ay
ut
s
"
Michael Armbrust <michael@databricks.com>,"Sun, 22 Nov 2015 14:21:52 -0800",[ANNOUNCE] Spark 1.6.0 Release Preview,"""dev@spark.apache.org"" <dev@spark.apache.org>","In order to facilitate community testing of Spark 1.6.0, I'm excited to
announce the availability of an early preview of the release. This is not a
release candidate, so there is no voting involved. However, it'd be awesome
if community members can start testing with this preview package and report
any problems they encounter.

This preview package contains all the commits to branch-1.6
<https://github.com/apache/spark/tree/branch-1.6> till commit
308381420f51b6da1007ea09a02d740613a226e0
<https://github.com/apache/spark/tree/v1.6.0-preview2>.

The staging maven repository for this preview build can be found here:
https://repository.apache.org/content/repositories/orgapachespark-1162

Binaries for this preview build can be found here:
http://people.apache.org/~pwendell/spark-releases/spark-v1.6.0-preview2-bin/

A build of the docs can also be found here:
http://people.apache.org/~pwendell/spark-releases/spark-v1.6.0-preview2-docs/

The full change log for this release can be found on JIRA
<https://issues.apache.org/jira/browse/SPARK-11908?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.6.0>
.

*== How can you help? ==*

If you are a Spark user, you can help us test this release by taking a
Spark workload and running on this preview release, then reporting any
regressions.

*== Major Features ==*

When testing, we'd appreciate it if users could focus on areas that have
changed in this release.  Some notable new features include:

SPARK-11787 <https://issues.apache.org/jira/browse/SPARK-11787> *Parquet
Performance* - Improve Parquet scan performance when using flat schemas.
SPARK-10810 <https://issues.apache.org/jira/browse/SPARK-10810> *Session *
*Management* - Multiple users of the thrift (JDBC/ODBC) server now have
isolated sessions including their own default database (i.e USE mydb) even
on shared clusters.
SPARK-9999  <https://issues.apache.org/jira/browse/SPARK-9999> *Dataset API* -
A new, experimental type-safe API (similar to RDDs) that performs many
operations on serialized binary data and code generation (i.e. Project
Tungsten)
SPARK-10000 <https://issues.apache.org/jira/browse/SPARK-10000> *Unified
Memory Management* - Shared memory for execution and caching instead of
exclusive division of the regions.
SPARK-10978 <https://issues.apache.org/jira/browse/SPARK-10978> *Datasource
API Avoid Double Filter* - When implementing a datasource with filter
pushdown, developers can now tell Spark SQL to avoid double evaluating a
pushed-down filter.
SPARK-2629  <https://issues.apache.org/jira/browse/SPARK-2629> *New
improved state management* - trackStateByKey - a DStream transformation for
stateful stream processing, supersedes updateStateByKey in functionality
and performance.

Happy testing!

Michael
"
Luciano Resende <luckbr1975@gmail.com>,"Sun, 22 Nov 2015 18:49:03 -0800",Re: Bringing up JDBC Tests to trunk,Josh Rosen <rosenville@gmail.com>,"Hey Josh,

Thanks for helping bringing this up, I have just pushed a WIP PR for
bringing the DB2 tests to be running on Docker, and I have a question about
how the jdbc drivers are actually being setup for the other datasources
(MySQL and PostgreSQL), are these setup directly on the Jenkins slaves ? I
didn't see the jars or anything specific on the pom or other files...


Thanks




-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 23 Nov 2015 14:36:18 +0100",Re: Removing the Mesos fine-grained mode,Adam McElwee <adam@mcelwee.me>,"

I think it's ok to discuss it here.



I'm not very familiar with Ganglia, and how it computes utilization. But
one thing comes to mind: did you enable dynamic allocation
<https://spark.apache.org/docs/latest/running-on-mesos.html#dynamic-resource-allocation-with-mesos>
on coarse-grained mode?

iulian
"
Jerry Lam <chilinglam@gmail.com>,"Mon, 23 Nov 2015 09:24:36 -0500",Re: Removing the Mesos fine-grained mode,=?utf-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Hi guys,

Can someone confirm if it is true that dynamic allocation on mesos ""is designed to run one executor per slave with the configured amount of resources."" I copied this sentence from the documentation. Does this mean there is at most 1 executor per node? Therefore,  if you have a big machine, you need to allocate a fat executor on this machine in order to fully utilize it?

Best Regards,

Sent from my iPhone

ostly because it was the default. I started trying coarse-grained because of the recent chatter on the mailing list about wanting to move the mesos execution path to coarse-grained only. The odd things is, coarse-grained vs fine-grained seems to yield drastic cluster utilization metrics for any of our jobs that I've tried out this week.
erail this conversation. Otherwise, details below:
 at least 90% cpu utilization across the cluster. Making a single configuration change to use coarse-grained execution instead of fine-grained consistently yields a cpu utilization pattern that starts around 90% at the beginning of the job, and then it slowly decreases over the next 1-1.5 hours to level out around 65% cpu utilization on the cluster. Does anyone have a clue why I'd be seeing such a negative effect of switching to coarse-grained mode? GC activity is comparable in both cases. I've tried 1.5.2, as well as the 1.6.0 preview tag that's on github.
ne thing comes to mind: did you enable dynamic allocation on coarse-grained mode?
"
jan <jan@insidin.com>,"Mon, 23 Nov 2015 08:22:18 -0700 (MST)",Spark-1.6.0-preview2 trackStateByKey exception restoring state,dev@spark.apache.org,"Hi guys,

I'm trying out the new trackStateByKey API of the Spark-1.6.0-preview2
release and I'm encountering an exception when trying to restore previously
checkpointed state in spark streaming.

Use case:
- execute a stateful Spark streaming job using trackStateByKey
- interrupt / kill the job
- start the job again (without any code changes or cleaning out the
checkpoint directory)

Upon this restart, I encounter the exception below. The nature of the
exception makes me think either I am doing something wrong, or there's a
problem with this use case for the new trackStateByKey API.

I uploaded my job code (
https://gist.github.com/juyttenh/be7973b0c5c2eddd8a81), but it's basically
just a modified version of the spark streaming example
StatefulNetworkWordCount (that had already been updated to use
trackStateByKey). My version however includes the usage of
StreamingContext.getOrCreate to actually read the checkpointed state when
the job is started, leading to the exception below.

Just to make sure: using StreamingContext.getOrCreate should still be
compatible with using the trackStateByKey API?

Thanx,
Jan

15/11/23 10:55:07 ERROR StreamingContext: Error starting the context,
marking it as stopped

java.lang.IllegalArgumentException: requirement failed

at scala.Predef$.require(Predef.scala:221)

at
org.apache.spark.streaming.rdd.TrackStateRDD.<init>(TrackStateRDD.scala:133)

at
org.apache.spark.streaming.dstream.InternalTrackStateDStream$$anonfun$compute$2.apply(TrackStateDStream.scala:148)

at
org.apache.spark.streaming.dstream.InternalTrackStateDStream$$anonfun$compute$2.apply(TrackStateDStream.scala:143)

at scala.Option.map(Option.scala:145)

at
org.apache.spark.streaming.dstream.InternalTrackStateDStream.compute(TrackStateDStream.scala:143)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)

at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)

at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:424)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)

at scala.Option.orElse(Option.scala:257)

at
org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)

at
org.apache.spark.streaming.dstream.TrackStateDStreamImpl.compute(TrackStateDStream.scala:66)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)

at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)

at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:424)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)

at scala.Option.orElse(Option.scala:257)

at
org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)

at
org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:47)

at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:115)

at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:114)

at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)

at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)

at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)

at
org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:114)

at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$restart$4.apply(JobGenerator.scala:231)

at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$restart$4.apply(JobGenerator.scala:226)

at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)

at
org.apache.spark.streaming.scheduler.JobGenerator.restart(JobGenerator.scala:226)

at
org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:96)

at
org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:83)

at
org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:609)

at
org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:605)

at
org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:605)

at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()

at
org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:605)

at
org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:599)

at
org.apache.spark.examples.streaming.StatefulNetworkWordCount$.main(StatefulNetworkWordCount.scala:48)

at
org.apache.spark.examples.streaming.StatefulNetworkWordCount.main(StatefulNetworkWordCount.scala)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:483)

at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:727)

at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)

at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)

at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)

at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)




--"
Adam McElwee <adam@mcelwee.me>,"Mon, 23 Nov 2015 11:27:46 -0600",Re: Removing the Mesos fine-grained mode,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,".com>

se
s
 vs
of
n
r
t
ub.
rce-allocation-with-mesos>

Dynamic allocation is definitely not enabled. The only delta between runs
is adding --conf ""spark.mesos.coarse=true"" the job submission. Ganglia is
just pulling stats from the procfs, and I've never seen it report bad
results. If I sample any of the 100-200 nodes in the cluster, dstat
reflects the same average cpu that I'm seeing reflected in ganglia.

"
Andrew Or <andrew@databricks.com>,"Mon, 23 Nov 2015 11:41:56 -0800",Re: Removing the Mesos fine-grained mode,Adam McElwee <adam@mcelwee.me>,"@Jerry Lam

Can someone confirm if it is true that dynamic allocation on mesos ""is


Mesos inherently does not support multiple executors per slave currently.
This is actually not related to dynamic allocation. There is, however, an
outstanding patch to a"
Jerry Lam <chilinglam@gmail.com>,"Mon, 23 Nov 2015 14:55:40 -0500",Re: Removing the Mesos fine-grained mode,Andrew Or <andrew@databricks.com>,"Hi Andrew,

Thank you for confirming this. I’m referring to this because I used fine-grained mode before and it was a headache because of the memory issue. Therefore, I switched to Yarn with dynamic allocation. I was thinking if I can switch back to Mesos with coarse-grained mode + dynamic allocation but from what you explain to me, I still cannot have more than 1 executor per slave. This sounds like a deal breaker for me because if I have a slave of 100GB of RAM and a slave of 30GB, I cannot utilize the instance of 100GB of RAM fully if I specify spark.executor.memory = 20GB. The two slaves will each consume 20GB in this case even though there is 80GB left for the bigger machine. If I specify 90GB for spark.executor.memory, the only active slave is the one with 100GB. Therefore the slave with 30GB will be idled. 

Do you know the link to the JIRA that I can receive update for the feature you mention? We have intentions to use Mesos but it is proven difficult with our tight budget constraint. 

Best Regards,

Jerry


designed to run one executor per slave with the configured amount of resources."" I copied this sentence from the documentation. Does this mean there is at most 1 executor per node? Therefore,  if you have a big machine, you need to allocate a fat executor on this machine in order to fully utilize it?
currently. This is actually not related to dynamic allocation. There is, however, an outstanding patch to add support for multiple executors per slave. When that feature is merged, it will work well with dynamic allocation.
<mailto:adam@mcelwee.me>>:
week, mostly because it was the default. I started trying coarse-grained because of the recent chatter on the mailing list about wanting to move the mesos execution path to coarse-grained only. The odd things is, coarse-grained vs fine-grained seems to yield drastic cluster utilization metrics for any of our jobs that I've tried out this week.
to derail this conversation. Otherwise, details below:
maintain at least 90% cpu utilization across the cluster. Making a single configuration change to use coarse-grained execution instead of fine-grained consistently yields a cpu utilization pattern that starts around 90% at the beginning of the job, and then it slowly decreases over the next 1-1.5 hours to level out around 65% cpu utilization on the cluster. Does anyone have a clue why I'd be seeing such a negative effect of switching to coarse-grained mode? GC activity is comparable in both cases. I've tried 1.5.2, as well as the 1.6.0 preview tag that's on github.
But one thing comes to mind: did you enable dynamic allocation <https://spark.apache.org/docs/latest/running-on-mesos.html#dynamic-resource-allocation-with-mesos> on coarse-grained mode?
runs is adding --conf ""spark.mesos.coarse=true"" the job submission. Ganglia is just pulling stats from the procfs, and I've never seen it report bad results. If I sample any of the 100-200 nodes in the cluster, dstat reflects the same average cpu that I'm seeing reflected in ganglia.

"
Jerry Lam <chilinglam@gmail.com>,"Mon, 23 Nov 2015 15:02:29 -0500",Re: Removing the Mesos fine-grained mode,Andrew Or <andrew@databricks.com>,"@Andrew Or

I assume you are referring to this ticket [SPARK-5095]: https://issues.apache.org/jira/browse/SPARK-5095 <https://issues.apache.org/jira/browse/SPARK-5095> 
Thank you!

Best Regards,

Jerry

designed to run one executor per slave with the conf"
mkhaitman <mark.khaitman@chango.com>,"Mon, 23 Nov 2015 13:49:02 -0700 (MST)",Re: [ANNOUNCE] Spark 1.6.0 Release Preview,dev@spark.apache.org,"Nice! Built and testing on CentOS 7 on a Hadoop 2.7.1 cluster.

intended? I starting typing a line out and then changed my mind and wanted
to issue the good old ctrl+c to interrupt, but that didn't work.

Otherwise haven't seen any major issues yet!

Mark.



--

---------------------------------------------------------------------


"
Nezih <nyigitbasi@netflix.com>,"Mon, 23 Nov 2015 14:24:01 -0700 (MST)",question about combining small input splits,dev@spark.apache.org,"Hi Spark Devs,
I tried getting an answer to my question in the user mailing list, but so
far couldn't. That's why I wanted to try the dev mailing list too in case
someone can help me.

I have a Hive table that has a lot of small parquet files and I am creating
a data frame out of it to do some processing, but since I have a large
number of splits/files my job creates a lot of tasks, which I don't want.
Basically what I want is the same functionality that Hive provides, that is,
to combine these small input splits into larger ones by specifying a max
split size setting. Is this currently possible with Spark?

I look at coalesce() but with coalesce I can only control the number of
output files not their sizes. And since the total input dataset size can
vary significantly in my case, I cannot just use a fixed partition count as
the size of each output file can get very large. I then looked for getting
the total input size from an rdd to come up with some heuristic to set the
partition count, but I couldn't find any ways to do it.

Any help is appreciated.

Thanks,

Nezih



--

---------------------------------------------------------------------


"
Dean Wampler <deanwampler@gmail.com>,"Mon, 23 Nov 2015 15:36:07 -0600",Re: [ANNOUNCE] Spark 1.6.0 Release Preview,Michael Armbrust <michael@databricks.com>,"I'm seeing an RPC timeout with the 2.11 build, but not the Hadoop1, 2.10
build: The following session with two uses of sc.parallize causes it almost
every the time. Occasionally I don't see the stack trace and I don't see it
with just a single sc.parallize, even the bigger, second one. When the
error occurs, it does pause for about two minutes with no output before the
stack trace. I elided some output; why all the non-log4j warnings occur at
startup is another question:


$ pwd
/Users/deanwampler/projects/spark/spark-1.6.0-bin-hadoop1-scala2.11
$ ./bin/spark-shell
log4j:WARN No appenders could be found for logger
(org.apache.hadoop.security.Groups).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for
more info.
Using Spark's repl log4j profile:
org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel(""INFO"")
Spark context available as sc.
15/11/23 13:01:45 WARN Connection: BoneCP specified but not present in
CLASSPATH (or one of dependencies)
15/11/23 13:01:45 WARN Connection: BoneCP specified but not present in
CLASSPATH (or one of dependencies)
15/11/23 13:01:49 WARN ObjectStore: Version information not found in
metastore. hive.metastore.schema.verification is not enabled so recording
the schema version 1.2.0
15/11/23 13:01:49 WARN ObjectStore: Failed to get database default,
returning NoSuchObjectException
15/11/23 13:01:49 WARN NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable
15/11/23 13:01:50 WARN Connection: BoneCP specified but not present in
CLASSPATH (or one of dependencies)
15/11/23 13:01:50 WARN Connection: BoneCP specified but not present in
CLASSPATH (or one of dependencies)
SQL context available as sqlContext.
Welcome to
      ____              __
           / __/__  ___ _____/ /__
         _\ \/ _ \/ _ `/ __/  '_/
       /___/ .__/\_,_/_/ /_/\_\   version 1.6.0-SNAPSHOT
         /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.parallelize((1 to 100000), 10000).count()

[Stage 0:>                                                      (0 + 0) /
10000]
[Stage 0:==>                                                  (420 + 4) /
10000]
[Stage 0:===>                                                 (683 + 4) /
10000]
... elided ...
[Stage 0:==========================================>         (8264 + 4) /
10000]
[Stage 0:==============================================>     (8902 + 6) /
10000]
[Stage 0:=================================================>  (9477 + 4) /
10000]

res0: Long = 100000

scala> sc.parallelize((1 to 1000000), 100000).count()

[Stage 1:>                                                     (0 + 0) /
100000]
[Stage 1:>                                                     (0 + 0) /
100000]
[Stage 1:>                                                     (0 + 0) /
100000]15/11/23 13:04:09 WARN NettyRpcEndpointRef: Error sending message
[message = Heartbeat(driver,[Lscala.Tuple2;@7f9d659c,BlockManagerId(driver,
localhost, 55188))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120
seconds]. This timeout is controlled by spark.rpc.askTimeout
at org.apache.spark.rpc.RpcTimeout.org
$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
  at
org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
  at
org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
  at
scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
  at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
  at
org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
  at
org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
  at org.apache.spark.executor.Executor.org
$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:452)
  at
org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:472)
  at
org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:472)
  at
org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:472)
  at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1708)
  at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:472)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
  at
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
  at
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
  at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:744)
    Caused by: java.util.concurrent.TimeoutException: Futures timed out
after [120 seconds]
at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
  at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
  at
  at scala.concurrent.Await$.result(package.scala:190)
  at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
  ... 15 more
    15/11/23 13:04:56 WARN NettyRpcEndpointRef: Ignore message
Success(HeartbeatResponse(false))

[Stage 1:=>                                                 (2204 + 6) /
100000]
[Stage 1:=>                                                 (2858 + 4) /
100000]
[Stage 1:=>                                                 (3616 + 5) /
100000]
... elided ...
[Stage 1:=================================================>(98393 + 4) /
100000]
[Stage 1:=================================================>(99347 + 4) /
100000]
[Stage 1:=================================================>(99734 + 4) /
100000]

res1: Long = 1000000

scala>


Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Tathagata Das <tdas@databricks.com>,"Mon, 23 Nov 2015 14:26:15 -0800",Re: Spark-1.6.0-preview2 trackStateByKey exception restoring state,jan <jan@insidin.com>,"My intention is to make it compatible! Filed this bug -
https://issues.apache.org/jira/browse/SPARK-11932
Looking into it right now. Thanks for testing it out and reporting this!



"
huan zhang <zhanghuan929@gmail.com>,"Tue, 24 Nov 2015 09:36:58 +0800",why does shuffle in spark write shuffle data to disk by default?,dev@spark.apache.org,"Hi All,
    I'm wonderring why does shuffle in spark write shuffle data to disk by
default?
    In Stackoverflow, someone said it's used by FTS, but node down is the
most common reason of fault, and write to disk cannot do FTS in this case
either.
    So why not use ramdisk as default instread of SDD or HDD only?

Thanks
Hubert Zhang
"
Jakob Odersky <jodersky@gmail.com>,"Mon, 23 Nov 2015 17:59:28 -0800",Datasets on experimental dataframes?,dev <dev@spark.apache.org>,"Hi,

datasets are being built upon the experimental DataFrame API, does this
mean DataFrames won't be experimental in the near future?

thanks,
--Jakob
"
Reynold Xin <rxin@databricks.com>,"Mon, 23 Nov 2015 18:00:31 -0800",Re: Datasets on experimental dataframes?,Jakob Odersky <jodersky@gmail.com>,"The experimental tag is intended for user facing APIs. It has nothing to do
with internal dependencies.


"
Reynold Xin <rxin@databricks.com>,"Mon, 23 Nov 2015 19:27:04 -0800",Re: why does shuffle in spark write shuffle data to disk by default?,huan zhang <zhanghuan929@gmail.com>,"I think for most jobs the bottleneck isn't in writing shuffle data to disk,
since shuffle data needs to be ""shuffled"" and sent across the network.

You can always use a ramdisk yourself. Requiring ramdisk by default would
significantly complicate configuration and platform portability.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 24 Nov 2015 04:18:05 +0000",Fastest way to build Spark from scratch,Spark dev list <dev@spark.apache.org>,"Say I want to build a complete Spark distribution against Hadoop 2.6+ as
fast as possible from scratch.

This is what I’m doing at the moment:

./make-distribution.sh -T 1C -Phadoop-2.6

-T 1C instructs Maven to spin up 1 thread per available core. This takes
around 20 minutes on an m3.large instance.

I see that spark-ec2, on the other hand, builds Spark as follows
<https://github.com/amplab/spark-ec2/blob/a990752575cd8b0ab25731d7820a55c714798ec3/spark/init.sh#L21-L22>
when you deploy Spark at a specific git commit:

sbt/sbt clean assembly
sbt/sbt publish-local

This seems slower than using make-distribution.sh, actually.

Is there a faster way to do this?

Nick
​
"
Fengdong Yu <fengdongy@everstring.com>,"Tue, 24 Nov 2015 13:19:02 +0800",Re: load multiple directory using dataframe load,Renu Yadav <yrenu21@gmail.com>,"hiveContext.read.format(“orc”).load(“bypath/*”)



directory


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 23 Nov 2015 22:51:50 -0800",Re: A proposal for Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"I actually think the next one (after 1.6) should be Spark 2.0. The reason
is that I already know we have to break some part of the DataFrame/Dataset
API as part of the Dataset design. (e.g. DataFrame.map should return
Dataset rather than RDD). In that case, I'd rather break this sooner (in
one release) than later (in two releases). so the damage is smaller.

I don't think whether we call Dataset/DataFrame experimental or not matters
too much for 2.0. We can still call Dataset experimental in 2.0 and then
mark them as stable in 2.1. Despite being ""experimental"", there has been no
breaking changes to DataFrame from 1.3 to 1.6.




e
o
t
ike
ow us
t we
ase
might
ng.
, as
s
 Spark
 ...,
e.
m
n
 spark
r - so
e
before
DDs --
2.0
as the
,
l API
mes or
l issue for
th
t, and
or
 good
 of
.
y 2.0
 to be
remove certain
hat feature.
 of
.
f the
atei Zaharia <matei.zaharia@gmail.com> 写道：
o
eful
th
ter
agine
eryone
t
l
"
Amir Rahnama <amirrahnama@gmail.com>,"Tue, 24 Nov 2015 10:32:42 +0100",what should I know to implement twitter streaming for pyspark?,dev@spark.apache.org,"I wanna end the situation where python users of spark need to implement the
twitter source for streaming by themselves. Yuhu!

Anything I need to know, looked at scala and Java code and got some ideas.

-- 
Thanks and Regards,

Amir Hossein Rahnama

*Tel: +46 (0) 761 681 102*
Website: www.ambodi.com
Twitter: @_ambodi <https://twitter.com/_ambodi>
"
Yogesh Mahajan <ymahajan@snappydata.io>,"Tue, 24 Nov 2015 17:39:04 +0530",Streaming : stopping output transformations explicitly,dev@spark.apache.org,"Hi,

Is there a way to stop output transformations on a stream without stopping
streamingContext ?

Yogesh Mahajan
SnappayData Inc,
OLTP+OLAP inside Spark for real time analytics
"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Tue, 24 Nov 2015 15:52:52 +0100",Re: what should I know to implement twitter streaming for pyspark?,Amir Rahnama <amirrahnama@gmail.com>,"Hi Amir,

I believe that the first step should be looking for a library that implements the streaming API.


:
e twitter source for streaming by themselves. Yuhu!

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 24 Nov 2015 09:27:05 -0800",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"I think that Kostas' logic still holds.  The majority of Spark users, and
likely an even vaster majority of people running vaster jobs, are still on
RDDs and on the cusp of upgrading to DataFrames.  Users will probably want
to upgrade to the stable version of the Dataset / DataFrame API so they
don't need to do so twice.  Requiring that they absorb all the other ways
that Spark breaks compatibility in the move to 2.0 makes it much more
difficult for them to make this transition.

Using the same set of APIs also means that it will be easier to backport
critical fixes to the 1.x line.

It's not clear to me that avoiding breakage of an experimental API in the
1.x line outweighs these issues.

-Sandy


t
nd
z
he
o
to
et
like
low us
t
at we
ease
 might
ing.
y, as
d to
 Until
 best
, ...,
le.
em
 AFAIK
he
ve RDD
g
g
 before
RDDs --
 2.0
 as the
t,
al API
ames or
ul issue for
;
ith
lt, and
For
e good
e of
g.
D
h
e
ay 2.0
e to be
f
 remove certain
that feature.
e of
g.
D
h
of the
r
Matei Zaharia <matei.zaharia@gmail.com> 写道：
e
o be
t
ater
magine
veryone
n
ut
ll
e
)
-
-
-
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 24 Nov 2015 09:37:15 -0800",Re: [ANNOUNCE] Spark 1.6.0 Release Preview,Michael Armbrust <michael@databricks.com>,"If I am not mistaken, the binaries for Scala 2.11 were generated against
hadoop 1.

What about binaries for Scala 2.11 against hadoop 2.x ?

Cheers


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 24 Nov 2015 13:55:25 -0500",Re: A proposal for Spark 2.0,Sandy Ryza <sandy.ryza@cloudera.com>,"What are the other breaking changes in 2.0 though? Note that we're not removing Scala 2.10, we're just making the default build be against Scala 2.11 instead of 2.10. There seem to be very few changes that people would worry about. If people are going to update their apps, I think it's better to make the other small changes in 2.0 at the same time than to update once for Dataset and another time for 2.0.

BTW just refer to Reynold's original post for the other proposed API changes.

Matei

and likely an even vaster majority of people running vaster jobs, are still on RDDs and on the cusp of upgrading to DataFrames.  Users will probably want to upgrade to the stable version of the Dataset / DataFrame API so they don't need to do so twice.  Requiring that they absorb all the other ways that Spark breaks compatibility in the move to 2.0 makes it much more difficult for them to make this transition.
backport critical fixes to the 1.x line.
the 1.x line outweighs these issues.
reason is that I already know we have to break some part of the DataFrame/Dataset API as part of the Dataset design. (e.g. DataFrame.map should return Dataset rather than RDD). In that case, I'd rather break this sooner (in one release) than later (in two releases). so the damage is smaller.
matters too much for 2.0. We can still call Dataset experimental in 2.0 and then mark them as stable in 2.1. Despite being ""experimental"", there has been no breaking changes to DataFrame from 1.3 to 1.6.
fixing.  We're on the same page now.
z releases. The Dataset API is experimental and so we might be changing the APIs before we declare it stable. This is why I think it is important to first stabilize the Dataset API with a Spark 1.7 release before moving to Spark 2.0. This will benefit users that would like to use the new Dataset APIs but can't move to Spark 2.0 because of the backwards incompatible changes, like removal of deprecated APIs, Scala 2.11 etc.
instead of 1.6.1?
can talk about RDD vs. DS/DF more but lets refocus on Spark 2.0. I'd like to propose we have one more 1.x release after Spark 1.6. This will allow us to stabilize a few of the new features that were added in 1.6:
there will be users that won't be able to seamlessly upgrade given what we have discussed as in scope for 2.0. For these users, having a 1.x release with these new features/APIs stabilized will be very beneficial. This might make Spark 1.7 a lighter release but that is not necessarily a bad thing.
provide to developer, maybe the fundamental API is enough, like, the ShuffledRDD etc..  But PairRDDFunctions probably not in this category, as we can do the same thing easily with DF/DS, even better performance.
<mailto:mark@clearstorydata.com>] 
for retaining the RDD API but not as the first thing presented to new Spark developers: ""Here's how to use groupBy with DataFrames.... Until the optimizer is more fully developed, that won't always get you the best performance that can be obtained.  In these particular circumstances, ..., you may want to use the low-level RDD API while setting preservesPartitioning to true.  Like this....""
complete control of partitioning which is a key consideration at scale.  While partitioning control is still piecemeal in  DF/DS  it would seem premature to make RDD's a second-tier approach to spark dev.    
(/RDD) is already partitioned on the grouping expressions.  AFAIK the spark sql still does not allow that knowledge to be applied to the optimizer - so a full shuffle will be performed. However in the native RDD we can use preservesPartitioning=true.
<mailto:mark@clearstorydata.com>>:
about.  I think it may be going too far to deprecate it, but changing emphasis is something that we might consider.  The RDD API came well before DataFrames and DataSets, so programming guides, introductory how-to articles and the like have, to this point, also tended to emphasize RDDs -- or at least to deal with them early.  What I'm thinking is that with 2.0 maybe we should overhaul all the documentation to de-emphasize and reposition RDDs.  In this scheme, DataFrames and DataSets would be introduced and fully addressed before RDDs.  They would be presented as the normal/default/standard way to do things in Spark.  RDDs, in contrast, would be presented later as a kind of lower-level, closer-to-the-metal API that can be used in atypical, more specialized contexts where DataFrames or DataSets don't fully fit. 
it’s really worth to think about it in 2.0, as it is a painful issue for lots of users.
internal API only?)? As lots of its functionality overlapping with DataFrame or DataSet.
<mailto:kostas@cloudera.com>] 
dev@spark.apache.org <mailto:dev@spark.apache.org>; Reynold Xin
that with Spark 2.0 we can also look at better classpath isolation with user programs. I propose we build on spark.{driver|executor}.userClassPathFirst, setting it true by default, and not allow any spark transitive dependencies to leak into user code. For backwards compatibility we can have a whitelist if we want but I'd be good if we start requiring user apps to explicitly pull in all their dependencies. From what I can tell, Hadoop 3 is also moving in this direction.
features from MLlib to ML and deprecate the former. Current structure of two separate machine learning packages seems to be somewhat confusing.
in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.
things in 2.0 without removing or replacing them immediately. That way 2.0 doesn’t have to wait for everything that we want to deprecate to be replaced all at once.
2.0 is “to fix things that are broken in the current API and remove certain deprecated APIs”. At the same time I would be happy to have that feature.
features from MLlib to ML and deprecate the former. Current structure of two separate machine learning packages seems to be somewhat confusing.
in GraphX and switch to Dataframe. This will allow GraphX evolve with Tungsten.
<mailto:zhunanmcgill@gmail.com>] 
that PS shall exist as a third-party library instead of a component of the core code base, isn’t?
machine learning, For example, the parameter server.
i Zaharia <matei.zaharia@gmail.com <mailto:matei.zaharia@gmail.com>> 写道：
reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.
I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.
as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.
from a
broken
follow).
2.10, but
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Pranay Tonpay <ptonpay@gmail.com>,"Wed, 25 Nov 2015 00:51:52 +0530",sqlContext vs hivecontext,dev@spark.apache.org,"hi ,
when i am using hivecontext, i am able to query for individual columns from
a table as against when using sqlContext where only a select * works ....
Is is possible to use sqlContext and still query for specific columns from
a hive table ?
"
girishlg <girishlg@gmail.com>,"Tue, 24 Nov 2015 17:18:42 -0700 (MST)",pyspark does not seem to start py4j callback server,dev@spark.apache.org,"Hi

We have a use case where we call a scala function with a python object as a
callback. The python object implements a scala trait. The call to scala
function goes through but when it makes a call back through the passed in
python object we get a connection refused error. Looking further we noticed
the java_gateway.py launch_gateway method initiates the gateway object with
this line 

# Connect to the gateway
   gateway = JavaGateway(GatewayClient(port=gateway_port),
auto_convert=True)

and as per https://www.py4j.org/py4j_java_gateway.html the callback server
is started only if callback_server_parameters is passed.

Could someone please help us understand if in fact the callback server is
not started and if so any particular reason why it is disabled?

Thanks for any pointers
Girish




--

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 25 Nov 2015 01:54:10 -0700",Re: A proposal for Spark 2.0,Matei Zaharia <matei.zaharia@gmail.com>,"I see.  My concern is / was that cluster operators will be reluctant to
upgrade to 2.0, meaning that developers using those clusters need to stay
on 1.x, and, if they want to move to DataFrames, essentially need to port
their app twice.

I misunderstood and thought part of the proposal was to drop support for
2.10 though.  If your broad point is that there aren't changes in 2.0 that
will make it less palatable to cluster administrators than releases in the
1.x line, then yes, 2.0 as the next release sounds fine to me.

-Sandy



r
ce
n
t
:
n
et
and
g the
to
 to
set
m
m
 like
llow us
en what
ial.
ily a
e
ry, as
.
ed to
. Until
e best
s, ...,
ale.
eem
  AFAIK
the
ive RDD
but
 came
ctory
phasize
that
size and
d as the
st,
tal API
rames or
ful issue for
g;
with
ult, and
 For
be good
re of
ng.
e with
That way
recate to be
 and remove
 to have that
re of
ng.
e with
s
 of the
Matei Zaharia <matei.zaharia@gmail.com> 写道：
be
to be
ded
re? I
.
s
everyone
t
in
a
se
?)
--
--
--
"
"""wyphao.2007"" <wyphao.2007@163.com>","Wed, 25 Nov 2015 19:06:56 +0800 (CST)",Spark checkpoint problem,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, 


I am test checkpoint to understand how it works, My code as following:


scala> val data = sc.parallelize(List(""a"", ""b"", ""c""))
data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:15


scala> sc.setCheckpointDir(""/tmp/checkpoint"")
15/11/25 18:09:07 WARN spark.SparkContext: Checkpoint directory must be non-local if Spark is running on a cluster: /tmp/checkpoint1


scala> data.checkpoint


scala> val temp = data.map(item => (item, 1))
temp: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:17


scala> temp.checkpoint


scala> temp.count


but I found that only the temp RDD is checkpont in the /tmp/checkpoint directory, The data RDD is not checkpointed! I found the doCheckpoint function  in the org.apache.spark.rdd.RDD class:


  private[spark] def doCheckpoint(): Unit = {
    RDDOperationScope.withScope(sc, ""checkpoint"", allowNesting = false, ignoreParent = true) {
      if (!doCheckpointCalled) {
        doCheckpointCalled = true
        if (checkpointData.isDefined) {
          checkpointData.get.checkpoint()
        } else {
          dependencies.foreach(_.rdd.doCheckpoint())
        }
      }
    }
  }


from the code above, Only the last RDD(In my case is temp) will be checkpointed, My question : Is deliberately designed or this is a bug?


Thank you.





"
Stavros Kontopoulos <stavros.kontopoulos@typesafe.com>,"Wed, 25 Nov 2015 15:50:04 +0100",Re: Using spark MLlib without installing Spark,Reynold Xin <rxin@databricks.com>,"You can even use it without spark as well (besides local). For example i
have used the following algo in some web app:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/NaiveBayes.scala

Essentially some algorithms (i havent checked them all) they will have to
run the same steps in each partition so if you overlook the distributed
oriented parts (spark specific) of the code there is a lot of resuable
stuff.
You have just to use the api where that is public and conform to the
input/output contract of it.

There used to be some dependencies like Breeze for example in the api
hidden now (eg.
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/Vectors.scala),
but of course this is a hint not a list of what is available for your use
case. Mind though that this may not be the cleanest way to implement your
use case or might sound a hack ;)

As an alternative choice besides spark local you could use a job server (
https://github.com/spark-jobserver/spark-jobserver) to integrate with your
app as a proxy for spark or have a spark service there to respond back with
results. From a design point of you its best to separate the concerns for
several reasons: scaling , utilization etc.


s
u
rk
ML
way
out
ms


-- 

Stavros Kontopoulos

<http://www.typesafe.com>


<http://www.typesafe.com>
"
Rich Bowen <rbowen@rcbowen.com>,"Wed, 25 Nov 2015 12:32:10 -0500",[ANNOUNCE] CFP open for ApacheCon North America 2016,Rich Bowen <rbowen@apache.org>,"Community growth starts by talking with those interested in your
project. ApacheCon North America is coming, are you?

We are delighted to announce that the Call For Presentations (CFP) is
now open for ApacheCon North America. You can submit your proposed
sessions at
http://events.linuxfoundation.org/events/apache-big-data-north-america/program/cfp
for big data talks and
http://events.linuxfoundation.org/events/apachecon-north-america/program/cfp
for all other topics.

ApacheCon North America will be held in Vancouver, Canada, May 9-13th
2016. ApacheCon has been running every year since 2000, and is the place
to build your project communities.

While we will consider individual talks we prefer to see related
sessions that are likely to draw users and community members. When
submitting your talk work with your project community and with related
communities to come up with a full program that will walk attendees
through the basics and on into mastery of your project in example use
cases. Content that introduces what's new in your latest release is also
of particular interest, especially when it builds upon existing well
know application models. The goal should be to showcase your project in
ways that will attract participants and encourage engagement in your
community, Please remember to involve your whole project community (user
and dev lists) when building content. This is your chance to create a
project specific event within the broader ApacheCon conference.

Content at ApacheCon North America will be cross-promoted as
mini-conferences, such as ApacheCon Big Data, and ApacheCon Mobile, so
be sure to indicate which larger category your proposed sessions fit into.

Finally, please plan to attend ApacheCon, even if you're not proposing a
talk. The biggest value of the event is community building, and we count
on you to make it a place where your project community is likely to
congregate, not just for the technical content in sessions, but for
hackathons, project summits, and good old fashioned face-to-face networking.

-- 
rbowen@apache.org
http://apache.org/

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 25 Nov 2015 16:51:33 -0800",VerifyError running Spark SQL code?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've been running into this error when running Spark SQL recently; no
matter what I try (completely clean build or anything else) doesn't
seem to fix it. Anyone has some idea of what's wrong?

[info] Exception encountered when attempting to run a suite with class
name: org.apache.spark.sql.execution.ui.SQLListenerMemoryLeakSuite ***
ABORTED *** (4 seconds, 111 milliseconds)
[info]   java.lang.VerifyError: Bad <init> method call from inside of a branch
[info] Exception Details:
[info]   Location:
[info]     org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.<init>(Lorg/apache/spark/sql/catalyst/expressions/Expression;Lorg/apache/spark/sql/catalyst/expressions/Expression;)V
@82: invokespecial

Same happens with spark shell (when instantiating SQLContext), so not
an issue with the test code...

-- 
Marcelo

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 25 Nov 2015 16:54:36 -0800",Re: VerifyError running Spark SQL code?,Marcelo Vanzin <vanzin@cloudera.com>,"I think I've also seen this issue as well, but in a different suite. I
wasn't able to easily get to the bottom of it, though. What JDK / JRE are
you using? I'm on


Java(TM) SE Runtime Environment (build 1.7.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)

on OSX.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 25 Nov 2015 17:29:52 -0800",Re: VerifyError running Spark SQL code?,Josh Rosen <joshrosen@databricks.com>,"$ java -version
java version ""1.7.0_67""
Java(TM) SE Runtime Environment (build 1.7.0_67-b01)

that touches Spark SQL...




-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 25 Nov 2015 17:40:03 -0800",Re: VerifyError running Spark SQL code?,Josh Rosen <joshrosen@databricks.com>,"Seems to be some new thing with recent JDK updates according to the
intertubes. This patch seems to work around it:

--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.scala
@@ -63,11 +63,7 @@ case class HyperLogLogPlusPlus(
  def this(child: Expression, relativeSD: Expression) = {
    this(
      child = child,
-      relativeSD = relativeSD match {
-        case Literal(d: Double, DoubleType) => d
-        case _ =>
-          throw new AnalysisException(""The second argument should be
a double literal."")
-      },
+      relativeSD = HyperLogLogPlusPlus.validateRelativeSd(relativeSD),
      mutableAggBufferOffset = 0,
      inputAggBufferOffset = 0)
  }
@@ -448,4 +444,11 @@ object HyperLogLogPlusPlus {
    Array(189083, 185696.913, 182348.774, 179035.946, 175762.762,
172526.444, 169329.754, 166166.099, 163043.269, 159958.91, 156907.912,
153906.845,
150924.199, 147996.568, 145093.457, 142239.233, 139421.475, 136632.27,
133889.588, 131174.2, 128511.619, 125868.621, 123265.385, 120721.061,
118181.76
9, 115709.456, 113252.446, 110840.198, 108465.099, 106126.164,
103823.469, 101556.618, 99308.004, 97124.508, 94937.803, 92833.731,
90745.061, 88677.62
7, 86617.47, 84650.442, 82697.833, 80769.132, 78879.629, 77014.432,
75215.626, 73384.587, 71652.482, 69895.93, 68209.301, 66553.669,
64921.981, 63310.
323, 61742.115, 60205.018, 58698.658, 57190.657, 55760.865, 54331.169,
52908.167, 51550.273, 50225.254, 48922.421, 47614.533, 46362.049,
45098.569, 43
926.083, 42736.03, 41593.473, 40425.26, 39316.237, 38243.651,
37170.617, 36114.609, 35084.19, 34117.233, 33206.509, 32231.505,
31318.728, 30403.404, 2
9540.0550000001, 28679.236, 27825.862, 26965.216, 26179.148, 25462.08,
24645.952, 23922.523, 23198.144, 22529.128, 21762.4179999999,
21134.779, 20459.
117, 19840.818, 19187.04, 18636.3689999999, 17982.831,
17439.7389999999, 16874.547, 16358.2169999999, 15835.684, 15352.914,
14823.681, 14329.313, 1381
6.897, 13342.874, 12880.882, 12491.648, 12021.254, 11625.392,
11293.7610000001, 10813.697, 10456.209, 10099.074, 9755.39000000001,
9393.18500000006, 9
047.57900000003, 8657.98499999999, 8395.85900000005, 8033,
7736.95900000003, 7430.59699999995, 7258.47699999996,
6924.58200000005, 6691.29399999999, 6
357.92500000005, 6202.05700000003, 5921.19700000004, 5628.28399999999,
5404.96799999999, 5226.71100000001, 4990.75600000005,
4799.77399999998, 4622.93
099999998, 4472.478, 4171.78700000001, 3957.46299999999,
3868.95200000005, 3691.14300000004, 3474.63100000005,
3341.67200000002, 3109.14000000001, 307
1.97400000005, 2796.40399999998, 2756.17799999996, 2611.46999999997,
2471.93000000005, 2382.26399999997, 2209.22400000005,
2142.28399999999, 2013.9610
0000001, 1911.18999999994, 1818.27099999995, 1668.47900000005,
1519.65800000005, 1469.67599999998, 1367.13800000004,
1248.52899999998, 1181.2360000000
3, 1022.71900000004, 1088.20700000005, 959.03600000008,
876.095999999903, 791.183999999892, 703.337000000058,
731.949999999953, 586.86400000006, 526.0
24999999907, 323.004999999888, 320.448000000091, 340.672999999952,
309.638999999966, 216.601999999955, 102.922999999952,
19.2399999999907, -0.11400000
0059605, -32.6240000000689, -89.3179999999702, -153.497999999905,
-64.2970000000205, -143.695999999996, -259.497999999905,
-253.017999999924, -213.948
000000091, -397.590000000084, -434.006000000052, -403.475000000093,
-297.958000000101, -404.317000000039, -528.898999999976,
-506.621000000043, -513.2
05000000075, -479.351000000024, -596.139999999898, -527.016999999993,
-664.681000000099, -680.306000000099, -704.050000000047,
-850.486000000034, -757
.43200000003, -713.308999999892)
  )
  // scalastyle:on
+
+  private def validateRelativeSd(relativeSD: Expression): Double =
relativeSD match {
+    case Literal(d: Double, DoubleType) => d
+    case _ =>
+      throw new AnalysisException(""The second argument should be a
double literal."")
+  }
+
}





-- 
Marcelo

---------------------------------------------------------------------


"
"""wyphao.2007"" <wyphao.2007@163.com>","Thu, 26 Nov 2015 10:04:21 +0800 (CST)",Spark checkpoint problem,user <user@spark.apache.org>,"



I am test checkpoint to understand how it works, My code as following:


scala> val data = sc.parallelize(List(""a"", ""b"", ""c""))
data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:15


scala> sc.setCheckpointDir(""/tmp/checkpoint"")
15/11/25 18:09:07 WARN spark.SparkContext: Checkpoint directory must be non-local if Spark is running on a cluster: /tmp/checkpoint1


scala> data.checkpoint


scala> val temp = data.map(item => (item, 1))
temp: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:17


scala> temp.checkpoint


scala> temp.count


but I found that only the temp RDD is checkpont in the /tmp/checkpoint directory, The data RDD is not checkpointed! I found the doCheckpoint function  in the org.apache.spark.rdd.RDD class:


  private[spark] def doCheckpoint(): Unit = {
    RDDOperationScope.withScope(sc, ""checkpoint"", allowNesting = false, ignoreParent = true) {
      if (!doCheckpointCalled) {
        doCheckpointCalled = true
        if (checkpointData.isDefined) {
          checkpointData.get.checkpoint()
        } else {
          dependencies.foreach(_.rdd.doCheckpoint())
        }
      }
    }
  }


from the code above, Only the last RDD(In my case is temp) will be checkpointed, My question : Is deliberately designed or this is a bug?


Thank you.










 "
"""=?GBK?B?1cXWvse/KM360Pkp?="" <zzq98736@alibaba-inc.com>","Thu, 26 Nov 2015 13:19:39 +0800",RE: Spark checkpoint problem,"""'wyphao.2007'"" <wyphao.2007@163.com>,
	""'user'"" <user@spark.apache.org>","Whats your spark version?

 

: wyphao.2007 [mailto:wyphao.2007@163.com] 
ʱ: 20151126 10:04
ռ: user
: dev@spark.apache.org
: Spark checkpoint problem

 

 

 

I am test checkpoint to understand how it works, My code as following:

 

scala> val data = sc.parallelize(List(""a"", ""b"", ""c""))

data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at
parallelize at <console>:15

 

scala> sc.setCheckpointDir(""/tmp/checkpoint"")

15/11/25 18:09:07 WARN spark.SparkContext: Checkpoint directory must be
non-local if Spark is running on a cluster: /tmp/checkpoint1

 

scala> data.checkpoint

 

scala> val temp = data.map(item => (item, 1))

temp: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map
at <console>:17

 

scala> temp.checkpoint

 

scala> temp.count

 

but I found that only the temp RDD is checkpont in the /tmp/checkpoint
directory, The data RDD is not checkpointed! I found the doCheckpoint
function  in the org.apache.spark.rdd.RDD class:

 

  private[spark] def doCheckpoint(): Unit = {

    RDDOperationScope.withScope(sc, ""checkpoint"", allowNesting = false,
ignoreParent = true) {

      if (!doCheckpointCalled) {

        doCheckpointCalled = true

        if (checkpointData.isDefined) {

          checkpointData.get.checkpoint()

        } else {

          dependencies.foreach(_.rdd.doCheckpoint())

        }

      }

    }

  }

 

checkpointed, My question : Is deliberately designed or this is a bug?

 

Thank you.

 

 

 

 

 

 

 

"
Alexander Pivovarov <apivovarov@gmail.com>,"Wed, 25 Nov 2015 21:19:37 -0800",How to add 1.5.2 support to ec2/spark_ec2.py ?,dev@spark.apache.org,"Hi Everyone

I noticed that spark ec2 script is outdated.
How to add 1.5.2 support to ec2/spark_ec2.py?
What else (except of updating spark version in the script) should be done
to add 1.5.2 support?

We also need to update scala to 2.10.4 (currently it's 2.10.3)

Alex
"
"""wyphao.2007"" <wyphao.2007@163.com>","Thu, 26 Nov 2015 13:23:26 +0800 (CST)",Re:RE: Spark checkpoint problem,=?GBK?B?1cXWvse/KM360Pkp?= <zzq98736@alibaba-inc.com>,"Spark 1.5.2.


 2015-11-26 13:19:39""־ǿ()"" <zzq98736@alibaba-inc.com> д


Whats your spark version?

: wyphao.2007 [mailto:wyphao.2007@163.com]
ʱ: 20151126 10:04
ռ: user
:dev@spark.apache.org
: Spark checkpoint problem

I am test checkpoint to understand how it works, My code as following:

 

scala> val data = sc.parallelize(List(""a"", ""b"", ""c""))

data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:15

 

scala> sc.setCheckpointDir(""/tmp/checkpoint"")

15/11/25 18:09:07 WARN spark.SparkContext: Checkpoint directory must be non-local if Spark is running on a cluster: /tmp/checkpoint1

 

scala> data.checkpoint

 

scala> val temp = data.map(item => (item, 1))

temp: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:17

 

scala> temp.checkpoint

 

scala> temp.count

 

but I found that only the temp RDD is checkpont in the /tmp/checkpoint directory, The data RDD is not checkpointed! I found the doCheckpoint function  in the org.apache.spark.rdd.RDD class:

 

  private[spark] def doCheckpoint(): Unit = {

    RDDOperationScope.withScope(sc, ""checkpoint"", allowNesting = false, ignoreParent = true) {

      if (!doCheckpointCalled) {

        doCheckpointCalled = true

        if (checkpointData.isDefined) {

          checkpointData.get.checkpoint()

        } else {

          dependencies.foreach(_.rdd.doCheckpoint())

        }

      }

    }

  }

 

from the code above, Only the last RDD(In my case is temp) will be checkpointed, My question : Is deliberately designed or this is a bug?

 

Thank you.

 

 

 

 

 

 

 "
Sachith Withana <swsachith@gmail.com>,"Thu, 26 Nov 2015 11:16:57 +0530",Incremental Analysis with Spark,dev@spark.apache.org,"Hi folks!

I'm wondering if Sparks supports or hopes to support incremental data
analysis.

There are few use cases that prompted me to wonder.

ex: If we need to summarize last 30 days worth of data everyday,

1. Does Spark support time range based query execution ?
select * from foo where timestamp in last30days

2. Can we reuse the previously executed data?
ex: use the sum of the last 29 days and add the last days worth of data?

The second usecase is very tempting as it would improve the performance
drastically.

Any suggestions would be greatly appreciated.

-- 
Thanks,
Sachith Withana
"
chester@alpinenow.com,"Wed, 25 Nov 2015 21:56:24 -0800",Re: Incremental Analysis with Spark,Sachith Withana <swsachith@gmail.com>,"For the 2nd use case, can you save the result for first 29 days, then just get the last day result and add yourself ? This can be done outside of spark. Does that work for you



Sent from my iPad

ysis.


astically. 
"
Reynold Xin <rxin@databricks.com>,"Wed, 25 Nov 2015 23:00:55 -0800",Re: A proposal for Spark 2.0,Sandy Ryza <sandy.ryza@cloudera.com>,"I don't think we should drop support for Scala 2.10, or make it harder in
terms of operations for people to upgrade.

If there are further objections, I'm going to bump remove the 1.7 version
and retarget things to 2.0 on JIRA.



t
e
a
d
er
nce
:
d
on
nt
s
e
p
this
 and
s
n
ng the
 to
g to
aset
e
e
d like
allow us
ven what
x
cial.
rily a
he
ory, as
e.
ted to
.. Until
he best
es, ...,
r
cale.
seem
.  AFAIK
 the
tive RDD
 but
I came
uctory
mphasize
 that
asize and
ed as the
ast,
etal API
Frames or
t
nful issue for
g
 with
ault, and
. For
 be good
s
ure of
ing.
ve with
 That way
precate to be
I and remove
y to have that
ure of
ing.
ve with
nent of
Matei Zaharia <matei.zaharia@gmail.com> 写道：
o
 be
 to be
dded
ere? I
f.
. Keeping
e
 a
a
---
---
---
"
eric wong <win19999@gmail.com>,"Thu, 26 Nov 2015 17:20:50 +0800",Re: RE: Spark checkpoint problem,"""wyphao.2007"" <wyphao.2007@163.com>","I don't think it is a deliberate design.

So you may need do action on the <data> RDD before the action of <temp>
RDD, if you want to explicitly checkpoint <data> RDD.


2015-11-26 13:23 GMT+08:00 wyphao.2007 <wyphao.2007@163.com>:

轩)"" <zzq98736@alibaba-inc.com> 写道：
 10:04
map



-- 
王海华
"
=?UTF-8?Q?Sergio_Ram=c3=adrez?= <sramirezga@ugr.es>,"Thu, 26 Nov 2015 10:29:36 +0100",Re: Unchecked contribution (JIRA and PR),Joseph Bradley <joseph@databricks.com>,"OK, I'll do that. Thanks for the response.

El 17/11/15 a las 01:36, Joseph Bradley escribió:


-- 

Sergio Ramírez Gallego
Research group on Soft Computing and Intelligent Information Systems,
Dept. Computer Science and Artificial Intelligence,
University of Granada, Granada, Spain.
Email: sramirez@decsai.ugr.es
Research Group URL: http://sci2s.ugr.es/

-------------------------------------------------------------------------

Este correo electrónico y, en su caso, cualquier fichero anexo al mismo,
contiene información de carácter confidencial exclusivamente dirigida a
su destinatario o destinatarios. Si no es vd. el destinatario indicado,
queda notificado que la lectura, utilización, divulgación y/o copia sin
autorización está prohibida en virtud de la legislación vigente. En el
caso de haber recibido este correo electrónico por error, se ruega
notificar inmediatamente esta circunstancia mediante reenvío a la
dirección electrónica del remitente.
Evite imprimir este mensaje si no es estrictamente necesario.

This email and any file attached to it (when applicable) contain(s)
confidential information that is exclusively addressed to its
recipient(s). If you are not the indicated recipient, you are informed
that reading, using, disseminating and/or copying it without
authorisation is forbidden in accordance with the legislation in effect.
If you have received this email by mistake, please immediately notify
the sender of the situation by resending it to their email address.
Avoid printing this message if it is not absolutely necessary.

"
Sean Owen <sowen@cloudera.com>,"Thu, 26 Nov 2015 09:45:48 +0000",Re: A proposal for Spark 2.0,Reynold Xin <rxin@databricks.com>,"Maintaining both a 1.7 and 2.0 is too much work for the project, which
is over-stretched now. This means that after 1.6 it's just small
maintenance releases in 1.x and no substantial features or evolution.
This means that the ""in progress"" APIs in 1.x that will stay that way,
unless one updates to 2.x. It's not unreasonable, but means the update
to the 2.x line isn't going to be that optional for users.

Scala 2.10 is already EOL right? Supporting it in 2.x means supporting
it for a couple years, note. 2.10 is still used today, but that's the
point of the current stable 1.x release in general: if you want to
stick to current dependencies, stick to the current release. Although
I think that's the right way to think about support across major
versions in general, I can see that 2.x is more of a required update
for those following the project's fixes and releases. Hence may indeed
be important to just keep supporting 2.10.

I can't see supporting 2.12 at the same time (right?). Is that a
concern? it will be long since GA by the time 2.x is first released.

There's another fairly coherent worldview where development continues
in 1.7 and focuses on finishing the loose ends and lots of bug fixing.
2.0 is delayed somewhat into next year, and by that time supporting
2.11+2.12 and Java 8 looks more feasible and more in tune with
currently deployed versions.

I can't say I have a strong view but I personally hadn't imagined 2.x
would start now.


y on
heir
at
he
la
ld
ter
once
e:
nd
 on
ant
ys
t
he
ap
 this
0 and
as
in
ing the
t to
ng to
taset
le
we
'd like to
low us to
iven what
.x release
his might
 thing.
the
gory, as we
nted to new
Until the
best
ces, ...,
m>
or
scale.
 seem
s.  AFAIK
o the
ative RDD
:
, but
PI came
ductory
emphasize
s that with
e and
e
ted as the
rast, would
API that
es or
ut
inful issue for
r
h DataFrame
g;
ng
n with user
PathFirst,
pendencies
 whitelist
licitly
 also
l
ture of two
.
lve with
. That way
eprecate to be
l
PI and remove
py to have that
l
ture of two
.
lve with
onent of
Matei Zaharia <matei.zaharia@gmail.com> 写道：
oo
n be useful
 worth
added later
I imagine
d. Keeping
.
:
m>
re
a
,
s
o
e
 a
?
----
----
----

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 26 Nov 2015 11:15:36 +0000",Re: A proposal for Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","
pgrade to 2.0, meaning that developers using those clusters need to stay on 1.x, and, if they want to move to DataFrames, essentially need to port their app twice.
2.10 though.  If your broad point is that there aren't changes in 2.0 that will make it less palatable to cluster administrators than releases in the 1.x line, then yes, 2.0 as the next release sounds fine to me.

mixing spark versions in a JAR cluster with compatible hadoop native libs isn't so hard: users just deploy them up separately. 

But: 

-mixing Scala version is going to be tricky unless the jobs people submit are configured with the different paths
-the history server will need to be of the most latest spark version being executed in the cluster

---------------------------------------------------------------------


"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Thu, 26 Nov 2015 09:43:38 -0800",question about combining small parquet files,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark people,
I have a Hive table that has a lot of small parquet files and I am
creating a data frame out of it to do some processing, but since I have a
large number of splits/files my job creates a lot of tasks, which I don't
want. Basically what I want is the same functionality that Hive provides,
that is, to combine these small input splits into larger ones by specifying
a max split size setting. Is this currently possible with Spark?

I look at coalesce() but with coalesce I can only control the number
of output files not their sizes. And since the total input dataset size
can vary significantly in my case, I cannot just use a fixed partition
count as the size of each output file can get very large. I then looked for
getting the total input size from an rdd to come up with some heuristic to
set the partition count, but I couldn't find any ways to do it (without
modifying the spark source).

Any help is appreciated.

Thanks,
Nezih

PS: this email is the same as my previous email as I learned that my
previous email ended up as spam for many people since I sent it through
nabble, sorry for the double post.
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 26 Nov 2015 10:42:47 -0800",Re: Using spark MLlib without installing Spark,Reynold Xin <rxin@databricks.com>,"Decoupling mlllib and core is difficult...it is not intended to run spark
core 1.5 with spark mllib 1.6 snapshot...core is more stabilized due to new
algorithms getting added to mllib and sometimes you might be tempted to do
that but its not recommend.

s
u
rk
ML
way
out
ms
"
Rad Gruchalski <radek@gruchalski.com>,"Thu, 26 Nov 2015 20:45:35 +0100",NettyRpcEnv adverisedPort,dev@spark.apache.org,"Dear all,  

I am currently looking at modifying NettyRpcEnv for this PR: https://github.com/apache/spark/pull/9608
The functionality which I’m trying to achieve is the following: if there is a configuration property spark.driver.advertisedPort, make executors reply to advertisedPort instead of spark.driver.port. This would enable NettyRpcEnv work correctly on Mesos with Docker bridge networking (something what is working spot on for AkkaRcpEnv).

I’ve spent some time looking at the new NettyRpcEnv and I think I know what is happening but seeking for confirmation.

The master RpcEndpointAddress appears to be shipped to the executor as part of a serialized message, when the executors are requested, inside of the NettyRpcEndpointRef. In order to make my PR work, I need to change the RcpEndpointAddress shipped to the executors on the master.
I think the best candidate is:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala#L125

Am I correct that this value becomes the RcpEndpointAddress? If so, my naive implementation would be like this:

if (server != null) RpcAddress(host, conf.getInt(“spark.driver.advertisedPort”, server.getPort())) else null

however, I am not sure what impact such change would have, about to test my changes as soon as the code from master branch builds.

If that is not the correct place to make such change, what would be the best place to investigate? Is there an overview of NettyRcpEnv available anywhere?










Kind regards, 
Radek Gruchalski
 radek@gruchalski.com (mailto:radek@gruchalski.com)  (mailto:radek@gruchalski.com)
de.linkedin.com/in/radgruchalski/ (http://de.linkedin.com/in/radgruchalski/)

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.


"
liushiqi9 <sliu@phemi.com>,"Thu, 26 Nov 2015 12:56:58 -0700 (MST)",SparkR read.df Option type doesn't match,dev@spark.apache.org,"I try to write a third party datasource plugin for spark.
It works perfect fine  in scala, but in R because I need to pass the
options, which is a map[string,string] in scala, and nothing in R works, I
failed.
I tried use named list in R, it cannot get the value since I use get in my
plugin to get the value in map.

In scala 
option=Map(""a""=>""b"",""c""->""d"")

How should I construct option in R to make it work.







--

---------------------------------------------------------------------


"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 26 Nov 2015 12:16:36 -0800",Re: NettyRpcEnv adverisedPort,Rad Gruchalski <radek@gruchalski.com>,"I think you are right. The executor gets the driver port from
""RpcEnv.address"".

Best Regards,
Shixiong Zhu

2015-11-26 11:45 GMT-08:00 Rad Gruchalski <radek@gruchalski.com>:

f there
 know
e
e/spark/rpc/netty/NettyRpcEnv.scala#L125
())) else null
"
Ndjido Ardo Bar <ndjido@gmail.com>,"Thu, 26 Nov 2015 21:53:05 +0100",Grid search with Random Forest,user@spark.apache.org,"
Hi folks,

Does anyone know whether the Grid Search capability is enabled since the issue spark-9011 of version 1.4.0 ? I'm getting the ""rawPredictionCol column doesn't exist"" when trying to perform a grid search with Spark 1.4.0.

Cheers,
Ardo 




---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Thu, 26 Nov 2015 15:59:34 -0500",Re: A proposal for Spark 2.0,Sandy Ryza <sandy.ryza@cloudera.com>,"I also thought the idea was to drop 2.10. Do we want to cross build for 3
scala versions?

t
e
a
d
er
nce
:
d
on
nt
s
e
p
this
 and
s
n
ng the
 to
g to
aset
e
e
d like
allow us
ven what
x
cial.
rily a
he
ory, as
e.
ted to
.. Until
he best
es, ...,
r
cale.
seem
.  AFAIK
 the
tive RDD
 but
I came
uctory
mphasize
 that
asize and
ed as the
ast,
etal API
Frames or
t
nful issue for
g
 with
ault, and
. For
 be good
s
ure of
ing.
ve with
 That way
precate to be
I and remove
y to have that
ure of
ing.
ve with
nent of
Matei Zaharia <matei.zaharia@gmail.com> 写道：
o
 be
 to be
dded
ere? I
f.
. Keeping
e
 a
a
---
---
---
"
Reynold Xin <rxin@databricks.com>,"Thu, 26 Nov 2015 13:01:09 -0800",Re: A proposal for Spark 2.0,Koert Kuipers <koert@tresata.com>,"I don't think there are any plan for Scala 2.12 support yet. We can always
add Scala 2.12 support later.



y
t
at
he
la
ld
ter
once
till
y
er
ore
t
ap
 this
0 and
as
anging
rtant
oving
c.
. I'd
 will
 1.6:
iven what
.x
icial.
arily a
the
gory, as
ce.
nted to
... Until
the best
ces, ...,
m>
 at
it would
s.  AFAIK
o the
ative RDD
:
, but
PI came
ductory
emphasize
s that
hasize and
e
ted as the
rast,
metal API
aFrames or
a painful issue
r
h
solation
fault, and
e. For
d be good
is
l
ture of
sing.
lve with
. That way
eprecate to be
l
PI and remove
py to have that
l
ture of
sing.
lve with
onent of
Matei Zaharia <matei.zaharia@gmail.com> 写道：
 even be
s to be
added
here? I
ff.
d. Keeping
.
:
m>
a
,
s
o
e
 a
?
----
----
----
"
liushiqi9 <sliu@phemi.com>,"Thu, 26 Nov 2015 15:08:31 -0700 (MST)",Re: SparkR read.df Option type doesn't match,dev@spark.apache.org,"I found the answer myself.
options should be added like:
read.df(sqlContext,path=NULL,source=""***"",option1="""",option2="""" )





--

---------------------------------------------------------------------


"
Rad Gruchalski <radek@gruchalski.com>,"Thu, 26 Nov 2015 23:41:04 +0100",Re: NettyRpcEnv adverisedPort,dev@spark.apache.org,"I did test my change: https://github.com/radekg/spark/commit/b21aae1468169ee0a388d33ba6cebdb17b895956#diff-0c89b4a60c30a7cd2224bb64d93da942R125  
It seems to do what I want it to do, however, not quite sure about the overall impact.

I’d appreciate if someone who knows the NettyRpcEnv details could point to some documentation.










Kind regards, 
Radek Gruchalski
 radek@gruchalski.com (mailto:radek@gruchalski.com)  (mailto:radek@gruchalski.com)
de.linkedin.com/in/radgruchalski/ (http://de.linkedin.com/in/radgruchalski/)

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.




thub.com/apache/spark/pull/9608
 if there is a configuration property spark.driver.advertisedPort, make executors reply to advertisedPort instead of spark.driver.port. This would enable NettyRpcEnv work correctly on Mesos with Docker bridge networking (something what is working spot on for AkkaRcpEnv).
 I know what is happening but seeking for confirmation.
part of a serialized message, when the executors are requested, inside of the NettyRpcEndpointRef. In order to make my PR work, I need to change the RcpEndpointAddress shipped to the executors on the master.
che/spark/rpc/netty/NettyRpcEnv.scala#L125
y naive implementation would be like this:
ver.advertisedPort”, server.getPort())) else null
t my changes as soon as the code from master branch builds.
 best place to investigate? Is there an overview of NettyRcpEnv available anywhere?
 (mailto:radek@gruchalski.com)
ski/)
nfidential and/or legally privileged.
 must you copy or show it to anyone; please delete/destroy and inform the sender immediately.


"
"""qinggangwang7@gmail.com"" <qinggangwang7@gmail.com>","Fri, 27 Nov 2015 08:55:19 +0800",steamingContext stop gracefully failed in yarn-cluster mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","





Hi all, I try to stop the streamingContext gracefully in yarn-cluster mode. But it seemes that the job is stopped and start again when I use ssc.stop(true,true). And the job is stopped when I use ssc.stop(true). Does it means that the steamingContext cannot be stopped gracefully inyarn-cluster mode? My spark versoin is 1.4.1. In addition, the steamingContext can be stopped gracefully in local mode.


qinggangwang7@gmail.com

"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 26 Nov 2015 21:22:17 -0500","tests blocked at ""don't call ssc.stop in listener""",dev@spark.apache.org,"Hi, all

Anyone noticed that some of the tests just blocked at the test case “don't call ssc.stop in listener” in StreamingListenerSuite?

Examples:

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/46766/console

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/46776/console


https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/46774/console


I originally found it in my own PR, and I thought it is a bug introduced by me….but later I found that the tests for the PRs on different things also blocked at the same point …

Just filed a JIRA https://issues.apache.org/jira/browse/SPARK-12021


Best,  

--  
Nan Zhu
http://codingcat.me

"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 27 Nov 2015 10:55:37 +0800","Re: tests blocked at ""don't call ssc.stop in listener""",Nan Zhu <zhunanmcgill@gmail.com>,"Might be related to this JIRA (
https://issues.apache.org/jira/browse/SPARK-11761), not very sure about it.


don't
console
console
console
things
"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 26 Nov 2015 19:05:25 -0800","Re: tests blocked at ""don't call ssc.stop in listener""",Saisai Shao <sai.sai.shao@gmail.com>,"Just found a potential dead-lock in this test. Will send a PR to fix it
soon.

Best Regards,
Shixiong Zhu

2015-11-26 18:55 GMT-08:00 Saisai Shao <sai.sai.shao@gmail.com>:

don't
/console
/console
/console
 things
"
Justin Uang <justin.uang@gmail.com>,"Fri, 27 Nov 2015 15:19:03 +0000",Subtract implementation using broadcast,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have seen massive gains with the broadcast hint for joins with
DataFrames, and I was wondering if we have thought about allowing the
broadcast hint for the implementation of subtract and intersect.

Right now, when I try it, it says that there is no plan for the broadcast
hint.

Justin
"
Felix Cheung <felixcheung_m@hotmail.com>,"Fri, 27 Nov 2015 11:06:51 -0800",RE: SparkR read.df Option type doesn't match,"liushiqi9 <sliu@phemi.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Yes - please see the code example on the SparkR API doc: http://spark.apache.org/docs/latest/api/R/read.df.html
Suggestion or contribution to improve the doc is welcome!

 
2="""" )
.n3.nabble.com/SparkR-read-df-Option-type-doesn-t-match-tp15365p15370.html
.com.
 		 	   		  "
liushiqi9 <sliu@phemi.com>,"Fri, 27 Nov 2015 12:57:16 -0700 (MST)",RE: SparkR read.df Option type doesn't match,dev@spark.apache.org,"There is a bug at this page in the examples
I have file it in the JIRA It's SPARK-12019.
I don't know how to change the page.
But I think an example that shows how to write options would be great.
Like 

sc <- sparkR.init(master=""yarn-client"",appName= ""SparkR"", sparkHome =
""/home/spark"",
                 sparkEnvir =list(spark.executor.memory=""1g""),
                 sparkExecutorEnv =list(LD_LIBRARY_PATH=""/directory of JVM
libraries (libjvm.so) on workers/""),
                 sparkJars =  c(""jarfile1.jar,jarfile2.jar""), sparkPackages
= """"
                 option1="""",option2="""")

sparkJars example in the
https://spark.apache.org/docs/1.5.2/api/R/sparkR.init.html have bug in it.
This one I showed here is the correct one.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 28 Nov 2015 08:36:59 +0800",Re: Subtract implementation using broadcast,Justin Uang <justin.uang@gmail.com>,"We need to first implement subtract and intersect in Spark SQL natively first (i.e. add physical operator for them rather than using RDD.subtract/intersect).

Then it should be pretty easy to do that, given it is just about injecting the right exchange operators.



s, and I was wondering if we have thought about allowing the broadcast hint for the implementation of subtract and intersect.
int.

---------------------------------------------------------------------


"
Tarek Elgamal <tarek.elgamal@gmail.com>,"Fri, 27 Nov 2015 20:39:59 -0600",Problem in running MLlib SVM,dev@spark.apache.org,"Hi,

I am trying to run the straightforward example of SVm but I am getting low
accuracy (around 50%) when I predict using the same data I used for
training. I am probably doing the prediction in a wrong way. My code is
below. I would appreciate any help.


import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.mllib.classification.SVMModel;
import org.apache.spark.mllib.classification.SVMWithSGD;
import org.apache.spark.mllib.regression.LabeledPoint;
import org.apache.spark.mllib.util.MLUtils;

import scala.Tuple2;
import edu.illinois.biglbjava.readers.LabeledPointReader;

public class SimpleDistSVM {
  public static void main(String[] args) {
    SparkConf conf = new SparkConf().setAppName(""SVM Classifier Example"");
    SparkContext sc = new SparkContext(conf);
    String inputPath=args[0];

    // Read training data
    JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc,
inputPath).toJavaRDD();

    // Run training algorithm to build the model.
    int numIterations = 3;
    final SVMModel model = SVMWithSGD.train(data.rdd(), numIterations);

    // Clear the default threshold.
    model.clearThreshold();


    // Predict points in test set and map to an RDD of 0/1 values where 0
is misclassication and 1 is correct classification
    JavaRDD<Integer> classification = data.map(new Function<LabeledPoint,
Integer>() {
         public Integer call(LabeledPoint p) {
           int label = (int) p.label();
           Double score = model.predict(p.features());
           if((score >=0 && label == 1) || (score <0 && label == 0))
           {
           return 1; //correct classiciation
           }
           else
            return 0;

         }
       }
     );
    // sum up all values in the rdd to get the number of correctly
classified examples
     int sum=classification.reduce(new Function2<Integer, Integer,
Integer>()
    {
    public Integer call(Integer arg0, Integer arg1)
    throws Exception {
    return arg0+arg1;
    }});

     //compute accuracy as the percentage of the correctly classified
examples
     double accuracy=((double)sum)/((double)classification.count());
     System.out.println(""Accuracy = "" + accuracy);

        }
      }
    );
  }
}
"
Jeff Zhang <zjffdu@gmail.com>,"Sat, 28 Nov 2015 19:14:36 +0800",Re: Problem in running MLlib SVM,Tarek Elgamal <tarek.elgamal@gmail.com>,"        if((score >=0 && label == 1) || (score <0 && label == 0))
             {
              return 1; //correct classiciation
             }
             else
              return 0;



I suspect score is always between 0 and 1







-- 
Best Regards

Jeff Zhang
"
Sasaki Kai <lewuathe@me.com>,"Sat, 28 Nov 2015 13:20:30 +0000 (GMT)",Export BLAS module on Spark MLlib,dev@spark.apache.org,"Hello

I'm developing a Spark package that manipulates Vector and Matrix for machine learning.
This package uses mllib.linalg.Vector and mllib.linalg.Matrix in order to achieve compatible interface to mllib itself. But mllib.linalg.BLAS module is private inside spark package. We cannot use BLAS from spark package.
Due to this, there is no way to manipulate mllib.linalg.{Vector, Matrix} from spark package side.

Is there any reason why BLAS module is not set public?
If we cannot use BLAS, what is the reasonable option to manipulate Vector and Matrix from spark package?

Regards
Kai Sasaki(@Lewuathe)


"
Koert Kuipers <koert@tresata.com>,"Sat, 28 Nov 2015 16:06:41 -0500",Re: Subtract implementation using broadcast,Reynold Xin <rxin@databricks.com>,"if i wanted to pimp DataFrame to add subtract and intersect myself with a
physical operator, without needing to modify spark directly, is that
currently possible/intended? or will i run into the private[spark] issue?


"
Tarek Elgamal <tarek.elgamal@gmail.com>,"Sat, 28 Nov 2015 19:13:41 -0600",Re: Problem in running MLlib SVM,Jeff Zhang <zjffdu@gmail.com>,"According to the documentation
<http://spark.apache.org/docs/latest/mllib-linear-methods.html>, by
default, if wTx≥0 then the outcome is positive, and negative otherwise. I
suppose that wTx is the ""score"" in my case. If score is more than 0 and the
label is positive, then I return 1 which is correct classification and I
return zero otherwise. Do you have any idea how to classify a point as
positive or negative using this score or another function ?


))
e"");
;
0
nt,
= 0))
"
Jeff Zhang <zjffdu@gmail.com>,"Sun, 29 Nov 2015 11:06:15 +0800",Re: Problem in running MLlib SVM,Tarek Elgamal <tarek.elgamal@gmail.com>,"I think this should represent the label of LabledPoint (0 means negative 1
means positive)
http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point

The document you mention is for the mathematical formula, not the
implementation.


rwise. I
he
0))
);
= 0))


-- 
Best Regards

Jeff Zhang
"
salexln <salexln@gmail.com>,"Sun, 29 Nov 2015 03:39:17 -0700 (MST)",FuzzyCMeans Implementation,dev@spark.apache.org,"Hi guys,
I'm working on implementation of FuzzyCMeans
(https://issues.apache.org/jira/browse/SPARK-2344),
and wanted your thought on whether should FuzzyCMeans class inherit from
KMeans?

algorithms based on KMeans (BisectingKMeans, StreamingKMeans), does not
inherit from it and maybe it is better to make an algorithm that does not
have dependencies of other algorithms.

What do you think? 



--

---------------------------------------------------------------------


"
Yuhao Yang <hhbyyh@gmail.com>,"Sun, 29 Nov 2015 09:12:31 -0500",Need suggestions on monitor Spark progress,dev@spark.apache.org,"Hi all,

I got a simple processing job for 20000 accounts on 8 partitions. It's
roughly 2500 accounts on each partition. Each account will take about 1s to
complete the computation. That means each partition will take about 2500
seconds to finish the batch.

My question is how can I get the detailed progress of how many accounts has
been processed for each partition during the computation. An ideal solution
would allow me to know how many accounts has been processed periodically
(like every minute) so I can monitor and take action to save some time.
Right now on UI I can only get that task is running.

I know one solution is to split the data horizontally on driver and submit
to spark in mini batches, yet I think that would waste some cluster
resource and create extra complexity for result handling.

Any experience or best practice is welcome. Thanks a lot.

Regards,
Yuhao
"
Adam McElwee <adam@mcelwee.me>,"Mon, 30 Nov 2015 11:06:25 -0600",Re: Removing the Mesos fine-grained mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","To eliminate any skepticism around whether cpu is a good performance metric
for this workload, I did a couple comparison runs of an example job to
demonstrate a more universal change in performance metrics (stage/job time)
between coarse and fine-grained mode on mesos.

The workload is identical here - pulling tgz archives from s3, parsing json
lines from the files and ultimately creating documents to index into solr.
The tasks are not inserting into solr (just to let you know that there's no
network side-effect of the map task). The runs are on the same exact
hardware in ec2 (m2.4xlarge, with 68GB of ram and 45G executor memory),
exact same jvm and it's not dependent on order of running the jobs, meaning
I get the same results whether I run the coarse first or whether I run the
fine-grained first. No other frameworks/tasks are running on the mesos
cluster during the test. I see the same results whether it's a 3-node
cluster, or whether it's a 200-node cluster.

With the CMS collector in fine-grained mode, the map stage takes roughly
2.9h, and coarse-grained mode takes 3.4h. Because both modes initially
start out performing similarly, the total execution time gap widens as the
job size grows. To put that another way, the difference is much smaller for
jobs/stages < 1 hour. When I submit this job for a much larger dataset that
takes 5+ hours, the difference in total stage time moves closer and closer
to roughly 20-30% longer execution time.

With the G1 collector in fine-grained mode, the map stage takes roughly
2.2h, and coarse-grained mode takes 2.7h. Again, the fine and coarse-grained
execution tests are on the exact same machines, exact same dataset, and
only changing spark.mesos.coarse to true/false.

Let me know if there's anything else I can provide here.

Thanks,
-Adam



fe.com
,
use
os
d vs
 of
o
le
er
ct
hub.
urce-allocation-with-mesos>
is
"
Fazlan Nazeem <fazlann@wso2.com>,"Mon, 30 Nov 2015 22:43:14 +0530",Re: Problem in running MLlib SVM,Jeff Zhang <zjffdu@gmail.com>,"You should never use the training data to measure your prediction accuracy.
Always use a fresh dataset (test data) for this purpose.


1
erwise. I
the
 0))
m
r
s
s);
e
== 0))



-- 
Thanks & Regards,

Fazlan Nazeem

*Software Engineer*

*WSO2 Inc*
Mobile : +94772338839
<%2B94%20%280%29%20773%20451194>
fazlann@wso2.com
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 30 Nov 2015 19:43:26 +0100",Re: Need suggestions on monitor Spark progress,Yuhao Yang <hhbyyh@gmail.com>,"Hi,

My limited understanding of Spark tells me that a task is the least
possible working unit and Spark itself won't give you much. It
wouldn't expect so since ""acount"" is a business entity not Spark's
one.

What about using mapPartitions* to know the details of partitions and
do whatever you want (log to stdout or whatever)? Just a thought.

Pozdrawiam,
Jacek

--
Jacek Laskowski | https://medium.com/@jaceklaskowski/ |
http://blog.jaceklaskowski.pl
Mastering Spark https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski



---------------------------------------------------------------------


"
Alex Rovner <alex.rovner@magnetic.com>,"Mon, 30 Nov 2015 14:33:52 -0500",Re: Need suggestions on monitor Spark progress,Jacek Laskowski <jacek@japila.pl>,"In these scenarios it's fairly standard to report the metrics either
directly or through accumulators (
http://spark.apache.org/docs/latest/programming-guide.html#accumulators-a-nameaccumlinka)
to a time series database such as Graphite (http://graphite.wikidot.com/)
or OpenTSDB (http://opentsdb.net/) and monitor the progress through the UI
provided by the DB.

*Alex Rovner*
*Director, Data Engineering *
*o:* 646.759.0052

* <http://www.magnetic.com/>*


"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Mon, 30 Nov 2015 11:41:15 -0800",Re: question about combining small parquet files,Ruslan Dautkhanov <dautkhanov@gmail.com>,"This looks interesting, thanks Ruslan. But, compaction with Hive is as
simple as an insert overwrite statement as Hive
supports CombineFileInputFormat, is it possible to do the same with Spark?


"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 30 Nov 2015 12:48:44 -0800",Re: Export BLAS module on Spark MLlib,Sasaki Kai <lewuathe@me.com>,"The workaround is have your code in the same package, or write some
utility wrapper in the same package so you can use them in your code.
Mostly we implement those BLAS for our own need, and we don't have
general use-case in mind. As a result, if we open them up prematurely,
people are asking for them, we will gradually make them public.

Thanks.

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D



---------------------------------------------------------------------


"
Timothy Chen <tnachen@gmail.com>,"Mon, 30 Nov 2015 13:10:29 -0800",Re: Removing the Mesos fine-grained mode,Adam McElwee <adam@mcelwee.me>,"Hi Adam,

Thanks for the graphs and the tests, definitely interested to dig a
bit deeper to find out what's could be the cause of this.

Do you have the spark driver logs for both runs?

Tim

ic
e)
on
.
no
ng
e
art
at
r
ned
nly
k,
ause
sos
ed vs
y of
to
gle
rained
t the
 hours
ve a
rained
ll as
t
ained
s
 is
ects

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Mon, 30 Nov 2015 13:53:05 -0800",Re: Bringing up JDBC Tests to trunk,Luciano Resende <luckbr1975@gmail.com>,"The JDBC drivers are currently being pulled in as test-scope dependencies
of the `sql/core` module:
https://github.com/apache/spark/blob/f2fbfa444f6e8d27953ec2d1c0b3abd603c963f9/sql/core/pom.xml#L91

In SBT, these wind up on the Docker JDBC tests' classpath as a transitive
dependency of the `spark-sql` test JAR. However, what we *should* be doing
is adding them as explicit test dependencies of the
`docker-integration-tests` subproject, since Maven handles transitive test
JAR dependencies differently than SBT (see
https://github.com/apache/spark/pull/9876#issuecomment-158593498 for some
discussion). If you choose to make that fix as part of your PR, be sure to
move the version handling to the root POM's <dependencyManagement> section
so that the versions in both modules stay in sync. We might also be able to
just simply move the JDBC driver dependencies to docker-integration-tests'
POM if it turns out that they're not used anywhere else (that's my hunch).


"
Burak Yavuz <brkyvz@gmail.com>,"Mon, 30 Nov 2015 14:15:55 -0800",Re: Export BLAS module on Spark MLlib,DB Tsai <dbtsai@dbtsai.com>,"Or you could also use reflection like in this Spark Package:
https://github.com/brkyvz/lazy-linalg/blob/master/src/main/scala/com/brkyvz/spark/linalg/BLASUtils.scala

Best,
Burak


"
DB Tsai <dbtsai@dbtsai.com>,"Mon, 30 Nov 2015 16:02:41 -0800",Re: Export BLAS module on Spark MLlib,Burak Yavuz <brkyvz@gmail.com>,"I used reflection initially, but I found it's very slow especially in
a tight loop. Maybe caching the reflection can help which I never try.

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0xAF08DF8D



---------------------------------------------------------------------


"
