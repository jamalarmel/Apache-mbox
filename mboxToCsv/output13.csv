Soren Macbeth <soren@yieldbot.com>,"Sat, 31 May 2014 19:06:57 -0700",SCALA_HOME or SCALA_LIBRARY_PATH not set during build,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,

Following the instructions for building spark 1.0.0, I encountered the
following error:

[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project
spark-core_2.10: An Ant BuildException has occured: Please set the
SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment
variables and retry.
[ERROR] around Ant part ...<fail message=""Please set the SCALA_HOME (or
SCALA_LIBRARY_PATH if scala is on the path) environment variables and
retry."">... @ 6:126 in
/Users/soren/src/spark-1.0.0/core/target/antrun/build-main.xml

No where in the documentation does it mention that having scala installed
and either of these env vars set nor what version should be installed.
Setting these env vars wasn't required for 0.9.1 with sbt.

I was able to get past it by downloading the scala 2.10.4 binary package to
a temp dir and setting SCALA_HOME to that dir.

Ideally, it would be nice to not have to require people to have a
standalone scala installation but at a minimum this requirement should be
documented in the build instructions no?

-Soren
"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Sat, 31 May 2014 20:34:30 -0700",Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build,dev@spark.apache.org,"Spark currently supports two build systems, sbt and maven.  sbt will
download the correct version of scala, but with Maven you need to supply it
yourself and set SCALA_HOME.

It sounds like the instructions need to be updated-- perhaps create a JIRA?

best,
Colin



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 1 Jun 2014 11:13:57 -0700",Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build,"""dev@spark.apache.org"" <dev@spark.apache.org>","This is a false error message actually - the Maven build no longer
requires SCALA_HOME but the message/check was still there. This was
fixed recently in master:

https://github.com/apache/spark/commit/d8c005d5371f81a2a06c5d27c7021e1ae43d7193

I can back port that fix into branch-1.0 so it will be in 1.0.1 as
well. For other people running into this, you can export SCALA_HOME to
any value and it will work.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 1 Jun 2014 11:21:15 -0700",Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build,"""dev@spark.apache.org"" <dev@spark.apache.org>","I went ahead and created a JIRA for this and back ported the
improvement into branch-1.0. This wasn't a regression per-se because
the behavior existed in all previous versions, but it's annoying
behavior so best to fix it.

https://issues.apache.org/jira/browse/SPARK-1984

- Patrick


"
Soren Macbeth <soren@yieldbot.com>,"Sun, 1 Jun 2014 11:23:01 -0700",Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build,"""dev@spark.apache.org"" <dev@spark.apache.org>","Cheers, I didn't think it was needed, but just wanted to point it out.



"
Soren Macbeth <soren@yieldbot.com>,"Sun, 1 Jun 2014 15:40:39 -0700",ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda,"""dev@spark.apache.org"" <dev@spark.apache.org>","https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/serializer/Serializer.scala#L64-L66

These changes to the SerializerInstance make it really gross to call
serialize and deserialize from non-scala languages. I'm not sure what the
purpose of a ClassTag is, but if we could get some other arities that don't
require classtags that would help a ton.
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 1 Jun 2014 16:24:31 -0700",Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda,dev@spark.apache.org,"Why do you need to call Serializer from your own program? It’s an internal developer API so ideally it would only be called to extend Spark. Are you looking to implement a custom Serializer?

Matei


https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/serializer/Serializer.scala#L64-L66
the
don't


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 1 Jun 2014 16:25:37 -0700",Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda,dev@spark.apache.org,"BTW passing a ClassTag tells the Serializer what the type of object being serialized is when you compile your program, which will allow for more efficient serializers (especially on streams).

Matei


internal developer API so ideally it would only be called to extend Spark. Are you looking to implement a custom Serializer?
https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/serializer/Serializer.scala#L64-L66
the
don't


"
Soren Macbeth <soren@yieldbot.com>,"Sun, 1 Jun 2014 16:33:08 -0700",Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm writing a Clojure DSL for Spark. I use kryo to serialize my clojure
functions and for efficiency I hook into Spark's kryo serializer. In order
to do that I get a SerializerInstance from SparkEnv and call the serialize
and deserialize methods. I was able to workaround it by making ClassTag
object in clojure, but it's less than ideal.



:
an
.
e/spark/serializer/Serializer.scala#L64-L66
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 1 Jun 2014 17:10:16 -0700",Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda,dev@spark.apache.org,"Ah, got it. In general it will always be safe to pass the ClassTag for java.lang.Object here — this is what our Java API does to say that type info is not known. So you can always pass that. Look at the Java code for how to get this ClassTag.

Matei


clojure
order
serialize
ClassTag
<matei.zaharia@gmail.com>
being
more
Spark.
https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/serializer/Serializer.scala#L64-L66
call
what
that


"
Soren Macbeth <soren@yieldbot.com>,"Sun, 1 Jun 2014 17:42:16 -0700",Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yep, that's what I'm doing.

(def OBJECT-CLASS-TAG (.apply ClassTag$/MODULE$ java.lang.Object))

ps - I'm planning to open source this Clojure DSL soon as well



t type
s an
e/spark/serializer/Serializer.scala#L64-L66
t
t
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 1 Jun 2014 17:50:23 -0700",Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda,dev@spark.apache.org,"Very cool, looking forward to it!

Matei


<matei.zaharia@gmail.com>
for
type
for
clojure
ClassTag
<matei.zaharia@gmail.com>
more
an
https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/serializer/Serializer.scala#L64-L66
call
what
that


"
Madhu <madhu@madhu.com>,"Mon, 2 Jun 2014 07:43:45 -0700 (PDT)",Eclipse Scala IDE/Scala test and Wiki,dev@spark.incubator.apache.org,"I was able to set up Spark in Eclipse using the Spark IDE plugin.
I also got unit tests running with Scala Test, which makes development quick
and easy.

I wanted to document the setup steps in this wiki page:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup

I can't seem to edit that page.
Confluence usually has a an ""Edit"" button in the upper right, but it does
not appear for me, even though I am logged in.

Am I missing something?



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 2 Jun 2014 10:05:06 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"Hi Patrick,

Thanks for all the explanations, that makes sense. @DeveloperApi
worries me a little bit especially because of the things Colin
mentions - it's sort of hard to make people move off of APIs, or
support different versions of the same API. But maybe if expectations
(or lack thereof) are set up front, there will be less issues.

You mentioned something in your shading argument that kinda reminded
me of something. Spark currently depends on slf4j implementations and
log4j with ""compile"" scope. I'd argue that's the wrong approach if
we're talking about Spark being used embedded inside applications;
Spark should only depend on the slf4j API package, and let the
application provide the underlying implementation.

The assembly jars could include an implementation (since I assume
those are currently targeted at cluster deployment and not embedding).

That way there is less sources of conflict at runtime (i.e. the
""multiple implementation jars"" messages you can see when running some
Spark programs).


-- 
Marcelo

"
Sean Owen <sowen@cloudera.com>,"Mon, 2 Jun 2014 18:50:26 +0100",Re: [VOTE] Release Apache Spark 1.0.0 (rc5),dev@spark.apache.org,"
Good idea in general; in practice, the drawback is that you can't do
things like set log levels if you only depend on the SLF4J API. There
are a few cases where that's nice to control, and that's only possible
if you bind to a particular logger as well.

You typically bundle a SLF4J binding anyway, to give a default, or
else the end-user has to know to also bind some SLF4J logger to get
output. Of course it does make for a bit more surgery if you want to
override the binding this way.

Shading can bring a whole new level of confusion; I myself would only
use it where essential as a workaround. Same with trying to make more
elaborate custom classloading schemes -- never in my darkest
nightmares have I imagine the failure modes that probably pop up when
that goes wrong. I think the library collisions will get better over
time as only later versions of Hadoop are in scope, for example,
and/or one build system is in play. I like tackling complexity along
those lines first.

"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 2 Jun 2014 17:30:52 -0700",Which version does the binary compatibility test against by default?,dev@spark.apache.org,"Is there a way to specify the target version? -Xiangrui

"
Reynold Xin <rxin@databricks.com>,"Mon, 2 Jun 2014 18:27:49 -0700",Re: Eclipse Scala IDE/Scala test and Wiki,dev@spark.apache.org,"I tried but didn't find where I could add you. You probably need Matei to
help out with this.




"
Matei Zaharia <matei@databricks.com>,"Mon, 2 Jun 2014 18:32:52 -0700",Re: Eclipse Scala IDE/Scala test and Wiki,madhu@madhu.com,"Madhu, can you send me your Wiki username? (Sending it just to me is fine.) I can add you to the list to edit it.

Matei


to help out with this.
quick
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup
does
http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Scala-IDE-Scala-test-and-Wiki-tp6908.html
Nabble.com.

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Jun 2014 19:44:04 -0700",Re: Which version does the binary compatibility test against by default?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah - check out sparkPreviousArtifact in the build:
https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L325

- Patrick


"
npanj <nitinpanj@gmail.com>,"Mon, 2 Jun 2014 22:38:06 -0700 (PDT)","Spark 1.1-snapshot: java.io.FileNotFoundException from
 ShuffleMapTask",dev@spark.incubator.apache.org,"Quite often I notice that shuffle file is missing thus FileNotFoundException
is throws.
Any idea why shuffle file will be missing ? Am I running low in memory?
(I am using latest code from master branch on yarn-hadoop-2.2)

--
java.io.FileNotFoundException:
/var/storage/sda3/nm-local/usercache/npanj/appcache/application_1401394632504_0131/spark-local-20140603050956-6728/20/shuffle_0_2_97
(No such file or directory)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at
org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:116)
	at
org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:177)
	at
org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:161)
	at
org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:158)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
--



--

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Jun 2014 23:21:40 -0700",Spark 1.1 Window and 1.0 Wrap-up,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

I wanted to announce the the Spark 1.1 release window:
June 1 - Merge window opens
July 25 - Cut-off for new pull requests
August 1 - Merge window closes (code freeze), QA period starts
August 15+ - RC's and voting

This is consistent with the ""3 month"" release cycle we are targeting.
I'd really encourage people submitting larger features to do so during
the month of June, as features submitted closer to the window closing
could end up getting pushed into the next release.

I wanted to reflect a bit as well on the 1.0 release. First, thanks to
everyone who was involved in this release. It was the largest release
ever and it's something we should all be proud of.

In the 1.0 release, we cleaned up and consolidated several parts of the
Spark code base. In particular, we  consolidated the previously
fragmented process of submitting Spark jobs across a wide variety of
environments {YARN/Mesos/Standalone, Windows/Unix, Python/Java/Scala}.
We also brought the three language API's into much closer alignment.
These were difficult (but critical) tasks towards having a stable
deployment environment on which higher level libraries can build.

These cross-cutting changes also had associated test burden resulting
in an extended QA period. The 1.1, 1.2, 1.3, family of releases are
intended to be smaller releases, and I'd like to deliver them with
very predictable timing to the community. This will mean being fairly
strict about freezes and investing in QA infrastructure to allow us to
get through voting more quickly.

With 1.0 shipped, now is a great time to catch up on code reviews and look at
outstanding patches. Despite the large queue, we've actually been
consistently merging/closing about 80% of proposed PR's, which
is definitely good (for instance, we have 170 outstanding out of 950
proposed), but there remain a lot of people waiting on reviews, and
it's something everyone can help with!

Thanks again to everyone involved. Looking forward to more great releases!

- Patrick

"
Andrew Ash <andrew@andrewash.com>,"Mon, 2 Jun 2014 23:35:41 -0700",Scala Language NPE,dev@spark.apache.org,"// observed in Spark 1.0

Scala devs,

I was observing an unusual NPE in my code recently, and came up with the
below minimal test case:

class Super extends Serializable {
    lazy val superVal: String = null
}

class Sub extends Super {
    lazy val subVal: String = {
        try {
            ""literal""
        } catch {
            case _:Throwable => return superVal
        }
    }
}

Save this to a file, open the Spark shell, and load with "":l
/tmp/test.scala""

I got the below really unusual exception.  It goes away though when
removing the try/catch inside subVal and just returning either a straight
literal or superVal.


Is this a bug in Spark, or Scala, or my code, or what?  I think it might be
related to the Spark Repl doing magic but I'm unsure what.

Cheers!
Andrew




scala> :l /tmp/test.scala
Loading /tmp/test.scala...
defined class Super

     while compiling: <console>
        during phase: mixin
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args:

  last tree to typer: Apply(constructor $read)
              symbol: constructor $read in class $read (flags: <method>
<triedcooking>)
   symbol definition: def <init>(): $line9.$read
                 tpe: $line9.$read
       symbol owners: constructor $read -> class $read -> package $line9
      context owners: class iwC$Sub -> package $line9

== Enclosing template or block ==

Template( // val <local Sub>: <notype>, tree.tpe=$line9.iwC$Sub
  ""$line5.$read$$iwC$$iwC$$iwC$$iwC$Super"" // parents
  ValDef(
    private
    ""_""
    <tpt>
    <empty>
  )
  // 6 statements
  ValDef( // lazy private[this] var subVal: String
    private <mutable> <local> lazy <triedcooking>
    ""subVal ""
    <tpt> // tree.tpe=String
    <empty>
  )
  DefDef( // lazy val subVal(): String
    <method> <stable> <accessor> lazy
    ""subVal""
    []
    List(Nil)
    <tpt> // tree.tpe=String
    Block( // tree.tpe=String
      ValDef( // val nonLocalReturnKey1: Object
        <synthetic> <triedcooking>
        ""nonLocalReturnKey1""
        <tpt> // tree.tpe=Object
        Apply( // def <init>(): Object in class Object, tree.tpe=Object
          new Object.""<init>"" // def <init>(): Object in class Object,
tree.tpe=()Object
          Nil
        )
      )
      Try( // tree.tpe=String
        Block( // tree.tpe=String
          Assign( // tree.tpe=Unit
            $read$$iwC$$iwC$$iwC$$iwC$Sub.this.""subVal "" // lazy
private[this] var subVal: String, tree.tpe=String
            Block( // tree.tpe=String
              {}
              Apply( // final private[this] def
liftedTree1$1(nonLocalReturnKey1$1: Object): String, tree.tpe=String
                $read$$iwC$$iwC$$iwC$$iwC$Sub.this.""liftedTree1$1"" // final
private[this] def liftedTree1$1(nonLocalReturnKey1$1: Object): String,
tree.tpe=(nonLocalReturnKey1$1: Object)String
                ""nonLocalReturnKey1"" // val nonLocalReturnKey1: Object,
tree.tpe=Object
              )
            )
          )
          $read$$iwC$$iwC$$iwC$$iwC$Sub.this.""subVal "" // lazy
private[this] var subVal: String, tree.tpe=String
        )
        CaseDef( // tree.tpe=String
          Bind( // val ex: runtime.NonLocalReturnControl,
tree.tpe=runtime.NonLocalReturnControl
            ""ex""
            Typed( // tree.tpe=runtime.NonLocalReturnControl
              ""_"" // tree.tpe=runtime.NonLocalReturnControl
              <tpt> // tree.tpe=runtime.NonLocalReturnControl
            )
          )
          If( // tree.tpe=String
            Apply( // final def eq(x$1: Object): Boolean in class Object,
tree.tpe=Boolean
              ex.key().""eq"" // final def eq(x$1: Object): Boolean in class
Object, tree.tpe=(x$1: Object)Boolean
              ""nonLocalReturnKey1"" // val nonLocalReturnKey1: Object,
tree.tpe=Object
            )
            Apply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in class
Object, tree.tpe=String
              TypeApply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in
class Object, tree.tpe=()String
                ex.value().""$asInstanceOf"" // final def $asInstanceOf[T0 >:
? <: ?](): T0 in class Object, tree.tpe=[T0 >: ? <: ?]()T0
                <tpt> // tree.tpe=String
              )
              Nil
            )
            Throw(""ex"")tree.tpe=Nothing
          )
        )
      )
    )
  )
  ValDef( // protected val $outer: $line9.iwC
    protected <synthetic> <paramaccessor> <triedcooking>
    ""$outer ""
    <tpt> // tree.tpe=$line9.iwC
    <empty>
  )
  DefDef( // val $outer(): $line9.iwC
    <method> <synthetic> <stable> <expandedname>
    ""$line9$$read$$iwC$$iwC$$iwC$$iwC$Sub$$$outer""
    []
    List(Nil)
    <tpt> // tree.tpe=$line9.iwC
    $read$$iwC$$iwC$$iwC$$iwC$Sub.this.""$outer "" // protected val $outer:
$line9.iwC, tree.tpe=$line9.iwC
  )
  DefDef( // final private[this] def liftedTree1$1(nonLocalReturnKey1$1:
Object): String
    <method> private final <local> <lifted> <triedcooking>
    ""liftedTree1""
    []
    // 1 parameter list
    ValDef( // nonLocalReturnKey1$1: Object
      <param> <synthetic>
      ""nonLocalReturnKey1$1""
      <tpt> // tree.tpe=Object
      <empty>
    )
    <tpt> // tree.tpe=String
    Try( // tree.tpe=String
      ""literal""
      CaseDef( // tree.tpe=Nothing
        Typed( // tree.tpe=Throwable
          ""_"" // tree.tpe=Throwable
          <tpt> // tree.tpe=Throwable
        )
        Throw( // tree.tpe=Nothing
          Apply( // def <init>(key: Object,value: Object):
scala.runtime.NonLocalReturnControl in class NonLocalReturnControl,
tree.tpe=scala.runtime.NonLocalReturnControl
            new runtime.NonLocalReturnControl.""<init>"" // def <init>(key:
Object,value: Object): scala.runtime.NonLocalReturnControl in class
NonLocalReturnControl, tree.tpe=(key: Object, value:
Object)scala.runtime.NonLocalReturnControl
            // 2 arguments
            ""nonLocalReturnKey1$1"" // nonLocalReturnKey1$1: Object,
tree.tpe=Object
            Apply( // lazy val superVal(): String, tree.tpe=String
              $read$$iwC$$iwC$$iwC$$iwC$Sub.this.""superVal"" // lazy val
superVal(): String, tree.tpe=()String
              Nil
            )
          )
        )
      )
    )
  )
  DefDef( // def <init>(arg$outer: $line9.iwC): $line9.iwC$Sub
    <method>
    ""<init>""
    []
    // 1 parameter list
    ValDef( // $outer: $line9.iwC
      <param> <triedcooking>
      ""$outer""
      <tpt> // tree.tpe=$line9.iwC
      <empty>
    )
    <tpt> // tree.tpe=$line9.iwC$Sub
    Block( // tree.tpe=Unit
      // 2 statements
      If( // tree.tpe=Unit
        Apply( // final def eq(x$1: Object): Boolean in class Object,
tree.tpe=Boolean
          ""$outer"".""eq"" // final def eq(x$1: Object): Boolean in class
Object, tree.tpe=(x$1: Object)Boolean
          null
        )
        Throw( // tree.tpe=Nothing
          Apply( // def <init>(): NullPointerException in class
NullPointerException, tree.tpe=NullPointerException
            new NullPointerException.""<init>"" // def <init>():
NullPointerException in class NullPointerException,
tree.tpe=()NullPointerException
            Nil
          )
        )
        Assign( // tree.tpe=Unit
          $read$$iwC$$iwC$$iwC$$iwC$Sub.this.""$outer "" // protected val
$outer: $line9.iwC, tree.tpe=$line9.iwC
          ""$outer"" // $outer: $line9.iwC, tree.tpe=$line9.iwC
        )
      )
      Apply( // def <init>(arg$outer: $line5.iwC): $line5.iwC$Super,
tree.tpe=$line5.iwC$Super
        $read$$iwC$$iwC$$iwC$$iwC$Sub.super.""<init>"" // def
<init>(arg$outer: $line5.iwC): $line5.iwC$Super, tree.tpe=(arg$outer:
$line5.iwC)$line5.iwC$Super
        Apply( // val $iw(): $line5.iwC, tree.tpe=$line5.iwC

$outer.$line9$$read$$iwC$$iwC$$iwC$$iwC$$$outer().$VAL1().$iw().$iw().$iw().""$iw""
// val $iw(): $line5.iwC, tree.tpe=()$line5.iwC
          Nil
        )
      )
      ()
    )
  )
)

== Expanded type of tree ==

TypeRef(TypeSymbol(class $read extends Serializable))

unhandled exception while transforming <console>
error: uncaught exception during compilation: java.lang.NullPointerException
java.lang.NullPointerException
at scala.reflect.internal.Trees$class.Select(Trees.scala:1066)
 at scala.reflect.internal.SymbolTable.Select(SymbolTable.scala:13)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:908)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:904)
 at scala.reflect.internal.Trees$class.deriveDefDef(Trees.scala:1598)
at scala.reflect.internal.SymbolTable.deriveDefDef(SymbolTable.scala:13)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1(Mixin.scala:904)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
 at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
 at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
 at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.addCheckedGetters$1(Mixin.scala:945)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.addNewDefs(Mixin.scala:1013)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$postTransform(Mixin.scala:1147)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
 at scala.reflect.internal.SymbolTable.atPhase(SymbolTable.scala:207)
at scala.reflect.internal.SymbolTable.afterPhase(SymbolTable.scala:216)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1261)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at scala.reflect.api.Trees$Transformer.transformTemplate(Trees.scala:2904)
at
scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1280)
 at
scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1279)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
 at scala.reflect.internal.Trees$class.itransform(Trees.scala:1278)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
 at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at
scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2927)
at
scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2925)
 at scala.collection.immutable.List.loop$1(List.scala:170)
at scala.collection.immutable.List.mapConserve(List.scala:186)
 at scala.reflect.api.Trees$Transformer.transformStats(Trees.scala:2925)
at
scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
 at
scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
 at scala.reflect.internal.Trees$class.itransform(Trees.scala:1297)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
 at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at scala.tools.nsc.ast.Trees$Transformer.transformUnit(Trees.scala:227)
at scala.tools.nsc.transform.Transform$Phase.apply(Transform.scala:30)
 at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
at scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
 at
scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at scala.tools.nsc.Global$GlobalPhase.run(Global.scala:431)
 at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
 at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
at
org.apache.spark.repl.SparkIMain.compileSourcesKeepingRun(SparkIMain.scala:468)
 at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.compileAndSaveRun(SparkIMain.scala:859)
at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.compile(SparkIMain.scala:815)
 at
org.apache.spark.repl.SparkIMain$Request.compile$lzycompute(SparkIMain.scala:1009)
at org.apache.spark.repl.SparkIMain$Request.compile(SparkIMain.scala:1004)
 at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:644)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
 at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:621)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:618)
 at
scala.reflect.io.Streamable$Chars$class.applyReader(Streamable.scala:104)
at scala.reflect.io.File.applyReader(File.scala:82)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SparkILoop.scala:618)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
at org.apache.spark.repl.SparkILoop.savingReplayStack(SparkILoop.scala:150)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply$mcV$sp(SparkILoop.scala:617)
at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
at org.apache.spark.repl.SparkILoop.savingReader(SparkILoop.scala:155)
 at org.apache.spark.repl.SparkILoop.interpretAllFrom(SparkILoop.scala:616)
at
org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:681)
 at
org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:680)
at org.apache.spark.repl.SparkILoop.withFile(SparkILoop.scala:674)
 at org.apache.spark.repl.SparkILoop.loadCommand(SparkILoop.scala:680)
at
org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
 at
org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
at
scala.tools.nsc.interpreter.LoopCommands$LineCmd.apply(LoopCommands.scala:81)
 at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:748)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
 at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
 at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
 at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
at
scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
 at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:983)
 at org.apache.spark.repl.Main$.main(Main.scala:31)
at org.apache.spark.repl.Main.main(Main.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
 at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:256)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:54)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


Abandoning crashed session.

scala>
"
Andrew Ash <andrew@andrewash.com>,"Mon, 2 Jun 2014 23:52:31 -0700",Re: Scala Language NPE,dev@spark.apache.org,"Ah nevermind, the fix is to get rid of ""return"" from my method.  There's
probably a bug somewhere related to the repl taking bad input more cleanly,
but this isn't the end of the world once you figure out what the issue is.

Thanks for the time,
Andrew



"
Jim Donahue <jdonahue@adobe.com>,"Tue, 3 Jun 2014 16:20:03 +0000",sbt run with spark.ContextCleaner ERROR,"""dev@spark.apache.org"" <dev@spark.apache.org>","This was posted before, but I couldn't find an answer (http://mail-archives.apache.org/mod_mbox/incubator-spark-user/201405.mbox/%3C1399196458177-5304.post@n3.nabble.com%3E)


i use sbt to run my spark application, after the app completes, error
occurs:

14/05/04 17:32:28 INFO network.ConnectionManager: Selector thread was
interrupted!
14/05/04 17:32:28 ERROR spark.ContextCleaner: Error in cleaning thread
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
        at
org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:116)
        at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:64)



Now I'm seeing it.
"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Tue, 3 Jun 2014 10:26:39 -0700",Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build,dev@spark.apache.org,"Cool.  Nice to not have to set this any more.

best,
Colin



"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Jun 2014 10:59:26 -0700",Removing spark-debugger.md file from master?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

Seemed like the spark-debugger.md is no longer accurate (see
http://spark.apache.org/docs/latest/spark-debugger.html) and since it
was originally written Spark has evolved that makes the doc obsolete.

There are already work pending for new replay debugging (I could not
find the PR links for it) so I

With version control we could always reinstate the old doc if needed,
but as of today the doc is no longer reflect the current state of
Spark's RDD.

If no objection I could send PR to remove the md file in master.

Thoughts?

- Henry

"
Ankur Dave <ankurdave@gmail.com>,"Tue, 3 Jun 2014 11:11:43 -0700",Re: Removing spark-debugger.md file from master?,dev@spark.apache.org,"I agree, let's go ahead and remove it.

Ankur <http://www.ankurdave.com/>
"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Jun 2014 11:25:27 -0700",Re: Removing spark-debugger.md file from master?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Cool, thanks Ankur, sounds good. PR is coming.

- Henry


"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Jun 2014 11:33:09 -0700",Add my JIRA username (hsaputra) to Spark's contributor's list,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Could someone with right karma kindly add my username (hsaputra) to
Spark's contributor list?

I was added before but somehow now I can no longer assign ticket to
myself nor update tickets I am working on.


Thanks,

- Henry

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Jun 2014 11:36:42 -0700",Re: Add my JIRA username (hsaputra) to Spark's contributor's list,dev@spark.apache.org,"Done. Looks like this was lost in the JIRA import.

Matei




"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 3 Jun 2014 11:39:29 -0700",Re: Add my JIRA username (hsaputra) to Spark's contributor's list,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Matei!

- Henry


"
Kan Zhang <kzhang@apache.org>,"Tue, 3 Jun 2014 12:00:05 -0700",Re: Add my JIRA username (hsaputra) to Spark's contributor's list,dev@spark.apache.org,"Same here please, username (kzhang). Thanks!



"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 3 Jun 2014 15:09:00 -0400","Re: Add my JIRA username (hsaputra) to Spark's contributor's
 list",dev@spark.apache.org,"I think I lost that permission too?  

Patrick once helped to recover the permission, but I lost that permission again?

username is CodingCat, or Nan Zhu (Iâ€™m not sure which one you use when doing this)?

Best,  

--  
Nan Zhu






"
Doris Xin <doris.s.xin@gmail.com>,"Tue, 3 Jun 2014 14:08:02 -0700",collectAsMap doesn't return a multiMap?,dev@spark.apache.org,"Hey guys,

Just wanted to check real quick if collectAsMap was by design not to
return a multimap (so multiple values to the same key can overwrite
the same entry). It seems like it's only used in some unit tests in
the codebase. I added a warning in the comment saying not to expect a
multimap. Let me know if this is actually a bug that needs to be
fixed.

Thanks,
Doris

"
Madhu <madhu@madhu.com>,"Tue, 3 Jun 2014 14:45:45 -0700 (PDT)",Re: Eclipse Scala IDE/Scala test and Wiki,dev@spark.incubator.apache.org,"I was able to edit the page and add Eclipse setup steps.

Thanks Matei and Reynold!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Jun 2014 17:29:57 -0700",Re: collectAsMap doesn't return a multiMap?,dev@spark.apache.org,"Yup, it’s meant to be just a Map. You should probably use collect() and build a multimap instead if you’d like that.

Matei




"
Takuya UESHIN <ueshin@happy-camper.st>,"Wed, 4 Jun 2014 12:33:31 +0900",What is the correct Spark version of master/branch-1.0?,dev@spark.apache.org,"Hi all,

I'm wondering what is the correct Spark version of each HEAD of master
and branch-1.0.

current master HEAD (e8d93ee5284cb6a1d4551effe91ee8d233323329):
- pom.xml: 1.0.0-SNAPSHOT
- SparkBuild.scala: 1.1.0-SNAPSHOT

It should be 1.1.0-SNAPSHOT?


current branch-1.0 HEAD (d96794132e37cf57f8dd945b9d11f8adcfc30490):
- pom.xml: 1.0.1-SNAPSHOT
- SparkBuild.scala: 1.0.0

It should be 1.0.1-SNAPSHOT?


Thanks.

-- 
Takuya UESHIN
Tokyo, Japan

http://twitter.com/ueshin

"
"""Shihaoliang (Shihaoliang)"" <shihaoliang@huawei.com>","Wed, 4 Jun 2014 07:15:48 +0000",enable Spark on Mesos security delegation token transfer,"""dev@mesos.apache.org"" <dev@mesos.apache.org>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi,

Since spark 1.0 has security integretion with YARN, it enabled transfer credetials include delegation token from scheduler to executor side.
It is done in startContainerRequest RPC call, a crendetial will be pass to the executor side, so that executor UserGroupInformation will load the credential and get authenticated with secured HDFS;
We know that hadoop¡¯s RPC can be configured to encrypted, so spark on yarn¡¯s security is good.

While for spark on mesos, credential can not trasnfered to the executor side, we can not integrate secured HDFS in mesos deployment.

To do the credential transfering, my solution is

1)       Add crendetial field in the mesos¡¯s proto structure named TaskInfo

2)       Modify spark scheduler¡¯s code, read credential from UserGroupInformation and store it into the field mentioned in 1).

3)       Modify spark executor¡¯s code, add credetianl load logic before executor started.

In this way, the mesos can do the credential transfer in the launchTask message.

But still, the libprocess message in mesos is not encrypted, it can not protect the crendetial in tranferring.

There is 2 solutions

1)       Make the libprocess communitication layer support encryption. May should add ssl support to the libprocess

2)       Just encrypt the credential part, using some pre-deployed secret key in mesos.

Currently we choose the second.

This work will effect both spark and mesos layer, and will change one interface between them£»

I don¡¯t have much dev experience on spark and mesos, so and ideas/suggestions, please let me know.

Thanks.
Peter Shi

"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 4 Jun 2014 10:47:14 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","Testing... Resending as it appears my message didn't go through last week.ent mode, pyspark, spark-shell) on hadoop 0.23 and 2.4. 

Tom

 X and ubuntu.
Deployed app we are building on spark and poured data throhe following candidate as Apache Spark version 1.0.0!
> 
> This has a few important bug fixes on top of rc10:
> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> SPARK-1870: https://github.com/apache/spark/pull/848
> SPARK-1897: https://github.com/apache/spark/pull/849
> 
> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
> 
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11/
> 
> Release
 artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
> 
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1019/
> 
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
> 
> Please vote on releasing this package as Apache Spark 1.0.0!
> 
> The vote is open until
 Thursday, May 29, at 16:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
> 
> To learn more about Apache Spark, please see
> http://spark.apache.org/
> 
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
> 
> Changes to ML vector specification:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
> 
> Changes to the Java API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> 
> Changes to the streaming API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> 
> Changes to the GraphX API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> 
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
> 
> SparkContext.jarOfClass returns Option[String] instead of
 Seq[String]
> ==> Call toSeq on the result to restore old behavior"
Patrick Wendell <pwendell@gmail.com>,"Wed, 4 Jun 2014 10:49:33 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>, Tom Graves <tgraves_cs@yahoo.com>","Received!


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 4 Jun 2014 11:16:53 -0700",Re: What is the correct Spark version of master/branch-1.0?,"""dev@spark.apache.org"" <dev@spark.apache.org>","It should be 1.1-SNAPSHOT. Feel free to submit a PR to clean up any
inconsistencies.


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 4 Jun 2014 12:19:18 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),dev@spark.apache.org,"Hi Patrick,

We maintain internal Spark mirror in sync with Spark github master...

What's the way to get the 1.0.0 stable release from github to deploy on our
production cluster ? Is there a tag for 1.0.0 that I should use to deploy ?

Thanks.
Deb




"
Patrick Wendell <pwendell@gmail.com>,"Wed, 4 Jun 2014 13:00:45 -0700",Re: [VOTE] Release Apache Spark 1.0.0 (RC11),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey There,

The best way is to use the v1.0.0 tag:
https://github.com/apache/spark/releases/tag/v1.0.0

- Patrick


"
"""=?ISO-8859-1?B?d2l0Z28=?="" <witgo@qq.com>","Thu, 5 Jun 2014 10:17:46 +0800",Re: Add my JIRA username (hsaputra) to Spark's contributor's list,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","Uh,write my name wrong, right should be Guoqiang Li rather than Guoquiang Li




------------------ Original ------------------
From:  ""Kan Zhang"";<kzhang@apache.org>;
Date:  Wed, Jun 4, 2014 03:00 AM
To:  ""dev""<dev@spark.apache.org>; 

Subject:  Re: Add my JIRA username (hsaputra) to Spark's contributor's list



Same here please, username (kzhang). Thanks!


On Tue, Jun 3, 2014 at 11:39 AM, Henry Saputra <henry.saputra@gmail.com>
wrote:

> Thanks Matei!
>
> - Henry
>
> On Tue, Jun 3, 2014 at 11:36 AM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > Done. Looks like this was lost in the JIRA import.
> >
> > Matei
> >
> > On Jun 3, 2014, at 11:33 AM, Henry Saputra <henry.saputra@gmail.com>
> wrote:
> >
> >> Hi,
> >>
> >> Could someone with right karma kindly add my username (hsaputra) to
> >> Spark's contributor list?
> >>
> >> I was added before but somehow now I can no longer assign ticket to
> >> myself nor update tickets I am working on.
> >>
> >>
> >> Thanks,
> >>
> >> - Henry
> >
>"
Rahul Singhal <Rahul.Singhal@guavus.com>,"Thu, 5 Jun 2014 03:29:16 +0000",Re: Announcing Spark 1.0.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Could someone please clarify my confusion or is this not an issue that we
should be concerned about?

Thanks,
Rahul Singhal








"
Patrick Wendell <pwendell@gmail.com>,"Wed, 4 Jun 2014 21:39:47 -0700",Re: Announcing Spark 1.0.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Rahul,

The v1.0.0 tag is correct. When we release Spark we create multiple
rc11 is also the same as the official v1.0.0 release.

- Patrick


"
Takuya UESHIN <ueshin@happy-camper.st>,"Thu, 5 Jun 2014 14:48:45 +0900",Re: What is the correct Spark version of master/branch-1.0?,dev@spark.apache.org,"Thank you for your reply.

I've sent pull requests.


Thanks.


2014-06-05 3:16 GMT+09:00 Patrick Wendell <pwendell@gmail.com>:



-- 
Takuya UESHIN
Tokyo, Japan

http://twitter.com/ueshin

"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Thu, 5 Jun 2014 14:09:07 +0800","How to add user local repository defined in localRepository in
 settings.xml into Spark SBT build",dev@spark.apache.org,"Hi,

I customized MVN_HOME/conf/settings.xml's localRepository tag To manage
maven local jars.

<localRepository>F:/Java/maven-build/.m2/repository</localRepository>

However when I build Spark with SBT, it seems that it still gets the
default .m2 repository under

Path.userHome + ""/.m2/repository""

How should I let SBT pick up my customized localRepository instead?

Thanks,
Qiuzhuang
"
Andrew Ash <andrew@andrewash.com>,"Thu, 5 Jun 2014 10:29:17 -0700",Implementing rdd.scanLeft(),dev@spark.apache.org,"I have a use case that would greatly benefit from RDDs having a .scanLeft()
method.  Are the project developers interested in adding this to the public
API?


Looking through past message traffic, this has come up a few times.  The
recommendation from the list before has been to implement a parallel prefix
scan.

http://comments.gmane.org/gmane.comp.lang.scala.spark.user/1880
https://groups.google.com/forum/#!topic/spark-users/ts-FdB50ltY

The algorithm Reynold sketched in the first link leads to this working
implementation:

val vector = sc.parallelize(1 to 20, 3)

val sums = 0 +: vector.mapPartitionsWithIndex{ case(partition, iter) =>
Iterator(iter.sum) }.collect.scanLeft(0)(_+_).drop(1)

val prefixScan = vector.mapPartitionsWithIndex { case(partition, iter) =>
  val base = sums(partition)
  println(partition, base)
  iter.scanLeft(base)(_+_).drop(1)
}.collect


I'd love to have that replaced with this:

val vector = sc.parallelize(1 to 20, 3)
val cumSum: RDD[Int] = vector.scanLeft(0)(_+_)


Any thoughts on whether this contribution would be accepted?  What pitfalls
exist that I should be thinking about?

Thanks!
Andrew
"
Reynold Xin <rxin@databricks.com>,"Thu, 5 Jun 2014 10:47:47 -0700",Re: Implementing rdd.scanLeft(),dev@spark.apache.org,"I think the main concern is this would require scanning the data twice, and
maybe the user should be aware of it ...



"
Andrew Ash <andrew@andrewash.com>,"Thu, 5 Jun 2014 12:24:38 -0700",Re: Implementing rdd.scanLeft(),dev@spark.apache.org,"I that something that documentation on the method can solve?



"
Meisam Fathi <meisam.fathi@gmail.com>,"Thu, 5 Jun 2014 15:36:21 -0400",Building Spark against Scala 2.10.1 virtualized,dev@spark.apache.org,"Hi community,

How should I change sbt to compile spark core with a different version
of Scala? I see maven pom files define dependencies to scala 2.10.4. I
need to override/ignore the maven dependencies and use Scala
virtualized, which needs these lines in a build.sbt file:

scalaOrganization := ""org.scala-lang.virtualized""

scalaVersion := ""2.10.1""

libraryDependencies += ""EPFL"" %% ""lms"" % ""0.3-SNAPSHOT""

scalacOptions += ""-Yvirtualize""


Thanks,
Meisam

"
dataginjaninja <rickett.stephanie@gmail.com>,"Thu, 5 Jun 2014 13:30:19 -0700 (PDT)",Re: Timestamp support in v1.0,dev@spark.incubator.apache.org,"I can confirm that the patch fixed my issue. :-)



-----
Cheers,

Stephanie
--

"
Michael Armbrust <michael@databricks.com>,"Thu, 5 Jun 2014 13:35:06 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"Awesome, thanks for testing!



"
Michael Armbrust <michael@databricks.com>,"Thu, 5 Jun 2014 13:35:06 -0700",Re: Timestamp support in v1.0,dev@spark.apache.org,"Awesome, thanks for testing!



"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 5 Jun 2014 13:51:56 -0700",Re: Building Spark against Scala 2.10.1 virtualized,dev@spark.apache.org,"You can modify project/SparkBuild.scala and build Spark with sbt instead of Maven.





"
Tim Kellogg <tim@2lemetry.com>,"Thu, 5 Jun 2014 15:45:37 -0600",Cassandra Examples Don't Work,dev@spark.apache.org,"Hi,

Iâ€™ve tried running the CassandraTest example against several versions of Cassandra and I canâ€™t get it to work. Iâ€™m wondering if Iâ€™m doing something wrong, or if they simply donâ€™t work. Please help!

http://stackoverflow.com/q/24069039/503826

Much Thanks!

Tim Kellogg
Sr. Software Engineer, Protocols
2lemetry
605-593-7099
@kellogh"
Stephen Watt <swatt@redhat.com>,"Thu, 5 Jun 2014 17:51:29 -0400 (EDT)",Contributing Spark Infrastructure Configuration Docs,dev@spark.apache.org,"Hi Folks

My name is Steve Watt and I work in the CTO Office at Red Hat. I've recently spent quite a bit of time designing single rack and multi-rack infrastructures for Spark for our own hardware procurement at Red Hat and I thought the diagrams and server specs for both Dell and HP would be useful to the broader community as well. Even if folks don't want to go with my exact design, having the designs as a starting point should save  quite a bit of time. I think I can fold this quite easily into http://spark.apache.org/docs/latest/hardware-provisioning.html

However, before submitting a pull request for the modified page I thought I'd send this note up front and see if anyone wanted to provide some feedback first. If there is none, I'll submit the pull request early next week.

Regards
Steve Watt

"
kriskalish <kris@kalish.net>,"Thu, 5 Jun 2014 15:05:10 -0700 (PDT)",Cannot use pyspark to aggregate on remote EC2 cluster,dev@spark.incubator.apache.org,"I'm in a situation where I have two compute nodes in Amazon EC2 and a third
node that is used to just execute queries. The third node is not part of the
cluster. It's also configured slightly differently. That is, the third node
runs Ubuntu 14.04 while the two cluster nodes run CentOS. 

I launch pyspark on the third node using the following command:
./bin/pyspark --master spark://<ip of cluster master node>:7077

I'm attempting to execute the following python snippet:

data = sc.textFile('s3n://<keyhere>@test/data/y=2014/m=06/d=05/h=02/')
data.count()

However, I get the following exception:

Py4JJavaError: An error occurred while calling o130.collectPartitions.
: org.apache.spark.api.python.PythonException: Traceback (most recent call
last):
  File ""/root/spark/python/pyspark/worker.py"", line 77, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/root/spark/python/pyspark/serializers.py"", line 191, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/root/spark/python/pyspark/serializers.py"", line 123, in dump_stream
    for obj in iterator:
  File ""/root/spark/python/pyspark/serializers.py"", line 180, in _batched
    for item in iterator:
  File ""/root/spark/python/pyspark/rdd.py"", line 856, in takeUpToNum
    yield next(iterator)
  File ""<ipython-input-14-cf25bfc806aa>"", line 1, in <lambda>
IndexError: list index out of range

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:115)
	at
org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:78)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at
org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:574)
	at
org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:559)


I have been able to narrow the problem down to something related to py4j
because I can run the analogous code in the scala shell. To make matters
more confusing, if I ssh into the master node (CentOS), then execute
pyspark, the exact same code snippet works fine.

Additionally, if I execute data.collect() instead of data.count() on the
third node, then all of the data is written to the console as expected.


If anyone has some ideas on how to continue troubleshooting this problem,
the help would be appreciated. 

Thanks,
Kris





--

"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 5 Jun 2014 16:54:58 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hi Deb,

Why do you want to make those methods public? If you only need to
replace the solver for subproblems. You can try to make the solver
pluggable. Now it supports least squares and non-negative least
squares. You can define an interface for the subproblem solvers and
maintain the IPM solver at your own code base, if the only information
you need is Y^T Y and Y^T b.

Btw, just curious, what is the use case for quadratic constraints?

Best,
Xiangrui


"
Debasish Das <debasish.das83@gmail.com>,"Thu, 5 Jun 2014 15:38:58 -0700",Constraint Solver for Spark,dev@spark.apache.org,"Hi,

We are adding a constrained ALS solver in Spark to solve matrix
factorization use-cases which needs additional constraints (bounds,
equality, inequality, quadratic constraints)

We are using a native version of a primal dual SOCP solver due to its small
memory footprint and sparse ccs matrix computation it uses...The solver
depends on AMD and LDL packages from Timothy Davis for sparse ccs matrix
algebra (released under lgpl)...

Due to GPL dependencies, it won't be possible to release the code as Apache
license for now...If we get good results on our use-cases, we will plan to
write a version in breeze/modify joptimizer for sparse ccs operations...

I derived ConstrainedALS from Spark mllib ALS and I am comparing the
performance with default ALS and non-negative ALS as baseline. Plan is to
release the code as GPL license for community review...I have kept the
package structure as org.apache.spark.mllib.recommendation

There are some private functions defined in ALS, which I would like to
reuse....Is it possible to take the private out from the following
functions:

1. makeLinkRDDs
2. makeInLinkBlock
3. makeOutLinkBlock
4. randomFactor
5. unblockFactors

I don't want to copy any code.... I can ask for a PR to make these
changes...

Thanks.
Deb
"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 5 Jun 2014 16:04:27 -0700",Re: Contributing Spark Infrastructure Configuration Docs,dev@spark.apache.org,"Stephen,
    We are working thru Dell configurations; would be happy to review your
diagrams and offer feedback from our experience. Let me know the URLs.
Cheers
<k/>



"
Andrew Ash <andrew@andrewash.com>,"Thu, 5 Jun 2014 16:09:58 -0700",Re: Contributing Spark Infrastructure Configuration Docs,dev@spark.apache.org,"I would appreciate seeing the specs you came up with as well but don't need
to particularly quickly.  I'll wait until seeing the PR to comment on the
specifics, but have some questions about the thought process that went into
configuring the hardware.

Is the idea to see how you spec'd out memory/disk/processor for each
physical machine?  How did you optimize the ratios between the three for
Spark specifically?  Were you able to test this configuration against
others to optimize for price per performance?

Sorry for the barrage of questions -- I know talking hardware tends to
bring out the crazy in people.

Cheers,
Andrew



"
Debasish Das <debasish.das83@gmail.com>,"Thu, 5 Jun 2014 19:20:41 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hi Xiangrui,

For orthogonality properties in the factors we need a constraint solver
other than the usuals (l1, upper and lower bounds, l2 etc)

The interface of constraint solver is standard and I can add it in mllib
optimization....

But I am not sure how will I call the gpl licensed ipm solver from
mllib....assume the solver interface is as follows:

Qpsolver (densematrix h, array [double] f, int linearEquality, int
linearInequality, bool lb, bool ub)

And then I have functions to update equalities, inequalities, bounds etc
followed by the run which generates the solution....

For l1 constraints I have to use epigraph formulation which needs a
variable transformation before the solve....

I was thinking that for the problems that does not need constraints people
will use ALS.scala and ConstrainedALS.scala will have the constrained
formulations....

I can point you to the code once it is ready and then you can guide me how
to refactor it to mllib als ?

Thanks.
Deb
Hi Deb,

Why do you want to make those methods public? If you only need to
replace the solver for subproblems. You can try to make the solver
pluggable. Now it supports least squares and non-negative least
squares. You can define an interface for the subproblem solvers and
maintain the IPM solver at your own code base, if the only information
you need is Y^T Y and Y^T b.

Btw, just curious, what is the use case for quadratic constraints?

Best,
Xiangrui

small
Apache
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 5 Jun 2014 23:31:25 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"I don't quite understand why putting linear constraints can promote
orthogonality. For the interfaces, if the subproblem is determined by
Y^T Y and Y^T b for each iteration, then the least squares solver, the
non-negative least squares solver, or your convex solver is simply a
function

(A, b) -> x.

You can define it as an interface, and make the solver pluggable by
adding a setter to ALS. If you want to use your lgpl solver, just
include it in the classpath. Creating two separate files still seems
unnecessary to me. Could you create a JIRA and we can move our
discussion there? Thanks!

Best,
Xiangrui


"
Jim Donahue <jdonahue@adobe.com>,"Fri, 6 Jun 2014 16:03:15 +0000",Getting this sporadically running Spark Streaming,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm seeing this sporadically while running a Spark Streaming job on Spark 1.0.0.



Jim


14/06/06 08:53:56 ERROR LiveListenerBus: Listener JobProgressListener threw an exception

java.util.NoSuchElementException: key not found: 1029063

at scala.collection.MapLike$class.default(MapLike.scala:228)

at scala.collection.AbstractMap.default(Map.scala:58)

at scala.collection.mutable.HashMap.apply(HashMap.scala:64)

at org.apache.spark.ui.jobs.JobProgressListener.onStageCompleted(JobProgressListener.scala:78)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(SparkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(SparkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:81)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:79)

at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at org.apache.spark.scheduler.SparkListenerBus$class.foreachListener(SparkListenerBus.scala:79)

at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:48)

at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:32)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at scala.Option.foreach(Option.scala:236)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)

14/06/06 08:53:56 ERROR LiveListenerBus: Listener JobProgressListener threw an exception

java.util.NoSuchElementException: key not found: 1029105

at scala.collection.MapLike$class.default(MapLike.scala:228)

at scala.collection.AbstractMap.default(Map.scala:58)

at scala.collection.mutable.HashMap.apply(HashMap.scala:64)

at org.apache.spark.ui.jobs.JobProgressListener.onStageCompleted(JobProgressListener.scala:78)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(SparkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(SparkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:81)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:79)

at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at org.apache.spark.scheduler.SparkListenerBus$class.foreachListener(SparkListenerBus.scala:79)

at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:48)

at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:32)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at scala.Option.foreach(Option.scala:236)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)

"
Debasish Das <debasish.das83@gmail.com>,"Fri, 6 Jun 2014 10:42:11 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hi Xiangrui,

It's not the linear constraint, It is quadratic inequality with slack,
first order taylor approximation of off diagonal cross terms and a cyclic
coordinate descent, which we think will yield orthogonality....It's still
under works...

Also we want to put a L1 constraint as set of linear equations when solving
for ALS...

I will create the JIRA...as I see it, this will evolve to a generic
constraint solver for machine learning problems that has a QP
structure....ALS is one example....another example is kernel SVMs...

I did not know that lgpl solver can be added to the classpath....if it can
be then definitely we should add these in ALS.scala...

Thanks.
Deb




"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Fri, 6 Jun 2014 18:26:56 +0000",Opiq for SParkSQL?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Can anybody share your thoughts/comments/interests of applicability of the ""optiq""  framework to Spark, and SparkSQL in particular?

Thanks,
"
Christopher Nguyen <ctn@adatao.com>,"Sat, 7 Jun 2014 13:26:43 -0700",Re: Opiq for SParkSQL?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yan, it looks like Julian did anticipate exactly this possibility:

https://github.com/julianhyde/optiq/tree/master/spark

Optiq is a cool project vision in terms of hiding various engines behind
one consistent API.

That said, from just the Spark perspective, I don't see a huge value add to
layer Optiq above SparkSQL---until and unless Optiq provides a lot more
idioms and/or operational facilities than just making Spark RDDs look like
tables, which SparkSQL already does quite nicely and increasingly.
Warehousing, perhaps?

Here, I can't avoid a mention of DDF which aims to add more algorithmic and
data manipulation value in addition to the table abstraction (
https://spark-summit.org/2014/talk/distributed-dataframe-ddf-on-apache-spark-simplifying-big-data-for-the-rest-of-us
)
--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
Gil Vernik <GILV@il.ibm.com>,"Sun, 8 Jun 2014 11:01:50 +0300",Apache Spark and Swift object store,dev@spark.apache.org,"Hello everyone,

I would like to initiate discussion about integration Apache Spark and 
Openstack Swift. 
(https://issues.apache.org/jira/browse/SPARK-938 was created while ago)

I created a patch (https://github.com/apache/spark/pull/1010) that 
provides initial information how to connect Swift and Spark. Currently it 
uses Hadoop 2.3.0 and only stand alone mode of Spark. This patch is mainly 
used to provide community a way to experiment with this integration.
I have it fully working on my private cluster and it works very well, 
allowing me to make various analytics using Spark.

My next planned patches will include information how to configure Swift 
for other cluster deployment of Spark and also information how to 
integrate Spark and Swift with earlier versions of Hadoop. 
I am confident that the integration between Spark and Swift is very 
important future that will  benefit greatly for the exposure of Spark.

The integration between Spark and Swift is very similar to how Spark 
integrates with S3.

Will be great to hear comments / suggestions / remarks from the community!

All the best,
Gil Vernik."
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Jun 2014 12:40:04 -0700",MIMA Compatiblity Checks,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Some people may have noticed PR failures due to binary compatibility
checks. We've had these enabled in several of the sub-modules since
the 0.9.0 release but we've turned them on in Spark core post 1.0.0
which has much higher churn.

The checks are based on the ""migration manager"" tool from Typesafe.
classes or methods. Prashant Sharma has built instrumentation that
adds partial support for package-privacy (via a workaround) but since
there isn't really native support for this in MIMA we are still
finding cases in which we trigger false positives.

In the next week or two we'll make it a priority to handle more of
these false-positive cases. In the mean time users can add manual
excludes to:

project/MimaExcludes.scala

to avoid triggering warnings for certain issues.

This is definitely annoying - sorry about that. Unfortunately we are
the first open source Scala project to ever do this, so we are dealing
with uncharted territory.

Longer term I'd actually like to see us just write our own sbt-based
tool to do this in a better way (we've had trouble trying to extend
MIMA itself, it e.g. has copy-pasted code in it from an old version of
the scala compiler). If someone in the community is a Scala fan and
wants to take that on, I'm happy to give more details.

- Patrick

"
Paul Brown <prb@mult.ifario.us>,"Sun, 8 Jun 2014 12:45:54 -0700",Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0,dev@spark.apache.org,"Moving over to the dev list, as this isn't a user-scope issue.

I just ran into this issue with the missing saveAsTestFile, and here's a
little additional information:

- Code ported from 0.9.1 up to 1.0.0; works with local[n] in both cases.
- Driver built as an uberjar via Maven.
- Deployed to smallish EC2 cluster in standalone mode (S3 storage) with
Spark 1.0.0-hadoop1 downloaded from Apache.

Given that it functions correctly in local mode but not in a standalone
cluster, this suggests to me that the issue is in a difference between the
Maven version and the hadoop1 version.

In the spirit of taking the computer at its word, we can just have a look
in the JAR files.  Here's what's in the Maven dep as of 1.0.0:

jar tvf
~/.m2/repository/org/apache/spark/spark-core_2.10/1.0.0/spark-core_2.10-1.0.0.jar
| grep 'rdd/RDD' | grep 'saveAs'
  1519 Mon May 26 13:57:58 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 13:57:58 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class


And here's what's in the hadoop1 distribution:

jar tvf spark-assembly-1.0.0-hadoop1.0.4.jar| grep 'rdd/RDD' | grep 'saveAs'


I.e., it's not there.  It is in the hadoop2 distribution:

jar tvf spark-assembly-1.0.0-hadoop2.2.0.jar| grep 'rdd/RDD' | grep 'saveAs'
  1519 Mon May 26 07:29:54 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 07:29:54 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class


So something's clearly broken with the way that the distribution assemblies
are created.

FWIW and IMHO, the ""right"" way to publish the hadoop1 and hadoop2 flavors
of Spark to Maven Central would be as *entirely different* artifacts
(spark-core-h1, spark-core-h2).

Logged as SPARK-2075 <https://issues.apache.org/jira/browse/SPARK-2075>.

Cheers.
-- Paul



â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



a
cessorImpl.java:57)
ructorAccessorImpl.java:45)
)
ricsSystem.scala:136)
ricsSystem.scala:130)
8)
8)
)
)
130)
.scala:167)
:57)
mpl.java:43)
45)
le
)
)
s(JavaSerializer.scala:60)
alizer.scala:63)
1)
alizer.scala:63)
izer.scala:85)
:1145)
a:615)
saveAsTextFile-after-upgrade-Spark-0-9-0-1-0-0-tp6832p7122.html
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Jun 2014 13:02:07 -0700",Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Paul,

Could you give the version of Java that you are building with and the
version of Java you are running with? Are they the same?

Just off the cuff, I wonder if this is related to:
https://issues.apache.org/jira/browse/SPARK-1520

If it is, it could appear that certain functions are not in the jar
because they go beyond the extended zip boundary `jar tvf` won't list
them.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Jun 2014 13:02:40 -0700",Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Also I should add - thanks for taking time to help narrow this down!


"
Sean Owen <sowen@cloudera.com>,"Sun, 8 Jun 2014 16:12:20 -0400",Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0,user@spark.apache.org,"I suspect Patrick is right about the cause. The Maven artifact that
was released does contain this class (phew)

http://search.maven.org/#artifactdetails%7Corg.apache.spark%7Cspark-core_2.10%7C1.0.0%7Cjar

As to the hadoop1 / hadoop2 artifact question -- agree that is often
done. Here the working theory seems to be to depend on the one
artifact (whose API should be identical regardless of dependencies)
and then customize the hadoop-client dep. Here, there are not two
versions deployed to Maven at all.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Jun 2014 15:11:42 -0700",Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0,"user@spark.apache.org, ""dev@spark.apache.org"" <dev@spark.apache.org>","Okay I think I've isolated this a bit more. Let's discuss over on the JIRA:

https://issues.apache.org/jira/browse/SPARK-2075

do
e:
 a
s.
h
e
0-1.0.0.jar
is
orAccessorImpl.java:57)
onstructorAccessorImpl.java:45)
(MetricsSystem.scala:136)
(MetricsSystem.scala:130)
la:98)
la:98)
:226)
ala:130)
stem.scala:167)
2)
)
java:57)
sorImpl.java:43)
445)
366)
ala:45)
0
)
)
Class(JavaSerializer.scala:60)
2)
771)
90)
798)
Serializer.scala:63)
la:61)
41)
7)
796)
Serializer.scala:63)
rializer.scala:85)
java:1145)
.java:615)
ith-saveAsTextFile-after-upgrade-Spark-0-9-0-1-0-0-tp6832p7122.html

"
Alex Levin <salexln@gmail.com>,"Mon, 9 Jun 2014 09:43:35 +0300",Contributing algorithms to MLlib,dev@spark.apache.org,"Hi,



I'm a M.Sc. computer science student at Tel-Aviv College, Israel (
www.mta.ac.il) and as part of my final project that is dealing with Machine
Learning algorithms in distributed systems,

I would like to contribute couple of algorithms to MLlib.



My advisor, Dr. Uzi Hadad, and I thought of starting with an implementation
of* Fuzzy k - means* algorithm and continuing with *Hidden Markov Model*
algorithm.




Do you know if anyone is currently working on an implementation of these
algorithms for MLlib?







Regards,

Alex
"
dataginjaninja <rickett.stephanie@gmail.com>,"Mon, 9 Jun 2014 08:48:10 -0700 (PDT)",implementing the VectorAccumulatorParam,dev@spark.incubator.apache.org,"The  programming-guide
<http://spark.apache.org/docs/latest/programming-guide.html>   has the
following:

However, when I try to use this I get an error:


Last thing, am I posting on the wrong list?



-----
Cheers,

Stephanie
--

"
dataginjaninja <rickett.stephanie@gmail.com>,"Mon, 9 Jun 2014 08:57:48 -0700 (PDT)",implementing the VectorAccumulatorParam,dev@spark.incubator.apache.org,"The  programming-guide
<http://spark.apache.org/docs/latest/programming-guide.html>   has the
following:
    
    object VectorAccumulatorParam extends AccumulatorParam[Vector] {
      def zero(initialValue: Vector): Vector = {
        Vector.zeros(initialValue.size)
      }
      def addInPlace(v1: Vector, v2: Vector): Vector = {
        v1 += v2
      }
    }


// Then, create an Accumulator of this type:
val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam)

However, when I try to use this I get an error:

scala> import org.apache.spark.AccumulatorParam
import org.apache.spark.AccumulatorParam

scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
     |   def zero(initialValue: Vector): Vector = {
     |     Vector.zeros(initialValue.size)
     |   }
     |   def addInPlace(v1: Vector, v2: Vector): Vector = {
     |     v1 += v2
     |   }
     | }
<console>:12: error: type Vector takes type parameters
       object VectorAccumulatorParam extends AccumulatorParam[Vector] {
                                                              ^


Last thing, am I posting on the wrong list?



-----
Cheers,

Stephanie
--

"
Sean Owen <sowen@cloudera.com>,"Mon, 9 Jun 2014 12:20:49 -0400",Re: implementing the VectorAccumulatorParam,"""dev@spark.apache.org"" <dev@spark.apache.org>, user@spark.apache.org","(The user@ list might be a bit better but I can see why it might look
like a dev@ question.)

Did you import org.apache.spark.mllib.linalg.Vector ? I think you are
picking up Scala's Vector class instead.



"
dataginjaninja <rickett.stephanie@gmail.com>,"Mon, 9 Jun 2014 09:22:54 -0700 (PDT)",Re: implementing the VectorAccumulatorParam,dev@spark.incubator.apache.org,"You are right. I was using the wrong vector class. Thanks.



-----
Cheers,

Stephanie
--

"
dataginjaninja <rickett.stephanie@gmail.com>,"Mon, 9 Jun 2014 09:25:32 -0700 (PDT)",Re: implementing the VectorAccumulatorParam,dev@spark.incubator.apache.org,"New error :-(

scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
     |   def zero(initialValue: Vector): Vector = {
     |     Vector.zeros(initialValue.size)
     |   }
     |   def addInPlace(v1: Vector, v2: Vector): Vector = {
     |     v1 += v2
     |   }
     | }
<console>:12: error: not found: type AccumulatorParam
       object VectorAccumulatorParam extends AccumulatorParam[Vector] {
                                             ^
<console>:14: error: value zeros is not a member of object
scala.collection.immutable.Vector
           Vector.zeros(initialValue.size)
                  ^
<console>:17: error: value += is not a member of
org.apache.spark.mllib.linalg.Vector
           v1 += v2
              ^




-----
Cheers,

Stephanie
--

"
dataginjaninja <rickett.stephanie@gmail.com>,"Mon, 9 Jun 2014 09:27:25 -0700 (PDT)",Re: implementing the VectorAccumulatorParam,dev@spark.incubator.apache.org,"New error :-(

scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
     |   def zero(initialValue: Vector): Vector = {
     |     Vector.zeros(initialValue.size)
     |   }
     |   def addInPlace(v1: Vector, v2: Vector): Vector = {
     |     v1 += v2
     |   }
     | }
<console>:14: error: value zeros is not a member of object
scala.collection.immutable.Vector
           Vector.zeros(initialValue.size)
                  ^
<console>:17: error: value += is not a member of
org.apache.spark.mllib.linalg.Vector
           v1 += v2



-----
Cheers,

Stephanie
--

"
Sean Owen <sowen@cloudera.com>,"Mon, 9 Jun 2014 12:33:12 -0400",Re: implementing the VectorAccumulatorParam,user@spark.apache.org,"(BCC dev@)

The example is out of date with respect to current Vector class. The
zeros() method is on ""Vectors"". There is not currently a += operation
for Vector anymore.

To be fair the example doesn't claim this illustrates use of the Spark
Vector class but it did work with the now-deprecated Vector.

Make sure you still have AccumulableParam imported.

You could make a PR to adjust the example to something that works with
the newer class once you have it working.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Jun 2014 22:17:40 -0700",Emergency maintenace on jenkins,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just a heads up - due to an outage at UCB we've lost several of the
Jenkins slaves. I'm trying to spin up new slaves on EC2 in order to
compensate, but this might fail some ongoing builds.

The good news is if we do get it working with EC2 workers, then we
will have burst capability in the future - e.g. on release deadlines.
So it's not all bad!

- Patrick

"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 9 Jun 2014 23:35:05 -0700",Re: Emergency maintenace on jenkins,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for letting us know Patrick.

- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 10 Jun 2014 01:15:59 -0700",Re: Emergency maintenace on jenkins,"""dev@spark.apache.org"" <dev@spark.apache.org>","No luck with this tonight - unfortunately our Python tests aren't
working well with Python 2.6 and some other issues made it hard to get
the EC2 worker up to speed. Hopefully we can have this up and running
tomororw.

- Patrick


"
DanielH <dhurwitz@ebay.com>,"Tue, 10 Jun 2014 00:36:59 -0700 (PDT)",Re: debugger,dev@spark.incubator.apache.org,"Hi Josh,

I came across this post when looking for a debugger or RDD visualization
tool for Spark. I am using Spark 0.9.1 and upgrading soon to Spark 1.0. The
links you posted are dead. Can you please direct me to how I can debug my
existing Spark job.

Will I need to edit my existing job's code in addition to setting any
environment variables/parameters.

The problem: I am running Bagel on a very large graph and when the job gets
to the final step (saveAsTextFile) it will hang for up to many days until I
kill it. Oftentimes if I simply rerun the job, it will finish in an hour
which is the expected amount of time it should take.

Thanks!



--

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 10 Jun 2014 15:18:41 -0700",Re: Emergency maintenace on jenkins,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey just to update people - as of around 1pm PT we were back up and
running with Jenkins slaves on EC2. Sorry about the disruption.

- Patrick


"
=?utf-8?B?55Sz5q+F5p2w?= <henry.yijieshen@gmail.com>,"Wed, 11 Jun 2014 10:39:45 +0800",Run ScalaTest inside Intellij IDEA ,dev@spark.apache.org,"Hi All,

I want to run ScalaTest Suite in IDEA directly, but it seems didn’t pass the make phase before test running.
The problems are as follows:

/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark/executor/MesosExecutorBackend.scala
Error:(44, 35) type mismatch;
 found   : org.apache.mesos.protobuf.ByteString
 required: com.google.protobuf.ByteString
      .setData(ByteString.copyFrom(data))
                                  ^
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala
Error:(119, 35) type mismatch;
 found   : org.apache.mesos.protobuf.ByteString
 required: com.google.protobuf.ByteString
      .setData(ByteString.copyFrom(createExecArg()))
                                  ^
Error:(257, 35) type mismatch;
 found   : org.apache.mesos.protobuf.ByteString
 required: com.google.protobuf.ByteString
      .setData(ByteString.copyFrom(task.serializedTask))
                                  ^

Before I run test in IDEA, I build spark through ’sbt/sbt assembly’,
import projects into IDEA after ’sbt/sbt gen-idea’, 
and able to run test in Terminal ’sbt/sbt test’

Are there anything I leave out in order to run/debug testsuite inside IDEA?

Best regards,
Yijie
"
innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>,"Wed, 11 Jun 2014 12:10:38 +0900",Suggestion: rdd.compute(),<dev@spark.apache.org>,"Hi,

Regarding the following scenario, Would it be nice to have an action method
named like 'compute()' that does nothing but computing/materializing the
whole partitions of an RDD?
It can also be useful for the profiling.



What I (seems to) know about RDD persisting API is as follows:
- cache() and persist() is not an action. It only does a marking.
- unpersist() is also not an action. It only removes a marking. But if the
rdd is already in memory, it is unloaded.

And there seems no API to forcefully materialize the RDD without requiring a
data by an action method, for example first().

So, I am faced with the following scenario.

{
    JavaRDD<T> rddUnion = sc.parallelize(new ArrayList<T>());  // create
empty for merging
    for (int i = 0; i < 10; i++)
    {
        JavaRDD<T2> rdd = sc.textFile(inputFileNames[i]);
        rdd.cache();  // Since it will be used twice, cache.
        rdd.map(...).filter(...).saveAsTextFile(outputFileNames[i]);  //
Transform and save, rdd materializes
        rddUnion = rddUnion.union(rdd.map(...).filter(...));  // Do another
transform to T and merge by union
        rdd.unpersist();  // Now it seems not needed. (But needed actually)
    }
    // Here, rddUnion actually materializes, and needs all 10 rdds that
already unpersisted.
    // So, rebuilding all 10 rdds will occur.
    rddUnion.saveAsTextFile(mergedFileName);
}

If rddUnion can be materialized before the rdd.unpersist() line and
cache()d, the rdds in the loop will not be needed on
rddUnion.saveAsTextFile().

Now what is the best strategy?
- Do not unpersist all 10 rdds in the loop.
- Materialize rddUnion in the loop by calling 'light' action API, like
first().
- Give up and just rebuild/reload all 10 rdds when saving rddUnion.

Is there some misunderstanding?

Thanks.



"
Ankur Dave <ankurdave@gmail.com>,"Tue, 10 Jun 2014 20:16:46 -0700",Re: Suggestion: rdd.compute(),dev@spark.apache.org,"You can achieve an equivalent effect by calling rdd.foreach(x => {}), which
is the lightest possible action that forces materialization of the whole
RDD.

Ankur <http://www.ankurdave.com/>
"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Wed, 11 Jun 2014 11:17:51 +0800",Re: Run ScalaTest inside Intellij IDEA,"dev@spark.apache.org, henry.yijieshen@gmail.com","I also run into this problem when running examples in IDEA. The issue looks
that it uses depends on too many jars and that the classpath seems to have
length limit. So I import the assembly jar and put the head of the list
dependent path and it works.

Thanks,
Qiuzhuang



™t pass
k/executor/MesosExecutorBackend.scala
k/scheduler/cluster/mesos/MesosSchedulerBackend.scala
yâ€™,
A?
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 10 Jun 2014 20:56:13 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Hi,

I am bit confused wiht the code here:

// Solve the least-squares problem for each user and return the new feature
vectors

    Array.range(0, numUsers).map { index =>

      // Compute the full XtX matrix from the lower-triangular part we got
above

      fillFullMatrix(userXtX(index), fullXtX)

      // Add regularization

      var i = 0

      while (i < rank) {

        fullXtX.data(i * rank + i) += lambda

        i += 1

      }

      // Solve the resulting matrix, which is symmetric and
positive-definite

      algo match {

        case ALSAlgo.Implicit =>
Solve.solvePositive(fullXtX.addi(YtY.get.value),
userXy(index)).data

        case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
(index)).data

      }

    }



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 10 Jun 2014 20:58:24 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"Sorry last one went out by mistake:

Is not for users (0 to numUsers), fullXtX is same ? In the ALS formulation
this is W^TW or H^TH which should be same for all the users ? Why we are
reading userXtX(index) and adding it to fullXtX in the loop over all
numUsers ?

// Solve the least-squares problem for each user and return the new feature
vectors

    Array.range(0, numUsers).map { index =>

      // Compute the full XtX matrix from the lower-triangular part we got
above

      fillFullMatrix(userXtX(index), fullXtX)

      // Add regularization

      var i = 0

      while (i < rank) {

        fullXtX.data(i * rank + i) += lambda

        i += 1

      }

      // Solve the resulting matrix, which is symmetric and
positive-definite

      algo match {

        case ALSAlgo.Implicit =>
Solve.solvePositive(fullXtX.addi(YtY.get.value),
userXy(index)).data

        case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
(index)).data

      }

    }



"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 11 Jun 2014 00:21:29 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"For explicit feedback, ALS uses only observed ratings for computation.
So XtXs are not the same. -Xiangrui


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 11 Jun 2014 02:20:09 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"I got it...ALS formulation is solving the matrix completion problem....

To convert the problem to matrix factorization or take user feedback
(missing entries means the user hate the site ?), we should put 0 to the
missing entries (or may be -1)...in that case we have to use computeYtY and
accumulate over users in each block to generate full gram matrix...and
after that while computing userXy(index) we have to be careful in putting
0/-1 for rest of the features...

Is implicit feedback trying to do something like this ?

Basically I am trying to see if it is possible to cache the gram matrix and
it's cholesky factorization, and then call the QpSolver multiple times with
updated gradient term...I am expecting better runtimes than dposv when
ranks are high...

But seems like that's not possible without a broadcast step which might
kill all the runtime gain...



"
Yijie Shen <henry.yijieshen@gmail.com>,"Wed, 11 Jun 2014 21:51:19 +0800",Re: Run ScalaTest inside Intellij IDEA ,"Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,
 dev@spark.apache.org","Thx Qiuzhuang, the problems disappeared after I add assembly jar at the head of list dependencies in *.iml, but while running test in Spark SQL(SQLQuerySuite in sql-core), another two error occurs:

Error 1: 
Error:scalac: 
     while compiling: /Users/yijie/code/apache.spark.master/sql/core/src/main/scala/org/apache/spark/sql/test/TestSQLContext.scala
        during phase: jvm
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args: -Xmax-classfile-name 120 -deprecation -P:genjavadoc:out=/Users/yijie/code/apache.spark.master/sql/core/target/java -feature -classpath /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Conteâ€¦
â€¦
...
/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-library-2.10.4.jar -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc-plugin_2.10.4-0.5.jar -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc-plugin_2.10.4-0.5.jar
  last tree to typer: Literal(Constant(parquet.io.api.Converter))
              symbol: null
   symbol definition: null
                 tpe: Class(classOf[parquet.io.api.Converter])
       symbol owners: 
      context owners: object TestSQLContext -> package test
== Enclosing template or block ==
Template( // val <local TestSQLContext>: <notype> in object TestSQLContext, tree.tpe=org.apache.spark.sql.test.TestSQLContext.type
  ""org.apache.spark.sql.SQLContext"" // parents
  ValDef(
    private
    ""_""
    <tpt>
    <empty>
  )
  // 2 statements
  DefDef( // private def readResolve(): Object in object TestSQLContext
    <method> private <synthetic>
    ""readResolve""
    []
    List(Nil)
    <tpt> // tree.tpe=Object
    test.this.""TestSQLContext"" // object TestSQLContext in package test, tree.tpe=org.apache.spark.sql.test.TestSQLContext.type
  )
  DefDef( // def <init>(): org.apache.spark.sql.test.TestSQLContext.type in object TestSQLContext
    <method>
    ""<init>""
    []
    List(Nil)
    <tpt> // tree.tpe=org.apache.spark.sql.test.TestSQLContext.type
    Block( // tree.tpe=Unit
      Apply( // def <init>(sparkContext: org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=org.apache.spark.sql.SQLContext
        TestSQLContext.super.""<init>"" // def <init>(sparkContext: org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=(sparkContext: org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext
        Apply( // def <init>(master: String,appName: String,conf: org.apache.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=org.apache.spark.SparkContext
          new org.apache.spark.SparkContext.""<init>"" // def <init>(master: String,appName: String,conf: org.apache.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=(master: String, appName: String, conf: org.apache.spark.SparkConf)org.apache.spark.SparkContext
          // 3 arguments
          ""local""
          ""TestSQLContext""
          Apply( // def <init>(): org.apache.spark.SparkConf in class SparkConf, tree.tpe=org.apache.spark.SparkConf
            new org.apache.spark.SparkConf.""<init>"" // def <init>(): org.apache.spark.SparkConf in class SparkConf, tree.tpe=()org.apache.spark.SparkConf
            Nil
          )
        )
      )
      ()
    )
  )
)
== Expanded type of tree ==
ConstantType(value = Constant(parquet.io.api.Converter))
uncaught exception during compilation: java.lang.AssertionError

Error 2:

Error:scalac: Error: assertion failed: List(object package$DebugNode, object package$DebugNode)
java.lang.AssertionError: assertion failed: List(object package$DebugNode, object package$DebugNode)
	at scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)
	at scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:2988)
	at scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:2991)
	at scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenASM.scala:1371)
	at scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:120)
	at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
	at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
	at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
	at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
	at xsbt.CachedCompiler0.run(CompilerInterface.scala:126)
	at xsbt.CachedCompiler0.run(CompilerInterface.scala:102)
	at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
	at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
	at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
	at org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(IdeaIncrementalCompiler.scala:28)
	at org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:25)
	at org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:64)
	at org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:22)
	at org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)



looks that it uses depends on too many jars and that the classpath seems to have length limit. So I import the assembly jar and put the head of the list dependent path and it works.
t pass the make phase before test running.
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark/executor/MesosExecutorBackend.scala
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala
assemblyâ€™,
IDEA?

"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Wed, 11 Jun 2014 22:16:49 +0800",Re: Run ScalaTest inside Intellij IDEA,Yijie Shen <henry.yijieshen@gmail.com>,"I run into this issue too today via 'mvn install -DskipTests' command
today, then I issue a mvn clean and rebuild and it works.

Thanks,
Qiuzhuang



spark/sql/test/TestSQLContext.scala
/java
avafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Conteâ€¦
jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-library-2.10.4.jar
c-plugin_2.10.4-0.5.jar
c-plugin_2.10.4-0.5.jar
:
,
:2988)
2991)
71)
:57)
mpl.java:43)
(IdeaIncrementalCompiler.scala:28)
.scala:25)
2)
:57)
mpl.java:43)
to
™t pass
rk/executor/MesosExecutorBackend.scala
rk/scheduler/cluster/mesos/MesosSchedulerBackend.scala
lyâ€™,
"
SURAJ SHETH <shethsh@gmail.com>,"Wed, 11 Jun 2014 21:14:50 +0530","MLLib : Decision Tree not getting built for 5 or more
 levels(maxDepth=5) and the one built for 3 levels is performing poorly","user@spark.apache.org, dev@spark.apache.org","Hi,
I have been trying to build a Decision Tree using a dataset that I have.

Dataset Decription :

Train data size = 689,763

Test data size = 8,387,813

Each row in the dataset has 321 numerical features out of which 139th value
is the ground truth.

The number of positives in the dataset is low. Number of positives = 12028

There are absolutely no missing values in the dataset. This is ensured by
preprocessing the dataset.


The outcome against which we are building the tree is a binary variable
taking values 0 OR 1.

Due to a few reasons, I am building a Regression Tree and not a
Classification Tree.


When we have 3 levels(maxDepth = 3), we get the tree immediately(a few
minutes), but it is performing poorly. When I computed the correlation
coefficient between the ground truth scores and the scores obtained by the
tree, I get a correlation coefficient of 0.013140 which is very low.


Even looking at individual predictions manually, it is seen that the
predictions are almost same at around 0.07 to 0.09 irrespective of whether
the particular row is positive or negative in the ground truth.


When the maxDepth is set to 5, it doesn't complete building the tree even
after several hours.


When I include the ground truth in the train data, it builds the tree in a
very small amount of time and even the predictions are correct, accuracy is
around 100%.(As Expected)


So, I have two queries :

1) Why is the performance so poor when we have maxDepth = 3 ?

2) Why isn't building a Regression Decision Tree feasible with maxDepth = 5
?


Here is the core part of the code I am using :

    val ssc = new SparkContext(sparkMaster, ""Spark exp 001"", sparkHome,
jars)
    val labelRDD = ssc.textFile(hdfsNN + ""Path to data /training/part*, 12)
                     .map{st =>
                       val parts = st.split("","").map(_.toDouble)
                       LabeledPoint(parts(138), Vectors.dense((parts take
138) ++ (parts drop 139)))}
    print(labelRDD.first)

    val model = DecisionTree.train(labelRDD, Regression, Variance, 3)
    val parsedData = ssc.textFile(hdfsNN + ""Path to data /testing/part*"",
12)
                     .map{st =>
                       val parts = st.split("","").map(_.toDouble)
                       LabeledPoint(parts(138), Vectors.dense((parts take
138) ++ (parts drop 139)))}
    val labelAndPreds = parsedData.map { point =>
        val prediction = model.predict(point.features)
        (point.label, prediction)
    }
    labelAndPreds.saveAsTextFile(hdfsNN + ""Output path /labels"")

When I build a Random Forest for the same dataset using Mahout, it builds
the forest in less than 5 minutes and gives a good accuracy. The amount of
memory and other resources available to Spark and to Mahout are comparable.

Spark had a memory of 30GB * 3 workers = 90GB in total.

Thanks and Regards,
Suraj Sheth
"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Wed, 11 Jun 2014 12:17:51 -0400",Re: Error During ReceivingConnection,Surendranauth Hiraman <suren.hiraman@velos.io>,"It looks like this was due to another executor on a different node closing
the connection on its side. I found the entries below in the remote side's
logs.

Can anyone comment on why one ConnectionManager would close its connection
to another node and what could be tuned to avoid this? It did not have any
errors on its side.


This is from the ConnectionManager on the side shutting down the
connection, not the ConnectionManager that had the ""Connection Reset By
Peer"".

14/06/10 18:51:14 INFO network.ConnectionManager: Removing
ReceivingConnection to ConnectionManagerId(172.16.25.125,45610)

14/06/10 18:51:14 INFO network.ConnectionManager: Removing
SendingConnection to ConnectionManagerId(172.16.25.125,45610)







-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Wed, 11 Jun 2014 17:56:23 -0400",Compression with DISK_ONLY persistence,"""dev@spark.apache.org"" <dev@spark.apache.org>, user@spark.apache.org","Hi,

Will spark.rdd.compress=true enable compression when using DISK_ONLY
persistence?



SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 11 Jun 2014 15:21:59 -0700",Re: Constraint Solver for Spark,dev@spark.apache.org,"You idea is close to what implicit feedback does. You can check the
paper, which is short and concise. In the ALS setting, all subproblems
are independent in each iteration. This is part of the reason why ALS
is scalable. If you have some global constraints that make the
subproblems no longer decoupled, that would certainly affects
scalability. -Xiangrui


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 11 Jun 2014 15:47:24 -0700",Re: Compression with DISK_ONLY persistence,user@spark.apache.org,"Yes, actually even if you don’t set it to true, on-disk data is compressed. (This setting only affects serialized data in memory).

Matei


persistence? 

"
Yijie Shen <henry.yijieshen@gmail.com>,"Thu, 12 Jun 2014 09:07:00 +0800",Re: Run ScalaTest inside Intellij IDEA ,Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"I got a clean version of the master branch, and do the steps as follows:

export MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512mâ€
mvn -U -Dhadoop.version=2.2.0 -DskipTests clean package

after these steps, I open the project in IDEA through pom.xml in the root folder, but while run the same test SQLQuerySuite in sql-core, the two errors above still occurs, any ideas? 


today, then I issue a mvn clean and rebuild and it works.
the head of list dependencies in *.iml, but while running test in Spark SQL(SQLQuerySuite in sql-core), another two error occurs:
/Users/yijie/code/apache.spark.master/sql/core/src/main/scala/org/apache/spark/sql/test/TestSQLContext.scala
-P:genjavadoc:out=/Users/yijie/code/apache.spark.master/sql/core/target/java -feature -classpath /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Conteâ€¦
/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-library-2.10.4.jar -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc-plugin_2.10.4-0.5.jar -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc-plugin_2.10.4-0.5.jar
TestSQLContext, tree.tpe=org.apache.spark.sql.test.TestSQLContext.type
TestSQLContext
test, tree.tpe=org.apache.spark.sql.test.TestSQLContext.type
org.apache.spark.sql.test.TestSQLContext.type in object TestSQLContext
org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=org.apache.spark.sql.SQLContext
org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=(sparkContext: org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext
org.apache.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=org.apache.spark.SparkContext
<init>(master: String,appName: String,conf: org.apache.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=(master: String, appName: String, conf: org.apache.spark.SparkConf)org.apache.spark.SparkContext
SparkConf, tree.tpe=org.apache.spark.SparkConf
org.apache.spark.SparkConf in class SparkConf, tree.tpe=()org.apache.spark.SparkConf
object package$DebugNode)
package$DebugNode, object package$DebugNode)
scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)
scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:2988)
scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:2991)
scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenASM.scala:1371)
scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:120)
scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(IdeaIncrementalCompiler.scala:28)
org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:25)
org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:64)
org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:22)
org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
looks that it uses depends on too many jars and that the classpath seems to have length limit. So I import the assembly jar and put the head of the list dependent path and it works.
™t pass the make phase before test running.
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark/executor/MesosExecutorBackend.scala
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala
assemblyâ€™,
IDEA?

"
"""=?GBK?B?0+C4+cOvKNPguPnDryk=?="" <genmao.ygm@alibaba-inc.com>","Thu, 12 Jun 2014 21:03:59 +0800",GraphX can not unpersist edges of old graph?,<dev@spark.apache.org>,"Hi All,

         

I know the benefit of RDD caching, but abuse of using cache may cause memory
leak. In graphx, we can cache a graph by using graph.cache(), and many
transformation of graph create and cache new edges, like partitionBy() and
subgraph(). However, I can not find an interface to unpersist edges. I
wonder the purpose of design like this. It can indeed improve performance.
But, it may lead to memory leak if not unpersist edges in some cases. For
example, in Spark-1.0,  the pregel can not unpersist edges of old graph
effectively, and leads to memory leak.

 

 

Much Thanks!

 

"
Ankur Dave <ankurdave@gmail.com>,"Thu, 12 Jun 2014 13:47:33 -0700",Re: GraphX can not unpersist edges of old graph?,"dev@spark.apache.org, =?UTF-8?B?5L2Z5qC56IyCKOS9meagueiMgik=?= <genmao.ygm@alibaba-inc.com>","We didn't provide an unpersist API for Graph because the internal
dependency structure of a graph can make it hard to unpersist correctly in
a way that avoids recomputation. However, you can directly unpersist a
graph's vertices and edges RDDs using graph.vertices.unpersist() and
graph.edges.unpersist().

By the way, the memory leak bug with Pregel (SPARK-2025
<https://issues.apache.org/jira/browse/SPARK-2025>) is fixed in master.

Ankur <http://www.ankurdave.com/>
"
Yanjie Gao <396154235@qq.com>,"Thu, 12 Jun 2014 18:53:46 -0700 (PDT)",Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.incubator.apache.org,"Hi All,
We are facing a Serialization issue ,



This issue has been submit to JIRA

https://issues.apache.org/jira/browse/SPARK-2018





We have an application run on Spark on Power7 System .
But we meet an important issue about serialization.
The example HdfsWordCount can meet the problem.
./bin/run-example org.apache.spark.examples.streaming.HdfsWordCount localdir
We used Power7 (Big-Endian arch) and Redhat 6.4.
Big-Endian is the main cause since the example ran successfully in another
Power-based Little Endian setup.
here is the exception stack and log:
Spark Executor Command: ""/opt/ibm/java-ppc64-70//bin/java"" ""-cp""
""/home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/""
""-XX:MaxPermSize=128m"" ""-Xdebug""
""-Xrunjdwp:transport=dt_socket,address=99999,server=y,suspend=n"" ""-Xms512M""
""-Xmx512M"" ""org.apache.spark.executor.CoarseGrainedExecutorBackend""
""akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler"" ""2""
""p7hvs7br16"" ""4"" ""akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker""
""app-20140604023054-0000""
========================================
14/06/04 02:31:20 WARN util.NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable
14/06/04 02:31:21 INFO spark.SecurityManager: Changing view acls to:
test1,yifeng
14/06/04 02:31:21 INFO spark.SecurityManager: SecurityManager:
authentication disabled; ui acls disabled; users with view permissions:
Set(test1, yifeng)
14/06/04 02:31:22 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/06/04 02:31:22 INFO Remoting: Starting remoting
14/06/04 02:31:22 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://sparkExecutor@p7hvs7br16:39658]
14/06/04 02:31:22 INFO Remoting: Remoting now listens on addresses:
[akka.tcp://sparkExecutor@p7hvs7br16:39658]
14/06/04 02:31:22 INFO executor.CoarseGrainedExecutorBackend: Connecting to
driver: akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler
14/06/04 02:31:22 INFO worker.WorkerWatcher: Connecting to worker
akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker
14/06/04 02:31:23 INFO worker.WorkerWatcher: Successfully connected to
akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker
14/06/04 02:31:24 INFO executor.CoarseGrainedExecutorBackend: Successfully
registered with driver
14/06/04 02:31:24 INFO spark.SecurityManager: Changing view acls to:
test1,yifeng
14/06/04 02:31:24 INFO spark.SecurityManager: SecurityManager:
authentication disabled; ui acls disabled; users with view permissions:
Set(test1, yifeng)
14/06/04 02:31:24 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/06/04 02:31:24 INFO Remoting: Starting remoting
14/06/04 02:31:24 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://spark@p7hvs7br16:58990]
14/06/04 02:31:24 INFO Remoting: Remoting now listens on addresses:
[akka.tcp://spark@p7hvs7br16:58990]
14/06/04 02:31:24 INFO spark.SparkEnv: Connecting to MapOutputTracker:
akka.tcp://spark@9.186.105.141:60253/user/MapOutputTracker
14/06/04 02:31:25 INFO spark.SparkEnv: Connecting to BlockManagerMaster:
akka.tcp://spark@9.186.105.141:60253/user/BlockManagerMaster
14/06/04 02:31:25 INFO storage.DiskBlockManager: Created local directory at
/tmp/spark-local-20140604023125-3f61
14/06/04 02:31:25 INFO storage.MemoryStore: MemoryStore started with
capacity 307.2 MB.
14/06/04 02:31:25 INFO network.ConnectionManager: Bound socket to port 39041
with id = ConnectionManagerId(p7hvs7br16,39041)
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Trying to register
BlockManager
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Registered BlockManager
14/06/04 02:31:25 INFO spark.HttpFileServer: HTTP File server directory is
/tmp/spark-7bce4e43-2833-4666-93af-bd97c327497b
14/06/04 02:31:25 INFO spark.HttpServer: Starting HTTP Server
14/06/04 02:31:25 INFO server.Server: jetty-8.y.z-SNAPSHOT
14/06/04 02:31:26 INFO server.AbstractConnector: Started
SocketConnector@0.0.0.0:39958
14/06/04 02:31:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned
task 2
14/06/04 02:31:26 INFO executor.Executor: Running task ID 2
14/06/04 02:31:26 ERROR executor.Executor: Exception in task ID 2
java.io.InvalidClassException: scala.reflect.ClassTag$$anon$1; local class
incompatible: stream classdesc serialVersionUID = -8102093212602380348,
local class serialVersionUID = -4937928798201944954
at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:678)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1678)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1573)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1827)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
at
org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)
at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1893)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1852)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
at
org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:781)
14/06/04 02:31:26 ERROR executor.CoarseGrainedExecutorBackend: Driver
Disassociated [akka.tcp://sparkExecutor@p7hvs7br16:39658] ->
[akka.tcp://spark@9.186.105.141:60253] disassociated! Shutting down.

Now I have some new detection.
(1) Compare
server Spark exec mode pass or not jdk 
x86(Little Endian) Local+cluster pass x86
p8(Little Endian) Local+cluster pass IBM(little endian)
P7(Big Endian) Local mode pass(I change some jar classpath then can't pass)
IBM(Big endian)
P7 (Big Endian) Cluster mode not IBM(Big endian)
(2) The Exception priciple
2.1 Main Error : Exception in thread ""main"" org.apache.spark.SparkException:
Job aborted due to stage failure: Task 1.0:0 failed 4 times, most recent
failure: Exception failure in TID 3 on host arlab105.austin.ibm.com:
java.io.InvalidClassException: org.apache.spark.SerializableWritable; local
class incompatible: stream classdesc serialVersionUID = 6301214776158303468,
local class serialVersionUID = -7785455416944904980
(other may has the same reason)
2.2 Exception in thread ""main"" org.apache.spark.SparkException: Job aborted
due to stage failure: Task 0.0:0 failed 1 times, most recent failure:
Exception failure in TID 1 on host localhost: java.io.InvalidClassException:
scala.Tuple2; invalid descriptor for field _1
Now we analysis 2.1 Bug .
refer:
serialVersionUID has two generate method
1 default 1Lprivate static final long serialVersionUID = 1L
2 Generated by hash. Class name ,interface name ,method ,attribute can
affect the result.
Our error is not 1L .So it generated by method 2.
UID is used when the process deserialize the byte array .the process read
the local class file ,and find the class's UID.If it is diff with the array
.Then throw the Exception.
Let's see the work flow of Spark Serilization
Local mode 
once serialize 
object ----serialize(thread1 or thread2)-->array----deserialize(thread2 or
process2)--->object
Cluster mode
twice serialize
object ---serialize(thread1 or thread2)-->array-Actor send message serialize
--->message-->Actor receive and deserialize it ----->array
------deserialize(thread2 or process2)--->object
summary:
let't compare (1) 's four situation.
I think the reason is that IBM jdk and (scala lib and akka lib) may have
some intersection of some class. But they compile in diff platform use diff
javac .They may generate diff UID. 
In run time .jvm may load the same class from diff .class file.
(3)Method to fix it.
I think
The reason is the same class load diff class file.
There are two method .May be there are other better method.
4.1 Let the two file has the same version UID:Compile scala lib and akka lib
in P7 platform
4.2 Let the two loader load the same Jar. Use some method like extend class
loader or OSGI .We force the jvm to load the same class file.(The difficult
thing is that classes is in jar and class num is too large .)




But  I want to know  what's the real reason of this bug ?
How can we fast fix this bug?


Thanks a lot

Best Regards!
Yanjie Gao 



--

"
Priya Ch <learnings.chitturi@gmail.com>,"Tue, 10 Jun 2014 15:13:17 +0530",Subscription request for developer community,dev@spark.apache.org,"Please accept the request
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 12 Jun 2014 22:10:50 -0700",Fwd: ApacheCon CFP closes June 25,"user@spark.apache.org,
 dev@spark.apache.org","(I’m forwarding this message on behalf of the ApacheCon organizers, who’d like to see involvement from every Apache project!)

As you may be aware, ApacheCon will be held this year in Budapest, on November 17-23. (See http://apachecon.eu for more info.)

The Call For Papers for that conference is still open, but will be closing soon. We need you talk proposals, to represent Spark at ApacheCon. We need all kinds of talks - deep technical talks, hands-on tutorials, introductions for beginners, or case studies about the awesome stuff you're doing with Spark.

Please consider submitting a proposal, at http://events.linuxfoundation.org//events/apachecon-europe/program/cfp

Thanks!

Matei"
Reynold Xin <rxin@databricks.com>,"Fri, 13 Jun 2014 00:15:53 -0700",openstack swift integration with Spark,"user@spark.apache.org, dev@spark.apache.org","If you are interested in openstack/swift integration with Spark, please
drop me a line. We are looking into improving the integration.

Thanks.
"
Yanjie Gao <396154235@qq.com>,"Fri, 13 Jun 2014 18:40:44 -0700 (PDT)",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.incubator.apache.org,"Hi All
Yesterday,we rewrite the class Serializable's UID  as static .Then find that
the Executor could resolve the class .But it also thow the Exception
imcompitable UID.We also debug the program in remote mode found jvm use
bigendian in master and slave.

1.0.0 then it works.
We  think that Spark-1.0.0  may  fix something.


But run on standalone mode use kyro or java serilazer  it also has the
problem  .

May be  Spark cause this problem  on  Big Endian environment?

Thanks a lot!



--

"
"""anishsneh@yahoo.co.in"" <anishsneh@yahoo.co.in>","Sat, 14 Jun 2014 14:06:57 +0800 (SGT)",Fw: How Spark Choose Worker Nodes for respective HDFS block,"""user@spark.apache.org"" <user@spark.apache.org>,
  ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All

Is there any communication between Spark MASTER node and Hadoop NameNode while distributing work to WORKER nodes, like we have in MapReduce.

Please suggest

TIA

-- 
Anish Sneh
""Experience is the best teacher.""
http://in.linkedin.com/in/anishsneh

"
Matthew Farrellee <matt@redhat.com>,"Sat, 14 Jun 2014 11:52:49 -0400",Re: Apache Spark and Swift object store,dev@spark.apache.org,"
gil,

the sahara project within openstack is also taking on this effort.

https://wiki.openstack.org/wiki/Sahara/SparkPlugin

there's currently a plugin to provision a spark cluster on openstack and 
folks on #openstack-sahara will be very interested to hear what you're 
working on.

the theory atm is that the work done to create the swift dfs plugin will 
easily integrate spark and swift, and it's great to see that your patch 
suggests this works in practice.

best,


matt


"
Gil Vernik <GILV@il.ibm.com>,"Sat, 14 Jun 2014 20:02:11 +0300",Re: Apache Spark and Swift object store,dev@spark.apache.org,"Hi Matthew,

Thanks for the information about Spark plugin for  Sahara.
I will send emails to Sahara and Swift mailing lists notifying them about 
this patch.

All the best,
Gil.





From:   Matthew Farrellee <matt@redhat.com>
To:     dev@spark.apache.org, 
Date:   14/06/2014 06:53 PM
Subject:        Re: Apache Spark and Swift object store



it
mainly
community!

gil,

the sahara project within openstack is also taking on this effort.

https://wiki.openstack.org/wiki/Sahara/SparkPlugin

there's currently a plugin to provision a spark cluster on openstack and 
folks on #openstack-sahara will be very interested to hear what you're 
working on.

the theory atm is that the work done to create the swift dfs plugin will 
easily integrate spark and swift, and it's great to see that your patch 
suggests this works in practice.

best,


matt


"
Nan Zhu <zhunanmcgill@gmail.com>,"Sat, 14 Jun 2014 21:15:40 -0400",review of two PRs,dev@spark.apache.org,"Hi, all 

Any admin wants to review these two PRs?

https://github.com/apache/spark/pull/637 (to make DAGScheduler self-contained)

https://github.com/apache/spark/pull/731 (enable multiple executors per Worker)

Thanks 

-- 
Nan Zhu

"
gchen <chenguancheng@gmail.com>,"Sun, 15 Jun 2014 19:04:38 -0700 (PDT)",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.incubator.apache.org,"To anyone who is interested in this issue, the root cause if from a third
party code com.ning.compress.lzf.impl.UnsafeChunkEncoderBE class since they
have a broken implementation. A bug will be raised in Ning project, thanks.



--

"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Jun 2014 23:49:38 -0700",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.apache.org,"Thanks for sending the update. Do you mind posting a link to the bug
reported in the lzf project here as well? Cheers.




"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Jun 2014 23:49:38 -0700",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.apache.org,"Thanks for sending the update. Do you mind posting a link to the bug
reported in the lzf project here as well? Cheers.




"
Bhaskar Dutta <bhaskie@gmail.com>,"Mon, 16 Jun 2014 12:38:34 +0530",Re: Spark 1.1 Window and 1.0 Wrap-up,dev@spark.apache.org,"
Hi,

Do you have a plan for 1.0.1 as well?

Thanks,
Bhaskar
"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 16 Jun 2014 15:20:18 +0530",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.apache.org,"In that case, does it work if you use snappy instead of lzf ?


Regards,
Mridul



"
gchen <chenguancheng@gmail.com>,"Mon, 16 Jun 2014 16:15:22 -0700 (PDT)",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.incubator.apache.org,"Hi Reynold, thanks for your interest on this issue. The work here is part of
incorporating Spark into PowerLinux ecosystem. 

Here is the bug raised in ning by my colleague:
https://github.com/ning/compress/issues/37

Would you mind to share whether some insights of Spark's support for Big
Enidan Arch (such as POWER7)? Has that been fully tested before? Thanks.



--

"
Reynold Xin <rxin@databricks.com>,"Mon, 16 Jun 2014 16:18:10 -0700",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.apache.org,"I think you guys are / will be leading the effort on that :)




"
Reynold Xin <rxin@databricks.com>,"Mon, 16 Jun 2014 16:18:10 -0700",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.apache.org,"I think you guys are / will be leading the effort on that :)




"
gchen <chenguancheng@gmail.com>,"Mon, 16 Jun 2014 16:22:39 -0700 (PDT)",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.incubator.apache.org,"Surely the community's kind support is essential:)



--

"
gchen <chenguancheng@gmail.com>,"Mon, 16 Jun 2014 16:26:07 -0700 (PDT)",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.incubator.apache.org,"I didn't find ning's source code in Spark git repository (or maybe I missed
it?), so next time when we meet bug caused by third party code, can we do
something (to fix the bug) based on the Spark repository?



--

"
Reynold Xin <rxin@databricks.com>,"Mon, 16 Jun 2014 16:27:40 -0700",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.apache.org,"It is here:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala



"
qingyang li <liqingyang1985@gmail.com>,"Tue, 17 Jun 2014 12:24:40 +0800",encounter jvm problem when integreation spark with mesos,dev@spark.apache.org,"hi, I encounter  jvm problem when integreation spark with mesos,
here is the log when i run ""spark-shell"":
-48ce131dc5af
14/06/17 12:24:55 INFO HttpServer: Starting HTTP Server
14/06/17 12:24:55 INFO SparkUI: Started Spark Web UI at
http://bigdata001:4040
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f94f4843d21, pid=5956, tid=140277175580416
#
# JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build
1.7.0_51-mockbuild_2014_01_15_01_39-b00)
# Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd64
compressed oops)
# Problematic frame:
# V  [libjvm.so+0x5e5d21]  JNI_CreateJavaVM+0x6551
#
# Core dump written. Default location:
/home/zjw/spark/spark-0.9.0-incubating-bin-hadoop2/core or core.5956
#
# An error report file with more information is saved as:
# /tmp/jvm-5956/hs_error.log
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   http://icedtea.classpath.org/bugzilla
#
bin/spark-shell: line 101:  5956 Aborted                 (core dumped)
$FWDIR/bin/spark-class $OPTIONS org.apache.spark.repl.Main ""$@""
"
Andrew Ash <andrew@andrewash.com>,"Mon, 16 Jun 2014 21:29:50 -0700",Compile failure with SBT on master,dev@spark.apache.org,"I can't run sbt/sbt gen-idea on a clean checkout of Spark master.

I get resolution errors on junit#junit;4.10!junit.zip(source)

As shown below:

aash@aash-mbp /tmp/git/spark$ sbt/sbt gen-idea
Using /Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home as
default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
[info] Loading project definition from
/private/tmp/git/spark/project/project
[info] Loading project definition from /private/tmp/git/spark/project
[info] Set current project to root (in build file:/private/tmp/git/spark/)
[info] Creating IDEA module for project 'assembly' ...
[info] Updating {file:/private/tmp/git/spark/}core...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[warn] [FAILED     ] junit#junit;4.10!junit.zip(source):  (0ms)
[warn] ==== local: tried
[warn]   /Users/aash/.ivy2/local/junit/junit/4.10/sources/junit.zip
[warn] ==== public: tried
[warn]   http://repo1.maven.org/maven2/junit/junit/4.10/junit-4.10.zip
[warn] ==== Maven Repository: tried
[warn]   http://repo.maven.apache.org/maven2/junit/junit/4.10/junit-4.10.zip
[warn] ==== Apache Repository: tried
[warn]
https://repository.apache.org/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== JBoss Repository: tried
[warn]
https://repository.jboss.org/nexus/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== MQTT Repository: tried
[warn]
https://repo.eclipse.org/content/repositories/paho-releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== Cloudera Repository: tried
[warn]
http://repository.cloudera.com/artifactory/cloudera-repos/junit/junit/4.10/junit-4.10.zip
[warn] ==== Pivotal Repository: tried
[warn]   http://repo.spring.io/libs-release/junit/junit/4.10/junit-4.10.zip
[warn] ==== Maven2 Local: tried
[warn]   file:/Users/aash/.m2/repository/junit/junit/4.10/junit-4.10.zip
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] ::              FAILED DOWNLOADS            ::
[warn] :: ^ see resolution messages for details  ^ ::
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] :: junit#junit;4.10!junit.zip(source)
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
sbt.ResolveException: download failed: junit#junit;4.10!junit.zip(source)

By bumping the junit dependency to 4.11 I'm able to generate the IDE files.
 Are other people having this problem or does everyone use the maven
configuration?

Andrew
"
Andrew Ash <andrew@andrewash.com>,"Mon, 16 Jun 2014 21:31:19 -0700",Re: encounter jvm problem when integreation spark with mesos,dev@spark.apache.org,"Hi qingyang,

This looks like an issue with the open source version of the Java runtime
(called OpenJDK) that causes the JVM to fail.  Can you try using the JVM
released by Oracle and see if it has the same issue?

Thanks!
Andrew



"
Ted Yu <yuzhihong@gmail.com>,"Mon, 16 Jun 2014 21:57:49 -0700",Re: Compile failure with SBT on master,dev@spark.apache.org,"I used the same command on Linux and it passed:

Linux k.net 2.6.32-220.23.1.el6.YAHOO.20120713.x86_64 #1 SMP Fri Jul 13
11:40:51 CDT 2012 x86_64 x86_64 x86_64 GNU/Linux

Cheers



"
Andrew Ash <andrew@andrewash.com>,"Mon, 16 Jun 2014 22:04:31 -0700",Re: Compile failure with SBT on master,dev@spark.apache.org,"Maybe it's a Mac OS X thing?



"
qingyang li <liqingyang1985@gmail.com>,"Tue, 17 Jun 2014 15:57:20 +0800",Re: encounter jvm problem when integreation spark with mesos,dev@spark.apache.org,"somebody else has also encountered such problem:
http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CAFC0D60983129F4F9FBAD571AA422C9A5AF8F098@MAIL-MBX1.ad.renci.org%3E


2014-06-17 12:31 GMT+08:00 Andrew Ash <andrew@andrewash.com>:

"
andy petrella <andy.petrella@gmail.com>,"Tue, 17 Jun 2014 10:12:36 +0200",Re: encounter jvm problem when integreation spark with mesos,dev@spark.apache.org,"Yep but no real resolution nor advances on this topic, since finally we've
chosen to stick with a ""compatible"" version of Mesos (0.14.1 ftm).
But I'm still convince it has to do with native libs clash :-s

 aâ„•dy â„™etrella
about.me/noootsab
[image: aâ„•dy â„™etrella on about.me]

<http://about.me/noootsab>



0983129F4F9FBAD571AA422C9A5AF8F098@MAIL-MBX1.ad.renci.org%3E
me
M
)
"
qingyang li <liqingyang1985@gmail.com>,"Tue, 17 Jun 2014 17:33:07 +0800",Re: encounter jvm problem when integreation spark with mesos,dev@spark.apache.org,"here is the core stack info:
-----------------------------
(gdb) bt
#0  0x00007fc0153fc925 in raise () from /lib64/libc.so.6
#1  0x00007fc0153fe105 in abort () from /lib64/libc.so.6
#2  0x00007fc014d78405 in os::abort(bool) ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#3  0x00007fc014ef7347 in VMError::report_and_die() ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#4  0x00007fc014d7cd8f in JVM_handle_linux_signal ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  0x00007fc014b96ce9 in jni_GetByteArrayElements ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#7  0x00007fbff70f002c in GetByteArrayElements (env=<value optimized out>,
jobj=0x7fbf8c000f80)
    at /home/zjw/jdk1.7/jdk1.7.0_51//include/jni.h:1668
#8  construct<mesos::FrameworkInfo> (env=<value optimized out>,
jobj=0x7fbf8c000f80)
    at ../../src/java/jni/construct.cpp:123
#9  0x00007fbff70f51c8 in
Java_org_apache_mesos_MesosSchedulerDriver_initialize (env=0x7fc010d189e8,
    thiz=0x7fbfd94f6830) at
../../src/java/jni/org_apache_mesos_MesosSchedulerDriver.cpp:528



2014-06-17 16:12 GMT+08:00 andy petrella <andy.petrella@gmail.com>:

e
0983129F4F9FBAD571AA422C9A5AF8F098@MAIL-MBX1.ad.renci.org%3E
m
64
6
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 17 Jun 2014 08:02:47 -0700",Re: Compile failure with SBT on master,dev@spark.apache.org,"I didn't get that error on Mac either:

java version ""1.7.0_55""
Java(TM) SE Runtime Environment (build 1.7.0_55-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)

Darwin TYus-MacBook-Pro.local 12.5.0 Darwin Kernel Version 12.5.0: Sun Sep
29 13:33:47 PDT 2013; root:xnu-2050.48.12~1/RELEASE_X86_64 x86_64



"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 17 Jun 2014 12:37:02 -0400",anyone can mark this issue as resolved?,dev@spark.apache.org,"Hi, 

Just found it occasionally 

https://issues.apache.org/jira/browse/SPARK-1471 

Best, 

-- 
Nan Zhu

"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Tue, 17 Jun 2014 13:40:06 -0400",Re: Java IO Stream Corrupted - Invalid Type AC?,"user@spark.apache.org, ""dev@spark.apache.org"" <dev@spark.apache.org>","Matt/Ryan,

Did you make any headway on this? My team is running into this also.
Doesn't happen on smaller datasets. Our input set is about 10 GB but we
generate 100s of GBs in the flow itself.

-Suren








-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Michael Giannakopoulos <miccagiann@gmail.com>,"Tue, 17 Jun 2014 18:58:08 -0400",Contribute to Spark - Need a mentor.,dev@spark.incubator.apache.org,"Hi all,

My name is Michael Giannakopoulos and I am a recent M.Sc. graduate
from University of Toronto majoring in Computer Science. I would like to
contribute in the development of this open source project. Is it possible
to work
under the supervision of a mentor? I specialize in Data Analytics,
Data Management and Machine Learning. I am currently learning the
Scala language and I have experience using Java, C++, C and Matlab. I have
already read the Spark and Shark papers.

Together with this mail you will find attached my resume.

Thank you so much for your time and your help,
Michael
"
Xiaokai Wei <xwei@palantir.com>,"Wed, 18 Jun 2014 00:00:00 +0000",Contributing to MLlib on GLM,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am an intern at PalantirTech and we are building some stuff on top of
MLlib. In Particular, GLM is of great interest to us.  Though
GeneralizedLinearModel in MLlib 1.0.0 has some important GLMs such as
Logistic Regression, Linear Regression, some other important GLMs like
Poisson Regression are still missing.

I am curious that if anyone is already working on other GLMs (e.g. Poisson,
Gamma). If not, we would like to contribute to MLlib on GLM. Is adding more
GLMs on the roadmap of MLlib?


Sincerely,

Xiaokai


"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 17 Jun 2014 18:10:37 -0700",Re: Run ScalaTest inside Intellij IDEA,"""dev@spark.apache.org"" <dev@spark.apache.org>","I got stuck on this one too after did git pull from master.

Have not been able to resolve it yet =(


- Henry

te:
ead of list dependencies in *.iml, but while running test in Spark SQL(SQLQuerySuite in sql-core), another two error occurs:
main/scala/org/apache/spark/sql/test/TestSQLContext.scala
:out=/Users/yijie/code/apache.spark.master/sql/core/target/java -feature -classpath /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Conteâ€¦
jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-library-2.10.4.jar -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc-plugin_2.10.4-0.5.jar -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc-plugin_2.10.4-0.5.jar
t, tree.tpe=org.apache.spark.sql.test.TestSQLContext.type
tree.tpe=org.apache.spark.sql.test.TestSQLContext.type
in object TestSQLContext
org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=org.apache.spark.sql.SQLContext
che.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=(sparkContext: org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext
che.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=org.apache.spark.SparkContext
: String,appName: String,conf: org.apache.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=(master: String, appName: String, conf: org.apache.spark.SparkConf)org.apache.spark.SparkContext
rkConf, tree.tpe=org.apache.spark.SparkConf
apache.spark.SparkConf in class SparkConf, tree.tpe=()org.apache.spark.SparkConf
ect package$DebugNode)
, object package$DebugNode)
678)
mbols.scala:2988)
bols.scala:2991)
SM.scala:1371)
20)
583)
orImpl.java:57)
odAccessorImpl.java:43)
2)
:48)
:41)
ler.compile(IdeaIncrementalCompiler.scala:28)
LocalServer.scala:25)
la:64)
.scala:22)
scala)
orImpl.java:57)
odAccessorImpl.java:43)
rote:
oks that it uses depends on too many jars and that the classpath seems to have length limit. So I import the assembly jar and put the head of the list dependent path and it works.
™t pass the make phase before test running.
rk/executor/MesosExecutorBackend.scala
rk/scheduler/cluster/mesos/MesosSchedulerBackend.scala
lyâ€™,
EA?

"
Gang Bai <baigang@staff.sina.com.cn>,"Wed, 18 Jun 2014 10:13:42 +0800",Int tolerance in LBFGS.setConvergenceTol causes problems,<dev@spark.apache.org>,"Hi folks,

I am implementing a regression model for count data which uses LBFGS for parameter estimation. Following the patterns in mllib.regression, I created an object PoissonRegressionModelWithLBFGS, which creates a new instance of class PoissonRegressionModelWithLBFGS and invokes the run method to get the weights/parameters. 

The implementations are straightforward. But I encountered a problem while using class LBFGS. The parameter of setConvergenceTol is of type Int rather than Double. This leads to the inability to specify a tolerance less than 1.0 and a type mismatch error when passing a Double value. In fact, the class LBFGS internally uses convergenceTol as a Double var. So we can safely change the parameter of setConvergenceTol from type Int to type Double. 

I’ve created a pull request for this. Please take a review here: https://github.com/apache/spark/pull/1104/files

Best regards,
Gang"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 17 Jun 2014 19:37:01 -0700",Re: Contributing to MLlib on GLM,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Xiaokai,

I think MLLib is definitely interested in supporting additional GLMs.  I'm
not aware of anybody working on this at the moment.

-Sandy



"
gchen <chenguancheng@gmail.com>,"Tue, 17 Jun 2014 19:47:35 -0700 (PDT)",Re: Big-Endian (IBM Power7) Spark Serialization issue,dev@spark.incubator.apache.org,"Cool, so maybe when we swith to Snappy instead of LZF, we can workaround the
bug until the LZF upstream fix it, right?

In addition, is it valuable to add support for other compression codecs such
as LZ4? We observed 5% end-to-end improvement using LZ4 vs Snappy in
Terasort (Hadoop MR).



--

"
DB Tsai <dbtsai@stanford.edu>,"Tue, 17 Jun 2014 20:07:22 -0700",Re: Int tolerance in LBFGS.setConvergenceTol causes problems,dev@spark.apache.org,"Hi Gang,

This is a bug, and I'm the one who did it :) Just add the comment to your PR.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


:
parameter estimation. Following the patterns in mllib.regression, I created an object PoissonRegressionModelWithLBFGS, which creates a new instance of class PoissonRegressionModelWithLBFGS and invokes the run method to get the weights/parameters.
e using class LBFGS. The parameter of setConvergenceTol is of type Int rather than Double. This leads to the inability to specify a tolerance less than 1.0 and a type mismatch error when passing a Double value. In fact, the class LBFGS internally uses convergenceTol as a Double var. So we can safely change the parameter of setConvergenceTol from type Int to type Double.
https://github.com/apache/spark/pull/1104/files

"
Reynold Xin <rxin@databricks.com>,"Tue, 17 Jun 2014 21:07:12 -0700",Re: Big-Endian (IBM Power7) Spark Serialization issue,"""dev@spark.apache.org"" <dev@spark.apache.org>","It is actually pluggable. You can implement new compression codecs and just
change the config variable to use those.


"
Andrew Ash <andrew@andrewash.com>,"Wed, 18 Jun 2014 00:47:00 -0400",Re: Contributing to MLlib on GLM,dev@spark.apache.org,"Hi Xiaokai,

Also take a look through Xiangrui's slides from HadoopSummit a few weeks
back: http://www.slideshare.net/xrmeng/m-llib-hadoopsummit  The roadmap
starting at slide 51 will probably be interesting to you.

Andrew



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 17 Jun 2014 22:42:32 -0700",Re: Java IO Stream Corrupted - Invalid Type AC?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Out of curiosity - are you guys using speculation, shuffle
consolidation, or any other non-default option? If so that would help
narrow down what's causing this corruption.


"
Reynold Xin <rxin@databricks.com>,"Wed, 18 Jun 2014 01:19:11 -0700",Re: Contribute to Spark - Need a mentor.,dev@spark.apache.org,"Hi Michael,

Unfortunately the Apache mailing list filters out attachments. That said,
you can usually just start by looking at the JIRA for Spark and find issues
tagged with the starter tag and work on them. You can submit pull requests
to the github repo or email the dev list for feedbacks on specific issues.
https://github.com/apache/spark



"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Wed, 18 Jun 2014 08:49:56 -0400",Re: Java IO Stream Corrupted - Invalid Type AC?,user@spark.apache.org,"Patrick,

My team is using shuffle consolidation but not speculation. We are also
using persist(DISK_ONLY) for caching.

Here are some config changes that are in our work-in-progress.

We've been trying for 2 weeks to get our production flow (maybe around
50-70 stages, a few forks and joins with up to 20 branches in the forks) to
run end to end without any success, running into other problems besides
this one as well. For example, we have run into situations where saving to
HDFS just hangs on a couple of tasks, which are printing out nothing in
their logs and not taking any CPU. For testing, our input data is 10 GB
across 320 input splits and generates maybe around 200-300 GB of
intermediate and final data.


        conf.set(""spark.executor.memory"", ""14g"")     // TODO make this
configurable

        // shuffle configs
        conf.set(""spark.default.parallelism"", ""320"") // TODO make this
configurable
        conf.set(""spark.shuffle.consolidateFiles"",""true"")

        conf.set(""spark.shuffle.file.buffer.kb"", ""200"")
        conf.set(""spark.reducer.maxMbInFlight"", ""96"")

        conf.set(""spark.rdd.compress"",""true""

        // we ran into a problem with the default timeout of 60 seconds
        // this is also being set in the master's spark-env.sh. Not sure if
it needs to be in both places
        conf.set(""spark.worker.timeout"",""180"")

        // akka settings
        conf.set(""spark.akka.threads"", ""300"")
        conf.set(""spark.akka.timeout"", ""180"")
        conf.set(""spark.akka.frameSize"", ""100"")
        conf.set(""spark.akka.batchSize"", ""30"")
        conf.set(""spark.akka.askTimeout"", ""30"")

        // block manager
        conf.set(""spark.storage.blockManagerTimeoutIntervalMs"", ""180000"")
        conf.set(""spark.blockManagerHeartBeatMs"", ""80000"")

-Suren







-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 19 Jun 2014 00:45:09 +0530",Re: Java IO Stream Corrupted - Invalid Type AC?,dev@spark.apache.org,"

Use of shuffle consolidation is probably what is causing the issue.
Would be good idea to try again with that turned off (which is the default).

It should get fixed most likely in 1.1 timeframe.


Regards,
Mridul



"
Will Benton <willb@redhat.com>,"Wed, 18 Jun 2014 18:22:54 -0400 (EDT)",question about Hive compatiblilty tests,dev@spark.apache.org,"Hi all,

Does a ""Failed to generate golden answer for query"" message from HiveComparisonTests indicate that it isn't possible to run the query in question under Hive from Spark's test suite rather than anything about Spark's implementation of HiveQL?  The stack trace I'm getting implicates Hive code and not Spark code, but I wanted to make sure I wasn't missing something.


thanks,
wb

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 18 Jun 2014 17:35:30 -0700",Re: Java IO Stream Corrupted - Invalid Type AC?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just wondering, do you get this particular exception if you are not
consolidating shuffle data?


"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Wed, 18 Jun 2014 20:39:30 -0400",Re: Java IO Stream Corrupted - Invalid Type AC?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Good question. At this point, I'd have to re-run it to know for sure. We've
been trying various different things, so I'd have to reset the flow config
back to that state.

I can say that by removing persist(DISK_ONLY), the flows are running more
stably, probably due to removing disk contention. We won't be able to run
our full production flows without some type of disk persistence but for
testing, this is how we are continuing to try for now.

I can try tomorrow if you'd like.

-Suren







-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Doris Xin <doris.s.xin@gmail.com>,"Wed, 18 Jun 2014 18:03:29 -0700",Re: Run ScalaTest inside Intellij IDEA,dev@spark.apache.org,"Here's the JIRA on this known issue:
https://issues.apache.org/jira/browse/SPARK-1835

tl;dr: manually delete mesos-0.18.1.jar from lib_managed/jars after
running sbt/sbt
gen-idea. You should be able to run units inside Intellij after doing so.

Doris



spark/sql/test/TestSQLContext.scala
/java
avafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Conteâ€¦
jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-library-2.10.4.jar
c-plugin_2.10.4-0.5.jar
c-plugin_2.10.4-0.5.jar
,
e
:
:2988)
2991)
71)
:57)
mpl.java:43)
(IdeaIncrementalCompiler.scala:28)
.scala:25)
:57)
mpl.java:43)
)
to
ijieshen@gmail.com>
€™t
k/executor/MesosExecutorBackend.scala
k/scheduler/cluster/mesos/MesosSchedulerBackend.scala
mblyâ€™,
"
Michael Armbrust <michael@databricks.com>,"Thu, 19 Jun 2014 03:19:15 +0200",Re: question about Hive compatiblilty tests,dev@spark.apache.org,"I assume you are adding tests?  because that is the only time you should
see that message.

That error could mean a couple of things:
 1) The query is invalid and hive threw an exception
 2) Your Hive setup is bad.

Regarding #2, you need to have the source for Hive 0.12.0 available and
built as well as a hadoop installation.  You also have to have the
environment vars set as specified here:
https://github.com/apache/spark/tree/master/sql

Michael



"
Will Benton <willb@redhat.com>,"Wed, 18 Jun 2014 22:48:24 -0400 (EDT)",Re: question about Hive compatiblilty tests,dev@spark.apache.org,"
Yes, I had added the HAVING test to the whitelist.


Thanks!  The other Hive compatibility tests seem to work, so I'll dig in a bit more to see if I can figure out what's happening here.


best,
wb


"
Mark Baker <distobj@acm.org>,"Thu, 19 Jun 2014 10:47:45 -0400",Problems with Pyspark + Dill tests,dev@spark.apache.org,"Hi. As part of my attempt to port Pyspark to Python 3, I've
re-applied, with modifications, Josh's old commit for using Dill with
Pyspark (as Dill already supports Python 3). Alas, I ran into an odd
problem that I could use some help with.

Josh's old commit;

https://github.com/JoshRosen/incubator-spark/commit/2ac8986f3009f0dc133b11d16887fc8ddb33c3d1

My Dill branch;

https://github.com/distobj/spark/tree/dill

(Note; I've been running this in a virtualenv into which I
pip-installed dill. I haven't yet figured out the new way to package
it in python/lib as was done for py4j)

So the problem is that run_tests is failing with this pickle.py error
on most of the tests (those using .cache() it seems, unsurprisingly);

    PicklingError: Can't pickle <type '_sre.SRE_Pattern'>: it's not
found as _sre.SRE_Pattern

What's odd is that the same doctests work fine when run from the shell.

TIA for any ideas...

"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Thu, 19 Jun 2014 14:19:11 -0400",Re: Trailing Tasks Saving to HDFS,Surendranauth Hiraman <suren.hiraman@velos.io>,"I've created an issue for this but if anyone has any advice, please let me
know.

Basically, on about 10 GBs of data, saveAsTextFile() to HDFS hangs on two
remaining tasks (out of 320). Those tasks seem to be waiting on data from
another task on another node. Eventually (about 2 hours later) they time
out with a connection reset by peer.

All the data actually seems to be on HDFS as the expected part files. It
just seems like the remaining tasks have corrupted ""metadata"", so that they
do not realize that they are done. Just a guess though.

https://issues.apache.org/jira/browse/SPARK-2202

-Suren







-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 19 Jun 2014 14:47:50 -0400",assign SPARK-2126 to me?,dev@spark.apache.org,"Hi, all

Any admin can assign this issue https://issues.apache.org/jira/browse/SPARK-2126 to me?

I have started working on this

Thanks,

-- 
Nan Zhu

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 19 Jun 2014 12:08:41 -0700",Re: Trailing Tasks Saving to HDFS,user@spark.apache.org,"I'll make a comment on the JIRA - thanks for reporting this, let's get
to the bottom of it.


"
Josh Rosen <rosenville@gmail.com>,"Thu, 19 Jun 2014 12:56:51 -0700",Re: Problems with Pyspark + Dill tests,dev@spark.apache.org,"Thanks for helping with the Dill integration; I had some early first attempts, but had to set them aside when I got busy with some other work.

Just to bring everyone up to speed regarding context:
There are some objects that PySparkâ€™s `cloudpickle` library doesnâ€™t serialize properly, such as operator.getattr (https://issues.apache.org/jira/browse/SPARK-791) or NamedTuples (https://issues.apache.org/jira/browse/SPARK-1687).
My early attempt at replacing CloudPickle with Dill ran into problems because of slight differences in how Dill pickles functions defined in doctests versus functions defined elsewhere. Â I opened a bug report for this with the Dill developers (https://github.com/uqfoundation/dill/issues/18), who subsequently fixed the bug (https://github.com/uqfoundation/dill/pull/29).
It looks like thereâ€™s already a couple of Dill issues with examples of the â€œCanâ€™t pickle _ itâ€™s not found as _â€ bug (https://github.com/uqfoundation/dill/search?q=%22not+found+as%22&type=Issues). Â If you can find a small test case that reproduces this bug, Iâ€™d consider opening a new Dill issue.

- Josh

Hi. As part of my attempt to port Pyspark to Python 3, I've  
re-applied, with modifications, Josh's old commit for using Dill with  
Pyspark (as Dill already supports Python 3). Alas, I ran into an odd  
problem that I could use some help with.  

Josh's old commit;  

https://github.com/JoshRosen/incubator-spark/commit/2ac8986f3009f0dc133b11d16887fc8ddb33c3d1  

My Dill branch;  

https://github.com/distobj/spark/tree/dill  

(Note; I've been running this in a virtualenv into which I  
pip-installed dill. I haven't yet figured out the new way to package  
it in python/lib as was done for py4j)  

So the problem is that run_tests is failing with this pickle.py error  
on most of the tests (those using .cache() it seems, unsurprisingly);  

PicklingError: Can't pickle <type '_sre.SRE_Pattern'>: it's not  
found as _sre.SRE_Pattern  

What's odd is that the same doctests work fine when run from the shell.  

TIA for any ideas...  
"
=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>,"Thu, 19 Jun 2014 23:13:06 +0100",Rationale behind scala enumerations instead of sealed traits and case objects,dev@spark.apache.org,"While I was trying to execute a job using spark-submit, I discover a
scala.MatchError at runtime... a DriverStateChanged.FAILED message was send
to an actor, and the match statement used was not taking that value into
account.

When I inspected that DriverStateChange.scala file I discovered that it's a
scala enumeration; if a sealed trait and case objects would have used, the
compiler would have refused to compile spark until that match statement
would have covered all possible values.

As a scala developer I prefer to catch that kind of errors at compile time
and I would like to understand why a scala enumeration has been used
instead of a sealed trait + case object. If changing that to a sealed (and
possible others) keeps binary compatibility, would that kind of PR be
welcomed?

Kind regards,

Luis
"
Will Benton <willb@redhat.com>,"Fri, 20 Jun 2014 15:04:16 -0400 (EDT)","Re: Scala examples for Spark do not work as written in
 documentation",dev@spark.apache.org,"Hey, sorry to reanimate this thread, but just a quick question:  why do the examples (on http://spark.apache.org/examples.html) use ""spark"" for the SparkContext reference?  This is minor, but it seems like it could be a little confusing for people who want to run them in the shell and need to change ""spark"" to ""sc"".  (I noticed because this was a speedbump for a colleague who is trying out Spark.)


thanks,
wb

	by minotaur.apache.org (Postfix) with SMTP id 037E2116D6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Jun 2014 22:02:17 +0000 (UTC)
Received: (qmail 55961 invoked by uid 500); 20 Jun 2014 22:02:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55902 invoked by uid 500); 20 Jun 2014 22:02:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55890 invoked by uid 99); 20 Jun 2014 22:02:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Jun 2014 22:02:16 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.42 as permitted sender)
Received: from [209.85.219.42] (HELO mail-oa0-f42.google.com) (209.85.219.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Jun 2014 22:02:13 +0000
Received: by mail-oa0-f42.google.com with SMTP id eb12so8077538oac.1
        for <dev@spark.apache.org>; Fri, 20 Jun 2014 15:01:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=xTpgOtGPIUkWiaUoos8vUzykgNgqxydvvAyYvQY9Mk0=;
        b=V1w5LsQ38JGAZ9cpO5Wv2KfaFXKHy10s8FtUMGkhrFJNH524X/uPjAI6DeF9ak6tpm
         rQKIxfcgJ1O+p+W9rblmrkdtm8zdQ8yMz1+jw+naPXMqtI/CrEmQolfZsykYTKU90c7P
         VOjC41o8tLty81uJyMppEfuMxlCrrw9aXtJVRXjr6IahpZH2oMht8de35NekdHyDz1GI
         uo4Z02iwqDB4R+krYg7SLBlO0phhAklQyZi9Dzzh9wcqv/j+l2IxNmS4hUVfXZ7QMO6/
         dygeMq7cNT1OaFe6FqJ9qQVVUqhIRkFxI9OvCoSnclohbDy2pvjz+qKwsJXXi3He1HE1
         Yxgg==
MIME-Version: 1.0
X-Received: by 10.182.251.170 with SMTP id zl10mr6273196obc.5.1403301709233;
 Fri, 20 Jun 2014 15:01:49 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 20 Jun 2014 15:01:49 -0700 (PDT)
In-Reply-To: <641768824.37830898.1403291056624.JavaMail.zimbra@redhat.com>
References: <1400258494709-6593.post@n3.nabble.com>
	<CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
	<CAAsvFP=8e079xR0JrHEQ2tM56t-Ag2Lt8JqthUum0veMcEOd7g@mail.gmail.com>
	<CALEZFQwLNKsTZa9GYpcrBVSjbH45gjp+Pn0NZVANONP4Sor+aQ@mail.gmail.com>
	<641768824.37830898.1403291056624.JavaMail.zimbra@redhat.com>
Date: Fri, 20 Jun 2014 15:01:49 -0700
Message-ID: <CABPQxsu8R=_kZ7_cF=ZvY_mg0oOWsBu=MTqefUgfDTturPEPvQ@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in documentation
From: Patrick Wendell <pwendell@gmail.com>
To: ""dev@spark.apache.org"" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Those are pretty old - but I think the reason Matei did that was to
make it less confusing for brand new users. `spark` is actually a
valid identifier because it's just a variable name (val spark =3D new
SparkContext()) but I agree this could be confusing for users who want
to drop into the shell.

he examples (on http://spark.apache.org/examples.html) use ""spark"" for the =
SparkContext reference?  This is minor, but it seems like it could be a lit=
tle confusing for people who want to run them in the shell and need to chan=
ge ""spark"" to ""sc"".  (I noticed because this was a speedbump for a colleagu=
e who is trying out Spark.)
ation
(1)
m
.com
es-for-Spark-do-not-work-as-written-in-documentation-tp6593.html

"
Liquan Pei <liquanpei@gmail.com>,"Fri, 20 Jun 2014 17:47:58 -0700",Current status of Sparrow,dev@spark.apache.org,"Hi

What is the current status of Sparrow integration with Spark? I would like
to integrate Sparrow with Spark 1.0 on a 100 node cluster. Any suggestions?

Thanks a lot for your help!
Liquan
"
"""=?utf-8?B?Z3V4aWFvYm8xOTgy?="" <guxiaobo1982@qq.com>","Sat, 21 Jun 2014 22:52:22 +0800",What about a general schema registration method for JavaSchemaRDD?,"""=?utf-8?B?ZGV2?="" <dev@spark.incubator.apache.org>","Hi ,
 
The current implementation of JavaSchemaRDD need a special JavaBean class to define schema information for tables, but when developing applications using the Spark SQL API, table is a more dynamic component, the awkward thing is when new tables are defined, we must create a new JavaBean, and redeploy the whole application. So here comes an idea regarding a more general schema registration method,
 
 
 
Step1: Defile a new Java class named RowSchema in API to define column information, column name and data type are most important ones.
 
 
 
Step2: the actual data is store just as JavaRDD<Row>;
 
 
 
Step3:When loading data into JavaRDD<Row>, the API provides a general map function, which takes a RowSchema object as parameter, to map each line to a Row object.
 
 
 
Step4: Add a new applySchema method , which takes a RowSchema object as parameter, to the JavaSQLContext class ,
 
 
 
Step 5: The registerAsTable and all other SQL releated methods of JavaSQLContext class should take care of the difference of defining schema throw JavaBean and RowSchemas.(Thatâ€™s the work of the API layer)
 
 
 
The API is something like this:
 
 
 
Public Class RowSchema{
 
Public RowSchema(List<String> colNames, List<String> colDataTypes);
 
Public String getColName(integer i);//return column string of column I;
 
Public integer getColDataType(integer i);//return data type of column I;
 
Public integer getColNumber();// return number of columns 
 
 
 
};
 
RowSchema rs = new RowSchema(â€¦â€¦);
 
 
 
JavaRDD<Row> table = ctx.textFile(â€œfile pathâ€).map(rs);
 
 
 
JavaSchemaRDD schemaPeople = sqlCtx.applySchema(table, rs);
 
schemaPeople.registerAsTable(""people"");
 
Regards,
 

 Xiaobo Gu"
Reynold Xin <rxin@databricks.com>,"Sat, 21 Jun 2014 14:34:00 -0700",Re: What about a general schema registration method for JavaSchemaRDD?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for the message.


There is an open issue about the public type / schema system that is
related to this topic: https://issues.apache.org/jira/browse/SPARK-2179

You probably want to comment on that ticket as well.




o
a
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 22 Jun 2014 00:24:50 -0700","Assorted project updates (tests, build, etc)","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

1. The original test infrastructure hosted by the AMPLab has been
fully restored and also expanded with many more executor slots for
tests. Thanks to Matt Massie at the Amplab for helping with this.

2. We now have a nightly build matrix across different Hadoop
versions. It appears that the Maven build is failing tests with some
of the newer Hadoop versions. If people from the community are
interested, diagnosing and fixing test issues would be welcome patches
(they are all dependency related).

https://issues.apache.org/jira/browse/SPARK-2232

3. Prashant Sharma has spent a lot of time to make it possible for our
sbt build to read dependencies from Maven. This will save us a huge
amount of headache keeping the builds consistent. I just wanted to
give a heads up to users about this - we should retain compatibility
with features of the sbt build, but if you are e.g. hooking into deep
internals of our build it may affect you. I'm hoping this can be
updated and merged in the next week:

https://github.com/apache/spark/pull/77

4. We've moved most of the documentation over to recommending users
build with Maven when creating official packages. This is just to
provide a single ""reference build"" of Spark since it's the one we test
and package for releases, we make sure all recursive dependencies are
correct, etc. I'd recommend that all downstream packagers use this
build.

For day-to-day development I imagine sbt will remain more popular
(repl, incremental builds, etc). Prashant's work allows us to get the
""best of both worlds"" which is great.

- Patrick

"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 22 Jun 2014 10:27:03 -0700","Re: Assorted project updates (tests, build, etc)",dev@spark.apache.org,"Just a couple of FYI notes: With Zinc and the scala-maven-plugin, repl and
incremental builds are also available to those doing day-to-day development
using Maven.  As long as you don't have to delve into the extra boilerplate
and verbosity of Maven's POMs relative to an SBT build file, there is
little day-to-day functional difference between the two -- if anything, I
find that Maven supports faster development cycles.



"
dash <bshi@nd.edu>,"Sun, 22 Jun 2014 13:51:18 -0700 (PDT)",GraphX's VertexRDD can not be materialized by calling count(),dev@spark.incubator.apache.org,"Hi there,

Seems one can not materialize VertexRDD by simply calling count method,
which is overridden by VertexRDD. But if you call RDD's count, it could
materialize it.

Is this a feature that designed to get the count without materialize
VertexRDD? 

If so, do you guys think it is necessary to add a materialize method to
VertexRDD? I did that and ready to send a pull request, but I just want to
make sure this is reasonable. 

By the way, does count() is the cheapest way to materialize a RDD? Or it
just cost the same resources like other actions?

Best, 



--

"
dash <bshi@nd.edu>,"Sun, 22 Jun 2014 15:14:46 -0700 (PDT)",Checkpointed RDD still causing StackOverflow,dev@spark.incubator.apache.org,"Hi,

I'm doing iterative computing now, and due to lineage chain, we need to
checkpoint the RDD in order to cut off lineage and prevent StackOverflow
error. 

The following code still having StackOverflowError, I checked
`isCheckpointed` and the result is true. Also, I write a function to count
the lineage, but the lineage is not big. Any idea about that? Please give me
some hit so I can dig into the source code and try to fix it.




Best,



--

"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 22 Jun 2014 17:20:59 -0700",Re: Checkpointed RDD still causing StackOverflow,dev@spark.apache.org,"After checkpoint(), please call count(). This is similar to cache(),
the RDD is only marked as to be checked with checkpoint(). -Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 22 Jun 2014 17:20:59 -0700",Re: Checkpointed RDD still causing StackOverflow,dev@spark.apache.org,"After checkpoint(), please call count(). This is similar to cache(),
the RDD is only marked as to be checked with checkpoint(). -Xiangrui


"
dash <bshi@nd.edu>,"Sun, 22 Jun 2014 18:37:12 -0700 (PDT)",Re: Checkpointed RDD still causing StackOverflow,dev@spark.incubator.apache.org,"Hi Xiangrui,

According to my knowledge, calling count is for materialize the RDD, does
collect do the same thing since it also an action? I can not call count
because for a Graph object, count does not materialize the RDD. I already
send an issue on that.

My question is, why there still have stack overflow even if `isCheckpointed`
is true?



--

"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Mon, 23 Jun 2014 10:27:38 -0700",Re: Current status of Sparrow,dev@spark.apache.org,"Hi Liquan,

Sparrow is not currently integrated into the Spark distribution, so if
you'd like to use Spark with Sparrow, you need to use a forked version of
Spark (https://github.com/kayousterhout/spark/tree/sparrow).  This version
of Spark was forked off an older version of Spark so some work will be
involved to bring this up to date with the latest version of Spark; I can
help with this.

Unfortunately there are also a few practical problems with using Sparrow
with Spark that may or may not be compatible with your target workload.
 Sparrow distributes scheduling over many Sparrow schedulers that are each
associated with their own Spark driver (this is where Sparrow's
improvements stem from -- there's no longer a single driver serving as the
bottleneck for your application, but all of the schedulers/drivers share
the same slots for scheduling tasks).  As a result, data stored in Spark's
block manager on one Spark driver (and created as part of a job scheduled
by the associated Sparrow scheduler) cannot be accessed by other Spark
drivers.  If you're storing data in Tachyon or have a workload where
different jobs have disjoint working sets, this won't be an issue.

-Kay



"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 23 Jun 2014 14:20:08 -0700",Re: Checkpointed RDD still causing StackOverflow,dev@spark.apache.org,"Calling checkpoint() alone doesn't cut the lineage. It only marks the
RDD as to be checkpointed. The lineage is cut after the first time
this RDD is materialized. You see StackOverflow becaure the lineage is
still there. -Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 23 Jun 2014 14:20:08 -0700",Re: Checkpointed RDD still causing StackOverflow,dev@spark.apache.org,"Calling checkpoint() alone doesn't cut the lineage. It only marks the
RDD as to be checkpointed. The lineage is cut after the first time
this RDD is materialized. You see StackOverflow becaure the lineage is
still there. -Xiangrui


"
Mark Baker <distobj@acm.org>,"Mon, 23 Jun 2014 17:27:22 -0400",Re: Problems with Pyspark + Dill tests,dev@spark.apache.org,"mpts, but had to set them aside when I got busy with some other work.
â€™t serialize properly, such as operator.getattr (https://issues.apache.org/jira/browse/SPARK-791) or NamedTuples (https://issues.apache.org/jira/browse/SPARK-1687).
ause of slight differences in how Dill pickles functions defined in doctests versus functions defined elsewhere.  I opened a bug report for this with the Dill developers (https://github.com/uqfoundation/dill/issues/18), who subsequently fixed the bug (https://github.com/uqfoundation/dill/pull/29).
es of the â€œCanâ€™t pickle _ itâ€™s not found as _â€ bug (https://github.com/uqfoundation/dill/search?q=%22not+found+as%22&type=Issues).  If you can find a small test case that reproduces this bug, Iâ€™d consider opening a new Dill issue.

Thanks for the context, Josh.

I've gone ahead and created a new test case and just opened a new issue;

https://github.com/uqfoundation/dill/issues/49

"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 23 Jun 2014 15:57:40 -0700",RFC: [SPARK-529] Create constants for known config variables.,dev@spark.apache.org,"I started with some code to implement an idea I had for SPARK-529, and
before going much further (since it's a large and kinda boring change)
I'd like to get some feedback from people.

Current code it at:
https://github.com/vanzin/spark/tree/SPARK-529

There are still some parts I haven't fully fleshed out yet (see TODO
list in the commit message), but that's the basic idea. Let me know if
you have any feedback or different ideas.

Thanks!


-- 
Marcelo

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 23 Jun 2014 19:58:59 -0700",Re: RFC: [SPARK-529] Create constants for known config variables.,dev@spark.apache.org,"Hey Marcelo,

When we did the configuration pull request, we actually avoided having a big list of defaults in one class file, because this creates a file that all the components in the project depend on. For example, since we have some settings specific to streaming and the REPL, do we want those settings to appear in a file that’s in “core”? It might be better to just make sure we use constants for defaults in the code, or maybe have a separate class per project.

The other problem with this kind of change is that it’s disruptive to all the other ongoing patches, so I wouldn’t consider it high-priority right now. We haven’t had a ton of problems with settings being mistyped.

If you do want to do something like this though, apart from the comment above about modules, please make sure this is not a public API. As soon as we add it to the API, it means we can’t change or remove those config settings. I’d also suggest giving each config setting a single name instead of having “ui”, “shuffle”, etc objects, since the chained calls to conf.ui.port.value look somewhat confusing.

Matei




"
Reynold Xin <rxin@databricks.com>,"Mon, 23 Jun 2014 21:51:43 -0700","Re: [jira] [Created] (SPARK-1867) Spark Documentation Error causes
 java.lang.IllegalStateException: unread block data","""dev@spark.apache.org"" <dev@spark.apache.org>","Mridul,

Can you comment a little bit more on this issue? We are running into the
same stack trace but not sure whether it is just different Spark versions
on each cluster (doesn't seem likely) or a bug in Spark.

Thanks.




"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 24 Jun 2014 10:57:50 +0530","Re: [jira] [Created] (SPARK-1867) Spark Documentation Error causes
 java.lang.IllegalStateException: unread block data",dev@spark.apache.org,"There are a few interacting issues here - and unfortunately I dont
recall all of it (since this was fixed a few months back).

a) With shuffle consolidation, data sent to remote node incorrectly
includes data from partially constructed blocks - not just the request
blocks.
Actually, with shuffle consolidation (with and without failures) quite
a few things broke.

b) There might have been a few other bugs in DiskBlockObjectWriter too.

c) We also suspected buffers overlapping when using cached kryo
serializer (though never proved this, just disabled caching across
board for now : and always create new instance).

The way we debug'ed it is by introducing an Input/Output stream which
introduced checksum into the data stream and validating that at each
side for compression, serialization, etc.

Apologies for being non specific ... I really dont have the details
right now, and our internal branch is in flux due to merge effort to
port our local changes to master.
Hopefully we will be able to submit PR's as soon as this is done and
testcases are added to validate.


Regards,
Mridul





"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 24 Jun 2014 10:17:19 -0700",Re: RFC: [SPARK-529] Create constants for known config variables.,dev@spark.apache.org,"Hi Matei, thanks for the comments.

ote:
big list of defaults in one class file, because this creates a file that all the components in the project depend on. For example, since we have some settings specific to streaming and the REPL, do we want those settings to appear in a file thatâ€™s in â€œcoreâ€? It might be better to just make sure we use constants for defaults in the code, or maybe have a separate class per project.

I'm actually following the style used in Hadoop here, but expanding it
a little bit to add type-safety too. (e.g., see DFSConfigKeys.java in
HDFS.) I sort of see your point of ""everybody needs to depend on this
file"", but then all the affected code already depends on SparkConf.

e to all the other ongoing patches, so I wouldnâ€™t consider it high-priority right now. We havenâ€™t had a ton of problems with settings being mistyped.

Yes, large changes like this are always disruptive. But since it
mostly touches setup code, not the innards of algorithms and the like,
it shouldn't be hard to merge things.

bove about modules, please make sure this is not a public API. As soon as we add it to the API, it means we canâ€™t change or remove those config settings. Iâ€™d also suggest giving each config setting a single name instead of having â€œuiâ€, â€œshuffleâ€, etc objects, since the chained calls to conf.ui.port.value look somewhat confusing.

Making it not public is easy; but then how do we want to expose this
to users? It's normal right now, as you say, to hardcode the settings
in the code (or config files - which this change would not help with).
In the process of writing the p.o.c., I found cases of the same
setting with different casing in different source files, and the same
setting with different defaults. So maybe it has not affected any
users yet, but it's easy to miss things like that with the current
approach.

I don't have a good suggestion for separating the module-specific
configs; I kinda liked the ""everything under SparkConf"" approach and
tried to group settings in a way that made sense. Creating objects in
each module to hold the configs for those is possible, but would
probably look a bit uglier.

But in any case, this is not urgent, more of a nice thing to have in my opinion.

-- 
Marcelo

"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Wed, 25 Jun 2014 03:48:36 +0530",Re: Checkpointed RDD still causing StackOverflow,dev <dev@spark.apache.org>,"Do not call collect as that will perform materialization as well as
transfer of data to driver (might actually cause driver to fail if the data
is huge). You have to materialize the RDD in some way(call save, count,
collect).

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Wed, 25 Jun 2014 03:48:36 +0530",Re: Checkpointed RDD still causing StackOverflow,dev <dev@spark.apache.org>,"Do not call collect as that will perform materialization as well as
transfer of data to driver (might actually cause driver to fail if the data
is huge). You have to materialize the RDD in some way(call save, count,
collect).

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Wed, 25 Jun 2014 04:11:11 +0530",Re: balancing RDDs,"user@spark.apache.org, dev <dev@spark.apache.org>","This would be really useful. Especially for Shark where shift of
partitioning effects all subsequent queries unless task scheduling time
beats spark.locality.wait. Can cause overall low performance for all
subsequent tasks.

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




ach node
r
"
dash <bshi@nd.edu>,"Tue, 24 Jun 2014 21:57:35 -0700 (PDT)",Re: Checkpointed RDD still causing StackOverflow,dev@spark.incubator.apache.org,"Due to SPARK-2245, you can not use count to materialize VertexRDD. That
actually materialize PartitionRDD, so checkpoint for VertexRDD won't work.
I'll trying to fix that right now.



--

"
Andy Konwinski <andykonwinski@gmail.com>,"Tue, 24 Jun 2014 22:06:05 -0700",Fwd: 2014 Mesos community survey results,dev@spark.apache.org,"I think it's cool that the Mesos team did a survey of usage and published
the aggregate results. It would be cool to do a survey for the Spark
project and publish the results on the Spark website like the Mesos team
did.
---------- Forwarded message ----------
From: ""Dave Lester"" <davelester@gmail.com>
Date: Jun 24, 2014 2:07 PM
Subject: 2014 Mesos community survey results
To: ""user@mesos.apache.org"" <user@mesos.apache.org>
Cc:

This afternoon I blogged the results of a community survey I led in May to
get the pulse of the Apache Mesos community and improve our understanding
of how others are using the software. We received a total of 55 survey
responses. Thanks all who took the time to provide this critical feedback
for the community!

http://mesos.apache.org/blog/mesos-community-survey-2014-results/

The blog post incl highlights, and a summary of the responses. As I mention
in the post, this is the first time weâ€™ve run a community survey like this;
feedback is welcome on how we may improve it when we run it again! Feel
free to respond to this thread.

Dave
"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Wed, 25 Jun 2014 16:48:58 +0000",Re: balancing RDDs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yep exactly!  I’m not sure how complicated it would be to pull off.  If someone wouldn’t mind helping to get me pointed in the right direction I would be happy to look into and contribute this functionality.  I imagine this would be implemented in the scheduler codebase and there would be some sort of rebalance configuration property to enable it possibly?

Does anyone else have any thoughts on this?

Cheers,

Sean



om>
ode
er
o


"
Mark Baker <distobj@acm.org>,"Wed, 25 Jun 2014 15:55:37 -0400",Re: Problems with Pyspark + Dill tests,dev@spark.apache.org,"Hey,


So that one's dealt with; it was a sys.prefix issue with me using a
virtualenv and was fixed in a soon-to-be pull request a couple weeks
ago.

With that patch applied though, I'm now running into other doctest
issues, these involving serializing Py4J objects, and again, only
occurring inside doctests, not from the shell. I've been unable to
distill this one down to a compact test case, nor gain any insight to
the cause, and could really use a nudge in the right direction.
Thanks!

Top and bottom of sample trace (excluded middle is the usual recursive
pickling calls);

File ""pyspark/rdd.py"", line 1487, in __main__.PipelinedRDD
Failed example:
    rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib/python2.7/doctest.py"", line 1289, in __run
        compileflags, 1) in test.globs
      File ""<doctest __main__.PipelinedRDD[1]>"", line 1, in <module>
        rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
      File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rdd.py"",
line 201, in cache
        self._jrdd.cache()
      File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rdd.py"",
line 1531, in _jrdd
        pickled_command = DillSerializer().dumps(command)
      File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/serializers.py"",
line 284, in dumps
        def dumps(self, obj): return dill.dumps(obj, 2)
      File ""/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py"",
line 169, in dumps
        dump(obj, file, protocol, byref)
      File ""/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py"",
line 162, in dump
        pik.dump(obj)
      File ""/usr/lib/python2.7/pickle.py"", line 224, in dump
        self.save(obj)
...
      File ""/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py"",
line 543, in save_module_dict
        StockPickler.save_dict(pickler, obj)
      File ""/usr/lib/python2.7/pickle.py"", line 650, in save_dict
        self._batch_setitems(obj.iteritems())
      File ""/usr/lib/python2.7/pickle.py"", line 682, in _batch_setitems
        save(v)
      File ""/usr/lib/python2.7/pickle.py"", line 307, in save
        rv = reduce(self.proto)
      File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"",
line 537, in __call__
        self.target_id, self.name)
      File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"",
line 304, in get_return_value
        format(target_id, '.', name, value))
    Py4JError: An error occurred while calling o15.__getnewargs__. Trace:
    py4j.Py4JException: Method __getnewargs__([]) does not exist
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
        at py4j.Gateway.invoke(Gateway.java:251)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:207)
        at java.lang.Thread.run(Thread.java:745)

"
Sung Hwan Chung <codedeft@cs.stanford.edu>,"Wed, 25 Jun 2014 13:36:55 -0700",Re: Contributing to MLlib on GLM,dev@spark.apache.org,"Well, as you said, MLLib already supports GLM in a sense. Except they only
support two link functions - identity (linear regression) and logit
(logistic regression). It should not be too hard to add other link
functions, as all you have to do is add a different gradient function for
Poisson/Gamma, etc - look at Gradient.scala in mllib.



"
Josh Rosen <rosenville@gmail.com>,"Wed, 25 Jun 2014 23:20:53 -0700",Re: Problems with Pyspark + Dill tests,dev@spark.apache.org,"The problem seems to be that unpicklable RDD objects are being pulled into function closures. Â In your failing dockets, it looks like theÂ rdd created through sc.parallelize is being pulled into the map lambdaâ€™s function closure.

I opened a new Dill bug with a small test case that reproduces this issue:Â https://github.com/uqfoundation/dill/issues/50.

I tried manually modifying dill to drop `rdd` and `sc` variables from function globals dicts, which seemed to solve the problem, but thatâ€™s not a general-purpose fix.

Hey,  

;  

So that one's dealt with; it was a sys.prefix issue with me using a  
virtualenv and was fixed in a soon-to-be pull request a couple weeks  
ago.  

With that patch applied though, I'm now running into other doctest  
issues, these involving serializing Py4J objects, and again, only  
occurring inside doctests, not from the shell. I've been unable to  
distill this one down to a compact test case, nor gain any insight to  
the cause, and could really use a nudge in the right direction.  
Thanks!  

Top and bottom of sample trace (excluded middle is the usual recursive  
pickling calls);  

File ""pyspark/rdd.py"", line 1487, in __main__.PipelinedRDD  
Failed example:  
rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()  
Exception raised:  
Traceback (most recent call last):  
File ""/usr/lib/python2.7/doctest.py"", line 1289, in __run  
compileflags, 1) in test.globs  
File ""<doctest __main__.PipelinedRDD[1]>"", line 1, in <module>  
rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()  
File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rdd.py"",  
line 201, in cache  
self._jrdd.cache()  
File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rdd.py"",  
line 1531, in _jrdd  
pickled_command = DillSerializer().dumps(command)  
File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/serializers.py"",  
line 284, in dumps  
def dumps(self, obj): return dill.dumps(obj, 2)  
File ""/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py"",  
line 169, in dumps  
dump(obj, file, protocol, byref)  
File ""/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py"",  
line 162, in dump  
pik.dump(obj)  
File ""/usr/lib/python2.7/pickle.py"", line 224, in dump  
self.save(obj)  
...  
File ""/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py"",  
line 543, in save_module_dict  
StockPickler.save_dict(pickler, obj)  
File ""/usr/lib/python2.7/pickle.py"", line 650, in save_dict  
self._batch_setitems(obj.iteritems())  
File ""/usr/lib/python2.7/pickle.py"", line 682, in _batch_setitems  
save(v)  
File ""/usr/lib/python2.7/pickle.py"", line 307, in save  
rv = reduce(self.proto)  
File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py"",  
line 537, in __call__  
self.target_id, self.name)  
File ""/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py"",  
line 304, in get_return_value  
format(target_id, '.', name, value))  
Py4JError: An error occurred while calling o15.__getnewargs__. Trace:  
py4j.Py4JException: Method __getnewargs__([]) does not exist  
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)  
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)  
at py4j.Gateway.invoke(Gateway.java:251)  
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)  
at py4j.commands.CallCommand.execute(CallCommand.java:79)  
at py4j.GatewayConnection.run(GatewayConnection.java:207)  
at java.lang.Thread.run(Thread.java:745)  
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Fri, 27 Jun 2014 05:12:33 +0530",Google Cloud Engine adds out of the box Spark/Shark support,"user@spark.apache.org, dev <dev@spark.apache.org>","https://groups.google.com/forum/#!topic/gcp-hadoop-announce/EfQms8tK5cE

I suspect they are using thr own builds.. has anybody had a chance to look
at it?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Fri, 27 Jun 2014 05:46:13 +0530",Re: balancing RDDs,,"I would imagine this would be an extension of SchemaRDD (for Sparksql)  or
a new RDD altogether.
The RDD location is determined based on where task generating the RDD is
scheduled, the scheduler schedules basis of input RDD/sourcedata location.
So ideally RDD codebase needs to check location of input partition across
nodes & scheduling  preference of task related to unbalanced partition to
different nodes.. I am not sure if RDD can influence location of tasks
/partition location.


Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



m

f.  If
ction I
me
n each
d:
y
do
"
Bert Greevenbosch <Bert.Greevenbosch@huawei.com>,"Fri, 27 Jun 2014 00:46:29 +0000",Artificial Neural Network in Spark?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello all,

I was wondering whether Spark/mllib supports Artificial Neural Networks (ANNs)?

If not, I am currently working on an implementation of it. I re-use the code for linear regression and gradient descent as much as possible.

Would the community be interested in such implementation? Or maybe somebody is already working on it?

Best regards,
Bert
"
xwei <weixiaokai@gmail.com>,"Thu, 26 Jun 2014 18:04:00 -0700 (PDT)",Re: Contributing to MLlib on GLM,dev@spark.incubator.apache.org,"Yes, that's what we did: adding two gradient functions to Gradient.scala and
create PoissonRegression and GammaRegression using these gradients. We made
a PR on this.



--

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 26 Jun 2014 19:06:46 -0700",[VOTE] Release Apache Spark 1.0.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.0.1!

The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1020/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/

Please vote on releasing this package as Apache Spark 1.0.1!

The vote is open until Monday, June 30, at 03:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

=== About this release ===
This release fixes a few high-priority bugs in 1.0 and has a variety
of smaller fixes. The full list is here: http://s.apache.org/b45. Some
of the more visible patches are:

SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
SPARK-1790: Support r3 instance types on EC2.

This is the first maintenance release on the 1.0 line. We plan to make
additional maintenance releases as new fixes come in.

- Patrick

"
"""Ron Chung Hu (Ron Hu, ARC)"" <ron.hu@huawei.com>","Fri, 27 Jun 2014 02:43:23 +0000",IntelliJ IDEA cannot compile TreeNode.scala,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am a Spark newbie.  I just downloaded Spark1.0.0 and latest IntelliJ version 13.1 with Scala plug-in.  At spark-1.0.0 top level, I executed the following SBT commands and they ran successfully.


-          ./sbt/sbt assembly

-          ./sbt/sbt update gen-idea

After opening IntelliJ IDEA, I tried to compile ...../sql/catalyst/trees/TreeNode.scala inside IntelliJ.  I got many compile errors such as ""cannot resolve symbol children"", ""cannot resolve symbol id"".  Actually both symbols are defined in the same file.   As Spark was built successfully with ""sbt/sbt assembly"" command, I wondered what went wrong in compiling TreeNode.scala.  Any pointer will be appreciated.

Thanks.

Best,
Ron Hu

"
Reynold Xin <rxin@databricks.com>,"Thu, 26 Jun 2014 20:57:12 -0700",Re: IntelliJ IDEA cannot compile TreeNode.scala,"""dev@spark.apache.org"" <dev@spark.apache.org>","IntelliJ parser/analyzer/compiler behaves differently from Scala compiler,
and sometimes lead to inconsistent behavior. This is one of the case.

In general while we use IntelliJ, we don't use it to build stuff. I
personally always build in command line with sbt or Maven.




"
Bharath Ravi Kumar <reachbach@gmail.com>,"Fri, 27 Jun 2014 09:47:54 +0530",NPE calling reduceByKey on JavaPairRDD,dev@spark.apache.org,"Hi,

I've been encountering a NPE invoking reduceByKey on JavaPairRDD since
upgrading to 1.0.0 . The issue is straightforward to reproduce with 1.0.0
and doesn't occur with 0.9.0.  The stack trace is as follows:

14/06/26 21:05:35 WARN scheduler.TaskSetManager: Loss was due to
java.lang.NullPointerException
java.lang.NullPointerException
at
org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:750)
at
org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:750)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:59)
at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:96)
at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:95)
at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:582)
at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:582)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
at org.apache.spark.scheduler.Task.run(Task.scala:51)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:722)


I've raised a bug to track this issue :
https://issues.apache.org/jira/browse/SPARK-2292

Thanks,
Bharath
"
Reynold Xin <rxin@databricks.com>,"Thu, 26 Jun 2014 21:33:51 -0700",Re: NPE calling reduceByKey on JavaPairRDD,"""dev@spark.apache.org"" <dev@spark.apache.org>","Responded on the jira...





"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 27 Jun 2014 05:48:34 +0000",Re: IntelliJ IDEA cannot compile TreeNode.scala,"""<dev@spark.apache.org>"" <dev@spark.apache.org>","Hi Ron Hu,

The Idea project generated with ""update gen-idea"" didn't work properly for me as well. My workaround is to open corresponding Maven project in Idea (File->Open look for .bom file). To compile the opened project I use Maven window in Idea (View->show Maven ). However, tests fail to compile due to some mess with paths (it tries to create something like c:\project\blabla\c:\project\tests). I am also able to compile separate files by right-clicking on a file in Idea project tree and choosing ""compile"". This solves the test problem, because I am able to run test files with this approach.

Best regards, Alexander

27.06.2014, × 6:43, ""Ron Chung Hu (Ron Hu, ARC)"" <ron.hu@huawei.com> ÎÁÐÉÓÁÌ(Á):

rsion 13.1 with Scala plug-in.  At spark-1.0.0 top level, I executed the following SBT commands and they ran successfully.
TreeNode.scala inside IntelliJ.  I got many compile errors such as ""cannot resolve symbol children"", ""cannot resolve symbol id"".  Actually both symbols are defined in the same file.   As Spark was built successfully with ""sbt/sbt assembly"" command, I wondered what went wrong in compiling TreeNode.scala.  Any pointer will be appreciated.

"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 27 Jun 2014 05:51:43 +0000",Re: Artificial Neural Network in Spark?,"""<dev@spark.apache.org>"" <dev@spark.apache.org>","Hi Bert,

It would be extremely interesting. Do you plan to implement autoencoder as well? It would be great to have deep learning in Spark.

Best regards, Alexander

27.06.2014, × 4:47, ""Bert Greevenbosch"" <Bert.Greevenbosch@huawei.com> ÎÁÐÉÓÁÌ(Á):

ANNs)?
ode for linear regression and gradient descent as much as possible.
dy is already working on it?

"
Debasish Das <debasish.das83@gmail.com>,"Thu, 26 Jun 2014 23:02:28 -0700",Re: Artificial Neural Network in Spark?,dev@spark.apache.org,"Look into Powered by Spark page...I found a project there which used
autoencoder functions...It's not updated for a long time now !

m

s
m>
"
Krakna H <shankark+sys@gmail.com>,"Fri, 27 Jun 2014 03:40:56 -0700 (PDT)",Re: Spark Matrix Factorization,dev@spark.incubator.apache.org,"Hi all,

Just found this thread -- is there an update on including DSGD in Spark? We
have a project that entails topic modeling on a document-term matrix using
matrix factorization, and were wondering if we should use ALS or attempt
writing our own matrix factorization implementation on top of Spark.

Thanks.



--

"
Debasish Das <debasish.das83@gmail.com>,"Fri, 27 Jun 2014 09:46:55 -0700",Re: Spark Matrix Factorization,dev@spark.apache.org,"Hi,

In my experiments with Jellyfish I did not see any substantial RMSE loss
over DSGD for Netflix dataset...

So we decided to stick with ALS and implemented a family of Quadratic
Minimization solvers that stays in the ALS realm but can solve interesting
constraints(positivity, bounds, L1, equality constrained bounds etc)...We
are going to show it at the Spark Summit...Also ALS structure is favorable
to matrix factorization use-cases where missing entries means zero and you
want to compute a global gram matrix using broadcast and use that for each
Quadratic Minimization for all users/products...

Implementing DSGD in the data partitioning that Spark ALS uses will be
straightforward but I would be more keen to see a dataset where DSGD is
showing you better RMSEs than ALS....

If you have a dataset where DSGD produces much better result could you
please point it to us ?

Also you can use Jellyfish to run DSGD benchmarks to compare against
ALS...It is multithreaded and if you have good RAM, you should be able to
run fairly large datasets...

Be careful about the default Jellyfish...it has been tuned for netflix
dataset (regularization, rating normalization etc)...So before you compare
RMSE make sure ALS and Jellyfish is running same algorithm (L2 regularized
Quadratic Loss)....

Thanks.
Deb



"
Debasish Das <debasish.das83@gmail.com>,"Fri, 27 Jun 2014 09:46:55 -0700",Re: Spark Matrix Factorization,dev@spark.apache.org,"Hi,

In my experiments with Jellyfish I did not see any substantial RMSE loss
over DSGD for Netflix dataset...

So we decided to stick with ALS and implemented a family of Quadratic
Minimization solvers that stays in the ALS realm but can solve interesting
constraints(positivity, bounds, L1, equality constrained bounds etc)...We
are going to show it at the Spark Summit...Also ALS structure is favorable
to matrix factorization use-cases where missing entries means zero and you
want to compute a global gram matrix using broadcast and use that for each
Quadratic Minimization for all users/products...

Implementing DSGD in the data partitioning that Spark ALS uses will be
straightforward but I would be more keen to see a dataset where DSGD is
showing you better RMSEs than ALS....

If you have a dataset where DSGD produces much better result could you
please point it to us ?

Also you can use Jellyfish to run DSGD benchmarks to compare against
ALS...It is multithreaded and if you have good RAM, you should be able to
run fairly large datasets...

Be careful about the default Jellyfish...it has been tuned for netflix
dataset (regularization, rating normalization etc)...So before you compare
RMSE make sure ALS and Jellyfish is running same algorithm (L2 regularized
Quadratic Loss)....

Thanks.
Deb



"
"""Ron Chung Hu (Ron Hu, ARC)"" <ron.hu@huawei.com>","Fri, 27 Jun 2014 17:02:10 +0000",RE: IntelliJ IDEA cannot compile TreeNode.scala,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Reynold for advice.

Ron

-----Original MessagSent: Thursday, June 26, 2014 8:57 PM
To: dev@spark.apache.org
Subject: Re: IntelliJ IDEA cannot compile TreeNode.scala

IntelliJ parser/analyzer/compiler behaves differently from Scala compiler,
and sometimes lead to inconsistent behavior. This is one of the case.

In general while we use IntelliJ, we don't use it to build stuff. I
personally always build in command line with sbt or Maven.



On Thu, Jun 26, 2014 at 7:43 PM, Ron Chung Hu (Ron Hu, ARC) <
ron.hu@huawei.com> wrote:

> Hi,
>
> I am a Spark newbie.  I just downloaded Spark1.0.0 and latest IntelliJ
> version 13.1 with Scala plug-in.  At spark-1.0.0 top level, I executed the
> following SBT commands and they ran successfully.
>
>
> -          ./sbt/sbt assembly
>
> -          ./sbt/sbt update gen-idea
>
> After opening IntelliJ IDEA, I tried to compile
> ...../sql/catalyst/trees/TreeNode.scala inside IntelliJ.  I got many
> compile errors such as ""cannot resolve symbol children"", ""cannot resolve
> symbol id"".  Actually both symbols are defined in the same file.   As Spark
> was built successfully with ""sbt/sbt assembly"" command, I wondered what
> went wrong in compiling TreeNode.scala.  Any pointer will be appreciated.
>
> Thanks.
>
> Best,
> Ron Hu
>
>
"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 27 Jun 2014 11:18:17 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),dev@spark.apache.org,"+1

Tested it out on Mac OS X and Windows, looked through docs.

Matei


version 1.0.1!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
at:
https://repository.apache.org/content/repositories/orgapach"
Debasish Das <debasish.das83@gmail.com>,"Fri, 27 Jun 2014 14:54:29 -0700",Linear CG solver,dev@spark.apache.org,"Hi,

I am looking for an efficient linear CG to be put inside the Quadratic
Minimization algorithms we added for Spark mllib.

With a good linear CG, we should be able to solve kernel SVMs with this
solver in mllib...

I use direct solves right now using cholesky decomposition which has higher
complexity as matrix sizes become large...

I found out some jblas example code:

https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java

I was wondering if mllib developers have any experience using this solver
and if this is better than apache commons linear CG ?

Thanks.
Deb
"
David Hall <dlwh@cs.berkeley.edu>,"Fri, 27 Jun 2014 18:12:43 -0400",Re: Linear CG solver,dev@spark.apache.org,"I have no ideas on benchmarks, but breeze has a CG solver:
https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze/optimize/linear/ConjugateGradient.scala

https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala

It's based on the code from TRON, and so I think it's more targeted for
norm-constrained solutions of the CG problem.









"
Andrew Or <andrew@databricks.com>,"Fri, 27 Jun 2014 15:42:11 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","There is an issue with the SparkUI: the storage page continues to display
RDDs that are dropped from memory. This is fixed in
https://github.com/apache/spark/commit/21e0f77b6321590ed86223a60cdb8ae08ea4057f
but is not part of this RC.


2014-06-27 11:18 GMT-07:00 Matei Zaharia <matei.zaharia@gmail.com>:

"
Debasish Das <debasish.das83@gmail.com>,"Fri, 27 Jun 2014 15:42:38 -0700",Re: Linear CG solver,dev@spark.apache.org,"Thanks David...Let me try it...I am keen to see the results first and later
will look into runtime optimizations...

Deb






"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Fri, 27 Jun 2014 23:14:03 +0000",RE: IntelliJ IDEA cannot compile TreeNode.scala,"""dev@spark.apache.org"" <dev@spark.apache.org>","One question, then, is what to use to debug Spark if Intellij can only be used for code browsing for the sake of unresolved symbols as mentioned by Ron? 
More specifically, if one builds from command line, but would like to debug a running Spark from a IDE, Intellij, e.g., what could he do?

Another note is that the problem seems to start to appear on Spark 1.0.0 and not with Spark 0.8.0 at least. Any lights to shed on this difference between the versions?

Thanks,

Yan

-----Original Message-----
From: Reynold Xin [mailto:rxin@databricks.com] 
Sent: Thursday, June 26, 2014 8:57 PM
To: dev@spark.apache.org
Subject: Re: IntelliJ IDEA cannot compile TreeNode.scala

IntelliJ parser/analyzer/compiler behaves differently from Scala compiler, and sometimes lead to inconsistent behavior. This is one of the case.

In general while we use IntelliJ, we don't use it to build stuff. I personally always build in command line with sbt or Maven.



On Thu, Jun 26, 2014 at 7:43 PM, Ron Chung Hu (Ron Hu, ARC) < ron.hu@huawei.com> wrote:

> Hi,
>
> I am a Spark newbie.  I just downloaded Spark1.0.0 and latest IntelliJ 
> version 13.1 with Scala plug-in.  At spark-1.0.0 top level, I executed 
> the following SBT commands and they ran successfully.
>
>
> -          ./sbt/sbt assembly
>
> -          ./sbt/sbt update gen-idea
>
> After opening IntelliJ IDEA, I tried to compile 
> ...../sql/catalyst/trees/TreeNode.scala inside IntelliJ.  I got many 
> compile errors such as ""cannot resolve symbol children"", ""cannot resolve
> symbol id"".  Actually both symbols are defined in the same file.   As Spark
> was built successfully with ""sbt/sbt assembly"" command, I wondered 
> what went wrong in compiling TreeNode.scala.  Any pointer will be appreciated.
>
> Thanks.
>
> Best,
> Ron Hu
>
>
"
Andrew Or <andrew@databricks.com>,"Fri, 27 Jun 2014 16:27:46 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","(Forgot to mention, that UI bug is not in Spark 1.0.0, so it is technically
a regression)


2014-06-27 15:42 GMT-07:00 Andrew Or <andrew@databricks.com>:

"
=?gb2312?B?sNe41Q==?= <baigang@staff.sina.com.cn>,"Sat, 28 Jun 2014 01:44:47 +0000",Re: Contributing to MLlib on GLM,"""<dev@spark.apache.org>"" <dev@spark.apache.org>","Hi Xiaokai,

My bad. I didn't notice this before I created another PR for Poisson regression. The mails were buried in junk by the corp mail master. Also, thanks for considering my comments and advice in your PR.

Adding my two cents here:

* PoissonRegressionModel and GammaRegressionModel have the same fields and prediction method. Shall we use one instead of two redundant classes? Say, a LogLinearModel.
* The LBFGS optimizer takes fewer iterations and results in better convergence than SGD. I implemented two GeneralizedLinearAlgorithm classes using LBFGS and SGD respectively. You may take a look into it. If it's OK to you, I'd be happy to send a PR to your branch.
* In addition to the generated test data, We may use some real-world data for testing. In my implementation, I added the test data from https://onlinecourses.science.psu.edu/stat504/node/223. Please check my test suite.

-Gang
Sent from my iPad

> On 2014Äê6ÔÂ27ÈÕ, at ÏÂÎç6:03, ""xwei"" <weixiaokai@gmail.com> wrote:
> 
> 
> Yes, that's what we did: adding two gradient functions to Gradient.scala and
> create PoissonRegression and GammaRegression using these gradients. We made
> a PR on this.
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7088.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 27 Jun 2014 20:39:30 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),dev@spark.apache.org,"+1
Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2, YARN)
Smoke Tests (sparkPi,spark-shell, web UI) successful

Cheers
<k/>



"
Krakna H <shankark+sys@gmail.com>,"Sat, 28 Jun 2014 02:46:20 -0700 (PDT)",Re: Spark Matrix Factorization,dev@spark.incubator.apache.org,"Hi Deb,

Thanks so much for your response! At this point, we haven't determined
which of DSGD/ALS to go with and were waiting on guidance like yours to
tell us what the right option would be. It looks like ALS seems to be good
enough for our purposes.

Regards.







--"
Debasish Das <debasish.das83@gmail.com>,"Sat, 28 Jun 2014 07:56:30 -0700",Re: Spark Matrix Factorization,dev@spark.apache.org,"Factorization problems are non-convex and so both ALS and DSGD will
converge to local minima and it is not clear which minima will be better
than the other until we run both the algorithms and see...

So I will still say get a DSGD version running in the test setup while you
experiment with the Spark ALS...so that you can see if on your particular
dataset DSGD is converging to a better minima...

If you want I can put the DSGD code base that I used for experimentation on
github...I am not sure if Professor Re already put it on github...



d
rs
s
We
to
ctorization-tp55p7097.html
on
ctorization-tp55p7098.html
rvlet.jtp?macro=unsubscribe_by_code&node=1&code=c2hhbmthcmsrc3lzQGdtYWlsLmNvbXwxfDk3NjU5Mzg0
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ctorization-tp55p7109.html
"
Krakna H <shankark+sys@gmail.com>,"Sat, 28 Jun 2014 08:55:38 -0700 (PDT)",Re: Spark Matrix Factorization,dev@spark.incubator.apache.org,"Hi Deb,

Putting your code on github will be much appreciated -- it will give us a
good starting point to adapt for our purposes.

Regards.







--"
Tom Vacek <minnesota.cs@gmail.com>,"Sat, 28 Jun 2014 13:26:47 -0500",Re: Linear CG solver,dev@spark.apache.org,"What flavor of SVM are you trying to support? LSSVM doesn't need a bound
constraint, but most other formulations do.  There have been ideas for
bound-constrained CG, though bounded LBFGS is more common.  I think code
for Nystrom approximations or kernel mappings would be more useful.



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 28 Jun 2014 11:47:16 -0700",Re: Linear CG solver,dev@spark.apache.org,"Hi,

I am coming up with an iterative solver for Equality and bound constrained
quadratic minimization...

I have the cholesky versions running but cholesky does not scale for large
dimensions but works fine for matrix factorization use-cases where ranks
are low..

Minimize 0.5x'Px + q'x
s.t Aeq x = beq
lb <= x <= ub

Based on your decomposition you will end up using linear CG  in x-update or
NLCG/BFGS with bounds...I am not sure which one is better unless I see both
of them running on datasets....

I am hoping we can re-use the solver for SVM variants...

Could you please point to some implementation references for Nystrom
approximations or kernel mappings ?

Thanks.
Deb



"
Tom Vacek <minnesota.cs@gmail.com>,"Sat, 28 Jun 2014 14:12:49 -0500",Re: Linear CG solver,dev@spark.apache.org,"What is your general solver?  IPM or simplex or something else?  I have
seen a lot of attempts to apply iterative solvers for the subproblems on
those without much luck because the conditioning of the linear systems gets
worse and worse near the optimum.  IPOPT (interior point method) has an
LBFGS subproblem solver built in, so maybe it's worth a try to see if the
approach would meet your needs.  Methods more amenable to iterative
subproblem solvers might be augmented Lagrangian, nonlinear rescaling, or
Murty's (
http://www.researchgate.net/publication/250180549_A_New_Practically_Efficient_Interior_Point_Method_for_Quadratic_Programming)
methods.

This blog post gives a decent introduction to the kernel approximation
topic:
http://peekaboo-vision.blogspot.com/2012/12/kernel-approximations-for-efficient.html.
Missing is mention of the research into how to choose the best set of
prototype vectors.  (I believe Kmeans on the data is practically best.)  In
the approximation domain, ""Fastfood"" by Smola, et al. is a neat idea.
That's something I've thought about for MLLib, but I think the numeric
support is lacking in Java land.



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 28 Jun 2014 12:24:39 -0700",Re: Linear CG solver,dev@spark.apache.org,"Thanks Tom for the pointers...

I have a IPM running on the JVM which uses SOCP formulation for the
quadratic program I wrote above

We are going to show the details of it at the Summit....IPM runtimes and
accuracy give a baseline for the problem that we are solving...

Now we are trying to see how to come up with efficient versions for cases
where the constraints are not that many or not very complicated...which is
what most ML problems have...The idea is to use ADMM decomposition for
them...

If the constraints are LP style complex, it is better to use the IPM with
the SOCP directly...

Let me look into the blog and update you more...with jblas and
breeze-netlib-java I doubt there is any numerics that we cannot do on JVM
!...With these packages we call BLAS libraries from fortran...Missing is
sparse linear algebra from Tim Davis which will be exposed to the JVM in
the IPM package that I built...




"
Evan Sparks <evan.sparks@gmail.com>,"Sat, 28 Jun 2014 17:11:35 -0700",Re: Linear CG solver,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey,

We're actually working on similar ideas in the AMPlab with spark - for example we've got some image classification pipelines built on this idea - http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf

Approximating kernel methods via random projections hit with nonlinearity. 

Additionally, block coordinate descent seems to work pretty well for these scenarios where you end up with lots of features. An advantage of this approach in spark is that you can avoid materializing the whole data matrix if you're working on a subset of columns at a time. 

We're hoping to have some code out for public consumption later in the summer.

- Evan

s
ent_Interior_Point_Method_for_Quadratic_Programming)
cient.html.
n
d
e
r
th



/optimize/linear/ConjugateGradient.scala
77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala

"
xwei <weixiaokai@gmail.com>,"Sat, 28 Jun 2014 17:17:05 -0700 (PDT)",Re: Contributing to MLlib on GLM,dev@spark.incubator.apache.org,"Hi Gang,

No worries! 

I agree LBFGS would converge faster and your test suite is more comprehensive. I'd like to merge my branch with yours.

I also agree with your viewpoint on the redundancy issue. For different GLMs, usually they only differ in gradient calculation but the ****regression.scala files are quite similar. For example, linearRegressionSGD, logisticRegressionSGD, RidgeRegressionSGD, poissonRegressionSGD all share quite a bit of common code in their class implementations. Since such redundancy is already there in the legacy code, simply merging Poisson and Gamma does not seem to help much. So I suggest we just leave them as separate classes for the time being. 


Best regards,

Xiaokai

ote:

ession. The mails were buried in junk by the corp mail master. Also, thanks for considering my comments and advice in your PR. 
d prediction method. Shall we use one instead of two redundant classes? Say, a LogLinearModel. 
gence than SGD. I implemented two GeneralizedLinearAlgorithm classes using LBFGS and SGD respectively. You may take a look into it. If it's OK to you, I'd be happy to send a PR to your branch. 
 for testing. In my implementation, I added the test data from https://onlinecourses.science.psu.edu/stat504/node/223. Please check my test suite. 
a and 
made 
51.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7088.html
le.com. 
below:
-MLlib-on-GLM-tp7033p7107.html





--
3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7117.html
om."
Reynold Xin <rxin@databricks.com>,"Sun, 29 Jun 2014 16:56:18 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","We should make sure we include the following two patches:

https://github.com/apache/spark/pull/1264

https://github.com/apache/spark/pull/1263





"
Chester Chen <chester@alpinenow.com>,"Sun, 29 Jun 2014 19:57:04 -0700",Application level progress monitoring and communication,dev@spark.apache.org,"Hi Spark dev community:

I have several questions regarding Application and Spark communication

1) Application Level Progress Monitoring

Currently, our application using in YARN_CLUSTER model running Spark Jobs.
This works well so far, but we would like to monitoring the application
level progress ( not spark system level progress).

For example,
If we are doing Machine Learning Training, I would like to send some
message back the our application, current status of the training, number of
iterations etc via API.

We can't use YARN_CLIENT mode for this purpose as we are running the spark
application in servlet container (tomcat/Jetty). If we run the yarn_client
mode, we will be limited to one SparkContext per JVM.

So we are considering to leverage Akka messaging, essentially create
another Actor to send message back to the client application.
Notice that Spark already has an Akka ActorSystem defined for each
Executor. All we need to find Actor address (host, port) for the spark
driver executor.

The trouble is that driver's host and port are not known until later when
Resource Manager give to the executor node. How to communicate the host,
port info back to the client application ?

May be there is an Yarn API to obtain this information from Yarn Client.


2) Application and Spark Job communication In YARN Cluster mode.

    There are several use cases we are thinking may require communication
between the client side application and Spark Running Job.

       * Try to stop a running job -- while job is running, abort the long
running job in Yarn

      Again, we are think to use Akka Actor to send a STOP job message.



So here some of  questions:

* Is there any work regarding this area in the community ?

* what do you think the Akka approach ? Alternatives ?

* Is there a way to get Spark's Akka host and port from Yarn Resource
Manager to Yarn Client ?

Any suggestions welcome

Thanks
Chester
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 29 Jun 2014 22:54:14 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

We're going to move onto another rc because of this vote.
Unfortunately with the summit activities I haven't been able to usher
in the necessary patches and cut the RC. I will do so as soon as
possible and we can commence official voting.

- Patrick


"
Reynold Xin <rxin@databricks.com>,"Sun, 29 Jun 2014 23:07:03 -0700",Re: Application level progress monitoring and communication,"""dev@spark.apache.org"" <dev@spark.apache.org>","This isn't exactly about Spark itself, more about how an application on
YARN/Mesos can communicate with another one.

How about your application launch program just takes in a parameter (or env
variable or command line argument) for the IP address of your client
application, and just send updates? You basically just want to send
messages to report progress. You can do it with a lot of different ways,
such as Akka, custom REST API, Thrift ... I think any of them will do.





"
Andrew Ash <andrew@andrewash.com>,"Sun, 29 Jun 2014 23:09:12 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),dev@spark.apache.org,"Thanks for helping shepherd the voting on 1.0.1 Patrick.

I'd like to call attention to
https://issues.apache.org/jira/browse/SPARK-2157 and
https://github.com/apache/spark/pull/1107 -- ""Ability to write tight
firewall rules for Spark""

I'm currently unable to run Spark on some projects because our cloud ops
team is uncomfortable with the firewall situation around Spark at the
moment.  Currently Spark starts listening on random ephemeral ports and
does server to server communication on them.  This keeps the team from
writing tight firewall rules between the services -- they get real queasy
when asked to open inbound connections to the entire ephemeral port range
of a cluster.  We can tighten the size of the ephemeral range using kernel
settings to mitigate the issue, but it doesn't actually solve the problem.

The PR above aims to make every listening port on JVMs in a Spark
standalone cluster configurable with an option.  If not set, the current
behavior stands (start listening on an ephemeral port).  Is this something
the Spark team would consider merging into 1.0.1?

Thanks!
Andrew




"
Reynold Xin <rxin@databricks.com>,"Sun, 29 Jun 2014 23:14:42 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Andrew,

The port stuff is great to have, but they are pretty big changes to the
core that are introducing new features and are not exactly fixing important
bugs. For this reason, it probably can't block a release (I'm not even sure
if it should go into a maintenance release where we fix critical bugs for
Spark core).

We should definitely include them for 1.1.0 though (~Aug).





"
Andrew Ash <andrew@andrewash.com>,"Sun, 29 Jun 2014 23:23:18 -0700",Re: [VOTE] Release Apache Spark 1.0.1 (RC1),dev@spark.apache.org,"Ok that's reasonable -- it's certainly more of an enhancement than a
critical bug-fix.  I would like to get this in for 1.1.0 though, so let's
talk through the right way to do that on the PR.

In the meantime the best alternative is running with lax firewall settings,
which can be somewhat mitigated by modifying the ephemeral port range.

Thanks!
Andrew



"
salexln <salexln@gmail.com>,"Mon, 30 Jun 2014 08:54:58 -0700 (PDT)",Contributing to MLlib,dev@spark.incubator.apache.org,"Hi guys,

I'm new to Spark & MLlib and this may be a dumb question, but still....

As part of my M.Sc project, i'm working on implementation of Fuzzy C-means
(FCM) algorithm in MLlib.
FCM has many things in common with K - Means algorithm, which is already
implemented,  and I wanted to know whether should I create some inheritance
between them (some base class that would hold all the common stuff).

I could not find an answer to that in the ""Spark Coding Guide""
(https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide)


Appreciate your help

thanks,
Alex 




--

"
Chester Chen <chester@alpinenow.com>,"Mon, 30 Jun 2014 09:17:07 -0700",Re: Application level progress monitoring and communication,dev@spark.apache.org,"Reynold
    thanks for the reply. It's true, this is more to Yarn communication
than Spark.
But this is a general enough problem for all the YARN_CLUSTER mode
application. I thought
just to reach out to the community.

  If we choose to using Akka solution, then this is related to Spark, as
there is only one Akka actor system per JVM.

  Thanks for the suggestion regarding pass the client IP address. I was
only thinking  how to find out the IP address
of the spark drive node initially.

  Reporting Progress is just one of the use case, stopping spark job, We
are also considering interactive query jobs.

This gives me some thing to start with. I will try to with Akka first. Will
let community know once we got somewhere.

thanks
Chester



"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 30 Jun 2014 23:07:55 +0530",Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"Hi,

  While sending map output tracker result, the same serialized byte
array is sent multiple times - but the akka implementation copies it
to a private byte array within ByteString for each send.
Caching a ByteString instead of Array[Byte] did not help, since akka
does not support special casing ByteString : serializes the
ByteString, and copies the result out to an array before creating
ByteString out of it (in Array[Byte] serializing is thankfully simply
returning same array - so one copy only).


Given the need to send immutable data large number of times, is there
any way to do it in akka without copying internally in akka ?


To see how expensive it is, for 200 nodes withi large number of
mappers and reducers, the status becomes something like 30 mb for us -
and pulling this about 200 to 300 times results in OOM due to the
large number of copies sent out.


Thanks,
Mridul

"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 1 Jul 2014 00:43:26 +0530",Re: Eliminate copy while sending data : any Akka experts here ?,dev@spark.apache.org,"Our current hack is to use Broadcast variables when serialized
statuses are above some (configurable) size : and have the workers
directly pull them from master.
This is a workaround : so would be great if there was a
better/principled solution.

Please note that the responses are going to different workers
requesting for the output statuses for shuffle (after map) - so not
sure if back pressure buffers, etc would help.


Regards,
Mridul



"
