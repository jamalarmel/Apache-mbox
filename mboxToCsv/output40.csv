Georgios Samaras <georgesamarasdit@gmail.com>,"Thu, 1 Sep 2016 15:35:23 -0700",Is Spark's KMeans unable to handle bigdata?,dev@spark.apache.org,"Dear all,

  the random initialization works well, but the default initialization is
k-means|| and has made me struggle. Also, I had heard people one year ago
struggling with it too, and everybody would just skip it and use random,
but I cannot keep it inside me!

  I have posted a minimal example here
<http://stackoverflow.com/questions/39260820/is-sparks-kmeans-unable-to-handle-bigdata>
..

Please advice,
George Samaras
"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Fri, 02 Sep 2016 12:18:48 +0800",shuffle files not deleted after executor restarted,dev <dev@spark.apache.org>,"Hi all,

I discovered that sometimes executor exits unexpectedly and when it is restarted, it will create another blockmgr directory without deleting the old ones. Thus, for a long running application, some shuffle files will never be cleaned up. Sometimes those files could take up the whole disk. 

Is there a way to clean up those unused file automatically? Or is it safe to delete the old directory manually only leaving the newest one£ø

Here is the executor°Øs local directory.


Any advice on this?

Thanks.

Yang"
Liz Bai <lizbai@icloud.com>,"Fri, 02 Sep 2016 13:51:36 +0800",Dynamic Partitions When Writing Parquet,dev@spark.apache.org,"Hi there,

I have a question about writing Parquet using SparkSQL. Spark 1.4 has already supported writing DataFrames as Parquet files with ‚ÄúpartitionBy(colNames: String*)‚Äù, as Spark-6561 fixed.
Is there any method or plan to write Parquet with dynamic partitions? For example, instead of partitioning on the column Year(range:1900-2016) directly, do partition on the *decade* of the Year(range:190-201).
Thanks.

Best,
Ran
---------------------------------------------------------------------


"
Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Fri, 2 Sep 2016 10:51:44 +0300",Support for Hive 2.x,dev@spark.apache.org,"Hello!

I tried compiling Spark 2.0 with Hive 2.0, but as expected this failed.

So I am wondering if there is any talks going on about adding support of
Hive 2.x to Spark? I was unable to find any JIRA about this.


Thanks,
Rostyslav
"
Sean Owen <sowen@cloudera.com>,"Fri, 2 Sep 2016 09:07:35 +0100",Re: Is Spark's KMeans unable to handle bigdata?,Georgios Samaras <georgesamarasdit@gmail.com>,"Hm, what do you mean? k-means|| init is certainly slower because it's
making passes over the data in order to pick better initial centroids.
The idea is that you might then spend fewer iterations converging
later, and converge to a better clustering.

Your problem doesn't seem to be related to scale. You aren't even
running out of memory it seems. Your memory settings are causing YARN
to kill the executors for using more memory than they advertise. That
could mean it never proceeds if this happens a lot.

I don't have any problems with it.


---------------------------------------------------------------------


"
Sun Rui <sunrise_win@163.com>,"Fri, 2 Sep 2016 16:11:53 +0800",Re: shuffle files not deleted after executor restarted,=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Hi,
Could you give more information about your Spark environment? cluster manager, spark version, using dynamic allocation or not, etc..

Generally, executors will delete temporary directories for shuffle files on exit because JVM shutdown hooks are registered. Unless they are brutally killed.

You can safely delete the directories when you are sure that the spark applications related to them have finished. A crontab task may be used for automatic clean up.

restarted, it will create another blockmgr directory without deleting the old ones. Thus, for a long running application, some shuffle files will never be cleaned up. Sometimes those files could take up the whole disk. 
safe to delete the old directory manually only leaving the newest one£ø



---------------------------------------------------------------------


"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Fri, 02 Sep 2016 17:29:51 +0800",Re: shuffle files not deleted after executor restarted,Sun Rui <sunrise_win@163.com>,"Thank you for you response. 

We are using spark-1.6.2 on standalone deploy mode with dynamic allocation disabled.

I have traced the code. IMHO, it seems this cleanup is not handled by shutdown hooks directly. The shutdown hooks only send a °∞ExecutorStateChanged°± message to the worker and if the worker see the message, it will cleanup the directory only when this application is finished. In our case, the application is not finished (long running). The executor exits due to some unknown error and it is restarted by worker right away. In this scenario, those old directories are not going to be deleted. 

If the application is still running, is it safe to delete the old °∞blockmgr°± directory and leaving only the newest one?

Our temporary solution is to restart our application regularly and we are seeking a more elegant way. 

Thanks.

Yang


<sunrise_win@163.com> –¥µ¿£∫
manager, spark version, using dynamic allocation or not, etc..
files on exit because JVM shutdown hooks are registered. Unless they are brutally killed.
applications related to them have finished. A crontab task may be used for automatic clean up.
is restarted, it will create another blockmgr directory without deleting the old ones. Thus, for a long running application, some shuffle files will never be cleaned up. Sometimes those files could take up the whole disk. 
safe to delete the old directory manually only leaving the newest one£ø

"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Fri, 02 Sep 2016 09:40:13 +0000",Re: shuffle files not deleted after executor restarted,"=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>, 
	Sun Rui <sunrise_win@163.com>","Hi Yang,

Isn't external shuffle service better for long running applications?
""It runs as a standalone application and manages shuffle output files so
they are available for executors at all time""

It is described here:
https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-ExternalShuffleService.html

---
Artur


n
orker see the
r
1ÔºåSun Rui <sunrise_win@163.com> ÂÜôÈÅìÔºö
ly
r
te:
--
Artur Sukhenko
"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Fri, 02 Sep 2016 17:58:04 +0800",Re: shuffle files not deleted after executor restarted,Artur Sukhenko <artur.sukhenko@gmail.com>,"Yeah, using external shuffle service is a reasonable choice but I think we will still face the same problems. We use SSDs to store shuffle files for performance considerations. If the shuffle files are not going to be used anymore, we want them to be deleted instead of taking up valuable SSD space.

Sukhenko <artur.sukhenko@gmail.com> –¥µ¿£∫

so they are available for executors at all time""
https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-ExternalShuffleService.html <https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-ExternalShuffleService.html>
allocation disabled.
shutdown hooks directly. The shutdown hooks only send a °∞ExecutorStateChanged°± message to the worker and if the worker see the message, it will cleanup the directory only when this application is finished. In our case, the application is not finished (long running). The executor exits due to some unknown error and it is restarted by worker right away. In this scenario, those old directories are not going to be deleted. 
°∞blockmgr°± directory and leaving only the newest one?
are seeking a more elegant way. 
<sunrise_win@163.com <mailto:sunrise_win@163.com>> –¥µ¿£∫
manager, spark version, using dynamic allocation or not, etc..
files on exit because JVM shutdown hooks are registered. Unless they are brutally killed.
spark applications related to them have finished. A crontab task may be used for automatic clean up.
is restarted, it will create another blockmgr directory without deleting the old ones. Thus, for a long running application, some shuffle files will never be cleaned up. Sometimes those files could take up the whole disk. 
safe to delete the old directory manually only leaving the newest one£ø
<mailto:dev-unsubscribe@spark.apache.org>

"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Fri, 02 Sep 2016 18:01:00 +0800",Re: shuffle files not deleted after executor restarted,Artur Sukhenko <artur.sukhenko@gmail.com>,"
<tiandiwoxin@icloud.com> –¥µ¿£∫
think we will still face the same problems. We use SSDs to store shuffle files for performance considerations. If the shuffle files are not going to be used anymore, we want them to be deleted instead of taking up valuable SSD space.
Not very familiar with external shuffle service though. Is it going to help in this case? -:)
Sukhenko <artur.sukhenko@gmail.com <mailto:artur.sukhenko@gmail.com>> –¥µ¿£∫

so they are available for executors at all time""
https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-ExternalShuffleService.html <https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-ExternalShuffleService.html>
allocation disabled.
shutdown hooks directly. The shutdown hooks only send a °∞ExecutorStateChanged°± message to the worker and if the worker see the message, it will cleanup the directory only when this application is finished. In our case, the application is not finished (long running). The executor exits due to some unknown error and it is restarted by worker right away. In this scenario, those old directories are not going to be deleted. 
°∞blockmgr°± directory and leaving only the newest one?
are seeking a more elegant way. 
<sunrise_win@163.com <mailto:sunrise_win@163.com>> –¥µ¿£∫
cluster manager, spark version, using dynamic allocation or not, etc..
files on exit because JVM shutdown hooks are registered. Unless they are brutally killed.
spark applications related to them have finished. A crontab task may be used for automatic clean up.
is restarted, it will create another blockmgr directory without deleting the old ones. Thus, for a long running application, some shuffle files will never be cleaned up. Sometimes those files could take up the whole disk. 
it safe to delete the old directory manually only leaving the newest one£ø
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>

"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Fri, 02 Sep 2016 10:32:47 +0000",Re: shuffle files not deleted after executor restarted,=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>,"I believe in your case it will help, as executor's shuffle files will be
managed by external service.
It is described in spark docs: graceful-decommission-of-executors
<http://spark.apache.org/docs/latest/job-scheduling.html#graceful-decommission-of-executors>


Artur




8ÔºåÊ±™Ê¥ã <tiandiwoxin@icloud.com> ÂÜôÈÅìÔºö
e
ce.
0ÔºåArtur Sukhenko <artur.sukhenko@gmail.com> ÂÜôÈÅìÔºö
xternalShuffleService.html
worker see the
er
e
11ÔºåSun Rui <sunrise_win@163.com> ÂÜôÈÅìÔºö
lly
or
ote:
e
e
ü
--
Artur Sukhenko
"
Paul R <pj.rizk@gmail.com>,"Fri, 2 Sep 2016 11:26:26 -0400",sparkR array type not supported,dev@spark.apache.org,"Hi there,

I‚Äôve noticed the following command in sparkR 


Throws this error

array

Was wondering if this is a bug as the documentation says ‚Äúarray‚Äù should be implemented

Thanks 
---------------------------------------------------------------------


"
Dayne Sorvisto <daynesorvisto@yahoo.ca.INVALID>,"Fri, 2 Sep 2016 15:53:53 +0000 (UTC)",help from other committers on getting started,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I'd like to request help from committers/contributors to work on some trivial bug fixes or documentation for the Spark project. I'm very interested in the machine learning side of things as I have a math background. I recently passed the databricks cert and feel I have a decent understanding of the key concepts I need to get started as a beginner contributor. My github is DayneSorvisto (Dayne )¬†and I've signed up for a Jira account.
| ¬† |
| ¬† |  | ¬† | ¬† | ¬† | ¬† | ¬† |
| DayneSorvisto (Dayne )DayneSorvisto has 11 repositories available. Follow their code on GitHub. |
|  |
| View on github.com | Preview by Yahoo |
|  |
| ¬† |



Thank you,Dayne Sorvisto"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 2 Sep 2016 09:28:20 -0700",Re: sparkR array type not supported,Paul R <pj.rizk@gmail.com>,"I think it needs a type for the elements in the array. For example

f <- structField(""x"", ""array<integer>"")

Thanks
Shivaram

‚Äù should be implemented

---------------------------------------------------------------------


"
Georgios Samaras <georgesamarasdit@gmail.com>,"Fri, 2 Sep 2016 09:47:11 -0700",Re: Is Spark's KMeans unable to handle bigdata?,Sean Owen <sowen@cloudera.com>,"So you were able to execute the minimal example I posted?

I mean that the application doesn't progresses, it hangs (I would be OK if
it was just slower). It doesn't seem to me a configuration issue.


"
Sean Owen <sowen@cloudera.com>,"Fri, 2 Sep 2016 18:33:53 +0100",Re: Is Spark's KMeans unable to handle bigdata?,Georgios Samaras <georgesamarasdit@gmail.com>,"Yes it works fine, though each iteration of the parallel init step is
slow indeed -- about 5 minutes on my cluster. Given your question I
think you are actually 'hanging' because resources are being killed.

I think this init may need some love and optimization. For example, I
think treeAggregate might work better. An Array[Float] may be just
fine and cut down memory usage, etc.


---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Fri, 2 Sep 2016 10:40:17 -0700",Re: Support for Hive 2.x,Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Hi, Rostyslav,

After your email, I also tried to search in this morning, but I didn't find
a proper one.

The last related issue is SPARK-8064, `Upgrade Hive to 1.2`

https://issues.apache.org/jira/browse/SPARK-8064

If you want, you can file an JIRA issue including your pain points, then
you can monitor through it.

I guess you have more reasons to do that, not just a compilation issue.

Bests,
Dongjoon.




"
Sean Owen <sowen@cloudera.com>,"Fri, 2 Sep 2016 18:43:29 +0100",Re: Is Spark's KMeans unable to handle bigdata?,Georgios Samaras <georgesamarasdit@gmail.com>,"Eh... more specifically, since Spark 2.0 the ""runs"" parameter in the
KMeans mllib implementation has been ignored and is always 1. This
means a lot of code that wraps this stuff up in arrays could be
simplified quite a lot. I'll take a shot at optimizing this code and
see if I can measure an effect.


---------------------------------------------------------------------


"
Georgios Samaras <georgesamarasdit@gmail.com>,"Fri, 2 Sep 2016 10:45:34 -0700",Re: Is Spark's KMeans unable to handle bigdata?,Sean Owen <sowen@cloudera.com>,"I am not using the ""runs"" parameter anyway, but I see your point. If you
could point out any modifications in the minimal example I posted, I would
be more than interested to try them!


"
dayne sorvisto <daynesorvisto@gmail.com>,"Fri, 2 Sep 2016 12:56:52 -0600",Re: help getting started,dev@spark.apache.org,"Hi,

I'd like to request help from committers/contributors to work on some
trivial bug fixes or documentation for the Spark project. I'm very
interested in the machine learning side of things as I have a math
background. I recently passed the databricks cert and feel I have a decent
understanding of the key concepts I need to get started as a beginner
contributor.  and I've signed up for a Jira account.

Thank you


"
tomerk11 <tomer.kaftan@gmail.com>,"Fri, 2 Sep 2016 12:32:13 -0700 (MST)",Re: critical bugs to be fixed in Spark 2.0.1?,dev@spark.apache.org,"We are regularly hitting the issue described in SPARK-17110
(https://issues.apache.org/jira/browse/SPARK-17110) and this is blocking us
from upgrading from 1.6 to 2.0.0.

It would be great if this could be fixed for 2.0.1 



--

---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Fri, 2 Sep 2016 13:23:42 -0700",Re: help getting started,dayne sorvisto <daynesorvisto@gmail.com>,"Hi Dayne,
you can look at this page for some starter issues:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened).
Also check out this guide on how to contribute to Spark
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

regards,
--Jakob


---------------------------------------------------------------------


"
vonnagy <ivan@vadio.com>,"Fri, 2 Sep 2016 15:50:47 -0700 (MST)",Committing Kafka offsets when using DirectKafkaInputDStream,dev@spark.apache.org,"I have upgrading to Spark 2.0 and am experimenting with using Kafka 0.10.0. I
have a stream that I extract the data and would like to update the Kafka
offsets as each partition is handled. With Spark 1.6 or Spark 2.0 and Kafka
0.8.2 I was able to update the offsets, but now there seems no way to do so.
Here is an example

val stream = getStream

stream.forEachRDD { rdd =>
    val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges

    rdd.foreachPartition { events =>
        val partId = TaskContext.get.partitionId
        val offsets = offsetRanges(partId)

        // Do something with the data

        // Update the offsets for the partition so at most, the partition's
data would be duplicated
    }
}

With the new stream, I could call `commitAsync` with the offsets, but the
drawback here is that it would only update the offsets after the entire RDD
is handled. This can be a real issue for near ""exactly once"".

With the new logic, each partition has a Kafka consumer associated with each
partition, however, there is no access to it. I have looked at the
CachedKafkaConsumer classes and there is no way at the cache as well so that
I could call a commit on the offsets.

Beyond that I have tried to use the new Kafka 0.10 APIs, but always run into
errors as it requires one to subscribe to the topic and get assigned
partitions. I only want to update the offsets in Kafka. 

Any ideas would be helpful of how I might work with the Kafka API to set the
offsets or get Spark to add logic to allow the commitment of offsets on a
partition basis.

Thanks,

Ivan



--

---------------------------------------------------------------------


"
"""Miao Wang"" <wangmiao@us.ibm.com>","Fri, 2 Sep 2016 15:59:09 -0700",Re: critical bugs to be fixed in Spark 2.0.1?,tomerk11 <tomer.kaftan@gmail.com>,"
I am trying to reproduce it on my cluster based on your instructions.



From:	tomerk11 <tomer.kaftan@gmail.com>
To:	dev@spark.apache.org
Date:	09/02/2016 12:32 PM
Subject:	Re: critical bugs to be fixed in Spark 2.0.1?



We are regularly hitting the issue described in SPARK-17110
(https://issues.apache.org/jira/browse/SPARK-17110) and this is blocking us
from upgrading from 1.6 to 2.0.0.

It would be great if this could be fixed for 2.0.1



--
View this message in context:
http://apache-spark-developers-list.1001551.n3.nabble.com/critical-bugs-to-be-fixed-in-Spark-2-0-1-tp18686p18838.html

Nabble.com.

---------------------------------------------------------------------



"
Michael Allman <michael@videoamp.com>,"Fri, 2 Sep 2016 18:04:27 -0700",Re: help from other committers on getting started,Dayne Sorvisto <daynesorvisto@yahoo.ca>,"Hi Dayne,

Have a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>. I think you'll find answers to most of your questions there.

Cheers,

Michael


trivial bug fixes or documentation for the Spark project. I'm very interested in the machine learning side of things as I have a math background. I recently passed the databricks cert and feel I have a decent understanding of the key concepts I need to get started as a beginner contributor. My github is DayneSorvisto (Dayne ) <http://github.com/daynesorvisto> and I've signed up for a Jira account.
available. Follow their code on GitHub.

"
Dayne Sorvisto <daynesorvisto@yahoo.ca.INVALID>,"Sat, 3 Sep 2016 01:06:19 +0000 (UTC)",Re: help from other committers on getting started,Michael Allman <michael@videoamp.com>,"thank you Michael!I didn't know apache was a deep website on the clear net :P But I didn't expect anything less lol very coollll 

 

 Hi Dayne,
Have a look at¬†https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark. I think you'll find answers to most of your questions there.
Cheers,
Michael


Hi,
I'd like to request help from committers/contributors to work on some trivial bug fixes or documentation for the Spark project. I'm very interested in the machine learning side of things as I have a math background. I recently passed the databricks cert and feel I have a decent understanding of the key concepts I need to get started as a beginner contributor. My github is DayneSorvisto (Dayne )¬†and I've signed up for a Jira account.
| ¬† |
| ¬† |  | ¬† | ¬† | ¬† | ¬† | ¬† |
| DayneSorvisto (Dayne )DayneSorvisto has 11 repositories available. Follow their code on GitHub. |
|  |
| View on github.com | Preview by Yahoo |
|  |
| ¬† |



Thank you,Dayne Sorvisto



   "
Sean Owen <sowen@cloudera.com>,"Sat, 3 Sep 2016 09:22:37 +0100",Re: Is Spark's KMeans unable to handle bigdata?,Georgios Samaras <georgesamarasdit@gmail.com>,"I opened https://issues.apache.org/jira/browse/SPARK-17389 to track
some improvements, but by far the big one is that the init steps
defaults to 5, when the paper says that 2 is pretty much optimal here.
It's much faster with that setting.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 3 Sep 2016 11:12:01 +0000",Re: Support for Hive 2.x,,"
On 2 Sep 2016, at 18:40, Dongjoon Hyun <dongjoon@apache.org<mailto:dongjoon@apache.org>> wrote:

Hi, Rostyslav,

After your email, I also tried to search in this morning, but I didn't find a proper one.

The last related issue is SPARK-8064, `Upgrade Hive to 1.2`

https://issues.apache.org/jira/browse/SPARK-8064

If you want, you can file an JIRA issue including your pain points, then you can monitor through it.

I guess you have more reasons to do that, not just a compilation issue.



That was a pretty major change, as Spark SQL and Spark Thrift server  make use of the library in ways that the Hive authors never intended ‚Äîand so forced the spark teams to do terrible things to get stuff to hook up (thrift)

In The SQL side of things, parser changes broke stuff, as did changed error messages. Work there involved catching up with the changes, and differentiating regressions from simple changes in error messages triggering false alarms.

oh, and then there was the kryo version. Twitter have been moving Chill -> Kryo 3 in sync with their other codebase (storm?), spark's kryo version is driven by Chill; Hive needs to be in sync there or (as is done for Spark, a custom build of hive.jar made forcing it into the same version as chill & spark).

I did some preparatory work on a branch opening hive thrift server up for better subclassing

https://issues.apache.org/jira/browse/SPARK-10793

(FWIW Hive 1.2.1 actually uses a coy and past of the Hadoop 0.23 version of the hadoop yarn service classes, without the YARN-117 changes. If they could be moved back to the Hadoop reference implementation (i.e. commit to Hadoop 2.2+ and migrate back), and the thrift classes were reworked for better subclassing, life would be simpler ‚Äîleaving only the SQL changes and protobuf and kryo versions...

Bests,
Dongjoon.



On Fri, Sep 2, 2016 at 12:51 AM, Rostyslav Sotnychenko <r.sotnychenko@gmaiI tried compiling Spark 2.0 with Hive 2.0, but as expected this failed.

So I am wondering if there is any talks going on about adding support of Hive 2.x to Spark? I was unable to find any JIRA about this.


Thanks,
Rostyslav



"
Kapil Malik <kapil.malik@snapdeal.com>,"Sat, 3 Sep 2016 17:49:51 +0530","Catalog, SessionCatalog and ExternalCatalog in spark 2.0","user@spark.apache.org, dev@spark.apache.org","Hi all,

I have a Spark SQL 1.6 application in production which does following on
executing sqlContext.sql(...) -
1. Identify the table-name mentioned in query
2. Use an external database to decide where's the data located, in which
format (parquet or csv or jdbc) etc.
3. Load the dataframe
4. Register it as temp table (for future calls to this table)

This is achieved by extending HiveContext, and correspondingly HiveCatalog.
I have my own implementation of trait ""Catalog"", which over-rides the
""lookupRelation"" method to do the magic behind the scenes.

However, in spark 2.0, I can see following -
SessionCatalog - which contains lookupRelation method, but doesn't have any
interface / abstract class to it.
ExternalCatalog - which deals with CatalogTable instead of Df / LogicalPlan.
Catalog - which also doesn't expose any method to lookup Df / LogicalPlan.

So apparently it looks like I need to extend SessionCatalog only.
However, just wanted to get a feedback on if there's a better / recommended
approach to achieve this.


Thanks and regards,


Kapil Malik
*Sr. Principal Engineer | Data Platform, Technology*
M: +91 8800836581 | T: 0124-4330000 | EXT: 20910
ASF Centre A | 1st Floor | Udyog Vihar Phase IV |
Gurgaon | Haryana | India

*Disclaimer:* This communication is for the sole use of the addressee and
is confidential and privileged information. If you are not the intended
recipient of this communication, you are prohibited from disclosing it and
are required to delete it forthwith. Please note that the contents of this
communication do not necessarily represent the views of Jasper Infotech
Private Limited (""Company""). E-mail transmission cannot be guaranteed to be
secure or error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The Company,
therefore, does not accept liability for any loss caused due to this
communication. *Jasper Infotech Private Limited, Registered Office: 1st
Floor, Plot 238, Okhla Industrial Estate, New Delhi - 110020 INDIA CIN:
U72300DL2007PTC168097*
"
Omkar Reddy <omkarreddy2008@gmail.com>,"Sat, 3 Sep 2016 19:48:53 +0530",Subscription,dev@spark.apache.org,"Subscribe me!
"
Cody Koeninger <cody@koeninger.org>,"Sat, 3 Sep 2016 09:37:20 -0500",Re: Committing Kafka offsets when using DirectKafkaInputDStream,vonnagy <ivan@vadio.com>,"The Kafka commit api isn't transactional, you aren't going to get
exactly once behavior out of it even if you were committing offsets on
a per-partition basis.  This doesn't really have anything to do with
Spark; the old code you posted was already inherently broken.

Make your outputs idempotent and use commitAsync.
Or store offsets transactionally in your own data store.




---------------------------------------------------------------------


"
Kapil Malik <kapil.malik@snapdeal.com>,"Sat, 3 Sep 2016 20:28:56 +0530","Re: Catalog, SessionCatalog and ExternalCatalog in spark 2.0",Raghavendra Pandey <raghavendra.pandey@gmail.com>,"Thanks Raghavendra :)
Will look into Analyzer as well.


Kapil Malik
*Sr. Principal Engineer | Data Platform, Technology*
M: +91 8800836581 | T: 0124-4330000 | EXT: 20910
ASF Centre A | 1st Floor | Udyog Vihar Phase IV |
Gurgaon | Haryana | India

*Disclaimer:* This communication is for the sole use of the addressee and
is confidential and privileged information. If you are not the intended
recipient of this communication, you are prohibited from disclosing it and
are required to delete it forthwith. Please note that the contents of this
communication do not necessarily represent the views of Jasper Infotech
Private Limited (""Company""). E-mail transmission cannot be guaranteed to be
secure or error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The Company,
therefore, does not accept liability for any loss caused due to this
communication. *Jasper Infotech Private Limited, Registered Office: 1st
Floor, Plot 238, Okhla Industrial Estate, New Delhi - 110020 INDIA CIN:
U72300DL2007PTC168097*



"
Georgios Samaras <georgesamarasdit@gmail.com>,"Sat, 3 Sep 2016 10:31:58 -0700",Re: Is Spark's KMeans unable to handle bigdata?,Sean Owen <sowen@cloudera.com>,"Thank you very much Sean! If you would like, this could serve as an answer
in StackOverflow's question:
[Is Spark's kMeans unable to handle bigdata?](
http://stackoverflow.com/questions/39260820/is-sparks-kmeans-unable-to-handle-bigdata
).

Enjoy your weekend,
George


"
"""Miao Wang"" <wangmiao@us.ibm.com>","Sat, 3 Sep 2016 12:28:57 -0700",Re: critical bugs to be fixed in Spark 2.0.1?,"""Miao Wang"" <wangmiao@us.ibm.com>","
I just noticed JoshRosen sent a PR to this bug.



From:	Miao Wang/San Francisco/IBM@IBMUS
To:	tomerk11 <tomer.kaftan@gmail.com>
Cc:	dev@spark.apache.org
Date:	09/02/2016 04:04 PM
Subject:	Re: critical bugs to be fixed in Spark 2.0.1?



I am trying to reproduce it on my cluster based on your instructions.

Inactive hide details for tomerk11 ---09/02/2016 12:32:22 PM---We are
regularly hitting the issue described in SPARK-17110 (htttomerk11
---09/02/2016 12:32:22 PM---We are regularly hitting the issue described in
SPARK-17110 (https://issues.apache.org/jira/browse/S

From: tomerk11 <tomer.kaftan@gmail.com>
To: dev@spark.apache.org
Date: 09/02/2016 12:32 PM
Subject: Re: critical bugs to be fixed in Spark 2.0.1?



We are regularly hitting the issue described in SPARK-17110
(https://issues.apache.org/jira/browse/SPARK-17110) and this is blocking us
from upgrading from 1.6 to 2.0.0.

It would be great if this could be fixed for 2.0.1



--
View this message in context:
http://apache-spark-developers-list.1001551.n3.nabble.com/critical-bugs-to-be-fixed-in-Spark-2-0-1-tp18686p18838.html

Nabble.com.

---------------------------------------------------------------------





"
aditya1702 <adityavyas17@gmail.com>,"Sat, 3 Sep 2016 12:58:30 -0700 (MST)",Contribution to Apache Spark,dev@spark.apache.org,"Hello,
I am Aditya Vyas and I am currently in my third year of college doing BTech
in my engineering. I know python, a little bit of Java. I want to start
contribution in Apache Spark. This is my first time in the field of Big
Data. Can someone please help me as to how to get started. Which resources
to look at?



--

---------------------------------------------------------------------


"
=?utf-8?B?VG9tYXN6IEdhd8SZZGE=?= <tomasz.gaweda@outlook.com>,"Sat, 3 Sep 2016 20:09:43 +0000",Re: Contribution to Apache Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Contribution rules are described here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Pozdrawiam / Best regards,

Tomek Gawƒôda

W dniu 2016-09-03 o 21:58, aditya1702 pisze:

Hello,
I am Aditya Vyas and I am currently in my third year of college doing BTech
in my engineering. I know python, a little bit of Java. I want to start
contribution in Apache Spark. This is my first time in the field of Big
Data. Can someone please help me as to how to get started. Which resources
to look at?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contribution-to-Apache-Spark-tp18852.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>



"
Russell Spitzer <russell.spitzer@gmail.com>,"Sun, 04 Sep 2016 16:31:24 +0000",Re: spark cassandra issue,"Mich Talebzadeh <mich.talebzadeh@gmail.com>, Selvam Raman <selmna@gmail.com>","https://github.com/datastax/spark-cassandra-connector/blob/v1.3.1/doc/14_data_frames.md
In Spark 1.3 it was illegal to use ""table"" as a key in Spark SQL so in that
version of Spark the connector needed to use the option ""c_table""

val df = sqlContext.read.
     | format(""org.apache.spark.sql.cassandra"").
     | options(Map( ""c_table"" -> ""****"", ""keyspace"" -> ""***"")).
     | load()



d6zP6AcPCCdOABUrV8Pw
dOABUrV8Pw>*
m
_loading.md
bJd6zP6AcPCCdOABUrV8Pw
CCdOABUrV8Pw>*
es
(ddl.scala:151)
dl.scala:151)
ultSource.scala:120)
urce.scala:56)
ResolvedDataSource.scala:125)
Æµ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ®‡ØÜ‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡Æ®‡Æø‡ÆÆ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ""
Æµ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ®‡ØÜ‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡Æ®‡Æø‡ÆÆ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ""
µ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ®‡ØÜ‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡Æ®‡Æø‡ÆÆ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ""
"
Russell Spitzer <russell.spitzer@gmail.com>,"Sun, 04 Sep 2016 16:34:07 +0000",Re: spark cassandra issue,"Mich Talebzadeh <mich.talebzadeh@gmail.com>, Selvam Raman <selmna@gmail.com>","This would also be a better question for the SCC user list :)
https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user


data_frames.md
Jd6zP6AcPCCdOABUrV8Pw
CdOABUrV8Pw>*
s
2_loading.md
rbJd6zP6AcPCCdOABUrV8Pw
PCCdOABUrV8Pw>*
y
ges
:
t(ddl.scala:151)
ddl.scala:151)
aultSource.scala:120)
ource.scala:56)
(ResolvedDataSource.scala:125)
Æµ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ®‡ØÜ‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡Æ®‡Æø‡ÆÆ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ""
Æµ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ®‡ØÜ‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡Æ®‡Æø‡ÆÆ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ""
µ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ®‡ØÜ‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡Æ®‡Æø‡ÆÆ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ""
"
Georgios Samaras <georgesamarasdit@gmail.com>,"Sun, 4 Sep 2016 10:52:00 -0700",Active tasks is a negative number spark ui,dev <dev@spark.apache.org>,"Dear all,

  as discussed in this Stackoverflow question with a bounty
<http://stackoverflow.com/questions/38964007/active-tasks-is-a-negative-number-spark-ui>,
I was experiencing a similar situation to this:


Does anybody have an idea on why this would happen?

Best,
George
"
dsorvisto <daynesorvisto@gmail.com>,"Sun, 4 Sep 2016 11:40:07 -0700 (MST)",Re: Update Data in mySql using spark,dev@spark.apache.org,"Could you please provide more details, the question is a bit too narrow.



--

---------------------------------------------------------------------


"
Georgios Samaras <georgesamarasdit@gmail.com>,"Sun, 4 Sep 2016 20:07:44 -0700",Re: Active tasks is a negative number spark ui,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","Will do Sean, thank you!


"
"""farman.bsse1855"" <farman.ali@zigron.com>","Sun, 4 Sep 2016 21:51:08 -0700 (MST)",Re: Update Data in mySql using spark,dev@spark.apache.org,"I want to update specific data in mySql Table from Spark data-frames or
RDD. for example i have column
Name       ID
Farman     1
Ali              2
now i want to update name where id=1.How can I update this specific record.
how it is possible in Spark?





-- 
Farman Ali |Technology Group |Zigron Inc.
*Cell #:* +923217939066
*Email:* farman.ali@zigron.com

This message may contain confidential and/or privileged information.  If
you are not the addressee or authorized to receive this for the addressee,
you must not use, copy, disclose or take any action based on this message
or any information herein.  If you have received this message in error,
please advise the sender immediately by reply e-mail and delete this
message.  Thank you for your cooperation.




--"
"""farman.bsse1855"" <farman.ali@zigron.com>","Sun, 4 Sep 2016 21:55:21 -0700 (MST)",Re: Update Data in mySql using spark,dev@spark.apache.org,"I want to update specific data in mySql Table from Spark data-frames or RDD.
for example i have column
Name       ID
Farman     1
Ali              2
now i want to update name where id=1.How can I update this specific record.
how it is possible in Spark?



--

---------------------------------------------------------------------


"
Radoslaw Gruchalski <radek@gruchalski.com>,"Mon, 5 Sep 2016 06:33:59 -0400",Re: Fwd: seeing this message repeatedly.,"kant kodali <kanth909@gmail.com>, dev <dev@spark.apache.org>","All your workers go via public IP. Do you have the ports opened? Why public
IP? Is it not better to use the private 10.x address?

‚Äì
Best regards,
Radek Gruchalski
radek@gruchalski.com


:



---------- Forwarded message ----------
From: kant kodali <kanth909@gmail.com>
Date: Sat, Sep 3, 2016 at 5:39 PM
Subject: seeing this message repeatedly.
To: ""user @spark"" <user@spark.apache.org>



Hi Guys,

I am running my driver program on my local machine and my spark cluster is
on AWS. The big question is I don't know what are the right settings to get
around this public and private ip thing on AWS? my spark-env.sh currently
has the the following lines

export SPARK_PUBLIC_DNS=""52.44.36.224""
export SPARK_WORKER_CORES=12
export SPARK_MASTER_OPTS=""-Dspark.deploy.defaultCores=4""

I am seeing the lines below when I run my driver program on my local
machine. not sure what is going on ?



16/09/03 17:32:15 INFO DAGScheduler: Submitting 50 missing tasks from
ShuffleMapStage 0 (MapPartitionsRDD[1] at start at Consumer.java:41)
16/09/03 17:32:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 50 tasks
16/09/03 17:32:30 WARN TaskSchedulerImpl: Initial job has not accepted any
resources; check your cluster UI to ensure that workers are registered and
have sufficient resources
16/09/03 17:32:45 WARN TaskSchedulerImpl: Initial job has not accepted any
resources; check your cluster UI to ensure that workers are registered and
have sufficient resources
"
aditya1702 <adityavyas17@gmail.com>,"Mon, 5 Sep 2016 12:23:06 -0700 (MST)",Re: Contribution to Apache Spark,dev@spark.apache.org,"Thank you for replying. I will surely check the link :)



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 6 Sep 2016 12:42:48 -0700",Replacement for SparkSqlSerializer.deserialize[,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
In hbase-spark module of hbase, we previously had this code:

  def hbaseFieldToScalaType(
      f: Field,
      src: Array[Byte],
      offset: Int,
      length: Int): Any = {
...
        case BinaryType =>
          val newArray = new Array[Byte](length)
          System.arraycopy(src, offset, newArray, 0, length)
          newArray
        // TODO: add more data type support
        case _ => SparkSqlSerializer.deserialize[Any](src)

SparkSqlSerializer is no longer accessible as of spark 2.0.

Is there replacement for it ?

Thanks
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 6 Sep 2016 22:32:36 +0200",df.groupBy('m).agg(sum('n)).show dies with 10^3 elements?,dev <dev@spark.apache.org>,"Hi,

I'm concerned with the OOME in local mode with the version built today:

scala> val intsMM = 1 to math.pow(10, 3).toInt
intsMM: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4,
5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,
58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,
75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91,
92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106,
107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,
121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134,
135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148,
149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162,
163, 164, 165, 166, 167, 168, 169, 1...
scala> val df = intsMM.toDF(""n"").withColumn(""m"", 'n % 2)
df: org.apache.spark.sql.DataFrame = [n: int, m: int]

scala> df.groupBy('m).agg(sum('n)).show
...
16/09/06 22:28:02 ERROR Executor: Exception in task 6.0 in stage 0.0 (TID 6)
java.lang.OutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0
...

Please see https://gist.github.com/jaceklaskowski/906d62b830f6c967a7eee5f8eb6e9237
and let me know if I should file an issue. I don't think 10^3 elements
and groupBy should kill spark-shell.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Tue, 06 Sep 2016 21:51:37 +0000",Re: df.groupBy('m).agg(sum('n)).show dies with 10^3 elements?,"Jacek Laskowski <jacek@japila.pl>, dev <dev@spark.apache.org>","I think that this is a simpler case of
https://issues.apache.org/jira/browse/SPARK-17405. I'm going to comment on
that ticket with your simpler reproduction.


"
vinodep <vparames@andrew.cmu.edu>,"Tue, 6 Sep 2016 15:37:56 -0700 (MST)",BlockMatrix Multiplication fails with Out of Memory,dev@spark.apache.org,"Hi, 
I am trying to multiply Matrix of size 67584*67584 in a loop. In the first
iteration, multiplication goes through, but in the second iteration, it
fails with Java heap out of memory issue. I'm using pyspark and below is the
configuration.
Setup:
70 nodes (1driver+69 workers) with
SPARK_DRIVER_MEMORY=32g,SPARK_WORKER_CORES=16,SPARK_WORKER_MEMORY=20g,SPARK_EXECUTOR_MEMORY=5g,spark.executor.cores=5

Data : 67584 matrix size, block size is 1024
So, i basically load number of mat files (matlab .mat) files using textFile,
form a Block RDD with each file read being a block, and create a
blockmatrix(A)
Then, i multiply the matrix with itself in the loop, basically to get the
powers (A^^2,A^^4). But somehow multiplication always fails with out of
memory issues after second iteration.I'm using multiply method from
BlockMatrix

for i in range(3):
    A = A.multiply(A)

What am i missing? What is a correct way to load a big matrix file (.mat
)from local filesystem into rdd and create a blockmatrix and do repeated
multiplication? 



--

---------------------------------------------------------------------


"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Tue, 6 Sep 2016 16:05:21 -0700",Unable to run docker  jdbc integrations test ?,dev <dev@spark.apache.org>,"Hi, 


I am getting the following error , when I am trying to run jdbc docker integration tests on my laptop.   Any ideas , what I might be be doing wrong ?

build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0  -Phive-thriftserver -Phive -DskipTests clean install
build/mvn -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.11  compile test

Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
Discovery starting.
Discovery completed in 200 milliseconds.
Run starting. Expected test count is: 10
MySQLIntegrationSuite:

Error:
16/09/06 11:52:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9.31.117.25, 51868)
*** RUN ABORTED ***
  java.lang.AbstractMethodError:
  at org.glassfish.jersey.model.internal.CommonConfig.configureAutoDiscoverableProviders(CommonConfig.java:622)
  at org.glassfish.jersey.client.ClientConfig$State.configureAutoDiscoverableProviders(ClientConfig.java:357)
  at org.glassfish.jersey.client.ClientConfig$State.initRuntime(ClientConfig.java:392)
  at org.glassfish.jersey.client.ClientConfig$State.access$000(ClientConfig.java:88)
  at org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:120)
  at org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:117)
  at org.glassfish.jersey.internal.util.collection.Values$LazyValueImpl.get(Values.java:340)
  at org.glassfish.jersey.client.ClientConfig.getRuntime(ClientConfig.java:726)
  at org.glassfish.jersey.client.ClientRequest.getConfiguration(ClientRequest.java:285)
  at org.glassfish.jersey.client.JerseyInvocation.validateHttpMethodAndEntity(JerseyInvocation.java:126)
  ...
16/09/06 11:52:00 INFO SparkContext: Invoking stop() from shutdown hook
16/09/06 11:52:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!



Thanks
-suresh

"
Jacek Laskowski <jacek@japila.pl>,"Wed, 7 Sep 2016 06:27:05 +0200",Re: df.groupBy('m).agg(sum('n)).show dies with 10^3 elements?,Josh Rosen <joshrosen@databricks.com>,"Hi Josh,

Yes, that seems to be the issue. As I commented out in the JIRA, just
yesterday (after I had sent the email), such simple queries like the
following killed spark-shell:

Seq(1).toDF.groupBy('value).count.show

Hoping to see it get resolved soon. If there's anything I could help
you with to fix/reproduce the issue, let me know. I wish I knew how to
write a unit test for this. Where in the code to look for inspiration?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Yanbo Liang <ybliang8@gmail.com>,"Tue, 6 Sep 2016 23:36:48 -0700",Discuss SparkR executors/workers support virtualenv,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,


Many users have requirements to use third party R packages in
executors/workers, but SparkR can not satisfy this requirements elegantly.
For example, you should to mess with the IT/administrators of the cluster
to deploy these R packages on each executors/workers node which is very
inflexible.

I think we should support third party R packages for SparkR users as what
we do for jar packages in the following two scenarios:
1, Users can install R packages from CRAN or custom CRAN-like repository
for each executors.
2, Users can load their local R packages and install them on each executors.

To achieve this goal, the first thing is to make SparkR executors support
virtualenv like Python conda. I have investigated and found packrat(
http://rstudio.github.io/packrat/) is one of the candidates to support
virtualenv for R. Packrat is a dependency management system for R and can
isolate the dependent R packages in its own private package space. Then
SparkR users can install third party packages in the application
scope(destroy after the application exit) and don‚Äôt need to bother
IT/administrators to install these packages manually.

I would like to know whether it make sense.


Thanks

Yanbo
"
Sean Owen <sowen@cloudera.com>,"Wed, 7 Sep 2016 09:29:05 +0100","Removing published kinesis, ganglia artifacts due to license issues?",dev <dev@spark.apache.org>,"It's worth calling attention to:

https://issues.apache.org/jira/browse/SPARK-17418
https://issues.apache.org/jira/browse/SPARK-17422

It looks like we need to at least not publish the kinesis *assembly*
Maven artifact because it contains Amazon Software Licensed-code
directly.

However there's a reasonably strong reason to believe that we'd have
to remove the non-assembly Kinesis artifact too, as well as the
Ganglia one. This doesn't mean it goes away from the project, just
means it would no longer be published as a Maven artifact. (These have
never been bundled in the main Spark artifacts.)

I wanted to give a heads up to see if anyone a) believes this
conclusion is wrong or b) wants to take it up with legal@? I'm
inclined to believe we have to remove them given the interpretation
Luciano has put forth.

Sean

---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 7 Sep 2016 02:38:26 -0700 (MST)",implement UDF/UDAF supporting whole stage codegen,dev@spark.apache.org,"Hi,
I want to write a UDF/UDAF which provides native processing performance. Currently, when creating a UDF/UDAF in a normal manner the performance is hit because it breaks optimizations.
For a simple example I wanted to create a UDF which tests whether the value is smaller than 10.
I tried something like this :

import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.analysis.TypeCheckResult
import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}
import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
import org.apache.spark.sql.catalyst.util.TypeUtils
import org.apache.spark.sql.types._
import org.apache.spark.util.Utils
import org.apache.spark.sql.catalyst.expressions._

case class genf(child: Expression) extends UnaryExpression with Predicate with ImplicitCastInputTypes {

  override def inputTypes: Seq[AbstractDataType] = Seq(IntegerType)

  override def toString: String = s""$child < 10""

  override def eval(input: InternalRow): Any = {
    val value = child.eval(input)
    if (value == null)
    {
      false
    } else {
      child.dataType match {
        case IntegerType => value.asInstanceOf[Int] < 10
      }
    }
  }

  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
   defineCodeGen(ctx, ev, c => s""($c) < 10"")
  }
}


However, this doesn't work as some of the underlying classes/traits are private (e.g. AbstractDataType is private) making it problematic to create a new case class.
Is there a way to do it? The idea is to provide a couple of jars with a bunch of functions our team needs.
Thanks,
                Assaf.





--"
"""farman.bsse1855"" <farman.ali@zigron.com>","Wed, 7 Sep 2016 04:27:20 -0700 (MST)",How to get 2 years prior date from currentdate using Spark Sql,dev@spark.apache.org,"I need to derive 2 years prior date of current date using a query in Spark
Sql. For ex : today's date is 2016-09-07. I need to get the date exactly 2
years before this date in the above format (YYYY-MM-DD).

Please let me know if there are multiple approaches and which one would be
better.

Thanks



--

---------------------------------------------------------------------


"
Yong Zhang <java8964@hotmail.com>,"Wed, 7 Sep 2016 13:13:35 +0000",Re: How to get 2 years prior date from currentdate using Spark Sql,"farman.bsse1855 <farman.ali@zigron.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","https://issues.apache.org/jira/browse/SPARK-8185

[SPARK-8185] date/time function: datediff - ASF JIRA<https://issues.apache.org/jira/browse/SPARK-8185>
issues.apache.org
Spark; SPARK-8159 Improve expression function coverage (Spark 1.5) SPARK-8185; date/time function: datediff




________________________________
From: farman.bsse1855 <farman.ali@zigron.com>
Sent: Wednesday, September 7, 2016 7:27 AM
To: dev@spark.apache.org
Subject: How to get 2 years prior date from currentdate using Spark Sql

I need to derive 2 years prior date of current date using a query in Spark
Sql. For ex : today's date is 2016-09-07. I need to get the date exactly 2
years before this date in the above format (YYYY-MM-DD).

Please let me know if there are multiple approaches and which one would be
better.

Thanks



--
3.nabble.com/How-to-get-2-years-prior-date-from-currentdate-using-Spark-Sql-tp18875.html
Apache Spark Developers List - How to get 2 years prior date from currentdate using Spark Sql<http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-get-2-years-prior-date-from-currentdate-using-Spark-Sql-tp18875.html>
apache-spark-developers-list.1001551.n3.nabble.com
How to get 2 years prior date from currentdate using Spark Sql. I need to derive 2 years prior date of current date using a query in Spark Sql. For ex : today's date is 2016-09-07. I need to get the...


om.

---------------------------------------------------------------------

"
Yong Zhang <java8964@hotmail.com>,"Wed, 7 Sep 2016 13:14:48 +0000",Re: How to get 2 years prior date from currentdate using Spark Sql,"farman.bsse1855 <farman.ali@zigron.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","sorry, should be date_sub


https://issues.apache.org/jira/browse/SPARK-8187

[SPARK-8187] date/time function: date_sub - ASF JIRA<https://issues.apache.org/jira/browse/SPARK-8187>
issues.apache.org
Apache Spark added a comment - 12/Jun/15 06:56 User 'adrian-wang' has created a pull request for this issue: https://github.com/apache/spark/pull/6782




________________________________
From: Yong Zhang <java8964@hotmail.com>
Sent: Wednesday, September 7, 2016 9:13 AM
To: farman.bsse1855; dev@spark.apache.org
Subject: Re: How to get 2 years prior date from currentdate using Spark Sql


https://issues.apache.org/jira/browse/SPARK-8185

[SPARK-8185] date/time function: datediff - ASF JIRA<https://issues.apache.org/jira/browse/SPARK-8185>
issues.apache.org
Spark; SPARK-8159 Improve expression function coverage (Spark 1.5) SPARK-8185; date/time function: datediff




________________________________
From: farman.bsse1855 <farman.ali@zigron.com>
Sent: Wednesday, September 7, 2016 7:27 AM
To: dev@spark.apache.org
Subject: How to get 2 years prior date from currentdate using Spark Sql

I need to derive 2 years prior date of current date using a query in Spark
Sql. For ex : today's date is 2016-09-07. I need to get the date exactly 2
years before this date in the above format (YYYY-MM-DD).

Please let me know if there are multiple approaches and which one would be
better.

Thanks



--
3.nabble.com/How-to-get-2-years-prior-date-from-currentdate-using-Spark-Sql-tp18875.html
Apache Spark Developers List - How to get 2 years prior date from currentdate using Spark Sql<http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-get-2-years-prior-date-from-currentdate-using-Spark-Sql-tp18875.html>
apache-spark-developers-list.1001551.n3.nabble.com
How to get 2 years prior date from currentdate using Spark Sql. I need to derive 2 years prior date of current date using a query in Spark Sql. For ex : today's date is 2016-09-07. I need to get the...


om.

---------------------------------------------------------------------

"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Wed, 7 Sep 2016 15:22:02 +0200",Re: How to get 2 years prior date from currentdate using Spark Sql,Yong Zhang <java8964@hotmail.com>,"This is more a @use question.

You can write the following in sql: select date '2016-09-07' - interval 2
years

HTH


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 7 Sep 2016 09:07:43 -0700",Re: Discuss SparkR executors/workers support virtualenv,Yanbo Liang <ybliang8@gmail.com>,"I think this makes sense -- making it easier to use additional R
packages would be a good feature. I am not sure we need Packrat for
this use case though. Lets continue discussion on the JIRA at
https://issues.apache.org/jira/browse/SPARK-17428

Thanks
Shivaram

.
 to
 we
for
rs.
.
r

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 7 Sep 2016 10:43:58 -0700","Re: Removing published kinesis, ganglia artifacts due to license issues?",Sean Owen <sowen@cloudera.com>,"I agree, we should not be publishing both of them.
Thanks for bringing this up !

Regards,
Mridul



---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Wed, 7 Sep 2016 12:44:52 -0500","Re: Removing published kinesis, ganglia artifacts due to license issues?",Sean Owen <sowen@cloudera.com>,"I don't see a reason to remove the non-assembly artifact, why would
you?  You're not distributing copies of Amazon licensed code, and the
Amazon license goes out of its way not to over-reach regarding
derivative works.

This seems pretty clearly to fall in the spirit of

http://www.apache.org/legal/resolved.html#optional

I certainly think the majority of Spark users will still want to use
Spark without adding Kinesis


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 7 Sep 2016 19:35:02 +0100","Re: Removing published kinesis, ganglia artifacts due to license issues?",Cody Koeninger <cody@koeninger.org>,"(Credit to Luciano for pointing it out)

Yes it's clear why the assembly can't be published but I had the same
question about the non-assembly Kinesis (and ganglia) artifact,
because the published artifact has no code from Kinesis.

See the related discussion at
https://issues.apache.org/jira/browse/LEGAL-198 ; the point I took
from there is that the Spark Kinesis artifact is optional with respect
to Spark, but still something published by Spark, and it requires the
Amazon-licensed code non-optionally.

I'll just ask that question to confirm or deny.

(It also has some background on why the Amazon License is considered
""Category X"" in ASF policy due to field of use restrictions. I myself
take that as read rather than know the details of that decision.)


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 7 Sep 2016 11:57:46 -0700","Re: Removing published kinesis, ganglia artifacts due to license issues?",Sean Owen <sowen@cloudera.com>,"I think you should ask legal about how to have some Maven artifacts for these. Both Ganglia and Kinesis are very widely used, so it's weird to ask users to build them from source. Maybe the Maven artifacts can be marked as being under a different license?

In the initial discussion for LEGAL-198, we were told the following:

""If the component that uses this dependency is not required for the rest of Spark to function then you can have a subproject to build the component. See http://www.apache.org/legal/resolved.html#optional. This means you will have to provide instructions for users to enable the optional component (which IMO should provide pointers to the licensing).""

It's not clear whether ""enable the optional component"" means ""every user must build it from source"", or whether we could tell users ""here's a Maven coordinate you can add to your project if you're okay with the licensing"".

Matei

have
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 7 Sep 2016 20:14:19 +0100","Re: Removing published kinesis, ganglia artifacts due to license issues?",Matei Zaharia <matei.zaharia@gmail.com>,"Agree, I've asked the question on that thread and will follow it up.
I'd prefer not to pull these unless it's fairly clear it's going to be
against policy.


te:
hese. Both Ganglia and Kinesis are very widely used, so it's weird to ask users to build them from source. Maybe the Maven artifacts can be marked as being under a different license?
of Spark to function then you can have a subproject to build the component. See http://www.apache.org/legal/resolved.html#optional. This means you will have to provide instructions for users to enable the optional component (which IMO should provide pointers to the licensing).""
must build it from source"", or whether we could tell users ""here's a Maven coordinate you can add to your project if you're okay with the licensing"".

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 7 Sep 2016 12:20:25 -0700","Re: Removing published kinesis, ganglia artifacts due to license issues?",Matei Zaharia <matei.zaharia@gmail.com>,"It is good to get clarification, but the way I read it, the issue is
whether we publish it as official Apache artifacts (in maven, etc).

Users can of course build it directly (and we can make it easy to do so) -
as they are explicitly agreeing to additional licenses.

Regards
Mridul



"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 7 Sep 2016 12:40:25 -0700","Re: Removing published kinesis, ganglia artifacts due to license issues?",Matei Zaharia <matei.zaharia@gmail.com>,"
As long as they are not part of an ""Apache licensed""  distribution. Note
that Ganglia seems to have changed license to BSD and we might be able to
better support that.



I think the key here is ""optional"", while the Kinesis is optional for Spark
(which makes it ok to have it in Spark) it is not optional for Kinesis
extension, which thenm IMHO, does not allow us to publish the Kinesis
artifact either.

But let's wait on the response from Legal before we actually implement a
solution.




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 7 Sep 2016 12:41:56 -0700","Re: Removing published kinesis, ganglia artifacts due to license issues?",Mridul Muralidharan <mridul@gmail.com>,"
+1, by providing instructions on how the user would build, and attaching
the license details on the instructions, we are then safe on the legal
aspects of it.



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Cody Koeninger <cody@koeninger.org>,"Wed, 7 Sep 2016 16:05:21 -0500","Re: Removing published kinesis, ganglia artifacts due to license issues?",Luciano Resende <luckbr1975@gmail.com>,"To be clear, ""safe"" has very little to do with this.

It's pretty clear that there's very little risk of the spark module
for kinesis being considered a derivative work, much less all of
spark.

The use limitation in 3.3 that caused the amazon license to be put on
the apache X list also doesn't have anything to do with a legal safety
risk here.  Really, what are you going to use a kinesis connector for,
except for connecting to kinesis?



---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 7 Sep 2016 14:36:29 -0700",Re: Unable to run docker jdbc integrations test ?,Suresh Thalamati <suresh.thalamati@gmail.com>,"It looks like there is nobody running these tests, and after some
dependency upgrades in Spark 2.0 this has stopped working. I have tried to
bring up this but I am having some issues with getting the right
dependencies loaded and satisfying the docker-client expectations.

The question then is: Does the community find value on having these tests
available ? Then we can focus on bringing them up and I can go push my
previous experiments as a WIP PR. Otherwise we should just get rid of these
tests.

Thoughts ?





-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 7 Sep 2016 14:42:13 -0700","Re: Removing published kinesis, ganglia artifacts due to license issues?",Cody Koeninger <cody@koeninger.org>,"The question is just whether the metadata and instructions involving these Maven packages counts as sufficient to tell the user that they have different licensing terms. For example, our Ganglia package was called spark-ganglia-lgpl (so you'd notice it's a different license even from its name), and our Kinesis one was called spark-streaming-kinesis-asl, and our docs both mentioned these were under different licensing terms. But is that enough? That's the question.

Matei

<mridul@gmail.com>
so) -
attaching the
aspects


---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 07 Sep 2016 21:46:24 +0000",Re: Unable to run docker jdbc integrations test ?,"Luciano Resende <luckbr1975@gmail.com>, Suresh Thalamati <suresh.thalamati@gmail.com>","I think that these tests are valuable so I'd like to keep them. If
possible, though, we should try to get rid of our dependency on the Spotify
docker-client library, since it's a dependency hell nightmare. Given our
relatively simple use of Docker here, I wonder whether we could just write
some simple scripting over the `docker` command-line tool instead of
pulling in such a problematic library.


"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 7 Sep 2016 16:58:15 -0700",Re: Unable to run docker jdbc integrations test ?,Josh Rosen <joshrosen@databricks.com>,"That might be a reasonable and much more simpler approach to try... but if
we resolve these issues, we should make it part of some frequent build to
make sure the build don't regress and that the actual functionality don't
regress either. Let me look into this again...




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 8 Sep 2016 08:33:28 +0200",FileStreamSource source checks path eagerly?,dev <dev@spark.apache.org>,"Hi,

I'm wondering what's the rationale for checking the path option
eagerly in FileStreamSource? My thinking is that until start is called
there's no processing going on that is supposed to happen on executors
(not the driver) with the path available.

I could (and perhaps should) use dfs but IMHO that just hides the real
question of the text source eagerness.

Please help me understand the rationale of the choice. Thanks!

scala> spark.version
res0: String = 2.1.0-SNAPSHOT

scala> spark.readStream.format(""text"").load(""/var/logs"")
org.apache.spark.sql.AnalysisException: Path does not exist: /var/logs;
  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:229)
  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:81)
  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:81)
  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)
  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:142)
  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:153)
  ... 48 elided

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Fred Reiss <freiss.oss@gmail.com>,"Thu, 8 Sep 2016 00:03:18 -0700",Re: FileStreamSource source checks path eagerly?,Jacek Laskowski <jacek@japila.pl>,"The input directory does need to be visible from the driver process, since
FileStreamSource does its polling from the driver. FileStreamSource creates
a Dataset for each microbatch.

I suppose the type-inference-time check for the presence of the input
directory could be moved to the FileStreamSource's initialization. But if
the directory isn't there when the source is being created, it probably
won't be there when the source is instantiated.

Fred


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 8 Sep 2016 11:20:22 +0200",Re: FileStreamSource source checks path eagerly?,Fred Reiss <freiss.oss@gmail.com>,"

Hi Fred,

Thanks for your prompt response, Fred.

Isn't it opposite to sc.textFile? The source might not be available
until load. There's no reason it should. Yet it is definitely not
against the ""contract"" of DataFrameReader.textFile and perhaps it's
implictly assumed in SQL.

scala> spark.read.textFile(""whatever"")
org.apache.spark.sql.AnalysisException: Path does not exist:
file:/Users/jacek/dev/oss/spark/whatever;
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:371)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:360)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.immutable.List.flatMap(List.scala:344)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:360)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)
  at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:500)
  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:536)
  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:509)
  ... 48 elided

I thought it might've been due to schema inference but...

scala> spark.read.schema(StructType(Seq())).textFile(""whatever"")
org.apache.spark.sql.AnalysisException: User specified schema not
supported with `textFile`;
  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:534)
  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:509)
  ... 50 elided

(which also confuses me, but don't wanna drag this thread in multiple
directions) Definitely need some help to understand the rationale
behing this eager behaviour.

Thanks!

Pozdrawiam,
Jacek

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 8 Sep 2016 09:20:35 +0000",Re: FileStreamSource source checks path eagerly?,Jacek Laskowski <jacek@japila.pl>,"failfast generally means that you find problems sooner rather than later, and here, potentially, that your code runs but simply returns empty data without any obvious cue as to what is wrong.

As is always good in OSS, follow those stack trace links to see what they say:

        // Check whether the path exists if it is not a glob pattern.
        // For glob pattern, we do not check it because the glob pattern might only make sense
        // once the streaming job starts and some upstream source starts dropping data.

If you specify a glob pattern, you'll get the late check at the expense of the risk of that empty data source if the pattern is wrong. Something like ""/var/log\s"" would suffice, as the presence of the backslash is enough for SparkHadoopUtil.isGlobPath() to conclude that its something for the globber.


taSource.scala:229)
ompute(DataSource.scala:81)
Source.scala:81)
eamingRelation.scala:30)
.scala:142)
.scala:153)


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 8 Sep 2016 11:23:56 +0200",Re: FileStreamSource source checks path eagerly?,Steve Loughran <stevel@hortonworks.com>,"Hi Steve,

Thank you for more source-oriented answer. Helped but didn't explain
the reason for such eagerness. The file(s) might not be on the driver
but on executors only where the Spark job(s) run. I don't see why
Spark should check the file(s) regardless of glob pattern being used.

You see my way of thinking?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


ote:
 and here, potentially, that your code runs but simply returns empty data without any obvious cue as to what is wrong.
 say:
might only make sense
dropping data.
f the risk of that empty data source if the pattern is wrong. Something like ""/var/log\s"" would suffice, as the presence of the backslash is enough for SparkHadoopUtil.isGlobPath() to conclude that its something for the globber.
ataSource.scala:229)
compute(DataSource.scala:81)
aSource.scala:81)
reamingRelation.scala:30)
r.scala:142)
r.scala:153)

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 8 Sep 2016 14:24:57 +0200",@scala.annotation.varargs or @_root_.scala.annotation.varargs?,dev <dev@spark.apache.org>,"Hi,

The code is not consistent with @scala.annotation.varargs annotation.
There are classes with @scala.annotation.varargs like DataFrameReader
or functions as well as examples of @_root_.scala.annotation.varargs,
e.g. Window or UserDefinedAggregateFunction.

I think it should be consistent and @scala.annotation.varargs only. WDYT?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 8 Sep 2016 17:27:24 +0100",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Jacek Laskowski <jacek@japila.pl>,"I think the @_root_ version is redundant because
@scala.annotation.varargs is redundant. Actually wouldn't we just
import varargs and write @varargs?


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 8 Sep 2016 12:49:54 -0700",Re: FileStreamSource source checks path eagerly?,Jacek Laskowski <jacek@japila.pl>,"This source is meant to be used for a shared file system such as HDFS or NFS, where both the driver and the workers can see the same folders. There's no support in Spark for just working with local files on different workers.

Matei

later, and here, potentially, that your code runs but simply returns empty data without any obvious cue as to what is wrong.
they say:
pattern might only make sense
starts dropping data.
expense of the risk of that empty data source if the pattern is wrong. Something like ""/var/log\s"" would suffice, as the presence of the backslash is enough for SparkHadoopUtil.isGlobPath() to conclude that its something for the globber.
called
executors
real
/var/logs;
org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:229)
org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:81)
org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:81)
org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)
org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:142)
org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:153)
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Thu, 8 Sep 2016 13:28:13 -0700",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Sean Owen <sowen@cloudera.com>,"+1 to Sean's answer, importing varargs.
In this case the _root_ is also unnecessary (it would be required in
case you were using it in a nested package called ""scala"" itself)


---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Fri, 9 Sep 2016 08:58:35 +0900",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Jakob Odersky <jakob@odersky.com>,"I was also actually wondering why it is being written like this.

I actually took a look for this before and wanted to fix them but I found
https://github.com/apache/spark/pull/12077/files#r58041468

So, I kind of persuaded myself that committers already know about it and
there is a reason for this.

I'd like to know the full details why we don't import but write full path
though.


"
Reynold Xin <rxin@databricks.com>,"Fri, 9 Sep 2016 09:07:56 +0900",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Hyukjin Kwon <gurwls223@gmail.com>,"There is a package called scala.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 8 Sep 2016 17:11:44 -0700",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Reynold Xin <rxin@databricks.com>,"Not after SPARK-14642, right?




-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 9 Sep 2016 09:13:51 +0900",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Marcelo Vanzin <vanzin@cloudera.com>,"Yea but the earlier email was asking they were introduced in the first
place.


"
"""liguoqiang (I)"" <liguoqiang9@huawei.com>","Fri, 9 Sep 2016 00:45:38 +0000",I'm working on SPARK-6235 Address various 2G limits.,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, all

I'm working on SPARK-6235 Address various 2G limits<https://issues.apache.org/jira/browse/SPARK-6235>, welcome to ask questions or make comments.
The jira:  https://issues.apache.org/jira/browse/SPARK-6235
and the code: https://github.com/apache/spark/pull/14995


Thanks
Guoqiang Li
"
Priya Ch <learnings.chitturi@gmail.com>,"Fri, 9 Sep 2016 14:46:54 +0530",Video analytics on SPark,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","Hi All,

I have video surveillance data and this needs to be processed in Spark. I
am going through the Spark + OpenCV. How to load .mp4 images into an RDD ?
Can we directly do this or the video needs to be coverted to sequenceFile ?

Thanks,
Padma CH
"
Sean Owen <sowen@cloudera.com>,"Fri, 9 Sep 2016 13:29:58 +0100",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Reynold Xin <rxin@databricks.com>,"Oh I get it now. I was necessary in the past. Sure, seems like it
could be standardized now.


---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Fri, 9 Sep 2016 22:55:26 +0900","Change the settings in AppVeyor to prevent triggering the tests in
 other PRs in other branches",dev <dev@spark.apache.org>,"Hi all,


Currently, it seems the settings in AppVeyor is default and runs some tests
on different branches. For example,


https://github.com/apache/spark/pull/15023

https://github.com/apache/spark/pull/15022


It seems it happens only in other branches as they don‚Äôt have appveyor.yml
and try to refer the configuration in the web (although I have to test
this).


I‚Äôd be great if any of auhorized one sets the branch to test to master
brunch only as described in

https://github.com/apache/spark/blob/master/dev/appveyor-guide.md#specifying-the-branch-for-building-and-setting-the-build-schedule


I just manually tested this. With the setting, it would not trigger the
test for another branch, for example,
https://github.com/spark-test/spark/pull/5

Currently, with the default settings, it will run the tests on another
branch, for example, https://github.com/spark-test/spark/pull/4


Thanks.


‚Äã
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 9 Sep 2016 08:41:03 -0700","Re: Change the settings in AppVeyor to prevent triggering the tests
 in other PRs in other branches",Hyukjin Kwon <gurwls223@gmail.com>,"Thanks for debugging - I'll reply on
https://issues.apache.org/jira/browse/INFRA-12590 and ask for this
change.

FYI I don't any of the committers have access to the appveyor account
which is at https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark
 . To request changes that need to be done in the UI we need to open a
INFRA ticket.

Thanks
Shivaram

ts
veyor.yml
aster
ing-the-branch-for-building-and-setting-the-build-schedule
est
/5

---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 10 Sep 2016 00:50:42 +0900","Re: Change the settings in AppVeyor to prevent triggering the tests
 in other PRs in other branches",shivaram@eecs.berkeley.edu,"Ah, thanks! I wasn't too sure on this so I thought asking here somehow
reaches out to who's in charge of the account :).


k
 master
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 9 Sep 2016 08:52:14 -0700","Re: Change the settings in AppVeyor to prevent triggering the tests
 in other PRs in other branches",Hyukjin Kwon <gurwls223@gmail.com>,"branch-1.6 -- Do you think that will fix the problem ?

rk
:
o master
ifying-the-branch-for-building-and-setting-the-build-schedule
e

---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 10 Sep 2016 00:59:47 +0900","Re: Change the settings in AppVeyor to prevent triggering the tests
 in other PRs in other branches",shivaram@eecs.berkeley.edu,"Yes, if we don't have any PRs to other branches on branch-1.5 and lower
versions, I think it'd be fine.

checked it passes on branch-2.0 before).

I can try to check if it passes and identify the related causes if it does
not pass.


e
e
st
 to master
er
"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 10 Sep 2016 01:33:37 +0900","Re: Change the settings in AppVeyor to prevent triggering the tests
 in other PRs in other branches",shivaram@eecs.berkeley.edu,"FYI, I just ran the SparkR tests on Windows for branch-2.0 and 1.6.

branch-2.0 - https://github.com/spark-test/spark/pull/7
branch-1.6 - https://github.com/spark-test/spark/pull/8




2016-09-10 0:59 GMT+09:00 Hyukjin Kwon <gurwls223@gmail.com>:

s
:
a
me
ve
t to
"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Fri, 9 Sep 2016 10:10:43 -0700",Re: Unable to run docker jdbc integrations test ?,Luciano Resende <luckbr1975@gmail.com>,"I agree with Josh. These tests are valuable , even if  then  can not be run on Jenkins due to setup issues. It will be good to run them atleast manually, when jdbc data source specific changes are made . Filed Jira for this problem. 

https://issues.apache.org/jira/browse/SPARK-17473



but if we resolve these issues, we should make it part of some frequent build to make sure the build don't regress and that the actual functionality don't regress either. Let me look into this again...
possible, though, we should try to get rid of our dependency on the Spotify docker-client library, since it's a dependency hell nightmare. Given our relatively simple use of Docker here, I wonder whether we could just write some simple scripting over the `docker` command-line tool instead of pulling in such a problematic library.
dependency upgrades in Spark 2.0 this has stopped working. I have tried to bring up this but I am having some issues with getting the right dependencies loaded and satisfying the docker-client expectations. 
tests available ? Then we can focus on bringing them up and I can go push my previous experiments as a WIP PR. Otherwise we should just get rid of these tests.
integration tests on my laptop.   Any ideas , what I might be be doing wrong ?
-Phive-thriftserver -Phive -DskipTests clean install
:spark-docker-integration-tests_2.11  compile test
MaxPermSize=512m; support was removed in 8.0
BlockManagerId(driver, 9.31.117.25, 51868)
org.glassfish.jersey.model.internal.CommonConfig.configureAutoDiscoverableProviders(CommonConfig.java:622)
org.glassfish.jersey.client.ClientConfig$State.configureAutoDiscoverableProviders(ClientConfig.java:357)
org.glassfish.jersey.client.ClientConfig$State.initRuntime(ClientConfig.java:392)
org.glassfish.jersey.client.ClientConfig$State.access$000(ClientConfig.java:88)
org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:120)
org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:117)
org.glassfish.jersey.internal.util.collection.Values$LazyValueImpl.get(Values.java:340)
org.glassfish.jersey.client.ClientConfig.getRuntime(ClientConfig.java:726)
org.glassfish.jersey.client.ClientRequest.getConfiguration(ClientRequest.java:285)
org.glassfish.jersey.client.JerseyInvocation.validateHttpMethodAndEntity(JerseyInvocation.java:126)
hook
MapOutputTrackerMasterEndpoint stopped!
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 9 Sep 2016 10:56:09 -0700","Re: Change the settings in AppVeyor to prevent triggering the tests
 in other PRs in other branches",Hyukjin Kwon <gurwls223@gmail.com>,"The infra ticket has been updated so I'd say let's stick to running tests
on master branch. We can of course create JIRAs for tests that fail in
branch 2.0 and 1.6

Shivaram


w
t
 a
ave
st to
r
-
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 9 Sep 2016 19:25:45 +0000",scalable-deeplearning 1.0.0 released,"""dev@spark.apache.org"" <dev@spark.apache.org>, User
	<user@spark.apache.org>","Dear Spark users and developers,

I have released version 1.0.0 of scalable-deeplearning package. This package is based on the implementation of artificial neural networks in Spark ML. It is intended for new Spark deep learning features that were not yet merged to Spark ML or that are too specific to be merged. The package provides ML pipeline API, distributed training, optimized numerical processing with tensor library, and extensible API for developers. Current features are the multilayer perceptron classifier and stacked autoencoder.

As a Spark package: https://spark-packages.org/package/avulanov/scalable-deeplearning

The source code: https://github.com/avulanov/scalable-deeplearning

Contributions are very welcome! Please, let me know if you have any comment or questions.

Best regards, Alexander
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sun, 11 Sep 2016 22:49:18 -0700 (MST)",Test fails when compiling spark with tests,dev@spark.apache.org,"Hi,
I am trying to set up a spark development environment. I forked the spark git project and cloned the fork. I then checked out branch-2.0 tag (which I assume is the released source code).
I then compiled spark twice.
The first using:
mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -DskipTests clean package
This compiled successfully.
The second using mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 clean package
This got a failure in Spark Project Core with the following test failing:
- caching in memory and disk, replicated
- caching in memory and disk, serialized, replicated *** FAILED ***
  java.util.concurrent.TimeoutException: Can't find 2 executors before 30000 milliseconds elapsed
  at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:573)
  at org.apache.spark.DistributedSuite.org$apache$spark$DistributedSuite$$testCaching(DistributedSuite.scala:154)
  at org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply$mcV$sp(DistributedSuite.scala:191)
  at org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply(DistributedSuite.scala:191)
  at org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply(DistributedSuite.scala:191)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  at org.scalatest.Transformer.apply(Transformer.scala:20)
  ...
- compute without caching when no partitions fit in memory

I made no changes to the code whatsoever. Can anyone help me figure out what is wrong with my environment?
BTW I am using maven 3.3.9 and java 1.8.0_101-b13

Thanks,
                Assaf




--"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Mon, 12 Sep 2016 03:43:32 -0700 (MST)",UDF and native functions performance,dev@spark.apache.org,"I am trying to create UDFs with improved performance. So I decided to compare several ways of doing it.
In general I created a dataframe using range with 50M elements, cached it and counted it to manifest it.

I then implemented a simple predicate (x<10) in 4 different ways, counted the elements and timed it.
The 4 ways were:

-          Standard expression (took 90 millisonds)

-          Udf  (took 539 miliseconds)

-          Codegen (took 358 miliseconds)

-          Dataset filter (took 1022 miliseconds)

I understand why filter is so much slower. I also understand why UDF is slower (with volcano model taking up processing time).
I do not understand why the codegen I created is so slow. What am I missing?

The code to generate the numbers is followed:

import org.apache.spark.sql.codegenFuncs._
val df = spark.range(50000000).withColumnRenamed(""id"",""smaller"")
df.cache().count()

val base_filter_df = df.filter(df(""smaller"") < 10)

import org.apache.spark.sql.functions.udf
def asUdf=udf((x: Int) => x < 10)
val udf_filter_df = df.filter(asUdf(df(""smaller"")))

val my_func = df.filter(genf_func(df(""smaller"")))

case class tmpclass(smaller: BigInt)

val simpleFilter = df.as[tmpclass].filter((x: tmpclass) => (x.smaller < 10))

def time[R](block: => R) = {
    val t0 = System.nanoTime()
    val result = block    // call-by-name
    val t1 = System.nanoTime()
    (t1 - t0)/1000000
}

def avgTime[R](block: => R) = {
    val times = for (i <- 1 to 5) yield time(block)
    times.sum / 5
}


println(""base "" + avgTime(base_filter_df.count()))
//>> got a result of 90
println(""udf "" + avgTime(udf_filter_df.count()))
//>> got a result of 539
println(""codegen "" + avgTime(my_func.count()))
//>> got a result of 358
println(""filter "" + avgTime(simpleFilter.count()))
//>> got a result of 1022

And the code for the genf_func:

package org.apache.spark.sql

import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}
import org.apache.spark.sql.types._
import org.apache.spark.sql.catalyst.expressions._

object codegenFuncs {
  case class genf(child: Expression) extends UnaryExpression with Predicate with ImplicitCastInputTypes {

    override def inputTypes: Seq[AbstractDataType] = Seq(IntegerType)

    override def toString: String = s""$child < 10""

    override def eval(input: InternalRow): Any = {
      val value = child.eval(input)
      if (value == null)
      {
        false
      } else {
        child.dataType match {
          case IntegerType => value.asInstanceOf[Int] < 10
        }
      }
    }

    override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
      defineCodeGen(ctx, ev, c => s""($c) < 10"")
    }
  }

  private def withExpr(expr: Expression): Column = Column(expr)

  def genf_func(v: Column): Column = withExpr { genf(v.expr) }
}






--"
whitefalcon <aron_macdonald@hotmail.com>,"Mon, 12 Sep 2016 06:16:35 -0700 (MST)",Re: Fwd: HANA data access from SPARK,dev@spark.apache.org,"Hi Dushyant,

I saw this same error with an older Hana JDBC driver, but the error went
away when I tried a later ngdbc.jar driver file  (dated May 2016). I've not
tried to 

Heres an example I did using the later driver with Spark 1.6.2 running
standalone.
http://scn.sap.com/community/hana-in-memory/blog/2016/09/09/calling-hana-views-from-apache-spark

Regards
Aron



--

---------------------------------------------------------------------


"
Nasser Ebrahim <enasser@linux.vnet.ibm.com>,"Mon, 12 Sep 2016 19:30:40 +0530",GZIP compression support for Spark internal data,dev@spark.apache.org,"Hi,

Can we use GZIP compression for internal data such as RDD partitions, 
broadcast variables and shuffle outputs so that user will have more 
choice compared to the available LZ4, LZF and Snappy?  Is there any 
specific reason we are not supporting the JDK inbuilt compression? If 
not, shall I create a JIRA to get this implemented.

Thank you,
Nasser Ebrahim

"
Jacek Laskowski <jacek@japila.pl>,"Mon, 12 Sep 2016 16:23:48 +0200",Why CatalogImpl.makeDataset and SparkSession.createDataset?,dev <dev@spark.apache.org>,"Hi,

I've stumbled upon CatalogImpl.makeDataset [1] -- the only
private[sql] method in the CatalogImpl object -- that looks like
SparkSession.createDataset [2].

What do you think about removing CatalogImpl.makeDataset? If not,
what's so special about one over the other to keep them both?

I'd appreciate your help with this. Thanks.

[1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala#L385

[2] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L413

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 13 Sep 2016 00:49:42 +0900",Re: GZIP compression support for Spark internal data,Nasser Ebrahim <enasser@linux.vnet.ibm.com>,"Hi,

Have you seen https://issues.apache.org/jira/browse/SPARK-4633 ?

// maropu




-- 
---
Takeshi Yamamuro
"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 12 Sep 2016 16:09:29 +0000",Re: Organizing Spark ML example packages,"""dev@spark.apache.org"" <dev@spark.apache.org>","Never actually got around to doing this - do folks still think it
worthwhile?


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 13 Sep 2016 01:12:23 +0900",Re: UDF and native functions performance,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Hi,

I think you'd better off comparing the gen'd code of `df.filter` and your
gen'd code
by using .debugCodegen().

// maropu





-- 
---
Takeshi Yamamuro
"
Stephen Boesch <javadba@gmail.com>,"Mon, 12 Sep 2016 09:13:38 -0700",Re: Organizing Spark ML example packages,Nick Pentreath <nick.pentreath@gmail.com>,"Yes: will you have cycles to do it?

2016-09-12 9:09 GMT-07:00 Nick Pentreath <nick.pentreath@gmail.com>:

"
shane knapp <sknapp@berkeley.edu>,"Mon, 12 Sep 2016 09:31:25 -0700",[build system] brief jenkins downtime this morning,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","our weekly backups failed due to a hung job.   even though i tried to
change the backup scheduler (internal to jenkins) to run tonite, it's
still insisting that it needs to run immediately and is continually
putting jenkins in to quiet mode.

short of killing all of the current jobs and restarting jenkins, we'll
let the backups run later this morning and things should return to
normal by lunchtime.

i'll keep an eye on this and make sure things move along properly.

sorry about the glitch!  :(

shane

---------------------------------------------------------------------


"
"""Mendelson, Assaf"" <Assaf.Mendelson@rsa.com>","Mon, 12 Sep 2016 17:48:15 +0000",RE: UDF and native functions performance,"""dev@spark.apache.org"" <dev@spark.apache.org>","I did, they look the same:

scala> my_func.explain(true)
== Parsed Logical Plan ==
Filter smaller#3L < 10
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Analyzed Logical Plan ==
smaller: bigint
Filter smaller#3L < 10
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Optimized Logical Plan ==
Filter smaller#3L < 10
+- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :  +- *Project [id#0L AS smaller#3L]
   :     +- *Range (0, 500000000, splits=1)

== Physical Plan ==
*Filter smaller#3L < 10
+- InMemoryTableScan [smaller#3L], [smaller#3L < 10]
   :  +- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :     :  +- *Project [id#0L AS smaller#3L]
   :     :     +- *Range (0, 500000000, splits=1)

scala> base_filter_df.explain(true)
== Parsed Logical Plan ==
'Filter (smaller#3L < 10)
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Analyzed Logical Plan ==
smaller: bigint
Filter (smaller#3L < cast(10 as bigint))
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Optimized Logical Plan ==
Filter (smaller#3L < 10)
+- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :  +- *Project [id#0L AS smaller#3L]
   :     +- *Range (0, 500000000, splits=1)

== Physical Plan ==
*Filter (smaller#3L < 10)
+- InMemoryTableScan [smaller#3L], [(smaller#3L < 10)]
   :  +- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :     :  +- *Project [id#0L AS smaller#3L]
   :     :     +- *Range (0, 500000000, splits=1)


Also when I do:

import org.apache.spark.sql.execution.debug._
df.debugCodegen

on both of them they are identical.
I did notice that if I change the code to do > instead of < then they have almost the same performance so I imagine this has something to do with some optimization that understands that range is ordered and therefore once the first condition fails, all would fail.
The problem is I don‚Äôt see this in the plan, nor can I find it in the code.


From: Takeshi Yamamuro [mailto:linguin.m.s@gmail.com]
Sent: Monday, September 12, 2016 7:12 PM
To: Mendelson, Assaf
Cc: dev@spark.apache.org
Subject: Re: UDF and native functions performance

Hi,

I think you'd better off comparing the gen'd code of `df.filter` and your gen'd code
by using .debugCodegen().

// maropu

On Mon, Sep 12, 2016 at 7:43 PM, assaf.mendelson <assaf.mendelson@rsa.com<mailto:assaf.mendelson@rsa.com>> wrote:
I am trying to create UDFs with improved performance. So I decided to compare several ways of doing it.
In general I created a dataframe using range with 50M elements, cached it and counted it to manifest it.

I then implemented a simple predicate (x<10) in 4 different ways, counted the elements and timed it.
The 4 ways were:

-          Standard expression (took 90 millisonds)

-          Udf  (took 539 miliseconds)

-          Codegen (took 358 miliseconds)

-          Dataset filter (took 1022 miliseconds)

I understand why filter is so much slower. I also understand why UDF is slower (with volcano model taking up processing time).
I do not understand why the codegen I created is so slow. What am I missing?

The code to generate the numbers is followed:

import org.apache.spark.sql.codegenFuncs._
val df = spark.range(50000000).withColumnRenamed(""id"",""smaller"")
df.cache().count()

val base_filter_df = df.filter(df(""smaller"") < 10)

import org.apache.spark.sql.functions.udf
def asUdf=udf((x: Int) => x < 10)
val udf_filter_df = df.filter(asUdf(df(""smaller"")))

val my_func = df.filter(genf_func(df(""smaller"")))

case class tmpclass(smaller: BigInt)

val simpleFilter = df.as<http://df.as>[tmpclass].filter((x: tmpclass) => (x.smaller < 10))

def time[R](block: => R) = {
    val t0 = System.nanoTime()
    val result = block    // call-by-name
    val t1 = System.nanoTime()
    (t1 - t0)/1000000
}

def avgTime[R](block: => R) = {
    val times = for (i <- 1 to 5) yield time(block)
    times.sum / 5
}


println(""base "" + avgTime(base_filter_df.count()))
//>> got a result of 90
println(""udf "" + avgTime(udf_filter_df.count()))
//>> got a result of 539
println(""codegen "" + avgTime(my_func.count()))
//>> got a result of 358
println(""filter "" + avgTime(simpleFilter.count()))
//>> got a result of 1022

And the code for the genf_func:

package org.apache.spark.sql

import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}
import org.apache.spark.sql.types._
import org.apache.spark.sql.catalyst.expressions._

object codegenFuncs {
  case class genf(child: Expression) extends UnaryExpression with Predicate with ImplicitCastInputTypes {

    override def inputTypes: Seq[AbstractDataType] = Seq(IntegerType)

    override def toString: String = s""$child < 10""

    override def eval(input: InternalRow): Any = {
      val value = child.eval(input)
      if (value == null)
      {
        false
      } else {
        child.dataType match {
          case IntegerType => value.asInstanceOf[Int] < 10
        }
      }
    }

    override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
      defineCodeGen(ctx, ev, c => s""($c) < 10"")
    }
  }

  private def withExpr(expr: Expression): Column = Column(expr)

  def genf_func(v: Column): Column = withExpr { genf(v.expr) }
}



________________________________
View this message in context: UDF and native functions performance<http://apache-spark-developers-list.1001551.n3.nabble.com/UDF-and-native-functions-performance-tp18920.html>
Sent from the Apache Spark Developers List mailing list archive<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.



--
---
Takeshi Yamamuro
"
shane knapp <sknapp@berkeley.edu>,"Mon, 12 Sep 2016 11:05:18 -0700",Re: [build system] brief jenkins downtime this morning,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","the backup is done and we're building again!


---------------------------------------------------------------------


"
Nasser Ebrahim <enasser@linux.vnet.ibm.com>,"Tue, 13 Sep 2016 00:35:24 +0530",Re: GZIP compression support for Spark internal data,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thank you Takeshi for sharing the info. I agree with Patrick and you 
that there is no point in adding more codec unless it is showing better 
performance results (at least with some work loads on some platforms). 
The performance of GZIP depends upon its implementation on the 
platforms. Will do some performance tests to see how it is performing 
compared to the existing codec in spark.



"
Reynold Xin <rxin@databricks.com>,"Tue, 13 Sep 2016 09:36:19 +0900",Re: UDF and native functions performance,"""Mendelson, Assaf"" <Assaf.Mendelson@rsa.com>","Not sure if this is why but perhaps the constraint framework?


e
me
e
 the code.
 <
t,
redicate
e)
-functions-performance-tp18920.html>
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Mon, 12 Sep 2016 21:01:01 -0700 (MST)",RE: UDF and native functions performance,dev@spark.apache.org,"What is the constraint framework?
How would I add the same optimization to the sample function I created?


From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n18932h81@n3.nabble.com]
Sent: Tuesday, September 13, 2016 3:37 AM
To: Mendelson, Assaf
Subject: Re: UDF and native functions performance

Not sure if this is why but perhaps the constraint framework?

I did, they look the same:

scala> my_func.explain(true)
== Parsed Logical Plan ==
Filter smaller#3L < 10
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Analyzed Logical Plan ==
smaller: bigint
Filter smaller#3L < 10
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Optimized Logical Plan ==
Filter smaller#3L < 10
+- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :  +- *Project [id#0L AS smaller#3L]
   :     +- *Range (0, 500000000, splits=1)

== Physical Plan ==
*Filter smaller#3L < 10
+- InMemoryTableScan [smaller#3L], [smaller#3L < 10]
   :  +- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :     :  +- *Project [id#0L AS smaller#3L]
   :     :     +- *Range (0, 500000000, splits=1)

scala> base_filter_df.explain(true)
== Parsed Logical Plan ==
'Filter (smaller#3L < 10)
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Analyzed Logical Plan ==
smaller: bigint
Filter (smaller#3L < cast(10 as bigint))
+- Project [id#0L AS smaller#3L]
   +- Range (0, 500000000, splits=1)

== Optimized Logical Plan ==
Filter (smaller#3L < 10)
+- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :  +- *Project [id#0L AS smaller#3L]
   :     +- *Range (0, 500000000, splits=1)

== Physical Plan ==
*Filter (smaller#3L < 10)
+- InMemoryTableScan [smaller#3L], [(smaller#3L < 10)]
   :  +- InMemoryRelation [smaller#3L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
   :     :  +- *Project [id#0L AS smaller#3L]
   :     :     +- *Range (0, 500000000, splits=1)


Also when I do:

import org.apache.spark.sql.execution.debug._
df.debugCodegen

on both of them they are identical.
I did notice that if I change the code to do > instead of < then they have almost the same performance so I imagine this has something to do with some optimization that understands that range is ordered and therefore once the first condition fails, all would fail.
The problem is I don‚Äôt see this in the plan, nor can I find it in the code.


From: Takeshi Yamamuro [mailto:<a href=""javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;linguin.m.s@gmail.com&#39;);"" target=""_blank"">linguin.m.s@<mailto:linguin.m.s@>...]
Sent: Monday, September 12, 2016 7:12 PM
To: Mendelson, Assaf
Cc: <a href=""javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;dev@spark.apache.org&#39;);"" target=""_blank"">dev@...
Subject: Re: UDF and native functions performance

Hi,

I think you'd better off comparing the gen'd code of `df.filter` and your gen'd code
by using .debugCodegen().

// maropu

%7B%7D,&#39;cvml&#39;,&#39;assaf.mendelson@rsa.com&#39;);"" target=""_blankI am trying to create UDFs with improved performance. So I decided to compare several ways of doing it.
In general I created a dataframe using range with 50M elements, cached it and counted it to manifest it.

I then implemented a simple predicate (x<10) in 4 different ways, counted the elements and timed it.
The 4 ways were:

-          Standard expression (took 90 millisonds)

-          Udf  (took 539 miliseconds)

-          Codegen (took 358 miliseconds)

-          Dataset filter (took 1022 miliseconds)

I understand why filter is so much slower. I also understand why UDF is slower (with volcano model taking up processing time).
I do not understand why the codegen I created is so slow. What am I missing?

The code to generate the numbers is followed:

import org.apache.spark.sql.codegenFuncs._
val df = spark.range(50000000).withColumnRenamed(""id"",""smaller"")
df.cache().count()

val base_filter_df = df.filter(df(""smaller"") < 10)

import org.apache.spark.sql.functions.udf
def asUdf=udf((x: Int) => x < 10)
val udf_filter_df = df.filter(asUdf(df(""smaller"")))

val my_func = df.filter(genf_func(df(""smaller"")))

case class tmpclass(smaller: BigInt)

val simpleFilter = df.as<http://df.as>[tmpclass].filter((x: tmpclass) =
def time[R](block: => R) = {
    val t0 = System.nanoTime()
    val result = block    // call-by-name
    val t1 = System.nanoTime()
    (t1 - t0)/1000000
}

def avgTime[R](block: => R) = {
    val times = for (i <- 1 to 5) yield time(block)
    times.sum / 5
}


println(""base "" + avgTime(base_filter_df.count()))
//>> got a result of 90
println(""udf "" + avgTime(udf_filter_df.count()))
//>> got a result of 539
println(""codegen "" + avgTime(my_func.count()))
//>> got a result of 358
println(""filter "" + avgTime(simpleFilter.count()))
//>> got a result of 1022

And the code for the genf_func:

package org.apache.spark.sql

import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, ExprCode}
import org.apache.spark.sql.types._
import org.apache.spark.sql.catalyst.expressions._

object codegenFuncs {
  case class genf(child: Expression) extends UnaryExpression with Predicate with ImplicitCastInputTypes {

    override def inputTypes: Seq[AbstractDataType] = Seq(IntegerType)

    override def toString: String = s""$child < 10""

    override def eval(input: InternalRow): Any = {
      val value = child.eval(input)
      if (value == null)
      {
        false
      } else {
        child.dataType match {
          case IntegerType => value.asInstanceOf[Int] < 10
        }
      }
    }

    override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
      defineCodeGen(ctx, ev, c => s""($c) < 10"")
    }
  }

  private def withExpr(expr: Expression): Column = Column(expr)

  def genf_func(v: Column): Column = withExpr { genf(v.expr) }
}



________________________________
View this message in context: UDF and native functions performance<http://apache-spark-developers-list.1001551.n3.nabble.com/UDF-and-native-functions-performance-tp18920.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.



--
---
Takeshi Yamamuro

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/UDF-and-native-functions-performance-tp18920p18932.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/UDF-and-native-functions-performance-tp18920p18933.html
om."
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Tue, 13 Sep 2016 15:08:47 +0000",Spark SQL - Applying transformation on a struct inside an array,"user <user@spark.apache.org>, dev <dev@spark.apache.org>","Hi everyone,I'm currently trying to create a generic transformation mecanism on
a Dataframe to modify an arbitrary column regardless of the underlying the
schema.
It's ""relatively"" straightforward for complex types like struct<struct<‚Ä¶>> to
apply an arbitrary UDF on the column and replace the data ""inside"" the struct,
however I'm struggling to make it work for complex types containing arrays along
the way like struct<array<struct<‚Ä¶>>>.
Michael Armbrust seemed to allude on the mailing list/forum to a way of using
Encoders to do that, I'd be interested in any pointers, especially considering
that it's not possible to output any Row or GenericRowWithSchema from a UDF
(thanks to
https://github.com/apache/spark/blob/v2.0.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L657
it seems).
To sum up, I'd like to find a way to apply a transformation on complex nested
datatypes (arrays and struct) on a Dataframe updating the value itself.
Regards,
Olivier Girardot"
Danil Kirsanov <danil.kirsanov@gmail.com>,"Tue, 13 Sep 2016 16:45:59 -0700 (MST)",Nominal Attribute,dev@spark.apache.org,"NominalAttribute in MLib is used to represent categorical data internally. 
It is barely documented though and has a number of limitations: for example,
it supports only integer and string data. 
Is there any current effort to expose it (and categorical data handling in
general) to the users, or is it intended to be an internal MLib data
representation only?

Thank you,
Danil



--

---------------------------------------------------------------------


"
Chan Chor Pang <chin-sh@indetail.co.jp>,"Wed, 14 Sep 2016 10:13:57 +0900",REST api for monitoring Spark Streaming,dev@spark.apache.org,"Hi everyone,

Trying to monitoring our streaming application using Spark REST interface
only to found that there is no such thing for Streaming.

I wonder if anyone already working on this or I should just start 
implementing my own one?

-- 
BR
Peter Chan


---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Tue, 13 Sep 2016 18:29:39 -0700",Re: Test fails when compiling spark with tests,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","There are some flaky tests that occasionally fail, my first
recommendation would be to re-run the test suite. Another thing to
check is if there are any applications listening to spark's default
ports.
Btw, what is your environment like? In case it is windows, I don't
think tests are regularly run against that platform and therefore
could very well be broken.


---------------------------------------------------------------------


"
sririshindra <sririshindra@gmail.com>,"Wed, 14 Sep 2016 02:57:40 -0700 (MST)","sqlContext.registerDataFrameAsTable is not working properly in
 pyspark 2.0",dev@spark.apache.org,"Hi,

I have a production job that is registering four different dataframes as
tables in pyspark 1.6.2 . when we upgraded to spark 2.0 only three of the
four dataframes are getting registered. the fourth dataframe is not getting
registered. There are no code changes whatsoever. The only change is the
spark verion. When I revert the spark version to 1.6.2 the dataframe is
getting registered properly.  Did anyone face a similar issue? Is this a bug
in spark 2.0 or is it just a compatibility issue?



--

---------------------------------------------------------------------


"
Daniel Lopes <daniel@onematch.com.br>,"Wed, 14 Sep 2016 11:35:56 -0300",Re: Subscription,Omkar Reddy <omkarreddy2008@gmail.com>,"Hi Omkar,

Look at this link http://spark.apache.org/community.html to subscribe to
the right list.

Best,

*Daniel Lopes*
c: +55 (18) 99764-2733 | https://www.linkedin.com/in/dslopes

www.onematch.com.br
<http://www.onematch.com.br/?utm_source=EmailSignature&utm_term=daniel-lopes>


"
Tao Li <tli@hortonworks.com>,"Wed, 14 Sep 2016 17:42:03 +0000",Question about impersonation on Spark executor,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am new to Spark and would like to have a quick question about the end user impersonation on Spark executor process.

Basically I am running SQL queries through Spark thrift server with doAs set to true to enable end user impersonation. In my experiment, I was able to start session for multiple end users at the same time and all queries look fine. For example, user A can query table 1, which is accessible to A exclusively (according to HDFS permission). At the same time, user B can query table 2, which is accessible to B exclusively. Looks like the end user UGI has been flowed to the executor process successfully. I checked SparkContext code and looks like the end user info is flowed to executor by specifying ‚ÄúSPARK_USER‚Äù env variable. Correct me if I am wrong.

I only see 1 executor process running for all the queries from multiple users in my experiment. The question is why the single process can impersonate multiple end users at the same time. I assume the value of ‚ÄúSPARK_USER‚Äù env variable should be either user A or B in the executor. Then there has to be HDFS permission errors for the other user. But I did not see any error for any user.

Can someone give some insights on that question? Thanks so much.
"
Fred Reiss <freiss.oss@gmail.com>,"Wed, 14 Sep 2016 10:44:57 -0700",Re: Spark SQL - Applying transformation on a struct inside an array,Olivier Girardot <o.girardot@lateral-thoughts.com>,"+1 to this request. I talked last week with a product group within IBM that
is struggling with the same issue. It's pretty common in data cleaning
applications for data in the early stages to have nested lists or sets
inconsistent or incomplete schema inf"
Fred Reiss <freiss.oss@gmail.com>,"Wed, 14 Sep 2016 10:47:45 -0700",Re: Test fails when compiling spark with tests,Jakob Odersky <jakob@odersky.com>,"Also try doing a fresh clone of the git repository. I've seen some of those
rare failure modes corrupt parts of my local copy in the past.

FWIW the main branch as of yesterday afternoon is building fine in my
environment.

Fred


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Wed, 14 Sep 2016 19:33:38 +0000",Not all KafkaReceivers processing the data Why?,"""'user@spark.apache.org'"" <user@spark.apache.org>,
	""'dev@spark.apache.org'"" <dev@spark.apache.org>","Hello all,

I have created a Kafka topic with 5 partitions.  And I am using createStream receiver API like following.   But somehow only one receiver is getting the input data. Rest of receivers are not processign anything.  Can you please help?

JavaPairDStream<String, String> messages = null;

            if(sparkStreamCount > 0){
                // We create an input DStream for each partition of the topic, unify those streams, and then repartition the unified stream.
                List<JavaPairDStream<String, String>> kafkaStreams = new ArrayList<JavaPairDStream<String, String>>(sparkStreamCount);
                for (int i = 0; i < sparkStreamCount; i++) {
                                kafkaStreams.add( KafkaUtils.createStream(jssc, contextVal.getString(KAFKA_ZOOKEEPER), contextVal.getString(KAFKA_GROUP_ID), kafkaTopicMap));
                }
                messages = jssc.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));
            }
            else{
                messages =  KafkaUtils.createStream(jssc, contextVal.getString(KAFKA_ZOOKEEPER), contextVal.getString(KAFKA_GROUP_ID), kafkaTopicMap);
            }



[cid:image001.png@01D20E84.3558F520]




"
Reynold Xin <rxin@databricks.com>,"Wed, 14 Sep 2016 12:44:20 -0700",Re: Saving less data to improve Pregel performance in GraphX?,Fang Zhang <fang.zhang.dx@gmail.com>,"This is definitely useful, but in reality it might be very difficult to do.



"
Jeremy Smith <jeremy.smith@acorns.com>,"Wed, 14 Sep 2016 12:59:24 -0700",Re: Not all KafkaReceivers processing the data Why?,Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Take a look at how the messages are actually distributed across the
partitions. If the message keys have a low cardinality, you might get poor
distribution (i.e. all the messages are actually only in two of the five
partitions, leading to what you see in Spark).

If you take a look at the Kafka data directories, you can probably get an
idea of the distribution by just examining the sizes of each partition.

Jeremy


"
Akshay Sachdeva <akshay.sachdeva@gmail.com>,"Wed, 14 Sep 2016 15:58:30 -0700 (MST)",CSV Reader with row numbers,dev@spark.apache.org,"Environment:
Apache Spark 1.6.2
Scala: 2.10

I am currently using the spark-csv package courtesy of databricks and I
would like to have a (pre processing ?) stage when reading the CSV file that
also adds a row number to each row of data being read from the csv file. 
This will allow for better traceability and data lineage in case of
validation or data processing issues downstream.

In doing the research it seems like the zipWithIndex API is the right or
only way to get this pattern implemented.

Would this be the preferred route?  Would this be safe for parallel
operations as far as respect no collisions?  Any body have a similar
requirement and have a better solution you can point me to.

Appreciate any help and responses anyone can offer.

Thanks
-a



--

---------------------------------------------------------------------


"
"""Mario Ds Briggs"" <mario.briggs@in.ibm.com>","Thu, 15 Sep 2016 22:51:34 +0530",Compatibility of 1.6 spark.eventLog with a 2.0 History Server,dev@spark.apache.org,"

Hi,

I would like to use a Spark 2.0 History Server instance on spark1.6
generated eventlogs. (This is because clicking the refresh button in
browser, updates the UI with latest events, where-as in the 1.6 code base,
this does not happen)

My question is whether this is safe to do and are they any known issues
where event log contents of 1.6 are incompatible with a 2.0 history server.
Any official tracking of this ( i do know that the public SparkListener
interface didnt change across these versions)


thanks
Mario
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 15 Sep 2016 10:23:22 -0700",Re: Compatibility of 1.6 spark.eventLog with a 2.0 History Server,Mario Ds Briggs <mario.briggs@in.ibm.com>,"It should work fine. 2.0 dropped support for really old event logs
(pre-Spark 1.3 I think), but 1.6 should work, and if it doesn't it
should be considered a bug.




-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Sep 2016 10:29:06 -0700",Re: Compatibility of 1.6 spark.eventLog with a 2.0 History Server,Mario Ds Briggs <mario.briggs@in.ibm.com>,"They should be compatible.



"
Ryan Williams <ryan.blake.williams@gmail.com>,"Thu, 15 Sep 2016 17:49:35 +0000",Re: Compatibility of 1.6 spark.eventLog with a 2.0 History Server,"Reynold Xin <rxin@databricks.com>, Mario Ds Briggs <mario.briggs@in.ibm.com>","What is meant by:

""""""
(This is because clicking the refresh button in browser, updates the UI
with latest events, where-as in the 1.6 code base, this does not happen)
""""""

Hasn't refreshing the page updated all the information in the UI through
the 1.x line?
"
"""Mario Ds Briggs"" <mario.briggs@in.ibm.com>","Thu, 15 Sep 2016 23:38:29 +0530",Re: Compatibility of 1.6 spark.eventLog with a 2.0 History Server,Ryan Williams <ryan.blake.williams@gmail.com>,"
I had checked in 1.6.2 and it doesnt. I didnt check in lower versions. The
history server logs do show a 'Replaying log path: file:xxx.inprogress'
when the file is changed , but a refresh on UI doesnt show the new
jobs/stages/tasks whatever



thanks
Mario



From:	Ryan Williams <ryan.blake.williams@gmail.com>
To:	Reynold Xin <rxin@databricks.com>, Mario Ds
            Briggs/India/IBM@IBMIN
Cc:	""dev@spark.apache.org"" <dev@spark.apache.org>
Date:	15/09/2016 11:19 pm
Subject:	Re: Compatibility of 1.6 spark.eventLog with a 2.0 History
            Server



What is meant by:

""""""
(This is because clicking the refresh button in browser, updates the UI
with latest events, where-as in the 1.6 code base, this does not happen)
""""""

Hasn't refreshing the page updated all the information in the UI through
the 1.x line?

"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Thu, 15 Sep 2016 18:15:25 -0700 (MST)",Why we get 0 when the key is null?,dev@spark.apache.org,"this func is in Partitioner
  def getPartition(key: Any): Int = key match {
    case null => 0
//    case None => 0
    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)
  }



--

---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Thu, 15 Sep 2016 18:17:52 -0700 (MST)",What's the use of RangePartitioner.hashCode,dev@spark.apache.org,"who can give me an example of the use of RangePartitioner.hashCode, thank
you!



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Sep 2016 18:18:35 -0700",Re: Why we get 0 when the key is null?,WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"What else do you expect to get? A non-zero hash value?

It can technically be any constant.



"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Thu, 15 Sep 2016 18:26:24 -0700 (MST)",Re: Why we get 0 when the key is null?,dev@spark.apache.org,"When the key is not In the rdd, I can also get an value , I just feel a
little strange.



--

---------------------------------------------------------------------


"
Kelvin Chu <2dot7kelvin@gmail.com>,"Thu, 15 Sep 2016 19:07:55 -0700",Re: Compatibility of 1.6 spark.eventLog with a 2.0 History Server,Mario Ds Briggs <mario.briggs@in.ibm.com>,"We tried it and it works that v2.0 History Server can read the v1.6 logs.

Note that the UI has a regression. When there are too many jobs, the UI
will freeze because the new code tries to cache everything. We submitted a
JIRA: https://issues.apache.org/jira/browse/SPARK-17243


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Thu, 15 Sep 2016 23:54:58 -0700 (MST)",What's the meaning when the partitions is zero?,dev@spark.apache.org,"class HashPartitioner(partitions: Int) extends Partitioner {
  require(partitions >= 0, s""Number of partitions ($partitions) cannot be
negative."")

the soruce code require(partitions >=0) ,but I don't know why it makes sense
when the partitions is 0.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 16 Sep 2016 08:44:10 +0100",Re: What's the meaning when the partitions is zero?,WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"There are almost no cases in which you'd want a zero-partition RDD.
The only one I can think of is an empty RDD, where the number of
partitions is irrelevant. Still, I would not be surprised if other
parts of the code assume at least 1 partition.

Maybe this check could be tightened. It would be interesting to see if
the tests catch any scenario where a 0-partition RDD is created, and
why.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 16 Sep 2016 08:44:46 +0100",Re: Why we get 0 when the key is null?,WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"""null"" is a valid value in an RDD, so it has to be partition-able.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 16 Sep 2016 06:51:44 -0700",Re: What's the meaning when the partitions is zero?,Sean Owen <sowen@cloudera.com>,"They are valid, especially in partition pruning.


"
Ewan Leith <ewan.leith@realitymine.com>,"Fri, 16 Sep 2016 18:16:32 +0000",Spark 2.0.1 release?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Apologies if I've missed anything, but is there likely to see a 2.0.1 bug fix release, or does a jump to 2.1.0 with additional features seem more probable?

The issues for 2.0.1 seem pretty much done here https://issues.apache.org/jira/browse/SPARK/fixforversion/12336857/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-issues-panel

And there's a lot of good bugfixes in there that I'd love to be able to use without deploying our own builds.

Thanks,
Ewan



This email and any attachments to it may contain confidential information and are intended solely for the addressee.



If you are not the intended recipient of this email or if you believe you have received this email in error, please contact the sender and remove it from your system.Do not use, copy or disclose the information contained in this email or in any attachment.

RealityMine Limited may monitor email traffic data including the content of email for the purposes of security.

RealityMine Limited is a company registered in England and Wales. Registered number: 07920936 Registered office: Warren Bruce Court, Warren Bruce Road, Trafford Park, Manchester M17 1LB
"
Reynold Xin <rxin@databricks.com>,"Fri, 16 Sep 2016 11:23:14 -0700",Re: Spark 2.0.1 release?,Ewan Leith <ewan.leith@realitymine.com>,"2.0.1 is definitely coming soon.  Was going to tag a rc yesterday but ran
into some issue. I will try to do it early next week for rc.



"
Parth Brahmbhatt <brahmbhatt.parth@gmail.com>,"Fri, 16 Sep 2016 11:42:44 -0700",Re: Compatibility of 1.6 spark.eventLog with a 2.0 History Server,Mario Ds Briggs <mario.briggs@in.ibm.com>,"The problem is we backported the Sql tab ui changes from 2.0 in our 1.6.1. They changed a parameter name in SQLMetricInfo. Still the community version is compatible, ours is not.

ote:
 history server logs do show a 'Replaying log path: file:xxx.inprogress' when the file is changed , but a refresh on UI doesnt show the new jobs/stages/tasks whatever
""""

th latest events, where-as in the 1.6 code base, this does not happen)
he 1.x line?
"
Sean Owen <sowen@cloudera.com>,"Fri, 16 Sep 2016 19:54:39 +0100",Re: Spark 2.0.1 release?,Reynold Xin <rxin@databricks.com>,"There are a few blockers for 2.0.1, but just two. For example
https://issues.apache.org/jira/browse/SPARK-17418 must be resolved
before another release.


---------------------------------------------------------------------


"
Ewan Leith <ewan.leith@realitymine.com>,"Fri, 16 Sep 2016 18:59:20 +0000",Re: Spark 2.0.1 release?,Reynold Xin <rxin@databricks.com>,"That's great news, since it's that close I'll get started on building and testing the branch myself

Thanks,
Ewan

2.0.1 is definitely coming soon.  Was going to tag a rc yesterday but ran into some issue. I will try to do it early next week for rc.



Hi all,

Apologies if I've missed anything, but is there likely to see a 2.0.1 bug fix release, or does a jump to 2.1.0 with additional features seem more probable?

The issues for 2.0.1 seem pretty much done here https://issues.apache.org/jira/browse/SPARK/fixforversion/12336857/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-issues-panel

And there's a lot of good bugfixes in there that I'd love to be able to use without deploying our own builds.

Thanks,
Ewan



This email and any attachments to it may contain confidential information and are intended solely for the addressee.



If you are not the intended recipient of this email or if you believe you have received this email in error, please contact the sender and remove it from your system.Do not use, copy or disclose the information contained in this email or in any attachment.

RealityMine Limited may monitor email traffic data including the content of email for the purposes of security.

RealityMine Limited is a company registered in England and Wales. Registered number: 07920936 Registered office: Warren Bruce Court, Warren Bruce Road, Trafford Park, Manchester M17 1LB





This email and any attachments to it may contain confidential information and are intended solely for the addressee.



If you are not the intended recipient of this email or if you believe you have received this email in error, please contact the sender and remove it from your system.Do not use, copy or disclose the information contained in this email or in any attachment.

RealityMine Limited may monitor email traffic data including the content of email for the purposes of security.

RealityMine Limited is a company registered in England and Wales. Registered number: 07920936 Registered office: Warren Bruce Court, Warren Bruce Road, Trafford Park, Manchester M17 1LB
"
Michael Heuer <heuermh@gmail.com>,"Fri, 16 Sep 2016 14:03:39 -0500",Re: Spark 1.x/2.x qualifiers in downstream artifact names,Sean Owen <sowen@cloudera.com>,"

It is worse (or better) than that, profiles didn't work for us in
combination with Scala 2.10/2.11, so we modify the POM in place as part of
CI and the release process.




We shim over Hadoop changes where necessary but the Spark changes between
1.x and 2.x are too much.

We have since resolved to deploy separate Spark 1.x and 2.x artifactIds as
described below.  Relevant pull requests:

https://github.com/bigdatagenomics/adam/pull/1123
https://github.com/bigdatagenomics/utils/pull/78

Thanks!

   michael



"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Fri, 16 Sep 2016 18:23:44 -0700 (MST)",Re: What's the meaning when the partitions is zero?,dev@spark.apache.org,"if so, we will get exception when the numPartitions is 0.
 def getPartition(key: Any): Int = key match {
    case null => 0
//    case None => 0
    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)
  }



--

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 16 Sep 2016 18:33:51 -0700",Re: What's the meaning when the partitions is zero?,WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"When numPartitions is 0, there is no data in the rdd: so getPartition is
never invoked.

-  Mridul


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Fri, 16 Sep 2016 20:41:14 -0700 (MST)",Doubt about ExternalSorter.spillMemoryIteratorToDisk,dev@spark.apache.org,"We can see that when the number of been written objects equals
serializerBatchSize, the flush() will be called.  But if the objects written
exceeds the  default buffer size, what will happen? if this situation
happens,will the flush() be called automatelly?

private[this] def spillMemoryIteratorToDisk(inMemoryIterator:
WritablePartitionedIterator)
      : SpilledFile = {

    // ignore some code here
    try {
      while (inMemoryIterator.hasNext) {
        val partitionId = inMemoryIterator.nextPartition()
        require(partitionId >= 0 && partitionId < numPartitions,
          s""partition Id: ${partitionId} should be in the range [0,
${numPartitions})"")
        inMemoryIterator.writeNext(writer)
        elementsPerPartition(partitionId) += 1
        objectsWritten += 1
        if (objectsWritten == serializerBatchSize) {
          flush()
        }
      }
     
     // ignore some code here
    SpilledFile(file, blockId, batchSizes.toArray, elementsPerPartition)
  }



--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 17 Sep 2016 21:14:11 +0200",Different versions of dependencies in assembly/target/scala-2.11/jars?,dev <dev@spark.apache.org>,"Hi,

Just noticed in assembly/target/scala-2.11/jars that similar libraries
have different versions:

-rw-r--r--  1 jacek  staff   1230201 17 wrz 09:51 netty-3.8.0.Final.jar
-rw-r--r--  1 jacek  staff   2305335 17 wrz 09:51 netty-all-4.0.41.Final.jar

and

-rw-r--r--  1 jacek  staff    218076 17 wrz 09:51 parquet-hadoop-1.8.1.jar
-rw-r--r--  1 jacek  staff   2796935 17 wrz 09:51
parquet-hadoop-bundle-1.6.0.jar

and

-rw-r--r--  1 jacek  staff     46983 17 wrz 09:51 jackson-annotations-2.6.5.jar
-rw-r--r--  1 jacek  staff    258876 17 wrz 09:51 jackson-core-2.6.5.jar
-rw-r--r--  1 jacek  staff    232248 17 wrz 09:51 jackson-core-asl-1.9.13.jar
-rw-r--r--  1 jacek  staff   1171380 17 wrz 09:51 jackson-databind-2.6.5.jar
-rw-r--r--  1 jacek  staff     18336 17 wrz 09:51 jackson-jaxrs-1.9.13.jar
-rw-r--r--  1 jacek  staff    780664 17 wrz 09:51 jackson-mapper-asl-1.9.13.jar
-rw-r--r--  1 jacek  staff     41263 17 wrz 09:51
jackson-module-paranamer-2.6.5.jar
-rw-r--r--  1 jacek  staff    515604 17 wrz 09:51
jackson-module-scala_2.11-2.6.5.jar
-rw-r--r--  1 jacek  staff     27084 17 wrz 09:51 jackson-xc-1.9.13.jar

and

-rw-r--r--  1 jacek  staff    188671 17 wrz 09:51 commons-beanutils-1.7.0.jar
-rw-r--r--  1 jacek  staff    206035 17 wrz 09:51
commons-beanutils-core-1.8.0.jar

and

-rw-r--r--  1 jacek  staff    445288 17 wrz 09:51 antlr-2.7.7.jar
-rw-r--r--  1 jacek  staff    164368 17 wrz 09:51 antlr-runtime-3.4.jar
-rw-r--r--  1 jacek  staff    302248 17 wrz 09:51 antlr4-runtime-4.5.3.jar

Even if that does not cause any class mismatches, it might be worth to
exclude them to minimize the size of the Spark distro.

What do you think?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Hiral Mehta <hiralmehta0703@gmail.com>,"Sat, 17 Sep 2016 15:32:17 -0500",Fwd: Question regarding merging to two RDDs,"dev@spark.apache.org, user@spark.apache.org","Hi,

I have two separate csv files one with header and other with data. I read
those two files in 2 different RDDs and now I need to merge both the RDDs.

I tried various options such as union, zip, join but none worked for my
problem.
What is the best way to merge two RDDs so that the header and data are
merged into new RDD with header and data?

Thanks,
Hiral Mehta
"
Xiang Gao <qasdfgtyuiop@gmail.com>,"Sat, 17 Sep 2016 14:08:23 -0700 (MST)","java.lang.NoClassDefFoundError, is this a bug?",dev@spark.apache.org,"Hi,

In my application, I got a weird error message:
java.lang.NoClassDefFoundError: Could not initialize class XXXXX

This happens only when I try to submit my application in cluster mode. It
works perfectly in client mode.

I'm able to reproduce this error message by a simple 16-line program:
https://github.com/zasdfgbnm/spark-test1/blob/master/src/main/scala/test.scala

To reproduce it, simply clone this git repo, and then execute command like:
sbt package && spark-submit --master spark://localhost:7077
target/scala-2.11/createdataset_2.11-0.0.1-SNAPSHOT.jar

Can anyone check whether this is a bug of spark?



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 17 Sep 2016 22:34:44 +0100",Re: Different versions of dependencies in assembly/target/scala-2.11/jars?,Jacek Laskowski <jacek@japila.pl>,"No, these are different major versions of these components, each of
which gets used by something in the transitive dependency graph. They
are not redundant because they're not actually presenting roughly the
same component in the same namespace.

However the parquet-hadoop bit looks wrong, in that it should be
harmonized to one 1.x version. It's not that Spark uses inconsistent
versions but that transitive deps do. We can still harmonize them in
the build if it causes problems.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 18 Sep 2016 00:08:26 +0200",Re: Different versions of dependencies in assembly/target/scala-2.11/jars?,Sean Owen <sowen@cloudera.com>,"Hi Sean,

Thanks a lot for help understanding the different jars.

Do you think there's anything that should be reported as an
enhancement/issue/task in JIRA?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 18 Sep 2016 00:34:04 +0200","Re: java.lang.NoClassDefFoundError, is this a bug?",Xiang Gao <qasdfgtyuiop@gmail.com>,"Hi,

I'm surprised too. Here's the entire stack trace for reference. I'd
also like to know what causes the issue.

Caused by: java.lang.NoClassDefFoundError: Could not initialize class Main$
at Main$$anonfun$main$1.apply(test.scala:14)
at Main$$anonfun$main$1.apply(test.scala:14)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown
Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:277)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Xiang Gao <qasdfgtyuiop@gmail.com>,"Sat, 17 Sep 2016 15:44:18 -0700 (MST)","Re: java.lang.NoClassDefFoundError, is this a bug?",dev@spark.apache.org,"Besides, if you replace line #14 with:
Env.spark.createDataset(Seq(""a"",""b"",""c"")).rdd.map(func).collect()

You will have the same problem with a different stack trace:

Caused by: java.lang.NoClassDefFoundError: Could not initialize class Main$
        at Main$$anonfun$main$1.apply(test.scala:14)
        at Main$$anonfun$main$1.apply(test.scala:14)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
        at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
        at
        at scala.collection.AbstractIterator.to(Iterator.scala:1336)
        at
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
        at
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
        at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912)
        at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912)
        at
org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1918)
        at
org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1918)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)



--

---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sat, 17 Sep 2016 17:52:27 -0700 (MST)",Re: Fwd: Question regarding merging to two RDDs,dev@spark.apache.org,"maybe you can use dataframe ,with the header file as a schema 



--

---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sat, 17 Sep 2016 20:04:53 -0700 (MST)","Re: java.lang.NoClassDefFoundError, is this a bug?",dev@spark.apache.org,"do you run this on yarn mode or else?



--

---------------------------------------------------------------------


"
Xiang Gao <qasdfgtyuiop@gmail.com>,"Sat, 17 Sep 2016 20:08:59 -0700 (MST)","Re: java.lang.NoClassDefFoundError, is this a bug?",dev@spark.apache.org,"spark standalone cluster



--

---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sat, 17 Sep 2016 20:24:02 -0700 (MST)","Re: java.lang.NoClassDefFoundError, is this a bug?",dev@spark.apache.org,"if I remove this abstract class A[T : Encoder] {}  it's ok!



--

---------------------------------------------------------------------


"
Xiang Gao <qasdfgtyuiop@gmail.com>,"Sat, 17 Sep 2016 20:26:08 -0700 (MST)","Re: java.lang.NoClassDefFoundError, is this a bug?",dev@spark.apache.org,"Yes. Besides, if you change the ""T : Encoder"" to ""T"", it OK too.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 18 Sep 2016 16:14:51 +0100",Re: Different versions of dependencies in assembly/target/scala-2.11/jars?,Jacek Laskowski <jacek@japila.pl>,"Boy it's a long story, but I think the short answer is that it's only
worth manually fixing the mismatches that are clearly going to cause a
problem.

Dependency version mismatch is inevitable, and Maven will always
settle on one version of a particular group/artifact using a
nearest-wins rule (SBT uses latest-wins). However it's possible that
you get different versions of closely-related artifacts. Often it
doesn't matter; sometimes it does.

It's always possible to force a version of a group/artifact with
<dependencyManagement>. The drawback is that, as dependencies evolve,
you may be silently forcing that to an older version than other
dependencies want. It builds up its own quiet legacy problem.




---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 18 Sep 2016 17:22:45 +0200",Re: Different versions of dependencies in assembly/target/scala-2.11/jars?,Sean Owen <sowen@cloudera.com>,"Hi Sean,

""Boy it's a long story""...yeah, you tell me! :) I don't seem to find
anything worth reporting so...let's keep these possible discrepancies
in mind and be back to them when they hit us.

Thanks a lot, Sean. Your patience with dealing with people here and on
JIRA has always made me wish for having it. Kudos!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sun, 18 Sep 2016 09:06:33 -0700 (MST)",Memory usage for spark types,dev@spark.apache.org,"Hi,
I am trying to understand how spark types are kept in memory and accessed.
I tried to look at the code at the definition of MapType and ArrayType for example and I can't seem to find the relevant code for its actual implementation.

I am trying to figure out how these two types are implemented to understand how they match my needs.
In general, it appears the size of a map is the same as two arrays which is about double the na√Øve array implementation: if I have 1000 rows, each with a map from 10K integers to 10K integers, I find through caching the dataframe that the total is is ~150MB (the na√Øve implementation of two arrays would code 1000*10000*(4+4) or a total of ~80MB). I see the same size if I use two arrays. Second, what would be the performance of updating the map/arrays as they are immutable (i.e. some copying is required).

The reason I am asking this is because I wanted to do an aggregate function which calculates a variation of a histogram.
The most na√Øve solution for this would be to have a map from the bin to the count. But since we are talking about an immutable map, wouldn't that cost a lot more?
An even further optimization would be to use a mutable array where we combine the key and value to a single value (key and value are both int in my case). Assuming the maximum number of bins is small (e.g. less than 10), it is often cheaper to just search the array for the right key (and in this case the size of the data is expected to be significantly smaller than map). In my case, most of the type (90%) there are less than 3 elements in the bin and If I have more than 10 bins I basically do a combination to reduce the number.

For few elements, a map becomes very inefficient  - If I create 10M rows with 1 map from int to int each I get an overall of ~380MB meaning ~38 bytes per element (instead of just 8). For array, again it is too large (229MB, i.e. ~23 bytes per element).

Is there a way to implement a simple mutable array type to use in the aggregation buffer? Where is the portion of the code that handles the actual type handling?
Thanks,
                Assaf.




--
3.nabble.com/Memory-usage-for-spark-types-tp18984.html
om."
Reynold Xin <rxin@databricks.com>,"Sun, 18 Sep 2016 14:22:34 -0700",Re: Memory usage for spark types,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Take a look at UnsafeArrayData and UnsafeMapData.



.
r
 each
wo
ng
 to
ôt that
n
),
is
n
or-spark-types-tp18984.html>
"
Nitin Goyal <nitin2goyal@gmail.com>,"Mon, 19 Sep 2016 22:27:19 +0530",Continuous warning while consuming using new kafka-spark010 API,dev@spark.apache.org,"Hi All,

I am using the new kafka-spark010 API to consume messages from Kafka
(brokers running kafka 0.10.0.1).

I am seeing continuous following warning only when producer is writing
messages to kafka in parallel (increased
spark.streaming.kafka.consumer.poll.ms to 1024 ms as well) :-

16/09/19 16:44:53 WARN TaskSetManager: Lost task 97.0 in stage 32.0 (TID
4942, host-3): java.lang.AssertionError: assertion failed: Failed to get
records for spark-executor-example topic2 8 1052989 after polling for 1024

while at same time, I see this in spark UI corresponding to that job
topic: topic2    partition: 8    offsets: 1051731 to 1066124

Code :-

val stream = KafkaUtils.createDirectStream[String, String]( ssc,
PreferConsistent, Subscribe[String, String](topics, kafkaParams.asScala) )

stream.foreachRDD {rdd => rdd.filter(_ => false).collect}


Has anyone encountered this with the new API? Is this the expected
behaviour or am I missing something here?

-- 
Regards
Nitin Goyal
"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 20 Sep 2016 09:10:15 +0000",Re: Is RankingMetrics' NDCG implementation correct?,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","(cc'ing dev list also)

I think a more general version of ranking metrics that allows arbitrary
relevance scores could be useful. Ranking metrics are applicable to other
settings like search or other learning-to-rank use cases, so it should be a
little more generic than pure recommender settings.

The one issue with the proposed implementation is that it is not compatible
with the existing cross-validators within a pipeline.

As I've mentioned on the linked JIRAs & PRs, one option is to create a
special set of cross-validators for recommenders, that address the issues
of (a) dataset splitting specific to recommender settings (user-based
stratified sampling, time-based etc) and (b) ranking-based evaluation.

The other option is to have the ALSModel itself capable of generating the
""ground-truth"" set within the same dataframe output from ""transform"" (ie
predict top k) that can be fed into the cross-validator (with
RankingEvaluator) directly. That's the approach I took so far in
https://github.com/apache/spark/pull/12574.

Both options are valid and have their positives & negatives - open to
comments / suggestions.


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 20 Sep 2016 04:23:42 -0700 (MST)",RE: Memory usage for spark types,dev@spark.apache.org,"Thanks for the pointer.

I have been reading the code and trying to understand how to create an efficient aggregate function but I must be missing something because it seems to me that creating any kind of aggregation function which uses non primitive types would have a high overhead.
Consider the following simple example: We have a column which contains the numbers 1-10. We want to calculate a histogram for these values.
In an equivalent to the hand written code in https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html The trivial solution (a student solution) would look something like this:
var hist = new int[10]
for (v in col) {
  hist[v] += 1
}

The problem is that as far as I understand, spark wouldn‚Äôt create it this way.
Instead I would need to do something like ‚Äúupdate hist in position v by +1‚Äù which in practice would mean the array will be copied at least 3 times:
First it will be copied from its unsafe implementation to a scala sequence (even worse, since arrays always use offsets, the copying would have to be done element by element instead of a single memcopy), then since the array is immutable, we will have to create a new version of it (by copying and changing just the relevant element) and then we copy it back to the unsafe version.

I tried to look at examples in the code which have an intermediate buffer which is not a simple structure. Basically, I see two such types of examples: distinct operations (which, if I understand correctly, somehow internally has a hashmap to contain the distinct values but I can‚Äôt find the code which generates it) and collect functions (collect_list, collect_set) which do not appear to do any code generation BUT define their own buffer as they will (the buffer is NOT of a regular type).


So I was wondering, what is the right way to implement an efficient logic as above.
I see two options:

1.       Using UDAF ‚Äì In this case I would define the buffer to have 10 integer fields and manipulate each. This solution suffers from two problems: First it is slow (especially if there are other aggregations which are using spark sql expressions) and second it is limited (I can‚Äôt change the size of the array in the middle. For example, assuming the above histogram is made on a groupby and I know beforehand that in 99% of the cases there are 3 values but in 1% of the cases there are 100 values. If I would have used an array I would just convert to a bigger array the first time I see a value from the 100)

2.       Implement similar to collect_list and collect_set. If I look at the documentation for collect class, this uses the slower sort based aggregation path because the number of elmenets can not be determined in advance even though in the basic case above, we do know the size. (although I am not sure how its performance would compare to the UDAF option). This appears to be simpler than UDAF because I can use the data types I want directly, however I can‚Äôt figure out how the code generation is done as I do not see the relevant functions when doing debugCodegen on the result
I also believe there should be a third option by actually implementing the proper expression, but I have no idea how to do that.


Can anyone point me in the right direction?


From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n18985h24@n3.nabble.com]
Sent: Monday, September 19, 2016 12:23 AM
To: Mendelson, Assaf
Subject: Re: Memory usage for spark types

Take a look at UnsafeArrayData and UnsafeMapData.


Hi,
I am trying to understand how spark types are kept in memory and accessed.
I tried to look at the code at the definition of MapType and ArrayType for example and I can‚Äôt seem to find the relevant code for its actual implementation.

I am trying to figure out how these two types are implemented to understand how they match my needs.
In general, it appears the size of a map is the same as two arrays which is about double the na√Øve array implementation: if I have 1000 rows, each with a map from 10K integers to 10K integers, I find through caching the dataframe that the total is is ~150MB (the na√Øve implementation of two arrays would code 1000*10000*(4+4) or a total of ~80MB). I see the same size if I use two arrays. Second, what would be the performance of updating the map/arrays as they are immutable (i.e. some copying is required).

The reason I am asking this is because I wanted to do an aggregate function which calculates a variation of a histogram.
The most na√Øve solution for this would be to have a map from the bin to the count. But since we are talking about an immutable map, wouldn‚Äôt that cost a lot more?
An even further optimization would be to use a mutable array where we combine the key and value to a single value (key and value are both int in my case). Assuming the maximum number of bins is small (e.g. less than 10), it is often cheaper to just search the array for the right key (and in this case the size of the data is expected to be significantly smaller than map). In my case, most of the type (90%) there are less than 3 elements in the bin and If I have more than 10 bins I basically do a combination to reduce the number.

For few elements, a map becomes very inefficient  - If I create 10M rows with 1 map from int to int each I get an overall of ~380MB meaning ~38 bytes per element (instead of just 8). For array, again it is too large (229MB, i.e. ~23 bytes per element).

Is there a way to implement a simple mutable array type to use in the aggregation buffer? Where is the portion of the code that handles the actual type handling?
Thanks,
                Assaf.

________________________________
View this message in context: Memory usage for spark types<http://apache-spark-developers-list.1001551.n3.nabble.com/Memory-usage-for-spark-types-tp18984.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Memory-usage-for-spark-types-tp18984p18985.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Memory-usage-for-spark-types-tp18984p18990.html
om."
Jakob Odersky <jakob@odersky.com>,"Wed, 21 Sep 2016 14:21:57 -0700","Re: java.lang.NoClassDefFoundError, is this a bug?",Xiang Gao <qasdfgtyuiop@gmail.com>,"Hi Xiang,

this error also appears in client mode (maybe the situation that you
were referring to and that worked was local mode?), however the error
is expected and is not a bug.

this line in your snippet:
    object Main extends A[String] { //...
is, after desugaring, equivalent to:
    object Main extends
A[String]()(Env.spark.implicits.newStringEncoder) { //...
Essentially, when the singleton object `Main` is initialised, it will
evaluate all its parameters, i.e. it will call
`Env.spark.implicitcs.newStringEncoder`. Since your `main` method is
also defined in this object, it will be initialised as soon as your
application starts, that is before a spark session is started. The
""problem"" is that encoders require an active session and hence you
have an initialisation order problem. (You can replay the problem
simply by defining a `val x = Env.spark.implicits.newStringEncoder` in
your singleton object)

The error message is weird and not so helpful (I think this is due to
the way Spark uses ClassLoaders internally when running a submitted
application), however it isn't a bug in spark.

In local mode you will not experience the issue because you are
starting a session when the session builder is accessed the first time
via `Env.spark`.

Aside from the errors you're getting, there's another subtlety in your
snippet that may bite you later: the adding ""T : Encoder"" to your
super class has no effect with the current way that also imports
Env.spark.implicits._

best,
--Jakob



---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Wed, 21 Sep 2016 15:11:53 -0700",Re: What's the use of RangePartitioner.hashCode,WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Hi,
It is used jointly with a custom implementation of the `equals`
method. In Scala, you can override the `equals` method to change the
classes based on their parameter values (i.e. what case classes do).
Partitioners aren't case classes however it makes sense to have a
value comparison between them (see RDD.subtract for an example) and
hence they redefine the equals method.
When redefining an equals method, it is good practice to also redefine
the hashCode method so that `a == b` iff `a.hashCode == b.hashCode`
(e.g. this is useful when your objects will be stored in a hash map).
You can learn more about redefining the equals method and hashcodes
here https://www.safaribooksonline.com/library/view/scala-cookbook/9781449340292/ch04s16.html


regards,
--Jakob


---------------------------------------------------------------------


"
Andrew Duffy <root@aduffy.org>,"Wed, 21 Sep 2016 22:22:32 +0000 (UTC)",Re: What's the use of RangePartitioner.hashCode,"Jakob Odersky <jakob@odersky.com>,
	WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>","Pedantic note about hashCode and equals: the equality doesn't need to be bidirectional, you just need to ensure that a.hashCode == b.hashCode when a.equals(b), the bidirectional case is usually harder to satisfy due to possibility of collisions.

Good info: http://www.programcreek.com/2011/07/java-equals-and-hashcode-contract/
		_____________________________
From: Jakob Odersky <jakob@odersky.com>
Sent: Wednesday, September 21, 2016 15:12
Subject: Re: What's the use of RangePartitioner.hashCode
To: WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>
Cc: dev <dev@spark.apache.org>


Hi,
It is used jointly with a custom implementation of the `equals`
method. In Scala, you can override the `equals` method to change the
classes based on their parameter values (i.e. what case classes do).
Partitioners aren't case classes however it makes sense to have a
value comparison between them (see RDD.subtract for an example) and
hence they redefine the equals method.
When redefining an equals method, it is good practice to also redefine
the hashCode method so that `a == b` iff `a.hashCode == b.hashCode`
(e.g. this is useful when your objects will be stored in a hash map).
You can learn more about redefining the equals method and hashcodes
here https://www.safaribooksonline.com/library/view/scala-cookbook/9781449340292/ch04s16.html


regards,
--Jakob


---------------------------------------------------------------------




	"
Jakob Odersky <jakob@odersky.com>,"Wed, 21 Sep 2016 15:30:18 -0700",Re: What's the use of RangePartitioner.hashCode,Andrew Duffy <root@aduffy.org>,"Andrew, you're correct of course hashing is a one-way operation with
potential collisions


---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Wed, 21 Sep 2016 22:49:55 -0700 (MST)",Re: What's the use of RangePartitioner.hashCode,dev@spark.apache.org,"Than you very much sir!  but what i want to know is whether the hashcode
overflow will make a trouble. thank you!



--

---------------------------------------------------------------------


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 22 Sep 2016 06:05:26 +0000",Using Spark as a Maven dependency but with Hadoop 2.6,"user <user@spark.apache.org>, dev <dev@spark.apache.org>","Hi,when we fetch Spark 2.0.0 as maven dependency then we automatically end up
with hadoop 2.2 as a transitive dependency, I know multiple profiles are used to
generate the different tar.gz bundles that we can download, Is there by any
chance publications of Spark 2.0.0 with different classifier according to
different versions of Hadoop available ?
Thanks for your time !
Olivier Girardot"
Hemant Bhanawat <hemant9379@gmail.com>,"Thu, 22 Sep 2016 12:06:50 +0530",Memory usage by Spark jobs,"user <user@spark.apache.org>, dev@spark.apache.org","I am working on profiling TPCH queries for Spark 2.0.  I see lot of
temporary object creation (sometimes size as much as the data size) which
is justified for the kind of processing Spark does. But, from production
perspective, is there a guideline on how much memory should be allocated
for processing a specific data size of let's say parquet data? Also, has
someone investigated memory usage for the individual SQL operators like
Filter, group by, order by, Exchange etc.?

Hemant Bhanawat <https://www.linkedin.com/in/hemant-bhanawat-92a3811>
www.snappydata.io
"
Reynold Xin <rxin@databricks.com>,"Wed, 21 Sep 2016 23:47:37 -0700",R docs no longer building for branch-2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm working on packaging 2.0.1 rc but encountered a problem: R doc fails to
build. Can somebody take a look at the issue ASAP?



** knitting documentation of write.parquet
** knitting documentation of write.text
** knitting documentation of year
~/workspace/spark-release-docs/spark/R
~/workspace/spark-release-docs/spark/R


processing file: sparkr-vignettes.Rmd

  |
  |                                                                 |   0%
  |
  |.                                                                |   1%
   inline R code fragments


  |
  |.                                                                |   2%
label: unnamed-chunk-1 (with options)
List of 1
 $ message: logi FALSE

Loading required package: methods

Attaching package: 'SparkR'

The following objects are masked from 'package:stats':

    cov, filter, lag, na.omit, predict, sd, var, window

The following objects are masked from 'package:base':

    as.data.frame, colnames, colnames<-, drop, intersect, rank,
    rbind, sample, subset, summary, transform, union


  |
  |..                                                               |   3%
  ordinary text without R code


  |
  |..                                                               |   4%
label: unnamed-chunk-2 (with options)
List of 1
 $ message: logi FALSE

Spark package found in SPARK_HOME:
/home/jenkins/workspace/spark-release-docs/spark
Error: Could not find or load main class org.apache.spark.launcher.Main
Quitting from lines 30-31 (sparkr-vignettes.Rmd)
Error in sparkR.sparkContext(master, appName, sparkHome, sparkConfigMap,  :
  JVM is not ready after 10 seconds
Calls: render ... eval -> eval -> sparkR.session -> sparkR.sparkContext

Execution halted
jekyll 2.5.3 | Error:  R doc generation failed
Deleting credential directory
/home/jenkins/workspace/spark-release-docs/spark-utils/new-release-scripts/jenkins/jenkins-credentials-IXCkuX6w
Build step 'Execute shell' marked build as failure
[WS-CLEANUP] Deleting project workspace...[WS-CLEANUP] done
Finished: FAILURE
"
Hemant Bhanawat <hemant9379@gmail.com>,"Thu, 22 Sep 2016 12:42:55 +0530",Re: CSV Reader with row numbers,Akshay Sachdeva <akshay.sachdeva@gmail.com>,"zipWithIndex is fine. It will give you unique row IDs across your various
partitions.

You can also use zipWithUniqueId which saves an extra job that is fired by
zipWithIndex. However, there are some differences as to how indexes are
assigned to the row. You can read more about the two APIs in the API
documentation.

https://spark.apache.org/docs/1.6.1/api/scala/index.html#org.apache.spark.rdd.RDD

Hemant Bhanawat <https://www.linkedin.com/in/hemant-bhanawat-92a3811>
www.snappydata.io


"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Thu, 22 Sep 2016 10:13:59 +0200",Re: Memory usage by Spark jobs,Hemant Bhanawat <hemant9379@gmail.com>,"You should take also into account that spark has different option to represent data in-memory, such as Java serialized objects, Kyro serialized, Tungsten (columnar optionally compressed) etc. the tungsten thing depends heavily on the underlying data and sorting especially if compressed.
Then, you might think also about broadcasted data etc.

As such I am not aware of a specific guide, but there is also no magic behind it. could be a good jira task :) 

ary object creation (sometimes size as much as the data size) which is justified for the kind of processing Spark does. But, from production perspective, is there a guideline on how much memory should be allocated for processing a specific data size of let's say parquet data? Also, has someone investigated memory usage for the individual SQL operators like Filter, group by, order by, Exchange etc.? 
"
Sean Owen <sowen@cloudera.com>,"Thu, 22 Sep 2016 10:17:10 +0100",Re: Using Spark as a Maven dependency but with Hadoop 2.6,Olivier Girardot <o.girardot@lateral-thoughts.com>,"There can be just one published version of the Spark artifacts and they
have to depend on something, though in truth they'd be binary-compatible
with anything 2.2+. So you merely manage the dependency versions up to the
desired version in your <dependencyManagement>.


"
tahirhn <tahirhn@icloud.com>,"Thu, 22 Sep 2016 03:19:09 -0700 (MST)",Open source Spark based projects,dev@spark.apache.org,"I am planning to write a thesis on certain aspects (i.e testing, performance
optimisation, security) of Apache Spark. I need to study some projects that
are based on Apache Spark and are available as open source. 

If you know any such project (open source Spark based project), Please share
it here. Thanks



--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 22 Sep 2016 12:27:17 +0200","Deserializing InternalRow using a case class - how to avoid creating
 attrs manually?",dev <dev@spark.apache.org>,"Hi,

I've just discovered* that I can SerDe my case classes. What a nice
feature which I can use in spark-shell, too! Thanks a lot for offering
me so much fun!

What I don't really like about the code is the following part (esp.
that it conflicts with the implicit for Column):

import org.apache.spark.sql.catalyst.dsl.expressions._
// in spark-shell there are competing implicits
// That's why DslSymbol is used explicitly in the following line
scala> val attrs = Seq(DslSymbol('id).long, DslSymbol('name).string)
attrs: Seq[org.apache.spark.sql.catalyst.expressions.AttributeReference]
= List(id#8L, name#9)

scala> val jacekReborn = personExprEncoder.resolveAndBind(attrs).fromRow(row)
jacekReborn: Person = Person(0,Jacek)

Since I've got the ""schema"" as the case class Person I'd like to avoid
creating attrs manually. Is there a way to avoid the step and use a
""reflection""-like approach so the attrs are built out of the case
class?

Also, since we're at it, why's resolveAndBind required? Is this for
names and their types only?

Thanks for your help (and for such fantastic project Apache Spark!)

[*] yeah, took me a while, but the happiness is stronger and I'll
remember longer! :-)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Bi Linfeng <Linfeng371@outlook.com>,"Thu, 22 Sep 2016 11:11:29 +0000",A Spark resource scheduling order question,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I have a Spark resource scheduling order question when I read this code:

github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/Master.scala

In function schedule(), spark start drivers first, then start executors.
I‚Äôm wondering why we schedule in this order? Will the resource be wasted if a driver has been started but no resource for its executor?
Why don‚Äôt we start executors for the drivers have already running first?

Thanks,


Linfeng

"
Sean Owen <sowen@cloudera.com>,"Thu, 22 Sep 2016 13:06:16 +0100",Re: R docs no longer building for branch-2.0,Reynold Xin <rxin@databricks.com>,"FWIW it worked for me, but I may not be executing the same thing. I
was running the commands given in R/DOCUMENTATION.md

It succeeded for me in creating the vignette, on branch-2.0.

Maybe it's a version or library issue? what R do you have installed,
and are you up to date with packages like devtools and roxygen2?


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 22 Sep 2016 10:23:21 -0700",Re: R docs no longer building for branch-2.0,Sean Owen <sowen@cloudera.com>,"I looked into this and found the problem. Will send a PR now to fix this.

If you are curious about what is happening here: When we build the
docs separately we don't have the JAR files from the Spark build in
the same tree. We added a new set of docs recently in SparkR called an
R vignette that runs Spark and generates docs using outputs from the
run.  So this doesn't work when the JARs are not available.

Thanks
Shivaram


---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Thu, 22 Sep 2016 11:29:13 -0700",Re: What's the use of RangePartitioner.hashCode,WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Hash codes should try to avoid collisions of objects that are not
equal. Integer overflowing is not an issue by itself


---------------------------------------------------------------------


"
Asher Krim <akrim@hubspot.com>,"Thu, 22 Sep 2016 17:49:37 -0400",[SPARK-15717][GraphX] status,dev@spark.apache.org,"Does anyone know what the status of SPARK-15717 is? It's a simple enough
looking PR, but there has been no activity on it since June 16th.

I believe that we are hitting that bug with checkpointed distributed LDA.
It's a blocker for us and we would really appreciate getting it fixed.

Jira: https://issues.apache.org/jira/browse/SPARK-15717
GitHub: https://github.com/apache/spark/pull/13458

Thanks,
Asher
Senior Software Engineer
HubSpot
"
Reynold Xin <rxin@databricks.com>,"Thu, 22 Sep 2016 14:51:45 -0700",Re: [SPARK-15717][GraphX] status,Asher Krim <akrim@hubspot.com>,"Did you try the proposed fix? Would be good to know whether it fixes the
issue.


"
Xiang Gao <qasdfgtyuiop@gmail.com>,"Thu, 22 Sep 2016 21:33:36 -0700 (MST)","Re: java.lang.NoClassDefFoundError, is this a bug?",dev@spark.apache.org,"Yes, I mean local here. Thanks for pointing this out. Also thanks for
explaining the problem.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 22 Sep 2016 23:01:04 -0700",[VOTE] Release Apache Spark 2.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.1. The vote is open until Sunday, Sep 25, 2016 at 23:59 PDT and passes
if a majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.1
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.1-rc2
(04141ad49806a48afccc236b699827997142bd57)

This release candidate resolves 284 issues:
https://s.apache.org/spark-2.0.1-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1199

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc2-docs/


Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.0.

Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series.  Bugs already present
in 2.0.0, missing features, or bugs related to new features will not
necessarily block this release.

Q: What happened to 2.0.1 RC1?
A: There was an issue with RC1 R documentation during release candidate
preparation. As a result, rc1 was canceled before a vote was called.
"
Yash Sharma <yash360@gmail.com>,"Fri, 23 Sep 2016 17:04:06 +1000","Spark job fails as soon as it starts. Driver requested a total number
 of 168510 executor","""user@spark.apache.org"" <dev@spark.apache.org>","Hi All,
I have a spark job which runs over a huge bulk of data with Dynamic
allocation enabled.
The job takes some 15 minutes to start up and fails as soon as it starts*.

Is there anything I can check to debug this problem. There is not a lot of
information in logs for the exact cause but here is some snapshot below.

Thanks All.

* - by starts I mean when it shows something on the spark web ui, before
that its just blank page.

Logs here -

{code}
16/09/23 06:33:19 INFO ApplicationMaster: Started progress reporter thread
with (heartbeat : 3000, initial allocation : 200) intervals
16/09/23 06:33:27 INFO YarnAllocator: Driver requested a total number of
168510 executor(s).
16/09/23 06:33:27 INFO YarnAllocator: Will request 168510 executor
containers, each with 2 cores and 6758 MB memory including 614 MB overhead
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 22
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 19
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 18
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 12
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 11
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 20
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 15
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 7
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 8
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 16
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 21
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 6
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 13
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 14
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 9
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 3
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 17
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 1
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 10
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 4
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 2
16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for
non-existent executor 5
16/09/23 06:33:36 WARN ApplicationMaster: Reporter thread fails 1 time(s)
in a row.
java.lang.StackOverflowError
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
        at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
        at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
        at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
        at
scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)
{code}

... <trimmed logs>

{code}
16/09/23 06:33:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint:
Attempted to get executor loss reason for executor id 7 at RPC address ,
but got no response. Marking as slave lost.
org.apache.spark.SparkException: Fail to find loss reason for non-existent
executor 7
        at
org.apache.spark.deploy.yarn.YarnAllocator.enqueueGetLossReasonRequest(YarnAllocator.scala:554)
        at
org.apache.spark.deploy.yarn.ApplicationMaster$AMEndpoint$$anonfun$receiveAndReply$1.applyOrElse(ApplicationMaster.scala:632)
        at
org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:104)
        at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:204)
        at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
        at
org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:215)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}
"
"""Aditya"" <aditya.calangutkar@augmentiq.co.in>","Fri, 23 Sep 2016 12:42:26 +0530",Re: Spark job fails as soon as it starts. Driver requested a total number of 168510 executor,"""Yash Sharma"" <yash360@gmail.com>, =?us-ascii?Q?user=40spark=2Eapac?=
 =?us-ascii?Q?he=2Eorg?= <dev@spark.apache.org>","Hi Yash,

What is your total cluster memory and number of cores?
Problem might be with the number of executors you are allocating. The 
logs shows it as 168510 which is on very high side. Try reducing your 
executors.






---------------------------------------------------------------------


"
"""Jagadeesan As"" <as2@us.ibm.com>","Fri, 23 Sep 2016 07:25:43 +0000","=?UTF-8?B?RG9jdW1lbnRhdGlvbiBmb3IgcGFja2FnZSDigJhTcGFya1LigJk=?=
 =?UTF-8?B?IHZlcnNpb24gbWlzbWF0Y2g=?=",Reynold Xin <rxin@databricks.com>,"Hi Reyonld,

While checking the documentation for 'SparkR' in upcoming spark-2.0.1 
release, found version mismatch. Please find the attachments.
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc2-docs/

 

Thanks & Regards
Jagadeesan A S | Software Engineer | 
as2@us.ibm.com | jagadeesan_as@persistentco.in 


---------------------------------------------------------------------"
"""Aditya"" <aditya.calangutkar@augmentiq.co.in>","Fri, 23 Sep 2016 13:13:18 +0530",Re: Spark Yarn Cluster with Reference File,"""ABHISHEK"" <abhietc@gmail.com>, user@spark.apache.org, dev@spark.apache.org","Hi Abhishek,

 From your spark-submit it seems your passing the file as a parameter to 
the driver program. So now it depends what exactly you are doing with 
that parameter. Using --files option it will be available to all the 
worker nodes but if in your code if you are referencing using the 
specified path in distributed mode it wont get the file on the worker nodes.

If you can share the snippet of code it will be easy to debug.




"
Jacek Laskowski <jacek@japila.pl>,"Fri, 23 Sep 2016 11:18:52 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



-------------------------------------------------------------------"
Yash Sharma <yash360@gmail.com>,"Fri, 23 Sep 2016 19:42:50 +1000","Re: Spark job fails as soon as it starts. Driver requested a total
 number of 168510 executor",Aditya <aditya.calangutkar@augmentiq.co.in>,"Thanks Aditya, appreciate the help.

I had the exact thought about the huge number of executors requested.
I am going with the dynamic executors and not specifying the number of
executors. Are you suggesting that I should limit the number of executors
when the dynamic allocator requests for more number of executors.

Its a 12 node EMR cluster and has more than a Tb of memory.




"
aditya.calangutkar@augmentiq.co.in,"Fri, 23 Sep 2016 09:50:11 +0000",Re: Spark job fails as soon as it starts. Driver requested a total number of 168510 executor,"""Yash Sharma"" <yash360@gmail.com>","

For testing purpose can you run with fix number of executors and try. May be 12 executors for testing and let know the status.


Get Outlook for Android
















Thanks Aditya, appreciate the help.
I had the exact thought about the huge number of executors requested.I am going with the dynamic executors and not specifying the number of executors. Are you suggesting that I should limit the number of executors when the dynamic allocator requests for more number of executors.
Its a 12 node EMR cluster and has more than a Tb of memory.¬†


Hi Yash,



What is your total cluster memory and number of cores?

Problem might be with the number of executors you are allocating. The logs shows it as 168510 which is on very high side. Try reducing your executors.





Hi All,

I have a spark job which runs over a huge bulk of data with Dynamic allocation enabled.

The job takes some 15 minutes to start up and fails as soon as it starts*.



Is there anything I can check to debug this problem. There is not a lot of information in logs for the exact cause but here is some snapshot below.



Thanks All.



* - by starts I mean when it shows something on the spark web ui, before that its just blank page.



Logs here -



{code}

16/09/23 06:33:19 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals

16/09/23 06:33:27 INFO YarnAllocator: Driver requested a total number of 168510 executor(s).

16/09/23 06:33:27 INFO YarnAllocator: Will request 168510 executor containers, each with 2 cores and 6758 MB memory including 614 MB overhead

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 22

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 19

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 18

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 12

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 11

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 20

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 15

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 7

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 8

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 16

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 21

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 6

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 13

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 14

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 9

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 3

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 17

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 1

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 10

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 4

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 2

16/09/23 06:33:36 WARN YarnAllocator: Tried to get the loss reason for non-existent executor 5

16/09/23 06:33:36 WARN ApplicationMaster: Reporter thread fails 1 time(s) in a row.

java.lang.StackOverflowError

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

¬† ¬† ¬† ¬† at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

¬† ¬† ¬† ¬† at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

¬† ¬† ¬† ¬† at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

¬† ¬† ¬† ¬† at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)

{code}



... <trimmed logs>



{code}

16/09/23 06:33:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 7 at RPC address , but got no response. Marking as slave lost.

org.apache.spark.SparkException: Fail to find loss reason for non-existent executor 7

¬† ¬† ¬† ¬† at org.apache.spark.deploy.yarn.YarnAllocator.enqueueGetLossReasonRequest(YarnAllocator.scala:554)

¬† ¬† ¬† ¬† at org.apache.spark.deploy.yarn.ApplicationMaster$AMEndpoint$$anonfun$receiveAndReply$1.applyOrElse(ApplicationMaster.scala:632)

¬† ¬† ¬† ¬† at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:104)

¬† ¬† ¬† ¬† at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:204)

¬† ¬† ¬† ¬† at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)

¬† ¬† ¬† ¬† at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:215)

¬† ¬† ¬† ¬† at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

¬† ¬† ¬† ¬† at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

¬† ¬† ¬† ¬† at java.lang.Thread.run(Thread.java:745)

{code}



















"
Asher Krim <akrim@hubspot.com>,"Fri, 23 Sep 2016 10:51:42 -0400",Re: [SPARK-15717][GraphX] status,Anderson de Andrade <adeandradeds@gmail.com>,"Thanks Anderson!

I have not tried the fix yet due to the way we currently build spark (we


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 23 Sep 2016 17:28:45 +0200",Why Expression.deterministic method and Nondeterministic trait?,dev <dev@spark.apache.org>,"Hi,

Just came across the Expression trait [1] that can be check for
determinism by the method deterministic [2] and trait Nondeterministic
[3]. Why both?

[1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L53
[2] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L80
[3] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L271

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Fri, 23 Sep 2016 09:44:46 -0700",Re: Why Expression.deterministic method and Nondeterministic trait?,Jacek Laskowski <jacek@japila.pl>,"Jacek,

A non-deterministic expression usually holds some state. The
Nondeterministic trait makes sure a user can initialize this state
properly. Take a look at InterpretedProjection
<https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Projection.scala#L66-L69>
for instance.

HTH

-Herman


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 23 Sep 2016 19:46:13 +0200",Re: Why Expression.deterministic method and Nondeterministic trait?,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Hi Herman,

That helps to know that someone can explain why we've got the two
nondeterministic states.

It's not possible to say...a non-Nondeterministic expression can be
non-deterministic (the former is the trait while the latter is the
method) #strange

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


ly.
org/apache/spark/sql/catalyst/expressions/Expression.scala#L53
org/apache/spark/sql/catalyst/expressions/Expression.scala#L80
org/apache/spark/sql/catalyst/expressions/Expression.scala#L271

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 23 Sep 2016 11:16:37 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Reynold Xin <rxin@databricks.com>,"+1 (non-binding) also verified that the assembly files with license issues
are not being published to maven staging repositories.




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Fri, 23 Sep 2016 23:28:55 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Build:
OK, but can no longer use the ""--tgz"" option when
calling make-distribution.sh (maybe a problem on my side?)

Run:
No regressions from 2.0.0 detected. Tested our pipelines on a standalone
cluster (Python API)




"
Sean Owen <sowen@cloudera.com>,"Fri, 23 Sep 2016 23:08:12 +0100",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Reynold Xin <rxin@databricks.com>,"+1 Signatures and hashes check out. I checked that the Kinesis
assembly artifacts are not present.

I compiled and tested on Java 8 / Ubuntu 16 with -Pyarn -Phive
-Phive-thriftserver -Phadoop-2.7 -Psparkr and only saw one test
problem. This test never com"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 23 Sep 2016 15:24:57 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Sean Owen <sowen@cloudera.com>,"Similar but not identical configuration (Java 8/macOs 10.12 with build/mvn
-Phive -Phive-thriftserver -Phadoop-2.7 -Pyarn clean install);
Similar but not identical failure:

...

- line wrapper only initialized once when used as encoder outer scope

Spark context available as 'sc' (master = local-cluster[1,1,1024], app id =
app-20160923150640-0000).

Spark session available as 'spark'.

Exception in thread ""dispatcher-event-loop-1"" java.lang.OutOfMemoryError:
GC overhead limit exceeded

Exception in thread ""dispatcher-event-loop-7"" java.lang.OutOfMemoryError:
GC overhead limit exceeded

- define case class and create Dataset together with paste mode

java.lang.OutOfMemoryError: GC overhead limit exceeded

- should clone and clean line object in ClosureCleaner *** FAILED ***

  java.util.concurrent.TimeoutException: Futures timed out after [10
minutes]

...



"
vaquar khan <vaquar.khan@gmail.com>,"Fri, 23 Sep 2016 17:30:28 -0500",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Mark Hamstra <mark@clearstorydata.com>,"+1 non binding
No issue found.
Regards,
Vaquar khan


Similar but not identical configuration (Java 8/macOs 10.12 with build/mvn
-Phive -Phive-thriftserver -Phadoop-2.7 -Pyarn clean install);
Similar but not identical failure:

...

- line wrapper only in"
Jacek Laskowski <jacek@japila.pl>,"Sat, 24 Sep 2016 00:32:43 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Sean Owen <sowen@cloudera.com>,"Hi,

Not that it could fix the issue but no -Pmesos?

Jacek


"
Reynold Xin <rxin@databricks.com>,"Fri, 23 Sep 2016 15:35:04 -0700",Re: Why Expression.deterministic method and Nondeterministic trait?,Jacek Laskowski <jacek@japila.pl>,"deterministic method describes whether this instance of the expression tree
is deterministic, whereas Nondeterministic trait is about a class.



"
Yash Sharma <yash360@gmail.com>,"Sat, 24 Sep 2016 10:27:55 +1000","Re: Spark job fails as soon as it starts. Driver requested a total
 number of 168510 executor",Aditya <aditya.calangutkar@augmentiq.co.in>,"Have been playing around with configs to crack this. Adding them here where
it would be helpful to others :)
Number of executors and timeout seemed like the core issue.

{code}
--driver-memory 4G \
--conf spark.dynamicAllocation.enabled=true \
--conf spark.dynamicAllocation.maxExecutors=500 \
--conf spark.core.connection.ack.wait.timeout=6000 \
--conf spark.akka.heartbeat.interval=6000 \
--conf spark.akka.frameSize=100 \
--conf spark.akka.timeout=6000 \
{code}

Cheers !


"
Yash Sharma <yash360@gmail.com>,"Sat, 24 Sep 2016 10:54:56 +1000","Re: Spark job fails as soon as it starts. Driver requested a total
 number of 168510 executor",Aditya <aditya.calangutkar@augmentiq.co.in>,"Is there anywhere I can help fix this ?

I can see the requests being made in the yarn allocator. What should be the
upperlimit of the requests made ?

https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L222


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 24 Sep 2016 12:27:21 +0900",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Sean Owen <sowen@cloudera.com>,"Then, are we going to submit a PR and fix this maybe?


"
Yash Sharma <yash360@gmail.com>,"Sat, 24 Sep 2016 14:37:28 +1000","Re: Spark job fails as soon as it starts. Driver requested a total
 number of 168510 executor",dhruve ashar <dhruveashar@gmail.com>,"Hi Dhruve, thanks.
I've solved the issue with adding max executors.
I wanted to find some place where I can add this behavior in Spark so that
user should not have to worry about the max executors.

Cheers

- Thanks, via mobile,  excuse brevity.


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 24 Sep 2016 08:39:46 +0200",Re: @scala.annotation.varargs or @_root_.scala.annotation.varargs?,Hyukjin Kwon <gurwls223@gmail.com>,"
https://issues.apache.org/jira/browse/SPARK-17656

Thanks Hyukjin! Unless someone beats me to it, I'm going to have a PR
over the weekend.

Jacek

---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sat, 24 Sep 2016 01:05:35 -0700 (MST)",Re: What's the use of RangePartitioner.hashCode,dev@spark.apache.org,"thank you!



--

---------------------------------------------------------------------


"
Yash Sharma <yash360@gmail.com>,"Sat, 24 Sep 2016 09:08:58 +0000","Re: Spark job fails as soon as it starts. Driver requested a total
 number of 168510 executor",ayan guha <guha.ayan@gmail.com>,"We have too many (large)  files. We have about 30k partitions with about 4
years worth data and we need to process entire history in a one time
monolithic job.

I would like to know how spark decides the number of executors requested.
I've seen testcases where the max executors count is Integer's Max value,
 was wondering if we can compute an appropriate max executor count based on
the cluster resources.

Would be happy to contribute back if I can get some info on the executors
requests.

Cheers



"
Jacek Laskowski <jacek@japila.pl>,"Sat, 24 Sep 2016 12:42:34 +0200",Re: Why Expression.deterministic method and Nondeterministic trait?,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

That's my point indeed. If non-determinism is a trait/property of a
class (objects really at runtime) it's by definition part of a trait
and only Nondeterministic expressions are...well...non-deterministic.

I'm yet to review the code regarding the trait, but I suspect that
there are places where you check whether an expression is
deterministic by the method not the trait. If so, there's this
ambiguity I'm talking about.

Anyway, I'm glad to learn from you guys! Thanks.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


ee
:
c
la/org/apache/spark/sql/catalyst/expressions/Expression.scala#L53
la/org/apache/spark/sql/catalyst/expressions/Expression.scala#L80
la/org/apache/spark/sql/catalyst/expressions/Expression.scala#L271

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Sat, 24 Sep 2016 12:25:47 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"+1 (non binding)

I compiled and tested on the following two systems.

- CentOS 7.2 / Oracle JDK 1.8.0_77 / R 3.3.1 with -Pyarn -Phadoop-2.7
-Pkinesis-asl -Phive -Phive-thriftserver -Dsparkr
- CentOS 7.2 / Open JDK 1.8.0_102 with -Pyarn -Phadoop-2.7 -Pkin"
Jacek Laskowski <jacek@japila.pl>,"Sat, 24 Sep 2016 23:19:07 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Dongjoon Hyun <dongjoon@apache.org>,"Hi,

I keep asking myself why are you guys not including -Pmesos in your
builds? Is this on purpose or have you overlooked it?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 24 Sep 2016 22:23:31 +0100",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"The binary artifact that's published here is built with -Pmesos. The
'real' artifact from a process standpoint is the source release,
however. That's why we do (should) test the source release foremost.

I suppose individuals are invited to test with a configuration that is
of interest to them, and so I enable just the flags I would care to
test I suppose.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 24 Sep 2016 15:05:16 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

The R API documentation version error was reported in a separate thread.
I've built a release candidate (RC3) and will send out a new vote email in
a bit.


"
Reynold Xin <rxin@databricks.com>,"Sat, 24 Sep 2016 15:08:35 -0700",[VOTE] Release Apache Spark 2.0.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.1. The vote is open until Tue, Sep 27, 2016 at 15:30 PDT and passes if
a majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.1
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.1-rc3
(9d28cc10357a8afcfb2fa2e6eecb5c2cc2730d17)

This release candidate resolves 290 issues:
https://s.apache.org/spark-2.0.1-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1201/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-docs/


Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.0.

Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series.  Bugs already present
in 2.0.0, missing features, or bugs related to new features will not
necessarily block this release.

Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.2, rather than 2.0.1. If a new RC
(i.e. RC4) is cut, I will change the fix version of those patches to 2.0.1.
"
Reynold Xin <rxin@databricks.com>,"Sat, 24 Sep 2016 15:09:35 -0700","=?UTF-8?Q?Re=3A_Documentation_for_package_=E2=80=98SparkR=E2=80=99_version_m?=
	=?UTF-8?Q?ismatch?=",Jagadeesan As <as2@us.ibm.com>,"Thanks for reporting. I sent out an email for rc3 fixing the issue.

We have also automated the version number update for documentation pages so
this won't happen again in the future.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Sat, 24 Sep 2016 20:09:38 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"There is no ""mesos"" profile in 2.0.1.




-- 
Marcelo

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 25 Sep 2016 13:12:56 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"Hi,

That's even more interesting. How's so since the profile got added a
week ago or later and RC2 was cut two/three days ago? Anyone know?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 25 Sep 2016 12:21:46 +0100",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"It was added to the master branch, and this is a release from the 2.0.x branch.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 25 Sep 2016 13:26:11 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Sean Owen <sowen@cloudera.com>,"Hi Sean,

Sure, but then the question is why it's not a part of 2.0.1? I thought
it was considered ready for prime time and so should be shipped in
2.0.1.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 25 Sep 2016 12:27:55 +0100",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"It's a change to the structure of the project, and probably not
appropriate for a maintenance release. 2.0.1 core would then no longer
contain Mesos code while 2.0.0 did.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 25 Sep 2016 13:30:05 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Sean Owen <sowen@cloudera.com>,"Hi Sean,

So, another question would be when is the change going to be released
then? What's the version for the master? The next release's 2.0.2 so
it's not for mesos profile either :(

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 25 Sep 2016 12:35:20 +0100",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"Master is implicitly 2.1.x right now. When branch-2.1 is cut, master
becomes the de facto 2.2.x branch. It's not true that the next release
is 2.0.2. You can see the master version:
https://github.com/apache/spark/blob/master/pom.xml#L29


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 25 Sep 2016 14:08:29 +0100",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Reynold Xin <rxin@databricks.com>,"+1 binding. Same as for RC2 -- it all checks out, from license to sigs
to compile and test. We have no issues of any kind targeted for 2.0.1.

I do have one test failure that I have seen with some regularity in
just kill all my ssh sessions (!)

YarnClust"
Jacek Laskowski <jacek@japila.pl>,"Sun, 25 Sep 2016 17:31:19 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Sean Owen <sowen@cloudera.com>,"Hi Sean,

I remember a similar discussion about the releases in Spark and I must
admit it again -- I simply don't get it. I seem to not have paid
enough attention to details to appreciate it. I apologize for asking
the very same questions again and again. Sorry.

Re the next release, I was referring to JIRA where 2.0.2 came up quite
recently for issues not included in 2.0.1. This disjoint between
releases and JIRA versions causes even more frustration whenever I'm
asked what and when the next release is going to be. It's not as
simple as I think it should be (for me).

(I really hope it's only me with this mental issue)

Unless I'm mistaken, -Pmesos won't get included in 2.0.x releases
unless someone adds it to branch-2.0. Correct?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 25 Sep 2016 17:51:32 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Reynold Xin <rxin@databricks.com>,"+1

Ship it!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------"
Sean Owen <sowen@cloudera.com>,"Sun, 25 Sep 2016 16:57:08 +0100","Master, branches and versioning",Jacek Laskowski <jacek@japila.pl>,"(Renaming thread to keep it separate from RC vote)

If you're asking why there's a version 2.0.2 in JIRA, it's because we
have to have that entity in order to target anything to version 2.0.2.
2.2.0 exists as well as version labels in JIRA. None are marked as
'released' because there is no such release, but it makes perfect
sense to have a noun to talk about in JIRA.

There may never be a 2.0.2 software release. But anything committed to
2.0.x after this point will be released in 2.0.2 _if it does get
released_.
If 2.0.2 happens it may happen before or after 2.1.0; that's normal. I
suspect it would happen after, if ever, and I expect the next actual
software release chronologically will be 2.1.0.

I think this is all standard software procedure; what's the confusion?

There is no formal plan for when which releases happen, so I don't
think anyone can answer that definitively. I happens when it happens
by loose consensus.

The -Pmesos change is not in branch-2.0 and therefore would not be in
any 2.0.x release.



---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Sun, 25 Sep 2016 11:40:42 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Reynold Xin <rxin@databricks.com>,"+1 (non binding)

RC3 is compiled and tested on the following two systems, too. All tests
passed.

* CentOS 7.2 / Oracle JDK 1.8.0_77 / R 3.3.1
   with -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver
-Dsparkr
* CentOS 7.2 / Open JDK 1.8.0_102"
Yin Huai <yhuai@databricks.com>,"Sun, 25 Sep 2016 13:16:14 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Dongjoon Hyun <dongjoon@apache.org>,1
Josh Rosen <joshrosen@databricks.com>,"Sun, 25 Sep 2016 20:25:07 +0000",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"Yin Huai <yhuai@databricks.com>, Dongjoon Hyun <dongjoon@apache.org>",1
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 25 Sep 2016 13:35:50 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Josh Rosen <joshrosen@databricks.com>,"+1

Matei

tests passed.
-Dsparkr
version 2.0.1. The vote is open until Tue, Sep 27, 2016 at 15:30 PDT and passes if a majority of at least 3+1 PMC votes are cast.
(9d28cc10357a8afcfb2fa2e6eecb5c2cc2730d17)
https://s.apache.org/spark-2.0.1-jira <https://s"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Sun, 25 Sep 2016 23:05:22 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Built and tested on
- Ubuntu 16.04 / OpenJDK 1.8.0_91
- CentOS / Oracle Java 1.7.0_55
(-Phadoop-2.7 -Dhadoop.version=2.7.3 -Phive -Phive-thriftserver -Pyarn)



"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 25 Sep 2016 14:11:51 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"Spark's branch-2.0 is a maintenance branch, effectively meaning that only
bug-fixes will be added to it.  There are other maintenance branches (such
as branch-1.6) that are also receiving bug-fixes in theory, but not so much
in fact as maintenance branches get older.  The major and minor version
numbers of maintenance branches stay fixed, with only the patch-level
version number changing as new releases are made from a maintenance
branch.  Thus, the next release from branch-2.0 will be 2.0.1, the set of
bug-fixes contributing to the next branch-2.0 release will result in 2.0.2,
etc.

New work, both bug-fixes and non-bug-fixes, is contributed to the master
branch.  New releases from the master branch increment the minor version
number (unless they include API-breaking changes, in which case the major
version number changes -- e.g. Spark 1.x.y to Spark 2.0.0).  Thus the first
release from the current master branch will be 2.1.0, the next will be
2.2.0, etc.

There should be active ""next JIRA numbers"" for whatever will be the next
release from the master as well as each of the maintenance branches.

This is all just basic SemVer (http://semver.org/), so it surprises me some
that you are finding the concepts to be new, difficult or frustrating.


"
Jason White <jason.white@shopify.com>,"Sun, 25 Sep 2016 15:13:30 -0700 (MST)",ArrayType support in Spark SQL,dev@spark.apache.org,"It seems that `functions.lit` doesn't support ArrayTypes. To reproduce:

org.apache.spark.sql.functions.lit(2 :: 1 :: Nil)

java.lang.RuntimeException: Unsupported literal type class
scala.collection.immutable.$colon$colon List(2, 1)
  at
org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:59)
  at org.apache.spark.sql.functions$.lit(functions.scala:101)
  ... 48 elided

This is about the first thing I tried to do with ArrayTypes in Spark SQL. Is
this usage supported, or on the roadmap?



--

---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Sun, 25 Sep 2016 15:26:39 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)


"
Jason White <jason.white@shopify.com>,"Sun, 25 Sep 2016 15:58:42 -0700 (MST)",Re: ArrayType support in Spark SQL,dev@spark.apache.org,"Continuing to dig, I encountered:
https://github.com/apache/spark/blob/master/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/LiteralExpressionSuite.scala#L125

  // TODO(davies): add tests for ArrayType, MapType and StructType

I guess others have thought of this already, just not implemented yet. :)

For others reading this thread, someone suggested using a SQL UDF to return
the constant - this works as a hack for now.




--

---------------------------------------------------------------------


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Mon, 26 Sep 2016 10:41:41 +0900",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),dev <dev@spark.apache.org>,"+1 (non-binding)



"
vaquar khan <vaquar.khan@gmail.com>,"Sun, 25 Sep 2016 20:58:54 -0500",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"+1 (non-binding)

Regards,
Vaquar khan


)
 and
ate,
will
 to
"
Luciano Resende <luckbr1975@gmail.com>,"Sun, 25 Sep 2016 19:02:43 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Pete Lee <petermaxlee@gmail.com>,"Sun, 25 Sep 2016 22:43:04 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1



)
 and
ate,
will
 to
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Sun, 25 Sep 2016 23:03:14 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Ôºã1


n)
T and
date,
 will
0
w
s to
"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 26 Sep 2016 14:20:11 +0800",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","+1

m

<
rn)
:
r
r
DT and
5
d
idate,
y
s will
atches to


-- 
Best Regards

Jeff Zhang
"
Lakshmi Rajagopalan <lakshmi@indix.com>,"Mon, 26 Sep 2016 15:39:58 +0530",Deep Equals support on Maptype,dev@spark.incubator.apache.org,"Hi,

 We wanted to extend the existing '===' on Column to support deep equals on
Maps.


Currently it checks for == which does referential checks for maps.

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L420

Is there any possible work around for this? I have a patch in my mind to
fix Equals for MapType also. Any thoughts?

Thanks,
Lakshmi Rajagopalan
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Mon, 26 Sep 2016 19:13:04 +0900",Re: Deep Equals support on Maptype,Lakshmi Rajagopalan <lakshmi@indix.com>,"Hi,

Have you check this jira?
https://issues.apache.org/jira/browse/SPARK-9415

// maropu





-- 
---
Takeshi Yamamuro
"
Lakshmi Rajagopalan <lakshmi@indix.com>,"Mon, 26 Sep 2016 16:50:00 +0530",Re: Deep Equals support on Maptype,Takeshi Yamamuro <linguin.m.s@gmail.com>,"Can you please help me understand why the MapType shouldn't be part of
equality tests? Practically, if we are using json line formats, the ideal
equals is every key should map to exactly the same value in both the maps
Which also hold true in Aesthetic case where a MapType can be thought of as
a function (Extensional definition).


"
Lakshmi Rajagopalan <lakshmi@indix.com>,"Mon, 26 Sep 2016 17:34:09 +0530",Re: Deep Equals support on Maptype,Takeshi Yamamuro <linguin.m.s@gmail.com>,"If optimization is the problem, can we use precomputed hashes?



"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Mon, 26 Sep 2016 21:26:28 +0900",Re: Deep Equals support on Maptype,Lakshmi Rajagopalan <lakshmi@indix.com>,"yea, for all I know, there is no reasonable way to implement fast and
efficient equality checks on  ArrayBasedMapData (See also:
https://github.com/apache/spark/pull/13847).





-- 
---
Takeshi Yamamuro
"
Lakshmi Rajagopalan <lakshmi@indix.com>,"Mon, 26 Sep 2016 18:01:22 +0530",Re: Deep Equals support on Maptype,Takeshi Yamamuro <linguin.m.s@gmail.com>,"Ok, but at least we can have a separate binary comparison called
DeepEqualTo as an Expression which at least makes the performance issues
explicit and also have a way to achieve the equality on maps.
In our case, the maps are very small. And this restriction completely
reduces the expressibility of such use cases in spark.


"
Denny Lee <denny.g.lee@gmail.com>,"Mon, 26 Sep 2016 14:47:25 +0000",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"Jeff Zhang <zjffdu@gmail.com>, ""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","+1 (non-binding)

:
 <
arn)
g
er
PDT and
-bin/
-1201/
-docs/
didate,
w features
patches to
"
Yang Cao <cybeater@gmail.com>,"Mon, 26 Sep 2016 23:22:31 +0800",increase efficiency of working with mongo and mysql database,"""user @spark"" <user@spark.apache.org>,
 dev@spark.apache.org","Dear all,

I am currently working with spark 1.6.2, mongodb and mysql. I am stuck with the performance problem. The working scenario is that reading data from mongo to spark and then do some counting work get results(several rows), write to mysql database. With pseudocodeÔºö

val offset = ‚Ä¶
val mongoDF = getMongoDF by strait package(0.11.0).filter(based on offset)

val resDF = doing counting job based on mongoDF

resDF.write().jdbc(info of connection)

Logic is quite simple. But after several test, I found the efficiency of loading from mongo and saving to mysql become bottleneck of my application.

For the job of reading data from mongo, I find it always split into 2 tasks. The first is one is flatMap at MongodbSchema.scala:41 and the second one is aggregate at MongodbSchema.scala:47. In my situation, it looks like this:



It shows that in first step, it only get one task and one executor, which will be extremely slow in working with collection in billions rows. Sometime, it will take 1hr in first step but only several seconds in second.

While in jdbc side, it is similar, saving process also in two steps, one with one task and other with 200, which in DataFrameWriter.scala:311 . 

So my application always get stuck in the stage with only one task. My cluster has free resource and my mongo server also get idle resources. Can someone explain that why these stages only get one executor? Is there any suggestion to speed up the stages? 

I have set the configuration, spark.default.parallelism 400. It looks not help.

Need suggestion. THX.

Best,

Matthew Cao"
Jeremy Davis <jerdavis@speakeasy.net>,"Mon, 26 Sep 2016 09:03:53 -0700",Sliding Window Memory use,dev@spark.apache.org,"
Hi, I posted this to users, but didn‚Äôt get any responses.
I just wanted to highlight what seems like excessive memory use when using sliding windows.
I have attached a test case where starting with certainly less than 1MB of data I can OOM a 10G heap.

Regards,
-JD



--------------

import java.sql.Timestamp

import org.apache.spark.sql.SparkSession
import org.junit.Test
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

import scala.collection.mutable.ArrayBuffer


/**
 * A Small Unit Test to demonstrate Spark Window Functions OOM
 */
class SparkTest {


  @Test
  def testWindows() {
    val sparkSession = SparkSession.builder().master(""local[7]"").appName(""tests"").getOrCreate()
    import sparkSession.implicits._

    println(""Init Dataset"")

    val partitions = (0 until 4)
    val entries = (0 until 6500)

    //val windows = (5 to 15 by 5) //Works
    val windows = (5 to 65 by 5)   //OOM 10G

    val testData = new ArrayBuffer[(String,Timestamp,Double)]


    for( p <- partitions) {
      for( e <- entries ) yield {
        testData += ((""Key""+p,new Timestamp(60000*e),e*2.0))
      }
    }

    val ds = testData.toDF(""key"",""datetime"",""value"")
    ds.show()


    var resultFrame = ds
    resultFrame.schema.fields.foreach(println)


    val baseWin = Window.partitionBy(""key"").orderBy(""datetime"")
    for( win <- windows ) {
      resultFrame = resultFrame.withColumn(""avg""+(-win),avg(""value"").over(baseWin.rowsBetween(-win,0)))
            .withColumn(""stddev""+(-win),stddev(""value"").over(baseWin.rowsBetween(-win,0)))
            .withColumn(""min""+(-win),min(""value"").over(baseWin.rowsBetween(-win,0)))
            .withColumn(""max""+(-win),max(""value"").over(baseWin.rowsBetween(-win,0)))
    }
    resultFrame.show()

  }

}


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 27 Sep 2016 01:05:46 +0900",Re: ArrayType support in Spark SQL,Jason White <jason.white@shopify.com>,"Hi,

Since `Literal#default` can handle array types, it seems there is no strong
reason
for unsupporting the type in `Literal#apply`, that is, `functions.lit`.
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala#L119

// maropu




-- 
---
Takeshi Yamamuro
"
Joseph Bradley <joseph@databricks.com>,"Mon, 26 Sep 2016 09:36:45 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Denny Lee <denny.g.lee@gmail.com>,"+1


r <
m
 PDT and
ndidate,
ew features
 patches to
"
Davies Liu <davies@databricks.com>,"Mon, 26 Sep 2016 09:54:27 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Joseph Bradley <joseph@databricks.com>,"+1 (non-binding)

te:
er
l
m>
k
0 PDT and
c3-bin/
rk-1201/
:
c3-docs/
y
andidate,
new features
a
e patches to

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 26 Sep 2016 10:08:35 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Jacek Laskowski <jacek@japila.pl>,"The part I don't understand is: why do you care so much about the mesos profile?

The same code exists in branch-2.0, it just doesn't need a separate
profile to be enabled (it's part of core). As Sean said, the change in
master was purely organizational, there's no added or lost
functionality.




-- 
Marcelo

---------------------------------------------------------------------


"
Sameer Agarwal <sameer@databricks.com>,"Mon, 26 Sep 2016 10:56:26 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Davies Liu <davies@databricks.com>,"+1 (non-binding)


lier
m
o
f


-- 
Sameer Agarwal
Software Engineer | Databricks Inc.
http://cs.berkeley.edu/~sameerag
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Mon, 26 Sep 2016 20:48:32 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Sameer Agarwal <sameer@databricks.com>,"+1
At last :)

2016-09-26 19:56 GMT+02:00 Sameer Agarwal <sameer@databricks.com>:

:
flier
r
e
e
e



-- 
Maciek Bry≈Ñski
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 26 Sep 2016 11:59:52 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"I'm seeing some test failures with Python 3 that could definitely be
environmental (going to rebuild my virtual env and double check), I'm just
wondering if other people are also running the Python tests on this release
or if everyone is focused on the Scala tests?

rote:

rflier
er
.
be
.
e
d
.



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 26 Sep 2016 13:38:50 -0700","StructuredStreaming Custom Sinks (motivated by Structured Streaming
 Machine Learning)","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark Developers,

After some discussion on SPARK-16407
<https://issues.apache.org/jira/browse/SPARK-16407> (and on the PR
<https://github.com/apache/spark/pull/14691>) we‚Äôve decided to jump back to
the developer list (SPARK-16407
<https://issues.apache.org/jira/browse/SPARK-16407> itself comes from our
early work on SPARK-16424
<https://issues.apache.org/jira/browse/SPARK-16424> to enable ML with the
new Structured Streaming API). SPARK-16407 is proposing to extend the
current DataStreamWriter API to allow users to specify a specific instance
of a StreamSinkProvider
<http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.sources.StreamSinkProvider>
- this makes it easier for users to create sinks that are configured with
things besides strings (for example things like lambdas). An example of
something like this already inside Spark is the ForeachSink.

We have been working on adding support for online learning in Structured
Streaming, similar to what Spark Streaming and MLLib provide today. Details
are available in  SPARK-16424
<https://issues.apache.org/jira/browse/SPARK-16424>. Along the way, we
noticed that there is currently no way for code running in the driver to
access the streaming output of a Structured Streaming query (in our case
ideally as an Dataset or RDD - but regardless of the underlying data
structure). In our specific case, we wanted to update a model in the driver
using aggregates computed by a Structured Streaming query.

A lot of other applications are going to have similar requirements. For
example, there is no way (outside of using private Spark internals)* to
implement a console sink with a user supplied formatting function, or
configure a templated or generic sink at runtime, trigger a custom Python
call-back or even implement the ForeachSink outside of Spark. For work
inside of Spark to enable Structured Streaming with ML we clearly don‚Äôt
need SPARK-16407 <https://issues.apache.org/jira/browse/SPARK-16407> as we
can directly access the internals (although it would be cleaner to not have
to) but if we want to empower people working outside of the Spark codebase
itself with Structured Streaming I think we need to provide some mechanism
for this and it would be great to see what options/ideas the community can
come up with.

<https://issues.apache.org/jira/browse/SPARK-16407> seems to be mostly that
it exposes the Sink API which is implemented using micro-batching, but the
counter argument to this is that the Sink API is already exposed (instead
of passing in an instance the user needs to pass in a class name which is
then created through reflection and has configuration parameters passed in
as a map of strings).

Personally I think we should exposed a more nicely typed API instead of
depending on Strings for all configuration, and that if at some point the
Sink API itself needs to change if/when Spark Streaming moves away from
micro-batching we would still likely want to allow users to provide the
typed interface as well to give Sink creators more flexibility with
configuration.

Now obviously this is based on my understanding of the lay of the land
which could be a little off since the Spark Structured Streaming design
docs and JIRAs don‚Äôt seem to be being actively updated - so I‚Äôd love to
know what assumptions I‚Äôve made that don‚Äôt match the current plans for
structured streaming.

Cheers,

Holden :)

Related Links:

   -

   The JIRA for this proposal
   https://issues.apache.org/jira/browse/SPARK-16407
   -

   The Structured Streaming ML JIRA
   https://issues.apache.org/jira/browse/SPARK-16424
   -


   https://docs.google.com/document/d/1snh7x7b0dQIlTsJNHLr-IxIFgP43RfRV271YK2qGiFQ/edit?usp=sharing
   -

   https://github.com/apache/spark/pull/14691
   -

   https://github.com/holdenk/spark-structured-streaming-ml


*Strictly speaking one _could_ pass in a string of Java code and then
compile it inside the Sink with Janino - but that clearly isn‚Äôt reasonable.

-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Mon, 26 Sep 2016 13:43:51 -0700",Re: ArrayType support in Spark SQL,Takeshi Yamamuro <linguin.m.s@gmail.com>,"Seems fair & easy to support. Can somebody open a JIRA ticket and patch?



"
Xiao Li <gatorsmile@gmail.com>,"Mon, 26 Sep 2016 15:00:44 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Holden Karau <holden@pigscanfly.ca>,"+1

2016-09-26 11:59 GMT-07:00 Holden Karau <holden@pigscanfly.ca>:
t
se
erflier
ver
m>
o.
0.1-rc3-bin/
hespark-1201/
nd
0.1-rc3-docs/
s
d
.

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 26 Sep 2016 15:13:11 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Holden Karau <holden@pigscanfly.ca>,"I do run both Python and Scala. But via iPython/Python2 with my own test
code. Not running the tests from the distribution.
Cheers
<k/>

:

t
se
erflier
ver
m
s
d
.
"
akchin <akchin@us.ibm.com>,"Mon, 26 Sep 2016 16:53:41 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),dev@spark.apache.org,"+1 (non-bind)
-Pyarn -Phadoop-2.7 -Phive -Phive-thriftserver -Psparkr
CentOS 7.2 / openjdk version ""1.8.0_101""




-----
IBM Spark Technology Center 
--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 26 Sep 2016 17:12:10 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Holden Karau <holden@pigscanfly.ca>,"Disclaimer - I am not very closely involved with Structured Streaming
design / development, so this is just my two cents from looking at the
discussion in the linked JIRAs and PRs.

It seems to me there are a couple of issues being conflated here: (a)
is the question of how to specify or add more functionality to the
Sink API such as ability to get model updates back to the driver [A
design issue IMHO] (b) question of how to pass parameters to
DataFrameWriter, esp. strings vs. typed objects and whether the API is
stable vs. experimental

TLDR is that I think we should first focus on refactoring the Sink and
add new functionality after that. Detailed comments below.

Sink design / functionality: Looking at SPARK-10815, a JIRA linked
from SPARK-16407, it looks like the existing Sink API is limited
because it is tied to the RDD/Dataframe definitions. It also has
surprising limitations like not being able to run operators on `data`
and only using `collect/foreach`.  Given these limitations, I think it
makes sense to redesign the Sink API first *before* adding new
functionality to the existing Sink. I understand that we have not
marked this experimental in 2.0.0 -- but I guess since
StructuredStreaming is new as a whole, so we can probably break the
Sink API in a upcoming 2.1.0 release.

As a part of the redesign, I think we need to do two things: (i) come
up with a new data handle that separates RDD from what is passed to
the Sink (ii) Have some way to specify code that can run on the
driver. This might not be an issue if the data handle already has
clean abstraction for this.

Micro-batching: Ideally it would be good to not expose the micro-batch
processing model in the Sink API as this might change going forward.
Given the consistency model we are presenting I think there will be
some notion of batch / time-range identifier in the API. But I think
if we can avoid having hard constraints on where functions will get
run (i.e. on the driver vs. as a part of a job etc.) and when
functions will get run (i.e. strictly after every micro-batch) it
might give us more freedom in improving performance going forward [1].

Parameter passing: I think your point that typed is better than
untyped is pretty good and supporting both APIs isn't necessarily bad
either. My understand of the discussion around this is that we should
do this after Sink is refactored to avoid exposing the old APIs ?

Thanks
Shivaram

[1] FWIW this is something I am looking at and
https://spark-summit.org/2016/events/low-latency-execution-for-apache-spark/
has some details about this.


d to jump
on
407
r
ady
ls
 or
 by
Äôt need
of
es
ing
ed
ich
d
ve to know what
for structured
407
2qGiFQ/edit?usp=sharing
easonable.

---------------------------------------------------------------------


"
Yanbo Liang <ybliang8@gmail.com>,"Mon, 26 Sep 2016 20:30:53 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),akchin <akchin@us.ibm.com>,1
Reynold Xin <rxin@databricks.com>,"Mon, 26 Sep 2016 21:04:52 -0700","Re: renaming ""minor release"" to ""feature release""",Mark Hamstra <mark@clearstorydata.com>,"Yup that's a good point. I think we can easily explain that in the extended
description. I will update the wiki page to reflect that.



o
f
)
e do
‚Äúfeature release‚Äù
mantic
d ‚Äú1.3.0‚Äù would be minor releases.
nor‚Äù is
at introduces
ew
eases from an API
k
d actually call
ation from
 would be a
Äù and am open to
Äúminor‚Äù.
"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 27 Sep 2016 04:22:27 +0000",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),"Krishna Sankar <ksankar42@gmail.com>, Holden Karau <holden@pigscanfly.ca>","+1 on testing with Python2.


st
ase
m>
m
rver
e
t
n
/
1/
s/
gs
f
"
Reynold Xin <rxin@databricks.com>,"Mon, 26 Sep 2016 21:49:22 -0700",Re: Sliding Window Memory use,Jeremy Davis <jerdavis@speakeasy.net>,"I ran it on Databricks community edition which was a local[8] cluster with
6GB of RAM. It ran fine.

That said, looking at the plan, we can definitely simplify this quite a
bit. We had a new Window physical execution node for each window
expression, when we could have collapsed all of them into a single one.



g
f
e(""tests"").getOrCreate()
er(baseWin.rowsBetween(-win,0)))
Between(-win,0)))
n(-win,0)))
n(-win,0)))
"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 27 Sep 2016 14:51:45 +0900",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Denny Lee <denny.g.lee@gmail.com>,"+1 (non-binding)

2016-09-27 13:22 GMT+09:00 Denny Lee <denny.g.lee@gmail.com>:

ust
ease
l>
:
o
-
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 26 Sep 2016 22:54:08 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Denny Lee <denny.g.lee@gmail.com>,"+1 (non-binding)
PySpark Core, ML, MLlib, SQL tests pass w/Python3 on Ubuntu 14.04 - some
intermittent weird failures when running the streaming suite but seem to be
flaky test issue & not a real issue, which is makes sense given how some of
the Python st"
,"Tue, 27 Sep 2016 07:56:40 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),dev@spark.apache.org,"+1 (non binding)

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
"""Jagadeesan As"" <as2@us.ibm.com>","Tue, 27 Sep 2016 06:11:00 +0000",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),dev@spark.apache.org,"+1 (non binding)
 
Cheers,
Jagadeesan A S




From:   Jean-Baptiste Onofr√© <jb@nanthrax.net>
To:     dev@spark.apache.org
Date:   27-09-16 11:27 AM
Subject:        Re: [VOTE] Release Apache Spark 2.0.1 (RC3)



+1 (non binding)

Regards"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Tue, 27 Sep 2016 00:01:32 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),dev <dev@spark.apache.org>,"
+1 (non-binding)

-suresh


<ksankar42@gmail.com
my
env
are
everyone
<mailto:sameer@databricks.com>>>:
Bradley
Lee
<mailto:petermaxlee@gmail.com>>>
Herman
Ricardo

Zaharia
Rosen
<mailto:joshrosen@databricks.com>>>
<mailto:yhuai@databricks.com>>>
1.8.0_77 /
<mailto:rxin@databricks.com>>>
3+1
package
v2.0.1-rc3
290
<https://s.apache.org/spark-2.0.1-jira>
<https://s.apache.org/spark-2.0.1-jira>>
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-bin/ <http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-bin/>
<http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-bin/ <http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-bin/>>
with
https://people.apache.org/keys/committer/pwendell.asc <https://people.apache.org/keys/committer/pwendell.asc>
<https://people.apache.org/keys/committer/pwendell.asc <https://people.apache.org/keys/committer/pwendell.asc>>
https://repository.apache.org/content/repositories/orgapachespark-1201/ <https://repository.apache.org/content/repositories/orgapachespark-1201/>
<https://repository.apache.org/content/repositories/orgapachespark-1201/ <https://repository.apache.org/content/repositories/orgapachespark-1201/>>
to
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-docs/ <http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-docs/>
<http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-docs/ <http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-docs/>>
release?
can
workload
from
this
in
missing
release.
for
as
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>>
<http://cs.berkeley.edu/~sameerag>
<http://cs.berkeley.edu/~sameerag>>
<https://twitter.com/holdenkarau>
<https://twitter.com/holdenkarau>>

"
Jacek Laskowski <jacek@japila.pl>,"Tue, 27 Sep 2016 09:53:43 +0200",Should LeafExpression have children final override (like Nondeterministic)?,dev <dev@spark.apache.org>,"Hi,

Perhaps nitpicking...you've been warned.

While reviewing expressions in Catalyst I've noticed some
inconsistency, i.e. Nondeterministic trait has two methods
deterministic and foldable final override while LeafExpression does
not have children final (at the very least).

My thinking is that LeafExpression is to mark left expressions so
children is assumed to be Nil.

Should children be final in LeafExpression? Why not? #curious

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Sep 2016 00:57:40 -0700",Re: Should LeafExpression have children final override (like Nondeterministic)?,Jacek Laskowski <jacek@japila.pl>,"Yes - same thing with children in UnaryExpression, BinaryExpression.
Although I have to say the utility isn't that big here.



"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 27 Sep 2016 17:34:42 +0900",Re: ArrayType support in Spark SQL,Reynold Xin <rxin@databricks.com>,"Done: https://issues.apache.org/jira/browse/SPARK-17683




-- 
---
Takeshi Yamamuro
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 27 Sep 2016 07:02:13 -0700 (MST)",https://issues.apache.org/jira/browse/SPARK-17691,dev@spark.apache.org,"Hi,

I wanted to try to implement https://issues.apache.org/jira/browse/SPARK-17691.
So I started by looking at the implementation of collect_list. My idea was, do the same as they but when adding a new element, if there are already more than the threshold, remove one instead.
The problem with this is that since collect_list has no partial aggregation we would end up shuffling all the data anyway. So while it would mean the actual resulting column might be smaller, the whole process would be as expensive as collect_list.
So I thought of adding partial aggregation. The problem is that the merge function receives a buffer which is in a row format. Collect_list doesn't use the buffer and uses its own data structure for collecting the data.
I can change the implementation to use a spark ArrayType instead, however, since ArrayType is immutable it would mean that I would need to copy it whenever I do anything.
Consider the simplest implementation of the update function:
If there are few elements => add an element to the array (if I use regular Array this would mean copy as I grow it which is fine for this stage)
If there are enough elements => we do not grow the array. Instead we need to decide what to replace. If we want to have the top 10 for example and there are 10 elements, we need to drop the lowest and put the new one.
This means that if we simply loop across the array we would create a new copy and pay the copy + loop. If we keep it sorted then adding, sorting and removing the low one means 3 copies.
If I would have been able to use scala's array then I would basically copy whenever I grow and then when we grown to the max, all I would need to do is REPLACE the relevant element which is much cheaper.

The only other solution I see is to simply provide ""take first N"" agg function and have the user sort beforehand but this seems a bad solution to me both because sort is expensive and because if we do multiple aggregations we can't sort in two different ways.


I can't find a way to convert an internal buffer the way collect_list does it to an internal buffer before the merge.
I also can't find any way to use an array in the internal buffer as a mutable array. If I look at GenericMutableRow implementation then updating an array means creating a new one. I thought maybe of adding a function update_array_element which would change the relevant element (and similarly get_array_element to get an array element) which would allow to easily make the array mutable but if I look at the documentation it states this is not allowed.

Can anyone give me a tip on where to try to go from here?




--"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 27 Sep 2016 08:20:10 -0700",Re: https://issues.apache.org/jira/browse/SPARK-17691,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Hi Asaf,

The current collect_list/collect_set implementations have room for
improvement. We did not implement partial aggregation for these, because
the idea of a partial aggregation is that we can reduce network traffic (by
shipping fewer partially aggregated buffers); this does not really apply to
a collect_list where the typical use case is to change the shape of the
data.

I think you have two simple options here:

   1. In the latest branch we added a TypedImperativeAggregate. This allows
   you to use any object as an aggregation buffer. You will need to do some
   serialization though. The ApproximatePercentile aggregate function uses
   this technique.
   2. Exploit the fact that you want collect a limited amount of elements.
   You can use a row a as the buffer. This is much easier to work with. See
   HyperLogLogPlusPlus for an example of this.


HTH
-Herman


s
‚Äôt
,
lar
ed
nd
ally copy
‚Äù agg
to
list does
as a
g
ly
ke
t
pache-org-jira-browse-SPARK-17691-tp19107.html>
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 27 Sep 2016 10:18:44 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Reynold Xin <rxin@databricks.com>,"I've got a couple of build niggles that should really be investigated at
some point (what look to be OOM issues in spark-repl when building and
testing with mvn in a single pass instead of in two passes with -DskipTests
first; the killing of ssh sessions by YarnClusterSuite), but these aren't
anything that should hold up the release.

+1


"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Sep 2016 12:06:38 -0700",[discuss] Spark 2.x release cadence,"""dev@spark.apache.org"" <dev@spark.apache.org>","We are 2 months past releasing Spark 2.0.0, an important milestone for the
project. Spark 2.0.0 deviated (took 6 month from the regular release
cadence we had for the 1.x line, and we never explicitly discussed what the
release cadence should look like for 2.x. Thus this email.

During Spark 1.x, roughly every three months we make a new 1.x feature
release (e.g. 1.5.0 comes out three months after 1.4.0). Development
happened primarily in the first two months, and then a release branch was
cut at the end of month 2, and the last month was reserved for QA and
release preparation.

During 2.0.0 development, I really enjoyed the longer release cycle because
there was a lot of major changes happening and the longer time was critical
for thinking through architectural changes as well as API design. While I
don't expect the same degree of drastic changes in a 2.x feature release, I
do think it'd make sense to increase the length of release cycle so we can
make better designs.

My strawman proposal is to maintain a regular release cadence, as we did in
Spark 1.x, and increase the cycle from 3 months to 4 months. This
effectively gives us ~50% more time to develop (in reality it'd be slightly
less than 50% since longer dev time also means longer QA time). As for
maintenance releases, I think those should still be cut on-demand, similar
to Spark 1.x, but more aggressively.

To put this into perspective, 4-month cycle means we will release Spark
2.1.0 at the end of Nov or early Dec (and branch cut / code freeze at the
end of Oct).

I am curious what others think.
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 27 Sep 2016 13:19:00 -0700",Re: [discuss] Spark 2.x release cadence,Reynold Xin <rxin@databricks.com>,"+1 I think having a 4 month window instead of a 3 month window sounds good.

However I think figuring out a timeline for maintenance releases would
also be good. This is a common concern that comes up in many user
threads and it'll be better to have some "
Sean Owen <sowen@cloudera.com>,"Tue, 27 Sep 2016 16:26:01 -0400",Re: [discuss] Spark 2.x release cadence,Reynold Xin <rxin@databricks.com>,"+1 -- I think the minor releases were taking more like 4 months than 3
months anyway, and it was good for the reasons you give. This reflects
reality and is a good thing. All the better if we then can more
comfortably really follow the timeline.


-------"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Sep 2016 13:31:28 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Mark Hamstra <mark@clearstorydata.com>,"Actually I'm going to have to -1 the release myself. Sorry for crashing the
party, but I saw two super critical issues discovered in the last 2 days:

https://issues.apache.org/jira/browse/SPARK-17666  -- this would eventually
hang Spark when running against S3 (and many other storage systems)

https://issues.apache.org/jira/browse/SPARK-17673  -- this is a correctness
issue across all non-file data sources.

If we go ahead and release 2.0.1 based on this RC, we would need to cut
2.0.2 immediately.






"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 27 Sep 2016 14:01:22 -0700",Re: [discuss] Spark 2.x release cadence,Reynold Xin <rxin@databricks.com>,"+1

And I'll dare say that for those with Spark in production, what is more
important is that maintenance releases come out in a timely fashion than
that new features are released one month sooner or later.


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 27 Sep 2016 14:05:42 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Reynold Xin <rxin@databricks.com>,"If we're going to cut another RC, then it would be good to get this in as
well (assuming that it is merged shortly):
https://github.com/apache/spark/pull/15213

It's not a regression, and it shouldn't happen too often, but when failed
stages don't get resubmitted it is a fairly significant issue.


"
Jeremy Davis <jerdavis@speakeasy.net>,"Tue, 27 Sep 2016 14:37:23 -0700",Re: Sliding Window Memory use,Reynold Xin <rxin@databricks.com>,"Thanks Reynold, I appreciate it, that was my sense as well.
I will take a look at the community edition.

Btw, this is a profiler view a few moments before it went OOM‚Ä¶



-JD





with 6GB of RAM. It ran fine.
a bit. We had a new Window physical execution node for each window expression, when we could have collapsed all of them into a single one.
using sliding windows.
1MB of data I can OOM a 10G heap.
SparkSession.builder().master(""local[7]"").appName(""tests"").getOrCreate()
resultFrame.withColumn(""avg""+(-win),avg(""value"").over(baseWin.rowsBetween(-win,0)))
.withColumn(""stddev""+(-win),stddev(""value"").over(baseWin.rowsBetween(-win,0)))
.withColumn(""min""+(-win),min(""value"").over(baseWin.rowsBetween(-win,0)))
.withColumn(""max""+(-win),max(""value"").over(baseWin.rowsBetween(-win,0)))

"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Sep 2016 17:37:35 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Mark Hamstra <mark@clearstorydata.com>,"So technically the vote has passed, but IMHO it does not make sense to
release this and then immediately release 2.0.2. I will work on a new RC
once SPARK-17666 and SPARK-17673 are fixed.

Please shout if you disagree.



"
Felix Cheung <felixcheung_m@hotmail.com>,"Wed, 28 Sep 2016 03:54:29 +0000",Re: [discuss] Spark 2.x release cadence,Reynold Xin <rxin@databricks.com>,"+1 on longer release cycle at schedule and more maintenance releases.


_____________________________
From: Mark Hamstra <mark@clearstorydata.com<mailto:mark@clearstorydata.com>Sent: Tuesday, September 27, 2016 2:01 PM
Subject: Re: [discuss] Spark 2.x rel"
Aravindh <mail@aravindh.io>,"Tue, 27 Sep 2016 21:08:47 -0700 (MST)","Help required in validating an architecture using Structured
 Streaming",dev@spark.apache.org,"Hi, We are building an internal analytics application. Kind of an event
store. We have all the basic analytics use cases like filtering,
aggregation, segmentation etc. So far our architecture used ElasticSearch
is an event should be available for querying within 5 seconds of the event.
We were thinking of a lambda architecture where streaming data still goes to
elastic search (only 1 day's data), batch pipeline goes to s3. Every day
we were not able to solve was when a query comes, how to aggregate results
from 2 data sources (ES for current data & s3 for old data). We felt this
approach wont scale.

Spark Structured Streaming seems to solve this. Correct me if i am wrong.
With structured streaming, will the following architecture work?
Read data from kafka using spark. For every batch of data, do the
transformations and store in s3. But when a query comes, query from both s3
& in memory batch at the same time. Will this approach work? Also one more
condition is, querying should respond immediately. With a max latency of 1s
for simple queries and 5s for complex queries. If the above method is not
the right way, please suggest an alternative to solve this.

Thanks
Aravindh.S



--

---------------------------------------------------------------------


"
"""Aditya"" <aditya.calangutkar@augmentiq.co.in>","Wed, 28 Sep 2016 12:17:29 +0530",Spark Executor Lost issue,"user@spark.mithiskyconnect.com,
	=?us-ascii?Q?user=40spark=2Eapache=2Eorg?= <dev@spark.apache.org>","I have a spark job which runs fine for small data. But when data 
increases it gives executor lost error.My executor and driver memory are 
set at its highest point. I have also tried increasing--conf 
spark.yarn.executor.memoryOverhead=600but still not able to fix the 
problem. Is there any other solution to fix the problem?


"
"""Aditya"" <aditya.calangutkar@augmentiq.co.in>","Wed, 28 Sep 2016 12:33:51 +0530",Re: Spark Executor Lost issue,"""Sushrut Ikhar"" <sushrutikhar94@gmail.com>","Thanks Sushrut for the reply.

Currently I have not defined spark.default.parallelism property.
Can you let me know how much should I set it to?


Regards,
Aditya Calangutkar




"
"""Aditya"" <aditya.calangutkar@augmentiq.co.in>","Wed, 28 Sep 2016 12:38:57 +0530",Re: Spark Executor Lost issue,"""Sushrut Ikhar"" <sushrutikhar94@gmail.com>",":



"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 28 Sep 2016 09:18:02 +0200","java.util.NoSuchElementException when serializing Map with default
 value","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

I suspect there is no point in submitting a JIRA to fix this (not a
Spark issue?) but I would like to know if this problem is documented
anywhere. Somehow Kryo is loosing default value during serialization:

    scala> import org.apache.spark.{SparkContext, SparkConf}
    import org.apache.spark.{SparkContext, SparkConf}

    scala> val aMap = Map[String, Long]().withDefaultValue(0L)
    aMap: scala.collection.immutable.Map[String,Long] = Map()

    scala> aMap(""a"")
    res6: Long = 0

    scala> val sc = new SparkContext(new
    SparkConf().setAppName(""bar"").set(""spark.serializer"",
    ""org.apache.spark.serializer.KryoSerializer""))

    scala> sc.parallelize(Seq(aMap)).map(_(""a"")).first
    16/09/28 09:13:47 ERROR Executor: Exception in task 2.0 in stage 2.0
    (TID 7)
    java.util.NoSuchElementException: key not found: a

while Java serializer works just fine:

    scala> val sc = new SparkContext(new
    SparkConf().setAppName(""bar"").set(""spark.serializer"",
    ""org.apache.spark.serializer.JavaSerializer""))

    scala> sc.parallelize(Seq(aMap)).map(_(""a"")).first
    res9: Long = 0

-- 
Best regards,
Maciej

"
Grant Digby <digbyg@gmail.com>,"Wed, 28 Sep 2016 01:46:04 -0700 (MST)",IllegalArgumentException: spark.sql.execution.id is already set,dev@spark.apache.org,"Hi,

We've received the following error a handful of times and once it's occurred
all subsequent queries fail with the same exception until we bounce the
instance:

IllegalArgumentException: spark.sql.execution.id is already set
        at
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
        at
org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)

ForkJoinWorkerThreads call into SQLExecution#withNewExecutionId, are
assigned an execution Id into their InheritableThreadLocal and this is later
cleared in the finally block.
I've noted that these ForkJoinWorkerThreads can create additional
ForkJoinWorkerThreads and (as of SPARK-10563) the child threads receive a
copy of the parent's properties.
It seems that Prior to SPARK-10563, clearing the parent's executionId would
have cleared the child's, but now it's a copy of the properties the child's
executionId is never cleared leading to the above exception. 
I'm yet to recreate the issue locally, whilst I've seen
ForkJoinWorkerThreads creating others and the properties being copied across
I've not seen this from within the body of withNewExecutionId.

Does this all sound reasonable? 
Our plan for a short term work around is to allow the condition to arise but
remove the execution.id from the thread local before throwing the
IllegalArgumentException so it succeeds on re-try.




--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 28 Sep 2016 05:52:20 -0400",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Reynold Xin <rxin@databricks.com>,"(Process-wise there's no problem with that. The vote is open for at
least 3 days and ends when the RM says it ends. So it's valid anyway
as the vote is still open.)


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 28 Sep 2016 06:11:06 -0400","Re: java.util.NoSuchElementException when serializing Map with
 default value",Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"My guess is that Kryo specially handles Maps generically or relies on
some mechanism that does, and it happens to iterate over all
key/values as part of that and of course there aren't actually any
key/values in the map. The Java serialization is a much more literal
(expensive) field-by-field serialization which works here because
there's no special treatment. I think you could register a custom
serializer that handles this case. Or work around it in your client
code. I know there have been other issues with Kryo and Map because,
for example, sometimes a Map in an application is actually some
non-serializable wrapper view.


---------------------------------------------------------------------


"
Marcin Tustin <mtustin@handybook.com>,"Wed, 28 Sep 2016 09:26:16 -0400",Re: IllegalArgumentException: spark.sql.execution.id is already set,Grant Digby <digbyg@gmail.com>,"I've solved this in the past by using a thread pool which runs clean up
code on thread creation, to clear out stale values.



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
"""Aditya"" <aditya.calangutkar@augmentiq.co.in>","Wed, 28 Sep 2016 19:30:32 +0530",Re: Spark Executor Lost issue,"""Sushrut Ikhar"" <sushrutikhar94@gmail.com>","Hi All,

Any updates on this?




"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Wed, 28 Sep 2016 07:03:24 -0700 (MST)",Broadcast big dataset,dev@spark.apache.org,"Hi Devs
 In my application, i just broadcast a dataset(about 500M) to  the
ececutors(100+), I got a java heap error
Jmartad-7219.hadoop.jd.local:53591 (size: 4.0 MB, free: 3.3 GB)
16/09/28 15:56:48 INFO BlockManagerInfo: Added broadcast_9_piece19 in memory
on BJHC-Jmartad-9012.hadoop.jd.local:53197 (size: 4.0 MB, free: 3.3 GB)
16/09/28 15:56:49 INFO BlockManagerInfo: Added broadcast_9_piece8 in memory
on BJHC-Jmartad-84101.hadoop.jd.local:52044 (size: 4.0 MB, free: 3.3 GB)
16/09/28 15:56:58 INFO BlockManagerInfo: Removed broadcast_8_piece0 on
172.22.176.114:37438 in memory (size: 2.7 KB, free: 3.1 GB)
16/09/28 15:56:58 WARN TaskSetManager: Lost task 125.0 in stage 7.0 (TID
130, BJHC-Jmartad-9376.hadoop.jd.local): java.lang.OutOfMemoryError: Java
heap space
	at java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:3465)
	at
java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3271)
	at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1789)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706)

My configuration is 4G memory in driver.  Any advice is appreciated.
Thank you!



--

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 28 Sep 2016 14:30:19 +0000 (UTC)",Re: [discuss] Spark 2.x release cadence,"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 to 4 months.
Tom 

 

 We are 2 months past releasing Spark 2.0.0, an important milestone for the project. Spark 2.0.0 deviated (took 6 month from the regular release cadence we had for the 1.x line, and we never explicitly discussed what the release c"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Wed, 28 Sep 2016 17:16:01 +0000",Re: Using Spark as a Maven dependency but with Hadoop 2.6,Sean Owen <sowen@cloudera.com>,"ok, don't you think it could be published with just different classifiers
<classifier>hadoop-2.6</classifier><classifier>hadoop-2.4</classifier>
<classifier>hadoop-2.2</classifier> being the current default.
So for now, I should just override spark 2.0.0's dependencies with the ones
defined in the pom profile
 





Owen sowen@cloudera.com
There can be just one published version of the Spark artifacts and they have to
depend on something, though in truth they'd be binary-compatible with anything
2.2+. So you merely manage the dependency versions up to the desired version in
your <dependencyManagement>.
Hi,when we fetch Spark 2.0.0 as maven dependency then we automatically end up
with hadoop 2.2 as a transitive dependency, I know multiple profiles are used to
generate the different tar.gz bundles that we can download, Is there by any
chance publications of Spark 2.0.0 with different classifier according to
different versions of Hadoop available ?
Thanks for your time !
Olivier Girardot

 


Olivier Girardot| Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
Sean Owen <sowen@cloudera.com>,"Wed, 28 Sep 2016 13:21:22 -0400",Re: Using Spark as a Maven dependency but with Hadoop 2.6,Olivier Girardot <o.girardot@lateral-thoughts.com>,"I guess I'm claiming the artifacts wouldn't even be different in the first
place, because the Hadoop APIs that are used are all the same across these
versions. That would be the thing that makes you need multiple versions of
the artifact under multiple classifiers.


he
are
r
"
Michael Armbrust <michael@databricks.com>,"Wed, 28 Sep 2016 10:52:21 -0700",Spark SQL JSON Column Support,"""dev@spark.apache.org"" <dev@spark.apache.org>","Spark SQL has great support for reading text files that contain JSON data.
However, in many cases the JSON data is just one column amongst others.
This is particularly true when reading from sources such as Kafka. This PR
<https://github.com/apache/spark/pull/15274> adds a new functions
from_json that
converts a string column into a nested StructType with a user specified
schema, using the same internal logic as the json Data Source.

Would love to hear any comments / suggestions.

Michael
"
Nathan Lande <nathanlande@gmail.com>,"Wed, 28 Sep 2016 11:02:45 -0700",Re: Spark SQL JSON Column Support,Michael Armbrust <michael@databricks.com>,"We are currently pulling out the JSON columns, passing them through
read.json, and then joining them back onto the initial DF so something like
from_json would be a nice quality of life improvement for us.


"
Burak Yavuz <brkyvz@gmail.com>,"Wed, 28 Sep 2016 11:04:59 -0700",Re: Spark SQL JSON Column Support,Nathan Lande <nathanlande@gmail.com>,"I would really love something like this! It would be great if it doesn't
throw away corrupt_records like the Data Source.


"
Michael Segel <msegel_hadoop@hotmail.com>,"Wed, 28 Sep 2016 18:15:05 +0000",Re: Spark SQL JSON Column Support,Michael Armbrust <michael@databricks.com>,"Silly  question?
When you talk about ‚Äòuser specified schema‚Äô do you mean for the user to supply an additional schema, or that you‚Äôre using the schema that‚Äôs described by the JSON string?
(or both? [either/or] )

Thx

On Sep 28, 2016, at 12:52 PM, Michael Armbrust <michael@databricks.com<mailto:michael@databricks.com>> wrote:

Spark SQL has great support for reading text files that contain JSON data. However, in many cases the JSON data is just one column amongst others. This is particularly true when reading from sources such as Kafka. This PR<https://github.com/apache/spark/pull/15274> adds a new functions from_json that converts a string column into a nested StructType with a user specified schema, using the same internal logic as the json Data Source.

Would love to hear any comments / suggestions.

Michael

"
Andrew Duffy <root@aduffy.org>,"Wed, 28 Sep 2016 18:24:51 +0000 (UTC)",Re: Broadcast big dataset,"WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,  <dev@spark.apache.org>","Have you tried upping executor memory? There's a separate spark conf for that: spark.executor.memory
In general driver configurations don't automatically apply to executors.















Hi Devs
 In my application, i just broadcast a dataset(about 500M) to  the
ececutors(100+), I got a java heap error
Jmartad-7219.hadoop.jd.local:53591 (size: 4.0 MB, free: 3.3 GB)
16/09/28 15:56:48 INFO BlockManagerInfo: Added broadcast_9_piece19 in memory
on BJHC-Jmartad-9012.hadoop.jd.local:53197 (size: 4.0 MB, free: 3.3 GB)
16/09/28 15:56:49 INFO BlockManagerInfo: Added broadcast_9_piece8 in memory
on BJHC-Jmartad-84101.hadoop.jd.local:52044 (size: 4.0 MB, free: 3.3 GB)
16/09/28 15:56:58 INFO BlockManagerInfo: Removed broadcast_8_piece0 on
172.22.176.114:37438 in memory (size: 2.7 KB, free: 3.1 GB)
16/09/28 15:56:58 WARN TaskSetManager: Lost task 125.0 in stage 7.0 (TID
130, BJHC-Jmartad-9376.hadoop.jd.local): java.lang.OutOfMemoryError: Java
heap space
	at java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:3465)
	at
java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3271)
	at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1789)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706)

My configuration is 4G memory in driver.  Any advice is appreciated.
Thank you!



--

---------------------------------------------------------------------






"
Michael Armbrust <michael@databricks.com>,"Wed, 28 Sep 2016 12:17:09 -0700",Re: Spark SQL JSON Column Support,Burak Yavuz <brkyvz@gmail.com>,"Burak, you can configure what happens with corrupt records for the
datasource using the parse mode.  The parse will still fail, so we can't
get any data out of it, but we do leave the JSON in another column for you
to inspect.

In the case of this function, we'll just return null if its unparable.  You
could filter for rows where the function returns null and inspect the input
if you want to see whats going wrong.

When you talk about ‚Äòuser specified schema‚Äô do you mean for the user to
‚Äôs


I mean we don't do schema inference (which we might consider adding, but
that would be a much larger change than this PR).  You need to construct a
StructType that says what columns you want to extract from the JSON column
and pass that in.  I imagine in many cases the user will run schema
inference ahead of time and then encode the inferred schema into their
program.



ike
ka. This
"
Joseph Bradley <joseph@databricks.com>,"Wed, 28 Sep 2016 13:35:27 -0700",Re: [discuss] Spark 2.x release cadence,Tom Graves <tgraves_cs@yahoo.com>,"+1 for 4 months.  With QA taking about a month, that's very reasonable.

My main ask (especially for MLlib) is for contributors and committers to
take extra care not to delay on updating the Programming Guide for new
APIs.  Documentation debt often collec"
Jakob Odersky <jakob@odersky.com>,"Wed, 28 Sep 2016 17:05:24 -0700","Re: java.util.NoSuchElementException when serializing Map with
 default value",Sean Owen <sowen@cloudera.com>,"I agree with Sean's answer, you can check out the relevant serializer
here https://github.com/twitter/chill/blob/develop/chill-scala/src/main/scala/com/twitter/chill/Traversable.scala


---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Wed, 28 Sep 2016 18:13:37 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC3),Sean Owen <sowen@cloudera.com>,"+1

I know this is cancelled, but FYI, RC3 passes mesos/spark integration tests




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Sep 2016 19:14:31 -0700",[VOTE] Release Apache Spark 2.0.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.1. The vote is open until Sat, Oct 1, 2016 at 20:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.1
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.1-rc4
(933d2c1ea4e5f5c4ec8d375b5ccaa4577ba4be38)

This release candidate resolves 301 issues:
https://s.apache.org/spark-2.0.1-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc4-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1203/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc4-docs/


Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.0.

Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series.  Bugs already present
in 2.0.0, missing features, or bugs related to new features will not
necessarily block this release.

Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.2, rather than 2.0.1. If a new RC
(i.e. RC5) is cut, I will change the fix version of those patches to 2.0.1.
"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Wed, 28 Sep 2016 20:32:22 -0700 (MST)",Re: Broadcast big dataset,dev@spark.apache.org,"First thank you very muchÔºÅ
  My executor memeory is also 4GÔºå but my spark version is 1.5. Does spark
version make a trouble?




--
3.nabble.com/Broadcast-big-dataset-tp19127p19143.html
om.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Sep 2016 23:10:15 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","I will kick it off with my own +1.



"
Grant Digby <digbyg@gmail.com>,"Thu, 29 Sep 2016 02:18:43 -0700 (MST)",Re: IllegalArgumentException: spark.sql.execution.id is already set,dev@spark.apache.org,"Yeah that would work although I was worried that they used
InheritableThreadLocal vs Threadlocal because they did want the child
threads to inherit the parent's executionId, maybe to stop the child threads
from kicking off their own queries whilst working for the parent. I think
the fix would be to somehow ensure the child's execution.id was cleared when
the parents is.



--

---------------------------------------------------------------------


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 29 Sep 2016 09:37:57 +0000",Re: Using Spark as a Maven dependency but with Hadoop 2.6,Sean Owen <sowen@cloudera.com>,"I know that the code itself would not be the same, but it would be useful to at
least have the pom/build.sbt transitive dependencies different when fetching the
artifact with a specific classifier, don't you think ?For now I've overriden
them myself using the dependency versions defined in the pom.xml of spark.So
it's not a blocker issue, it may be useful to document it, but a blog post would
be sufficient I think.
 





I guess I'm claiming the artifacts wouldn't even be different in the first
place, because the Hadoop APIs that are used are all the same across these
versions. That would be the thing that makes you need multiple versions of the
artifact under multiple classifiers.
Olivier Girardot <
ok, don't you think it could be published with just different classifiers
<classifier>hadoop-2.6</classifier><classifier>hadoop-2.4</classifier>
<classifier>hadoop-2.2</classifier> being the current default.
So for now, I should just override spark 2.0.0's dependencies with the ones
defined in the pom profile

 





Owen sowen@cloudera.com
There can be just one published version of the Spark artifacts and they have to
depend on something, though in truth they'd be binary-compatible with anything
2.2+. So you merely manage the dependency versions up to the desired version in
your <dependencyManagement>.
Hi,when we fetch Spark 2.0.0 as maven dependency then we automatically end up
with hadoop 2.2 as a transitive dependency, I know multiple profiles are used to
generate the different tar.gz bundles that we can download, Is there by any
chance publications of Spark 2.0.0 with different classifier according to
different versions of Hadoop available ?
Thanks for your time !
Olivier Girardot

 


Olivier Girardot| Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
 


Olivier Girardot| Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
Sean Owen <sowen@cloudera.com>,"Thu, 29 Sep 2016 05:42:34 -0400",Re: Using Spark as a Maven dependency but with Hadoop 2.6,Olivier Girardot <o.girardot@lateral-thoughts.com>,"No, I think that's what dependencyManagent (or equivalent) is definitely for.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 29 Sep 2016 12:37:07 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"Hi Marcelo,

The reason I asked about the mesos profile was that I thought it was
part of the branch already and wondered why nobody used it to compile
Spark with all the code available.

I do understand no code changes were introduced during this profile
maintenance, but with the profile that code does not get compiled
unless you enable the profile explicitly. I've learnt it's not part of
the release, though.

Thanks for all the clarifications! I appreciate your patience dealing
with my questions a lot! Thanks.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Thu, 29 Sep 2016 13:05:18 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Built (-Phadoop-2.7 -Dhadoop.version=2.7.3 -Phive -Phive-thriftserver
-Pyarn) and tested on:
- Ubuntu 16.04 / OpenJDK 1.8.0_91
- CentOS / Oracle Java 1.7.0_55

No regressions from 2.0.0 found while running our workloads (Python API)



"
Marcin Tustin <mtustin@handybook.com>,"Thu, 29 Sep 2016 07:12:17 -0400",Re: IllegalArgumentException: spark.sql.execution.id is already set,Grant Digby <digbyg@gmail.com>,"That's not possible because inherited primitive values are copied, not
shared. Clearing problematic values on thread creation should eliminate
this problem.

As to your idea as a design goal, that's also not desirable, because Java
thread pooling is implemented in a very surprising way. The standard Java
thread pool doesn't use a master thread to spawn new threads, instead
pooled worker threads run the pooling code. This means that there is no
specific relationship between parent and child threads other than the fact
of one spawning the other. Essentially a thread's parent thread is
completely arbitrary, and for that reason inherited clearing of values is
undesirable.

I can try to dig up the PR where the reasons for the current design were
explained to me. My idea was to implement explicit inheritable
and non inheritable local properties. At this point I'm not deep enough in
the issues to have a strong opinion on whether that's a good design, and I
for one would very much welcome your design ideas on this.



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
"""Jagadeesan As"" <as2@us.ibm.com>","Thu, 29 Sep 2016 16:48:33 +0530",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Ricardo Almeida <ricardo.almeida@actnowib.com>,"+1 (non binding)

Ubuntu 14.04.2/openjdk  ""1.8.0_72""
(-Pyarn -Phadoop-2.7 -Psparkr -Pkinesis-asl -Phive-thriftserver)
 
Cheers,
Jagadeesan A S



From:   Ricardo Almeida <ricardo.almeida@actnowib.com>
To:     ""dev@spark.apache.org"" <dev@spark.apache.org>
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 29 Sep 2016 16:08:51 +0200",Dynamic allocation / killing executors work? Perhaps it's just web UI?,dev <dev@spark.apache.org>,"Hi,

I'm doubtful that dynamic allocation / killing executors work in
Standalone and YARN as far as web UI's concerned (perhaps it's just
web UI).

I can successfully request as many executors as I want using
sc.requestTotalExecutors and they show up nicely in the web UI as
ACTIVE, but whenever I request sc.killExecutor(s) I can see the
following in YARN logs:

INFO YarnAllocator: Received 2 containers from YARN, launching
executors on 2 of them.
INFO YarnAllocator: Driver requested a total number of 3 executor(s).
INFO YarnAllocator: Canceling requests for 1 executor container(s) to
have a new desired total 3 executors.
INFO YarnAllocator: Driver requested a total number of 2 executor(s).
DEBUG YarnAllocator: Completed 1 containers
DEBUG YarnAllocator: Finished processing 1 completed containers.
Current running executor count: 2.
INFO YarnAllocator: Driver requested a total number of 1 executor(s).
INFO YarnAllocator: Will request 1 executor container(s), each with 1
core(s) and 1408 MB memory (including 384 MB of overhead)
INFO YarnAllocator: Submitted 1 unlocalized container requests.
DEBUG YarnAllocator: Completed 2 containers
DEBUG YarnAllocator: Finished processing 2 completed containers.
Current running executor count: 0.
DEBUG YarnAllocator: Allocated containers: 1. Current executor count:
0. Cluster resources: <memory:5120, vCores:1>.
INFO YarnAllocator: Launching container
container_1475155700518_0003_01_000006 on host 192.168.65.1
INFO YarnAllocator: Received 1 containers from YARN, launching
executors on 1 of them.
INFO YarnAllocator: Driver requested a total number of 0 executor(s).
DEBUG YarnAllocator: Completed 1 containers
DEBUG YarnAllocator: Finished processing 1 completed containers.
Current running executor count: 0.

But the web UI shows all the executors as ACTIVE in Status column in
Executors tab.

Can anyone confirm that dynamic allocation works fine and web UI shows
the current status of executors?

What other information do you want me to offer to verify it. I'm
doubtful that web UI shows what it's supposed to show regarding
executors.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 29 Sep 2016 10:02:23 -0500",Re: [discuss] Spark 2.x release cadence,Joseph Bradley <joseph@databricks.com>,"Regarding documentation debt, is there a reason not to deploy
documentation updates more frequently than releases?  I recall this
used to be the case.


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 29 Sep 2016 10:08:03 -0500",Re: Spark SQL JSON Column Support,Michael Armbrust <michael@databricks.com>,"Will this be able to handle projection pushdown if a given job doesn't
utilize all the columns in the schema?  Or should people have a
per-job schema?

get
ou
ut
for the user to
t‚Äôs
a
n
like
fka.
nto a
l

---------------------------------------------------------------------


"
Marcin Tustin <mtustin@handybook.com>,"Thu, 29 Sep 2016 11:58:38 -0400",Re: IllegalArgumentException: spark.sql.execution.id is already set,Grant Digby <digbyg@gmail.com>,"And that PR as promised: https://github.com/apache/spark/pull/12456



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Samy Dindane <samy@dindane.com>,"Thu, 29 Sep 2016 18:16:39 +0200",Questions about DataFrame's filter(),dev@spark.apache.org,"Hi,

I noticed that the following code compiles:

   val df = 
spark.read.format(""com.databricks.spark.avro"").load(""/tmp/whatever/output"")
   val count = df.filter(x => x.getAs[Int](""day"") == 2).count

It surprises me as `filter()` takes a Column, not a `Row => Boolean`.

Also, this code returns the right result, but takes 1m30 to run (while 
it takes less than 1 second when using `$""day"" === 2`) and gives the 
error pasted in the bottom of this message.

I was just wondering why it does work (implicit conversion?), why it is 
slow, and why the error occurs.
Can someone explain please?

Thank you,

Samy

--

[error] org.codehaus.commons.compiler.CompileException: File 
'generated.java', Line 398, Column 41: Expression ""scan_isNull10"" is not 
an rvalue
[error] 	at 
org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
[error] 	at 
org.codehaus.janino.UnitCompiler.toRvalueOrCompileException(UnitCompiler.java:6036)
[error] 	at 
org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:4440)
[error] 	at 
org.codehaus.janino.UnitCompiler.access$9900(UnitCompiler.java:185)
[error] 	at 
org.codehaus.janino.UnitCompiler$11.visitAmbiguousName(UnitCompiler.java:4417)
[error] 	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:3138)
[error] 	at 
org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4427)
[error] 	at 
org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:4634)
[error] 	at 
org.codehaus.janino.UnitCompiler.access$8900(UnitCompiler.java:185)
[error] 	at 
org.codehaus.janino.UnitCompiler$11.visitBinaryOperation(UnitCompiler.java:4394)
[error] 	at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:3768)
[error] 	at 
org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4427)
[error] 	at 
org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4360)
[error] 	at 
org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
[error] 	at 
org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
[error] 	at 
org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
[error] 	at 
org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
[error] 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
[error] 	at 
org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
[error] 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
[error] 	at 
org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
[error] 	at 
org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
[error] 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
[error] 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
[error] 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
[error] 	at 
org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
[error] 	at 
org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
[error] 	at 
org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
[error] 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
[error] 	at 
org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
[error] 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
[error] 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
[error] 	at 
org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
[error] 	at 
org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
[error] 	at 
org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
[error] 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
[error] 	at 
org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
[error] 	at 
org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
[error] 	at 
org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
[error] 	at 
org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
[error] 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
[error] 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
[error] 	at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
[error] 	at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
[error] 	at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
[error] 	at 
org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[error] 	at 
org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[error] 	at 
org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[error] 	at 
org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
[error] 	at 
org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
[error] 	at 
org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
[error] 	at 
org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
[error] 	at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
[error] 	at 
org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:350)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
[error] 	at 
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
[error] 	at 
org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:86)
[error] 	at 
org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:122)
[error] 	at 
org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:113)
[error] 	at 
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:49)
[error] 	at 
org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:113)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
[error] 	at 
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
[error] 	at 
org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)
[error] 	at 
org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)
[error] 	at 
org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
[error] 	at 
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
[error] 	at 
org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
[error] 	at 
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)
[error] 	at 
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
[error] 	at 
org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
[error] 	at 
org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)
[error] 	at 
org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)
[error] 	at 
org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2217)
[error] 	at 
org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2216)
[error] 	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
[error] 	at org.apache.spark.sql.Dataset.count(Dataset.scala:2216)
[error] 	at 
com.sam4m.kafkafsconnector.Foo$.delayedEndpoint$com$sam4m$kafkafsconnector$Foo$1(App.scala:92)
[error] 	at 
com.sam4m.kafkafsconnector.Foo$delayedInit$body.apply(App.scala:80)
[error] 	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
[error] 	at 
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
[error] 	at scala.App$$anonfun$main$1.apply(App.scala:76)
[error] 	at scala.App$$anonfun$main$1.apply(App.scala:76)
[error] 	at scala.collection.immutable.List.foreach(List.scala:381)
[error] 	at 
scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
[error] 	at scala.App$class.main(App.scala:76)
[error] 	at com.sam4m.kafkafsconnector.Foo$.main(App.scala:80)
[error] 	at com.sam4m.kafkafsconnector.Foo.main(App.scala)
[error] 16/09/29 17:49:49 WARN WholeStageCodegenExec: Whole-stage 
codegen disabled for this plan:
[error]  *HashAggregate(keys=[], functions=[partial_count(1)], 
output=[count#41L])
[error] +- *Project
[error]    +- *Filter <function1>.apply
[error]       +- *Scan avro 
[minute#0,second#1,info#2,status#3,year#4,month#5,day#6,hour#7] Format: 
com.databricks.spark.avro.DefaultSource@5864e8bf, InputPaths: 
file:/tmp/k2d-tests/output, PushedFilters: [], ReadSchema: 
struct<minute:int,second:int,info:struct<date:bigint,statID:string,eventType:string,deviceAdverti...

---------------------------------------------------------------------


"
Weiqing Yang <yangweiqing001@gmail.com>,"Thu, 29 Sep 2016 10:58:14 -0700",Re: [discuss] Spark 2.x release cadence,Cody Koeninger <cody@koeninger.org>,"+1 (non binding)



RC4 is compiled and tested on the system: CentOS Linux release
7.0.1406 / openjdk 1.8.0_102 / R 3.3.1

 All tests passed.



./build/mvn -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver
-Dpyspark -Dsparkr -DskipTests clean "
Weiqing Yang <yangweiqing001@gmail.com>,"Thu, 29 Sep 2016 10:59:11 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Jagadeesan As <as2@us.ibm.com>,"+1 (non binding)



RC4 is compiled and tested on the system: CentOS Linux release
7.0.1406 / openjdk 1.8.0_102 / R 3.3.1

 All tests passed.



./build/mvn -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver
-Dpyspark -Dsparkr -DskipTests clean "
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 29 Sep 2016 10:59:47 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non binding)


"
Weiqing Yang <yangweiqing001@gmail.com>,"Thu, 29 Sep 2016 11:01:48 -0700",Re: [discuss] Spark 2.x release cadence,Cody Koeninger <cody@koeninger.org>,"Sorry. I think I just replied to the wrong thread. :(


WQ


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 29 Sep 2016 11:36:40 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),=?utf-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"+1

Matei

7.0.1406 / openjdk 1.8.0_102 / R 3.3.1
-Phive-thriftserver -Dpyspark -Dsparkr -DskipTests clean package
-Phive-thriftserver -Dpyspark -Dsparkr test
<mailto:ricardo.almeida@actnowib.com>>
<dev@spark.apache.org <mailto:dev@spark.apache.org>>
-Phi"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 29 Sep 2016 11:51:36 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Reynold Xin <rxin@databricks.com>,"+1

Regards,
Mridul


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 29 Sep 2016 12:00:37 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Mridul Muralidharan <mridul@gmail.com>,1
Sean Owen <sowen@cloudera.com>,"Thu, 29 Sep 2016 15:04:17 -0400",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Reynold Xin <rxin@databricks.com>,"+1 from me too, same result as my RC3 vote/testing.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 29 Sep 2016 12:13:09 -0700",Re: Spark SQL JSON Column Support,Cody Koeninger <cody@koeninger.org>,"
per-job schema?

As currently written, we will do a little bit of extra work to pull out
fields that aren't needed.  I think it would be pretty straight forward to
add a rule to the optimizer that prunes the schema passed to the
JsonToStruct expression when there is another Project operator present.

I‚Äôm not a spark guru, but I would have hoped that DataSets and DataFrames


We are dynamic in that all of these decisions can be made at runtime, and
you can even look at the data when making them.  We do however need to know
the schema before any single query begins executing so that we can give
good analysis error messages and so that we can generate efficient byte
code in our code generation.


ur
roblem.


I agree that for ad-hoc use cases we should make it easy to infer the
schema.  I would also argue that for a production pipeline you need the
ability to specify it manually to avoid surprises.

There are several tricky cases here.  You bring up the fact that the first
record might be missing fields, but in many data sets there are fields that
are only present in 1 out of 100,000s records.  Even if all fields are
present, sometimes it can be very expensive to get even the first record
(say you are reading from an expensive query coming from the JDBC data
source).

Another issue, is that inference means you need to read some data before
the user explicitly starts the query.  Historically, cases where we do this
have been pretty confusing to users of Spark (think: the surprise job that
finds partition boundaries for RDD.sort).

So, I think we should add inference, but that it should be in addition to
the API proposed in this PR.
"
Sameer Agarwal <sameer@databricks.com>,"Thu, 29 Sep 2016 12:20:51 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Sean Owen <sowen@cloudera.com>,"+1




-- 
Sameer Agarwal
Software Engineer | Databricks Inc.
http://cs.berkeley.edu/~sameerag
"
Michael Armbrust <michael@databricks.com>,"Thu, 29 Sep 2016 12:22:38 -0700",Re: Questions about DataFrame's filter(),Samy Dindane <samy@dindane.com>,"-dev +user

It surprises me as `filter()` takes a Column, not a `Row => Boolean`.


There are several overloaded versions of Dataset.filter(...)

def filter(func: FilterFunction[T]): Dataset[T]
def filter(func: (T) ‚áí Boolean): Dataset[T]
def filter(cond"
Cody Koeninger <cody@koeninger.org>,"Thu, 29 Sep 2016 14:34:38 -0500",Re: Spark SQL JSON Column Support,Michael Armbrust <michael@databricks.com>,"Totally agree that specifying the schema manually should be the
baseline.  LGTM, thanks for working on it.  Seems like it looks good
to others too judging by the comment on the PR that it's getting
merged to master :)

o
ataFrames
ow
ood
n
at
irst
m.
t
at
the
ave
ds

---------------------------------------------------------------------


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Thu, 29 Sep 2016 21:11:47 -0000",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),<dev@spark.apache.org>,"+1 (non-binding)

At this time, I tested RC4 on the followings.

- CentOS 6.8 (Final)
- OpenJDK 1.8.0_101
- Python 2.7.12

/build/mvn -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver -Dpyspark -Dsparkr -DskipTests clean package
/build/mvn -Pya"
Joseph Bradley <joseph@databricks.com>,"Thu, 29 Sep 2016 15:11:55 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Dongjoon Hyun <dongjoon@apache.org>,1
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 29 Sep 2016 15:17:04 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Reynold Xin <rxin@databricks.com>,"+1




-- 
Marcelo

---------------------------------------------------------------------


"
jpuro <jpuro@mustwin.com>,"Thu, 29 Sep 2016 15:46:51 -0700 (MST)",Running Spark master/slave instances in non Daemon mode,dev@spark.apache.org,"Hi,

I recently tried deploying Spark master and slave instances to container
based environments such as Docker, Nomad etc. There are two issues that I've
found with how the startup scripts work. The sbin/start-master.sh and
sbin/start-slave.sh start a daemon by default, but this isn't as compatible
with container deployments as one would think. The first issue is that the
daemon runs in the background and some container solutions require the apps
to run in the foreground or they consider the application to not be running
and they may close down the task. The second issue is that logs don't seem
to get integrated with the logging mechanism in the container solution. What
is the possibility of adding additional flags or startup scripts for
supporting Spark to run in the foreground? It would be great if a flag like
SPARK_NO_DAEMONIZE could be added or another script for foreground
execution.

Regards,

Jeff



--

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 29 Sep 2016 16:07:59 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Yin Huai <yhuai@databricks.com>,"Thu, 29 Sep 2016 16:27:53 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Luciano Resende <luckbr1975@gmail.com>,1
Kyle Kelley <rgbkrk@gmail.com>,"Thu, 29 Sep 2016 16:33:17 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),,"+1




-- 
Kyle Kelley (@rgbkrk <https://twitter.com/rgbkrk>; lambdaops.com)
"
Jakob Odersky <jakob@odersky.com>,"Thu, 29 Sep 2016 17:25:00 -0700",Re: Running Spark master/slave instances in non Daemon mode,jpuro <jpuro@mustwin.com>,"I'm curious, what kind of container solutions require foreground
processes? Most init systems work fine with ""starter"" processes that
run other processes. IIRC systemd and start-stop-daemon have an option
called ""fork"", that will expect the main process to run another one in
the background and only consider the former complete when the latter
exits. I'm not against having a non-forking start script, I'm just
wondering where you'd run into issues.

Regarding the logging, would it be an option to create a custom slf4j
logger that uses the standard mechanisms exposed by the system?

best,
--Jakob


---------------------------------------------------------------------


"
Burak Yavuz <brkyvz@gmail.com>,"Thu, 29 Sep 2016 18:27:24 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Kyle Kelley <rgbkrk@gmail.com>,1
Mike Ihbe <mike@mustwin.com>,"Thu, 29 Sep 2016 18:47:49 -0700",Re: Running Spark master/slave instances in non Daemon mode,Jakob Odersky <jakob@odersky.com>,"Our particular use case is for Nomad, using the ""exec"" configuration
described here: https://www.nomadproject.io/docs/drivers/exec.html. It's
not exactly a container, just a cgroup. It performs a simple fork/exec of a
command and binds to the output fds from that process, so daemonizing is
causing us minor hardship and seems like an easy thing to make optional.
We'd be happy to make the PR as well.

--Mike




-- 
Mike Ihbe
MustWin - Principal

mike@mustwin.com
mikejihbe@gmail.com
skype: mikeihbe
Cell: 651.283.0815
"
Tejas Patil <tejas.patil.cs@gmail.com>,"Thu, 29 Sep 2016 18:54:27 -0700",[question] Why Spark SQL grammar allows <column name> : <data type> ?,dev@spark.apache.org,"Is there any reason why Spark SQL supports ""<column name>"" "":"" ""<data
type>"" while specifying columns ? eg. sql(""CREATE TABLE t1 (column1:INT)"")
works fine.

Here is relevant snippet in the grammar [0]:

```
colType
    : identifier ':'? dataType (COMMENT STRING)?
    ;
```

I do not see MySQL[1], Hive[2], Presto[3] and PostgreSQL [4] supporting "":""
while specifying columns. They all use space as a delimiter.

[0] :
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4#L596
[1] : http://dev.mysql.com/doc/refman/5.7/en/create-table.html
[2] :
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTable
[3] : https://prestodb.io/docs/current/sql/create-table.html
[4] : https://www.postgresql.org/docs/9.1/static/sql-createtable.html

Thanks,
Tejas
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Thu, 29 Sep 2016 19:00:16 -0700",Issues in compiling spark 2.0.0 code using scala-maven-plugin,"user@spark.apache.org, dev@spark.apache.org","Hi ALL,

i am trying to compile code using maven ,which was working with spark
1.6.2, but when i try for spark 2.0.0 then i get below error,

org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (default) on
project NginxLoads-repartition: wrap:
org.apache.commons.exec.ExecuteException: Process exited with an error: 1
(Exit value: 1)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
at
org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
at
org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: wrap:
org.apache.commons.exec.ExecuteException: Process exited with an error: 1
(Exit value: 1)
at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:490)
at
org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
... 20 more
Caused by: org.apache.commons.exec.ExecuteException: Process exited with an
error: 1 (Exit value: 1)
at
org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:377)
at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:160)
at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:147)
at
scala_maven_executions.JavaMainCallerByFork.run(JavaMainCallerByFork.java:100)
at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:161)
at scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:99)
at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
... 22 more


PFB pom.xml that i am using, any help would be appreciated.

<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0""
         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0
http://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>

    <groupId>NginxLoads-repartition</groupId>
    <artifactId>NginxLoads-repartition</artifactId>
    <version>1.1-SNAPSHOT</version>
    <name>${project.artifactId}</name>
    <description>This is a boilerplate maven project to start using
Spark in Scala</description>
    <inceptionYear>2010</inceptionYear>

    <properties>
        <maven.compiler.source>1.6</maven.compiler.source>
        <maven.compiler.target>1.6</maven.compiler.target>
        <encoding>UTF-8</encoding>
        <scala.tools.version>2.11</scala.tools.version>
        <scalaCompatVersion>2.11</scalaCompatVersion>
        <!-- Put the Scala version of the cluster -->
        <scala.version>2.11.8</scala.version>
    </properties>

    <!-- repository to add org.apache.spark -->
    <repositories>
        <repository>
            <id>cloudera-repo-releases</id>
            <url>https://repository.cloudera.com/artifactory/repo/</url>
        </repository>
    </repositories>

    <build>
        <sourceDirectory>src/main/scala</sourceDirectory>
        <testSourceDirectory>src/test/scala</testSourceDirectory>
        <plugins>
            <!-- any other plugins -->
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.5.1</version>
                <configuration>
                    <source>1.7</source>
                    <target>1.7</target>
                </configuration>
            </plugin>
            <plugin>
                <!-- see http://davidb.github.com/scala-maven-plugin -->
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <configuration>
                    <!--<recompileMode>incremental</recompileMode>-->
                    <!--<useZincServer>true</useZincServer>-->
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                        <configuration>
                            <args>
                                <arg>-make:transitive</arg>
                                <arg>dependencyfile</arg>

<arg>${project.build.directory}/.scala_dependencies</arg>
                            </args>
                        </configuration>
                    </execution>
                </executions>
            </plugin>


            <plugin>
                <artifactId>maven-antrun-plugin</artifactId>
                <executions>
                    <execution>
                        <phase>deploy</phase>
                        <configuration>
<!--                            <tasks>

                                <resources>
                                    <resource>

<directory>src/main/resources</directory>
                                        <includes>
                                            <include>regexes.yaml</include>
                                            <include>patterns.txt</include>
                                        </includes>
                                    </resource>
                                </resources>
                            </tasks>-->
                        </configuration>
                        <goals>
                            <goal>run</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <!-- ""package"" command plugin -->
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>2.4.1</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
<!--        <resources>
            <resource>
                <directory>src/main/resources</directory>
                <includes>
                    <include>regexes.yaml</include>
                    <include>patterns.txt</include>
                </includes>
            </resource>
        </resources>-->
    </build>
    <pluginRepositories>
        <pluginRepository>
            <id>scala-tools.org</id>
            <name>Scala-tools Maven2 Repository</name>
            <url>http://scala-tools.org/repo-releases</url>
        </pluginRepository>
    </pluginRepositories>
    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-csv</artifactId>
            <version>1.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-auth</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-yarn-api</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-core</artifactId>
            <version>1.2.1</version>
        </dependency>
        <dependency>
            <groupId>org.yaml</groupId>
            <artifactId>snakeyaml</artifactId>
            <version>1.17</version>
        </dependency>
        <dependency>
            <groupId>com.twitter</groupId>
            <artifactId>util-collection_2.10</artifactId>
            <version>6.23.0</version>
        </dependency>
        <dependency>
            <groupId>org.specs2</groupId>
            <artifactId>specs2-core_2.10</artifactId>
            <version>2.4.15</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-jdbc</artifactId>
            <version>1.2.1</version>
        </dependency>
        <dependency>
            <groupId>io.thekraken</groupId>
            <artifactId>grok</artifactId>
            <version>0.1.3</version>
        </dependency>
    </dependencies>


</project>
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 29 Sep 2016 19:22:30 -0700","Re: [question] Why Spark SQL grammar allows <column name> : <data
 type> ?",Tejas Patil <tejas.patil.cs@gmail.com>,"Tejas,

This is because we use the same rule to parse top level and nested data
fields. For example:

create table tbl_x(
  id bigint,
  nested struct<col1:string,col2:string>
)

Shows both syntaxes. We should split this rule in a top-level and nested
rule.

Could you open a ticket?

Thanks,
Herman




"
Reynold Xin <rxin@databricks.com>,"Thu, 29 Sep 2016 19:24:14 -0700","Re: [question] Why Spark SQL grammar allows <column name> : <data
 type> ?",=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Is there any harm in supporting it? Mostly curious whether we really need
to ""fix"" this.




"")
"
Tejas Patil <tejas.patil.cs@gmail.com>,"Thu, 29 Sep 2016 20:09:42 -0700","Re: [question] Why Spark SQL grammar allows <column name> : <data
 type> ?",Reynold Xin <rxin@databricks.com>,"Herman : Thanks for the explanation. That makes sense. Logged a jira :
https://issues.apache.org/jira/browse/SPARK-17741
Reynold : I am not affected by this but it felt odd while I was reading the
code.

Thanks,
Tejas


)"")
"
Denny Lee <denny.g.lee@gmail.com>,"Fri, 30 Sep 2016 04:00:29 +0000",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"Jeff Zhang <zjffdu@gmail.com>, Burak Yavuz <brkyvz@gmail.com>","+1 (non-binding)


"
vaquar khan <vaquar.khan@gmail.com>,"Fri, 30 Sep 2016 00:01:15 -0500",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Denny Lee <denny.g.lee@gmail.com>,"+1 (non-binding)
Regards,
Vaquar  khan


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Fri, 30 Sep 2016 08:16:33 +0200",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),vaquar khan <vaquar.khan@gmail.com>,"+1

2016-09-30 7:01 GMT+02:00 vaquar khan <vaquar.khan@gmail.com>:

T and
a
d
idate,
y
s will
atches to


-- 
Maciek Bry≈Ñski
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 30 Sep 2016 10:12:07 +0000",Re: Using Spark as a Maven dependency but with Hadoop 2.6,,"

I know that the code itself would not be the same, but it would be useful to at least have the pom/build.sbt transitive dependencies different when fetching the artifact with a specific classifier, don't you think ?
For now I've overriden them myself using the dependency versions defined in the pom.xml of spark.
So it's not a blocker issue, it may be useful to document it, but a blog post would be sufficient I think.



The problem here is that it's not directly something that maven repo is set up to deal with. What could be done would be to publish multiple pom-only artifacts, spark-scala-2.11-hadoop-2.6.pom which would declare the transitive stuff appropriately for the right version. You wouldn't need to actually rebuild everything, just declare a dependency on the spark 2.2 artifacts excluding all of hadoop 2.2, pulling in 2.6.

This wouldn't even need to be an org.apache.spark artifact, just something any can build and publish under their own name.

Volunteers?
"
Grant Digby <digbyg@gmail.com>,"Fri, 30 Sep 2016 04:08:32 -0700 (MST)",Re: IllegalArgumentException: spark.sql.execution.id is already set,dev@spark.apache.org,"Thanks for the link. Yeah if there's no need to copy execution.id from parent
to child then I agree, you could strip it out, presumably in this part of
the code using some kind of configuration as to which properties shouldn't
go across

SparkContext:
 protected[spark] val localProperties = new
InheritableThreadLocal[Properties] {
    override protected def childValue(parent: Properties): Properties = {
      // Note: make a clone such that changes in the parent properties
aren't reflected in
      // the those of the children threads, which has confusing semantics
(SPARK-10563).
      SerializationUtils.clone(parent).asInstanceOf[Properties]
    }
    override protected def initialValue(): Properties = new Properties()
  }



--

---------------------------------------------------------------------


"
Marcin Tustin <mtustin@handybook.com>,"Fri, 30 Sep 2016 07:14:32 -0400",Re: IllegalArgumentException: spark.sql.execution.id is already set,Grant Digby <digbyg@gmail.com>,"The solution is to strip it out in a hook on your threadpool, by overriding
beforeExecute. See:
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Fri, 30 Sep 2016 13:35:59 +0200","Re: java.util.NoSuchElementException when serializing Map with
 default value","Jakob Odersky <jakob@odersky.com>, Sean Owen <sowen@cloudera.com>","Thanks guys.

This is not a big issue in general. More an annoyance and can be rather
confusing when encountered for the first time.


/scala/com/twitter/chill/Traversable.scala
park
re.
(TID 7)

-- 
Best regards,
Maciej



---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 30 Sep 2016 13:19:19 +0000 (UTC)",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1
Tom 

 

 Please vote on releasing the following candidate as Apache Spark version 2.0.1. The vote is open until Sat, Oct 1, 2016 at 20:00 PDT and passes if a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0."
Aleksander Eskilson <aleksanderesk@gmail.com>,"Fri, 30 Sep 2016 08:36:20 -0500",Catalyst - ObjectType for Encoders,dev@spark.apache.org,"Hi there,



Currently Catalyst supports encoding custom classes represented as Java
Beans (among others). This Java Bean implementation depends internally on
Catalyst‚Äôs ObjectType extension of DataType. Currently, this class is
private to the sql package [1], which is sensible, as it is only necessary
for representing objects in the encoder. However, its private scope makes
it more difficult (if not impossible) to write full custom encoders for
other classes, themselves perhaps composed of additional objects.



Could the definition of the ObjectType be made public in order to support
writing custom Encoders? I can't see any particular danger in this.

In order to implement an encoder for Avro-specified classes, I have to work
off a fork where the ObjectType definition has be liberalized as described.



[1] -- https://github.com/apache/spark/blob/master/sql/
catalyst/src/main/scala/org/apache/spark/sql/types/ObjectType.scala#L39



Thanks,

Aleksander Eskilson
"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 30 Sep 2016 08:34:09 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Reynold Xin <rxin@databricks.com>,"0

RC4 is causing a build regression for me on at least one of my machines.
RC3 built and ran tests successfully, but the tests consistently fail with
RC4 unless I revert 9e91a1009e6f916245b4d4018de1664ea3decfe7,
""[SPARK-15703][SCHEDULER][CORE][WEBUI] Make ListenerBus event queue size
configurable (branch 2.0)"".  This is using build/mvn -U -Pyarn -Phadoop-2.7
-Pkinesis-asl -Phive -Phive-thriftserver -Dpyspark -Dsparkr -DskipTests
clean package; build/mvn -U -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive
-Phive-thriftserver -Dpyspark -Dsparkr test.  Environment is macOS 10.12,
Java 1.8.0_102.

There are no tests that go red.  Rather, the core tests just end after...

...
BlockManagerSuite:
...
- overly large block
- block compression
- block store put failure

...with only the generic ""[ERROR] Failed to execute goal
org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
spark-core_2.11: There are test failures"".

I'll try some other environments today to see whether I can turn this 0
into either a -1 or +1, but right now that commit is looking deeply
suspicious to me.


"
Peter Figliozzi <pete.figliozzi@gmail.com>,"Fri, 30 Sep 2016 10:40:02 -0500","code questions, sql.functions.scala",dev@spark.apache.org,"Taking isnan as a simple example, I'd like to understand what happens
downstream of sql.functions.scala.

1. The withExpr { } construct.  How does that work?  I see it refers to the
IsNan case class, which gets passed not the column e but e.expr, which is
an Expression.   I also see that withExpr is a private function of the
Column class.

2.  The IsNaN case class.  How does that work together with the call to
isnan?  I notice it only has to handle DoubleType and IntegerType, as
defined in its inputTypes override.  So what happens (or what has happened)
to any strings that were encountered in the call to .isnan?

3.  Expressions.  I see that values and datatypes are extracted from
Expressions.  But this happens in eval(), which is somehow passed an
Internal row.  It seems this is pretty central to understanding how data
gets to a sql.function to be acted on.  Can someone summarize how this
works?

Thanks much,

Pete
"
akchin <akchin@us.ibm.com>,"Fri, 30 Sep 2016 09:52:23 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),dev@spark.apache.org,"+1 (non-binding) 
Tested with following:
-Pyarn -Phadoop-2.7 -Phive -Phive-thriftserver -Psparkr 
CentOS 7.2 / openjdk version ""1.8.0_101"" 




-----
IBM Spark Technology Center 
--

---------------------------------------------------------------------


"
Mahendra Kutare <mahendra.kutare@gmail.com>,"Fri, 30 Sep 2016 11:09:05 -0700",Re: Restful WS for Spark,ABHISHEK <abhietc@gmail.com>,"Try Cloudera Livy https://github.com/cloudera/livy

It may be helpful for your requirement.


Cheers,

Mahendra
about.me/mahendrakutare
<https://about.me/mahendrakutare?promo=email_sig&utm_source=email_sig&utm_medium=email_sig&utm_campaign=external_links>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
can go.


"
Michael Armbrust <michael@databricks.com>,"Fri, 30 Sep 2016 12:24:56 -0700",Re: Catalyst - ObjectType for Encoders,Aleksander Eskilson <aleksanderesk@gmail.com>,"I'd be okay removing that modifier, with one caveat.  The code in
org.apache.spark.sql.catalyst.* is purposefully excluded from published
documentation and does not have the same compatibility guarantees as the
rest of the Spark's Public APIs.  We leave most of it not ""private"" so that
advanced users can experiment.

So, you can write a custom encoder, but until we finalize the encoder API,
you might need to update it each Spark release.


s is
y
"
