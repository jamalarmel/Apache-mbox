Ted Yu <yuzhihong@gmail.com>,"Fri, 30 Sep 2016 18:38:27 -0700",Re: Issues in compiling spark 2.0.0 code using scala-maven-plugin,satyajit vegesna <satyajit.apasprk@gmail.com>,"Was there any error prior to 'LifecycleExecutionException' ?


"
Kevin Grealish <kevingre@microsoft.com>,"Sat, 1 Oct 2016 01:49:00 +0000","regression: no longer able to use HDFS wasbs:// path for additional
 python files on LIVY batch submit","""andrewor14@gmail.com"" <andrewor14@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, ""tdas@databricks.com"" <tdas@databricks.com>","I'm seeing a regression when submitting a batch PySpark program with additional files using LIVY. This is YARN cluster mode. The program files are placed into the mounted Azure Storage before making the call to LIVY. This is happening from an application which has credentials for the storage and the LIVY endpoint, but not local file systems on the cluster. This previously worked but now I'm getting the error below.

Seems this restriction was introduced with https://github.com/apache/spark/commit/5081a0a9d47ca31900ea4de570de2cbb0e063105 (new in 1.6.2 and 2.0.0).

How should the scenario above be achieved now? Am I missing something?


Exception in thread ""main"" java.lang.IllegalArgumentException: Launching Python applications through spark-submit is currently only supported for local files: wasb://kevingrecluster2@xxxxxxxx.blob.core.windows.net/xxxxxxxxx/xxxxxxx.py
                at org.apache.spark.deploy.PythonRunner$.formatPath(PythonRunner.scala:104)
                at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
                at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
                at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
                at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
                at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
                at org.apache.spark.deploy.PythonRunner$.formatPaths(PythonRunner.scala:136)
                at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$11.apply(SparkSubmit.scala:639)
                at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$11.apply(SparkSubmit.scala:637)
                at scala.Option.foreach(Option.scala:236)
                at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:637)
                at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:154)
                at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
                at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
java.lang.Exception: spark-submit exited with code 1}.

"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Fri, 30 Sep 2016 20:47:11 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),Mark Hamstra <mark@clearstorydata.com>,"Hey Mark,

I can reproduce the failure locally using your command. There were a lot of
OutOfMemoryError in the unit test log. I increased the heap size from 3g to
4g at https://github.com/apache/spark/blob/v2.0.1-rc4/pom.xml#L2029 and it
passed tests. I think the patch you mentioned increased the memory
usage of BlockManagerSuite
and made the tests easy to OOM. It can be fixed by mocking SparkContext (or
may be not necessary since Jenkins's maven and sbt builds are green now).

However, since this is only a test issue, it should not be a blocker.



"
Felix Cheung <felixcheung_m@hotmail.com>,"Sat, 1 Oct 2016 17:29:03 +0000",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","+1

Tested and didn't find any blocker - found a few minor R doc issues to follow up.


_____________________________
From: Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>
Sent: Wednesday, September 28, 2016 7:15 PM
Subject: [VOTE] Release A"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 1 Oct 2016 13:05:31 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Thanks for doing the investigation.  What I found out yesterday is that my
other macOs 10.12 machine ran into the same issue, while various Linux
machines did not, so there may well be an OS-specific component to this
particular OOM-in-tests problem.  Unfortunately, increasing the heap as you
suggest doesn't resolve the issue for me -- even if I increase it all the
way to 6g.  This does appear to be environment-specific (and not an
environment that I would expect to see in Spark deployments), so I agree
that this is not a blocker.

I looked a bit into the other annoying issue that I've been seeing for
awhile now with the shell terminating when YarnClusterSuite is run on an
Ubuntu 16.0.4 box.  Both Sean Owen and I have run into this problem when
running the tests over an ssh connection, and we each assumed that it was
an ssh-specific problem.  Yesterday, though, I spent some time logged
directly into both a normal graphical sessions and console sessions, and I
am seeing similar problems there. Running the tests from the graphical
session actually ends up failing and kicking me all the way out to the
login screen when YarnClusterSuite is run, while doing the same from the
console ends up terminating the shell.  All very strange, and I don't have
much of a clue what is going on yet, but it also seems to quite specific to
this environment, so I wouldn't consider this issue to be a blocker, either


"
Kabeer Ahmed <kabeer@linuxmail.org>,"Sun, 02 Oct 2016 23:39:09 +0100","Re: java.util.NoSuchElementException when serializing Map with
 default value","Jakob Odersky <jakob@odersky.com>, Sean Owen <sowen@cloudera.com>,
	Maciej Szymkiewicz <mszymkiewicz@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I have had a quick look at the query from Maciej. I see different 
behaviour while running the piece of code in spark-shell and a 
different one while running it as spark app.

1. While running in the spark-shell, I see the serialization error that 
Maciej has reported.
2. But while running the same code as SparkApp, I see a different 
behaviour.

I have put the code below. It would be great if someone can explain the 
difference in behaviour.

Thanks,
Kabeer.

------------------------------------------------
Spark-Shell:
scala> sc.stop

scala> :paste
// Entering paste mode (ctrl-D to finish)

import org.apache.spark._
val sc = new SparkContext(new
      SparkConf().setAppName(""bar"").set(""spark.serializer"",
    ""org.apache.spark.serializer.KryoSerializer""))

  println(sc.getConf.getOption(""spark.serializer""))

  val m = Map(""a"" -> 1, ""b"" -> 2)
  val rdd5 = sc.makeRDD(Seq(m))
  println(""Map RDD is: "")
  def mapFunc(input: Map[String, Int]) : Unit = 
println(input.getOrElse(""a"", -2))
  rdd5.map(mapFunc).collect()

// Exiting paste mode, now interpreting.

Some(org.apache.spark.serializer.KryoSerializer)
Map RDD is:
org.apache.spark.SparkException: Task not serializable
 at 
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)
 at 
org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
 at 
org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
-------------------------------------------------------------------------------------------------


----------------------------------------------------------------------------------------
Scenario 2:

Code:

package experiment

import org.apache.spark._

object Serialization1 extends App {

  val sc = new SparkContext(new
      SparkConf().setAppName(""bar"").set(""spark.serializer"",
    ""org.apache.spark.serializer.KryoSerializer"")
      .setMaster(""local[1]"")
  )

  println(sc.getConf.getOption(""spark.serializer""))

  val m = Map(""a"" -> 1, ""b"" -> 2)
  val rdd5 = sc.makeRDD(Seq(m))
  println(""Map RDD is: "")
  def mapFunc(input: Map[String, Int]) : Unit = 
println(input.getOrElse(""a"", -2))
  rdd5.map(mapFunc).collect()

}

Run command:

spark-submit --class experiment.Serialization1 
target/scala-2.10/learningspark_2.10-0.1-SNAPSHOT.jar

---------------------------------------------------------------------------------------------------------------




I agree with Sean's answer, you can check out the relevant serializer
here 
https://github.com/twitter/chill/blob/develop/chill-scala/src/main/scala/com/twitter/chill/Traversable.scala

 My guess is that Kryo specially handles Maps generically or relies on
 some mechanism that does, and it happens to iterate over all
 key/values as part of that and of course there aren't actually any
 key/values in the map. The Java serialization is a much more literal
 (expensive) field-by-field serialization which works here because
 there's no special treatment. I think you could register a custom
 serializer that handles this case. Or work around it in your client
 code. I know there have been other issues with Kryo and Map because,
 for example, sometimes a Map in an application is actually some
 non-serializable wrapper view.

 Hi everyone,

 I suspect there is no point in submitting a JIRA to fix this (not a 
Spark
 issue?) but I would like to know if this problem is documented 
anywhere.
 Somehow Kryo is loosing default value during serialization:

 scala> import org.apache.spark.{SparkContext, SparkConf}
 import org.apache.spark.{SparkContext, SparkConf}

 scala> val aMap = Map[String, Long]().withDefaultValue(0L)
 aMap: scala.collection.immutable.Map[String,Long] = Map()

 scala> aMap(""a"")
 res6: Long = 0

 scala> val sc = new SparkContext(new
 SparkConf().setAppName(""bar"").set(""spark.serializer"",
 ""org.apache.spark.serializer.KryoSerializer""))

 scala> sc.parallelize(Seq(aMap)).map(_(""a"")).first
 16/09/28 09:13:47 ERROR Executor: Exception in task 2.0 in stage 2.0 
(TID 7)
 java.util.NoSuchElementException: key not found: a

 while Java serializer works just fine:

 scala> val sc = new SparkContext(new
 SparkConf().setAppName(""bar"").set(""spark.serializer"",
 ""org.apache.spark.serializer.JavaSerializer""))

 scala> sc.parallelize(Seq(aMap)).map(_(""a"")).first
 res9: Long = 0

 --
 Best regards,
 Maciej

 ---------------------------------------------------------------------


---------------------------------------------------------------------
"
Reynold Xin <rxin@databricks.com>,"Sun, 2 Oct 2016 17:13:16 -0700",Re: [VOTE] Release Apache Spark 2.0.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for voting. The vote has passed with the following +1 votes and no
-1 votes. I will work on packaging the release.

+1

Reynold Xin*
Ricardo Almeida
Jagadeesan As
Weiqing Yang
Herman van HÃ¶vell tot Westerflier
Matei Zaharia*
Mridul Muralidharan*
Michael Armbrust*
Sean Owen*
Sameer Agarwal
Dongjoon Hyun
Joseph Bradley*
Marcelo Vanzin
Luciano Resende
Yin Huai*
Kyle Kelley
Burak Yavuz
Jeff Zhang
Denny Lee
vaquar khan
Maciej BryÅ„ski
Tom Graves*
akchin
Felix Cheung

0

Mark Hamstra

-1

N/A






 a
n
1.
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 3 Oct 2016 12:40:24 +0000","Re: regression: no longer able to use HDFS wasbs:// path for
 additional python files on LIVY batch submit",Kevin Grealish <kevingre@microsoft.com>,"
On 1 Oct 2016, at 02:49, Kevin Grealish <kevingre@microsoft.com<mailto:kevingre@microsoft.com>> wrote:

Iâ€™m seeing a regression when submitting a batch PySpark program with additional files using LIVY. This is YARN cluster mode. The program files are placed into the mounted Azure Storage before making the call to LIVY. This is happening from an application which has credentials for the storage and the LIVY endpoint, but not local file systems on the cluster. This previously worked but now Iâ€™m getting the error below.

Seems this restriction was introduced with https://github.com/apache/spark/commit/5081a0a9d47ca31900ea4de570de2cbb0e063105 (new in 1.6.2 and 2.0.0).

How should the scenario above be achieved now? Am I missing something?

This has been fixed in https://issues.apache.org/jira/browse/SPARK-17512 ; I don't know if its in 2.0.1 though


Exception in thread ""main"" java.lang.IllegalArgumentException: Launching Python applications through spark-submit is currently only supported for local files: wasb://kevingrecluster2@xxxxxxxx.blob.core.windows.net/xxxxxxxxx/xxxxxxx.py
                at org.apache.spark.deploy.PythonRunner$.formatPath(PythonRunner.scala:104)
                at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
                at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
                at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
                at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
                at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
                at org.apache.spark.deploy.PythonRunner$.formatPaths(PythonRunner.scala:136)
                at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$11.apply(SparkSubmit.scala:639)
                at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$11.apply(SparkSubmit.scala:637)
                at scala.Option.foreach(Option.scala:236)
                at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:637)
                at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:154)
                at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
                at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
java.lang.Exception: spark-submit exited with code 1}.

"
Aleksander Eskilson <aleksanderesk@gmail.com>,"Mon, 03 Oct 2016 13:27:30 +0000",Re: Catalyst - ObjectType for Encoders,Michael Armbrust <michael@databricks.com>,"Sounds good to me, I'll keep that in mind and pay attention to updates.
Shall I make a pull request from my fork, or would someone else prefer to
change those function signatures themselves?

Thanks,
Alek


at
,
s is
y
rg/apache/spark/sql/types/ObjectType.scala#L39
"
Tathagata Das <tdas@databricks.com>,"Mon, 3 Oct 2016 11:28:59 -0700",Metrics for monitoring Structured Streaming apps,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

I have been working on adding operational metrics for monitoring the health
and performance of Structured Streaming applications. The detailed design
and the WIP Github PR is here.

*JIRA *- SPARK-17731 <https://issues.apache.org/jira/browse/SPARK-17731>
*Design Doc* - https://docs.google.com/document/d/
1NIdcGuR1B3WIe8t7VxLrt58TJB4DtipWEbj5I_mzJys/edit?usp=sharing
*PR* - https://github.com/apache/spark/pull/15307

Would be wonderful if you can give me feedback on the selection of metrics
and the overall design.

TD
"
Jakob Odersky <jakob@odersky.com>,"Mon, 3 Oct 2016 11:30:56 -0700","Re: java.util.NoSuchElementException when serializing Map with
 default value",Kabeer Ahmed <kabeer@linuxmail.org>,"Hi Kabeer,

which version of Spark are you using? I can't reproduce the error in
latest Spark master.

regards,
--Jakob

---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Mon, 3 Oct 2016 12:01:49 -0700",Re: Running Spark master/slave instances in non Daemon mode,Mike Ihbe <mike@mustwin.com>,"Hi Mike,
I can imagine the trouble that daemonization is causing and I think
that having non-forking start script is a good idea. A simple,
non-intrusive, fix could be to change the ""spark-daemon.sh"" script to
conditionally omit the ""nohup &"".
Personally, I think the semantically correct approach would be to also
rename ""spark-daemon"" to something else (since it won't necessarily
start a background process anymore), however that may have the
potential to break things, in which case it is probably not worth
cosmetic rename.

best,
--Jakob



---------------------------------------------------------------------


"
Kevin Grealish <kevingre@microsoft.com>,"Mon, 3 Oct 2016 20:02:03 +0000","RE: regression: no longer able to use HDFS wasbs:// path for
 additional python files on LIVY batch submit",Steve Loughran <stevel@hortonworks.com>,"Great. Thanks for the pointer. I see the fix is in 2.0.1-rc4.

Will there be a 1.6.3? If so, how are fixes consievel@hortonworks.com]
Sent: Monday, October 3, 2016 5:40 AM
To: Kevin Grealish <kevingre@microsoft.com>
Cc: Apache Spark Dev <dev@spark.apache.org>
Subject: Re: regression: no longer able to use HDFS wasbs:// path for additional python files on LIVY batch submit


On 1 Oct 2016, at 02:49, Kevin Grealish <kevingre@microsoft.com<mailto:kevingre@microsoft.com>> wrote:

Iâ€™m seeing a regression when submitting a batch PySpark program with additional files using LIVY. This is YARN cluster mode. The program files are placed into the mounted Azure Storage before making the call to LIVY. This is happening from an application which has credentials for the storage and the LIVY endpoint, but not local file systems on the cluster. This previously worked but now Iâ€™m getting the error below.

Seems this restriction was introduced with https://github.com/apache/spark/commit/5081a0a9d47ca31900ea4de570de2cbb0e063105<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fapache%2Fspark%2Fcommit%2F5081a0a9d47ca31900ea4de570de2cbb0e063105&data=01%7C01%7Ckevingre%40microsoft.com%7C6de8fd563cb143a4015108d3eb8a73a9%7C72f988bf86f141af91ab2d7cd011db47%7C1&sdata=YiYyvdkzUMPKAHC6hPzN2kKm6vkgJWsb4a6KpkSUa18%3D&reserved=0> (new in 1.6.2 and 2.0.0).

How should the scenario above be achieved now? Am I missing something?

This has been fixed in https://issues.apache.org/jira/browse/SPARK-17512<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fissues.apache.org%2Fjira%2Fbrowse%2FSPARK-17512&data=01%7C01%7Ckevingre%40microsoft.com%7C6de8fd563cb143a4015108d3eb8a73a9%7C72f988bf86f141af91ab2d7cd011db47%7C1&sdata=zh7rOQL1s2ZSIdqW%2Fz0PktGPcFpMQ7HRFKETp5qIhJk%3D&reserved=0> ; I don't know if its in 2.0.1 though



Exception in thread ""main"" java.lang.IllegalArgumentException: Launching Python applications through spark-submit is currently only supported for local files: wasb://kevingrecluster2@xxxxxxxx.blob.core.windows.net/xxxxxxxxx/xxxxxxx.py
                at org.apache.spark.deploy.PythonRunner$.formatPath(PythonRunner.scala:104)
                at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
                at org.apache.spark.deploy.PythonRunner$$anonfun$formatPaths$3.apply(PythonRunner.scala:136)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
                at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
                at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
                at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
                at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
                at org.apache.spark.deploy.PythonRunner$.formatPaths(PythonRunner.scala:136)
                at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$11.apply(SparkSubmit.scala:639)
                at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$11.apply(SparkSubmit.scala:637)
                at scala.Option.foreach(Option.scala:236)
                at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:637)
                at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:154)
                at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
                at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
java.lang.Exception: spark-submit exited with code 1}.

"
Joseph Bradley <joseph@databricks.com>,"Mon, 3 Oct 2016 13:39:23 -0700",Re: Nominal Attribute,Danil Kirsanov <danil.kirsanov@gmail.com>,"There are plans...but not concrete ones yet:
https://issues.apache.org/jira/browse/SPARK-8515
I agree categorical data handling is a pain point and that we need to
improve it!


"
Herman Yu <herman.yu@teeupdata.com>,"Mon, 3 Oct 2016 17:10:31 -0400",access spark thrift server from another spark session,dev@spark.apache.org,"
I built spark data frame/dataset on top of several hive tables, and then registered dataframe/dataset as temporary tables, as well as exposed the temporary table through spark thrift server. Now the question is, this temporary table is only visible to the same spark session. How do I make the registered temporary table available to another spark session? 

I have tried manually set the hive.server2.thrift.port in the new spark session,but the temporary table is still not seen. I also tried connecting to thrift server through JDBC from the 2nd spark session, it doesnâ€™t work either (seems spark doesnâ€™t like the idea accessing hive through JDBC).

so the questions is: is it possible to share data frame/dataset based temporary tables through Spark thrift server between multiple spark sessions?

Thanks
Herman.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 3 Oct 2016 22:46:43 -0700",welcoming Xiao Li as a committer,"""dev@spark.apache.org"" <dev@spark.apache.org>, Xiao Li <gatorsmile@gmail.com>","Hi all,

Xiao Li, aka gatorsmile, has recently been elected as an Apache Spark
committer. Xiao has been a super active contributor to Spark SQL. Congrats
and welcome, Xiao!

- Reynold
"
Felix Cheung <felixcheung_m@hotmail.com>,"Tue, 4 Oct 2016 06:08:59 +0000",Re: welcoming Xiao Li as a committer,"Reynold Xin <rxin@databricks.com>, Xiao Li <gatorsmile@gmail.com>,
	""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats and welcome, Xiao!


_____________________________
From: Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>
Sent: Monday, October 3, 2016 10:47 PM
Subject: welcoming Xiao Li as a committer
To: Xiao Li <gatorsmile@gmail.com<mailto:gatorsmile@gmail.com>>, <dev@spark.apache.org<mailto:dev@spark.apache.org>>


Hi all,

Xiao Li, aka gatorsmile, has recently been elected as an Apache Spark committer. Xiao has been a super active contributor to Spark SQL. Congrats and welcome, Xiao!

- Reynold



"
"""Jagadeesan As"" <as2@us.ibm.com>","Tue, 4 Oct 2016 11:48:22 +0530",Re: welcoming Xiao Li as a committer,Xiao Li <gatorsmile@gmail.com>,"Congratulations Xiao Li.

Cheers
Jagadeesan A S



From:   Reynold Xin <rxin@databricks.com>
To:     ""dev@spark.apache.org"" <dev@spark.apache.org>, Xiao Li 
<gatorsmile@gmail.com>
Date:   04-10-16 11:17 AM
Subject:        welcoming Xiao Li as a committer



Hi all,

Xiao Li, aka gatorsmile, has recently been elected as an Apache Spark 
committer. Xiao has been a super active contributor to Spark SQL. Congrats 
and welcome, Xiao!

- Reynold



"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 3 Oct 2016 23:26:33 -0700",Re: welcoming Xiao Li as a committer,Jagadeesan As <as2@us.ibm.com>,"Congratulations, Xiao!

Bests,
Dongjoon.


"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 3 Oct 2016 23:29:04 -0700",Re: welcoming Xiao Li as a committer,Reynold Xin <rxin@databricks.com>,"Congratulations Sean !!!



-- 
Sent from my Mobile device
"
"""Dilip Biswal"" <dbiswal@us.ibm.com>","Mon, 3 Oct 2016 23:51:55 -0700",Re: welcoming Xiao Li as a committer,Reynold Xin <rxin@databricks.com>,"Hi Xiao,

Congratulations Xiao !!  This is indeed very well deserved !! 

Regards,
Dilip Biswal
Tel: 408-463-4980
dbiswal@us.ibm.com



From:   Reynold Xin <rxin@databricks.com>
To:     ""dev@spark.apache.org"" <dev@spark.apache.org>, Xiao Li 
<gatorsmile@gmail.com>
Date:   10/03/2016 10:47 PM
Subject:        welcoming Xiao Li as a committer



Hi all,

Xiao Li, aka gatorsmile, has recently been elected as an Apache Spark 
committer. Xiao has been a super active contributor to Spark SQL. Congrats 
and welcome, Xiao!

- Reynold



"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 4 Oct 2016 15:59:06 +0900",Re: welcoming Xiao Li as a committer,Dilip Biswal <dbiswal@us.ibm.com>,"Congratulations!

2016-10-04 15:51 GMT+09:00 Dilip Biswal <dbiswal@us.ibm.com>:

"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 4 Oct 2016 16:00:27 +0900",Re: welcoming Xiao Li as a committer,Xiao Li <gatorsmile@gmail.com>,"congrats, xiao!




-- 
---
Takeshi Yamamuro
"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 04 Oct 2016 07:03:09 +0000",Re: welcoming Xiao Li as a committer,"Takeshi Yamamuro <linguin.m.s@gmail.com>, Xiao Li <gatorsmile@gmail.com>","Congrats, Xiao!

"
Marcin Tustin <mtustin@handybook.com>,"Tue, 4 Oct 2016 06:54:17 -0400",Re: welcoming Xiao Li as a committer,,"Congratulations Xiao ðŸŽ‰


s

-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Cheng Lian <lian@databricks.com>,"Tue, 4 Oct 2016 18:56:53 +0800",Re: welcoming Xiao Li as a committer,"""dev@spark.apache.org"" <dev@spark.apache.org>, Xiao Li <gatorsmile@gmail.com>","Congratulations!!!

Cheng


"
Tarun Kumar <tarunk1407@gmail.com>,"Tue, 04 Oct 2016 10:59:48 +0000",Re: welcoming Xiao Li as a committer,"Cheng Lian <lian@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>, 
	Xiao Li <gatorsmile@gmail.com>","Congrats Xiao.

Thanks
Tarun

"
Kevin <kevinyu98@gmail.com>,"Tue, 4 Oct 2016 06:40:30 -0700",Re: welcoming Xiao Li as a committer,Tarun Kumar <tarunk1407@gmail.com>,"Congratulations Xiao!!

Sent from my iPhone

mitter. Xiao has been a super active contributor to Spark SQL. Congrats and welcome, Xiao!
"
Weiqing Yang <yangweiqing001@gmail.com>,"Tue, 4 Oct 2016 08:48:19 -0700",Re: welcoming Xiao Li as a committer,Kevin <kevinyu98@gmail.com>,"Congrats Xiao!


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 4 Oct 2016 09:01:28 -0700",Re: welcoming Xiao Li as a committer,"""dev@spark.apache.org"" <dev@spark.apache.org>, Xiao Li <gatorsmile@gmail.com>","Congratulations Xiao! Very well deserved!


"
Yanbo Liang <ybliang8@gmail.com>,"Tue, 4 Oct 2016 09:09:05 -0700",Re: welcoming Xiao Li as a committer,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Congrats and welcome!


ts
"
Mridul Muralidharan <mridulm80@apache.org>,"Tue, 4 Oct 2016 10:25:29 -0700",Edit access for spark confluence wiki,dev@spark.apache.org,"Can someone add me to edit list for the spark wiki please ?

Thanks,
Mridul

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 4 Oct 2016 10:39:04 -0700",[ANNOUNCE] Announcing Spark 2.0.1,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","We are happy to announce the availability of Spark 2.0.1!

Apache Spark 2.0.1 is a maintenance release containing 300 stability and
bug fixes. This release is based on the branch-2.0 maintenance branch of
Spark. We strongly recommend all 2.0.0 users to upgrade to this stable
release.

To download Apache Spark 2.0.1, visit http://spark.apache.org/downloads.html

We would like to acknowledge all community members for contributing patches
to this release.
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 4 Oct 2016 10:47:28 -0700",Re: welcoming Xiao Li as a committer,Xiao Li <gatorsmile@gmail.com>,"Congrats!


:
ats
"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Tue, 4 Oct 2016 11:14:13 -0700",Re: welcoming Xiao Li as a committer,"""dev@spark.apache.org"" <dev@spark.apache.org>,
 Xiao Li <gatorsmile@gmail.com>","Congratulations, Xiao!



committer. Xiao has been a super active contributor to Spark SQL. Congrats and welcome, Xiao!


---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 4 Oct 2016 11:14:48 -0700",Re: welcoming Xiao Li as a committer,Suresh Thalamati <suresh.thalamati@gmail.com>,"Congratulations :D :) Yay!




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Jakob Odersky <jakob@odersky.com>,"Tue, 4 Oct 2016 11:53:03 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",shivaram@eecs.berkeley.edu,"Hi everyone,

is there any ongoing discussion/documentation on the redesign of sinks?
I think it could be a good thing to abstract away the underlying
streaming model, however that isn't directly related to Holden's first
point. The way I understand it, is to slightly change the
DataStreamWriter API (the thing that's returned when you call
""df.writeStream"") to allow passing in a custom sink provider instead
of only accepting strings. This would allow users to write their own
providers and sinks, and give them a strongly typed, possibly generic
way to do so. The sink api is currently available to users indirectly
(you can create your own sink provider and load it with the built-in
DataSource reflection functionality), therefore I don't quite
understand why exposing it indirectly through a typed interface should
be delayed before finalizing the API.
they are currently only available through a stringly-typed interface.
Could a similar solution be applied to sources? Maybe the writer and
reader api's could even be unified to a certain degree.

Shivaram, I like your ideas on the proposed redesign! Can we discuss
this further?

cheers,
--Jakob


rk/
e:
ed to jump
 on
6407
o
er
s
eady
ils
t or
d by
n
€™t need
e
 of
ses
sing
ted
f
e
hich
nd
ove to know what
 for structured
6407
K2qGiFQ/edit?usp=sharing
reasonable.

---------------------------------------------------------------------


"
Prajwal Tuladhar <praj@infynyxx.com>,"Tue, 4 Oct 2016 19:58:46 +0000",Re: [ANNOUNCE] Announcing Spark 2.0.1,Reynold Xin <rxin@databricks.com>,"Hi,

It seems like, 2.0.1 artifact hasn't been published to Maven Central. Can
anyone confirm?




-- 
--
Cheers,
Praj
"
Reynold Xin <rxin@databricks.com>,"Tue, 4 Oct 2016 13:23:19 -0700",Re: [ANNOUNCE] Announcing Spark 2.0.1,Prajwal Tuladhar <praj@infynyxx.com>,"They have been published yesterday, but can take a while to propagate.



"
Michael Armbrust <michael@databricks.com>,"Tue, 4 Oct 2016 15:02:55 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Jakob Odersky <jakob@odersky.com>,"
Spark has a long history
<https://spark-project.atlassian.net/browse/SPARK-1094> of maintaining
binary compatibility in its public APIs.  I strongly believe this is one of
the things that has made the project successful.  Exposing internals that
we know are going to change in the primary user facing API for creating
Streaming DataFrames seems directly counter to this goal.  I think the
argument that ""you can do it anyway"" fails to capture user expectations who
probably aren't closely following this discussion.

If advanced users want to dig though the code and experiment, great.  I
hope they report back on whats good and what can be improved.  However, if
you add the function suggested in the PR to DataStreamReader, you are
giving them a bad experience by leaking internals that don't even show up
in the published documentation.
"
Bryan Cutler <cutlerb@gmail.com>,"Tue, 4 Oct 2016 15:44:25 -0700",Re: welcoming Xiao Li as a committer,,"Congrats Xiao!


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Wed, 5 Oct 2016 08:09:54 +0900",Re: welcoming Xiao Li as a committer,Xiao Li <gatorsmile@gmail.com>,"Congratulations Xiao!

- Kousuke


"
Samkit Shah <samkit993@gmail.com>,"Wed, 5 Oct 2016 11:14:25 +0530",[ML]Random Forest Error : Size exceeds Integer.MAX_VALUE,dev@spark.apache.org,"Hello folks,
I am running Random Forest from ml from spark 1.6.1 on bimbo[1] dataset
with following configurations:

""-Xms16384M"" ""-Xmx16384M"" ""-Dspark.locality.wait=0s""
""-Dspark.driver.extraJavaOptions=-Xss10240k -XX:+PrintGCDetails
-XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution
-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ParallelGCThreads=2
-XX:-UseAdaptiveSizePolicy -XX:ConcGCThreads=2 -XX:-UseGCOverheadLimit
 -XX:CMSInitiatingOccupancyFraction=75 -XX:NewSize=8g -XX:MaxNewSize=8g
-XX:SurvivorRatio=3 -DnumPartitions=36"" ""-Dspark.submit.deployMode=cluster""
""-Dspark.speculation=true"" ""-Dspark.speculation.multiplier=2""
""-Dspark.driver.memory=16g"" ""-Dspark.speculation.interval=300ms""
 ""-Dspark.speculation.quantile=0.5"" ""-Dspark.akka.frameSize=768""
""-Dspark.driver.supervise=false"" ""-Dspark.executor.cores=6""
""-Dspark.executor.extraJavaOptions=-Xss10240k -XX:+PrintGCDetails
-XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution
-XX:-UseAdaptiveSizePolicy -XX:+UseParallelGC -XX:+UseParallelOldGC
-XX:ParallelGCThreads=6 -XX:NewSize=22g -XX:MaxNewSize=22g
-XX:SurvivorRatio=2 -XX:+PrintAdaptiveSizePolicy -XX:+PrintGCDateStamps""
""-Dspark.rpc.askTimeout=10"" ""-Dspark.executor.memory=40g""
""-Dspark.driver.maxResultSize=3g"" ""-Xss10240k"" ""-XX:+PrintGCDetails""
""-XX:+PrintGCTimeStamps"" ""-XX:+PrintTenuringDistribution""
""-XX:+UseConcMarkSweepGC"" ""-XX:+UseParNewGC"" ""-XX:ParallelGCThreads=2""
""-XX:-UseAdaptiveSizePolicy"" ""-XX:ConcGCThreads=2""
""-XX:-UseGCOverheadLimit"" ""-XX:CMSInitiatingOccupancyFraction=75""
""-XX:NewSize=8g"" ""-XX:MaxNewSize=8g"" ""-XX:SurvivorRatio=3""
""-DnumPartitions=36"" ""org.apache.spark.deploy.worker.DriverWrapper""
""spark://Worker@11.0.0.106:56419""


I get following error:
16/10/04 06:55:05 WARN TaskSetManager: Lost task 8.0 in stage 19.0 (TID
194, 11.0.0.106): java.lang.IllegalArgumentException: Size exceeds
Integer.MAX_VALUE
at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869)
at
org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:127)
at
org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:115)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1250)
at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:129)
at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:136)
at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:503)
at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:420)
at org.apache.spark.storage.BlockManager.get(BlockManager.scala:625)
at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:154)
at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
at
org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)


I have varied number of partitions from 24 to 48. I still get the same
error. How can this problem be tackled?


Thanks,
Samkit




[1]: https://www.kaggle.com/c/grupo-bimbo-inventory-demand
"
Joseph Bradley <joseph@databricks.com>,"Tue, 4 Oct 2016 23:40:13 -0700",Re: welcoming Xiao Li as a committer,Xiao Li <gatorsmile@gmail.com>,"Congrats!


"
Joseph Bradley <joseph@databricks.com>,"Tue, 4 Oct 2016 23:41:51 -0700",Re: [ML]Random Forest Error : Size exceeds Integer.MAX_VALUE,Samkit Shah <samkit993@gmail.com>,"Could you please file a bug report JIRA and also include more info about
what you ran?
* Random forest Param settings
* dataset dimensionality, partitions, etc.
Thanks!


"
Kabeer Ahmed <kabeer@gmx.co.uk>,"Wed, 05 Oct 2016 10:01:28 +0100","Re: java.util.NoSuchElementException when serializing Map with
 default value",Jakob Odersky <jakob@odersky.com>,"Hi Jakob,

I had multiple versions of Spark installed in my machine. The code now 
works without issues in spark-shell and the IDE. I have verified this 
with Spark 1.6 and 2.0.

Cheers,
Kabeer.






"
Michael Gummelt <mgummelt@mesosphere.io>,"Wed, 5 Oct 2016 12:06:33 -0700",Re: [ANNOUNCE] Announcing Spark 2.0.1,Reynold Xin <rxin@databricks.com>,"There seems to be no 2.0.1 tag?

https://github.com/apache/spark/tags




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Wed, 5 Oct 2016 12:11:21 -0700",Re: [ANNOUNCE] Announcing Spark 2.0.1,Michael Gummelt <mgummelt@mesosphere.io>,"There is now. Thanks for the email.


"
Sean Owen <sowen@cloudera.com>,"Wed, 05 Oct 2016 19:16:35 +0000",Re: [ANNOUNCE] Announcing Spark 2.0.1,Michael Gummelt <mgummelt@mesosphere.io>,"https://github.com/apache/spark/releases/tag/v2.0.1 ?


"
Holden Karau <holden@pigscanfly.ca>,"Wed, 5 Oct 2016 14:05:59 -0700","PySpark UDF Performance Exploration w/Jython (Early/rough 2~3X
 improvement*) [SPARK-15369]","""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi Python Spark Developers & Users,

As Datasets/DataFrames are becoming the core building block of Spark, and
as someone who cares about Python Spark performance, I've been looking more
at PySpark UDF performance.

I've got an early WIP/request for comments pull request open
<https://github.com/apache/spark/pull/13571> with a corresponding design
document
and
JIRA (SPARK-15369) <https://issues.apache.org/jira/browse/SPARK-15369> that
allows for selective UDF evaluation in Jython <http://www.jython.org/>. Now
that Spark 2.0.1 is out I'd really love peoples input or feedback on this
proposal so I can circle back with a more complete PR :) I'd love to hear
from people using PySpark if this is something which looks interesting (as
well as the PySpark developers) for some of the open questions :)

For users: If you have simple Python UDFs (or even better UDFs and
datasets) that you can share for bench-marking it would be really useful to
be able to add them to the bench-marking I've been looking at in the design
doc. It would also be useful to know if some, many, or none, of your UDFs
can be evaluated by Jython. If you have UDF you aren't comfortable sharing
on-list feel free to each out to me directly.

Some general open questions:

1) The draft PR does some magic** to allow being passed in functions at
least some of the time - is that something which people are interested in
or would it be better to leave the magic out and just require a string
representing the lambda be passed in?

2) Would it be useful to provide easy steps to use JyNI <http://jyni.org/>
 (its LGPL licensed <https://www.gnu.org/licenses/lgpl-3.0.en.html> so I
don't think we we can include it out of the bo
<https://www.apache.org/legal/resolved.html#category-x>x - but we could try
and make it easy for users to link with if its important)?

3) While we have a 2x speedup for tokenization/wordcount (getting close to
native scala perf) - what is performance like for other workloads (please
share your desired UDFs/workloads for my evil bench-marking plans)?

4) What does the eventual Dataset API look like for Python? (This could
partially influence #1)?

5) How important it is to not add the Jython dependencies to the weight for
non-Python users (and if desired which work around to chose - maybe
something like spark-hive?)

6) Do you often chain PySpark UDF operations and is that something we
should try and optimize for in Jython as well?

7) How many of your Python UDFs can / can not be evaluated in Jython for
one reason or another?

8) Do your UDFs depend on Spark accumulators or broadcast values?

9) What am I forgetting in my coffee fueled happiness?

Cheers,

Holden :)

*Bench-marking has been very limited 2~3X improvement likely different for
""real"" work loads (unless you really like doing wordcount :p :))
** Note: magic depends on dill <https://pypi.python.org/pypi/dill>.

P.S.

I leave you with this optimistic 80s style intro screen
<https://twitter.com/holdenkarau/status/783762213408497670> :)
Also if anyone happens to be going to PyData DC <http://pydata.org/dc2016/>
this weekend I'd love to chat with you in person about this (and of course
circle it back to the mailing list).
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Fred Reiss <freiss.oss@gmail.com>,"Wed, 5 Oct 2016 14:36:22 -0700",Re: welcoming Xiao Li as a committer,,"Congratulations, Xiao!

Fred


"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 5 Oct 2016 14:38:08 -0700",Re: welcoming Xiao Li as a committer,Fred Reiss <freiss.oss@gmail.com>,"Congrats, Xiao!

Sincerely,

DB Tsai
----------------------------------------------------------
Web: https://www.dbtsai.com
PGP Key ID: 0x9DCC1DBD7FC7BBB2



---------------------------------------------------------------------


"
Fred Reiss <freiss.oss@gmail.com>,"Wed, 5 Oct 2016 15:20:07 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Michael Armbrust <michael@databricks.com>,"Thanks for the thoughtful comments, Michael and Shivaram. From what Iâ€™ve
seen in this thread and on JIRA, it looks like the current plan with regard
to application-facing APIs for sinks is roughly:
1. Rewrite incremental query compilation for Structured Streaming.
2. Redesign Structured Streaming's source and sink APIs so that they do not
depend on RDDs.
3. Allow the new APIs to stabilize.
4. Open these APIs to use by application code.

Is there a way for those of us who arenâ€™t involved in the first two steps
to get some idea of the current plans and progress? I get asked a lot about
when Structured Streaming will be a viable replacement for Spark Streaming,
and I like to be able to give accurate advice.

Fred


of
ho
f
"
Liwei Lin <lwlin7@gmail.com>,"Thu, 6 Oct 2016 09:38:57 +0800",Re: welcoming Xiao Li as a committer,"Xiao Li <gatorsmile@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Congratulations, Xiao!

Cheers,
Liwei


"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 5 Oct 2016 20:37:36 -0700",Re: [ANNOUNCE] Announcing Spark 2.0.1,Reynold Xin <rxin@databricks.com>,"It usually don't take that long to be synced, I still don't see any 2.0.1
related artifacts on maven central

http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22%20AND%20v%3A%222.0.1%22





-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Reynold Xin <rxin@databricks.com>,"Wed, 5 Oct 2016 21:42:48 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Tom Graves <tgraves_cs@yahoo.com>,"I think this is fairly important to do so I went ahead and created a PR for
the first mini step: https://github.com/apache/spark/pull/15374




"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 5 Oct 2016 22:18:44 -0700",Re: [ANNOUNCE] Announcing Spark 2.0.1,Luciano Resende <luckbr1975@gmail.com>,"Yeah I see the apache maven repos have the 2.0.1 artifacts at
https://repository.apache.org/content/repositories/releases/org/apache/spark/spark-core_2.11/
-- Not sure why they haven't synced to maven central yet

Shivaram


---------------------------------------------------------------------


"
Jan-Hendrik Zab <jan@jhz.name>,"Thu, 6 Oct 2016 15:33:20 +0200",Apache Spark chat channel,dev@spark.apache.org,"Hello!

There was a request on scala-debate [0] to create a Spark centric chat
room under the scala namespace on Gitter with a focus on Scala related
questions.

This is just a heads up to the Apache Spark ""management"" to give them a
chance to get involved. It might be better to create a dedicated channel
under the Apache umbrella to better serve all users and not only those
using Scala. Avoiding any artificial split of the Spark community.
Reasons for having such a channel can be found in the linked thread.

ps.
Please CC me, since I'm not on the list.

Best,
        -jhz

(Resent, something apparently ate my first e-mail.)

[0] - https://groups.google.com/forum/#!topic/scala-debate/OVGnIU2SNmc

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 06 Oct 2016 13:36:58 +0000",Re: Apache Spark chat channel,"Jan-Hendrik Zab <jan@jhz.name>, dev@spark.apache.org","Yes this come up once in a while. There's no need or way to stop people
forming groups to chat, though blessing a new channel as 'official' is
tough because it means, in theory, everyone has to follow another channel
to see 100% of the discussion. I think that's why the couple of mailing
lists, which can be controlled and archived by the ASF, will stay the
official channels. But, naturally there's no problem with people forming
unofficial communities.


"
Dean Wampler <deanwampler@gmail.com>,"Thu, 6 Oct 2016 09:21:07 -0500",Re: Apache Spark chat channel,Sean Owen <sowen@cloudera.com>,"Since I'm a Scala Spark advocate, I'll try to get a Scala Spark Gitter
channel created, one way or another.

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Lightbend <http://lightbend.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Alexander Oleynikov <oleynikovav@gmail.com>,"Thu, 6 Oct 2016 16:38:11 +0200",Monitoring system extensibility,dev@spark.apache.org,"Hi. 

As of v2.0.1, the traits `org.apache.spark.metrics.source.Source` and `org.apache.spark.metrics.sink.Sink` are defined as private to â€˜sparkâ€™ package, so it becomes troublesome to create a new implementation in the userâ€™s code (but still possible in a hacky way).
This seems to be the only missing piece to allow extension of the metrics system and I wonder whether is was conscious design decision to limit the visibility. Is it possible to broaden the visibility scope for these traits in the future versions?

Thanks,
Alexander
---------------------------------------------------------------------


"
vonnagy <ivan@vadio.com>,"Thu, 6 Oct 2016 09:21:09 -0700 (MST)",Submit job with driver options in Mesos Cluster mode,dev@spark.apache.org,"I am trying to submit a job to spark running in a Mesos cluster. We need to
pass custom java options to the driver and executor for configuration, but
the driver task never includes the options. Here is an example submit. 

GC_OPTS=""-XX:+UseConcMarkSweepGC 
         -verbose:gc -XX:+PrintGCTimeStamps -Xloggc:$appdir/gc.out 
         -XX:MaxPermSize=512m 
         -XX:+CMSClassUnloadingEnabled "" 

EXEC_PARAMS=""-Dloglevel=DEBUG -Dkafka.broker-address=${KAFKA_ADDRESS}
-Dredis.master=${REDIS_MASTER} -Dredis.port=${REDIS_PORT} 

spark-submit \ 
  --name client-events-intake \ 
  --class ClientEventsApp \ 
  --deploy-mode cluster \ 
  --driver-java-options ""${EXEC_PARAMS} ${GC_OPTS}"" \ 
  --conf ""spark.ui.killEnabled=true"" \ 
  --conf ""spark.mesos.coarse=true"" \ 
  --conf ""spark.driver.extraJavaOptions=${EXEC_PARAMS}"" \ 
  --conf ""spark.executor.extraJavaOptions=${EXEC_PARAMS}"" \ 
  --master mesos://someip:7077 \ 
  --verbose \ 
  some.jar 

When the driver task runs in Mesos it is creating the following command: 

sh -c 'cd spark-1*;  bin/spark-submit --name client-events-intake --class
ClientEventsApp --master mesos://someip:5050 --driver-cores 1.0
--driver-memory 512M ../some.jar ' 

There are no options for the driver here, thus the driver app blows up
because it can't find the java options. However, the environment variables
contain the executor options: 

SPARK_EXECUTOR_OPTS -> -Dspark.executor.extraJavaOptions=-Dloglevel=DEBUG
... 

Any help would be great. I know that we can set some ""spark.*"" settings in
default configs, but these are not necessarily spark related. This is not an
issue when running the same logic outside of a Mesos cluster in Spark
standalone mode. 

Thanks! 



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 6 Oct 2016 12:37:56 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Fred Reiss <freiss.oss@gmail.com>,"Fred, I think thats a pretty good summary of my thoughts.  Thanks for
condensing them :)

Right now, my focus is to get more people using Structured Streaming so
that we can get some real world feedback on what is missing.  Right now
this means:
 - SPARK-15406 <https://issues.apache.org/jira/browse/SPARK-15406> Kafka
Support - since this seems to be the source of choice for many users
 - SPARK-17731 <https://issues.apache.org/jira/browse/SPARK-17731> Metrics
- right now its pretty hard to see what is going on, and where latency is
coming from.

the work on #1.

Relatedly, I'm curious to hear more about the types of questions you are
getting.  I think the dev list is a good place to discuss applications and
if/how structured streaming can handle them.


€™ve
rd
wo steps
ut
g,
 of
t
who
if
p
"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 6 Oct 2016 17:35:54 -0700",Re: [ANNOUNCE] Announcing Spark 2.0.1,shivaram@eecs.berkeley.edu,"I have created a Infra jira to track the issue with the maven artifacts for
Spark 2.0.1





-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Cody Koeninger <cody@koeninger.org>,"Thu, 6 Oct 2016 21:51:59 -0500",Spark Improvement Proposals,"""dev@spark.apache.org"" <dev@spark.apache.org>","I love Spark.  3 or 4 years ago it was the first distributed computing
environment that felt usable, and the community was welcoming.

But I just got back from the Reactive Summit, and this is what I observed:

- Industry leaders on stage making fun of Spark's streaming model
- Open source project leaders saying they looked at Spark's governance
as a model to avoid
- Users saying they chose Flink because it was technically superior
and they couldn't get any answers on the Spark mailing lists

Whether you agree with the substance of any of this, when this stuff
gets repeated enough people will believe it.

Right now Spark is suffering from its own success, and I think
something needs to change.

- We need a clear process for planning significant changes to the codebase.
I'm not saying you need to adopt Kafka Improvement Proposals exactly,
but you need a documented process with a clear outcome (e.g. a vote).
Passing around google docs after an implementation has largely been
decided on doesn't cut it.

- All technical communication needs to be public.
Things getting decided in private chat, or when 1/3 of the committers
work for the same company and can just talk to each other...
Yes, it's convenient, but it's ultimately detrimental to the health of
the project.
The way structured streaming has played out has shown that there are
significant technical blind spots (myself included).
involved, and listen to them.

- We need more committers, and more committer diversity.
Per committer there are, what, more than 20 contributors and 10 new
jira tickets a month?  It's too much.
There are people (I am _not_ referring to myself) who have been around
for years, contributed thousands of lines of code, helped educate the
public around Spark... and yet are never going to be voted in.

- We need a clear process for managing volunteer work.
Too many tickets sit around unowned, unclosed, uncertain.
If someone proposed something and it isn't up to snuff, tell them and
close it.  It may be blunt, but it's clearer than ""silent no"".
If someone wants to work on something, let them own the ticket and set
a deadline. If they don't meet it, close it or reassign it.

This is not me putting on an Apache Bureaucracy hat.  This is me
saying, as a fellow hacker and loyal dissenter, something is wrong
with the culture and process.

Please, let's change it.

---------------------------------------------------------------------


"
"""DW @ Gmail"" <deanwampler@gmail.com>","Thu, 6 Oct 2016 22:48:13 -0500",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"I was there, too. I agree with Cody's assessments and recommendations

Dean

Sent from my rotary phone. 



---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Oct 2016 21:14:21 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Hey Cody,

Thanks for bringing these things up. You're talking about quite a few different things here, but let me get to them each in turn.

1) About technical / design discussion -- I fully agree that everything big should go through a lot of review, and I like the idea of a more formal way to propose and comment on larger features. So far, all of this has been done through JIRA, but as a start, maybe marking JIRAs as large (we often use Umbrella for this) and also opening a thread on the list about each such JIRA would help. For Structured Streaming in particular, FWIW, there was a pretty complete doc on the proposed semantics at https://issues.apache.org/jira/browse/SPARK-8360 since March. But it's true that other things such as the Kafka source for it didn't have as much design on JIRA. Nonetheless, this component is still early on and there's still a lot of time to change it, which is happening.

2) About what people say at Reactive Summit -- there will always be trolls, but just ignore them and build a great project. Those of us involved in the project for a while have long seen similar stuff, e.g. a prominent company saying Spark doesn't scale past 100 nodes when there were many documented instances to the contrary, and the best answer is to just make the project better. This same company, if you read their website now, recommends Apache Spark for most anything. For streaming in particular, there is a lot of confusion because many of the concepts aren't well-defined (e.g. what is ""at least once"", etc), and it's also a crowded space. But Spark Streaming prioritizes a few things that it does very well: correctness (you can easily tell what the app will do, and it does the same thing despite failures), ease of programming (which also requires correctness), and scalability. We should of course both explain what it does in more places and work on improving it where needed (e.g. adding a higher level API with Structured Streaming and built-in primitives for external timestamps).

3) About number and diversity of committers -- the PMC is always working to expand these, and you should email people on the PMC (or even the whole list) if you have people you'd like to propose. In general I think nearly all committers added in the past year were from organizations that haven't long been involved in Spark, and the number of committers continues to grow pretty fast.

4) Finally, about better organizing JIRA, marking dead issues, etc, this would be great and I think we just need a concrete proposal for how to do it. It would be best to point to an existing process that someone else has used here BTW so that we can see it in action.

Matei

observed:
codebase.


---------------------------------------------------------------------


"
Xiao Li <gatorsmile@gmail.com>,"Thu, 6 Oct 2016 21:53:16 -0700",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"Let us continue to improve Apache Spark!

I volunteer to go through all the SQL-related open JIRAs.

Xiao Li

2016-10-06 21:14 GMT-07:00 Matei Zaharia <matei.zaharia@gmail.com>:
ferent things here, but let me get to them each in turn.
ig should go through a lot of review, and I like the idea of a more formal way to propose and comment on larger features. So far, all of this has been done through JIRA, but as a start, maybe marking JIRAs as large (we often use Umbrella for this) and also opening a thread on the list about each such JIRA would help. For Structured Streaming in particular, FWIW, there was a pretty complete doc on the proposed semantics at https://issues.apache.org/jira/browse/SPARK-8360 since March. But it's true that other things such as the Kafka source for it didn't have as much design on JIRA. Nonetheless, this component is still early on and there's still a lot of time to change it, which is happening.
s, but just ignore them and build a great project. Those of us involved in the project for a while have long seen similar stuff, e.g. a prominent company saying Spark doesn't scale past 100 nodes when there were many documented instances to the contrary, and the best answer is to just make the project better. This same company, if you read their website now, recommends Apache Spark for most anything. For streaming in particular, there is a lot of confusion because many of the concepts aren't well-defined (e.g. what is ""at least once"", etc), and it's also a crowded space. But Spark Streaming prioritizes a few things that it does very well: correctness (you can easily tell what the app will do, and it does the same thing despite failures), ease of programming (which also requires correctness), and scalability. We should of course both explain what it does in more places and work on improving it where needed (e.g. adding a higher level API with Structured Streaming and built-in primitives for external timestamps).
to expand these, and you should email people on the PMC (or even the whole list) if you have people you'd like to propose. In general I think nearly all committers added in the past year were from organizations that haven't long been involved in Spark, and the number of committers continues to grow pretty fast.
would be great and I think we just need a concrete proposal for how to do it. It would be best to point to an existing process that someone else has used here BTW so that we can see it in action.
d:
se.

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 07 Oct 2016 06:45:04 +0000",Re: [ANNOUNCE] Announcing Spark 2.0.1,Luciano Resende <luckbr1975@gmail.com>,"I believe Reynold mentioned he already did that. For anyone following:
https://issues.apache.org/jira/browse/INFRA-12717


"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 00:30:10 -0700",Re: Monitoring system extensibility,Alexander Oleynikov <oleynikovav@gmail.com>,"They have always been private, haven't they?

https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/metrics/source/Source.scala




parkâ€™
ts
"
Boris Lenzinger <boris.lenzinger@gmail.com>,"Fri, 7 Oct 2016 09:44:55 +0200",Fwd: Looking for a Spark-Python expert,dev@spark.apache.org,"Hi all,

I don't know where to post this announce so I really apologize to pollute
the ML with such a mail.

I'm looking for an expert in Spark 2.0 and its Python API. I have a
customer that is looking for an expertise mission (for one month but I
guess it can spread on 2 month seeing the goals to reach).

Here is the context : there is a team (3 personnes) that is studying
different solutions for an image processing framework and Spark has been
identified as a candidate. So they want to make a proof of concept around
this with a known use case.

Where does the mission take place ? Sophia-Antipolis in France (French
Riviera). Remote ? Not sure but could be a good solution. I will check and
potentially update the post.

Dates : the mission should start, in a perfect world, mid-October but tell
me your availability and I will try to negociate.

Price : first let's get in touch and send me your resume (or if you are
part of the authors of the framework, I guess it will be ok as a resume :-)
but I'm still interested in your general background so please send me a
resume )

I know that the deadlines are quite short so even if you cannot exactly on
those dates, do not hesitate to apply.

I hope that some of you will be interested in this.

Again sorry for posting on the dev list.

Have a nice day,

boris
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 00:49:22 -0700",Re: Looking for a Spark-Python expert,Boris Lenzinger <boris.lenzinger@gmail.com>,"Boris,

Thanks for the email, but this is not a list for soliciting job
applications. Please do not post any recruiting messages -- otherwise we
will ban your account.



"
Sean Owen <sowen@cloudera.com>,"Fri, 07 Oct 2016 08:05:29 +0000",Re: Looking for a Spark-Python expert,"Boris Lenzinger <boris.lenzinger@gmail.com>, dev@spark.apache.org","dev@ is for the project's own development discussions, so not the right
place. user@ is better, but job postings are discouraged in general on ASF
lists. I think people get away with the occasional legitimate, targeted
message prefixed with [JOBS], but I hesitate to open the flood gates,
because we also have no real way of banning the inevitable spam.


"
=?utf-8?B?546L56OKKOWuieWFqOmDqCk=?= <wangleikidding@didichuxing.com>,"Fri, 7 Oct 2016 08:37:20 +0000",Could we expose log likelihood of EM algorithm in MLLIB?,"""dev@spark.apache.org"" <dev@spark.apache.org>","
Hi,

Do you guys sometimes need to get the log likelihood of EM algorithm in MLLIB?

I mean the value in this line https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/GaussianMixture.scala#L228

Now copying the code here:


        val sums = breezeData.treeAggregate(ExpectationSum.zero(k, d))(compute.value, _ += _)

        // Create new distributions based on the partial assignments
        // (often referred to as the ""M"" step in literature)
        val sumWeights = sums.weights.sum

        if (shouldDistributeGaussians) {
        val numPartitions = math.min(k, 1024)
        val tuples =
        Seq.tabulate(k)(i => (sums.means(i), sums.sigmas(i), sums.weights(i)))
        val (ws, gs) = sc.parallelize(tuples, numPartitions).map { case (mean, sigma, weight) =>
        updateWeightsAndGaussians(mean, sigma, weight, sumWeights)
        }.collect().unzip
        Array.copy(ws.toArray, 0, weights, 0, ws.length)
        Array.copy(gs.toArray, 0, gaussians, 0, gs.length)
        } else {
        var i = 0
        while (i < k) {
        val (weight, gaussian) =
        updateWeightsAndGaussians(sums.means(i), sums.sigmas(i), sums.weights(i), sumWeights)
        weights(i) = weight
        gaussians(i) = gaussian
        i = i + 1
        }
        }

        llhp = llh // current becomes previous
        llh = sums.logLikelihood // this is the freshly computed log-likelihood
        iter += 1
        compute.destroy(blocking = false)
In my application, I need to know log likelihood to compare effect for different number of clusters.
And then I use the cluster number with the maximum log likelihood.

Is it a good idea to expose this value?



"
Pete Robbins <robbinspg@gmail.com>,"Fri, 07 Oct 2016 08:45:27 +0000",Re: Monitoring system extensibility,"Reynold Xin <rxin@databricks.com>, Alexander Oleynikov <oleynikovav@gmail.com>","I brought this up last year and there was a Jira raised:
https://issues.apache.org/jira/browse/SPARK-14151

For now I just have my SInk and Source in an o.a.s package name which is
not ideal but the only way round this.


pache/spark/metrics/source/Source.scala
m
parkâ€™
ts
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 01:50:28 -0700",Re: Monitoring system extensibility,Pete Robbins <robbinspg@gmail.com>,"So to be constructive and in order to actually open up these APIs, it would
be useful for users to comment on the JIRA ticket on their use cases
(rather than ""I want this to be public""), and then we can design an API
that would address those use cases. In some cases the solution is to just
make the existing internal API public. But turning some internal API public
without thinking about whether those APIs are sufficiently expressive and
maintainable is not a great way to design APIs in general.


˜sparkâ€™
e
r
"
Pete Robbins <robbinspg@gmail.com>,"Fri, 07 Oct 2016 09:03:43 +0000",Re: Monitoring system extensibility,Reynold Xin <rxin@databricks.com>,"Which has happened. The last comment being in August with someone saying it
was important to them. They PR has been around since March and despite a
request to be reviewed has not got any committer's attention. Without that,
it is going nowhere. The historic Jiras requesting other sinks such as
Kafka, OpenTSBD etc have also been ignored.

So for now we continue creating classes in o.a.s package.


s
ic
pache/spark/metrics/source/Source.scala
m
parkâ€™
ts
"
Sean Owen <sowen@cloudera.com>,"Fri, 07 Oct 2016 10:34:59 +0000",Re: Spark Improvement Proposals,"Matei Zaharia <matei.zaharia@gmail.com>, Cody Koeninger <cody@koeninger.org>","Suggestion actions way at the bottom.


since March. But it's true that other things such as the Kafka source for
it didn't have as much design on JIRA. Nonetheless, this component is still
early on and there's still a lot of time to change it, which is happening.


It's hard to drive design discussions in OSS. Even when diligently
publishing design docs, the doc happens after brainstorming, and that
happens inside someone's head or in chats.

The lazy consensus model that works for small changes doesn't work well
here. If a committer wants a change, that change will basically be made
modulo small edits; vetoes are for dire disagreement. (Otherwise we'd get
nothing done.) However this model means it's hard to significantly change a
design after draft 1.

I've heard this complaint a few times, and it has never been down to bad
faith. We should err further towards over-including early and often. I've
seen some great discussions start more with a problem statement and an RFC,
not a design doc. Keeping regular contributors enfranchised is essential,
so that they're willing and able to participate when design time comes.
(See below.)



2) About what people say at Reactive Summit -- there will always be trolls,
but just ignore them and build a great project. Those of us involved in the
project for a while have long seen similar stuff, e.g. a


The hype cycle may be turning against Spark, as is normal for this stage of
maturity. People idealize technologies they don't really use as greener
grass; it's the things they use and need to work that they love to hate.

I would not dismiss this as just trolling. Customer anecdotes I see suggest
that Spark underperforms their (inflated) expectations, and generally does
not Just Work. It takes expertise, tuning, patience, workarounds. And then
it gets great things done. I do see a gap between how the group here talks
about the technology, and how the users I see talk about it. The gap
manifests in attention given to making yet more things, and attention given
to fixing and project mechanics.

I would also not dismiss criticism of governance. We can recognize some big
problems that were resolved over even the past 3 months. Usually I hear,
well, we do better than most projects, right? and that is true. But, Spark
is bigger and busier than most any other project. Exceptional projects need
exceptional governance and we have merely ""good"". See next.


3) About number and diversity of committers -- the PMC is always working to
expand these, and you should email people on the PMC (or even the whole
list) if you have people you'd like to propose. In


If you're suggesting that it's mostly a matter of asking, then this doesn't
match my experience. I have seen a few people consistently soft-reject most
proposals. The reasons given usually sound like ""concerns about quality"",
which is probably the right answer to a somewhat wrong question.

We should probably be asking primarily who will net-net add efficiency to
some part of the project's mechanics. Per above, it wouldn't hurt to ask
who would expand coverage and add diversity of perspective too.

I disagree that committers are being added at a sufficient rate. The
overall committer-attention hours is dropping as the project grows -- am I
the only one that perceives many regular committers aren't working nearly
as much as before on the project?

I call it a problem because we have IMHO people who 'qualify', and not
giving them some stake is going to cost the project down the road. Always
Be Recruiting. This is what I would worry about, since the governance and
enfranchisement issues above kind of stem from this.



4) Finally, about better organizing JIRA, marking dead issues, etc, this
would be great and I think we just need a concrete proposal for how to do
it. It would be best to point to an existing process that someone else has
used here BTW so that we can see it in action.


I don't think we're wanting for proposals. I went on and on about it last
year, and don't think anyone disagreed about actions. I wouldn't suggest
that clearing out dead issues is more complex than just putting in time to
do it. It's just grunt work and understandably not appealing. (Thank you
Xiao for your recent run at SQL JIRAs.)

It requires saying 'no', which is hard, because it requires some
conviction. I have encountered reluctance to do this in Spark and think
that culture should change. Is it weird to say that a broader group of
gatekeepers can actually with more confidence and efficiency tackle the
triage issue? that pushing back on 'bad' contribution actually increases
the rate of 'good'?

FWIW I also find the project unpleasant to deal with day to day, mostly
because of the scale of the triage, and think we could use all the
qualified help we can get. I am looking to do less with the project over
time, which is no big deal in itself, but is a big deal if these several
factors are adding up to discourage fresh blood from joining the fray. Cody
makes me think there are, at least, 2 of us.

Concrete steps?

Go to spark-prs.com. Look at ""Users"". Look at your open PRs. Are any stale?
can you close them or advance them?

Look at the Stale PRs tab and sort by last updated. Do any look dead? can
you ask the author to update or close? does the parent JIRA look like it's
not otherwise relevant?

Go download JIRA Client at http://almworks.com/jiraclient/download.html Go
look at all open JIRAs sorted by last update. Are any pretty obviously
obsolete?

If you don't feel comfortable acting, feel free to at least propose a list
to dev@ for a look.
"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 10:00:40 -0500",Re: Spark Improvement Proposals,Sean Owen <sowen@cloudera.com>,"Sean, that was very eloquently put, and I 100% agree.  If I ever meet
you in person, I'll buy you multiple rounds of beverages of your
choice ;)
This is probably reiterating some of what you said in a less clear
manner, but I'll throw more of my 2 cents in.

- Design.
Yes, design by committee doesn't work.  The best designs are when a
person who understands the problem builds something that works for
them, shares with others, and most importantly iterates when it
doesn't work for others.  This iteration only works if you're willing
to change interfaces, but committer and user goals are not aligned
here.  Users want something that is clearly documented and helps them
get their job done.  Committers (not all) want to minimize interface
change, even at the expense of users being able to do their jobs.  In
this situation, it is critical that you understand early what users
need to be able to do.  This is what the improvement proposal process
should focus on: Goals, non-goals, possible solutions, rejected
solutions.  Not class-level design.  Most importantly, it needs a
clear, unambiguous outcome that is visible to the public.

- Trolling
It's not just trolling.  Event time and kafka are technically
important and should not be ignored.  I've been banging this drum for
years.  These concerns haven't been fully heard and understood by
committers.  This one example of why diversity of enfranchised users
is important and governance concerns shouldn't be ignored.

- Jira
Concretely, automate closing stale jiras after X amount of time.  It's
really surprising to me how much reluctance a community of programmers
have shown towards automating their own processes around stuff like
this (not to mention automatic code formatting of modified files).  I
understand the arguments against. but the current alternative doesn't
work.
Concretely, clearly reject and close jiras.  I have a backlog of 50+
kafka jiras, many of which are irrelevant at this point, but I do not
feel that I have the political power to close them.
Concretely, make it clear who is working on something.  This can be as
simple as just ""I'm working on this"", assign it to me, if I don't
follow up in X amount of time, close it or reassign.  That doesn't
mean there can't be competing work, but it does mean those people
should talk to each other.  Conversely, if committers currently don't
have time to work on something that is important, make that clear in
the ticket.



---------------------------------------------------------------------


"
Yanbo Liang <ybliang8@gmail.com>,"Fri, 7 Oct 2016 08:35:59 -0700",Re: Could we expose log likelihood of EM algorithm in MLLIB?,=?UTF-8?B?546L56OKKOWuieWFqOmDqCk=?= <wangleikidding@didichuxing.com>,"It's a good question and I had similar requirement in my work. I'm copying
the implementation from mllib to ml currently, and then exposing the
maximum log likelihood. I will send this PR soon.

Thanks.
Yanbo

¨) <wangleikidding@didichuxing.com>

/
"
Holden Karau <holden@pigscanfly.ca>,"Fri, 7 Oct 2016 09:16:38 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"First off, thanks Cody for taking the time to put together these proposals
- I think it has kicked off some wonderful discussion.

I think dismissing people's complaints with Spark as largely trolls does us
a disservice, itâ€™s important for us to recognize our own shortcomings -
otherwise we are blind to the weak spots where we need to improve and
instead focus on new features. Parts of the Python community seem to be
actively looking for alternatives, and Iâ€™d obviously like Spark continue to
be the place where we come together and collaborate from different
languages.

Iâ€™d be more than happy to do a review of the outstanding Python PRs (Iâ€™ve
been keeping on top of the new ones but largely havenâ€™t looked at the older
ones) and if there is a committer (maybe Davies or Sean?) who would be able
to help out with merging them once they are ready that would be awesome.
Iâ€™m at PyData DC this weekend but Iâ€™ll also start going through some of the
older Python JIRAs and seeing if they are still relevant, already fixed, or
something we are unlikely to be interested in bringing into Spark.

Iâ€™m giving a talk later on this month on how to get started contributing to
Apache Spark at OSCON London, and when Iâ€™ve given this talk before Iâ€™ve had
to include a fair number of warnings about the challenges that can face a
new contributor. Iâ€™d love to be able to drop those in future versions :)

P.S.

As one of the non-committers who has been working on Spark for several
years (see http://bit.ly/hkspmg ) I have strong feelings around the current
process being used for committers - but since Iâ€™m not on the PMC (catch-22
style) it's difficult to have any visibility into the process, so someone
who does will have to weigh in on that :)



et
d
ve
ee
e
.
,
ng
"",
to
k
h
is
st
t
u
e
an
'?
e
e
an


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
=?utf-8?B?546L56OKKOWuieWFqOmDqCk=?= <wangleikidding@didichuxing.com>,"Fri, 7 Oct 2016 16:21:59 +0000",Re: Could we expose log likelihood of EM algorithm in MLLIB?,Yanbo Liang <ybliang8@gmail.com>,"Thanks for replying.
When could you send out the PR?

å‘ä»¶äºº: Yanbo Liang <ybliang8@gmail.com<mailto:ybliang8@gmail.com>>
æ—¥æœŸ: 2016å¹´10æœˆ7æ—¥ æ˜ŸæœŸäº” ä¸‹åˆ1ngleikidding@didichuxing.com>>
æŠ„é€: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, ""user@spark.apache.org<mai likelihood of EM algorithm in MLLIB?

It's a good question and I had similar requirement in my work. I'm copying the implementation from mllib to ml currently, and then exposing the maximum log likelihood. I will send this PR soon.

Thanks.
Yanbo

On Fri, Oct 7, 2016 at 1:37 AM, çŽ‹ç£Š(å®‰å…¨éƒ¨) <wangleikidding@didichuxing.com<mailto:wangleikidding@didichuxing.com>> wrote:

Hi,

Do you guys sometimes need to get the log likelihood of EM algorithm in MLLIB?

I mean the value in this line https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/GaussianMixture.scala#L228

Now copying the code here:


        val sums = breezeData.treeAggregate(ExpectationSum.zero(k, d))(compute.value, _ += _)

        // Create new distributions based on the partial assignments
        // (often referred to as the ""M"" step in literature)
        val sumWeights = sums.weights.sum

        if (shouldDistributeGaussians) {
        val numPartitions = math.min(k, 1024)
        val tuples =
        Seq.tabulate(k)(i => (sums.means(i), sums.sigmas(i), sums.weights(i)))
        val (ws, gs) = sc.parallelize(tuples, numPartitions).map { case (mean, sigma, weight) =>
        updateWeightsAndGaussians(mean, sigma, weight, sumWeights)
        }.collect().unzip
        Array.copy(ws.toArray, 0, weights, 0, ws.length)
        Array.copy(gs.toArray, 0, gaussians, 0, gs.length)
        } else {
        var i = 0
        while (i < k) {
        val (weight, gaussian) =
        updateWeightsAndGaussians(sums.means(i), sums.sigmas(i), sums.weights(i), sumWeights)
        weights(i) = weight
        gaussians(i) = gaussian
        i = i + 1
        }
        }

        llhp = llh // current becomes previous
        llh = sums.logLikelihood // this is the freshly computed log-likelihood
        iter += 1
        compute.destroy(blocking = false)
In my application, I need to know log likelihood to compare effect for different number of clusters.
And then I use the cluster number with the maximum log likelihood.

Is it a good idea to expose this value?




"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 7 Oct 2016 10:03:54 -0700",Re: Spark Improvement Proposals,Holden Karau <holden@pigscanfly.ca>,"I think people misunderstood my comment about trolls a bit -- I'm not saying to just dismiss what people say, but to focus on what improves the project instead of being upset that people criticize stuff. This stuff happens all the time to any project in a ""hot"" area, as Sean said. I don't think there's anyone that wants to stop adding features to streaming for example, or stop listening to users, etc, or who thinks the project is already perfect (I certainly spend much of my time looking at how to improve it).

Just to comment on a few things:

proposals - I think it has kicked off some wonderful discussion.
does us a disservice, itâ€™s important for us to recognize our own shortcomings - otherwise we are blind to the weak spots where we need to improve and instead focus on new features. Parts of the Python community seem to be actively looking for alternatives, and Iâ€™d obviously like Spark continue to be the place where we come together and collaborate from different languages.
Python PRs (Iâ€™ve been keeping on top of the new ones but largely havenâ€™t looked at the older ones) and if there is a committer (maybe Davies or Sean?) who would be able to help out with merging them once they are ready that would be awesome. Iâ€™m at PyData DC this weekend but Iâ€™ll also start going through some of the older Python JIRAs and seeing if they are still relevant, already fixed, or something we are unlikely to be interested in bringing into Spark.

It would be great to also hear why people are looking for other stuff at a high level -- are there just many small issues in Python, or are there some bigger things missing? For example, one thing I'd like to see is easy installation of PySpark using pip install pyspark. Another idea would be making startup time and initialization easy enough that people use Spark regularly on a single machine, as a replacement for multiprocessing.


Love the idea of a more visible ""Spark Improvement Proposal"" process that solicits user input on new APIs. For what it's worth, I don't think committers are trying to minimize their own work -- every committer cares about making the software useful for users. However, it is always hard to get user input and so it helps to have this kind of process. I've certainly looked at the *IPs a lot in other software I use just to see the biggest things on the roadmap.

When you're talking about ""changing interfaces"", are you talking about public or internal APIs? I do think many people hate changing public APIs and I actually think that's for the best of the project. That's a technical debate, but basically, the worst thing when you're using a piece of software is that the developers constantly ask you to rewrite your app to update to a new version (and thus benefit from bug fixes, etc). Cue anyone who's used Protobuf, or Guava. The ""let's get everyone to change their code this release"" model works well within a single large company, but doesn't work well for a community, which is why nearly all *very* widely used programming interfaces (I'm talking things like Java standard library, Windows API, etc) almost *never* break backwards compatibility. All this is done within reason though, e.g. we do change things in major releases (2.x, 3.x, etc).


I agree about empowering people interested here to contribute, but I'm wondering, do you think there are technical things that people don't want to work on, or is it a matter of what there's been time to do? Everyone I know does want great Kafka support, event time, etc, it's just a question of working out the details and of course of getting the coding done. This is also an area where I'd love to see more contributions -- in the past, people have dome similar-scale contributions in other areas (e.g. better integration with Hive, on-the-wire encryption, etc).

FWIW, I think there are three things going on with streaming.

1) Structured Streaming, which is meant to provide a much higher-level new API. This was meant from the beginning to include event time, various complex form of windows, and great data source and sink support in a unified framework. It's also, IMHO, much simpler than most existing APIs for this stuff (i.e. look at the number of concepts you have to learn for those versus for this). However, this project is still very early on -- only the bare minimum API came out in 2.0. It's marked as alpha and it's precisely the type of system where I'd expect the API to improve in response to feedback. As with other APIs, such as Spark SQL's SchemaRDD and DataFrame, I think it's good to get it in front of *users* quickly and receive feedback -- even developers discussing among themselves can't anticipate all user needs.

2) Adding things in Spark Streaming. I haven't personally worked much on this lately, but it is a very reasonable thing that I'd love to see the project do to help current users. For example, consider adding an aggregate-by-event-time operator to Spark Streaming (it can be done using mapWithState), or a sessionization operator, etc.

3) Another thing that I think is possible is just lowering the latency of both Spark Streaming and Structured Streaming by 10x -- a few folks at Berkeley have been working on this (https://spark-summit.org/2016/events/low-latency-execution-for-apache-spark/). Happy to fork off a thread about how to do it. Their current system requires some new concepts in the Spark scheduler, but from measuring stuff it also seems that you can get somewhere with less intensive changes (most of the overhead is in RPCs, not in the scheduling logic or task execution).


Definitely agree with marking who's working on something early on, and timing it out if inactive. For closing JIRAs, I think the best way I've seen is for people to go through them once in a while. Automated closing is too impersonal IMO -- if I opened a JIRA on a project and nobody looked at it and that happened to me, I'd actively feel ignored. If you do that, you'll see people on stage saying ""I reported a bug for Spark and some bot just closed it after 3 months"", which is not ideal.

Matei


<matei.zaharia@gmail.com <mailto:matei.zaharia@gmail.com>>
source for
is still
happening.
that
well
made
we'd get
change a
bad
I've
an RFC,
essential, so
(See
involved
stage of
greener
hate.
suggest
generally does
And then
talks
attention given
some big
hear,
Spark
projects need
working
the whole
doesn't
soft-reject most
quality"",
efficiency to
ask who
overall
the only
much as
not
Always Be
and
this
to do
else has
last
suggest
time to
you
conviction.
culture
gatekeepers can
issue? that
'good'?
mostly
qualified
which
are
makes me
at your open PRs. Are any stale?
dead? can
like it's
http://almworks.com/jiraclient/download.html <http://almworks.com/jiraclient/download.html> Go
obviously
a list
<mailto:dev-unsubscribe@spark.apache.org>
<https://twitter.com/holdenkarau>
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 07 Oct 2016 17:27:01 +0000",Re: Spark Improvement Proposals,"Matei Zaharia <matei.zaharia@gmail.com>, Holden Karau <holden@pigscanfly.ca>","There are several important discussions happening simultaneously. Should we
perhaps split them up into separate threads? Otherwise itâ€™s really
difficult to follow.

It seems like the discussion about having a more formal â€œSpark Improvement
Proposalâ€ process should take priority here.

Other discussions that could be fleshed out in separate threads are:

   - Better managing â€œorganicâ€ community contributions (i.e. PRs, JIRA
   issues, etc).
   - Adjusting Sparkâ€™s governance model / adding more committers.
   - Discussing / addressing competition to Spark coming out of the Python
   community.

Nick


I think people misunderstood my comment about trolls a bit -- I'm not
t
s
comings -
ontinue to
Rs (Iâ€™ve
 the older
le
hrough some of the
or
a
me
ly
al
e
de
is
x,
w
nd
rk/).
ff
t
).
is
t
t
et
d
ve
ee
e
.
,
ng
"",
to
k
h
is
st
t
u
e
an
'?
e
e
an
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 10:38:38 -0700",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"I called Cody last night and talked about some of the topics in his email.
It became clear to me Cody genuinely cares about the project.

Some of the frustrations come from the success of the project itself
becoming very ""hot"", and it is difficult to get clarity from people who
don't dedicate all their time to Spark. In fact, it is in some ways similar
to scaling an engineering team in a successful startup: old processes that
worked well might not work so well when it gets to a certain size, cultures
can get diluted, building culture vs building process, etc.

I also really like to have a more visible process for larger changes,
especially major user facing API changes. Historically we upload design
docs for major changes, but it is not always consistent and difficult to
quality of the docs, due to the volunteering nature of the organization.

Some of the more concrete ideas we discussed focus on building a culture to
improve clarity:

Cody and I didn't discuss but an idea that just came to me is we should
create a design doc template for the project and ask everybody to follow.
The design doc template should also explicitly list goals and non-goals, to
make design doc more consistent.

- Process: Email dev@ to solicit feedback. We have some this with some
changes, but again very inconsistent. Just posting something on JIRA isn't
sufficient, because there are simply too many JIRAs and the signal get lost
in the noise. While this is generally impossible to enforce because we
can't force all volunteers to conform to a process (or they might not even
be aware of this),  those who are more familiar with the project can help
by emailing the dev@ when they see something that hasn't been.

- Culture: The design doc author(s) should be open to feedback. A design
doc should serve as the base for discussion and is by no means the final
design. Of course, this does not mean the author has to accept every
feedback. They should also be comfortable accepting / rejecting ideas on
technical grounds.

- Process / Culture: For major ongoing projects, it can be useful to have
some monthly Google hangouts that are open to the world. I am actually not
sure how well this will work, because of the volunteering nature and we
need to adjust for timezones for people across the globe, but it seems
worth trying.

- Culture: Contributors (including committers) should be more direct in
setting expectations, including whether they are working on a specific
issue, whether they will be working on a specific issue, and whether an
issue or pr or jira should be rejected. Most people I know in this
community are nice and don't enjoy telling other people no, but it is often
more annoying to a contributor to not know anything than getting a no.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 7 Oct 2016 10:50:53 -0700",Re: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"For the improvement proposals, I think one major point was to make them really visible to users who are not contributors, so we should do more type of JIRA called a SIP and have a link to a filter that shows all such JIRAs from http://spark.apache.org. I also like the idea of SIP and design doc templates (in fact many projects have them).

Matei

email. It became clear to me Cody genuinely cares about the project.
becoming very ""hot"", and it is difficult to get clarity from people who don't dedicate all their time to Spark. In fact, it is in some ways similar to scaling an engineering team in a successful startup: old processes that worked well might not work so well when it gets to a certain size, cultures can get diluted, building culture vs building process, etc.
especially major user facing API changes. Historically we upload design docs for major changes, but it is not always consistent and difficult to quality of the docs, due to the volunteering nature of the organization.
culture to improve clarity:
thing Cody and I didn't discuss but an idea that just came to me is we should create a design doc template for the project and ask everybody to follow. The design doc template should also explicitly list goals and non-goals, to make design doc more consistent.
changes, but again very inconsistent. Just posting something on JIRA isn't sufficient, because there are simply too many JIRAs and the signal get lost in the noise. While this is generally impossible to enforce because we can't force all volunteers to conform to a process (or they might not even be aware of this),  those who are more familiar with the project can help by emailing the dev@ when they see something that hasn't been.
design doc should serve as the base for discussion and is by no means the final design. Of course, this does not mean the author has to accept every feedback. They should also be comfortable accepting / rejecting ideas on technical grounds.
have some monthly Google hangouts that are open to the world. I am actually not sure how well this will work, because of the volunteering nature and we need to adjust for timezones for people across the globe, but it seems worth trying.
in setting expectations, including whether they are working on a specific issue, whether they will be working on a specific issue, and whether an issue or pr or jira should be rejected. Most people I know in this community are nice and don't enjoy telling other people no, but it is often more annoying to a contributor to not know anything than getting a no.
that solicits user input on new APIs. For what it's worth, I don't think committers are trying to minimize their own work -- every committer cares about making the software useful for users. However, it is always hard to get user input and so it helps to have this kind of process. I've certainly looked at the *IPs a lot in other software I use just to see the biggest things on the roadmap.
public or internal APIs? I do think many people hate changing public APIs and I actually think that's for the best of the project. That's a technical debate, but basically, the worst thing when you're using a piece of software is that the developers constantly ask you to rewrite your app to update to a new version (and thus benefit from bug fixes, etc). Cue anyone who's used Protobuf, or Guava. The ""let's get everyone to change their code this release"" model works well within a single large company, but doesn't work well for a community, which is why nearly all *very* widely used programming interfaces (I'm talking things like Java standard library, Windows API, etc) almost *never* break backwards compatibility. All this is done within reason though, e.g. we do change things in major releases (2.x, 3.x, etc).

"
Steve Loughran <stevel@hortonworks.com>,"Fri, 7 Oct 2016 17:56:20 +0000","Anyone interested in Spark & Cloud got time to look at the
 SPARK-7481 PR?",Apache Spark Dev <dev@spark.apache.org>,"
Some people may have noticed I've been working on adding packaging, docs & testing for getting Spark to work with S3, Azure and openstack into a Spark distribution,

https://github.com/apache/spark/pull/12004

It's been a WiP, but now I've got tests for all three cloud infrastructures, tests covering: basic IO, output committing, dataframe IO and streaming, the core test coverage is done; the packaging working.

Which means I'd really like some reviews by people who want to have spark work with S3, Azure or their local Swift endpoint to review that PR, ideally going through the documentation and validating that as well as the code.
It's Hadoop 2.7+ only, with a new profile, ""cloud"", to pull in the new module of the same name.

thanks

-Steve

PS: documentation (without templated code rendering):
https://github.com/steveloughran/spark/blob/features/SPARK-7481-cloud/docs/cloud-integration.md


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 8 Oct 2016 04:03:43 +0900",Re: Spark Improvement Proposals,Holden Karau <holden@pigscanfly.ca>,"I am glad that it was not only what I was thinking.
I also do agree with Holden, Sean and Cody. All I wanted to say were all
said.



2016-10-08 1:16 GMT+09:00 Holden Karau <holden@pigscanfly.ca>:

s
comings -
ontinue to
Rs (Iâ€™ve
 the older
le
hrough some of the
or
ibuting
fore Iâ€™ve
ce
rsions :)
the PMC
so
:
l
e
ad
r
e.
e
r,
o
e
st
e
ou
re
y
re
me
l
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 12:18:14 -0700",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"I like the lightweight proposal to add a SIP label.

During Spark 2.0 development, Tom (Graves) and I suggested using wiki to
track the list of major changes, but that never really materialized due to
the overhead. Adding a SIP label on major JIRAs and then link to them
prominently on the Spark website makes a lot of sense.



"
Deepak Sharma <deepakmca05@gmail.com>,"Sat, 8 Oct 2016 00:50:56 +0530",Reading back hdfs files saved as case class,dev@spark.apache.org,"Hi
I am saving RDD[Example] in hdfs from spark program , where Example is case
class.
Now when i am trying to read it back , it returns RDD[String] with the
content as below:
*Example(1,name,value)*

The workaround can be to write as a string in hdfs and read it back as
string and perform further processing.This way the case class name wouldn't
appear at all in the file being written in hdfs.
But i am keen to know if we can read the data directly in Spark if the
RDD[Case_Class] is written to hdfs?

-- 
Thanks
Deepak
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 12:23:13 -0700",Re: Reading back hdfs files saved as case class,Deepak Sharma <deepakmca05@gmail.com>,"You can use the Dataset API -- it should solve this issue for case classes
that are not very complex.


"
Deepak Sharma <deepakmca05@gmail.com>,"Sat, 8 Oct 2016 01:00:40 +0530",Re: Reading back hdfs files saved as case class,Reynold Xin <rxin@databricks.com>,"Thanks for the answer Reynold.
Yes I can use the dataset but it will solve the purpose I am supposed to
use it for.
I am trying to work on a solution where I need to save the case class along
with data in hdfs.
Further this data will move to different folders corresponding to different
case classes .
The spark programs reading these files are supposed to apply the case class
directly depending on the folder they are reading from.

Thanks
Deepak


"
Michael Armbrust <michael@databricks.com>,"Fri, 7 Oct 2016 12:41:19 -0700","Kafaka 0.8, 0.9 in Structured Streaming","""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","We recently merged support for Kafak 0.10.0 in Structured Streaming, but
I've been hearing a few people tell me that they are stuck on an older
version of Kafka and cannot upgrade.  I'm considering revisiting SPARK-17344
<https://issues.apache.org/jira/browse/SPARK-17344>, but it would be good
to have more information.

Could people please vote or comment on the above ticket if a lack of
support for older versions of kafka would block you from trying out
structured streaming?

Thanks!

Michael
"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 15:14:21 -0500",Re: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"+1 to adding an SIP label and linking it from the website.  I think it needs

- template that focuses it towards soliciting user goals / non goals
- clear resolution as to which strategy was chosen to pursue.  I'd
recommend a vote.

Matei asked me to clar"
Jeremy Smith <jeremy.smith@acorns.com>,"Fri, 7 Oct 2016 13:14:03 -0700","Re: Kafaka 0.8, 0.9 in Structured Streaming",Michael Armbrust <michael@databricks.com>,"+1

We're on CDH, and it will probably be a while before they support Kafka
0.10. At the same time, we don't use their Spark and we're looking forward
to upgrading to 2.0.x and using structured streaming.

I was just going to write our own Kafka Source im"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 13:15:33 -0700","Re: Kafaka 0.8, 0.9 in Structured Streaming",Jeremy Smith <jeremy.smith@acorns.com>,"Does Kafka 0.10 work on a Kafka 0.8/0.9 cluster?



"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 15:19:04 -0500","Re: Kafaka 0.8, 0.9 in Structured Streaming",Reynold Xin <rxin@databricks.com>,"0.10 consumers won't work on an earlier broker.

Earlier consumers will (should?) work on a 0.10 broker.

The main things earlier consumers lack from a user perspective is
support for SSL, and pre-fetching messages.  The implementation is
totally and completely different however, in ways that leak to the end
user.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 7 Oct 2016 13:25:35 -0700","Re: Kafaka 0.8, 0.9 in Structured Streaming",Cody Koeninger <cody@koeninger.org>,"

Can you elaborate? Especially in the context of the interface provided by
structured streaming.
"
Michael Armbrust <michael@databricks.com>,"Fri, 7 Oct 2016 13:28:17 -0700","Re: Kafaka 0.8, 0.9 in Structured Streaming",Cody Koeninger <cody@koeninger.org>,"
 This lines up with my testing.  Is there a page I'm missing that describes
this?  Like does a 0.9 client work with 0.8 broker?  Is it always old
clients can talk to new brokers but not vice versa?
"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 15:38:20 -0500","Re: Kafaka 0.8, 0.9 in Structured Streaming",Michael Armbrust <michael@databricks.com>,"Without a hell of a lot more work, Assign would be the only strategy usable.


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 15:48:11 -0500","Improving volunteer management / JIRAs (split from Spark Improvement
 Proposals thread)","""dev@spark.apache.org"" <dev@spark.apache.org>","Matei asked:




It's a matter of mismanagement and miscommunication.

The structured streaming kafka jira sat with multiple unanswered
requests for someone who was a committer to communicate whether they
were working on it and what the plan was.  I could have done that
implementation and had it in users' hands months ago.  I didn't
pre-emptively do it because I didn't want to then have to argue with
committers about why my code did or did not meet their uncommunicated
expectations.


I don't want to re-hash that particular circumstance, I just want to
make sure it never happens again.


Hopefully the SIP thread results in clearer expectations, but there
are still some ideas on the table regarding management of volunteer
contributions:


- Closing stale jiras.  I hear the bots are impersonal argument, but
the alternative of ""someone cleans it up"" is not sufficient right now
(with apologies to Sean and all the other janitors).

- Clear rejection of jiras.  This isn't mean, it's respectful.

- Clear ""I'm working on this"", with clear removal and reassignment if
they go radio silent.  This could be keyed to automated check for
staleness.

- Clear expectation that if someone is working on a jira, you can work
on your own alternative, but you need to communicate.


I'm sure I've missed some.

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 7 Oct 2016 13:56:07 -0700","Re: Kafaka 0.8, 0.9 in Structured Streaming",Cody Koeninger <cody@koeninger.org>,"

How would the current ""subscribe"" break?
"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 16:22:43 -0500","Improving governance / committers (split from Spark Improvement
 Proposals thread)","""dev@spark.apache.org"" <dev@spark.apache.org>","So concrete problems / potential solutions:


- Technical discussion needs to be public, or you don't hear use cases
and alternative viewpoints.
Yet email communication is low-bandwidth and hard to read people's
emotions, so committers who are colocated talk and decide things.
A possible alternative Reynold and I discussed is to have public video
streaming meetings on occasion.


- Each major area of the code needs at least one person who cares
about it that is empowered with a vote, otherwise decisions get made
that don't make technical sense.
I don't know if anyone with a vote is shepherding GraphX (or maybe
it's just dead), the Mesos relationship has always been weird, no one
with a vote really groks Kafka.
marmbrus and zsxwing are getting there quickly on the Kafka side, and
I appreciate it, but it's been bad for a while.
Because I don't have any political power, my response to seeing things
that I know are technically dangerous has been to yell really loud
until someone listens, which sucks for everyone involved.
I already apologized to Michael privately; Ryan, I'm sorry, it's not about you.
This seems pretty straightforward to fix, if politically awkward:
those people exist, just give them a vote.
Failing that, listen the first or second time they say something not
the third or fourth, and if it doesn't make sense, ask.


- More committers
Just looking at the ratio of committers to open tickets, or committers
to contributors, I don't think you have enough human power.
I realize this is a touchy issue.  I don't have dog in this fight,
because I'm not on either coast nor in a big company that views
committership as a political thing.  I just think you need more people
to do the work, and more diversity of viewpoint.
It's unfortunate that the Apache governance process involves giving
someone all the keys or none of the keys, but until someone really
starts screwing up, I think it's better to err on the side of
accepting hard-working people.

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 16:25:05 -0500","Re: Kafaka 0.8, 0.9 in Structured Streaming",Michael Armbrust <michael@databricks.com>,"The main thing is picking up new partitions.  You can't do that
without reimplementing portions of the consumer rebalance.  The
low-level consumer is really low level, and the old high-level
consumer is basically broken (it might have been fixed by the time
they abandoned it, I dunno)


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 16:29:58 -0500",Re: Spark Improvement Proposals,Stavros Kontopoulos <stavros.kontopoulos@lightbend.com>,"Yeah, in case it wasn't clear, I was talking about SIPs for major
user-facing or cross-cutting changes, not minor feature adds.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 07 Oct 2016 21:41:14 +0000","Re: Improving volunteer management / JIRAs (split from Spark
 Improvement Proposals thread)","Cody Koeninger <cody@koeninger.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I agree with Cody and others that we need some automation â€” or at least an
adjusted process â€” to help us manage organic contributions better.

The objections about automated closing being potentially abrasive are
understood, but I wouldnâ€™t accept that as a defeat for automation. Instead,
it seems like a constraint we should impose on any proposed solution: Make
sure it doesnâ€™t turn contributors off. Rolling as we have been wonâ€™t cut
it, and I donâ€™t think adding committers will ever be a sufficient solution
to this particular problem.

To me, it seems like we need a way to filter out viable contributions with
community support from other contributions when it comes to deciding that
automated action is appropriate. Our current tooling isnâ€™t perfect, but
perhaps we can leverage it to create such a filter.

For example, consider the following strawman proposal for how to cut down
on the number of pending but unviable proposals, and simultaneously help
contributors organize to promote viable proposals and get the attention of
committers:

   1. Have a bot scan for *stale* JIRA issues and PRsâ€”i.e. they havenâ€™t
   been updated in 20+ days (or D+ days, if you prefer).
   2. Depending on the level of community support, either close the item or
   ping specific people for action. Specifically:
   a. If the JIRA/PR has no input from a committer and the JIRA/PR has 5+
   votes (or V+ votes), ping committers for input. (For PRs, you could
   count comments from different people, or thumbs up on the initial PR post.)
   b. If the JIRA/PR has no input from a committer and the JIRA/PR has less
   than V votes, close it with a gentle message asking the contributor to
   solicit support from either the community or a committer, and try again
   later.
   c. If the JIRA/PR has input from a committer or committers, ping them
   for an update.

This is just a rough idea. The point is that when contributors have stale
proposals that they donâ€™t close, committers need to take action. A little
automation to selectively bring contributions to the attention of
committers can perhaps help them manage the backlog of stale contributions.
The â€œselectiveâ€ part is implemented in this strawman proposal by using JIRA
votes as a crude proxy for when the community is interested in something,
but it could be anything.

Also, this doesnâ€™t have to be used just to clear out stale proposalthe initial backlog is trimmed down, you could set D to 5 days and use this
as a regular way to bring contributions to the attention of committers.

I dunno if people think this is perhaps too complex, but at our scale I
feel we need some kind of loose but automated system for funneling
contributions through some kind of lifecycle. The status quo is just not
that good (e.g. 474 open PRs <https://github.com/apache/spark/pulls>
against Spark as of this moment).

Nick
â€‹


"
Cody Koeninger <cody@koeninger.org>,"Fri, 7 Oct 2016 16:59:33 -0500","Re: Improving volunteer management / JIRAs (split from Spark
 Improvement Proposals thread)",Nicholas Chammas <nicholas.chammas@gmail.com>,"I really like the idea of using jira votes (and/or watchers?) as a filter!

 least an
.
. Instead,
e
nâ€™t cut it,
ution to
h
t, but
 on
f
€™t been updated
g
tes
nts
 an
A little
ers
 by using JIRA
is
eel
ns
.
ant to

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 07 Oct 2016 22:06:46 +0000","Re: Improving volunteer management / JIRAs (split from Spark
 Improvement Proposals thread)",Cody Koeninger <cody@koeninger.org>,"Ah yes, on a given JIRA issue the number of watchers is often a better
indicator of community interest than votes.

But yeah, it could be any metric or formula we want, as long as it yielded
a ""reasonable"" bar to cross for unsolicited contributions to get committer
review--or at the very least a comment from them saying yes/no/later.


!
at least
er.
on.
wonâ€™t cut
olution to
at
ect, but
â€™t been
s
le
. A little
al by using JIRA
g,
osals.
'm
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Oct 2016 15:16:03 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Alright looks like there are quite a bit of support. We should wait to hear
from more people too.

To push this forward, Cody and I will be working together in the next
couple of weeks to come up with a concrete, detailed proposal on what this
entails, and then we can discuss this the specific proposal as well.



"
Arijit <ArijitT@live.com>,"Sat, 8 Oct 2016 00:06:13 +0000",Issue with Spark Streaming with checkpointing in Spark 2.0,"""user@spark.apache.org"" <dev@spark.apache.org>","In a Spark Streaming sample code I am trying to implicitly convert an RDD to DS and save to permanent storage. Below is the snippet of the code I am trying to run. The job runs fine first time when started with the checkpoint directory empty. However, if I kill and restart the job with the same checkpoint directory I get the following error resulting in job failure:


16/10/07 23:42:50 ERROR JobScheduler: Error running job streaming job 1475883550000 ms.0
java.lang.NullPointerException
 at org.apache.spark.sql.SQLImplicits.rddToDatasetHolder(SQLImplicits.scala:163)
 at com.microsoft.spark.streaming.examples.workloads.EventhubsToAzureBlobAsJSON$$anonfun$createStreamingContext$2.apply(EventhubsToAzureBlobAsJSON.scala:72)
 at com.microsoft.spark.streaming.examples.workloads.EventhubsToAzureBlobAsJSON$$anonfun$createStreamingContext$2.apply(EventhubsToAzureBlobAsJSON.scala:72)
 at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
 at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
 at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
 at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
 at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
 at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
 at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
 at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
 at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
 at scala.util.Try$.apply(Try.scala:192)
 at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
 at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:245)
 at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245)
 at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:245)
 at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
 at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:244)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)
16/10/07 23:42:50 INFO SparkContext: Starting job: print at EventhubsToAzureBlobAsJSON.scala:93


Does anyone have any sample recoverable Spark Streaming code using Spark Session constructs of 2.0?


object EventhubsToAzureBlobAsJSON {

  def createStreamingContext(inputOptions: ArgumentMap): StreamingContext = {

    .....

    val sparkSession : SparkSession = SparkSession.builder.config(sparkConfiguration).getOrCreate

    import sparkSession.implicits._

    val streamingContext = new StreamingContext(sparkSession.sparkContext,
      Seconds(inputOptions(Symbol(EventhubsArgumentKeys.BatchIntervalInSeconds)).asInstanceOf[Int]))
    streamingContext.checkpoint(inputOptions(Symbol(EventhubsArgumentKeys.CheckpointDirectory)).asInstanceOf[String])

    val eventHubsStream = EventHubsUtils.createUnionStream(streamingContext, eventHubsParameters)

    val eventHubsWindowedStream = eventHubsStream
      .window(Seconds(inputOptions(Symbol(EventhubsArgumentKeys.BatchIntervalInSeconds)).asInstanceOf[Int]))

    /**
      * This fails on restart
      */

    eventHubsWindowedStream.map(x => EventContent(new String(x)))
      .foreachRDD(rdd => rdd.toDS.toJSON.write.mode(SaveMode.Overwrite)
        .save(inputOptions(Symbol(EventhubsArgumentKeys.EventStoreFolder))
          .asInstanceOf[String]))

    /**
      * This runs fine on restart
      */

    /*
    eventHubsWindowedStream.map(x => EventContent(new String(x)))
      .foreachRDD(rdd => rdd.saveAsTextFile(inputOptions(Symbol(EventhubsArgumentKeys.EventStoreFolder))
          .asInstanceOf[String], classOf[GzipCodec]))
    */

    .....

  }

  def main(inputArguments: Array[String]): Unit = {

    val inputOptions = EventhubsArgumentParser.parseArguments(Map(), inputArguments.toList)

    EventhubsArgumentParser.verifyEventhubsToAzureBlobAsJSONArguments(inputOptions)

    //Create or recreate streaming context

    val streamingContext = StreamingContext
      .getOrCreate(inputOptions(Symbol(EventhubsArgumentKeys.CheckpointDirectory)).asInstanceOf[String],
        () => createStreamingContext(inputOptions))

    streamingContext.start()

    if(inputOptions.contains(Symbol(EventhubsArgumentKeys.TimeoutInMinutes))) {

      streamingContext.awaitTerminationOrTimeout(inputOptions(Symbol(EventhubsArgumentKeys.TimeoutInMinutes))
        .asInstanceOf[Long] * 60 * 1000)
    }
    else {

      streamingContext.awaitTermination()
    }
  }
}


Thanks, Arijit
"
=?utf-8?B?546L56OKKOWuieWFqOmDqCk=?= <wangleikidding@didichuxing.com>,"Sat, 8 Oct 2016 03:43:52 +0000",Re: Could we expose log likelihood of EM algorithm in MLLIB?,Yanbo Liang <ybliang8@gmail.com>,"https://issues.apache.org/jira/browse/SPARK-17825

Actually I had created a JIRA. Could you let me your progress to avoid duplicated work.

Thanks!

å‘ä»¶äºº: didi <wangleikidding@didichuxing.com<mailto:wangleikidding@didichuxing.com>>
æ—¥æœŸ: 2016å¹´10æœˆ8æ—¥ æ˜ŸæœŸå…­ ä¸Šåˆ12:21
è‡³: Yanbo Liang <ybliang8@gmail.com<mailto:ybliang8@gmail.com>>
æŠ„é€: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
ä¸»é¢˜: Re: Could we expose log likelihood of EM algorithm in MLLIB?

Thanks for replying.
When could you send out the PR?

å‘ä»¶äºº: Yanbo Liang <ybliang8@gmail.com<mailto:ybliang8@gmail.com>>
æ—¥æœŸ: 2016å¹´10æœˆ7æ—¥ æ˜ŸæœŸäº” ä¸‹åˆ11:35
è‡³: didi <wangleikidding@didichuxing.com<mailto:wangleikidding@didichuxing.com>>
æŠ„é€: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
ä¸»é¢˜: Re: Could we expose log likelihood of EM algorithm in MLLIB?

It's a good question and I had similar requirement in my work. I'm copying the implementation from mllib to ml currently, and then exposing the maximum log likelihood. I will send this PR soon.

Thanks.
Yanbo

On Fri, Oct 7, 2016 at 1:37 AM, çŽ‹ç£Š(å®‰å…¨éƒ¨) <wangleikidm>> wrote:

Hi,

Do you guys sometimes need to get the log likelihood of EM algorithm in MLLIB?

I mean the value in this line https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/GaussianMixture.scala#L228

Now copying the code here:


        val sums = breezeData.treeAggregate(ExpectationSum.zero(k, d))(compute.value, _ += _)

        // Create new distributions based on the partial assignments
        // (often referred to as the ""M"" step in literature)
        val sumWeights = sums.weights.sum

        if (shouldDistributeGaussians) {
        val numPartitions = math.min(k, 1024)
        val tuples =
        Seq.tabulate(k)(i => (sums.means(i), sums.sigmas(i), sums.weights(i)))
        val (ws, gs) = sc.parallelize(tuples, numPartitions).map { case (mean, sigma, weight) =>
        updateWeightsAndGaussians(mean, sigma, weight, sumWeights)
        }.collect().unzip
        Array.copy(ws.toArray, 0, weights, 0, ws.length)
        Array.copy(gs.toArray, 0, gaussians, 0, gs.length)
        } else {
        var i = 0
        while (i < k) {
        val (weight, gaussian) =
        updateWeightsAndGaussians(sums.means(i), sums.sigmas(i), sums.weights(i), sumWeights)
        weights(i) = weight
        gaussians(i) = gaussian
        i = i + 1
        }
        }

        llhp = llh // current becomes previous
        llh = sums.logLikelihood // this is the freshly computed log-likelihood
        iter += 1
        compute.destroy(blocking = false)
In my application, I need to know log likelihood to compare effect for different number of clusters.
And then I use the cluster number with the maximum log likelihood.

Is it a good idea to expose this value?




"
Yanbo Liang <ybliang8@gmail.com>,"Fri, 7 Oct 2016 21:22:15 -0700",Re: Could we expose log likelihood of EM algorithm in MLLIB?,=?UTF-8?B?546L56OKKOWuieWFqOmDqCk=?= <wangleikidding@didichuxing.com>,"Let's move the discussion to JIRA. Thanks!

¨) <wangleikidding@didichuxing.com>

å…­ ä¸Šåˆ12:21
spark.apache.org""
 MLLIB?
äº” ä¸‹åˆ11:35
spark.apache.org""
 MLLIB?
g
ƒ¨) <wangleikidding@didichuxing.com>
d
g
"
Sean Owen <sowen@cloudera.com>,"Sat, 08 Oct 2016 09:09:42 +0000",PSA: JIRA resolutions and meanings,dev <dev@spark.apache.org>,"That flood of emails means several people (Xiao, Holden mostly AFAICT) have
been updating the status of old JIRAs. Thank you, I think that really does
help.

I have a suggested set of conventions I've been using, just to bring some
order to the resolutions. It helps because JIRA functions as a huge archive
of decisions and the more accurately we can record that the better. What do
people think of this?

- Resolve as Fixed if there's a change you can point to that resolved the
issue
- If the issue is a proper subset of another issue, mark it a Duplicate of
that issue (rather than the other way around)
- If it's probably resolved, but not obvious what fixed it or when, then
Cannot Reproduce or Not a Problem
- Obsolete issue? Not a Problem
- If it's a coherent issue but does not seem like there is support or
interest in acting on it, then Won't Fix
- If the issue doesn't make sense (non-Spark issue, etc) then Invalid
- I tend to mark Umbrellas as ""Done"" when done if they're just containers
- Try to set Fix version
- Try to set Assignee to the person who most contributed to the resolution.
Usually the person who opened the PR. Strong preference for ties going to
the more 'junior' contributor

The only ones I think are sort of important are getting the Duplicate
pointers right, and possibly making sure that Fixed issues have a clear
path to finding what change fixed it and when. The rest doesn't matter much.
"
Cody Koeninger <cody@koeninger.org>,"Sat, 8 Oct 2016 08:37:18 -0500",Re: PSA: JIRA resolutions and meanings,Sean Owen <sowen@cloudera.com>,"That makes sense, thanks.

Jiras.  Can I go clean up the backlog of Kafka Jiras that weren't created
by me?

If there's an informal policy here, can we update the wiki to reflect it?
Maybe it's there already, but I didn't see it last time I looked.


That flood of emails means several people (Xiao, Holden mostly AFAICT) have
been updating the status of old JIRAs. Thank you, I think that really does
help.

I have a suggested set of conventions I've been using, just to bring some
order to the resolutions. It helps because JIRA functions as a huge archive
of decisions and the more accurately we can record that the better. What do
people think of this?

- Resolve as Fixed if there's a change you can point to that resolved the
issue
- If the issue is a proper subset of another issue, mark it a Duplicate of
that issue (rather than the other way around)
- If it's probably resolved, but not obvious what fixed it or when, then
Cannot Reproduce or Not a Problem
- Obsolete issue? Not a Problem
- If it's a coherent issue but does not seem like there is support or
interest in acting on it, then Won't Fix
- If the issue doesn't make sense (non-Spark issue, etc) then Invalid
- I tend to mark Umbrellas as ""Done"" when done if they're just containers
- Try to set Fix version
- Try to set Assignee to the person who most contributed to the resolution.
Usually the person who opened the PR. Strong preference for ties going to
the more 'junior' contributor

The only ones I think are sort of important are getting the Duplicate
pointers right, and possibly making sure that Fixed issues have a clear
path to finding what change fixed it and when. The rest doesn't matter much.
"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 8 Oct 2016 22:53:18 +0900",Re: PSA: JIRA resolutions and meanings,Cody Koeninger <cody@koeninger.org>,"I am uncertain too. It'd be great if these are documented too.

FWIW, in my case, I privately asked and told Sean first that I am going to
look though the JIRAs
and resolve some via the suggested conventions from Sean.
(Definitely all blames should be on me if I have done something terribly
wrong).



2016-10-08 22:37 GMT+09:00 Cody Koeninger <cody@koeninger.org>:

"
Ted Yu <yuzhihong@gmail.com>,"Sat, 8 Oct 2016 07:47:35 -0700",Re: PSA: JIRA resolutions and meanings,Hyukjin Kwon <gurwls223@gmail.com>,"I think only committers should resolve JIRAs which were not created by himself / herself. 

 look though the JIRAs
rong). 
ras.  Can I go clean up the backlog of Kafka Jiras that weren't created by me?
  Maybe it's there already, but I didn't see it last time I looked.
ve been updating the status of old JIRAs. Thank you, I think that really does help. 
 order to the resolutions. It helps because JIRA functions as a huge archive of decisions and the more accurately we can record that the better. What do people think of this?
 issue
f that issue (rather than the other way around)
annot Reproduce or Not a Problem
erest in acting on it, then Won't Fix

n. Usually the person who opened the PR. Strong preference for ties going to the more 'junior' contributor
nters right, and possibly making sure that Fixed issues have a clear path to finding what change fixed it and when. The rest doesn't matter much.
"
Holden Karau <holden@pigscanfly.ca>,"Sat, 8 Oct 2016 07:54:30 -0700",Re: PSA: JIRA resolutions and meanings,Ted Yu <yuzhihong@gmail.com>,"We could certainly do that system - but given the current somewhat small
set of active committers its clearly not scaling very well. There are many
developers  in Spark like Hyukjin, Cody, and myself who care about specific
areas and can verify if an issue is still present in mainline.

That being said if the general view is that only committers should resolve
JIRAs I'm happy to back off and leave that to the current committers (or we
could try ping them to close issues which I think are resolved instead of
closing them myself but given how many pings I sometimes have to make to
get an issue looked at I'm hesitant to suggest this system).

I'll hold off on my JIRA review for a bit while we get this sorted :)




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sat, 8 Oct 2016 08:21:39 -0700 (MST)","RE: Improving volunteer management / JIRAs (split from Spark
 Improvement Proposals thread)",dev@spark.apache.org,"I donâ€™t really have much experience with large open source projects but I have some experience with having lots of issues with no one handling them. Automation proved a good solution in my experience, but one thing that I found which was really important is giving people a chance to say â€œdonâ€™t close this pleaseâ€.
Basically, because closing you can send an email to the reporter (and probably people who are watching the issue) and tell them this is going to be closed. Allow them an option to ping back saying â€œdonâ€™t close this pleaseâ€ which would ping committers for input (as if there were 5+ votes as described by Nick).
The main reason for this is that many times people fine solutions and the issue does become stale but at other times, the issue is still important, it is just that no one noticed it because of the noise of other issues.
Thanks,
                Assaf.



From: Nicholas Chammas [via Apache Spark Developers List] [mailto:ml-node+s1001551n19310h99@n3.nabble.com]
Sent: Saturday, October 08, 2016 12:42 AM
To: Mendelson, Assaf
Subject: Re: Improving volunteer management / JIRAs (split from Spark Improvement Proposals thread)


I agree with Cody and others that we need some automation â€” or at least an adjusted process â€” to help us manage organic contributions better.

The objections about automated closing being potentially abrasive are understood, but I wouldnâ€™t accept that as a defeat for automation. Instead, it seems like a constraint we should impose on any proposed solution: Make sure it doesnâ€™t turn contributors off. Rolling as we have been wonâ€™t cut it, and I donâ€™t think adding committers will ever be a sufficient solution to this particular problem.

To me, it seems like we need a way to filter out viable contributions with community support from other contributions when it comes to deciding that automated action is appropriate. Our current tooling isnâ€™t perfect, but perhaps we can leverage it to create such a filter.

For example, consider the following strawman proposal for how to cut down on the number of pending but unviable proposals, and simultaneously help contributors organize to promote viable proposals and get the attention of committers:
1.      Have a bot scan for stale JIRA issues and PRsâ€”i.e. they havenâ€™t been updated in 20+ days (or D+ days, if you prefer).
2.      Depending on the level of community support, either close the item or ping specific people for action. Specifically:
a. If the JIRA/PR has no input from a committer and the JIRA/PR has 5+ votes (or V+ votes), ping committers for input. (For PRs, you could count comments from different people, or thumbs up on the initial PR post.)
b. If the JIRA/PR has no input from a committer and the JIRA/PR has less than V votes, close it with a gentle message asking the contributor to solicit support from either the community or a committer, and try again later.
c. If the JIRA/PR has input from a committer or committers, ping them for an update.

This is just a rough idea. The point is that when contributors have stale proposals that they donâ€™t close, committers need to take action. A little automation to selectively bring contributions to the attention of committers can perhaps help them manage the backlog of stale contributions. The â€œselectiveâ€ part is implemented in this strawman proposal by using JIRA votes as a crude proxy for when the community is interested in something, but it could be anything.

Also, this doesnâ€™t have to be used just to clear out stale proposaluse this as a regular way to bring contributions to the attention of committers.

I dunno if people think this is perhaps too complex, but at our scale I feel we need some kind of loose but automated system for funneling contributions through some kind of lifecycle. The status quo is just not that good (e.g. 474 open PRs<https://github.com/apache/spark/pulls> against Spark as of this moment).

Nick
â€‹

Matei asked:


ndering, do you think there are technical things that people don't want to work on, or is it a matter of what there's been time to do?


It's a matter of mismanagement and miscommunication.

The structured streaming kafka jira sat with multiple unanswered
requests for someone who was a committer to communicate whether they
were working on it and what the plan was.  I could have done that
implementation and had it in users' hands months ago.  I didn't
pre-emptively do it because I didn't want to then have to argue with
committers about why my code did or did not meet their uncommunicated
expectations.


I don't want to re-hash that particular circumstance, I just want to
make sure it never happens again.


Hopefully the SIP thread results in clearer expectations, but there
are still some ideas on the table regarding management of volunteer
contributions:


- Closing stale jiras.  I hear the bots are impersonal argument, but
the alternative of ""someone cleans it up"" is not sufficient right now
(with apologies to Sean and all the other janitors).

- Clear rejection of jiras.  This isn't mean, it's respectful.

- Clear ""I'm working on this"", with clear removal and reassignment if
they go radio silent.  This could be keyed to automated check for
staleness.

- Clear expectation that if someone is working on a jira, you can work
on your own alternative, but you need to communicate.


I'm sure I've missed some.

---------------------------------------------------------------------
=19310&i=1>

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Improving-volunteer-management-JIRAs-split-from-Spark-Improvement-Proposals-thread-tp19305p19310.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Improving-volunteer-management-JIRAs-split-from-Spark-Improvement-Proposals-thread-tp19305p19322.html
om."
Sean Owen <sowen@cloudera.com>,"Sat, 08 Oct 2016 15:27:55 +0000",Re: PSA: JIRA resolutions and meanings,"Ted Yu <yuzhihong@gmail.com>, dev <dev@spark.apache.org>","I know what you mean Ted, and I think actually what's happening here is
fine, but I'll explain my theory.

There are a range of actions in the project where someone makes a decision,
from answering an email to creating a release. We already accept that only
some of these require a formal status; anyone can answer emails, but only
the PMC can bless a release, for example.

The reason commits and releases require a certain status is not _entirely_
to block most people from participating in these activities. It's in part
because things the ASF's liability protections for releases depend on the
existence of well-defined governance models, that wouldn't quite be
compatible with anyone adding software at will.

Issue management isn't in this category, and, of course, we let anyone make
JIRAs and PRs. This causes problems occasionally but is on the whole
powerfully good. So, it seems reasonable to let people close JIRAs if, in
good faith, they have clear reason to do so. These things are reversible,
too. I also think there's a cost to not getting this triage work done, just
as there would be a cost to blocking people from creating issues.

I've reviewed the cleanup in the past 24 hours and agree with virtually
every action, so I have confidence that in practice this is a big positive.

That said, I have suggested in the past that perhaps only committers should
set ""Blocker"" and ""Target Version"", because this communicates something
specific about what will be committed and in what release, and acting on
those requires commit access. Although by the theory above we should let
anyone set these -- and actually, we do -- I have found it usually set
incorrectly, and so, argue that these fields should be treated differently
as a matter of convention.

Sean


"
Cody Koeninger <cody@koeninger.org>,"Sat, 8 Oct 2016 11:01:19 -0500","Re: Improving volunteer management / JIRAs (split from Spark
 Improvement Proposals thread)","""assaf.mendelson"" <assaf.mendelson@rsa.com>","Yeah, I've interacted with other projects that used that system and it was
pleasant.

1. ""this is getting closed cause its stale, let us know if thats a problem""
2. ""actually that matters to us""
3. ""ok well leave it open""

I'd be fine with totally automating step 1 as long as a human was involved
at step 2 and 3



ts but I
.
œdonâ€™t
o
t close this
+ votes as
 least an
.
. Instead,
e
nâ€™t cut
 solution
h
t, but
f
 havenâ€™t
A little
s.
osal by using JIRA
rs.
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
nteer-management-JIRAs-split-from-Spark-Improvement-Proposals-thread-tp19305p19322.html>
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 8 Oct 2016 09:05:16 -0700",Re: PSA: JIRA resolutions and meanings,Sean Owen <sowen@cloudera.com>,"Makes sense. 

I trust Hyukjin, Holden and Cody's judgement in respective areas. 

I just wish to see more participation from the committers. 

Thanks 


---------------------------------------------------------------------


"
Felix Cheung <felixcheung_m@hotmail.com>,"Sat, 8 Oct 2016 17:56:04 +0000",Re: PSA: JIRA resolutions and meanings,Sean Owen <sowen@cloudera.com>,"+1 on this proposal and everyone can contribute to updates and discussions on JIRAs

Will be great if this could be put on the Spark wiki.






Makes sense.

I trust Hyukjin, Holden and Cody's judgement in respective areas.

I just wish to see more parti"
Xiao Li <gatorsmile@gmail.com>,"Sat, 8 Oct 2016 11:06:56 -0700",Re: PSA: JIRA resolutions and meanings,Felix Cheung <felixcheung_m@hotmail.com>,"Thank you, Sean Owen! Your guideline looks pretty good. I will try to
follow it when closing the JIRAs. Please correct me if you found
anything is not appropriate.

Actually, the unresolved SQL JIRAs has almost 1000. Compared with the
other components, I think SQL might be the biggest one. I will try to
go over all of them at my best. Then, maybe the other Committers and
contributors can go over the still-open JIRAs again and see whether
some of them should be closed or resolved in the next releases.

In the future, personally, I will try to do it whenever I am free.
Hopefully, it can help the community.

BTW, since I started joining the Spark community, I already realized
Sean Owen, Reynold Xin, Yin Huai are doing it on a regular basis. I
believe most appreciate their contribution to the community. Here, I
just want to say thank you to them: THANK YOU!

Cheers,

Xiao

2016-10-08 10:56 GMT-07:00 Felix Cheung <felixcheung_m@hotmail.com>:

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 8 Oct 2016 11:41:38 -0700",Re: PSA: JIRA resolutions and meanings,Sean Owen <sowen@cloudera.com>,"

+1

This is consistent with my understanding. It would be good to document
these on JIRA. And I second ""The only ones I think are sort of important
are getting the Duplicate pointers right, and possibly making sure that
Fixed issues have a clear path to finding what change fixed it and when.
The rest doesn't matter much.""

I also think it is a good idea to give people rights to close tickets to
help with JIRA maintenance. We can always revoke that if we see a malicious
actor (or somebody with extremely bad judgement), but we are pretty far
away from that right now.
"
Cody Koeninger <cody@koeninger.org>,"Sat, 8 Oct 2016 14:06:13 -0500",Re: PSA: JIRA resolutions and meanings,Reynold Xin <rxin@databricks.com>,"So to be clear, can I go clean up the Kafka cruft?


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 8 Oct 2016 12:19:11 -0700",Re: PSA: JIRA resolutions and meanings,Cody Koeninger <cody@koeninger.org>,"I think so (at least I think it is socially acceptable). Of course, use
good judgement here :)




"
Cody Koeninger <cody@koeninger.org>,"Sat, 8 Oct 2016 14:22:08 -0500",Re: PSA: JIRA resolutions and meanings,Reynold Xin <rxin@databricks.com>,"Cool, I'll start going through stuff as I have time.  Already closed
one, if anyone sees a problem let me know.

Still think it would be nice to have some way to make it obvious to
the people who have the will and knowledge to do it that it's ok for
them to do it :)


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 8 Oct 2016 13:49:44 -0700",Re: Improving volunteer management / JIRAs (split from Spark Improvement Proposals thread),Cody Koeninger <cody@koeninger.org>,"I like this idea of asking them. BTW, one other thing we can do *provided the JIRAs are eventually under control* is to create a filter for old JIRAs that have not received a response in X amount of time and have the system automatically email the dev list with this report every month. Then everyone can see the list of items and maybe be reminded to take care to clean it up. This only works if the list is manageable and you actually want to read all of it.

Matei

was pleasant.
problem""
involved at step 2 and 3
projects but I have some experience with having lots of issues with no one handling them. Automation proved a good solution in my experience, but one thing that I found which was really important is giving people a chance to say â€œdonâ€™t close this pleaseâ€.
probably people who are watching the issue) and tell them this is going to be closed. Allow them an option to ping back saying â€œdonâ€™t close this pleaseâ€ which would ping committers for input (as if there were 5+ votes as described by Nick).
the issue does become stale but at other times, the issue is still important, it is just that no one noticed it because of the noise of other issues.
[mailto:ml-node+ <javascript:_e(%7B%7D,'cvml','ml-node%2B');>[hidden email] <http://user/SendEmail.jtp?type=node&node=19322&i=0>] 
Improvement Proposals thread)
at least an adjusted process â€” to help us manage organic contributions better.
understood, but I wouldnâ€™t accept that as a defeat for automation. Instead, it seems like a constraint we should impose on any proposed solution: Make sure it doesnâ€™t turn contributors off. Rolling as we have been wonâ€™t cut it, and I donâ€™t think adding committers will ever be a sufficient solution to this particular problem.
with community support from other contributions when it comes to deciding that automated action is appropriate. Our current tooling isnâ€™t perfect, but perhaps we can leverage it to create such a filter.
down on the number of pending but unviable proposals, and simultaneously help contributors organize to promote viable proposals and get the attention of committers:
they havenâ€™t been updated in 20+ days (or D+ days, if you prefer).
item or ping specific people for action. Specifically:
votes (or V+ votes), ping committers for input. (For PRs, you could count comments from different people, or thumbs up on the initial PR post.)
less than V votes, close it with a gentle message asking the contributor to solicit support from either the community or a committer, and try again later.
for an update.
stale proposals that they donâ€™t close, committers need to take action. A little automation to selectively bring contributions to the attention of committers can perhaps help them manage the backlog of stale contributions. The â€œselectiveâ€ part is implemented in this strawman proposal by using JIRA votes as a crude proxy for when the community is interested in something, but it could be anything.
5 days and use this as a regular way to bring contributions to the attention of committers.
I feel we need some kind of loose but automated system for funneling contributions through some kind of lifecycle. The status quo is just not that good (e.g. 474 open PRs <https://github.com/apache/spark/pulls> against Spark as of this moment).
I'm wondering, do you think there are technical things that people don't want to work on, or is it a matter of what there's been time to do?
<http://user/SendEmail.jtp?type=node&node=19310&i=1>
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Improving-volunteer-management-JIRAs-split-from-Spark-Improvement-Proposals-thread-tp19305p19310.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Improving-volunteer-management-JIRAs-split-from-Spark-Improvement-Proposals-thread-tp19305p19310.html>
email] <http://user/SendEmail.jtp?type=node&node=19322&i=1> 
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
JIRAs (split from Spark Improvement Proposals thread) <http://apache-spark-developers-list.1001551.n3.nabble.com/Improving-volunteer-management-JIRAs-split-from-Spark-Improvement-Proposals-thread-tp19305p19322.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 8 Oct 2016 14:11:07 -0700",Re: Improving governance / committers (split from Spark Improvement Proposals thread),Cody Koeninger <cody@koeninger.org>,"This makes a lot of sense; just to comment on a few things:


This is something the PMC is actively discussing. Historically, we've added committers when people contributed a new module or feature, basically to the point where other developers are asking them to review changes in that area (https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-BecomingaCommitter). For example, we added the original authors of GraphX when we merged in GraphX, the authors of new ML algorithms, etc. However, there's a good argument that some areas are simply not covered well now and we should add people there. Also, as the project has grown, there are also more people who focus on smaller fixes and are nonetheless contributing a lot.

about you.

Just as a note here -- it's true that some areas are not super well covered, but I also hope to avoid a situation where people have to yell to be listened to. I can't say anything about *all* technical discussions we've ever had, but historically, people have been able to comment on the design of many things without yelling. This is actually important because a culture of having to yell can drive away contributors. So it's awesome that you yelled about the Kafka source stuff, but at the same time, hopefully we make these types of things work without yelling. This would be a problem even if there were committers with more expertise in each area -- what if someone disagrees with the committers?

Matei


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 8 Oct 2016 14:22:22 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Sounds good. Just to comment on the compatibility part:


Experimental APIs and alpha components are indeed supposed to be changeable (https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy). Maybe people are being too conservative in some cases, but I do want to note that regardless of what precise policy we try to write down, this type of issue will ultimately be a judgment call. Is it worth making a small cosmetic change in an API that's marked experimental, but has been used widely for a year? Perhaps not. Is it worth making it in something one month old, or even in an older API as we move to 2.0? Maybe yes. I think we should just discuss each one (start an email thread if resolving it on JIRA is too complex) and perhaps be more religious about making things non-experimental when we think they're done.

Matei


to
due to
<matei.zaharia@gmail.com>
them
more than
type of
JIRAs from
email.
who
similar
processes that
cultures
changes,
design docs
quality
culture
thing
should
follow.
non-goals, to
some
isn't
get lost
we can't
be
help by
design
final
ideas on
have
actually not
we need
worth
in
specific
an
community
more
<matei.zaharia@gmail.com>
process that
cares
hard to
certainly
biggest
about
public APIs
technical
software
update to a
who's used
this
doesn't work
programming
API, etc)
within reason


---------------------------------------------------------------------


"
vaquar khan <vaquar.khan@gmail.com>,"Sat, 8 Oct 2016 17:21:12 -0500",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"+1 for SIP lebles,waiting for Reynolds detailed proposal .

Regards,
Vaquar khan


"
vaquar khan <vaquar.khan@gmail.com>,"Sat, 8 Oct 2016 18:41:00 -0500","Re: Improving volunteer management / JIRAs (split from Spark
 Improvement Proposals thread)",Matei Zaharia <matei.zaharia@gmail.com>,"Matei ,

I like your idea automatically email However it wont solve then problem.

Last few years i saw many enthusiastic sent mail and want to be Apache
Spark contributor ,Our response most welcome here is Jira , go and start
work on issues.
After few days /months struggle they have lost interest because not able to
understand where to start.

1) We should assign mentor to enthusiastic to understand issues and help to
start as contributor.
2) For high priority old Jira we should create separate group , who give
complete analysis in Jira so other can go and fix issue.



Reagrds,
Vaquar khan


As
s
m""
d
cts but I
m.
œdonâ€™t
to
™t close this
5+ votes as
e
,
t least
tter.
n. Instead,
ke
onâ€™t cut
t solution
g
perfect,
n
of
y
t.)
r
e
 A little
ns.
posal by using JIRA
,
sals.
t
n
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
unteer-management-JIRAs-split-from-Spark-Improvement-Proposals-thread-tp19305p19322.html>


-- 
Regards,
Vaquar Khan
+1 -224-436-0783

IT Architect / Lead Consultant
Greater Chicago
"
Cody Koeninger <cody@koeninger.org>,"Sat, 8 Oct 2016 19:03:58 -0500","Re: Improving governance / committers (split from Spark Improvement
 Proposals thread)",Matei Zaharia <matei.zaharia@gmail.com>,"It's not about technical design disagreement as to matters of taste,
it's about familiarity with the domain.  To make an analogy, it's as
if a committer in MLlib was firmly intent on, I dunno, treating a
collection of categorical variables as if it were an ordered range of
continuous variables.  It's just wrong.  That kind of thing, to a
greater or lesser degree, has been going on related to the Kafka
modules, for years.

te:
ed committers when people contributed a new module or feature, basically to the point where other developers are asking them to review changes in that area (https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-BecomingaCommitter). For example, we added the original authors of GraphX when we merged in GraphX, the authors of new ML algorithms, etc. However, there's a good argument that some areas are simply not covered well now and we should add people there. Also, as the project has grown, there are also more people who focus on smaller fixes and are nonetheless contributing a lot.
ut you.
ed, but I also hope to avoid a situation where people have to yell to be listened to. I can't say anything about *all* technical discussions we've ever had, but historically, people have been able to comment on the design of many things without yelling. This is actually important because a culture of having to yell can drive away contributors. So it's awesome that you yelled about the Kafka source stuff, but at the same time, hopefully we make these types of things work without yelling. This would be a problem even if there were committers with more expertise in each area -- what if someone disagrees with the committers?

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 09 Oct 2016 08:39:48 +0000",Re: PSA: JIRA resolutions and meanings,dev <dev@spark.apache.org>,"I added a variant on this text to
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ContributingtoJIRAMaintenance


"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 07:33:09 -0500",Re: PSA: JIRA resolutions and meanings,Sean Owen <sowen@cloudera.com>,"That's awesome Sean, very clear.



I added a variant on this text to https://cwiki.apache.org/
confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-
ContributingtoJIRAMaintenance



"
Hyukjin Kwon <gurwls223@gmail.com>,"Sun, 9 Oct 2016 23:47:35 +0900","Suggestion in README.md for guiding pull requests/JIRAs (probably
 about linking CONTRIBUTING.md or wiki)",dev <dev@spark.apache.org>,"Hi all,


I just noticed the README.md (https://github.com/apache/spark) does not
describe the steps or links to follow for creating a PR or JIRA directly. I
know probably it is sensible to search google about the contribution guides
first before trying to make a PR/JIRA but I think it seems not enough when
I see some inappropriate PRs/JIRAs time to time.

I guess flooding JIRAs and PRs is problematic (assuming from the emails in
dev mailing list) and I think we should explicitly mention and describe
this in the README.md and pull request template[1].

(I know we have CONTBITUTING.md[2] and wiki[3] but it seems pretty true
that we still have some PRs or JIRAs not following the documentation.)

So, my suggestions are as below:

- Create a section maybe ""Contributing To Apache Spark"" describing the Wiki
and CONTRIBUTING.md[2] in the README.md.

- Describe an explicit warning in pull request template[1], for example,
""Please double check if your pull request is from a branch to a branch. In
most cases, this change is not appropriate. Please ask to mailing list (
http://spark.apache.org/community.html) if you are not sure.""

[1]https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE
[2]https://github.com/apache/spark/blob/master/CONTRIBUTING.md
[3]https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage


Thank you all.
"
Sean Owen <sowen@cloudera.com>,"Sun, 09 Oct 2016 16:33:57 +0000","Re: Suggestion in README.md for guiding pull requests/JIRAs (probably
 about linking CONTRIBUTING.md or wiki)","Hyukjin Kwon <gurwls223@gmail.com>, dev <dev@spark.apache.org>","Yes, it's really CONTRIBUTING.md that's more relevant, because github
displays a link to it when opening pull requests.
https://github.com/apache/spark/blob/master/CONTRIBUTING.md  There is also
the pull request template:
https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE

I wouldn't want to duplicate info too much, but more pointers to a single
source of information seems OK. Although I don't know if it will help much,
sure, pointers from README.md are OK.


"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 10 Oct 2016 02:09:31 +0900","Re: Suggestion in README.md for guiding pull requests/JIRAs (probably
 about linking CONTRIBUTING.md or wiki)",Sean Owen <sowen@cloudera.com>,"Thanks for confirming this, Sean. I filed this in
https://issues.apache.org/jira/browse/SPARK-17840

I would appreciate if anyone who has a better writing skills better than me
tries to fix this.

I don't want to let reviewers make an effort to correct the grammar.



"
Felix Cheung <felixcheung_m@hotmail.com>,"Sun, 9 Oct 2016 17:14:48 +0000","Re: Suggestion in README.md for guiding pull requests/JIRAs (probably
 about linking CONTRIBUTING.md or wiki)","Hyukjin Kwon <gurwls223@gmail.com>, Sean Owen <sowen@cloudera.com>","Should we just link to

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark





Thanks for confirming this, Sean. I filed this in https://issues.apache.org/jira/browse/SPARK-17840

I would appreciate if anyone who has a better writing skills better than me tries to fix this.

I don't want to let reviewers make an effort to correct the grammar.


Yes, it's really CONTRIBUTING.md that's more relevant, because github displays a link to it when opening pull requests. https://github.com/apache/spark/blob/master/CONTRIBUTING.md  There is also the pull request template: https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE

I wouldn't want to duplicate info too much, but more pointers to a single source of information seems OK. Although I don't know if it will help much, sure, pointers from README.md are OK.

Hi all,


I just noticed the README.md (https://github.com/apache/spark) does not describe the steps or links to follow for creating a PR or JIRA directly. I know probably it is sensible to search google about the contribution guides first before trying to make a PR/JIRA but I think it seems not enough when I see some inappropriate PRs/JIRAs time to time.

I guess flooding JIRAs and PRs is problematic (assuming from the emails in dev mailing list) and I think we should explicitly mention and describe this in the README.md and pull request template[1].

(I know we have CONTBITUTING.md[2] and wiki[3] but it seems pretty true that we still have some PRs or JIRAs not following the documentation.)

So, my suggestions are as below:

- Create a section maybe ""Contributing To Apache Spark"" describing the Wiki and CONTRIBUTING.md[2] in the README.md.

- Describe an explicit warning in pull request template[1], for example, ""Please double check if your pull request is from a branch to a branch. In most cases, this change is not appropriate. Please ask to mailing list (http://spark.apache.org/community.html) if you are not sure.""

[1]https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE
[2]https://github.com/apache/spark/blob/master/CONTRIBUTING.md
[3]https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage


Thank you all.
"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 12:40:11 -0500",Re: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"Here's my specific proposal (meta-proposal?)

Spark Improvement Proposals (SIP)


Background:

The current problem is that design and implementation of large features are
often done in private, before soliciting user feedback.

When feedback is solicited, it is often as to detailed design specifics,
not focused on goals.

When implementation does take place after design, there is often
disagreement as to what goals are or are not in scope.

This results in commits that don't fully meet user needs.


Goals:

- Ensure user, contributor, and committer goals are clearly identified and
agreed upon, before implementation takes place.

- Ensure that a technically feasible strategy is chosen that is likely to
meet the goals.


Rejected Goals:

- SIPs are not for detailed design.  Design by committee doesn't work.

- SIPs are not for every change.  We dont need that much process.


Strategy:

My suggestion is outlined as a Spark Improvement Proposal process
documented at

https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md

Specifics of Jira manipulation are an implementation detail we can figure
out.

I'm suggesting voting; the need here is for a _clear_ outcome.


Rejected Strategies:

Having someone who understands the problem implement it first works, but
only if significant iteration after user feedback is allowed.

Historically this has been problematic due to pressure to limit public api
changes.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 9 Oct 2016 12:34:30 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Hi Cody,

I think this would be a lot more concrete if we had a more detailed template for SIPs. Right now, it's not super clear what's in scope -- e.g. are  they a way to solicit feedback on the user-facing behavior or on the internals? ""Goals"" can cover both things. I've been thinking of SIPs more as Product Requirements Docs (PRDs), which focus on *what* a code change should do as opposed to how.

In particular, here are some things that you may or may not consider in scope for SIPs:

- Goals and non-goals: This is definitely in scope, and IMO should focus on user-visible behavior (e.g. ""system supports SQL window functions"" or ""system continues working if one node fails""). BTW I wouldn't say ""rejected goals"" because some of them might become goals later, so we're not definitively rejecting them.

- Public API: Probably should be included in most SIPs unless it's too large to fully specify then (e.g. ""let's add an ML library"").

- Use cases: I usually find this very useful in PRDs to better communicate the goals.

- Internal architecture: This is usually *not* a thing users can easily comment on and it sounds more like a design doc item. Of course it's however, is that I think we'll have some SIPs primarily on internals (e.g. if somebody wants to refactor Spark's query optimizer or something).

- Rejected strategies: I personally wouldn't put this, because what's the point of voting to reject a strategy before you've really begun designing and implementing something? What if you discover that the strategy is actually better when you start doing stuff?

At a super high level, it depends on whether you want the SIPs to be PRDs for getting some quick feedback on the goals of a feature before it is designed, or something more like full-fledged design docs (just a more visible design doc for bigger changes). I looked at Kafka's KIPs, and they actually seem to be more like design docs. This can work too but it does require more work from the proposer and it can lead to the same problems you mentioned with people already having a design and implementation in mind.

Basically, the question is, are you trying to iterate faster on design by adding a step for user feedback earlier? Or are you just trying to make design docs for key features more visible (and their approval more formal)?

BTW note that in either case, I'd like to have a template for design docs too, which should also include goals. I think that would've avoided some of the issues you brought up.

Matei

features are often done in private, before soliciting user feedback.
specifics, not focused on goals.
disagreement as to what goals are or are not in scope.
and agreed upon, before implementation takes place.
to meet the goals.
documented at
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md <https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md>
figure out.
but only if significant iteration after user feedback is allowed.
api changes.
hear from more people too.
couple of weeks to come up with a concrete, detailed proposal on what this entails, and then we can discuss this the specific proposal as well.
user-facing or cross-cutting changes, not minor feature adds.
<stavros.kontopoulos@lightbend.com targets optimizing efforts, coordination etc. For example really small features should not need to go through this process (assuming they dont touch public interfaces)  or re-factorings and hope it will be kept this way. So as a guideline doc should be provided, like in the KIP case. 
simply having design docs and prototypes implementations in PRs is not something that has not worked so far. What is really a pain in many projects out there is discontinuity in progress of PRs, missing features, slow reviews which is understandable to some extent... it is not only about Spark but things can be improved for sure for this project in particular as already stated.
needs
wiki to
due to
them
<matei.zaharia@gmail.com <mailto:matei.zaharia@gmail.com>>
them
more than
type of
JIRAs from
idea of SIP and design doc
email.
itself
who
similar
processes that
cultures
changes,
design docs
quality
culture
should
follow.
non-goals, to
some
JIRA isn't
get lost
we can't
even be
help by
design
final
every
ideas on
to have
actually not
and we need
worth
direct in
specific
whether an
community
more
<matei.zaharia@gmail.com <mailto:matei.zaharia@gmail.com>>
process that
think
committer cares
hard to
certainly
biggest
about
public APIs
technical
of software
update to a
who's used
this
doesn't work
programming
API, etc)
within reason
etc).
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dave.martin@lightbend.com>

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 09 Oct 2016 20:25:50 +0000",Re: Spark Improvement Proposals,"Matei Zaharia <matei.zaharia@gmail.com>, Cody Koeninger <cody@koeninger.org>","   - Rejected strategies: I personally wouldnâ€™t put this, because whatâ€™s
   the point of voting to reject a strategy before youâ€™ve really begun
   designing and implementing something? What if you discover that the
   strategy is actually better when you start doing stuff?

I would guess the point is to document alternatives that were discussed and
rejected, so that later on people can be pointed to that discussion and the
devs donâ€™t have to repeat themselves unnecessarily every time someone comes
along and asks â€œWhy didnâ€™t you do this other thing?â€ That doesnâ€™t mean a
rejected proposal canâ€™t later be revisited and the SIP canâ€™t be updated.

For reference from the Python community, PEP 492
<https://www.python.org/dev/peps/pep-0492/>, a Python Enhancement Proposal
for adding async and await syntax and â€œfirst-classâ€ coroutines to Python,
has a section on rejected ideas
<https://www.python.org/dev/peps/pep-0492/#why-async-def> for the new
syntax. It captures a summary of what the devs discussed, but it doesnâ€™t
mean the PEP canâ€™t be updated and a previously rejected proposal canâ€™t be
revived.

At least in the Python community, a PEP serves not just as formal starting
point for a proposal (the â€œrealâ€ starting point is usually a discussion on
python-ideas or python-dev), but also as documentation of what was agreed
on and a living â€œspecâ€ of sorts. So PEPs sometimes get updated years after
they are approved when revisions are agreed upon. PEPs are also intended
for wide consumption, vs. bug tracker issues which the broader Python dev
community are not expected to follow closely.

Dunno if we want to follow a similar pattern for Spark, since the projectâ€™s
needs are different. But the Python community has used PEPs to help
organize and steer development since 2000; there are plenty of examples
there we can probably take inspiration from.

By the way, can we call these things something other than Spark Improvement
Proposals? The acronym, SIP, conflicts with Scala SIPs
<http://docs.scala-lang.org/sips/index.html>. Since the Scala and Spark
communities have a lot of overlap, we donâ€™t want, for example, names like
â€œSIP-10â€ to have an ambiguous meaning.

Nick
â€‹


.
ed
e
.
y
)?
of
d
oposals.md
i
s
:
s
ic
o
m
s
o
n
re
d
e
gn
al
on
e
h
n
n
m
d
t
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 9 Oct 2016 13:40:52 -0700",Re: Spark Improvement Proposals,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yup, but the example you gave is for alternatives about *user-facing behavior*, not implementation. The current SIP doc describes ""strategy"" more as implementation strategy. I'm just saying there are different possible goals for these types of docs.

BTW, PEPs and Scala SIPs focus primarily on user-facing behavior, but also require a reference implementation. This is a bit different from what Cody had in mind, I think.

Matei

whatâ€™s the point of voting to reject a strategy before youâ€™ve really begun designing and implementing something? What if you discover that the strategy is actually better when you start doing stuff?
discussed and rejected, so that later on people can be pointed to that discussion and the devs donâ€™t have to repeat themselves unnecessarily every time someone comes along and asks â€œWhy didnâ€™t you do this other thing?â€ That doesnâ€™t mean a rejected proposal canâ€™t later be revisited and the SIP canâ€™t be updated.
<https://www.python.org/dev/peps/pep-0492/>, a Python Enhancement Proposal for adding async and await syntax and â€œfirst-classâ€ coroutines to Python, has a section on rejected ideas <https://www.python.org/dev/peps/pep-0492/#why-async-def> for the new syntax. It captures a summary of what the devs discussed, but it doesnâ€™t mean the PEP canâ€™t be updated and a previously rejected proposal canâ€™t be revived.
starting point for a proposal (the â€œrealâ€ starting point is usually a discussion on python-ideas or python-dev), but also as documentation of what was agreed on and a living â€œspecâ€ of sorts. So PEPs sometimes get updated years after they are approved when revisions are agreed upon. PEPs are also intended for wide consumption, vs. bug tracker issues which the broader Python dev community are not expected to follow closely.
projectâ€™s needs are different. But the Python community has used PEPs to help organize and steer development since 2000; there are plenty of examples there we can probably take inspiration from.
Improvement Proposals? The acronym, SIP, conflicts with Scala SIPs <http://docs.scala-lang.org/sips/index.html>. Since the Scala and Spark communities have a lot of overlap, we donâ€™t want, for example, names like â€œSIP-10â€ to have an ambiguous meaning.
template for SIPs. Right now, it's not super clear what's in scope -- e.g. are  they a way to solicit feedback on the user-facing behavior or on the internals? ""Goals"" can cover both things. I've been thinking of SIPs more as Product Requirements Docs (PRDs), which focus on *what* a code change should do as opposed to how.
in scope for SIPs:
focus on user-visible behavior (e.g. ""system supports SQL window functions"" or ""system continues working if one node fails""). BTW I wouldn't say ""rejected goals"" because some of them might become goals later, so we're not definitively rejecting them.
large to fully specify then (e.g. ""let's add an ML library"").
communicate the goals.
easily comment on and it sounds more like a design doc item. Of course exception, however, is that I think we'll have some SIPs primarily on internals (e.g. if somebody wants to refactor Spark's query optimizer or something).
the point of voting to reject a strategy before you've really begun designing and implementing something? What if you discover that the strategy is actually better when you start doing stuff?
PRDs for getting some quick feedback on the goals of a feature before it is designed, or something more like full-fledged design docs (just a more visible design doc for bigger changes). I looked at Kafka's KIPs, and they actually seem to be more like design docs. This can work too but it does require more work from the proposer and it can lead to the same problems you mentioned with people already having a design and implementation in mind.
by adding a step for user feedback earlier? Or are you just trying to make design docs for key features more visible (and their approval more formal)?
docs too, which should also include goals. I think that would've avoided some of the issues you brought up.
features are often done in private, before soliciting user feedback.
specifics, not focused on goals.
disagreement as to what goals are or are not in scope.
identified and agreed upon, before implementation takes place.
likely to meet the goals.
work.
documented at
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md <https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md>
figure out.
but only if significant iteration after user feedback is allowed.
public api changes.
to hear from more people too.
couple of weeks to come up with a concrete, detailed proposal on what this entails, and then we can discuss this the specific proposal as well.
user-facing or cross-cutting changes, not minor feature adds.
<stavros.kontopoulos@lightbend.com targets optimizing efforts, coordination etc. For example really small features should not need to go through this process (assuming they dont touch public interfaces)  or re-factorings and hope it will be kept this way. So as a guideline doc should be provided, like in the KIP case. 
simply having design docs and prototypes implementations in PRs is not something that has not worked so far. What is really a pain in many projects out there is discontinuity in progress of PRs, missing features, slow reviews which is understandable to some extent... it is not only about Spark but things can be improved for sure for this project in particular as already stated.
it needs
think
split
frustrating
of
wiki to
due to
them
<matei.zaharia@gmail.com <mailto:matei.zaharia@gmail.com>>
them
more than
type of
JIRAs from
the idea of SIP and design doc
his email.
itself
people who
ways similar
processes that
cultures
changes,
design docs
to quality
culture
should
follow.
non-goals, to
some
JIRA isn't
get lost
because we can't
even be
help by
design
final
every
ideas on
to have
actually not
and we need
worth
direct in
specific
whether an
community
more
<matei.zaharia@gmail.com <mailto:matei.zaharia@gmail.com>>
process that
think
committer cares
hard to
certainly
biggest
about
public APIs
technical
of software
update to a
who's used
this
doesn't work
programming
Windows API, etc)
within reason
etc).
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dave.martin@lightbend.com>

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 09 Oct 2016 20:47:40 +0000",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"Oh, hmmâ€¦ I guess Iâ€™m a little confused on the relation between Codyâ€™s email
and the document he linked to, which says:

https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md#when

SIPs should be used for significant user-facing or cross-cutting changes,
not day-to-day improvements. When in doubt, if a committer thinks a change
needs an SIP, it does.

Nick
â€‹


o
y
 whatâ€™s
begun
nd
 someone
â€ That doesnâ€™t
â€™t be
 coroutines
€™t
canâ€™t be
g
y a discussion on
dated years after
PEPs to
mes like
.
ed
e
.
y
)?
of
d
oposals.md
i
s
:
s
ic
o
m
s
o
n
re
d
e
gn
al
on
e
h
n
n
m
d
t
"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 15:58:37 -0500",Re: Spark Improvement Proposals,Nicholas Chammas <nicholas.chammas@gmail.com>,"If there's confusion there, the document is specifically what I'm
proposing.  The email is just by way of introduction.

m

tween Codyâ€™s
e
at
e whatâ€™s
 begun
and
e someone
?â€ That doesnâ€™t
nâ€™t be
€ coroutines
â€™t
 canâ€™t be
 is usually a
 sometimes get
Ps
 PEPs to
ames like
.g.
he
re
e
s
r
cted
.g.
t is
hey
s
s
ake
al)?
d
,
t
:
this
ll
ont
this
ot
ures,
y about
ular as
s
is
s.
to
g
g
o
h
o
n
A
l
s
n
.
e
g
e
o
e
d
"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 16:14:20 -0500",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"So to focus the discussion on the specific strategy I'm suggesting,
documented at

https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md

""Goals: What must this allow people to do, that they can't currently?""

Is it unclear that this is focusing specifically on people-visible behavior?

Rejected goals -  are important because otherwise people keep trying
to argue about scope.  Of course you can change things later with a
different SIP and different vote, the point is to focus.

Use cases - are something that people are going to bring up in
discussion.  If they aren't clearly documented as a goal (""This must
allow me to connect using SSL""), they should be added.

Internal architecture - if the people who need specific behavior are
implementers of other parts of the system, that's fine.

Rejected strategies - If you have none of these, you have no evidence
that the proponent didn't just go with the first thing they had in
mind (or have already implemented), which is a big problem currently.
Approval isn't binding as to specifics of implementation, so these
aren't handcuffs.  The goals are the contract, the strategy is
evidence that contract can actually be met.

Design docs - I'm not touching design docs.  The markdown file I
linked specifically says of the strategy section ""This is not a full
design document.""  Is this unclear?  Design docs can be worked on
obviously, but that's not what I'm concerned with here.





---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 16:19:05 -0500",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"Regarding name, if the SIP overlap is a concern, we can pick a different name.
My tongue in cheek suggestion would be
Spark Lightweight Improvement process (SPARKLI)


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 9 Oct 2016 14:36:07 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Yup, this is the stuff that I found unclear. Thanks for clarifying here, but we should also clarify it in the writeup. In particular:

- Goals needs to be about user-facing behavior (""people"" is broad)

- I'd rename Rejected Goals to Non-Goals. Otherwise someone will dig up one of these and say ""Spark's developers have officially rejected X, which our awesome system has"".

- For user-facing stuff, I think you need a section on API. Virtually all other *IPs I've seen have that.

- I'm still not sure why the strategy section is needed if the purpose is to define user-facing behavior -- unless this is the strategy for setting the goals or for defining the API. That sounds squarely like a design doc issue. In some sense, who cares whether the proposal is technically feasible right now? If it's infeasible, that will be discovered later during design and implementation. Same thing with rejected strategies -- listing some of those is definitely useful sometimes, but if you make this a *required* section, people are just going to fill it in with bogus stuff (I've seen this happen before).

Matei

https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
behavior?
template
 they
internals?
Product
do as
in
focus on
""rejected
too large
communicate
easily
exception,
(e.g.
the
designing
PRDs
is
more
they
does
problems you
mind.
design by
make
formal)?
docs
some of
features are
specifics, not
identified and
likely to
work.
documented
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
figure
but
public api
to
next
what this
small
dont
kept this
case.
simply
something
out there
reviews which is
things can
stated.
<cody@koeninger.org>
think it
goals
I'd
think
split
design is
the
frustrating
to
features.
sides of
subject to
an
putting
going
of
wiki
materialized
them
make
do
new
such
doc
his
itself
people
ways
size,
upload
difficult to
organization.
a
we
to
with
JIRA
signal
because
even
can
A
the
every
useful to
nature and
seems
direct
whether
this
often
no.
think
committer
always
I've
the
talking
That's a
piece of
to
code
used
Windows
etc).
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Ofir Manor <ofir.manor@equalum.io>,"Mon, 10 Oct 2016 01:07:31 +0300",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"This is a great discussion!
Maybe you could have a look at Kafka's process - it also uses Rejected
Alternatives and I personally find it very clear actually (the link also
leads to all KIPs):

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals
Cody - maybe you could take one of the open issues and write a sample
proposal? A concrete example might make it clearer for those who see this
for the first time. Maybe the Kafka offset discussion or some other
Kafka/Structured Streaming open issue? Will that be helpful?

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 17:10:14 -0500",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"Users instead of people, sure.  Commiters and contributors are (or at least
should be) a subset of users.

Non goals, sure. I don't care what the name is, but we need to clearly say
e.g. 'no we are not maintaining compatibility with XYZ right now'.

API, what I care most about is whether it allows me to accomplish the
goals. Arguing about how ugly or pretty it is can be saved for design/
implementation imho.

Strategy, this is necessary because otherwise goals can be out of line with
reality.  Don't propose goals you don't have at least some idea of how to
implement.

Rejected strategies, given that commiters are the only ones I'm saying
should formally submit SPARKLIs or SIPs, if they put junk in a required
section then slap them down for it and tell them to fix it.


"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 17:20:56 -0500",Re: Spark Improvement Proposals,Ofir Manor <ofir.manor@equalum.io>,"Yeah, I've looked at KIPs and Scala SIPs.

I'm reluctant to use the Kafka structured streaming as an example
because of the pre-existing conflict around it.  If Michael or another
committer wanted to put it forth as an example, I'd participate in
good faith though.


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 9 Oct 2016 15:35:09 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Well, I think there are a few things here that don't make sense. First, why should only committers submit SIPs? Development in the project should be open to all contributors, whether they're committers or not. Second, I think unrealistic goals can be found just by inspecting the goals, and I'm not super worried that we'll accept a lot of SIPs that are then infeasible -- we can then submit new ones. But this depends on whether you want this process to be a ""design doc lite"", where people also agree on implementation strategy, or just a way to agree on goals. This is what I asked earlier about PRDs vs design docs (and I'm open to either one but I'd just like clarity). Finally, both as a user and designer of software, I always want to give feedback on APIs, so I'd really like a culture of having those early. People don't argue about prettiness when they discuss APIs, they argue about the core concepts to expose in order to meet various goals, and then they're stuck maintaining those for a long time.

Matei

least should be) a subset of users.
say e.g. 'no we are not maintaining compatibility with XYZ right now'.
goals. Arguing about how ugly or pretty it is can be saved for design/ implementation imho.
with reality.  Don't propose goals you don't have at least some idea of how to implement.
should formally submit SPARKLIs or SIPs, if they put junk in a required section then slap them down for it and tell them to fix it.
here, but we should also clarify it in the writeup. In particular:
up one of these and say ""Spark's developers have officially rejected X, which our awesome system has"".
all other *IPs I've seen have that.
is to define user-facing behavior -- unless this is the strategy for setting the goals or for defining the API. That sounds squarely like a design doc issue. In some sense, who cares whether the proposal is technically feasible right now? If it's infeasible, that will be discovered later during design and implementation. Same thing with rejected strategies -- listing some of those is definitely useful sometimes, but if you make this a *required* section, people are just going to fill it in with bogus stuff (I've seen this happen before).
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md <https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md>
currently?""
behavior?
evidence
currently.
template
are  they
internals?
Product
should do as
consider in
focus on
or
""rejected
too large
communicate
easily
it's
exception,
internals (e.g.
something).
what's the
designing
is
be PRDs
is
more
and they
does
problems you
mind.
design by
make
formal)?
design docs
some of
features are
specifics, not
identified and
likely to
work.
documented
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md <https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md>
figure
works, but
public api
wait to
next
what this
well.
it
small
they dont
kept this
case.
simply
something
projects out there
reviews which is
things can
stated.
<cody@koeninger.org <mailto:cody@koeninger.org>>
think it
goals
I'd
I think
and split
design is
the
frustrating
to
features.
work
sides of
subject to
that an
putting
going
rest of
<rxin@databricks.com <mailto:rxin@databricks.com>>
using wiki
materialized
to them
make
should do
new
such
like the idea of SIP and design doc
<mailto:rxin@databricks.com>>
in his
itself
people
some ways
size,
upload
difficult to
organization.
building a
we
everybody to
with
on JIRA
signal
because
not even
can
feedback. A
means the
accept every
rejecting
useful to
nature and
seems
direct
whether
this
often
no.
don't think
committer
always
I've
the
talking
changing
That's a
piece of
app to
anyone
code
used
Windows
done
3.x, etc).
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:stavros.kontopoulos@lightbend.com>

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 09 Oct 2016 23:00:45 +0000",Re: Spark Improvement Proposals,"Cody Koeninger <cody@koeninger.org>, Matei Zaharia <matei.zaharia@gmail.com>","

If others share my minor concern about the SIP name, I propose Spark
Enhancement Proposal (SEP), taking inspiration from the Python Enhancement
Proposal name.

So if we're going to number proposals like other projects do, they'd be
numbered SEP-1, SEP-2, etc. This avoids the naming conflict with Scala SIPs.

Another way to avoid a conflict is to stick with ""Spark Improvement
Proposal"" but use SPIP as the acronym. So SPIP-1, SPIP-2, etc.

Anyway, it's not a big deal. I just wanted to raise this point.

Nick
"
Cody Koeninger <cody@koeninger.org>,"Sun, 9 Oct 2016 18:22:50 -0500",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"project only commiters have explicit political power.  If a user can't
find a commiter willing to sponsor an SIP idea, they have no way to
get the idea passed in any case.  If I can't find a committer to
sponsor this meta-SIP idea, I'm out of luck.

I do not believe unrealistic goals can be found solely by inspection.
We've managed to ignore unrealistic goals even after implementation!
Focusing on APIs can allow people to think they've solved something,
when there's really no way of implementing that API while meeting the
goals.  Rapid iteration is clearly the best way to address this, but
we've already talked about why that hasn't really worked.  If adding a
non-binding API section to the template is important to you, I'm not
against it, but I don't think it's sufficient.

PRD.  Clear agreement on goals is the most important thing and that's
why it's the thing I want binding agreement on.  But I cannot agree to
goals unless I have enough minimal technical info to judge whether the
goals are likely to actually be accomplished.




---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 9 Oct 2016 17:37:09 -0700","Re: Suggestion in README.md for guiding pull requests/JIRAs (probably
 about linking CONTRIBUTING.md or wiki)",Felix Cheung <felixcheung_m@hotmail.com>,"Actually let's move the discussion to the JIRA ticket, given there is a
ticket.



"
Reynold Xin <rxin@databricks.com>,"Sun, 9 Oct 2016 17:36:21 -0700","Re: Suggestion in README.md for guiding pull requests/JIRAs (probably
 about linking CONTRIBUTING.md or wiki)",Felix Cheung <felixcheung_m@hotmail.com>,"Github already links to CONTRIBUTING.md. -- of course, a lot of people
contributing page in the template (but note that even that introduces some
overhead for every pull request).

Aside from that, I am not sure if the other suggestions in the JIRA ticket
are necessary. For example, the issue with creating a pull request from one
branch to another is a problem, but it happens perhaps less than once a
week and is trivially closeable. Adding an explicit warning there will fix
some cases, but won't entirely eliminate the problem (because I'm sure a
lot of people still don't read the template), and will introduce another
overhead for everybody who submits the proper way.



"
kant kodali <kanth909@gmail.com>,"Mon, 10 Oct 2016 03:13:23 +0000",This Exception has been really hard to trace,"""user @spark"" <user@spark.apache.org>, dev <dev@spark.apache.org>","I tried SpanBy but look like there is a strange error that happening no matter
which way I try. Like the one here described for Java solution.

http://qaoverflow.com/question/how-to-use-spanby-in-java/

java.lang.ClassCastException: cannot assign instance of
scala.collection.immutable.List$SerializationProxy to field
org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type
scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD

JavaPairRDD<ByteBuffer, Iterable<CassandraRow>> cassandraRowsRDD=javaFunctions
(sc).cassandraTable(""test"", ""hello""Â )
.select(""col1"", ""col2"", ""col3""Â )
.spanBy(newFunction<CassandraRow, ByteBuffer>() {
@Override
publicByteBuffer call(CassandraRow v1) {
returnv1.getBytes(""rowkey"");
}
}, ByteBuffer.class);

And then here I do this here is where the problem occurs
List<Tuple2<ByteBuffer, Iterable<CassandraRow>>> listOftuples =
cassandraRowsRDD.collect();Â // ERROR OCCURS HERE
Tuple2<ByteBuffer, Iterable<CassandraRow>> tuple =
listOftuples.iterator().next();
ByteBuffer partitionKey = tuple._1();
for(CassandraRow cassandraRow: tuple._2()) {
System.out.println(cassandraRow.getLong(""col1""));
}
so I tried this Â and same error
Iterable<Tuple2<ByteBuffer, Iterable<CassandraRow>>> listOftuples =
cassandraRowsRDD.collect();Â // ERROR OCCURS HERE
Tuple2<ByteBuffer, Iterable<CassandraRow>> tuple =
listOftuples.iterator().next();
ByteBuffer partitionKey = tuple._1();
for(CassandraRow cassandraRow: tuple._2()) {
System.out.println(cassandraRow.getLong(""col1""));
}
Although I understand that ByteBuffers aren't serializable I didn't get any not
serializable exception but still I went head and changed everything to byte[] so
no more ByteBuffers in the code.
I have also tried cassandraRowsRDD.collect().forEach() and
cassandraRowsRDD.stream().forEachPartition() and the same exact error occurs.
I am running everything locally and in a stand alone mode so my spark cluster is
just running on localhost.
Scala code runner version 2.11.8 Â // when I run scala -version or even
./spark-shell

compile group: 'org.apache.spark' name: 'spark-core_2.11' version: '2.0.0'
compile group: 'org.apache.spark' name: 'spark-streaming_2.11' version: '2.0.0'
compile group: 'org.apache.spark' name: 'spark-sql_2.11' version: '2.0.0'
compile group: 'com.datastax.spark' name: 'spark-cassandra-connector_2.11'
version: '2.0.0-M3':

So I don't see anything wrong with these versions.
2) I am bundling everything into one jar and so far it did worked out well
except for this error.
I am using Java 8 and Gradle.

any ideas on how I can fix this?"
Reynold Xin <rxin@databricks.com>,"Sun, 9 Oct 2016 20:48:00 -0700",Re: This Exception has been really hard to trace,kant kodali <kanth909@gmail.com>,"You should probably check with DataStax who build the Cassandra connector
for Spark.



"
kant kodali <kanth909@gmail.com>,"Mon, 10 Oct 2016 04:30:44 +0000",Re: This Exception has been really hard to trace,Reynold Xin <rxin@databricks.com>,"Hi Reynold,
Actually, I did that a well before posting my question here.
Thanks,kant
 





rxin@databricks.com
You should probably check with DataStax who build the Cassandra connector for
Spark.



I tried SpanBy but look like there is a strange error that happening no matter
which way I try. Like the one here described for Java solution.

http://qaoverflow.com/question/how-to-use-spanby-in-java/

java.lang.ClassCastException: cannot assign instance of
scala.collection.immutable.List$SerializationProxy to field
org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type
scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD

JavaPairRDD<ByteBuffer, Iterable<CassandraRow>> cassandraRowsRDD=javaFunctions
(sc).cassandraTable(""test"", ""hello""Â )
.select(""col1"", ""col2"", ""col3""Â )
.spanBy(newFunction<CassandraRow, ByteBuffer>() {
@Override
publicByteBuffer call(CassandraRow v1) {
returnv1.getBytes(""rowkey"");
}
}, ByteBuffer.class);

And then here I do this here is where the problem occurs
List<Tuple2<ByteBuffer, Iterable<CassandraRow>>> listOftuples =
cassandraRowsRDD.collect();Â // ERROR OCCURS HERE
Tuple2<ByteBuffer, Iterable<CassandraRow>> tuple =
listOftuples.iterator().next();
ByteBuffer partitionKey = tuple._1();
for(CassandraRow cassandraRow: tuple._2()) {
System.out.println(cassandraRow.getLong(""col1""));
}
so I tried this Â and same error
Iterable<Tuple2<ByteBuffer, Iterable<CassandraRow>>> listOftuples =
cassandraRowsRDD.collect();Â // ERROR OCCURS HERE
Tuple2<ByteBuffer, Iterable<CassandraRow>> tuple =
listOftuples.iterator().next();
ByteBuffer partitionKey = tuple._1();
for(CassandraRow cassandraRow: tuple._2()) {
System.out.println(cassandraRow.getLong(""col1""));
}
Although I understand that ByteBuffers aren't serializable I didn't get any not
serializable exception but still I went head and changed everything to byte[] so
no more ByteBuffers in the code.
I have also tried cassandraRowsRDD.collect().forEach() and
cassandraRowsRDD.stream().forEachPartition() and the same exact error occurs.
I am running everything locally and in a stand alone mode so my spark cluster is
just running on localhost.
Scala code runner version 2.11.8 Â // when I run scala -version or even
./spark-shell

compile group: 'org.apache.spark' name: 'spark-core_2.11' version: '2.0.0'
compile group: 'org.apache.spark' name: 'spark-streaming_2.11' version: '2.0.0'
compile group: 'org.apache.spark' name: 'spark-sql_2.11' version: '2.0.0'
compile group: 'com.datastax.spark' name: 'spark-cassandra-connector_2.11'
version: '2.0.0-M3':

So I don't see anything wrong with these versions.
2) I am bundling everything into one jar and so far it did worked out well
except for this error.
I am using Java 8 and Gradle.

any ideas on how I can fix this?"
Reynold Xin <rxin@databricks.com>,"Sun, 9 Oct 2016 21:50:10 -0700",SPARK-17845 - window function frame boundary API,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi all,

I tried to use the window function DataFrame API this weekend and found it
awkward to use, especially with respect to specifying frame boundaries. I
wrote down some options here and am curious your thoughts. If you have
suggestions on the API beyond what's already listed in the JIRA ticket, do
bring them up too.

Please comment on the JIRA ticket directly:
https://issues.apache.org/jira/browse/SPARK-17845


I've attached the content of the JIRA ticket here to save you a click:


ANSI SQL uses the following to specify the frame boundaries for window
functions:

ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING
ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDING
ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
ROWS BETWEEN CURRENT ROW AND UNBOUNDED PRECEDING
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING

In Spark's DataFrame API, we use integer values to indicate relative
position:

   - 0 means ""CURRENT ROW""
   - -1 means ""1 PRECEDING""
   - Long.MinValue means ""UNBOUNDED PRECEDING""
   - Long.MaxValue to indicate ""UNBOUNDED FOLLOWING""

// ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWINGWindow.rowsBetween(-3, +3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND 3
PRECEDINGWindow.rowsBetween(Long.MinValue, -3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT
ROWWindow.rowsBetween(Long.MinValue, 0)
// ROWS BETWEEN CURRENT ROW AND UNBOUNDED
PRECEDINGWindow.rowsBetween(0, Long.MaxValue)
// ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED
FOLLOWINGWindow.rowsBetween(Long.MinValue, Long.MaxValue)

I think using numeric values to indicate relative positions is actually a
good idea, but the reliance on Long.MinValue and Long.MaxValue to indicate
unbounded ends is pretty confusing:

1. The API is not self-evident. There is no way for a new user to figure
out how to indicate an unbounded frame by looking at just the API. The user
has to read the doc to figure this out.
2. It is weird Long.MinValue or Long.MaxValue has some special meaning.
3. Different languages have different min/max values, e.g. in Python we use
-sys.maxsize and +sys.maxsize.

To make this API less confusing, we have a few options:

Option 1. Add the following (additional) methods:

// ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWINGWindow.rowsBetween(-3, +3)
// this one exists already// ROWS BETWEEN UNBOUNDED PRECEDING AND 3
PRECEDINGWindow.rowsBetweenUnboundedPrecedingAnd(-3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT
ROWWindow.rowsBetweenUnboundedPrecedingAndCurrentRow()
// ROWS BETWEEN CURRENT ROW AND UNBOUNDED
PRECEDINGWindow.rowsBetweenCurrentRowAndUnboundedFollowing()
// ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED
FOLLOWINGWindow.rowsBetweenUnboundedPrecedingAndUnboundedFollowing()

This is obviously very verbose, but is very similar to how these functions
are done in SQL, and is perhaps the most obvious to end users, especially
if they come from SQL background.

Option 2. Decouple the specification for frame begin and frame end into two
functions. Assume the boundary is unlimited unless specified.

// ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWINGWindow.rowsFrom(-3).rowsTo(3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDINGWindow.rowsTo(-3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT
ROWWindow.rowsToCurrent() or Window.rowsTo(0)
// ROWS BETWEEN CURRENT ROW AND UNBOUNDED
PRECEDINGWindow.rowsFromCurrent() or Window.rowsFrom(0)
// ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING// no need to specify

If we go with option 2, we should throw exceptions if users specify
multiple from's or to's. A variant of option 2 is to require explicitly
specification of begin/end even in the case of unbounded boundary, e.g.:

Window.rowsFromBeginning().rowsTo(-3)
or
Window.rowsFromUnboundedPreceding().rowsTo(-3)
"
ayan guha <guha.ayan@gmail.com>,"Mon, 10 Oct 2016 16:48:13 +1100",Re: SPARK-17845 - window function frame boundary API,Reynold Xin <rxin@databricks.com>,"Hi Reynold

Thanks for asking. I am from sql world and use sparl sql with analytical
functions prety heavily.

IMHO, Window.rowsBetween() as a function name looks fine. What i would
propose would be:

Window.rowsBetween(startFrom=UNBOUNDED,endTo=CURRENT_ROW,preceeding=0,following=0)


startFrom, endTo: Determining range

preceeding,following: Anchor of current row and thus altering the range.


Calls:


//ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING

Window.rowsBetween(preceeding=3,following=3)

//ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDING

Window.rowsBetween(startFrom=UNBOUNDED,preceeding=3)

//ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW

Window.rowsBetween(startFrom=UNBOUNDED,endTo=CURRENT_ROW)

//ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING (Not PRECEDING)

Window.rowsBetween(startFrom=CURRENT_ROW,endTo=UNBOUNDED)

//ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING

Window.rowsBetween(startFrom=UNBOUNDED,endTo=UNBOUNDED)




//ROWS BETWEEN 3 FOLLOWING AND UNBOUNDED FOLLOWING (pair of (2))

Window.rowsBetween(endTo=UNBOUNDED,following=3)


This will be closer to SQL options, IMHO.


Thoughts?





-- 
Best Regards,
Ayan Guha
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sun, 9 Oct 2016 23:34:03 -0700 (MST)",RE: Spark Improvement Proposals,dev@spark.apache.org,"I agree with most of what Cody said.
Two things:
First we can always have other people suggest SIPs but mark them as ""unreviewed"" and have committers basically move them forward. The problem is that writing a good document takes time. This way we can leverage non committers to do some of this work (it is just another way to contribute).

As for strategy, in many cases implementation strategy can affect the goals. I will give  a small example: In the current structured streaming strategy, we group by the time to achieve a sliding window. This is definitely an implementation decision and not a goal. However, I can think of several aggregation functions which have the time inside their calculation buffer. For to implement this would be to make the set into a map and have the value contain the last time seen. Multiplying it across the groupby would cost a lot in performance. So adding such a strategy would have a great effect on the type of aggregations and their performance which does affect the goal. Without adding the strategy, it is easy for whoever goes to the design document to not think about these cases. Furthermore, it might be decided that these cases are rare enough so that the strategy is still good enough but how would we know it without user feedback?
I believe this example is exactly what Cody was talking about. Since many times implementation strategies have a large effect on the goal, we should have it discussed when discussing the goals. In addition, while it is often easy to throw out completely infeasible goals, it is often much harder to figure out that the goals are unfeasible without fine tuning.


Assaf.

From: Cody Koeninger-2 [via Apache Spark Developers List] [mailto:ml-node+s1001551n19359h18@n3.nabble.com]
Sent: Monday, October 10, 2016 2:25 AM
To: Mendelson, Assaf
Subject: Re: Spark Improvement Proposals

project only commiters have explicit political power.  If a user can't
find a commiter willing to sponsor an SIP idea, they have no way to
get the idea passed in any case.  If I can't find a committer to
sponsor this meta-SIP idea, I'm out of luck.

I do not believe unrealistic goals can be found solely by inspection.
We've managed to ignore unrealistic goals even after implementation!
Focusing on APIs can allow people to think they've solved something,
when there's really no way of implementing that API while meeting the
goals.  Rapid iteration is clearly the best way to address this, but
we've already talked about why that hasn't really worked.  If adding a
non-binding API section to the template is important to you, I'm not
against it, but I don't think it's sufficient.

PRD.  Clear agreement on goals is the most important thing and that's
why it's the thing I want binding agreement on.  But I cannot agree to
goals unless I have enough minimal technical info to judge whether the
goals are likely to actually be accomplished.




hy
ink
 we
ss
 to
.
out
're
st
y
ls.
th
ch
l
s
g
c
ible
gn
of
-proposals.md
ndEmail.jtp?type=node&node=19359&i=4>>
do
in
o
ly
n,
s
e
n
ke
es
d
y
.
t-proposals.md
c
dEmail.jtp?type=node&node=19359&i=6>>
to
t
t
/SendEmail.jtp?type=node&node=19359&i=7>>
t
e.
s
ser/SendEmail.jtp?type=node&node=19359&i=9>>
nk
d
n
he
o
es
t
an
of
/SendEmail.jtp?type=node&node=19359&i=10>>
=11>>
do
ew
ch
SendEmail.jtp?type=node&node=19359&i=12>>
d
t
.
a
.
e
to
th
an
 A
ul
e
ms
en
o.
=13>>
he
ng
s
ce
to
de
ed
----
node&node=19359&i=14>
=15>

---------------------------------------------------------------------
=19359&i=16>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Improvement-Proposals-tp19268p19359.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Spark-Improvement-Proposals-tp19268p19367.html
om."
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Sun, 9 Oct 2016 23:44:26 -0700",Re: This Exception has been really hard to trace,kant kodali <kanth909@gmail.com>,"Seems the runtime Spark is different from the compiled one. You should
mark the Spark components  ""provided"". See
https://issues.apache.org/jira/browse/SPARK-9219


"
Reynold Xin <rxin@databricks.com>,"Mon, 10 Oct 2016 01:00:51 -0700",Re: Monitoring system extensibility,Pete Robbins <robbinspg@gmail.com>,"I just took a quick look and set a target version on the JIRA. But Pete I
think the primary problem with the JIRA and pull request is that it really
just argues (or implements) opening up a private API, which is a valid
point but there are a lot more that needs to be done before making some
private API public.

At the very least, we need to answer the following:

1. Is the existing API maintainable? E.g. Is it OK to just expose coda hale
metrics in the API? Do we need to worry about dependency conflicts? Should
we wrap it?

2. Is the existing API sufficiently general (to cover use cases)? What
about security related setup?





 a
t,
es
t
lic
d
s
a new implementation
to
for
"
Pete Robbins <robbinspg@gmail.com>,"Mon, 10 Oct 2016 08:55:46 +0000",Re: Monitoring system extensibility,Reynold Xin <rxin@databricks.com>,"Yes I agree. I'm not sure how important this is anyway. It's a little
annoying but easy to work around.


y
 a
t,
s
ic
pache/spark/metrics/source/Source.scala
m
parkâ€™
ts
"
kant kodali <kanth909@gmail.com>,"Mon, 10 Oct 2016 09:33:11 +0000",Re: This Exception has been really hard to trace,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Hi
I use gradle and I don't think it really has ""provided"" but I was able to google
and create the following file but the same error still persist.
group 'com.company'version '1.0-SNAPSHOT'
apply plugin: 'java'apply plugin: 'idea'
repositories {Â Â Â Â mavenCentral()Â Â Â Â mavenLocal()}
configurations {Â Â Â Â provided}sourceSets {Â Â Â Â main {Â Â Â Â Â Â Â Â compileClasspath +=
configurations.providedÂ Â Â Â Â Â Â Â test.compileClasspath += configurations.provided
Â Â Â Â Â Â Â Â test.runtimeClasspath += configurations.providedÂ Â Â Â }}
idea {Â Â Â Â module {Â Â Â Â Â Â Â Â scopes.PROVIDED.plus += [ configurations.provided ]
Â Â Â Â }}
dependencies {Â Â Â Â compile 'org.slf4j:slf4j-log4j12:1.7.12'Â Â Â Â provided group:
'org.apache.spark', name: 'spark-core_2.11', version: '2.0.0'Â Â Â Â provided group:
'org.apache.spark', name: 'spark-streaming_2.11', version: '2.0.0'Â Â Â Â provided
group: 'org.apache.spark', name: 'spark-sql_2.11', version: '2.0.0'Â Â Â Â provided
group: 'com.datastax.spark', name: 'spark-cassandra-connector_2.11', version:
'2.0.0-M3'}


jar {Â Â Â Â from { configurations.provided.collect { it.isDirectory() ? it :
zipTree(it) } }Â Â Â // with jarÂ Â Â Â from sourceSets.test.outputÂ Â Â Â manifest {
Â Â Â Â Â Â Â Â attributes 'Main-Class': ""com.company.batchprocessing.Hello""Â Â Â Â }
Â Â Â Â exclude 'META-INF/.RSA', 'META-INF/.SF', 'META-INF/*.DSA'Â Â Â Â zip64 true}
This successfully creates the jar but the error still persists.
 





Seems the runtime Spark is different from the compiled one. You should markÂ the
Spark components Â ""provided"". See
https://issues.apache.org/jira/browse/SPARK-9219

I tried SpanBy but look like there is a strange error that happening no matter
which way I try. Like the one here described for Java solution.

http://qaoverflow.com/question/how-to-use-spanby-in-java/

java.lang.ClassCastException: cannot assign instance of
scala.collection.immutable.List$SerializationProxy to field
org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type
scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD

JavaPairRDD<ByteBuffer, Iterable<CassandraRow>> cassandraRowsRDD=javaFunctions
(sc).cassandraTable(""test"", ""hello""Â )
.select(""col1"", ""col2"", ""col3""Â )
.spanBy(newFunction<CassandraRow, ByteBuffer>() {
@Override
publicByteBuffer call(CassandraRow v1) {
returnv1.getBytes(""rowkey"");
}
}, ByteBuffer.class);

And then here I do this here is where the problem occurs
List<Tuple2<ByteBuffer, Iterable<CassandraRow>>> listOftuples =
cassandraRowsRDD.collect();Â // ERROR OCCURS HERE
Tuple2<ByteBuffer, Iterable<CassandraRow>> tuple =
listOftuples.iterator().next();
ByteBuffer partitionKey = tuple._1();
for(CassandraRow cassandraRow: tuple._2()) {
System.out.println(cassandraRow.getLong(""col1""));
}
so I tried this Â and same error
Iterable<Tuple2<ByteBuffer, Iterable<CassandraRow>>> listOftuples =
cassandraRowsRDD.collect();Â // ERROR OCCURS HERE
Tuple2<ByteBuffer, Iterable<CassandraRow>> tuple =
listOftuples.iterator().next();
ByteBuffer partitionKey = tuple._1();
for(CassandraRow cassandraRow: tuple._2()) {
System.out.println(cassandraRow.getLong(""col1""));
}
Although I understand that ByteBuffers aren't serializable I didn't get any not
serializable exception but still I went head and changed everything to byte[] so
no more ByteBuffers in the code.
I have also tried cassandraRowsRDD.collect().forEach() and
cassandraRowsRDD.stream().forEachPartition() and the same exact error occurs.
I am running everything locally and in a stand alone mode so my spark cluster is
just running on localhost.
Scala code runner version 2.11.8 Â // when I run scala -version or even
./spark-shell

compile group: 'org.apache.spark' name: 'spark-core_2.11' version: '2.0.0'
compile group: 'org.apache.spark' name: 'spark-streaming_2.11' version: '2.0.0'
compile group: 'org.apache.spark' name: 'spark-sql_2.11' version: '2.0.0'
compile group: 'com.datastax.spark' name: 'spark-cassandra-connector_2.11'
version: '2.0.0-M3':

So I don't see anything wrong with these versions.
2) I am bundling everything into one jar and so far it did worked out well
except for this error.
I am using Java 8 and Gradle.

any ideas on how I can fix this?"
Cody Koeninger <cody@koeninger.org>,"Mon, 10 Oct 2016 09:41:36 -0500",Re: Spark Improvement Proposals,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Yes, users suggesting SIPs is a good thing and is explicitly called
out in the linked document under the Who? section.  Formally proposing
them, not so much, because of the political realities.

Yes, implementation strategy definitely affects goals.  There are all
kinds of examples of this, I'll pick one that's my fault so as to
avoid sounding like I'm blaming:

When I implemented the Kafka DStream, one of my (not explicitly agreed
upon by the community) goals was to make sure people could use the
Dstream with however they were already using Kafka at work.  The lack
of explicit agreement on that goal led to all kinds of fighting with
committers, that could have been avoided.  The lack of explicit
up-front strategy discussion led to the DStream not really working
with compacted topics.  I knew about compacted topics, but don't have
a use for them, so had a blind spot there.  If there was explicit
up-front discussion that my strategy was ""assume that batches can be
defined on the driver solely by beginning and ending offsets"", there's
a greater chance that a user would have seen that and said, ""hey, what
about non-contiguous offsets in a compacted topic"".

This kind of thing is only going to happen smoothly if we have a
lightweight user-visible process with clear outcomes.

ard. The problem is
.
ls.
y,
.
lue
a
n
.
h
d
en
-
t
y.
ay
o
,
ll
is
ng
oc
n
t-proposals.md
""
e
r
oo
s
s
is
re
d
gn
ed
ly
k.
nt-proposals.md
ic
xt
at
.
t
y
pt
s
ws
'd
gn
to
k
ct
g
o
n
n
ad
lt
n.
 a
A.
we
n
t
.
g
re
's
e
e
-----
nt-Proposals-tp19268p19359.html

---------------------------------------------------------------------


"
Deepak Sharma <deepakmca05@gmail.com>,"Mon, 10 Oct 2016 20:18:47 +0530",Auto start spark jobs,dev@spark.apache.org,"Hi All
Is there any way to schedule the ever running spark in such a way that it
comes up on its own , after the cluster maintenance?


-- 
Thanks
Deepak
www.bigdatabig.com
www.keosha.net
"
jamborta <jamborta@gmail.com>,"Mon, 10 Oct 2016 08:26:13 -0700 (MST)",Spark 2.0.0 job completes but hangs,dev@spark.apache.org,"Hi all,

I have a spark job that takes about an hour to run, in the end it completes
all the task, then the job just hangs and does nothing (it writes to s3 as
the last step, which also gets completed, all files appear on s3).

any ideas how to debug this? 

see the thread dump below: 

""Attach Listener"" daemon prio=10 tid=0x00007f67a8001000 nid=0x7b90 waiting
on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""SparkUI-224"" daemon prio=10 tid=0x00007f6778001000 nid=0x7b4a waiting on
condition [0x00007f67c2cfa000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:745)

""SparkUI-223"" daemon prio=10 tid=0x00007f671008e000 nid=0x7b49 waiting on
condition [0x00007f67d1b4d000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:745)

""SparkUI-222"" daemon prio=10 tid=0x00007f677c006800 nid=0x7b48 waiting on
condition [0x00007f67c8e33000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:745)

""DestroyJavaVM"" prio=10 tid=0x00007f67ed157000 nid=0x23c9 waiting on
condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""SparkUI-205"" daemon prio=10 tid=0x00007f6718002000 nid=0x7aa0 waiting on
condition [0x00007f67b82ee000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
	at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:745)

""Scheduler-916842649"" prio=10 tid=0x00007f672c004000 nid=0x7a9e waiting on
condition [0x00007f67b89f5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c78ee520> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-7"" daemon prio=10 tid=0x00007f66e4007800 nid=0x61cf runnable
[0x00007f67c8429000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c7bdd8e0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c7bdd940> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c7bdd838> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""task-result-getter-3"" daemon prio=10 tid=0x00007f676c009000 nid=0x24b6
waiting on condition [0x00007f67b8bf7000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""task-result-getter-2"" daemon prio=10 tid=0x00007f6764007000 nid=0x24b5
waiting on condition [0x00007f67b8cf8000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""task-result-getter-1"" daemon prio=10 tid=0x00007f675c004000 nid=0x24b4
waiting on condition [0x00007f67b8df9000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""task-result-getter-0"" daemon prio=10 tid=0x00007f6768010000 nid=0x24b3
waiting on condition [0x00007f67c862b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-6"" daemon prio=10 tid=0x00007f66e4011800 nid=0x24b2 runnable
[0x00007f66cbef9000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8de9f60> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c8de9f50> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8de9f80> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-5"" daemon prio=10 tid=0x00007f66e4010000 nid=0x24b1 runnable
[0x00007f67b8efa000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c7bde078> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c7bde0d8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c7bddfe0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-4"" daemon prio=10 tid=0x00007f66e400c800 nid=0x24b0 runnable
[0x00007f66cdffc000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8e06ae8> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c8e06ad8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8e06b08> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-3"" daemon prio=10 tid=0x00007f66e400b000 nid=0x24af runnable
[0x00007f67b8ffb000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8e0d190> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c8e0d180> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8e0d1b0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-2"" daemon prio=10 tid=0x00007f66e4008800 nid=0x24ae runnable
[0x00007f67c01e2000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8e49100> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c8e490f0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8e49120> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" daemon prio=10 tid=0x00007f66e400a000 nid=0x24ad runnable
[0x00007f67d1f8e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8d31698> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c8d316f8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8d31600> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""java-sdk-http-connection-reaper"" daemon prio=10 tid=0x000000000256f000
nid=0x24ac waiting on condition [0x00007f67c04f0000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at
com.amazonaws.http.IdleConnectionReaper.run(IdleConnectionReaper.java:112)

""shuffle-server-7"" daemon prio=10 tid=0x00007f674800d000 nid=0x24ab runnable
[0x00007f67c25f3000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c6603918> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c66049d8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c6603870> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-6"" daemon prio=10 tid=0x00007f674800b000 nid=0x24aa runnable
[0x00007f67c26f4000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c78aa328> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c78aa348> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c78aa2e0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-5"" daemon prio=10 tid=0x00007f6748009000 nid=0x24a9 runnable
[0x00007f67c27f5000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8ccfca0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c8cd0d70> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8ccfbf8> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-4"" daemon prio=10 tid=0x00007f6748007000 nid=0x24a8 runnable
[0x00007f67c28f6000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c54e4cf0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c54e4db0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c54e4c48> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-3"" daemon prio=10 tid=0x00007f6748005000 nid=0x24a7 runnable
[0x00007f67c29f7000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c65dae00> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c65de9d8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c65dad58> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-2"" daemon prio=10 tid=0x00007f6748003800 nid=0x24a6 runnable
[0x00007f67c2af8000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8ccfa18> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c77ce728> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8ccf970> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" daemon prio=10 tid=0x00007f6748001800 nid=0x24a5 runnable
[0x00007f67c2bf9000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c54bf8c0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c54c3fb0> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c54bf818> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""SparkListenerBus"" daemon prio=10 tid=0x00007f6770e64800 nid=0x24a2 waiting
on condition [0x00007f67c2dfb000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c7a97800> (a
java.util.concurrent.Semaphore$NonfairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
	at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:67)
	at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
	at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:65)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1229)
	at
org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:64)

""context-cleaner-periodic-gc"" daemon prio=10 tid=0x00007f6770e61000
nid=0x24a1 waiting on condition [0x00007f67c2efc000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c78f33e8> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""Spark Context Cleaner"" daemon prio=10 tid=0x00007f6770e5f000 nid=0x24a0 in
Object.wait() [0x00007f67c2ffd000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000c78caac0> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked <0x00000000c78caac0> (a java.lang.ref.ReferenceQueue$Lock)
	at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:175)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1229)
	at
org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:172)
	at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:67)

""shuffle-server-0"" daemon prio=10 tid=0x00007f6770bf9800 nid=0x249f runnable
[0x00007f67c8126000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c8d55b88> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c8d5ce98> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c8d55af0> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""threadDeathWatcher-2-1"" daemon prio=10 tid=0x00007f66ec00f800 nid=0x249e
waiting on condition [0x00007f67c8227000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at
io.netty.util.ThreadDeathWatcher$Watcher.run(ThreadDeathWatcher.java:137)
	at
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)

""shuffle-client-0"" daemon prio=10 tid=0x00007f66e8003800 nid=0x249d runnable
[0x00007f67c8328000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x00000000c31e90a8> (a
io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x00000000c31e90c8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000c31e9060> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

""appclient-registration-retry-thread"" daemon prio=10 tid=0x00007f6760003800
nid=0x249b waiting on condition [0x00007f67c852a000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c3207ec0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""driver-revive-thread"" daemon prio=10 tid=0x00007f676800d000 nid=0x2499
waiting on condition [0x00007f67c872c000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c8d90dd0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""dag-scheduler-event-loop"" daemon prio=10 tid=0x00007f6770be7000 nid=0x2498
waiting on condition [0x00007f67c882d000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c8d897c8> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at
java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
	at
java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:46)

""netty-rpc-env-timeout"" daemon prio=10 tid=0x00007f670c00c800 nid=0x2497
waiting on condition [0x00007f67c892e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c32087c0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
	at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""Timer-0"" daemon prio=10 tid=0x00007f6770bb1800 nid=0x2496 in Object.wait()
[0x00007f67c8a2f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000c8d884f8> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000000c8d884f8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""heartbeat-receiver-event-loop-thread"" daemon prio=10 tid=0x00007f675c001800
nid=0x2495 waiting on condition [0x00007f67c8b30000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c78ef1f0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Abstract"
"
Sean Owen <sowen@cloudera.com>,Mon"," 10 Oct 2016 15:54:49 +0000""",Re: Spark 2.0.0 job completes but hangs,"jamborta <jamborta@gmail.com>, dev@spark.apache.org","I am pretty sure that's
https://issues.apache.org/jira/browse/SPARK-17707


"
Russell Spitzer <russell.spitzer@gmail.com>,"Mon, 10 Oct 2016 16:33:14 +0000",Official Stance on Not Using Spark Submit,dev <dev@spark.apache.org>,"I've seen a variety of users attempting to work around using Spark Submit
with at best middling levels of success. I think it would be helpful if the
project had a clear statement that submitting an application without using
Spark Submit is truly for experts only or is unsupported entirely.

I know this is a pretty strong stance and other people have had different
experiences than me so please let me know what you think :)
"
Tamas Jambor <jamborta@gmail.com>,"Mon, 10 Oct 2016 17:35:49 +0100",Re: Spark 2.0.0 job completes but hangs,Sean Owen <sowen@cloudera.com>,"Thanks Sean, that explains it.


"
Reynold Xin <rxin@databricks.com>,"Mon, 10 Oct 2016 09:58:29 -0700",Re: Official Stance on Not Using Spark Submit,Russell Spitzer <russell.spitzer@gmail.com>,"How are they using it? Calling some main function directly?


"
Marcin Tustin <mtustin@handybook.com>,"Mon, 10 Oct 2016 12:59:59 -0400",Re: Official Stance on Not Using Spark Submit,Reynold Xin <rxin@databricks.com>,"I've done this for some pyspark stuff. I didn't find it especially
problematic.



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Sean Owen <sowen@cloudera.com>,"Mon, 10 Oct 2016 17:04:59 +0000",Re: Official Stance on Not Using Spark Submit,"Russell Spitzer <russell.spitzer@gmail.com>, dev <dev@spark.apache.org>","I have also 'embedded' a Spark driver without much trouble. It isn't that
it can't work.

The Launcher API is ptobably the recommended way to do that though.
spark-submit is the way to go for non programmatic access.

If you're not doing one of those things and it is not working, yeah I think
people would tell you you're on your own. I think that's consistent with
all the JIRA discussions I have seen over time.


"
Russell Spitzer <russell.spitzer@gmail.com>,"Mon, 10 Oct 2016 17:17:44 +0000",Re: Official Stance on Not Using Spark Submit,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","I'm definitely only talking about non-embedded uses here as I also use
embedded Spark (cassandra, and kafka) to run tests. This is almost always
safe since everything is in the same JVM. It's only once we get to
launching against a real distributed env do we end up with issues.

Since Pyspark uses spark submit in the java gateway i'm not sure if that
matters :)

The cases I see are usually usually going through main directly, adding
jars programatically.

Usually ends up with classpath errors (Spark not on the CP, their jar not
on the CP, dependencies not on the cp),
conf errors (executors have the incorrect environment, executor classpath
broken, not understanding spark-defaults won't do anything),
Jar version mismatches
Etc ...


"
Russell Spitzer <russell.spitzer@gmail.com>,"Mon, 10 Oct 2016 17:21:12 +0000",Re: Official Stance on Not Using Spark Submit,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","I actually had not seen SparkLauncher before, that looks pretty great :)


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 10 Oct 2016 11:09:01 -0700",Re: Official Stance on Not Using Spark Submit,Russell Spitzer <russell.spitzer@gmail.com>,"What are the main use cases you've seen for this? Maybe we can add a page to the docs about how to launch Spark as an embedded library.

Matei

:)
embedded Spark (cassandra, and kafka) to run tests. This is almost always safe since everything is in the same JVM. It's only once we get to launching against a real distributed env do we end up with issues.
that matters :)
adding jars programatically. 
not on the CP, dependencies not on the cp), 
classpath broken, not understanding spark-defaults won't do anything),
that it can't work. 
spark-submit is the way to go for non programmatic access. 
think people would tell you you're on your own. I think that's consistent with all the JIRA discussions I have seen over time. 
Submit with at best middling levels of success. I think it would be helpful if the project had a clear statement that submitting an application without using Spark Submit is truly for experts only or is unsupported entirely.
different experiences than me so please let me know what you think :)

"
Russell Spitzer <russell.spitzer@gmail.com>,"Mon, 10 Oct 2016 18:13:15 +0000",Re: Official Stance on Not Using Spark Submit,Matei Zaharia <matei.zaharia@gmail.com>,"Just folks who don't want to use spark-submit, no real use-cases I've seen
yet.

I didn't know about SparkLauncher myself and I don't think there are any
official docs on that or launching spark as an embedded library for tests.


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 10 Oct 2016 11:54:08 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"+1 to votes to approve proposals. I agree that proposals should have an
official mechanism to be accepted, and a vote is an established means of
doing that well. I like that it includes a period to review the proposal
and I think proposals should have bee"
Cody Koeninger <cody@koeninger.org>,"Mon, 10 Oct 2016 14:02:01 -0500",Re: Spark Improvement Proposals,Ryan Blue <rblue@netflix.com>,"I think the main value is in being honest about what's going on.  No
one other than committers can cast a meaningful vote, that's the
reality.  Beyond that, if people think it's more open to allow formal
proposals from anyone, I'm not necessarily against it, but my main
question would be this:

If anyone can submit a proposal, are committers actually going to
clearly reject and close proposals that don't meet the requirements?

Right now we have a serious problem with lack of clarity regarding
contributions, and that cannot spill over into goal-setting.

and
d
 I
e:
orward. The
n
l
 values.
st
t
t,
I
e
er
ke
e
y
e
w
g
ed
y
se
n
,
ment-proposals.md
ng
a
st
re
ll
ed
d
""
t
s
y
it
it
in
o
e
ed
e:
ement-proposals.md
n
or
d
ly
re
t
t
]>
n
,
ve
ng
to
.
n
k
to
 a
l
s
t.
ct
m
n
r
is
nd
s
g
ct
am
re
 a
d
in
 a
l""
s.
ee
r
ut
y
--------
on
ement-Proposals-tp19268p19359.html

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 10 Oct 2016 12:02:43 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Sorry, I missed that the proposal includes majority approval. Why majority
instead of consensus? I think we want to build consensus around these
proposals and it makes sense to discuss until no one would veto.

rb


o
d
 I
orward. The
n
l
 values.
t
t,
I
e
er
ke
e
y
e
w
g
ed
y
n
,
ng
a
st
re
ll
ed
d
""
t
s
y
it
in
o
e
ed
e:
n
or
d
ly
re
t
t
]>
n
,
ve
to
.
n
k
to
l
s
t.
ct
m
n
r
nd
s
g
ct
am
re
 a
d
in
 a
l""
s.
ee
r
ut
y
on



-- 
Ryan Blue
Software Engineer
Netflix
"
Cody Koeninger <cody@koeninger.org>,"Mon, 10 Oct 2016 14:07:52 -0500",Re: Spark Improvement Proposals,Ryan Blue <rblue@netflix.com>,"I think this is closer to a procedural issue than a code modification
issue, hence why majority.  If everyone thinks consensus is better, I
don't care.  Again, I don't feel strongly about the way we achieve
clarity, just that we achieve clarity.

y
 and
ed
? I
forward. The
an
al
t values.
e
ct
n
r
t
a
o
e
d
 I
le
ue
t
ly
e
ne
ow
ng
g
,
ly
gn
:
g,
ement-proposals.md
e
 a
n
e
.
s
ld
s""
y
ot
's
gy
o
a
,
to
re
.
s
vement-proposals.md
an
e
n
nd
e
P
ot
ut
y
I
on
.
s,
h
o.
en
nk
e
ll
cs
om
d
in
.
er
is
ng
e
e
t
.
t
n
nd
s
g
s
a
ir
ly
,
---------
vement-Proposals-tp19268p19359.html
n


---------------------------------------------------------------------


"
Ofir Manor <ofir.manor@equalum.io>,"Mon, 10 Oct 2016 22:11:36 +0300",Re: Official Stance on Not Using Spark Submit,"Russell Spitzer <russell.spitzer@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>","Funny, someone from my team talked to me about that idea yesterday.
We use SparkLauncher, but it just calls spark-submit that calls other
scripts that starts a new Java program that tries to submit (in our case in
cluster mode - driver is started in the Spark cluster) and exit.
That make it a challenge to troubleshoot cases where submit fails,
especially when users tries our app on their own spark environment. He
hoped to get a more decent / specific exception if submit failed, or be
able to debug it in an IDE (the actual calling to the master, its response
etc).

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 10 Oct 2016 12:15:08 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Proposal submission: I think we should keep this as open as possible. If
there is a problem with too many open proposals, then we should tackle that
as a fix rather than excluding participation. Perhaps it will end up that
way, but I think it's worth trying a more open model first.

Majority vs consensus: My rationale is that I don't think we want to
consider a proposal approved if it had objections serious enough that
committers down-voted (or PMC depending on who gets a vote). If these
proposals are like PEPs, then they represent a significant amount of
community effort and I wouldn't want to move forward if up to half of the
community thinks it's an untenable idea.

rb

:

n
of
y
y
g
d
s
t
m forward. The
on
he
y
nct
e
e
is
n.
!
,
he
t
g
t
's
e:
'm
s
on
ys
e
en
g
)
ly
ng
g
ve
r
I
on
e
an
e
n
re
t
ut
e
on
n
g
en
s
't
t
d
s
]>
.
ng
e,
t
f
es
ly
s
om
e
]>
d
as
n
e
on
me
s
be
 I
g
ow
ry
o
u
.
g
ur
,
is
,



-- 
Ryan Blue
Software Engineer
Netflix
"
Cody Koeninger <cody@koeninger.org>,"Mon, 10 Oct 2016 14:19:32 -0500",Re: Spark Improvement Proposals,Ryan Blue <rblue@netflix.com>,"That seems reasonable to me.

I do not want to see lazy consensus used on one of these proposals
though, I want a clear outcome, i.e. call for a vote, wait at least 72
hours, get three +1s and no vetos.



at
te:
an
o
ly
l
by
ng
l
ed
k
e
's
at
s
em forward. The
ly
inct
d
he
d
ce
e
o
n!
g,
ut
ng
ot
e
is
t
se
:
r
.
f
ng
d)
d
r
ng
rovement-proposals.md
s
or
d
 I
a
he
e
e
r
se
e
un
s
st
me
on
ng
e
is
provement-proposals.md
e
.
it
ld
l
as
s.
s
l
s
w
k
.
/
st
u
e
of
e
gs
o
he
ed
s
d
]>
in
we
ls
o
.
ng
t
e
g
,
t
I
t
to
ou
t.
ng
e
y,
x,
------------
---
provement-Proposals-tp19268p19359.html
-


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 10 Oct 2016 12:26:24 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Agreed with this. As I said before regarding who submits: it's not a normal ASF process to require contributions to only come from committers. Committers are of course the only people who can *commit* stuff. But the whole point of an open source project is that anyone can *contribute* -- indeed, that is how people become committers. For example, in every ASF project, anyone can open JIRAs, submit design docs, submit patches, review patches, and vote on releases. This particular process is very similar to posting a JIRA or a design doc.

I also like consensus with a deadline (e.g. someone says ""here is a new SEP, we want to accept it by date X so please comment before"").

In general, with this type of stuff, it's better to start with very lightweight processes and then expand them if needed. Adding lots of rules from the beginning makes it confusing and can reduce contributions. Although, as engineers, we believe that anything can be solved using mechanical rules, in practice software development is a social process that ultimately requires humans to tackle things on a case-by-case basis.

Matei


If
tackle that
that
the
modification
I
these
have an
means
vote to
SEP.
formally
if
political
them by
<cody@koeninger.org>
called
proposing
all
agreed
the
lack
with
working
have
be
there's
what
as
them forward. The
leverage
affect
streaming
definitely
calculation
distinct
have
would
the
be
good
Since
we
it
to
implementation!
something,
meeting
but
adding
not
a
agree
whether
sense.
and
this
just
those
they
(or
now'.
accomplish
of
of
clarifying
broad)
will
rejected
for
listing
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
in
(""This
behavior
had
is
file I
a
worked
email]>
the
more
change
wouldn't
we're
unless
better
course
because
begun
SIPs
(just
same
on
trying
approval
for
large
design
is
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
we
outcome.
first
allowed.
limit
should
in
proposal
as
for
adds.
things
will
the
is
slow
Spark
website.
/
first
you
more
of
are
things
to
they're
the
suggested
then
we
shows
and
email]>
clarity
in
startup:
we
posted
to
goals
some
and
no
to
can
world.
volunteering
but
be
working
issue,
it
I
it
to
you
project.
using
Cue
company,
this
(2.x,
---------------------------------------------------------------------
---------------------------------------------------------------------
http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Improvement-Proposals-tp19268p19359.html
at
---------------------------------------------------------------------



---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Mon, 10 Oct 2016 19:28:38 +0000",Re: Spark 2.0.0 job completes but hangs,jamborta <jamborta@gmail.com>,"

Hi all,

I have a spark job that takes about an hour to run, in the end it completes
all the task, then the job just hangs and does nothing (it writes to s3 as
the last step, which also gets completed, all files appear on s3).


You are writing to S3? Is there a lot of data?

Because the file output committers do a rename, which on an object store is a copy operation, and takes time proportional to all the data (~6MB/S in my experiments)

some more details

https://github.com/steveloughran/spark/blob/features/SPARK-7481-cloud/docs/cloud-integration.md

and some settings which may speed things up
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
spark.hadoop.mapreduce.fileoutputcommitter.cleanup.skipped true

I am about to do some work paralellising the rename, which should make a difference, especially if you have some very large files which can be copied in parallel

https://issues.apache.org/jira/browse/HADOOP-13600

any ideas how to debug this?

see the thread dump below:

""Attach Listener"" daemon prio=10 tid=0x00007f67a8001000 nid=0x7b90 waiting
on condition [0x0000000000000000]
  java.lang.Thread.State: RUNNABLE

""SparkUI-224"" daemon prio=10 tid=0x00007f6778001000 nid=0x7b4a waiting on
condition [0x00007f67c2cfa000]
  java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
at java.lang.Thread.run(Thread.java:745)

""SparkUI-223"" daemon prio=10 tid=0x00007f671008e000 nid=0x7b49 waiting on
condition [0x00007f67d1b4d000]
  java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
at java.lang.Thread.run(Thread.java:745)

""SparkUI-222"" daemon prio=10 tid=0x00007f677c006800 nid=0x7b48 waiting on
condition [0x00007f67c8e33000]
  java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
at java.lang.Thread.run(Thread.java:745)

""DestroyJavaVM"" prio=10 tid=0x00007f67ed157000 nid=0x23c9 waiting on
condition [0x0000000000000000]
  java.lang.Thread.State: RUNNABLE

""SparkUI-205"" daemon prio=10 tid=0x00007f6718002000 nid=0x7aa0 waiting on
condition [0x00007f67b82ee000]
  java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c77cc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
at
org.spark_project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:531)
at
org.spark_project.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:47)
at
org.spark_project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:590)
at java.lang.Thread.run(Thread.java:745)

""Scheduler-916842649"" prio=10 tid=0x00007f672c004000 nid=0x7a9e waiting on
condition [0x00007f67b89f5000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c78ee520> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-7"" daemon prio=10 tid=0x00007f66e4007800 nid=0x61cf runnable
[0x00007f67c8429000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c7bdd8e0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c7bdd940> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c7bdd838> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""task-result-getter-3"" daemon prio=10 tid=0x00007f676c009000 nid=0x24b6
waiting on condition [0x00007f67b8bf7000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""task-result-getter-2"" daemon prio=10 tid=0x00007f6764007000 nid=0x24b5
waiting on condition [0x00007f67b8cf8000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""task-result-getter-1"" daemon prio=10 tid=0x00007f675c004000 nid=0x24b4
waiting on condition [0x00007f67b8df9000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""task-result-getter-0"" daemon prio=10 tid=0x00007f6768010000 nid=0x24b3
waiting on condition [0x00007f67c862b000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c78ef7d0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-6"" daemon prio=10 tid=0x00007f66e4011800 nid=0x24b2 runnable
[0x00007f66cbef9000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8de9f60> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c8de9f50> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8de9f80> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-5"" daemon prio=10 tid=0x00007f66e4010000 nid=0x24b1 runnable
[0x00007f67b8efa000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c7bde078> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c7bde0d8> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c7bddfe0> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-4"" daemon prio=10 tid=0x00007f66e400c800 nid=0x24b0 runnable
[0x00007f66cdffc000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8e06ae8> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c8e06ad8> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8e06b08> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-3"" daemon prio=10 tid=0x00007f66e400b000 nid=0x24af runnable
[0x00007f67b8ffb000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8e0d190> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c8e0d180> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8e0d1b0> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-2"" daemon prio=10 tid=0x00007f66e4008800 nid=0x24ae runnable
[0x00007f67c01e2000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8e49100> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c8e490f0> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8e49120> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" daemon prio=10 tid=0x00007f66e400a000 nid=0x24ad runnable
[0x00007f67d1f8e000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8d31698> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c8d316f8> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8d31600> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""java-sdk-http-connection-reaper"" daemon prio=10 tid=0x000000000256f000
nid=0x24ac waiting on condition [0x00007f67c04f0000]
  java.lang.Thread.State: TIMED_WAITING (sleeping)
at java.lang.Thread.sleep(Native Method)
at
com.amazonaws.http.IdleConnectionReaper.run(IdleConnectionReaper.java:112)

""shuffle-server-7"" daemon prio=10 tid=0x00007f674800d000 nid=0x24ab runnable
[0x00007f67c25f3000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c6603918> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c66049d8> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c6603870> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-6"" daemon prio=10 tid=0x00007f674800b000 nid=0x24aa runnable
[0x00007f67c26f4000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c78aa328> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c78aa348> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c78aa2e0> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-5"" daemon prio=10 tid=0x00007f6748009000 nid=0x24a9 runnable
[0x00007f67c27f5000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8ccfca0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c8cd0d70> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8ccfbf8> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-4"" daemon prio=10 tid=0x00007f6748007000 nid=0x24a8 runnable
[0x00007f67c28f6000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c54e4cf0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c54e4db0> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c54e4c48> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-3"" daemon prio=10 tid=0x00007f6748005000 nid=0x24a7 runnable
[0x00007f67c29f7000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c65dae00> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c65de9d8> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c65dad58> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-2"" daemon prio=10 tid=0x00007f6748003800 nid=0x24a6 runnable
[0x00007f67c2af8000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8ccfa18> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c77ce728> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8ccf970> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""shuffle-server-1"" daemon prio=10 tid=0x00007f6748001800 nid=0x24a5 runnable
[0x00007f67c2bf9000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c54bf8c0> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c54c3fb0> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c54bf818> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""SparkListenerBus"" daemon prio=10 tid=0x00007f6770e64800 nid=0x24a2 waiting
on condition [0x00007f67c2dfb000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c7a97800> (a
java.util.concurrent.Semaphore$NonfairSync)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:67)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:65)
at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1229)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:64)

""context-cleaner-periodic-gc"" daemon prio=10 tid=0x00007f6770e61000
nid=0x24a1 waiting on condition [0x00007f67c2efc000]
  java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c78f33e8> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""Spark Context Cleaner"" daemon prio=10 tid=0x00007f6770e5f000 nid=0x24a0 in
Object.wait() [0x00007f67c2ffd000]
  java.lang.Thread.State: TIMED_WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
- waiting on <0x00000000c78caac0> (a java.lang.ref.ReferenceQueue$Lock)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
- locked <0x00000000c78caac0> (a java.lang.ref.ReferenceQueue$Lock)
at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:175)
at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1229)
at
org.apache.spark.ContextCleaner.org<http://org.apache.spark.ContextCleaner.org>$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:172)
at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:67)

""shuffle-server-0"" daemon prio=10 tid=0x00007f6770bf9800 nid=0x249f runnable
[0x00007f67c8126000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c8d55b88> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c8d5ce98> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c8d55af0> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""threadDeathWatcher-2-1"" daemon prio=10 tid=0x00007f66ec00f800 nid=0x249e
waiting on condition [0x00007f67c8227000]
  java.lang.Thread.State: TIMED_WAITING (sleeping)
at java.lang.Thread.sleep(Native Method)
at
io.netty.util.ThreadDeathWatcher$Watcher.run(ThreadDeathWatcher.java:137)
at
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
at java.lang.Thread.run(Thread.java:745)

""shuffle-client-0"" daemon prio=10 tid=0x00007f66e8003800 nid=0x249d runnable
[0x00007f67c8328000]
  java.lang.Thread.State: RUNNABLE
at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
- locked <0x00000000c31e90a8> (a
io.netty.channel.nio.SelectedSelectionKeySet)
- locked <0x00000000c31e90c8> (a java.util.Collections$UnmodifiableSet)
- locked <0x00000000c31e9060> (a sun.nio.ch.EPollSelectorImpl)
at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
at java.lang.Thread.run(Thread.java:745)

""appclient-registration-retry-thread"" daemon prio=10 tid=0x00007f6760003800
nid=0x249b waiting on condition [0x00007f67c852a000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c3207ec0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""driver-revive-thread"" daemon prio=10 tid=0x00007f676800d000 nid=0x2499
waiting on condition [0x00007f67c872c000]
  java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c8d90dd0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""dag-scheduler-event-loop"" daemon prio=10 tid=0x00007f6770be7000 nid=0x2498
waiting on condition [0x00007f67c882d000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c8d897c8> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at
java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:489)
at
java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:678)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:46)

""netty-rpc-env-timeout"" daemon prio=10 tid=0x00007f670c00c800 nid=0x2497
waiting on condition [0x00007f67c892e000]
  java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000000c32087c0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
at
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
at
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

""Timer-0"" daemon prio=10 tid=0x00007f6770bb1800 nid=0x2496 in Object.wait()
[0x00007f67c8a2f000]
  java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
- waiting on <0x00000000c8d884f8> (a java.util.TaskQueue)
at java.lang.Object.wait(Object.java:503)
at java.util.TimerThread.mainLoop(Timer.java:526)
- locked <0x00000000c8d884f8> (a java.util.TaskQueue)
at java.util.TimerThread.run(Timer.java:505)

""heartbeat-receiver-event-loop-thread"" daemon prio=10 tid=0x00007f675c001800
nid=0x2495 waiting on condition [0x00007f67c8b30000]
  java.l"
"
Steve Loughran <stevel@hortonworks.com>,Mon"," 10 Oct 2016 19:38:03 +0000""",Re: Spark Improvement Proposals,Matei Zaharia <matei.zaharia@gmail.com>,"This is an interesting process proposal; I think it could work well.

-It's got the flavour of the ASF incubator; maybe some of the processes there: mentor, regular reporting in could help, in particular, help stop the -1 at the end of the work
-it may also aid collaboration to have a medium lived branch, so enabling collaboration with multiple people submitting PRs into the ASF codebase. This can reduce cost of merge and enable jenkins to keep on top of it. It also fits in well with the ASF ""do in apache infra"" community development process.


> On 10 Oct 2016, at 20:26, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> 
> Agreed with this. As I said before regarding who submits: it's not a normal ASF process to require contributions to only come from committers. Committers are of course the only people who can *commit* stuff. But the whole point of an open source project is that anyone can *contribute* -- indeed, that is how people become committers. For example, in every ASF project, anyone can open JIRAs, submit design docs, submit patches, review patches, and vote on releases. This particular process is very similar to posting a JIRA or a design doc.
> 
> I also like consensus with a deadline (e.g. someone says ""here is a new SEP, we want to accept it by date X so please comment before"").
> 
> In general, with this type of stuff, it's better to start with very lightweight processes and then expand them if needed. Adding lots of rules from the beginning makes it confusing and can reduce contributions. Although, as engineers, we believe that anything can be solved using mechanical rules, in practice software development is a social process that ultimately requires humans to tackle things on a case-by-case basis.
> 
> Matei
> 
> 
>> On Oct 10, 2016, at 12:19 PM, Cody Koeninger <cody@koeninger.org> wrote:
>> 
>> That seems reasonable to me.
>> 
>> I do not want to see lazy consensus used on one of these proposals
>> though, I want a clear outcome, i.e. call for a vote, wait at least 72
>> hours, get three +1s and no vetos.
>> 
>> 
>> 
>> On Mon, Oct 10, 2016 at 2:15 PM, Ryan Blue <rblue@netflix.com> wrote:
>>> Proposal submission: I think we should keep this as open as possible. If
>>> there is a problem with too many open proposals, then we should tackle that
>>> as a fix rather than excluding participation. Perhaps it will end up that
>>> way, but I think it's worth trying a more open model first.
>>> 
>>> Majority vs consensus: My rationale is that I don't think we want to
>>> consider a proposal approved if it had objections serious enough that
>>> committers down-voted (or PMC depending on who gets a vote). If these
>>> proposals are like PEPs, then they represent a significant amount of
>>> community effort and I wouldn't want to move forward if up to half of the
>>> community thinks it's an untenable idea.
>>> 
>>> rb
>>> 
>>> On Mon, Oct 10, 2016 at 12:07 PM, Cody Koeninger <cody@koeninger.org> wrote:
>>>> 
>>>> I think this is closer to a procedural issue than a code modification
>>>> issue, hence why majority.  If everyone thinks consensus is better, I
>>>> don't care.  Again, I don't feel strongly about the way we achieve
>>>> clarity, just that we achieve clarity.
>>>> 
>>>> On Mon, Oct 10, 2016 at 2:02 PM, Ryan Blue <rblue@netflix.com> wrote:
>>>>> Sorry, I missed that the proposal includes majority approval. Why
>>>>> majority
>>>>> instead of consensus? I think we want to build consensus around these
>>>>> proposals and it makes sense to discuss until no one would veto.
>>>>> 
>>>>> rb
>>>>> 
>>>>> On Mon, Oct 10, 2016 at 11:54 AM, Ryan Blue <rblue@netflix.com> wrote:
>>>>>> 
>>>>>> +1 to votes to approve proposals. I agree that proposals should have an
>>>>>> official mechanism to be accepted, and a vote is an established means
>>>>>> of
>>>>>> doing that well. I like that it includes a period to review the
>>>>>> proposal and
>>>>>> I think proposals should have been discussed enough ahead of a vote to
>>>>>> survive the possibility of a veto.
>>>>>> 
>>>>>> I also like the names that are short and (mostly) unique, like SEP.
>>>>>> 
>>>>>> Where I disagree is with the requirement that a committer must formally
>>>>>> propose an enhancement. I don't see the value of restricting this: if
>>>>>> someone has the will to write up a proposal then they should be
>>>>>> encouraged
>>>>>> to do so and start a discussion about it. Even if there is a political
>>>>>> reality as Cody says, what is the value of codifying that in our
>>>>>> process? I
>>>>>> think restricting who can submit proposals would only undermine them by
>>>>>> pushing contributors out. Maybe I'm missing something here?
>>>>>> 
>>>>>> rb
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> On Mon, Oct 10, 2016 at 7:41 AM, Cody Koeninger <cody@koeninger.org>
>>>>>> wrote:
>>>>>>> 
>>>>>>> Yes, users suggesting SIPs is a good thing and is explicitly called
>>>>>>> out in the linked document under the Who? section.  Formally proposing
>>>>>>> them, not so much, because of the political realities.
>>>>>>> 
>>>>>>> Yes, implementation strategy definitely affects goals.  There are all
>>>>>>> kinds of examples of this, I'll pick one that's my fault so as to
>>>>>>> avoid sounding like I'm blaming:
>>>>>>> 
>>>>>>> When I implemented the Kafka DStream, one of my (not explicitly agreed
>>>>>>> upon by the community) goals was to make sure people could use the
>>>>>>> Dstream with however they were already using Kafka at work.  The lack
>>>>>>> of explicit agreement on that goal led to all kinds of fighting with
>>>>>>> committers, that could have been avoided.  The lack of explicit
>>>>>>> up-front strategy discussion led to the DStream not really working
>>>>>>> with compacted topics.  I knew about compacted topics, but don't have
>>>>>>> a use for them, so had a blind spot there.  If there was explicit
>>>>>>> up-front discussion that my strategy was ""assume that batches can be
>>>>>>> defined on the driver solely by beginning and ending offsets"", there's
>>>>>>> a greater chance that a user would have seen that and said, ""hey, what
>>>>>>> about non-contiguous offsets in a compacted topic"".
>>>>>>> 
>>>>>>> This kind of thing is only going to happen smoothly if we have a
>>>>>>> lightweight user-visible process with clear outcomes.
>>>>>>> 
>>>>>>> On Mon, Oct 10, 2016 at 1:34 AM, assaf.mendelson
>>>>>>> <assaf.mendelson@rsa.com> wrote:
>>>>>>>> I agree with most of what Cody said.
>>>>>>>> 
>>>>>>>> Two things:
>>>>>>>> 
>>>>>>>> First we can always have other people suggest SIPs but mark them as
>>>>>>>> â€œunreviewedâ€ and have committers basically move them forward. The
>>>>>>>> problem is
>>>>>>>> that writing a good document takes time. This way we can leverage
>>>>>>>> non
>>>>>>>> committers to do some of this work (it is just another way to
>>>>>>>> contribute).
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> As for strategy, in many cases implementation strategy can affect
>>>>>>>> the
>>>>>>>> goals.
>>>>>>>> I will give  a small example: In the current structured streaming
>>>>>>>> strategy,
>>>>>>>> we group by the time to achieve a sliding window. This is definitely
>>>>>>>> an
>>>>>>>> implementation decision and not a goal. However, I can think of
>>>>>>>> several
>>>>>>>> aggregation functions which have the time inside their calculation
>>>>>>>> buffer.
>>>>>>>> For example, letâ€™s say we want to return a set of all distinct
>>>>>>>> values.
>>>>>>>> One
>>>>>>>> way to implement this would be to make the set into a map and have
>>>>>>>> the
>>>>>>>> value
>>>>>>>> contain the last time seen. Multiplying it across the groupby would
>>>>>>>> cost a
>>>>>>>> lot in performance. So adding such a strategy would have a great
>>>>>>>> effect
>>>>>>>> on
>>>>>>>> the type of aggregations and their performance which does affect the
>>>>>>>> goal.
>>>>>>>> Without adding the strategy, it is easy for whoever goes to the
>>>>>>>> design
>>>>>>>> document to not think about these cases. Furthermore, it might be
>>>>>>>> decided
>>>>>>>> that these cases are rare enough so that the strategy is still good
>>>>>>>> enough
>>>>>>>> but how would we know it without user feedback?
>>>>>>>> 
>>>>>>>> I believe this example is exactly what Cody was talking about. Since
>>>>>>>> many
>>>>>>>> times implementation strategies have a large effect on the goal, we
>>>>>>>> should
>>>>>>>> have it discussed when discussing the goals. In addition, while it
>>>>>>>> is
>>>>>>>> often
>>>>>>>> easy to throw out completely infeasible goals, it is often much
>>>>>>>> harder
>>>>>>>> to
>>>>>>>> figure out that the goals are unfeasible without fine tuning.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Assaf.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> From: Cody Koeninger-2 [via Apache Spark Developers List]
>>>>>>>> [mailto:ml-node+[hidden email]]
>>>>>>>> Sent: Monday, October 10, 2016 2:25 AM
>>>>>>>> To: Mendelson, Assaf
>>>>>>>> Subject: Re: Spark Improvement Proposals
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Only committers should formally submit SIPs because in an apache
>>>>>>>> project only commiters have explicit political power.  If a user
>>>>>>>> can't
>>>>>>>> find a commiter willing to sponsor an SIP idea, they have no way to
>>>>>>>> get the idea passed in any case.  If I can't find a committer to
>>>>>>>> sponsor this meta-SIP idea, I'm out of luck.
>>>>>>>> 
>>>>>>>> I do not believe unrealistic goals can be found solely by
>>>>>>>> inspection.
>>>>>>>> We've managed to ignore unrealistic goals even after implementation!
>>>>>>>> Focusing on APIs can allow people to think they've solved something,
>>>>>>>> when there's really no way of implementing that API while meeting
>>>>>>>> the
>>>>>>>> goals.  Rapid iteration is clearly the best way to address this, but
>>>>>>>> we've already talked about why that hasn't really worked.  If adding
>>>>>>>> a
>>>>>>>> non-binding API section to the template is important to you, I'm not
>>>>>>>> against it, but I don't think it's sufficient.
>>>>>>>> 
>>>>>>>> On your PRD vs design doc spectrum, I'm saying this is closer to a
>>>>>>>> PRD.  Clear agreement on goals is the most important thing and
>>>>>>>> that's
>>>>>>>> why it's the thing I want binding agreement on.  But I cannot agree
>>>>>>>> to
>>>>>>>> goals unless I have enough minimal technical info to judge whether
>>>>>>>> the
>>>>>>>> goals are likely to actually be accomplished.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> On Sun, Oct 9, 2016 at 5:35 PM, Matei Zaharia <[hidden email]>
>>>>>>>> wrote:
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> Well, I think there are a few things here that don't make sense.
>>>>>>>>> First,
>>>>>>>>> why
>>>>>>>>> should only committers submit SIPs? Development in the project
>>>>>>>>> should
>>>>>>>>> be
>>>>>>>>> open to all contributors, whether they're committers or not.
>>>>>>>>> Second, I
>>>>>>>>> think
>>>>>>>>> unrealistic goals can be found just by inspecting the goals, and
>>>>>>>>> I'm
>>>>>>>>> not
>>>>>>>>> super worried that we'll accept a lot of SIPs that are then
>>>>>>>>> infeasible
>>>>>>>>> --
>>>>>>>>> we
>>>>>>>>> can then submit new ones. But this depends on whether you want this
>>>>>>>>> process
>>>>>>>>> to be a ""design doc lite"", where people also agree on
>>>>>>>>> implementation
>>>>>>>>> strategy, or just a way to agree on goals. This is what I asked
>>>>>>>>> earlier
>>>>>>>>> about PRDs vs design docs (and I'm open to either one but I'd just
>>>>>>>>> like
>>>>>>>>> clarity). Finally, both as a user and designer of software, I
>>>>>>>>> always
>>>>>>>>> want
>>>>>>>>> to
>>>>>>>>> give feedback on APIs, so I'd really like a culture of having those
>>>>>>>>> early.
>>>>>>>>> People don't argue about prettiness when they discuss APIs, they
>>>>>>>>> argue
>>>>>>>>> about
>>>>>>>>> the core concepts to expose in order to meet various goals, and
>>>>>>>>> then
>>>>>>>>> they're
>>>>>>>>> stuck maintaining those for a long time.
>>>>>>>>> 
>>>>>>>>> Matei
>>>>>>>>> 
>>>>>>>>> On Oct 9, 2016, at 3:10 PM, Cody Koeninger <[hidden email]> wrote:
>>>>>>>>> 
>>>>>>>>> Users instead of people, sure.  Commiters and contributors are (or
>>>>>>>>> at
>>>>>>>>> least
>>>>>>>>> should be) a subset of users.
>>>>>>>>> 
>>>>>>>>> Non goals, sure. I don't care what the name is, but we need to
>>>>>>>>> clearly
>>>>>>>>> say
>>>>>>>>> e.g. 'no we are not maintaining compatibility with XYZ right now'.
>>>>>>>>> 
>>>>>>>>> API, what I care most about is whether it allows me to accomplish
>>>>>>>>> the
>>>>>>>>> goals.
>>>>>>>>> Arguing about how ugly or pretty it is can be saved for design/
>>>>>>>>> implementation imho.
>>>>>>>>> 
>>>>>>>>> Strategy, this is necessary because otherwise goals can be out of
>>>>>>>>> line
>>>>>>>>> with
>>>>>>>>> reality.  Don't propose goals you don't have at least some idea of
>>>>>>>>> how
>>>>>>>>> to
>>>>>>>>> implement.
>>>>>>>>> 
>>>>>>>>> Rejected strategies, given that commiters are the only ones I'm
>>>>>>>>> saying
>>>>>>>>> should formally submit SPARKLIs or SIPs, if they put junk in a
>>>>>>>>> required
>>>>>>>>> section then slap them down for it and tell them to fix it.
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> On Oct 9, 2016 4:36 PM, ""Matei Zaharia"" <[hidden email]> wrote:
>>>>>>>>>> 
>>>>>>>>>> Yup, this is the stuff that I found unclear. Thanks for clarifying
>>>>>>>>>> here,
>>>>>>>>>> but we should also clarify it in the writeup. In particular:
>>>>>>>>>> 
>>>>>>>>>> - Goals needs to be about user-facing behavior (""people"" is broad)
>>>>>>>>>> 
>>>>>>>>>> - I'd rename Rejected Goals to Non-Goals. Otherwise someone will
>>>>>>>>>> dig
>>>>>>>>>> up
>>>>>>>>>> one of these and say ""Spark's developers have officially rejected
>>>>>>>>>> X,
>>>>>>>>>> which
>>>>>>>>>> our awesome system has"".
>>>>>>>>>> 
>>>>>>>>>> - For user-facing stuff, I think you need a section on API.
>>>>>>>>>> Virtually
>>>>>>>>>> all
>>>>>>>>>> other *IPs I've seen have that.
>>>>>>>>>> 
>>>>>>>>>> - I'm still not sure why the strategy section is needed if the
>>>>>>>>>> purpose is
>>>>>>>>>> to define user-facing behavior -- unless this is the strategy for
>>>>>>>>>> setting
>>>>>>>>>> the goals or for defining the API. That sounds squarely like a
>>>>>>>>>> design
>>>>>>>>>> doc
>>>>>>>>>> issue. In some sense, who cares whether the proposal is
>>>>>>>>>> technically
>>>>>>>>>> feasible
>>>>>>>>>> right now? If it's infeasible, that will be discovered later
>>>>>>>>>> during
>>>>>>>>>> design
>>>>>>>>>> and implementation. Same thing with rejected strategies -- listing
>>>>>>>>>> some
>>>>>>>>>> of
>>>>>>>>>> those is definitely useful sometimes, but if you make this a
>>>>>>>>>> *required*
>>>>>>>>>> section, people are just going to fill it in with bogus stuff
>>>>>>>>>> (I've
>>>>>>>>>> seen
>>>>>>>>>> this happen before).
>>>>>>>>>> 
>>>>>>>>>> Matei
>>>>>>>>>> 
>>>>>>>> 
>>>>>>>>>>> On Oct 9, 2016, at 2:14 PM, Cody Koeninger <[hidden email]>
>>>>>>>>>>> wrote:
>>>>>>>>>>> 
>>>>>>>>>>> So to focus the discussion on the specific strategy I'm
>>>>>>>>>>> suggesting,
>>>>>>>>>>> documented at
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
>>>>>>>>>>> 
>>>>>>>>>>> ""Goals: What must this allow people to do, that they can't
>>>>>>>>>>> currently?""
>>>>>>>>>>> 
>>>>>>>>>>> Is it unclear that this is focusing specifically on
>>>>>>>>>>> people-visible
>>>>>>>>>>> behavior?
>>>>>>>>>>> 
>>>>>>>>>>> Rejected goals -  are important because otherwise people keep
>>>>>>>>>>> trying
>>>>>>>>>>> to argue about scope.  Of course you can change things later
>>>>>>>>>>> with a
>>>>>>>>>>> different SIP and different vote, the point is to focus.
>>>>>>>>>>> 
>>>>>>>>>>> Use cases - are something that people are going to bring up in
>>>>>>>>>>> discussion.  If they aren't clearly documented as a goal (""This
>>>>>>>>>>> must
>>>>>>>>>>> allow me to connect using SSL""), they should be added.
>>>>>>>>>>> 
>>>>>>>>>>> Internal architecture - if the people who need specific behavior
>>>>>>>>>>> are
>>>>>>>>>>> implementers of other parts of the system, that's fine.
>>>>>>>>>>> 
>>>>>>>>>>> Rejected strategies - If you have none of these, you have no
>>>>>>>>>>> evidence
>>>>>>>>>>> that the proponent didn't just go with the first thing they had
>>>>>>>>>>> in
>>>>>>>>>>> mind (or have already implemented), which is a big problem
>>>>>>>>>>> currently.
>>>>>>>>>>> Approval isn't binding as to specifics of implementation, so
>>>>>>>>>>> these
>>>>>>>>>>> aren't handcuffs.  The goals are the contract, the strategy is
>>>>>>>>>>> evidence that contract can actually be met.
>>>>>>>>>>> 
>>>>>>>>>>> Design docs - I'm not touching design docs.  The markdown file I
>>>>>>>>>>> linked specifically says of the strategy section ""This is not a
>>>>>>>>>>> full
>>>>>>>>>>> design document.""  Is this unclear?  Design docs can be worked
>>>>>>>>>>> on
>>>>>>>>>>> obviously, but that's not what I'm concerned with here.
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>>>> On Sun, Oct 9, 2016 at 2:34 PM, Matei Zaharia <[hidden email]>
>>>>>>>>>>> wrote:
>>>>>>>>>>>> Hi Cody,
>>>>>>>>>>>> 
>>>>>>>>>>>> I think this would be a lot more concrete if we had a more
>>>>>>>>>>>> detailed
>>>>>>>>>>>> template
>>>>>>>>>>>> for SIPs. Right now, it's not super clear what's in scope --
>>>>>>>>>>>> e.g.
>>>>>>>>>>>> are
>>>>>>>>>>>> they
>>>>>>>>>>>> a way to solicit feedback on the user-facing behavior or on the
>>>>>>>>>>>> internals?
>>>>>>>>>>>> ""Goals"" can cover both things. I've been thinking of SIPs more
>>>>>>>>>>>> as
>>>>>>>>>>>> Product
>>>>>>>>>>>> Requirements Docs (PRDs), which focus on *what* a code change
>>>>>>>>>>>> should
>>>>>>>>>>>> do
>>>>>>>>>>>> as
>>>>>>>>>>>> opposed to how.
>>>>>>>>>>>> 
>>>>>>>>>>>> In particular, here are some things that you may or may not
>>>>>>>>>>>> consider
>>>>>>>>>>>> in
>>>>>>>>>>>> scope for SIPs:
>>>>>>>>>>>> 
>>>>>>>>>>>> - Goals and non-goals: This is definitely in scope, and IMO
>>>>>>>>>>>> should
>>>>>>>>>>>> focus on
>>>>>>>>>>>> user-visible behavior (e.g. ""system supports SQL window
>>>>>>>>>>>> functions""
>>>>>>>>>>>> or
>>>>>>>>>>>> ""system continues working if one node fails""). BTW I wouldn't
>>>>>>>>>>>> say
>>>>>>>>>>>> ""rejected
>>>>>>>>>>>> goals"" because some of them might become goals later, so we're
>>>>>>>>>>>> not
>>>>>>>>>>>> definitively rejecting them.
>>>>>>>>>>>> 
>>>>>>>>>>>> - Public API: Probably should be included in most SIPs unless
>>>>>>>>>>>> it's
>>>>>>>>>>>> too
>>>>>>>>>>>> large
>>>>>>>>>>>> to fully specify then (e.g. ""let's add an ML library"").
>>>>>>>>>>>> 
>>>>>>>>>>>> - Use cases: I usually find this very useful in PRDs to better
>>>>>>>>>>>> communicate
>>>>>>>>>>>> the goals.
>>>>>>>>>>>> 
>>>>>>>>>>>> - Internal architecture: This is usually *not* a thing users
>>>>>>>>>>>> can
>>>>>>>>>>>> easily
>>>>>>>>>>>> comment on and it sounds more like a design doc item. Of course
>>>>>>>>>>>> it's
>>>>>>>>>>>> important to show that the SIP is feasible to implement. One
>>>>>>>>>>>> exception,
>>>>>>>>>>>> however, is that I think we'll have some SIPs primarily on
>>>>>>>>>>>> internals
>>>>>>>>>>>> (e.g.
>>>>>>>>>>>> if somebody wants to refactor Spark's query optimizer or
>>>>>>>>>>>> something).
>>>>>>>>>>>> 
>>>>>>>>>>>> - Rejected strategies: I personally wouldn't put this, because
>>>>>>>>>>>> what's
>>>>>>>>>>>> the
>>>>>>>>>>>> point of voting to reject a strategy before you've really begun
>>>>>>>>>>>> designing
>>>>>>>>>>>> and implementing something? What if you discover that the
>>>>>>>>>>>> strategy
>>>>>>>>>>>> is
>>>>>>>>>>>> actually better when you start doing stuff?
>>>>>>>>>>>> 
>>>>>>>>>>>> At a super high level, it depends on whether you want the SIPs
>>>>>>>>>>>> to
>>>>>>>>>>>> be
>>>>>>>>>>>> PRDs
>>>>>>>>>>>> for getting some quick feedback on the goals of a feature
>>>>>>>>>>>> before
>>>>>>>>>>>> it is
>>>>>>>>>>>> designed, or something more like full-fledged design docs (just
>>>>>>>>>>>> a
>>>>>>>>>>>> more
>>>>>>>>>>>> visible design doc for bigger changes). I looked at Kafka's
>>>>>>>>>>>> KIPs,
>>>>>>>>>>>> and
>>>>>>>>>>>> they
>>>>>>>>>>>> actually seem to be more like design docs. This can work too
>>>>>>>>>>>> but
>>>>>>>>>>>> it
>>>>>>>>>>>> does
>>>>>>>>>>>> require more work from the proposer and it can lead to the same
>>>>>>>>>>>> problems you
>>>>>>>>>>>> mentioned with people already having a design and
>>>>>>>>>>>> implementation
>>>>>>>>>>>> in
>>>>>>>>>>>> mind.
>>>>>>>>>>>> 
>>>>>>>>>>>> Basically, the question is, are you trying to iterate faster on
>>>>>>>>>>>> design
>>>>>>>>>>>> by
>>>>>>>>>>>> adding a step for user feedback earlier? Or are you just trying
>>>>>>>>>>>> to
>>>>>>>>>>>> make
>>>>>>>>>>>> design docs for key features more visible (and their approval
>>>>>>>>>>>> more
>>>>>>>>>>>> formal)?
>>>>>>>>>>>> 
>>>>>>>>>>>> BTW note that in either case, I'd like to have a template for
>>>>>>>>>>>> design
>>>>>>>>>>>> docs
>>>>>>>>>>>> too, which should also include goals. I think that would've
>>>>>>>>>>>> avoided
>>>>>>>>>>>> some of
>>>>>>>>>>>> the issues you brought up.
>>>>>>>>>>>> 
>>>>>>>>>>>> Matei
>>>>>>>>>>>> 
>>>>>>>>>>>> On Oct 9, 2016, at 10:40 AM, Cody Koeninger <[hidden email]>
>>>>>>>>>>>> wrote:
>>>>>>>>>>>> 
>>>>>>>>>>>> Here's my specific proposal (meta-proposal?)
>>>>>>>>>>>> 
>>>>>>>>>>>> Spark Improvement Proposals (SIP)
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> Background:
>>>>>>>>>>>> 
>>>>>>>>>>>> The current problem is that design and implementation of large
>>>>>>>>>>>> features
>>>>>>>>>>>> are
>>>>>>>>>>>> often done in private, before soliciting user feedback.
>>>>>>>>>>>> 
>>>>>>>>>>>> When feedback is solicited, it is often as to detailed design
>>>>>>>>>>>> specifics, not
>>>>>>>>>>>> focused on goals.
>>>>>>>>>>>> 
>>>>>>>>>>>> When implementation does take place after design, there is
>>>>>>>>>>>> often
>>>>>>>>>>>> disagreement as to what goals are or are not in scope.
>>>>>>>>>>>> 
>>>>>>>>>>>> This results in commits that don't fully meet user needs.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> Goals:
>>>>>>>>>>>> 
>>>>>>>>>>>> - Ensure user, contributor, and committer goals are clearly
>>>>>>>>>>>> identified
>>>>>>>>>>>> and
>>>>>>>>>>>> agreed upon, before implementation takes place.
>>>>>>>>>>>> 
>>>>>>>>>>>> - Ensure that a technically feasible strategy is chosen that is
>>>>>>>>>>>> likely
>>>>>>>>>>>> to
>>>>>>>>>>>> meet the goals.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> Rejected Goals:
>>>>>>>>>>>> 
>>>>>>>>>>>> - SIPs are not for detailed design.  Design by committee
>>>>>>>>>>>> doesn't
>>>>>>>>>>>> work.
>>>>>>>>>>>> 
>>>>>>>>>>>> - SIPs are not for every change.  We dont need that much
>>>>>>>>>>>> process.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> Strategy:
>>>>>>>>>>>> 
>>>>>>>>>>>> My suggestion is outlined as a Spark Improvement Proposal
>>>>>>>>>>>> process
>>>>>>>>>>>> documented
>>>>>>>>>>>> at
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
>>>>>>>>>>>> 
>>>>>>>>>>>> Specifics of Jira manipulation are an implementation detail we
>>>>>>>>>>>> can
>>>>>>>>>>>> figure
>>>>>>>>>>>> out.
>>>>>>>>>>>> 
>>>>>>>>>>>> I'm suggesting voting; the need here is for a _clear_ outcome.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> Rejected Strategies:
>>>>>>>>>>>> 
>>>>>>>>>>>> Having someone who understands the problem implement it first
>>>>>>>>>>>> works,
>>>>>>>>>>>> but
>>>>>>>>>>>> only if significant iteration after user feedback is allowed.
>>>>>>>>>>>> 
>>>>>>>>>>>> Historically this has been problematic due to pressure to limit
>>>>>>>>>>>> public
>>>>>>>>>>>> api
>>>>>>>>>>>> changes.
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> On Fri, Oct 7, 2016 at 5:16 PM, Reynold Xin <[hidden email]>
>>>>>>>>>>>> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Alright looks like there are quite a bit of support. We should
>>>>>>>>>>>>> wait
>>>>>>>>>>>>> to
>>>>>>>>>>>>> hear from more people too.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> To push this forward, Cody and I will be working together in
>>>>>>>>>>>>> the
>>>>>>>>>>>>> next
>>>>>>>>>>>>> couple of weeks to come up with a concrete, detailed proposal
>>>>>>>>>>>>> on
>>>>>>>>>>>>> what
>>>>>>>>>>>>> this
>>>>>>>>>>>>> entails, and then we can discuss this the specific proposal as
>>>>>>>>>>>>> well.
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> On Fri, Oct 7, 2016 at 2:29 PM, Cody Koeninger <[hidden
>>>>>>>>>>>>> email]>
>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> Yeah, in case it wasn't clear, I was talking about SIPs for
>>>>>>>>>>>>>> major
>>>>>>>>>>>>>> user-facing or cross-cutting changes, not minor feature adds.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> On Fri, Oct 7, 2016 at 3:58 PM, Stavros Kontopoulos
>>>>>>>>>>>>>> <[hidden email]> wrote:
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> +1 to the SIP label as long as it does not slow down things
>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>> it
>>>>>>>>>>>>>>> targets optimizing efforts, coordination etc. For example
>>>>>>>>>>>>>>> really
>>>>>>>>>>>>>>> small
>>>>>>>>>>>>>>> features should not need to go through this process
>>>>>>>>>>>>>>> (assuming
>>>>>>>>>>>>>>> they
>>>>>>>>>>>>>>> dont
>>>>>>>>>>>>>>> touch public interfaces)  or re-factorings and hope it will
>>>>>>>>>>>>>>> be
>>>>>>>>>>>>>>> kept
>>>>>>>>>>>>>>> this
>>>>>>>>>>>>>>> way. So as a guideline doc should be provided, like in the
>>>>>>>>>>>>>>> KIP
>>>>>>>>>>>>>>> case.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> IMHO so far aside from tagging things and linking them
>>>>>>>>>>>>>>> elsewhere
>>>>>>>>>>>>>>> simply
>>>>>>>>>>>>>>> having design docs and prototypes implementations in PRs is
>>>>>>>>>>>>>>> not
>>>>>>>>>>>>>>> something
>>>>>>>>>>>>>>> that has not worked so far. What is really a pain in many
>>>>>>>>>>>>>>> projects
>>>>>>>>>>>>>>> out there
>>>>>>>>>>>>>>> is discontinuity in progress of PRs, missing features, slow
>>>>>>>>>>>>>>> reviews
>>>>>>>>>>>>>>> which is
>>>>>>>>>>>>>>> understandable to some extent... it is not only about Spark
>>>>>>>>>>>>>>> but
>>>>>>>>>>>>>>> things can
>>>>>>>>>>>>>>> be improved for sure for this project in particular as
>>>>>>>>>>>>>>> already
>>>>>>>>>>>>>>> stated.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> On Fri, Oct 7, 2016 at 11:14 PM, Cody Koeninger <[hidden
>>>>>>>>>>>>>>> email]>
>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> +1 to adding an SIP label and linking it from the website.
>>>>>>>>>>>>>>>> I
>>>>>>>>>>>>>>>> think
>>>>>>>>>>>>>>>> it
>>>>>>>>>>>>>>>> needs
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> - template that focuses it towards soliciting user goals /
>>>>>>>>>>>>>>>> non
>>>>>>>>>>>>>>>> goals
>>>>>>>>>>>>>>>> - clear resolution as to which strategy was chosen to
>>>>>>>>>>>>>>>> pursue.
>>>>>>>>>>>>>>>> I'd
>>>>>>>>>>>>>>>> recommend a vote.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Matei asked me to clarify what I meant by changing
>>>>>>>>>>>>>>>> interfaces,
>>>>>>>>>>>>>>>> I
>>>>>>>>>>>>>>>> think
>>>>>>>>>>>>>>>> it's directly relevant to the SIP idea so I'll clarify
>>>>>>>>>>>>>>>> here,
>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>> split
>>>>>>>>>>>>>>>> a thread for the other discussion per Nicholas' request.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> I meant changing public user interfaces.  I think the first
>>>>>>>>>>>>>>>> design
>>>>>>>>>>>>>>>> is
>>>>>>>>>>>>>>>> unlikely to be right, because it's done at a time when you
>>>>>>>>>>>>>>>> have
>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>> least information.  As a user, I find it considerably more
>>>>>>>>>>>>>>>> frustrating
>>>>>>>>>>>>>>>> to be unable to use a tool to get my job done, than I do
>>>>>>>>>>>>>>>> having to
>>>>>>>>>>>>>>>> make minor changes to my code in order to take advantage of
>>>>>>>>>>>>>>>> features.
>>>>>>>>>>>>>>>> I've seen committers be seriously reluctant to allow
>>>>>>>>>>>>>>>> changes
>>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>> @experimental code that are needed in order for it to
>>>>>>>>>>>>>>>> really
>>>>>>>>>>>>>>>> work
>>>>>>>>>>>>>>>> right.  You need to be able to iterate, and if people on
>>>>>>>>>>>>>>>> both
>>>>>>>>>>>>>>>> sides
>>>>>>>>>>>>>>>> of
>>>>>>>>>>>>>>>> the fence aren't going to respect that some newer apis are
>>>>>>>>>>>>>>>> subject
>>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>> change, then why even mark them as such?
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Ideally a finished SIP should give me a checklist of things
>>>>>>>>>>>>>>>> that
>>>>>>>>>>>>>>>> an
>>>>>>>>>>>>>>>> implementation must do, and things that it doesn't need to
>>>>>>>>>>>>>>>> do.
>>>>>>>>>>>>>>>> Contributors/committers should be seriously discouraged
>>>>>>>>>>>>>>>> from
>>>>>>>>>>>>>>>> putting
>>>>>>>>>>>>>>>> out a version 0.1 that doesn't have at least a prototype
>>>>>>>>>>>>>>>> implementation of all those things, especially if they're
>>>>>>>>>>>>>>>> then
>>>>>>>>>>>>>>>> going
>>>>>>>>>>>>>>>> to argue against interface changes necessary to get the the
>>>>>>>>>>>>>>>> rest
>>>>>>>>>>>>>>>> of
>>>>>>>>>>>>>>>> the things done in the 0.2 version.
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> On Fri, Oct 7, 2016 at 2:18 PM, Reynold Xin <[hidden
>>>>>>>>>>>>>>>> email]>
>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>> I like the lightweight proposal to add a SIP label.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> During Spark 2.0 development, Tom (Graves) and I suggested
>>>>>>>>>>>>>>>>> using
>>>>>>>>>>>>>>>>> wiki
>>>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>> track the list of major changes, but that never really
>>>>>>>>>>>>>>>>> materialized
>>>>>>>>>>>>>>>>> due to
>>>>>>>>>>>>>>>>> the overhead. Adding a SIP label on major JIRAs and then
>>>>>>>>>>>>>>>>> link
>>>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>> them
>>>>>>>>>>>>>>>>> prominently on the Spark website makes a lot of sense.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> On Fri, Oct 7, 2016 at 10:50 AM, Matei Zaharia
>>>>>>>>>>>>>>>>> <[hidden email]>
>>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> For the improvement proposals, I think one major point
>>>>>>>>>>>>>>>>>> was
>>>>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>>> make
>>>>>>>>>>>>>>>>>> them
>>>>>>>>>>>>>>>>>> really visible to users who are not contributors, so we
>>>>>>>>>>>>>>>>>> should
>>>>>>>>>>>>>>>>>> do
>>>>>>>>>>>>>>>>>> more than
>>>>>>>>>>>>>>>>>> sending stuff to dev@. One very lightweight idea is to
>>>>>>>>>>>>>>>>>> have
>>>>>>>>>>>>>>>>>> a
>>>>>>>>>>>>>>>>>> new
>>>>>>>>>>>>>>>>>> type of
>>>>>>>>>>>>>>>>>> JIRA called a SIP and have a link to a filter that shows
>>>>>>>>>>>>>>>>>> all
>>>>>>>>>>>>>>>>>> such
>>>>>>>>>>>>>>>>>> JIRAs from
>>>>>>>>>>>>>>>>>> http://spark.apache.org. I also like the idea of SIP and
>>>>>>>>>>>>>>>>>> design
>>>>>>>>>>>>>>>>>> doc
>>>>>>>>>>>>>>>>>> templates (in fact many projects have them).
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> Matei
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> On Oct 7, 2016, at 10:38 AM, Reynold Xin <[hidden email]>
>>>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> I called Cody last night and talked about some of the
>>>>>>>>>>>>>>>>>> topics
>>>>>>>>>>>>>>>>>> in
>>>>>>>>>>>>>>>>>> his
>>>>>>>>>>>>>>>>>> email.
>>>>>>>>>>>>>>>>>> It became clear to me Cody genuinely cares about the
>>>>>>>>>>>>>>>>>> project.
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> Some of the frustrations come from the success of the
>>>>>>>>>>>>>>>>>> project
>>>>>>>>>>>>>>>>>> itself
>>>>>>>>>>>>>>>>>> becoming very ""hot"", and it is difficult to get clarity
>>>>>>>>>>>>>>>>>> from
>>>>>>>>>>>>>>>>>> people
>>>>>>>>>>>>>>>>>> who
>>>>>>>>>>>>>>>>>> don't dedicate all their time to Spark. In fact, it is in
>>>>>>>>>>>>>>>>>> some
>>>>>>>>>>>>>>>>>> ways
>>>>>>>>>>>>>>>>>> similar
>>>>>>>>>>>>>>>>>> to scaling an engineering team in a successful startup:
>>>>>>>>>>>>>>>>>> old
>>>>>>>>>>>>>>>>>> processes that
>>>>>>>>>>>>>>>>>> worked well might not"
"
Cody Koeninger <cody@koeninger.org>,Mon"," 10 Oct 2016 14:43:45 -0500""",Re: Spark Improvement Proposals,Steve Loughran <stevel@hortonworks.com>,"Updated on github,
https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md

I believe I've touched on all feedback with the exception of naming,
and API vs Strategy.

Do we want a straw poll on naming?

Matei, are your concerns about api vs strategy addressed if we add an
API bullet point to the template?

ote:
here: mentor, regular reporting in could help, in particular, help stop the -1 at the end of the work
 collaboration with multiple people submitting PRs into the ASF codebase. This can reduce cost of merge and enable jenkins to keep on top of it. It also fits in well with the ASF ""do in apache infra"" community development process.
mal ASF process to require contributions to only come from committers. Committers are of course the only people who can *commit* stuff. But the whole point of an open source project is that anyone can *contribute* -- indeed, that is how people become committers. For example, in every ASF project, anyone can open JIRAs, submit design docs, submit patches, review patches, and vote on releases. This particular process is very similar to posting a JIRA or a design doc.
SEP, we want to accept it by date X so please comment before"").
tweight processes and then expand them if needed. Adding lots of rules from the beginning makes it confusing and can reduce contributions. Although, as engineers, we believe that anything can be solved using mechanical rules, in practice software development is a social process that ultimately requires humans to tackle things on a case-by-case basis.
:
If
 that
hat
the
e
e:
e an
ns
 to
ally
if
cal
m by
d
sing
all
reed
ack
th
ave
be
re's
what
as
hem forward. The
ely
n
tinct
e
ld
the
od
nce
we
t
to
on!
ng,
but
ing
not
a
ee
r
his
st
ose
e:
or
'.
h
f
of
ing
ad)
l
ed
or
ing
rovement-proposals.md
s
or
d
 I
a
he
e
e
r
se
e
un
s
st
me
on
ng
e
is
provement-proposals.md
e
.
it
ld
l
as
s.
s
l
s
w
k
.
/
st
u
e
of
e
gs
o
he
ed
s
d
]>
in
we
ls
o
.
ng
t
e
g
,
t
I
t
to
ou
t.
ng
e
y,
x,
------------
----
mprovement-Proposals-tp19268p19359.html
t
---


---------------------------------------------------------------------


"
Nico Pappagianis <nico.pappagianis@salesforce.com>,"Mon, 10 Oct 2016 12:49:59 -0700","Quotes within a table name (phoenix table) getting failure:
 identifier expected at Spark level parsing",dev@spark.apache.org,"Hello,

*Some context:*
I have a Phoenix tenant-specific view named CUSTOM_ENTITY.""z02"" (Phoenix
tables can have quotes to specify case-sensitivity). I am attempting to
write to this table using Spark via a scala script. I am performing the
following read successfully:

val table = """"""CUSTOM_ENTITY.""z02""""""""
val tenantId = ""myTenantId""
val urlWithTenant =
""jdbc:phoenix:myZKHost1, myZKHost1, myZKHost2,
myZKHost3:2181;TenantId=myTenantId""
val driver = ""org.apache.phoenix.jdbc.PhoenixDriver""

val readOptions = Map(driver"" -> driver, ""url"" -> urlWithTenant, ""dbtable""
-> table
)

val df = sqlContext.read.format(""jdbc"").options(readOptions).load

This gives me the dataframe with data successfully read from my tenant view.

Now when I try to write back with this dataframe:

df.write.format(""jdbc"").insertInto(table)


I am getting the following exception:

java.lang.RuntimeException: [1.15] failure: identifier expected

CUSTOM_ENTITY.""z02""

                              ^

(caret is pointing under the '.' before ""z02"")

at scala.sys.package$.error(package.scala:27)

at
org.apache.spark.sql.catalyst.SqlParser$.parseTableIdentifier(SqlParser.scala:56)

at
org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:164)

Looking at the stack trace it appears that Spark doesn't know what to do
with the quotes around z02. I've tried escaping them in every way I could
think of but to no avail.

Is there a way to have Spark not complain about the quotes and correctly
pass them along?

Thanks
"
Tamas Jambor <jamborta@gmail.com>,"Mon, 10 Oct 2016 21:15:37 +0100",Re: Spark 2.0.0 job completes but hangs,Steve Loughran <stevel@hortonworks.com>,"Yes, that was a problem I came across before, updating to hadoop 2.7 and
using fileoutputcommitter.algorithm.version did speed up the operation a
lot.



"
Xiao Li <gatorsmile@gmail.com>,"Mon, 10 Oct 2016 15:10:03 -0700","Re: Quotes within a table name (phoenix table) getting failure:
 identifier expected at Spark level parsing",Nico Pappagianis <nico.pappagianis@salesforce.com>,"HI, Nico,

We use back ticks to quote it. For example,

CUSTOM_ENTITY.`z02`

Thanks,

Xiao Li

2016-10-10 12:49 GMT-07:00 Nico Pappagianis <nico.pappagianis@salesforce.com

"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 10 Oct 2016 15:36:32 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"If I'm correctly understanding the kind of voting that you are talking
about, then to be accurate, it is only the PMC members that have a vote,
not all committers:
https://www.apache.org/foundation/how-it-works.html#pmc-members

:

f
l
 forward. The
n
e
ct values.
he
gn
s
er
't
.
e
 a
s
to
he
:
ld
,
m
n
s
n
at
he
ig
X,
y
g
e
e:
le
h
in
se
n
g.
as
ay
n
to
e
 a
s,
t
n
n
t
s.
ss
he
on
g
be
IP
dy
 I
e.
,
s
y
th
m
s
ld
c.
n
e
he
ce
ht
e
I
it
w
is
y
is
 a
r
y,
s
-
-
en
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 10 Oct 2016 15:46:30 -0700",Re: Spark Improvement Proposals,Nicholas Chammas <nicholas.chammas@gmail.com>,"I'm not a fan of the SEP acronym.  Besides it prior established meaning of
""Somebody else's problem"", the are other inappropriate or offensive
connotations such as this Australian slang that often gets shortened to
just ""sep"":  http://www.urbandictionary.com/define.php?term=Seppo


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 10 Oct 2016 15:59:04 -0700",Re: Spark Improvement Proposals,"Cody Koeninger <cody@koeninger.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","There is a larger issue to keep in mind, and that is that what you are
proposing is a procedure that, as far as I am aware, hasn't previously been
adopted in an Apache project, and thus is not an easy or exact fit with
established practices that have been blessed as ""The Apache Way"".  As such,
we need to be careful, because we have run into some trouble in the past
with some inside the ASF but essentially outside the Spark community who
didn't like the way we were doing things.


e
an
o
ly
l
by
ng
l
ed
k
e
's
at
s
em forward. The
inct
d
he
d
ce
e
o
n!
g,
ut
ot
e
is
t
se
:
r
.
f
ng
d)
d
r
ng
s
d
 I
a
he
e
e
r
se
e
un
s
me
on
e
is
e
.
it
ld
l
as
s.
s
l
s
w
k
/
st
u
e
of
e
gs
o
he
ed
s
d
]>
in
we
o
.
ng
t
e
g
,
t
I
t
ou
t.
e
x,
-
"
Cody Koeninger <cody@koeninger.org>,"Mon, 10 Oct 2016 18:15:20 -0500",Re: Spark Improvement Proposals,Mark Hamstra <mark@clearstorydata.com>,"If someone wants to tell me that it's OK and ""The Apache Way"" for
Kafka and Flink to have a proposal process that ends in a lazy
majority, but it's not OK for Spark to have a proposal process that
ends in a non-lazy consensus...

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals#KafkaImprovementProposals-Process

In practice any PMC member can stop a proposal they don't like, so I'm
not sure how much it matters.



te:
en
h,
e:
re
, not
s
to
f
al
ll
ck
h
ve
e
as
hem forward. The
n
tinct
e
ld
od
we
t
to
a
ee
r
st
e:
or
'.
h
f
of
l
ed
or
provement-proposals.md
n
is
ad
s
e
 a
d
re
e
t
re
s
er
se
Ps
l
r
ge
n
mprovement-proposals.md
we
e.
t
.
n
al
r
gs
ll
e
is
ow
rk
e.
 /
ou
re
re
to
e
n
e
ws
nd
y
:
d
o
e
d
no
o
n
d.
ut
be
ng
e,
it
 I
it
ue
s
-------------
----
mprovement-Proposals-tp19268p19359.html
t
--


---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 10 Oct 2016 16:34:58 -0700","Re: Improving governance / committers (split from Spark Improvement
 Proposals thread)",Cody Koeninger <cody@koeninger.org>,"I think it is really important to ensure that someone with a good
understanding of Kafka is empowered around this component with a formal
voice around - but I don't have much dev experience with our Kafka
connectors so I can't speak to the specifics around it personally.

More generally, I also feel pretty strongly about commit bits, and while
I've been going back through the old Python JIRAs and PRs it's seems we are
leaving some good stuff out just because of reviewer bandwidth (not to
mention the people that get turned away from contributing more after their
first interaction or lack their of). Certainly the Python reviewer(s) knows
their stuff - but it feels like for Python there just isn't enough
committer time available to handle the contributor interest. Although - to
be fair - this may be one of those cases where as we add more committers we
will have more contributors never having enough time, but I see that as a
positive cycle we should embrace.

I'm curious - are developers working more in other components feeling
similarly? I've sort of assumed so personally - but it would be nice to
hear others experiences as well.

Of course my disclaimer from the original conversation applies
<http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Improvement-Proposals-tp19268p19284.html>
- I do very much ""have a horse in the race"" so I will avoid proposing new
criteria. I working on Spark is a core part of what I do most days, and
once my day job with Spark is done I go and do even more Spark like working
on a new Spark book focused on performance right now - and I very much do
want to see a healthy community flourish around Spark :)

More thoughts in-line:



I'm happy to hear this is something being actively discussed by the PMC.
I'm also glad the PMC took the time to create some documentation around
what it takes to be a committer - but, to me, it seems like there are maybe
some additional requirements or nuances to the requirements/process which
haven't quite been fully captured in the current wiki and I look forward to
seeing the result of the conversation and the clarity or changes it can
bring to the process.

I realize the default for the PMC may be to have the conversation around
this on private@ - but I think the dev (and maybe even user) community as a
whole is rather interested and we all could benefit by working together on
this (or at least being aware of the PMCs thoughts around this).With the
decisions and discussions around the committer process happen on the
private mailing list (or in person) its really difficult as an outsider (or
contributor interested in being a committer) feel that one has a good
understanding of what is going on. Sean Owen and Matei each provided some
insight from their points of view in Cody's initial thread
<http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Improvement-Proposals-tp19268.html>
along with some additional thoughts in this thread by Matei, but I'd really
love to hear more (from both of them as well as the rest of the PMC). I
also think it would be useful to hear from people with experience in other
projects with what their best practices are around similar processes and
doing this (or parts of it) on the dev@ or user@ list will be able to
provide a wider variety of experiences to share as the PMC considers the
best approach.

Of course I completely understand and respect the PMCs choice around which
parts of the conversation belong where, I'd just like to encourage a
default to slightly more open if possible.

P.S.

I want to thank everyone who has taken the time to read and respond, its
really good that we as a community are able to have these sometimes
difficult discussions and try and grow from them.

-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Xiao Li <gatorsmile@gmail.com>,"Mon, 10 Oct 2016 19:42:00 -0700","Re: Quotes within a table name (phoenix table) getting failure:
 identifier expected at Spark level parsing","Nico Pappagianis <nico.pappagianis@salesforce.com>, 
	""user@spark.apache.org"" <dev@spark.apache.org>","Hi, Nico,

It sounds like you hit a bug in Phoenix Connector. Our general JDBC
connector already fixed it, I think.

Thanks,

Xiao

2016-10-10 15:29 GMT-07:00 Nico Pappagianis <nico.pappagianis@salesforce.com

"
Fei Hu <hufei68@gmail.com>,"Tue, 11 Oct 2016 04:04:46 +0000",,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>, 
	""user@hadoop.apache.org"" <user@hadoop.apache.org>","Hi All,

I am running some spark scala code on zeppelin on CDH 5.5.1 (Spark version
1.5.0). I customized the Spark interpreter to use
org.apache.spark.serializer.KryoSerializer as spark.serializer. And in the
dependency I added Kyro-3.0.3 as following:
 com.esotericsoftware:kryo:3.0.3


When I wrote the scala notebook and run the program, I got the following
errors. But If I compiled these code as jars, and use spark-submit to run
it on the cluster, it worked well without errors.

WARN [2016-10-10 23:43:40,801] ({task-result-getter-1}
Logging.scala[logWarning]:71) - Lost task 0.0 in stage 3.0 (TID 9,
svr-A3-A-U20): java.io.EOFException

        at
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:196)

        at
org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:217)

        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:178)

        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1175)

        at
org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)

        at
org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)

        at
org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)

        at
org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)

        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)

        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)

        at org.apache.spark.scheduler.Task.run(Task.scala:88)

        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)

        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

        at java.lang.Thread.run(Thread.java:745)


There were also some errors when I run the Zeppelin Tutorial:

Caused by: java.io.IOException: java.lang.NullPointerException

        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1163)

        at
org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:70)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:497)

        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)

        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)

        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)

        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)

        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)

        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)

        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)

        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)

        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)

        at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)

        at
org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:98)

        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)

        ... 3 more

Caused by: java.lang.NullPointerException

        at
com.twitter.chill.WrappedArraySerializer.read(WrappedArraySerializer.scala:38)

        at
com.twitter.chill.WrappedArraySerializer.read(WrappedArraySerializer.scala:23)

        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)

        at
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:192)

        at
org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1$$anonfun$apply$mcV$sp$2.apply(ParallelCollectionRDD.scala:80)

        at
org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1$$anonfun$apply$mcV$sp$2.apply(ParallelCollectionRDD.scala:80)

        at
org.apache.spark.util.Utils$.deserializeViaNestedStream(Utils.scala:142)

        at
org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:80)

        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)

Is there anyone knowing why it happended?

Thanks in advance,
Fei
"
Fei Hu <hufei68@gmail.com>,"Tue, 11 Oct 2016 00:11:27 -0400",Kryo on Zeppelin,"user@spark.apache.org, dev@spark.apache.org, 
	""user@hadoop.apache.org"" <user@hadoop.apache.org>","Hi All,

I am running some spark scala code on zeppelin on CDH 5.5.1 (Spark version
1.5.0). I customized the Spark interpreter to use org.apache.spark.
serializer.KryoSerializer as spark.serializer. And in the dependency I
added Kyro-3.0.3 as following:
 com.esotericsoftware:kryo:3.0.3


When I wrote the scala notebook and run the program, I got the following
errors. But If I compiled these code as jars, and use spark-submit to run
it on the cluster, it worked well without errors.

WARN [2016-10-10 23:43:40,801] ({task-result-getter-1}
Logging.scala[logWarning]:71) - Lost task 0.0 in stage 3.0 (TID 9,
svr-A3-A-U20): java.io.EOFException

        at org.apache.spark.serializer.KryoDeserializationStream.
readObject(KryoSerializer.scala:196)

        at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(
TorrentBroadcast.scala:217)

        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$
readBroadcastBlock$1.apply(TorrentBroadcast.scala:178)

        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1175)

        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(
TorrentBroadcast.scala:165)

        at org.apache.spark.broadcast.TorrentBroadcast._value$
lzycompute(TorrentBroadcast.scala:64)

        at org.apache.spark.broadcast.TorrentBroadcast._value(
TorrentBroadcast.scala:64)

        at org.apache.spark.broadcast.TorrentBroadcast.getValue(
TorrentBroadcast.scala:88)

        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)

        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.
scala:62)

        at org.apache.spark.scheduler.Task.run(Task.scala:88)

        at org.apache.spark.executor.Executor$TaskRunner.run(
Executor.scala:214)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(
ThreadPoolExecutor.java:1142)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(
ThreadPoolExecutor.java:617)

        at java.lang.Thread.run(Thread.java:745)


There were also some errors when I run the Zeppelin Tutorial:

Caused by: java.io.IOException: java.lang.NullPointerException

        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1163)

        at org.apache.spark.rdd.ParallelCollectionPartition.readObject(
ParallelCollectionRDD.scala:70)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(
NativeMethodAccessorImpl.java:62)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(
DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:497)

        at java.io.ObjectStreamClass.invokeReadObject(
ObjectStreamClass.java:1058)

        at java.io.ObjectInputStream.readSerialData(
ObjectInputStream.java:1900)

        at java.io.ObjectInputStream.readOrdinaryObject(
ObjectInputStream.java:1801)

        at java.io.ObjectInputStream.readObject0(ObjectInputStream.
java:1351)

        at java.io.ObjectInputStream.defaultReadFields(
ObjectInputStream.java:2000)

        at java.io.ObjectInputStream.readSerialData(
ObjectInputStream.java:1924)

        at java.io.ObjectInputStream.readOrdinaryObject(
ObjectInputStream.java:1801)

        at java.io.ObjectInputStream.readObject0(ObjectInputStream.
java:1351)

        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)

        at org.apache.spark.serializer.JavaDeserializationStream.
readObject(JavaSerializer.scala:72)

        at org.apache.spark.serializer.JavaSerializerInstance.
deserialize(JavaSerializer.scala:98)

        at org.apache.spark.executor.Executor$TaskRunner.run(
Executor.scala:194)

        ... 3 more

Caused by: java.lang.NullPointerException

        at com.twitter.chill.WrappedArraySerializer.read(
WrappedArraySerializer.scala:38)

        at com.twitter.chill.WrappedArraySerializer.read(
WrappedArraySerializer.scala:23)

        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)

        at org.apache.spark.serializer.KryoDeserializationStream.
readObject(KryoSerializer.scala:192)

        at org.apache.spark.rdd.ParallelCollectionPartition$$
anonfun$readObject$1$$anonfun$apply$mcV$sp$2.apply(
ParallelCollectionRDD.scala:80)

        at org.apache.spark.rdd.ParallelCollectionPartition$$
anonfun$readObject$1$$anonfun$apply$mcV$sp$2.apply(
ParallelCollectionRDD.scala:80)

        at org.apache.spark.util.Utils$.deserializeViaNestedStream(
Utils.scala:142)

        at org.apache.spark.rdd.ParallelCollectionPartition$$
anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:80)

        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)

Is there anyone knowing why it happended?

Thanks in advance,
Fei
"
Reynold Xin <rxin@databricks.com>,"Mon, 10 Oct 2016 22:03:19 -0700",FYI - marking data type APIs stable,"""dev@spark.apache.org"" <dev@spark.apache.org>","I noticed today that our data types APIs (org.apache.spark.sql.types) are
actually DeveloperApis, which means they can be changed from one feature
release to another. In reality these APIs have been there since the
original introduction of the DataFrame API in Spark 1.3, and has not seen
any breaking changes since then. It makes more sense to mark them stable.

There are also a number of DataFrame related classes that have been
Experimental or DeveloperApi for eternity. I will be marking these stable
in the upcoming feature release (2.1).


Please shout if you disagree.
"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 11 Oct 2016 19:20:35 +0900",Re: Looking for a Spark-Python expert,Sean Owen <sowen@cloudera.com>,"Just as one of those who subscribed to dev/user mailing list, I would like
to avoid to recieve flooding emails about job recruiting.

In my personal opinion, I think that might mean virtually allowing that
this list is being used as the mean for some profits in an organisation.


"
Cosmin Ciobanu <ciobanu@adobe.com>,"Tue, 11 Oct 2016 08:20:34 -0700 (MST)","Spark Streaming deletes checkpointed RDD then tries to load it
 after restart",dev@spark.apache.org,"This is a follow up for this unanswered October 2015 issue:
http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-streaming-failed-recovery-from-checkpoint-td14832.html

The issue is that the Spark driver checkpoints an RDD, deletes it, the job
restarts, and *the new driver tries to load the deleted checkpoint RDD*.

The application is run in YARN, which attempts to restart the application a
number of times (100 in our case), all of which fail due to missing the
deleted RDD. 

Here is a Splunk log which shows the inconsistency in checkpoint behaviour:

*2016-10-09 02:48:43,533* [streaming-job-executor-0] INFO 
org.apache.spark.rdd.ReliableRDDCheckpointData - Done checkpointing RDD
73847 to
hdfs://proc-job/checkpoint/cadf8dcf-ebc2-4366-a2e1-0939976c6ce1/*rdd-73847*,
new parent is RDD 73872
host = ip-10-1-1-13.ec2.internal
*2016-10-09 02:53:14,696* [JobGenerator] INFO 
org.apache.spark.streaming.dstream.DStreamCheckpointData - Deleted
checkpoint file
'hdfs://proc-job/checkpoint/cadf8dcf-ebc2-4366-a2e1-0939976c6ce1/*rdd-73847*'
for time 1475981310000 ms
host = ip-10-1-1-13.ec2.internal
*Job restarts here, notice driver host change from ip-10-1-1-13.ec2.internal
to ip-10-1-1-25.ec2.internal.*
*2016-10-09 02:53:30,175* [Driver] INFO 
org.apache.spark.streaming.dstream.DStreamCheckpointData - Restoring
checkpointed RDD for time 1475981310000 ms from file
'hdfs://proc-job/checkpoint/cadf8dcf-ebc2-4366-a2e1-0939976c6ce1/*rdd-73847*'
host = ip-10-1-1-25.ec2.internal
*2016-10-09 02:53:30,491* [Driver] ERROR
org.apache.spark.deploy.yarn.ApplicationMaster - User class threw exception:
java.lang.IllegalArgumentException: requirement failed: Checkpoint directory
does not exist:
hdfs://proc-job/checkpoint/cadf8dcf-ebc2-4366-a2e1-0939976c6ce1/*rdd-73847*
java.lang.IllegalArgumentException: requirement failed: Checkpoint directory
does not exist:
hdfs://proc-job/checkpoint/cadf8dcf-ebc2-4366-a2e1-0939976c6ce1/*rdd-73847*
host = ip-10-1-1-25.ec2.internal

Spark streaming is configured with a microbatch interval of 30 seconds,
checkpoint interval of 120 seconds, and cleaner.ttl of 28800 (8 hours), but
as far as I can tell, this TTL only affects metadata cleanup interval. RDDs
seem to be deleted every 4-5 minutes after being checkpointed.

Running on top of Spark 1.5.1.

Questions: 
- How is checkpoint deletion metadata saved so that in case of a driver
restart the new driver does not read invalid metadata? Is there an interval
between the point when the checkpoint data is deleted and this information
is logged? 
- Why does Spark try to load data checkpointed 4-5 minutes in the past,
given the fact checkpoint interval is 120 seconds and thus 5 minute old data
is stale? There should be a newer checkpoint.
- Would the memory management in Spark 2.0 handle this differently? Since we
have not been able to reproduce the issue outside production environments,
it might be useful to know in advance if there have been changes in this
area.

This issue is constantly causing serious data loss in production
environments, I'd appreciate any assistance with it.



--

---------------------------------------------------------------------


"
Fred Reiss <freiss.oss@gmail.com>,"Tue, 11 Oct 2016 10:09:12 -0700","StructuredStreaming Custom Sinks (motivated by Structured Streaming
 Machine Learning)",Michael Armbrust <michael@databricks.com>,"
Details are difficult to share, but I can give the general gist without
revealing anything proprietary.

I find myself having the same conversation about twice a month. The other
party to the conversation is an IBM product group or an IBM client who
is using Spark for batch and interactive analytics. Their overall
application has or will soon have a real-time component. They want
information from the IBM Spark Technology Center on the relative merits
of different streaming systems for that part of the application or product.
Usually, the options on the table are Spark Streaming/Structured Streaming
and another more ""pure"" streaming system like Apache Flink or IBM Streams.

Right now, the best recommendation I can give is: ""Spark Streaming has
known shortcomings; here's a list. If you are certain that your application
can work within these constraints, then we recommend you give Spark
Streaming a try. Otherwise, check back 12-18 months from now, when
Structured Streaming will hopefully provide a usable platform for your
application.""

The specific unmet requirements that are most relevant to these
conversations are: latency, throughput, stability under bursty loads,
complex event processing support, state management, job graph scheduling,
and access to the newer Dataset-based Spark APIs.

Again, apologies for not being able to name names, but here's a redacted
description of why these requirements are relevant.

*Delivering low latency, high throughput, and stability simultaneously:* Right
now, our own tests indicate you can get at most two of these
characteristics out of Spark Streaming at the same time. I know of two
parties that have abandoned Spark Streaming because ""pick any two"" is not
an acceptable answer to the latency/throughput/stability question for them.

*Complex event processing and state management:* Several groups I've talked
to want to run a large number (tens or hundreds of thousands now, millions
in the near future) of state machines over low-rate partitions of a
high-rate stream. Covering these use cases translates roughly into a three
sub-requirements: maintaining lots of persistent state efficiently, feeding
tuples to each state machine in the right order, and exposing convenient
programmer APIs for complex event detection and signal processing tasks.

*Job graph scheduling and access to Dataset APIs: *These requirements come
up in the context of groups who want to do streaming ETL. The general
application profile that I've seen involves updating a large number of
materialized views based on a smaller number of streams, using a mixture of
SQL and nontraditional processing. The extended SQL that the Dataset APIs
provide is useful in these applications. As for scheduling needs, it's
common for multiple output tables to share intermediate computations. Users
need an easy way to ensure that this shared computation happens only once,
while controlling the peak memory utilization during each batch.

Hope this helps.

Fred
"
Michael Armbrust <michael@databricks.com>,"Tue, 11 Oct 2016 10:55:25 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Fred Reiss <freiss.oss@gmail.com>,"This is super helpful, thanks for writing it up!



Agree, this should be the major focus.



I've heard this one too, but don't know of anyone actively working on it.
Would be awesome to open a JIRA and start discussing what the APIs would
look like.



This sounds like two separate things to me.  High-level APIs (are streaming
DataFrames / Datasets missing anything?) and multi-query optimization for
streams.  I've been thinking about the latter.  I think we probably want to
crush latency/throughput/stability in the simpler case first, but after
that I think there is a lot of machinery already in SQL we can reuse (i.e.
the sameResult calculations used for caching).
"
Reynold Xin <rxin@databricks.com>,"Tue, 11 Oct 2016 10:57:51 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Michael Armbrust <michael@databricks.com>,"

There is an existing ticket for CEP:
https://issues.apache.org/jira/browse/SPARK-14745
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 11 Oct 2016 11:02:36 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Fred Reiss <freiss.oss@gmail.com>,"Thanks Fred - that is very helpful.

Could you expand a little bit more on stability ? Is it just bursty
workloads in terms of peak vs. average throughput ? Also what level of
latencies do you find users care about ? Is it on the order of 2-3
seconds vs. 1 second vs. 100s of milliseconds ?

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 11 Oct 2016 13:57:00 -0700",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"I don't think we will have trouble with whatever rule that is adopted for
accepting proposals. Considering committers' votes binding (if that is what
we choose) is an established practice as long as it isn't for specific
votes, like a release vote. From the Apache docs: ""Who is permitted to vote
is, to some extent, a community-specific thing."" [1] And, I also don't see
why it would be a problem to choose consensus, as long as we have an open
discussion and vote about these rules.

rb


t
o
n
g
l
:
ve
e
.
em
g
ed
e
g
m
 them forward. The
ge
ct
ng
istinct
t
t
be
,
e
r
y
o
ng
,
m
o
e.
nd
d
ey
d
/
a
m
:
e
a
ep
r
o
o
--
n
t
O
rs
ne
s
oo
er
e
]>
y
at
-
l
]>
al
le
s
ny
n
t.
do
ge
on
d
d
pe
e
y
.
nt
e
e
is
s,
r
ly
nd
n
y
e
t
o
y
/
d
I
t
,
st
e
e
e
ge
*
-


-- 
Ryan Blue
Software Engineer
Netflix
"
dhruve ashar <dhruveashar@gmail.com>,"Tue, 11 Oct 2016 17:12:59 -0500",Checkpointing in Spark - Cleaning files and Support across app attempts,dev <dev@spark.apache.org>,"While checkpointing RDDs as a part of an application that doesn't use
spark-streaming, I observed that the checkpointed files are not being
cleaned up even after the application completes successfully.

Is it because we assume that checkpointing would be primarily used for
spark-streaming applications which run in continuum?

Also the current mechanism supports recovery only in spark-streaming which
can survive driver crashes. There's no support to recover from previously
checkpointed RDDs in subsequent application attempts. It would be
consistent and nice to have the ability to recover across app attempts in
non streaming jobs.

Is there any specific reason for the current behavior of not cleaning the
files and lack of support across app attempts? If not I can raise a JIRA
for this.

Thanks,
Dhruve
"
Imran Rashid <irashid@cloudera.com>,"Wed, 12 Oct 2016 10:20:23 -0500",RFC / PRD: new executor & node blacklist mechanism (SPARK-8425),dev <dev@spark.apache.org>,"Some new features are about to land in spark to improve Spark's ability to
handle bad executors and nodes.  These are some significant changes, and
we'd like to gather more input from the community about it, especially
folks that use *large clusters*.

We've spent a lot of time discussing the right approach for this feature;
there are some tough tradeoffs that need to be made, and though we made our
best efforts at the right balance, more community input would be very
helpful.  The feature is marked experimental and off by default for now
while we see how it works for the broader community.

There is a design doc attached to SPARK-8425 with a more thorough
explanation, but I wanted to highlight some key aspects.  Before these
changes, spark was particularly vulnerable to flaky hardware on one node.
The most common example is one bad disk somewhere in your cluster.

After the changes go in, Spark will avoid scheduling tasks on executors and
nodes with previous failures.  A brief summary of the behavior when
blacklisting is turned on:

1) when a task fails, it won't ever be scheduled on the same executor.  If
it fails on another executor on the same node, that task won't ever be
scheduled on the same node again.

2) If an executor has two failed tasks within one task set, it is
blacklisted for the entire task set.  Similarly for a node.

3) After a taskset completes _successfully_, Spark check's whether it
should blacklist any executors or nodes for the entire application.
Executors or nodes that have more than 2 failures in *successful* task sets
are blacklisted for the application.  They are blacklisted for a
configurable timeout, with a default of 1 hour.

There are a number of confs added to control the cutoffs used, but we hope
these defaults are sensible for most users.

These changes will land in two patches.  SPARK-17675 adds the changes
described within a taskset.  SPARK-8425 will follow soon after that with
application-level blacklisting.  There are other follow up changes, eg.
adding this info to the UI, but the core functionality is in those.

Choosing the right behavior is particularly tough because Spark doesn't
know why tasks fail -- is it a real hardware issue?  Or is there just
temporary resource contention, perhaps due to an ill-behaved application?
We needed to balance:

1) Robustness -- making sure Spark would survive a bad node under a wide
variety of problems
2) Resource utilization  -- avoid blacklisting resources when they are
perfectly fine
3) Ease of use -- the feature needs to be easy to configure, and have
sensible logging etc. when there are issues.

Hopefully the approach delivers a good balance for the majority of use
cases, but we're particularly interested in hearing more from use cases
with large clusters where hardware failures are encountered more frequently.

A number of people have put in a lot of effort on driving discussion and
implementation of this issue, in particular: Kay Ousterhout, Tom Graves,
Mark Hamstra, Mridul Muralidharan, Wei Mao and Jerry Shao (sorry if I
missed anybody!).

thanks,
Imran
"
Tejas Patil <tejas.patil.cs@gmail.com>,"Wed, 12 Oct 2016 10:26:37 -0700",`Project` not preserving child partitioning ?,"Spark dev list <dev@spark.apache.org>, daoyuan.wang@intel.com","See
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala#L80

Project operator preserves child's sort ordering but for output
partitioning, it does not. I don't see any way projection would alter the
partitioning of the child plan because rows are not passed across
partitions when project happens (and if it does then it would also affect
the sort ordering won't it ?). Am I missing something obvious here ?

Thanks,
Tejas
"
Fred Reiss <freiss.oss@gmail.com>,"Wed, 12 Oct 2016 10:56:13 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Reynold Xin <rxin@databricks.com>,"
Yeah, Mario and Sachin opened up that CEP ticket a while back, and they had
an early prototype (https://github.com/apache/spark/pull/12518) on the old
DStream APIs. The project stalled out due to uncertainty about how state
management and streaming query languages would work on Structured
Streaming. The people who were working on it are now focusing on other
issues.

Getting CEP to work efficiently is a whole-stack affair. You need optimizer
support for things like pulling out common subexpressions from event specs
and deciding between eager vs. lazy evaluation for predicates. You need
good fine-grained state management in the engine, including support for
efficiently handling out-of-order event arrival. And CEP workloads with a
large number of interdependent, stateful tasks will put stress on the
scheduling layer.

Fred
"
Reynold Xin <rxin@databricks.com>,"Wed, 12 Oct 2016 11:26:53 -0700",Re: `Project` not preserving child partitioning ?,Tejas Patil <tejas.patil.cs@gmail.com>,"It actually does -- but do it through a really weird way.

UnaryNodeExec actually defines:

trait UnaryExecNode extends SparkPlan {
  def child: SparkPlan

  override final def children: Seq[SparkPlan] = child :: Nil

  override def outputPartitioning: Partitioning = child.outputPartitioning
}


I think this is very risky because preserving output partitioning should
not be a property of UnaryNodeExec (e.g. exchange). It would be better
(safer) to move the output partitioning definition into each of the
operator and remove it from UnaryExecNode.

Would you be interested in submitting the patch?




"
Tejas Patil <tejas.patil.cs@gmail.com>,"Wed, 12 Oct 2016 11:33:44 -0700",Re: `Project` not preserving child partitioning ?,Reynold Xin <rxin@databricks.com>,"Sure :)

Thanks,
Tejas


"
Koert Kuipers <koert@tresata.com>,"Wed, 12 Oct 2016 14:55:53 -0400",incorrect message that path appears to be local,"""dev@spark.apache.org"" <dev@spark.apache.org>","i see this warning when running jobs on cluster:

2016-10-12 14:46:47 WARN spark.SparkContext: Spark is not running in local
mode, therefore the checkpoint directory must not be on the local
filesystem. Directory '/tmp' appears to be on the local filesystem.

however the checkpoint ""directory"" that it warns about is a hadoop path. i
use an unqualified path, which means a path on the default filesystem by
hadoop convention. when running on the cluster my default filesystem is
hdfs (and it correctly uses hdfs).

how about if we change the method that does this check
(Utils.nonLocalPaths) to be aware of the default filesystem instead of
incorrectly assuming its local if not specified?
"
Holden Karau <holden@pigscanfly.ca>,"Wed, 12 Oct 2016 12:49:48 -0700",Python Spark Improvements (forked from Spark Improvement Proposals),"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi Spark Devs & Users,


Forking off from Codyâ€™s original thread
<http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Improvement-Proposals-td19268.html#none>
of Spark Improvements, and Matei's follow up on asking what issues the
Python community was facing with Spark, I think it would be useful for us
to discuss some of the motivations behind some of the Python community
looking at different technologies to replace Apache Spark with. My
viewpoints are based that of a developer who works on Apache Spark
day-to-day <http://bit.ly/hkspmg>, but also gives a fair number of talks at
Python conferences
<https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=holden+karau+pydata&start=0>
and I feel many (but not all) of the same challenges as the Python
community does trying to use Spark. Iâ€™ve included both the user@ and dev@
lists on this one since I think the user community can probably provide
more reasons why they have difficulty with PySpark. I should also point out
- the solution for all of these things may not live inside of the Spark
project itself, but it still impacts our usability as a whole.


   -

   Lack of pip installability

This is one of the points that Matei mentioned, and it something several
people have tried to provide for Spark in one way or another. It seems
getting reviewer time for this issue is rather challenging, and Iâ€™ve been
hesitant to ask the contributors to keep updating their PRs (as much as I
want to see some progress) because I just don't know if we have the time or
interest in this. Iâ€™m happy to pick up the latest from Juliet and try and
carry it over the finish line if we can find some committer time to work on
this since it now sounds like there is consensus we should do this.

   -

   Difficulty using PySpark from outside of spark-submit / pyspark shell

The FindSpark <https://pypi.python.org/pypi/findspark> package needing to
exist is one of the clearest examples of this challenge. There is also a PR
to make it easier for other shells to extend the Spark shell, and we ran
into some similar challenges while working on Sparkling Pandas. This could
be solved by making Spark pip installable so I won'tâ€™ say too much about
this point.

   -

   Minimal integration with IPython/IJupyter

This one is awkward since one of the areas that some of the larger
commercial players work in effectively â€œcompetesâ€ (in a very loose term)
with any features introduced around here. Iâ€™m not really super sure what
the best path forward is here, but I think collaborating with the IJupyter
people to enable more features found in the commercial offerings in open
source could be beneficial to everyone in the community, and maybe even
reduce the maintenance cost for some of the commercial entities. I
understand this is a tricky issue but having good progress indicators or
something similar could make a huge difference. (Note that Apache Toree
<https://toree.incubator.apache.org/> [Incubating] exists for Scala users
but hopefully the PySpark IJupyter integration could be achieved without a
new kernel).

   -

   Lack of virtualenv and or Python package distribution support

This one is also tricky since many commercial providers have their own
â€œsolutionâ€ to this, but there isnâ€™t a good story around supporting custom
virtual envs or user required Python packages. While spark-packages _can_
be Python this requires that the Python package developer go through rather
a lot of work to make their package available and realistically wonâ€™t
happen for most Python packages people want to use. And to be fair, the
addFiles mechanism does support Python eggs which works for some packages.
There are some outstanding PRs around this issue (and I understand these
are perhaps large issues which might require large changes to the current
suggested implementations - Iâ€™ve had difficulty keeping the current set of
open PRs around this straight in my own head) but there seems to be no
committer bandwidth or interest on working with the contributors who have
suggested these things. Is this an intentional decision or is this
something we as a community are willing to work on/tackle?

   -

   Speed/performance

This is often a complaint I hear from more â€œdata engineeringâ€ profile users
who are working in Python. These problems come mostly in places involving
the interaction of Python and the JVM (so UDFs, transformations with
arbitrary lambdas, collect() and toPandas()). This is an area Iâ€™m working
on (see https://github.com/apache/spark/pull/13571 ) and hopefully we can
start investigating Apache Arrow <https://arrow.apache.org/> to speed up
the bridge (or something similar) once itâ€™s a bit more ready (currently
Arrow just released 0.1 which is exciting). We also probably need to start
measuring these things more closely since otherwise random regressions will
continue to be introduced (like the challenge with unbalanced partitions
and block serialization together - see SPARK-17817
<https://issues.apache.org/jira/browse/SPARK-17817> which fixed this)

   -

   Configuration difficulties (especially related to OOMs)

This is a general challenge many people face working in Spark, but PySpark
users are also asked to somehow figure out what the correct amount of
memory is to give to the Python process versus the Scala/JVM processes.
This was maybe an acceptable solution at the start, but when combined with
the difficult to understand error messages it can become quite the time
sink. A quick work around would be picking a different default overhead for
applications using Python, but more generally hopefully some shared off-JVM
heap solution could also help reduce this challenge in the future.

   -

   API difficulties

The Spark API doesnâ€™t â€œfeelâ€ very Pythony is a complaint some people have,
but I think weâ€™ve done some excellent work in the DataFrame/Dataset API
here. At the same time weâ€™ve made some really frustrating choices with the
DataFrame API (e.g. removing map from DataFrames pre-emptively even when we
have no concrete plans to bring the Dataset API to PySpark).

A lot of users wish that our DataFrame API was more like the Pandas API
(and Wes has pointed out on some JIRAs where we have differences) as well
as covered more of the functionality of Pandas. This is a hard problem, and
it the solution might not belong inside of PySpark itself (Juliet and I did
some proof-of-concept work back in the day on Sparkling Pandas
<https://github.com/sparklingpandas/sparklingpandas>) - but since one of my
personal goals has been trying to become a committer Iâ€™ve been more focused
on contributing to Spark itself rather than libraries and very few people
seem to be interested in working on this project [although I still have
potential users ask if they can use it]. (Of course if there is sufficient
interest to reboot Sparkling Pandas or something similar that would be an
interesting area of work - but itâ€™s also a huge area of work - if you look
at Dask <http://dask.pydata.org/>, a good portion of the work is dedicated
just to supporting pandas like operations).

   -

   Incomprehensible error messages

I often have people ask me how to debug PySpark and they often have a
certain haunted look in their eyes while they ask me this (slightly
joking). More seriously, we really need to provide more guidance around how
to understand PySpark error messages and look at figuring out if there are
places where we can improve the messaging so users arenâ€™t hunting through
stack overflow trying to figure out where the Java exception they are
getting is related to their Python code. In one talk I gave recently
someone mentioned PySpark was the motivation behind finding the hide error
messages plugin/settings for IJupyter.

   -

   Lack of useful ML model & pipeline export/import

This is something weâ€™ve made great progress on, many of the PySpark models
are now able to use the underlying export mechanisms from Java. However I
often hear challenges with using these models in the rest of the Python
space once they have been exported from Spark. Iâ€™ve got a PR to add basic
PMML export in Scala to ML (which we can then bring to Python), but I think
the Python community is open to other formats if the Spark community
doesnâ€™t want to go the PMML route.

Now I donâ€™t think we will see the same challenges weâ€™ve seen develop in the
R community, but I suspect purely Python approaches to distributed systems
will continue to eat the â€œlow endâ€ of Spark (e.g. medium sized data
problems requiring parallelism). This isnâ€™t necessarily a bad thing, but if
there is anything Iâ€™ve learnt it that's the ""low end"" solution often
quickly eats the ""high end"" within a few years - and Iâ€™d rather see Spark
continue to thrive outside of the pure JVM space.

These are just the biggest issues that I hear come up commonly and
remembered on my flight back - itâ€™s quite possible Iâ€™ve missed important
things. I know contributing on a mailing list can be scary or intimidating
for new users (and even experienced developers may wish to stay out of
discussions they view as heated) - but I strongly encourage everyone to
participate (respectfully) in this thread and we can all work together to
help Spark continue to be the place where people from different languages
and backgrounds continue to come together to collaborate.


I want to be clear as well, while I feel these are all very important
issues (and being someone who has worked on PySpark & Spark for years
<http://bit.ly/hkspmg> without being a committer I may sometimes come off
as frustrated when I talk about these) I think PySpark as a whole is a
really excellent application and we do some really awesome stuff with it.
There are also things that I will be blind to as a result of having worked
on Spark for so long (for example yesterday I caught myself using the _
syntax in a Scala example without explaining it because it seems â€œnormalâ€
to me but often trips up new comers.) If we can address even some of these
issues I believe it will be a huge win for Spark adoption outside of the
traditional JVM space (and as the various community surveys continue to
indicate PySpark usage is already quite high).

Normally Iâ€™d bring in a box of timbits
<https://en.wikipedia.org/wiki/Timbits>/doughnuts or something if we were
having an in-person meeting for this but all I can do for the mailing list
is attach a cute picture and offer future doughnuts/coffee if people want
to chat IRL. So, in closing, Iâ€™ve included a picture of two of my stuffed
animals working on Spark on my flight back from a Python Data conference &
Spark meetup just to remind everyone that this is just a software project
and we can be friendly nice people if we try and things will be much more
awesome if we do :)
[image: Inline image 1]

-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Sean Owen <sowen@cloudera.com>,"Wed, 12 Oct 2016 20:25:09 +0000",Re: incorrect message that path appears to be local,"Koert Kuipers <koert@tresata.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I'm not sure this is applied consistently across Spark, but I'm dealing
with another change now where an unqualified path is assumed to be a local
file. The method Utils.resolvePath implements this logic and is used
several places. Therefore I think this is probably intended behavior and
you can write hdfs:///tmp if you mean to reference /tmp on HDFS.


"
vonnagy <ivan@vadio.com>,"Wed, 12 Oct 2016 13:33:09 -0700 (MST)",Memory leak warnings in Spark 2.0.1,dev@spark.apache.org,"I am getting excessive memory leak warnings when running multiple mapping and
aggregations and using DataSets. Is there anything I should be looking for
to resolve this or is this a known issue?

WARN  [Executor task launch worker-0]
org.apache.spark.memory.TaskMemoryManager - leak 16.3 MB memory from
org.apache.spark.unsafe.map.BytesToBytesMap@33fb6a15
WARN  [Executor task launch worker-0]
org.apache.spark.memory.TaskMemoryManager - leak a page:
org.apache.spark.unsafe.memory.MemoryBlock@29e74a69 in task 88341
WARN  [Executor task launch worker-0]
org.apache.spark.memory.TaskMemoryManager - leak a page:
org.apache.spark.unsafe.memory.MemoryBlock@22316bec in task 88341
WARN  [Executor task launch worker-0] org.apache.spark.executor.Executor -
Managed memory leak detected; size = 17039360 bytes, TID = 88341

Thanks,

Ivan



--

---------------------------------------------------------------------


"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Wed, 12 Oct 2016 23:09:02 +0200",Re: Python Spark Improvements (forked from Spark Improvement Proposals),"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","I would add to the list the lag between Scala and Python for
new released features. Some features/functions get implemented later for
Pyspark, others not available at all. Think GraphX (maybe not the best
example), usually mentioned as one of the main libraries, that didn't make
it to the Python API (and never will - fortunately GraphFrames came to the
rescue on this particular case).


ent-Proposals-td19268.html#none>
ie=UTF-8#q=holden+karau+pydata&start=0>
and dev@
ut
™ve been
or
 try and
on
PR
d
h about
ery loose term)
re what
r
a
around supporting custom
er
™t
.
nt set of
€ profile
€™m
ch is
k
h
or
VM
mplaint some people have,
et API
 with the
we
nd
id
 more
w
of work -
ow
e
 through
r
rk models
dd basic
nk
een develop in
 medium sized data
ng, but if
ten
ee Spark
issed important
g
d
œnormalâ€
e
t
 stuffed
&
"
msukmanowsky <mike.sukmanowsky@gmail.com>,"Wed, 12 Oct 2016 17:51:13 -0700 (MST)","Re: Python Spark Improvements (forked from Spark Improvement
 Proposals)",dev@spark.apache.org,"As very heavy Spark users at Parse.ly, I just wanted to give a +1 to all of
the issues raised by Holden and Ricardo. I'm also giving a talk at PyCon
Canada on PySpark https://2016.pycon.ca/en/schedule/096-mike-sukmanowsky/.

Being a Python shop, we were extremely pleased to learn about PySpark a few
years ago as our main ETL pipeline used Apache Pig at the time. I was one of
the only folks who understood Pig and Java so collaborating on this as a
team was difficult.

Spark provided a means for the entire team to collaborate, but we've hit our
fair share of issues all of which are enumerated in this thread.

Besides giving a +1 here, I think if I were to force rank these items for
us, it'd be:

1. Configuration difficulties: we've lost literally weeks to troubleshooting
memory issues for larger jobs. It took a long time to even understand *why*
certain jobs were failing since Spark would just report executors being
lost. Finally we tracked things down to understanding that
spark.yarn.executor.memoryOverhead controls the portion of memory reserved
for Python processes, but none of this is documented anywhere as far as I
can tell. We discovered this via trial and error. Both documentation and
better defaults for this setting when running a PySpark application are
probably sufficient. We've also had a number of troubles with saving Parquet
output as part of an ETL flow, but perhaps we'll save that for a blog post
of its own.

2. Dependency management: I've tried to help move the conversation on
https://issues.apache.org/jira/browse/SPARK-13587 but it seems we're a bit
stalled. Installing the required dependencies for a PySpark application is a
really messy ordeal right now.

3. Development workflow: I'd combine both ""incomprehensible error messages""
and ""
difficulty using PySpark from outside of spark-submit / pyspark shell"" here.
When teaching PySpark to new users, I'm reminded of how much inside
knowledge is needed to overcome esoteric errors. As one example is hitting
""PicklingError: Could not pickle object as excessively deep recursion
required."" errors. New users often do something innocent like try to pickle
a global logging object and hit this and begin the Google -> stackoverflow
search to try to comprehend what's going on. You can lose days to errors
like these and they completely kill the productivity flow and send you
hunting for alternatives.

4. Speed/performance: we are trying to use DataFrame/DataSets where we can
and do as much in Java as possible but when we do move to Python, we're well
aware that we're about to take a hit on performance. We're very keen to see
what Apache Arrow does for things here.

5. API difficulties: I agree that when coming from Python, you'd expect that
you can do the same kinds of operations on DataFrames in Spark that you can
with Pandas, but I personally haven't been too bothered by this. Maybe I'm
more used to this situation from using other frameworks that have similar
concepts but incompatible implementations.

We're big fans of PySpark and are happy to provide feedback and contribute
wherever we can.



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 13 Oct 2016 01:59:50 +0000",Re: Python Spark Improvements (forked from Spark Improvement Proposals),"Holden Karau <holden@pigscanfly.ca>, ""dev@spark.apache.org"" <dev@spark.apache.org>, 
	""user@spark.apache.org"" <user@spark.apache.org>","I'd add one item to this list: The lack of Python 3 support in Spark
Packages <https://github.com/databricks/sbt-spark-package/issues/26>. This
means that great packages like GraphFrames cannot be used with Python 3
<https://github.com/graphframes/graphframes/issues/85>.

This is quite disappointing since Spark itself supports Python 3 and since
-- at least in my circles -- Python 3 adoption is reaching a tipping point.
All new Python projects at my company and at my friends' companies are
being written in Python 3.

Nick



ent-Proposals-td19268.html#none>
ie=UTF-8#q=holden+karau+pydata&start=0>
and dev@
ut
™ve been
or
 try and
on
PR
d
h about
ery loose term)
re what
r
a
around supporting custom
er
™t
.
nt set of
€ profile
€™m
ch is
k
h
or
VM
mplaint some people have,
et API
 with the
we
nd
id
 more
w
of work -
ow
e
 through
r
rk models
dd basic
nk
een develop in
 medium sized data
ng, but if
ten
ee Spark
issed important
g
d
œnormalâ€
e
t
 stuffed
&
"
kant kodali <kanth909@gmail.com>,"Wed, 12 Oct 2016 20:30:03 -0700",Re: Spark Improvement Proposals,Ryan Blue <rblue@netflix.com.invalid>,"Some of you guys may have already seen this but in case if you haven't you
may want to check it out.

http://www.slideshare.net/sbaltagi/flink-vs-spark




at
te
e
h
st
ho
s
in
:
ng
o
al
?
P.
:
e
o
he
ng
t
n
,
e them forward.
f
distinct
at
ct
e
e
h
he
er
to
s,
'm
t
t
ed
nd
e
o
n/
t
'm
a
e:
he
y
 a
f
er
p
no
y
so
y
e
on
ot
MO
n
's
e
ve
s
ly
r
en
y
n
s
ed
he
ly
e.
o
he
e
he
a
or
on
e
k
ey
he
to
s
 /
,
 I
-
f
re
te
te
.
y*
e
"
Reynold Xin <rxin@databricks.com>,"Wed, 12 Oct 2016 21:26:12 -0700",Mark DataFrame/Dataset APIs stable,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","I took a look at all the public APIs we expose in o.a.spark.sql tonight,
and realized we still have a large number of APIs that are marked
experimental. Most of these haven't really changed, except in 2.0 we merged
DataFrame and Dataset. I think it's long overdue to mark them stable.

I'm tracking this via ticket:
https://issues.apache.org/jira/browse/SPARK-17900

*The list I've come up with to graduate are*:

Dataset/DataFrame
- functions, since 1.3
- ColumnName, since 1.3
- DataFrameNaFunctions, since 1.3.1
- DataFrameStatFunctions, since 1.4
- UserDefinedFunction, since 1.3
- UserDefinedAggregateFunction, since 1.5
- Window and WindowSpec, since 1.4

Data sources:
- DataSourceRegister, since 1.5
- RelationProvider, since 1.3
- SchemaRelationProvider, since 1.3
- CreatableRelationProvider, since 1.3
- BaseRelation, since 1.3
- TableScan, since 1.3
- PrunedScan, since 1.3
- PrunedFilteredScan, since 1.3
- InsertableRelation, since 1.3


*The list I think we should definitely keep experimental are*:

- CatalystScan in data source (tied to internal logical plans so it is not
stable by definition)
- all classes related to Structured streaming (introduced new in 2.0 and
will likely change)


*The ones that I'm not sure whether we should graduate are:*

Typed operations for Datasets, including:
- all typed methods on Dataset class
- KeyValueGroupedDataset
- o.a.s.sql.expressions.javalang.typed
- o.a.s.sql.expressions.scalalang.typed
- methods that return typed Dataset in SparkSession

Most of these were introduced in 1.6 and had gone through drastic changes
in 2.0. I think we should try very hard not to break them any more, but we
might still run into issues in the future that require changing these.


Let me know what you think.
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 13 Oct 2016 00:07:33 -0700 (MST)",RE: Official Stance on Not Using Spark Submit,dev@spark.apache.org,"I actually not use spark submit for several use cases, all of them currently revolve around running it directly with python.
Basically I have am using pycharm and configure it with a remote interpreter which runs on the server while my pycharm runs on my local windows machine.
In order for me to be able to effectively debug (stepping etc.), I want to define a run configuration in pycharm which would integrate fully with its debug tools. Unfortunately I couldnâ€™t figure out a way to use spark-submit effectively. Instead I chose the following solution:
I defined the project to use the remorete interpreter running on the driver in the cluster.
I defined environment variables in the run configuration including setting PYTHONPATH to include pyspark and py4j manually, set up the relevant PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON, set up PYSPARK_SUBMIT_ARGS to include relevant configurations (e.g. relevant jars) and made sure it ended with pyspark-shell.

By providing this type of behavior I could debug spark remotely as if it was local.

Similar use cases include using standard tools that know how to run â€œpythonâ€ script but are not aware of spark-submit.

I havenâ€™t found similar reasons for scala/java code though (although I wish there was a similar â€œremoteâ€ setup for scala).
Assaf.


From: RussS [via Apache Spark Developers List] [mailto:ml-node+s1001551n19384h93@n3.nabble.com]
Sent: Monday, October 10, 2016 9:14 PM
To: Mendelson, Assaf
Subject: Re: Official Stance on Not Using Spark Submit

Just folks who don't want to use spark-submit, no real use-cases I've seen yet.

I didn't know about SparkLauncher myself and I don't think there are any official docs on that or launching spark as an embedded library for tests.

What are the main use cases you've seen for this? Maybe we can add a page to the docs about how to launch Spark as an embedded library.

Matei


I actually had not seen SparkLauncher before, that looks pretty great :)

I'm definitely only talking about non-embedded uses here as I also use embedded Spark (cassandra, and kafka) to run tests. This is almost always safe since everything is in the same JVM. It's only once we get to launching against a real distributed env do we end up with issues.

Since Pyspark uses spark submit in the java gateway i'm not sure if that matters :)

The cases I see are usually usually going through main directly, adding jars programatically.

Usually ends up with classpath errors (Spark not on the CP, their jar not on the CP, dependencies not on the cp),
conf errors (executors have the incorrect environment, executor classpath broken, not understanding spark-defaults won't do anything),
Jar version mismatches
Etc ...

I have also 'embedded' a Spark driver without much trouble. It isn't that it can't work.

The Launcher API is ptobably the recommended way to do that though. spark-submit is the way to go for non programmatic access.

If you're not doing one of those things and it is not working, yeah I think people would tell you you're on your own. I think that's consistent with all the JIRA discussions I have seen over time.

I've seen a variety of users attempting to work around using Spark Submit with at best middling levels of success. I think it would be helpful if the project had a clear statement that submitting an application without using Spark Submit is truly for experts only or is unsupported entirely.

I know this is a pretty strong stance and other people have had different experiences than me so please let me know what you think :)


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Official-Stance-on-Not-Using-Spark-Submit-tp19376p19384.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Official-Stance-on-Not-Using-Spark-Submit-tp19376p19430.html
om."
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 13 Oct 2016 01:10:33 -0700 (MST)","RE: Python Spark Improvements (forked from Spark Improvement
 Proposals)",dev@spark.apache.org,"Hi,
We are actually using pyspark heavily.
I agree with all of your points,  for me I see the following as the main hurdles:

1.       Pyspark does not have support for UDAF. We have had multiple needs for UDAF and needed to go to java/scala to support these. Having python UDAF would have made life much easier (especially at earlier stages when we prototype).

2.       Performance. I cannot stress this enough. Currently we have engineers who take python UDFs and convert them to scala UDFs for performance. We are currently even looking at writing UDFs and UDAFs in a more native way (e.g. using expressions) to improve performance but working with pyspark can be really problematic.

BTW, other than using jython or arrow, I believe there are a couple of other ways to get improve performance:

1.       Python provides tool to generate AST for python code (https://docs.python.org/2/library/ast.html). This means we can use the AST to construct scala code very similar to how expressions are build for native spark functions in scala. Of course doing full conversion is very hard but at least handling simple cases should be simple.

2.       The above would of course be limited if we use python packages but over time it is possible to add some ""translation"" tools (i.e. take python packages and find the appropriate scala equivalent. We can even provide this to the user to supply their own conversions thereby looking as a regular python code but being converted to scala code behind the scenes).

3.       In scala, it is possible to use codegen to actually generate code from a string. There is no reason why we can't write the expression in python and provide a scala string. This would mean learning some scala but would mean we do not have to create a separate code tree.

BTW, the fact that all of the tools to access java are marked as private has me a little worried. Nearly all of our UDFs (and all of our UDAFs) are written in scala for performance. The wrapping to provide them in python uses way too many private elements for my taste.


From: msukmanowsky [via Apache Spark Developers List] [mailto:ml-node+s1001551n19426h95@n3.nabble.com]
Sent: Thursday, October 13, 2016 3:51 AM
To: Mendelson, Assaf
Subject: Re: Python Spark Improvements (forked from Spark Improvement Proposals)

As very heavy Spark users at Parse.ly, I just wanted to give a +1 to all of the issues raised by Holden and Ricardo. I'm also giving a talk at PyCon Canada on PySpark https://2016.pycon.ca/en/schedule/096-mike-sukmanowsky/.

Being a Python shop, we were extremely pleased to learn about PySpark a few years ago as our main ETL pipeline used Apache Pig at the time. I was one of the only folks who understood Pig and Java so collaborating on this as a team was difficult.

Spark provided a means for the entire team to collaborate, but we've hit our fair share of issues all of which are enumerated in this thread.

Besides giving a +1 here, I think if I were to force rank these items for us, it'd be:

1. Configuration difficulties: we've lost literally weeks to troubleshooting memory issues for larger jobs. It took a long time to even understand *why* certain jobs were failing since Spark would just report executors being lost. Finally we tracked things down to understanding that spark.yarn.executor.memoryOverhead controls the portion of memory reserved for Python processes, but none of this is documented anywhere as far as I can tell. We discovered this via trial and error. Both documentation and better defaults for this setting when running a PySpark application are probably sufficient. We've also had a number of troubles with saving Parquet output as part of an ETL flow, but perhaps we'll save that for a blog post of its own.

2. Dependency management: I've tried to help move the conversation on https://issues.apache.org/jira/browse/SPARK-13587 but it seems we're a bit stalled. Installing the required dependencies for a PySpark application is a really messy ordeal right now.

3. Development workflow: I'd combine both ""incomprehensible error messages"" and ""
difficulty using PySpark from outside of spark-submit / pyspark shell"" here. When teaching PySpark to new users, I'm reminded of how much inside knowledge is needed to overcome esoteric errors. As one example is hitting ""PicklingError: Could not pickle object as excessively deep recursion required."" errors. New users often do something innocent like try to pickle a global logging object and hit this and begin the Google -> stackoverflow search to try to comprehend what's going on. You can lose days to errors like these and they completely kill the productivity flow and send you hunting for alternatives.

4. Speed/performance: we are trying to use DataFrame/DataSets where we can and do as much in Java as possible but when we do move to Python, we're well aware that we're about to take a hit on performance. We're very keen to see what Apache Arrow does for things here.

5. API difficulties: I agree that when coming from Python, you'd expect that you can do the same kinds of operations on DataFrames in Spark that you can with Pandas, but I personally haven't been too bothered by this. Maybe I'm more used to this situation from using other frameworks that have similar concepts but incompatible implementations.

We're big fans of PySpark and are happy to provide feedback and contribute wherever we can.
________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Python-Spark-Improvements-forked-from-Spark-Improvement-Proposals-tp19422p19426.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
aditya1702 <adityavyas17@gmail.com>,"Thu, 13 Oct 2016 03:14:53 -0700 (MST)",Regularized Logistic regression,dev@spark.apache.org,"Hello, I am trying to solve a problem using regularized logistic regression
in spark. I am using the model created by LogisticRegression():

lr=LogisticRegression(regParam=10.0,maxIter=10,standardization=True)
model=lr.fit(data_train_df)
data_predict_with_model=model.transform(data_test_df)

However I am not able to get proper results. Can anyone tell me whether we
have to pass any other parameters in the model?



--

---------------------------------------------------------------------


"
"""Anurag Verma"" <anurag.verma@fnmathlogic.com>","Thu, 13 Oct 2016 16:23:24 +0530",RE: Regularized Logistic regression,"""'aditya1702'"" <adityavyas17@gmail.com>,
	<dev@spark.apache.org>","Probably your regularization parameter is set too high. Try regParam=0.1/
0.2  Also you should probably increase the number to iteration to something
like 500. Additionally you can specify elasticNetParam (between 0 and 1).

in spark. I am using the model created by LogisticRegression():

lr=LogisticRegression(regParam=10.0,maxIter=10,standardization=True)
model=lr.fit(data_train_df)
data_predict_with_model=model.transform(data_test_df)

However I am not able to get proper results. Can anyone tell me whether we
have to pass any other parameters in the model?



--
View this message in context:
http://apache-spark-developers-list.1001551.n3.nabble.com/Regularized-Logist
ic-regression-tp19432.html
Nabble.com.

---------------------------------------------------------------------


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 13 Oct 2016 14:56:29 +0200","DAGScheduler.handleJobCancellation uses jobIdToStageIds for
 verification while jobIdToActiveJob for lookup?",dev <dev@spark.apache.org>,"Hi,

Is there a reason why DAGScheduler.handleJobCancellation checks the
active job id in jobIdToStageIds [1] while looking the job up in
jobIdToActiveJob [2]? Perhaps synchronized earlier yet still
inconsistent.

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1372
[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1376

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Thu, 13 Oct 2016 10:02:00 -0500","Re: DAGScheduler.handleJobCancellation uses jobIdToStageIds for
 verification while jobIdToActiveJob for lookup?","Jacek Laskowski <jacek@japila.pl>, Mark Hamstra <mark@clearstorydata.com>","Hi Jacek,

doesn't look like there is any good reason -- Mark Hamstra might know this
best.  Feel free to open a jira & pr for it, you can ping Mark, Kay
Ousterhout, and me (@squito) for review.

Imran


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 13 Oct 2016 17:11:37 +0200","Re: DAGScheduler.handleJobCancellation uses jobIdToStageIds for
 verification while jobIdToActiveJob for lookup?",Imran Rashid <irashid@cloudera.com>,"Thanks Imran! Not only did the response come so promptly, but also
it's something I could work on (and have another Spark contributor
badge unlocked)! Thanks.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Fred Reiss <freiss.oss@gmail.com>,"Thu, 13 Oct 2016 08:39:22 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"
Regarding stability, I've seen two levels of concrete requirements.

The first is ""don't bring down my Spark cluster"". That is to say,
regardless of the input data rate, Spark shouldn't thrash or crash
outright. Processing may lag behind the data arrival rate, but the cluster
should stay up and remain fully functional.

The second level is ""don't bring down my application"". A common use for
streaming systems is to handle heavyweight computations that are part of a
larger application, like a web application, a mobile app, or a plant
control system. For example, an online application for car insurance might
need to do some pretty involved machine learning to produce an accurate
quote and suggest good upsells to the customer. If the heavyweight portion
times out, the whole application times out, and you lose a customer.

In terms of bursty vs. non-bursty, the ""don't bring down my Spark cluster
case"" is more about handling bursts, while the ""don't bring down my
application"" case is more about delivering acceptable end-to-end response
times under typical load.

100-200 msec range, driven by the need to display a web page on a browser
or mobile device. Another group in the Internet of Things space mentioned
times ranging from 5 seconds to 30 seconds throughout the conversation. But
most people I've talked to have been pretty vague about specific numbers.

My impression is that these groups are not motivated by anxiety about
meeting a particular latency target for a particular application. Rather,
they want to make low latency the norm so that they can stop having to
think about latency. Today, low latency is a special requirement of special
applications. But that policy imposes a lot of hidden costs. IT architects
have to spend time estimating the latency requirements of every application
and lobbying for special treatment when those requirements are strict.
Managers have to spend time engineering business processes around latency.
Data scientists have to spend time packaging up models and negotiating how
those models will be shipped over to the low-latency serving tier. And
customers who are accustomed to Google and smartphones end up with an
experience that is functional but unsatisfying.

It's best to think of latency as a sliding scale. A given level of latency
imposes a given level of cost enterprise-wide. Someone who is making a
decision on middleware policy will balance this cost against other costs:
How much does it cost to deploy the middleware? How much does it cost to
train developers to use the system? The winner will be the system that
minimizes the overall cost.

Fred
"
Cody Koeninger <cody@koeninger.org>,"Thu, 13 Oct 2016 11:15:51 -0500","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Fred Reiss <freiss.oss@gmail.com>,"I've always been confused as to why it would ever be a good idea to
put any streaming query system on the critical path for synchronous  <
100msec requests.  It seems to make a lot more sense to have a
streaming system do asynch updates of a store that has better latency
and quality of service characteristics for multiple users.  Then your
only latency concerns are event to update, not request to response.


---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Thu, 13 Oct 2016 09:59:15 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Cody Koeninger <cody@koeninger.org>,"This is a thing I often have people ask me about, and then I do my best
dissuade them from using Spark in the ""hot path"" and it's normally
something which most people eventually accept. Fred might have more
information for people for whom this is a hard requirement though.



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Holden Karau <holden@pigscanfly.ca>,"Thu, 13 Oct 2016 11:29:01 -0700",Re: Python Spark Improvements (forked from Spark Improvement Proposals),"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Awesome, good points everyone. The ranking of the issues is super useful
and I'd also completely forgotten about the lack of built in UDAF support
which is rather important. There is a PR to make it easier to call/register
JVM UDFs from Python which will hopefully help a bit there too. I'm getting
on a flight to London for OSCON but I want to continueo encourage users to
chime in with their experiences (to that end I'm trying to re include user@
since it doesn't seem to have been posted there despite my initial attempt
to do so.)


s
ng
ols (i.e. take
as
).
ression in
t
e
on
.
s
n
t
on
g
le
w
n
ve
e
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
mprovements-forked-from-Spark-Improvement-Proposals-tp19422p19431.html>


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 13 Oct 2016 12:02:56 -0700","Re: DAGScheduler.handleJobCancellation uses jobIdToStageIds for
 verification while jobIdToActiveJob for lookup?",Jacek Laskowski <jacek@japila.pl>,"There were at least a couple of ideas behind the original thinking on using
both of those Maps: 1) We needed the ability to efficiently get from jobId
to both ActiveJob objects and to sets of associated Stages, and using both
Maps here was an opportunity to do a little sanity checking to make sure
that the Maps looked at least minimally consistent for the Job at issue; 2)
Similarly, it could serve as a kind of hierarchical check -- first, for the
Job which we are being asked to cancel, that we ever knew enough to even
register its existence; second, that for a JobId that passes the first
test, that we still have an ActiveJob that can be canceled.

Now, without doing a bunch of digging into the code archives, I can't tell
you for sure whether those ideas were ever implemented completely correctly
or whether they still make valid sense in the current code, but from
looking at the lines that you highlighted, I can tell you that even if the
ideas still make sense and are worth carrying forward, the current code
doesn't implement them correctly.  In particular, if it is possible for the
`jobId` to not be in `jobIdToActiveJob`, we're going to produce a
`NoSuchElementException` for the missing key instead of handling it or even
reporting it in a more useful way.


"
Aleksander Eskilson <aleksanderesk@gmail.com>,"Thu, 13 Oct 2016 19:35:03 +0000","DataFrameReader Schema Supersedes Schema Provided by Encoder, Renders
 Fields Nullable","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi there,

Working in the space of custom Encoders/ExpressionEncoders, I've noticed
that the StructType schema as set when creating an object of the
ExpressionEncoder[T] class [1] is not the schema actually used to set types
for the columns of a Dataset, as created by using the .as(encoder) method
[2] on read data. Instead, what occurs is that the schema is either
inferred through analysis of the data, or a schema can be provided using
the .schema(structType) method [3] of the DataFrameReader. However, when
using the .schema(..) method of DataFrameReader, potentially undesirable
behaviour occurs: while the DataSource is being resolved, all FieldTypes of
the a StructType schema have their nullability set to *true* (using the
asNullable function of StructTypes) [4] when the data is read from a local
file, as opposed to a non-streaming source.

Of course, allowing null-values where they shouldn't exist can weaken the
type-guarantees for DataSets over certain types of encoded data.

Thinking on how this might be resolved, first, if it's a legitimate bug,
I'm not sure why ""non-streaming file based"" datasources need to have their
StructFields all rendered nullable. Simply removing the call to asNullable
would fix the issue. Second, if it's actually necessary for most
filesystem-read data-sources to have their StructFields potentially
nullable in this manner, we could instead let the StructType schema
provided to the Encoder have the final say in the DataSet's schema.

This latter option seems sensible to me: if a client is willing to provide
a custom Encoder via the .as(..) method on the reader, presumably in
setting the schema field of the encoder they have some legitimate notion of
how their object's types should be mapped to DataSet column types. Any
failure when resolving their data to a DataSet by means of their Encoder
can then be traced to their Encoder for their own debugging.

Thoughts? Thanks,
Alek Eskilson

[1] -
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L213
[2] -
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L374
[3] -
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L62
[4] -
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L426
"
aditya1702 <adityavyas17@gmail.com>,"Thu, 13 Oct 2016 12:46:12 -0700 (MST)",RE: Regularized Logistic regression,dev@spark.apache.org,"Thank you Anurag Verma for replying. I tried increasing the iterations.
However I still get underfitted results. I am checking the model's
prediction by seeing how many pairs of labels and predictions it gets right

data_predict_with_model=best_model.transform(data_test_df)
final_pred_df=data_predict_with_model.select(col('label'),col('prediction'))
ans=final_pred_df.map(lambda x:((x[0],x[1]),1)).reduceByKey(lambda
a,b:a+b).toDF()
ans.show()

---------+---+
|       _1| _2|
+---------+---+
|[1.0,1.0]|  5|
|[0.0,1.0]| 12|
+---------+---+

Do you know any other methods by which I can check the model? and what is it
that I am doing wrong. I have filtered the data and arranged it in a
features and label column. So now only the model creation part is wrong I
guess. Can anyone help me please. I am still learning machine learning.



--

---------------------------------------------------------------------


"
aditya1702 <adityavyas17@gmail.com>,"Thu, 13 Oct 2016 12:58:19 -0700 (MST)",RE: Regularized Logistic regression,dev@spark.apache.org,"Ok so I tried setting the regParam and tried lowering it. how do I evaluate
which regParam is best. Do I have to to do it by trial and error. I am
currently calculating the log_loss for the model. Is it good to find the
best regparam value. here is my code:

from math import exp,log
#from pyspark.sql.functions import log
epsilon = 1e-16
def sigmoid_log_loss(w,x):
  ans=float(1/(1+exp(-(w.dot(x.features)))))
  if ans==0:
    ans=ans+epsilon
  if ans==1:
    ans=ans-epsilon
  log_loss=-((x.label)*log(ans)+(1-x.label)*log(1-ans))
  return ((ans,x.label),log_loss)

-------------------------------------------------------
reg=0.02
from pyspark.ml.classification import LogisticRegression
lr=LogisticRegression(regParam=reg,maxIter=500,standardization=True,elasticNetParam=0.5)
model=lr.fit(data_train_df)

w=model.coefficients
intercept=model.intercept
data_predicted_df=data_val_df.map(lambda x:(sigmoid_log_loss(w,x)))
log_loss=data_predicted_df.map(lambda x:x[1]).mean()
print log_loss



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 13 Oct 2016 14:04:47 -0700","Re: DataFrameReader Schema Supersedes Schema Provided by Encoder,
 Renders Fields Nullable",Aleksander Eskilson <aleksanderesk@gmail.com>,"There is a <https://issues.apache.org/jira/browse/SPARK-15192> lot
<https://github.com/apache/spark/pull/15329> of
<https://github.com/apache/spark/pull/14124> confusion
<https://github.com/apache/spark/pull/13873> around
<https://github.com/apache/spark/pull/11785> nullable
<https://issues.apache.org/jira/browse/SPARK-11319> in
<https://issues.apache.org/jira/browse/SPARK-11868> StructType
<https://issues.apache.org/jira/browse/SPARK-13740> and we should definitly
come up with a consistent story and make sure we have better
documentation.  This might even mean deprecating this field.  At a high
level, I think the key problem is that internally, nullable is used as an
optimization, not an enforcement mechanism.  This is a lot different than
NOT NULL in a traditional database. Specifically, we take ""nulllable =
false"" as a promise that that column will never be null and use that fact
to do optimizations like skipping null checks.  This means that if you lie
to us, you actually can get wrong answers (i.e. 0 instead of null).  This
is clearly confusing and not ideal.

A little bit of explanation for some of the specific cases you brought up:
the reason that we call asNullable on file sources is that even if that
column is never null in the data, there are cases that can still produce a
null result.  For example, when parsing JSON, we null out all columns other
than _corrupt_record when we fail to parse a line.  The fact that its
different in streaming is a bug.

Would you mind opening up a JIRA ticket, and we discuss the right path
forward there?


"
Seth Hendrickson <seth.hendrickson16@gmail.com>,"Thu, 13 Oct 2016 15:22:56 -0700",Re: Regularized Logistic regression,aditya1702 <adityavyas17@gmail.com>,"Spark MLlib provides a cross-validation toolkit for selecting
hyperparameters. I think you'll find the documentation quite helpful:

http://spark.apache.org/docs/latest/ml-tuning.html#example-model-selection-via-cross-validation

There is actually a python example for logistic regression there. If you
still have questions after reading it, then please post back again.

Hope that helps.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 13 Oct 2016 16:43:04 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Fred Reiss <freiss.oss@gmail.com>,"Thanks Fred for the detailed reply. The stability points are
especially interesting as a goal for the streaming component in Spark.
In terms of next steps, one approach that might be helpful is trying
to create benchmarks or situations that mimic real-life workloads and
then we can work on isolating specific changes that are required etc.
It'd also be great to hear other approaches / next steps to concretize
some of these goals.

Thanks
Shivaram


---------------------------------------------------------------------


"
aditya1702 <adityavyas17@gmail.com>,"Fri, 14 Oct 2016 05:38:24 -0700 (MST)",Re: Regularized Logistic regression,dev@spark.apache.org,"I used the cross validator tool for tuning the parameter. My code is here:

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import BinaryClassificationEvaluator
reg=100.0
lr=LogisticRegression(maxIter=500)

paramGrid = ParamGridBuilder().addGrid(lr.regParam,
[0.02,0.01,0.2,0.1,1.0,2.0,10.0,15.0,20.0,100.0]).addGrid(lr.elasticNetParam,
[0.0, 0.5, 1.0]).build()

crossval = CrossValidator(estimator=lr,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(),
                          numFolds=3)
model=crossval.fit(data_train_df)

And finally predicted the values:

prediction = model.transform(data_test_df)
prediction.show()
  
+-----+----------+
|label|prediction|
+-----+----------+
|  1.0|       1.0|
|  1.0|       1.0|
|  1.0|       1.0|
|  1.0|       1.0|
|  1.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
+-----+----------+

Why am  I getting the wrong predictions?



--

---------------------------------------------------------------------


"
mariusvniekerk <marius.v.niekerk@gmail.com>,"Fri, 14 Oct 2016 05:42:50 -0700 (MST)","Re: Python Spark Improvements (forked from Spark Improvement
 Proposals)",dev@spark.apache.org,"So for the jupyter integration pieces.

I've made a simple library ( https://github.com/MaxPoint/spylon
<https://github.com/MaxPoint/spylon>  ) which allows a simpler way of
creating a SparkContext (with all the parameters available to spark-submit)
as well as some usability enhancements, progress bars, tab completion for
spark configuration properties, easier loading of scala objects via py4j.



--

---------------------------------------------------------------------


"
Aleksander Eskilson <aleksanderesk@gmail.com>,"Fri, 14 Oct 2016 14:24:46 +0000","Re: DataFrameReader Schema Supersedes Schema Provided by Encoder,
 Renders Fields Nullable",Michael Armbrust <michael@databricks.com>,"Interesting. I'm quite glad to read your explanation, it makes some of our
work quite a bit more clear. I'll open a ticket in a similar vein to this
discussion: https://github.com/apache/spark/pull/11785, contrasting
nullability implementation as optimization and and enforcement.

Additionally, shall I go ahead and open a ticket pointing out the missing
call to .asNullable in the streaming reader?

Thanks,
Alek


There is a <https://issues.apache.org/jira/browse/SPARK-15192> lot
<https://github.com/apache/spark/pull/15329> of
<https://github.com/apache/spark/pull/14124> confusion
<https://github.com/apache/spark/pull/13873> around
<https://github.com/apache/spark/pull/11785> nullable
<https://issues.apache.org/jira/browse/SPARK-11319> in
<https://issues.apache.org/jira/browse/SPARK-11868> StructType
<https://issues.apache.org/jira/browse/SPARK-13740> and we should definitly
come up with a consistent story and make sure we have better
documentation.  This might even mean deprecating this field.  At a high
level, I think the key problem is that internally, nullable is used as an
optimization, not an enforcement mechanism.  This is a lot different than
NOT NULL in a traditional database. Specifically, we take ""nulllable =
false"" as a promise that that column will never be null and use that fact
to do optimizations like skipping null checks.  This means that if you lie
to us, you actually can get wrong answers (i.e. 0 instead of null).  This
is clearly confusing and not ideal.

A little bit of explanation for some of the specific cases you brought up:
the reason that we call asNullable on file sources is that even if that
column is never null in the data, there are cases that can still produce a
null result.  For example, when parsing JSON, we null out all columns other
than _corrupt_record when we fail to parse a line.  The fact that its
different in streaming is a bug.

Would you mind opening up a JIRA ticket, and we discuss the right path
forward there?


Hi there,

Working in the space of custom Encoders/ExpressionEncoders, I've noticed
that the StructType schema as set when creating an object of the
ExpressionEncoder[T] class [1] is not the schema actually used to set types
for the columns of a Dataset, as created by using the .as(encoder) method
[2] on read data. Instead, what occurs is that the schema is either
inferred through analysis of the data, or a schema can be provided using
the .schema(structType) method [3] of the DataFrameReader. However, when
using the .schema(..) method of DataFrameReader, potentially undesirable
behaviour occurs: while the DataSource is being resolved, all FieldTypes of
the a StructType schema have their nullability set to *true* (using the
asNullable function of StructTypes) [4] when the data is read from a local
file, as opposed to a non-streaming source.

Of course, allowing null-values where they shouldn't exist can weaken the
type-guarantees for DataSets over certain types of encoded data.

Thinking on how this might be resolved, first, if it's a legitimate bug,
I'm not sure why ""non-streaming file based"" datasources need to have their
StructFields all rendered nullable. Simply removing the call to asNullable
would fix the issue. Second, if it's actually necessary for most
filesystem-read data-sources to have their StructFields potentially
nullable in this manner, we could instead let the StructType schema
provided to the Encoder have the final say in the DataSet's schema.

This latter option seems sensible to me: if a client is willing to provide
a custom Encoder via the .as(..) method on the reader, presumably in
setting the schema field of the encoder they have some legitimate notion of
how their object's types should be mapped to DataSet column types. Any
failure when resolving their data to a DataSet by means of their Encoder
can then be traced to their Encoder for their own debugging.

Thoughts? Thanks,
Alek Eskilson

[1] -
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L213
[2] -
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L374
[3] -
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L62
[4] -
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L426
"
Aleksander Eskilson <aleksanderesk@gmail.com>,"Fri, 14 Oct 2016 15:10:52 +0000","Re: DataFrameReader Schema Supersedes Schema Provided by Encoder,
 Renders Fields Nullable",Michael Armbrust <michael@databricks.com>,"I've opened a Jira to the issue you requested earlier:
https://issues.apache.org/jira/browse/SPARK-17939



"
Fred Reiss <freiss.oss@gmail.com>,"Fri, 14 Oct 2016 11:57:33 -0700","Re: StructuredStreaming Custom Sinks (motivated by Structured
 Streaming Machine Learning)",Holden Karau <holden@pigscanfly.ca>,"I think the way I phrased things earlier may be leading to some confusion
here. When I said ""don't bring down my application"", I was referring to the
application not meeting its end-to-end SLA, not to the app server crashing.

The groups I've talked to already isolate their front-end systems from
back-end systems with multiple solutions like message queues and key-value
stores. But some applications require a complex automated decision based on
information that is not available until the moment of the decision. Some
good examples are credit decisions, decisions about whether to blacklist a
hostile IP address, and product recommendations based on the user's
location and the link they just clicked. In all these cases, there is an
action that needs to happen in the real world, and that action has a
deadline, and you need to score a model to meet that deadline.

In a typical enterprise IT environment, the analytics tier is a much more
convenient place to run compute- and memory-intensive scoring tasks. The
hardware, software, and toolchain are tuned for the workload, and the data
science department has much more administrative control. So the question
naturally comes up: ""Can I score my [credit/security/recommender/...]
models on the same infrastructure that I use to build them?""

Fred




"
Michael Armbrust <michael@databricks.com>,"Fri, 14 Oct 2016 12:00:50 -0700","Re: DataFrameReader Schema Supersedes Schema Provided by Encoder,
 Renders Fields Nullable",Aleksander Eskilson <aleksanderesk@gmail.com>,"
Yes please! This probably affects correctness.
"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Oct 2016 13:39:31 -0700",cutting 1.6.3 release candidate,"""dev@spark.apache.org"" <dev@spark.apache.org>","It's been a while and we have fixed a few bugs in branch-1.6. I plan to cut
rc1 for 1.6.3 next week (just in time for Spark Summit Europe). Let me know
if there are specific issues that should be addressed before that. Thanks.
"
Alexander Pivovarov <apivovarov@gmail.com>,"Fri, 14 Oct 2016 13:57:36 -0700",Re: cutting 1.6.3 release candidate,Reynold Xin <rxin@databricks.com>,"Hi Reynold

Spark 1.6.x has serious bug related to shuffle functionality
https://issues.apache.org/jira/browse/SPARK-14560
https://issues.apache.org/jira/browse/SPARK-4452

Shuffle throws OOM on serious load. I've seen this error several times on
my heavy jobs

java.lang.OutOfMemoryError: Unable to acquire 75 bytes of memory, got 0
        at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:120)
        at org.apache.spark.shuffle.sort.ShuffleExternalSorter.acquireNewPageIfNecessary(ShuffleExternalSorter.java:346)


It was fixed in both spark-2.0.0 and spark-1.6.x BUT spark-1.6 fix was NOT
merged - https://github.com/apache/spark/pull/13027

Is it possible to include the fix to spark-1.6.3?


Thank you
Alex



"
Alexander Pivovarov <apivovarov@gmail.com>,"Fri, 14 Oct 2016 14:01:42 -0700",Re: cutting 1.6.3 release candidate,Reynold Xin <rxin@databricks.com>,"Also, can you include MaxPermSize fix to spark-1.6.3?
https://issues.apache.org/jira/browse/SPARK-15067
Literally, just 1 word should be replaced
https://github.com/apache/spark/pull/12985/files




"
Ethan Aubin <ethan.aubin@gmail.com>,"Fri, 14 Oct 2016 19:33:11 -0400",source for org.spark-project.hive:1.2.1.spark2,dev@spark.apache.org,"In an email thread [1] from Aug 2015, it was mentioned that the source
to org.spark-project.hive was at
https://github.com/pwendell/hive/commits/release-1.2.1-spark .
That branch has a 1.2.1.spark version but spark 2.0.1 uses
1.2.1.spark2. Could anyone point me to the repo for 1.2.1.spark2?
Thanks --Ethan

[https://mail-archives.apache.org/mod_mbox/spark-dev/201508.mbox/%3CA0AA8B38-DEEE-476A-93FF-92FEAD06E404@hortonworks.com%3E]

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Oct 2016 16:32:55 -0700",Re: cutting 1.6.3 release candidate,Alexander Pivovarov <apivovarov@gmail.com>,"I took a look at the pull request for memory management and I actually
agree with the existing assessment that the patch is too big and risky to
port into an existing maintenance branch. Things that are backported are
low-risk patches that won't break existing applications on 1.6.x. This
patch is large, invasive, directly hooks into the very internals of Spark.
The chance of it breaking an existing working 1.6.x application is not low.







"
Ryan Blue <rblue@netflix.com.INVALID>,"Fri, 14 Oct 2016 17:28:39 -0700",Re: source for org.spark-project.hive:1.2.1.spark2,Ethan Aubin <ethan.aubin@gmail.com>,"The Spark 2 branch is based on this one:
https://github.com/JoshRosen/hive/commits/release-1.2.1-spark2

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
roehst <ro.stevaux@gmail.com>,"Fri, 14 Oct 2016 17:38:32 -0700 (MST)",,dev@spark.apache.org,"Hi, I sometimes write convenience methods for pre-processing data frames, and
I wonder if it makes sense to make a contribution -- should this be included
in Spark or supplied as Spark Packages/3rd party libraries?

Example:

Get all fields in a DataFrame schema of a certain type.

I end up writing something like getFieldsByDataType(dataFrame: DataFrame,
dataType: DataType): List[StructField] and may be adding that to the Schema
class with implicits. Something like:

dataFrame.schema.fields.filter(_.dataType == dataType)

Should the fields variable in the Schema class contain a method like
""filterByDataType"" so we can write:

dataFrame.getFieldsByDataType(StringType)?

Is it useful? Is it too bloated? Would that be acceptable? That is a small
contribution that a junior developer might be able to write, for example.
This adds more code, but may be makes the library more user friendly (not
that it is not user friendly).

Just want to hear your thoughts on this question.

Thanks,
Rodrigo



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Oct 2016 19:18:14 -0700",,roehst <ro.stevaux@gmail.com>,"It is very difficult to give a general answer. We would need to discuss
each case. In general things that are trivially doable using existing APIs,
it is not a good idea to provide them, unless for compatibility with other
frameworks (e.g. Pandas).


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 15 Oct 2016 12:27:37 +0000",Re: source for org.spark-project.hive:1.2.1.spark2,Ryan Blue <rblue@netflix.com.INVALID>,"

The Spark 2 branch is based on this one: https://github.com/JoshRosen/hive/commits/release-1.2.1-spark2


Didn't know this had moved.... I had an outstanding PR against patricks which should really go in, if not already taken up ( HIVE-11720 ; https://github.com/pwendell/hive/pull/2 )


IMO I think it would make sense if -somehow- that hive fork were in the ASF; it's got to be in sync with Spark releases, and its not been ideal for me in terms of getting one or two fixes in, the other one being culling
groovy 2.4.4 as an export ( https://github.com/steveloughran/hive/tree/stevel/SPARK-13471-groovy-2.4.4 )

I don't know if the hive team themselves would be up to having it in their repo, or if committership logistics would suit it anyway. Otherwise, approaching infra@ and asking for a forked repo is likely to work with a bit of prodding

rb

In an email thread [1] from Aug 2015, it was mentioned that the source
to org.spark-project.hive was at
https://github.com/pwendell/hive/commits/release-1.2.1-spark .
That branch has a 1.2.1.spark version but spark 2.0.1 uses
1.2.1.spark2. Could anyone point me to the repo for 1.2.1.spark2?
Thanks --Ethan

[https://mail-archives.apache.org/mod_mbox/spark-dev/201508.mbox/%3CA0AA8B38-DEEE-476A-93FF-92FEAD06E404@hortonworks.com%3E]

---------------------------------------------------------------------
ibe@spark.apache.org>




--
Ryan Blue
Software Engineer
Netflix

"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sat, 15 Oct 2016 19:20:34 -0700 (MST)","Why the json file used by sparkSession.read.json must be a valid
 json object per line",dev@spark.apache.org,"Hi devs:
   I'm doubt about the design of spark.read.json,  why the json file is not
a standard json file, who can tell me the internal reason. Any advice is
appreciated.



--

---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sun, 16 Oct 2016 12:52:25 +0900","Re: Why the json file used by sparkSession.read.json must be a valid
 json object per line",WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Hi,


The reason is just simply JSON data source depends on Hadoop's
LineRecordReader when we first try to read the files.

There is a workaround for this here in this link,
http://searchdatascience.com/spark-adventures-1-processing-multi-line-json-files/

I hope this is helpful.


Thanks!


2016-10-16 11:20 GMT+09:00 WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>:

"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sun, 16 Oct 2016 01:24:02 -0700 (MST)","Re: Why the json file used by sparkSession.read.json must be a
 valid json object per line",dev@spark.apache.org,"Thank you very much! I will have a look about your link.




--

---------------------------------------------------------------------


"
trsell@gmail.com,"Sun, 16 Oct 2016 10:50:05 +0000","Re: Why the json file used by sparkSession.read.json must be a valid
 json object per line","WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>, dev@spark.apache.org","Think of it as jsonl instead of a json file.
Point people at this if they need an official looking spec:
http://jsonlines.org/

This make it work well with standard unix tools in pipes.



"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sun, 16 Oct 2016 04:37:09 -0700 (MST)","Re: Why the json file used by sparkSession.read.json must be a
 valid json object per line",dev@spark.apache.org,"thank youï¼
 But I think is's user unfriendly to process standard json file with
DataFrame. Need we provide a new overrided method to do this?



--
3.nabble.com/Why-the-json-file-used-by-sparkSession-read-json-must-be-a-valid-json-object-per-line-tp19464p19468.html
om.

---------------------------------------------------------------------


"
Dean Wampler <deanwampler@gmail.com>,"Sun, 16 Oct 2016 13:11:17 -0500",Re: Apache Spark chat channel,"""dev@spark.apache.org"" <dev@spark.apache.org>","Okay, here is a Gitter room for this purpose:

https://gitter.im/spark-scala/Lobby

If you use the APIs, please join and help those who are learning. I can't
answer every question.

dean

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Lightbend <http://lightbend.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
=?utf-8?B?VG9tYXN6IEdhd8SZZGE=?= <tomasz.gaweda@outlook.com>,"Sun, 16 Oct 2016 20:30:35 +0000",Re: Spark Improvement Proposals,"""dev@spark.apache.org"" <dev@spark.apache.org>, Cody Koeninger
	<cody@koeninger.org>","Hi everyone,

I'm quite late with my answer, but I think my suggestions may help a 
little bit. :) Many technical and organizational topics were mentioned, 
but I want to focus on these negative posts about Spark and about ""haters""

I really like Spark. Easy of use, speed, very good community - it's 
everything here. But Every project has to ""flight"" on ""framework market"" 
to be still no 1. I'm following many Spark and Big Data communities, 
maybe my mail will inspire someone :)

You (every Spark developer; so far I didn't have enough time to join 
contributing to Spark) has done excellent job. So why are some people 
saying that Flink (or other framework) is better, like it was posted in 
this mailing list? No, not because that framework is better in all 
cases.. In my opinion, many of these discussions where started after 
Flink marketing-like posts. Please look at StackOverflow ""Flink vs ...."" 
posts, almost every post in ""winned"" by Flink. Answers are sometimes 
saying nothing about other frameworks, Flink's users (often PMC's) are 
just posting same information about real-time streaming, about delta 
iterations, etc. It look smart and very often it is marked as an aswer, 
even if - in my opinion - there wasn't told all the truth.


My suggestion: I don't have enough money and knowledgle to perform huge 
performance test. Maybe some company, that supports Spark (Databricks, 
Cloudera? - just saying you're most visible in community :) ) could 
perform performance test of:

- streaming engine - probably Spark will loose because of mini-batch 
model, however currently the difference should be much lower that in 
previous versions

- Machine Learning models

- batch jobs

- Graph jobs

- SQL queries

People will see that Spark is envolving and is also a modern framework, 
because after reading posts mentioned above people may think ""it is 
outdated, future is in framework X"".

Matei Zaharia posted excellent blog post about how Spark Structured 
Streaming beats every other framework in terms of easy-of-use and 
reliability. Performance tests, done in various environments (in 
example: laptop, small 2 node cluster, 10-node cluster, 20-node 
cluster), could be also very good marketing stuff to say ""hey, you're 
telling that you're better, but Spark is still faster and is still 
getting even more fast!"". This would be based on facts (just numbers), 
not opinions. It would be good for companies, for marketing puproses and 
for every Spark developer


Second: real-time streaming. I've written some time ago about real-time 
streaming support in Spark Structured Streaming. Some work should be 
done to make SSS more low-latency, but I think it's possible. Maybe 
Spark may look at Gearpump, which is also built on top of Akka? I don't 
know yet, it is good topic for SIP. However I think that Spark should 
have real-time streaming support. Currently I see many posts/comments 
that ""Spark has too big latency"". Spark Streaming is doing very good 
jobs with micro-batches, however I think it is possible to add also more 
real-time processing.

Other people said much more and I agree with proposal of SIP. I'm also 
happy that PMC's are not saying that they will not listen to users, but 
they really want to make Spark better for every user.


What do you think about these two topics? Especially I'm looking at Cody 
(who has started this topic) and PMCs :)

Pozdrawiam / Best regards,

Tomasz


W dniu 2016-10-07 o 04:51, Cody Koeninger pisze:
> I love Spark.  3 or 4 years ago it was the first distributed computing
> environment that felt usable, and the community was welcoming.
>
> But I just got back from the Reactive Summit, and this is what I observed:
>
> - Industry leaders on stage making fun of Spark's streaming model
> - Open source project leaders saying they looked at Spark's governance
> as a model to avoid
> - Users saying they chose Flink because it was technically superior
> and they couldn't get any answers on the Spark mailing lists
>
> Whether you agree with the substance of any of this, when this stuff
> gets repeated enough people will believe it.
>
> Right now Spark is suffering from its own success, and I think
> something needs to change.
>
> - We need a clear process for planning significant changes to the codebase.
> I'm not saying you need to adopt Kafka Improvement Proposals exactly,
> but you need a documented process with a clear outcome (e.g. a vote).
> Passing around google docs after an implementation has largely been
> decided on doesn't cut it.
>
> - All technical communication needs to be public.
> Things getting decided in private chat, or when 1/3 of the committers
> work for the same company and can just talk to each other...
> Yes, it's convenient, but it's ultimately detrimental to the health of
> the project.
> The way structured streaming has played out has shown that there are
> significant technical blind spots (myself included).
> One way to address that is to get the people who have domain knowledge
> involved, and listen to them.
>
> - We need more committers, and more committer diversity.
> Per committer there are, what, more than 20 contributors and 10 new
> jira tickets a month?  It's too much.
> There are people (I am _not_ referring to myself) who have been around
> for years, contributed thousands of lines of code, helped educate the
> public around Spark... and yet are never going to be voted in.
>
> - We need a clear process for managing volunteer work.
> Too many tickets sit around unowned, unclosed, uncertain.
> If someone proposed something and it isn't up to snuff, tell them and
> close it.  It may be blunt, but it's clearer than ""silent no"".
> If someone wants to work on something, let them own the ticket and set
> a deadline. If they don't meet it, close it or reassign it.
>
> This is not me putting on an Apache Bureaucracy hat.  This is me
> saying, as a fellow hacker and loyal dissenter, something is wrong
> with the culture and process.
>
> Please, let's change it.
>
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
>
"
Debasish Das <debasish.das83@gmail.com>,"Sun, 16 Oct 2016 19:21:10 -0700",Re: Spark Improvement Proposals,=?UTF-8?Q?Tomasz_Gaw=C4=99da?= <tomasz.gaweda@outlook.com>,"Thanks Cody for bringing up a valid point...I picked up Spark in 2014 as
soon as I looked into it since compared to writing Java map-reduce and
Cascading code, Spark made writing distributed code fun...But now as we
went deeper with Spark and real-time streaming use-case gets more
prominent, I think it is time to bring a messaging model in conjunction
with the batch/micro-batch API that Spark is good at....akka-streams close
integration with spark micro-batching APIs looks like a great direction to
stay in the game with Apache Flink...Spark 2.0 integrated streaming with
batch with the assumption is that micro-batching is sufficient to run SQL
commands on stream but do we really have time to do SQL processing at
streaming data within 1-2 seconds ?

After reading the email chain, I started to look into Flink documentation
and if you compare it with Spark documentation, I think we have major work
to do detailing out Spark internals so that more people from community
start to take active role in improving the issues so that Spark stays
strong compared to Flink.

https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals

https://cwiki.apache.org/confluence/display/FLINK/Flink+Internals

Spark is no longer an engine that works for micro-batch and batch...We (and
I am sure many others) are pushing spark as an engine for stream and query
processing.....we need to make it a state-of-the-art engine for high speed
streaming data and user queries as well !

com>

""
"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 17 Oct 2016 02:07:34 +0000",Re: Python Spark Improvements (forked from Spark Improvement Proposals),Nicholas Chammas <nicholas.chammas@gmail.com>,"Thanks Holden to start this thread. I agree that spark devs should put more
efforts on pyspark but the reality is that we are a little slow on this
perspective . Since pyspark is to integrate spark into python, so I think
the focus is on the usability of pyspark. We should hear more feedback from
python community. And lots of data scientists love python, if we want to
more adoption of spark, then we should spend more time on pyspark.


I'd add one item to this list: The lack of Python 3 support in Spark
Packages <https://github.com/databricks/sbt-spark-package/issues/26>. This
means that great packages like GraphFrames cannot be used with Python 3
<https://github.com/graphframes/graphframes/issues/85>.

This is quite disappointing since Spark itself supports Python 3 and since
-- at least in my circles -- Python 3 adoption is reaching a tipping point.
All new Python projects at my company and at my friends' companies are
being written in Python 3.

Nick



Hi Spark Devs & Users,


Forking off from Codyâ€™s original thread
<http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Improvement-Proposals-td19268.html#none>
of Spark Improvements, and Matei's follow up on asking what issues the
Python community was facing with Spark, I think it would be useful for us
to discuss some of the motivations behind some of the Python community
looking at different technologies to replace Apache Spark with. My
viewpoints are based that of a developer who works on Apache Spark
day-to-day <http://bit.ly/hkspmg>, but also gives a fair number of talks at
Python conferences
<https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=holden+karau+pydata&start=0>
and I feel many (but not all) of the same challenges as the Python
community does trying to use Spark. Iâ€™ve included both the user@ and dev@
lists on this one since I think the user community can probably provide
more reasons why they have difficulty with PySpark. I should also point out
- the solution for all of these things may not live inside of the Spark
project itself, but it still impacts our usability as a whole.


   -

   Lack of pip installability

This is one of the points that Matei mentioned, and it something several
people have tried to provide for Spark in one way or another. It seems
getting reviewer time for this issue is rather challenging, and Iâ€™ve been
hesitant to ask the contributors to keep updating their PRs (as much as I
want to see some progress) because I just don't know if we have the time or
interest in this. Iâ€™m happy to pick up the latest from Juliet and try and
carry it over the finish line if we can find some committer time to work on
this since it now sounds like there is consensus we should do this.

   -

   Difficulty using PySpark from outside of spark-submit / pyspark shell

The FindSpark <https://pypi.python.org/pypi/findspark> package needing to
exist is one of the clearest examples of this challenge. There is also a PR
to make it easier for other shells to extend the Spark shell, and we ran
into some similar challenges while working on Sparkling Pandas. This could
be solved by making Spark pip installable so I won'tâ€™ say too much about
this point.

   -

   Minimal integration with IPython/IJupyter

This one is awkward since one of the areas that some of the larger
commercial players work in effectively â€œcompetesâ€ (in a very loose term)
with any features introduced around here. Iâ€™m not really super sure what
the best path forward is here, but I think collaborating with the IJupyter
people to enable more features found in the commercial offerings in open
source could be beneficial to everyone in the community, and maybe even
reduce the maintenance cost for some of the commercial entities. I
understand this is a tricky issue but having good progress indicators or
something similar could make a huge difference. (Note that Apache Toree
<https://toree.incubator.apache.org/> [Incubating] exists for Scala users
but hopefully the PySpark IJupyter integration could be achieved without a
new kernel).

   -

   Lack of virtualenv and or Python package distribution support

This one is also tricky since many commercial providers have their own
â€œsolutionâ€ to this, but there isnâ€™t a good story around supporting custom
virtual envs or user required Python packages. While spark-packages _can_
be Python this requires that the Python package developer go through rather
a lot of work to make their package available and realistically wonâ€™t
happen for most Python packages people want to use. And to be fair, the
addFiles mechanism does support Python eggs which works for some packages.
There are some outstanding PRs around this issue (and I understand these
are perhaps large issues which might require large changes to the current
suggested implementations - Iâ€™ve had difficulty keeping the current set of
open PRs around this straight in my own head) but there seems to be no
committer bandwidth or interest on working with the contributors who have
suggested these things. Is this an intentional decision or is this
something we as a community are willing to work on/tackle?

   -

   Speed/performance

This is often a complaint I hear from more â€œdata engineeringâ€ profile users
who are working in Python. These problems come mostly in places involving
the interaction of Python and the JVM (so UDFs, transformations with
arbitrary lambdas, collect() and toPandas()). This is an area Iâ€™m working
on (see https://github.com/apache/spark/pull/13571 ) and hopefully we can
start investigating Apache Arrow <https://arrow.apache.org/> to speed up
the bridge (or something similar) once itâ€™s a bit more ready (currently
Arrow just released 0.1 which is exciting). We also probably need to start
measuring these things more closely since otherwise random regressions will
continue to be introduced (like the challenge with unbalanced partitions
and block serialization together - see SPARK-17817
<https://issues.apache.org/jira/browse/SPARK-17817> which fixed this)

   -

   Configuration difficulties (especially related to OOMs)

This is a general challenge many people face working in Spark, but PySpark
users are also asked to somehow figure out what the correct amount of
memory is to give to the Python process versus the Scala/JVM processes.
This was maybe an acceptable solution at the start, but when combined with
the difficult to understand error messages it can become quite the time
sink. A quick work around would be picking a different default overhead for
applications using Python, but more generally hopefully some shared off-JVM
heap solution could also help reduce this challenge in the future.

   -

   API difficulties

The Spark API doesnâ€™t â€œfeelâ€ very Pythony is a complaint some people have,
but I think weâ€™ve done some excellent work in the DataFrame/Dataset API
here. At the same time weâ€™ve made some really frustrating choices with the
DataFrame API (e.g. removing map from DataFrames pre-emptively even when we
have no concrete plans to bring the Dataset API to PySpark).

A lot of users wish that our DataFrame API was more like the Pandas API
(and Wes has pointed out on some JIRAs where we have differences) as well
as covered more of the functionality of Pandas. This is a hard problem, and
it the solution might not belong inside of PySpark itself (Juliet and I did
some proof-of-concept work back in the day on Sparkling Pandas
<https://github.com/sparklingpandas/sparklingpandas>) - but since one of my
personal goals has been trying to become a committer Iâ€™ve been more focused
on contributing to Spark itself rather than libraries and very few people
seem to be interested in working on this project [although I still have
potential users ask if they can use it]. (Of course if there is sufficient
interest to reboot Sparkling Pandas or something similar that would be an
interesting area of work - but itâ€™s also a huge area of work - if you look
at Dask <http://dask.pydata.org/>, a good portion of the work is dedicated
just to supporting pandas like operations).

   -

   Incomprehensible error messages

I often have people ask me how to debug PySpark and they often have a
certain haunted look in their eyes while they ask me this (slightly
joking). More seriously, we really need to provide more guidance around how
to understand PySpark error messages and look at figuring out if there are
places where we can improve the messaging so users arenâ€™t hunting through
stack overflow trying to figure out where the Java exception they are
getting is related to their Python code. In one talk I gave recently
someone mentioned PySpark was the motivation behind finding the hide error
messages plugin/settings for IJupyter.

   -

   Lack of useful ML model & pipeline export/import

This is something weâ€™ve made great progress on, many of the PySpark models
are now able to use the underlying export mechanisms from Java. However I
often hear challenges with using these models in the rest of the Python
space once they have been exported from Spark. Iâ€™ve got a PR to add basic
PMML export in Scala to ML (which we can then bring to Python), but I think
the Python community is open to other formats if the Spark community
doesnâ€™t want to go the PMML route.

Now I donâ€™t think we will see the same challenges weâ€™ve seen develop in the
R community, but I suspect purely Python approaches to distributed systems
will continue to eat the â€œlow endâ€ of Spark (e.g. medium sized data
problems requiring parallelism). This isnâ€™t necessarily a bad thing, but if
there is anything Iâ€™ve learnt it that's the ""low end"" solution often
quickly eats the ""high end"" within a few years - and Iâ€™d rather see Spark
continue to thrive outside of the pure JVM space.

These are just the biggest issues that I hear come up commonly and
remembered on my flight back - itâ€™s quite possible Iâ€™ve missed important
things. I know contributing on a mailing list can be scary or intimidating
for new users (and even experienced developers may wish to stay out of
discussions they view as heated) - but I strongly encourage everyone to
participate (respectfully) in this thread and we can all work together to
help Spark continue to be the place where people from different languages
and backgrounds continue to come together to collaborate.


I want to be clear as well, while I feel these are all very important
issues (and being someone who has worked on PySpark & Spark for years
<http://bit.ly/hkspmg> without being a committer I may sometimes come off
as frustrated when I talk about these) I think PySpark as a whole is a
really excellent application and we do some really awesome stuff with it.
There are also things that I will be blind to as a result of having worked
on Spark for so long (for example yesterday I caught myself using the _
syntax in a Scala example without explaining it because it seems â€œnormalâ€
to me but often trips up new comers.) If we can address even some of these
issues I believe it will be a huge win for Spark adoption outside of the
traditional JVM space (and as the various community surveys continue to
indicate PySpark usage is already quite high).

Normally Iâ€™d bring in a box of timbits
<https://en.wikipedia.org/wiki/Timbits>/doughnuts or something if we were
having an in-person meeting for this but all I can do for the mailing list
is attach a cute picture and offer future doughnuts/coffee if people want
to chat IRL. So, in closing, Iâ€™ve included a picture of two of my stuffed
animals working on Spark on my flight back from a Python Data conference &
Spark meetup just to remind everyone that this is just a software project
and we can be friendly nice people if we try and things will be much more
awesome if we do :)
[image: image.png]

-- 
Cell : 425-233-8271 <(425)%20233-8271>
Twitter: https://twitter.com/holdenkarau




-- 
Best Regards

Jeff Zhang
"
Reynold Xin <rxin@databricks.com>,"Sun, 16 Oct 2016 22:28:36 -0700",cutting 2.0.2?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Since 2.0.1, there have been a number of correctness fixes as well as some
nice improvements to the experimental structured streaming (notably basic
Kafka support). I'm thinking about cutting 2.0.2 later this week, before
Spark Summit Europe. Let me know if there are specific things (bug fixes)
you really want to merge into branch-2.0.

Cheers.
"
=?utf-8?B?YW5keW1odWFuZyjpu4TmmI4p?= <andymhuang@tencent.com>,"Mon, 17 Oct 2016 07:01:01 +0000",Re: Spark Improvement Proposals(Internet mail),"=?utf-8?B?VG9tYXN6IEdhd8SZZGE=?= <tomasz.gaweda@outlook.com>, Debasish Das
	<debasish.das83@gmail.com>","Thereâ€™s no need to compare to Flinkâ€™s Streaming Model. Spark should focus more on how to go beyond itself.


From the beginning, Sparkâ€™s success comes from itâ€™s unified model can satisfiy SQLï¼ŒStreaming, Machine Learning Models and Graphs Jobs â€¦â€¦ all in One.  But From 1.6 to 2.0, the abstraction from RDD to DataFrame make no contribution to these two important areas (ML & Graph) with any substantial progress. Most things is for SQL and Streaming, which make Spark have to face the competition with Flink. But guys, these is not surposed to be the battle what Spark should face.


SIP is a good start. Voice from technical communication should be heard and accepted, not buried in the PR bodies. Nowadays, Spark donâ€™t lack of committers or contributors. The right direction and focus area, will decide where it goes, what competitor it encounter, and finally what it can be.

---------------
Sincerely
Andy

 åŽŸå§‹é‚®ä»¶
å‘ä»¶äºº: Debasish Das<debasish.das83@gmail.com>
æ”¶ä»¶äºº: Tomasz GawÄ™da<tomasz.gaweda@outlook.com>
æŠ„é€: dev@spark.apache.org<dev@spark.apache.org>; Cody Koeninger<cody@koeninger.org>
å‘é€æ—¶é—´: 2016å¹´10æœˆ17æ—¥(å‘¨ä¸€)â€‡10:21
ä¸»é¢˜: Re: Spark Improvement Proposals(Internet mail)

Thanks Cody for bringing up a valid point...I picked up Spark in 2014 as soon as I looked into it since compared to writing Java map-reduce and Cascading code, Spark made writing distributed code fun...But now as we went deeper with Spark and real-time streaming use-case gets more prominent, I think it is time to bring a messaging model in conjunction with the batch/micro-batch API that Spark is good at....akka-streams close integration with spark micro-batching APIs looks like a great direction to stay in the game with Apache Flink...Spark 2.0 integrated streaming with batch with the assumption is that micro-batching is sufficient to run SQL commands on stream but do we really have time to do SQL processing at streaming data within 1-2 seconds ?

After reading the email chain, I started to look into Flink documentation and if you compare it with Spark documentation, I think we have major work to do detailing out Spark internals so that more people from community start to take active role in improving the issues so that Spark stays strong compared to Flink.

https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals

https://cwiki.apache.org/confluence/display/FLINK/Flink+Internals

Spark is no longer an engine that works for micro-batch and batch...We (and I am sure many others) are pushing spark as an engine for stream and query processing.....we need to make it a state-of-the-art engine for high speed streaming data and user queries as well !

On Sun, Oct 16, 2016 at 1:30 PM, Tomasz GawÄ™da <tomasz.gaweda@outlook.com<mailto:tomasz.gaweda@outlook.com>> wrote:
Hi everyone,

I'm quite late with my answer, but I think my suggestions may help a
little bit. :) Many technical and organizational topics were mentioned,
but I want to focus on these negative posts about Spark and about ""haters""

I really like Spark. Easy of use, speed, very good community - it's
everything here. But Every project has to ""flight"" on ""framework market""
to be still no 1. I'm following many Spark and Big Data communities,
maybe my mail will inspire someone :)

You (every Spark developer; so far I didn't have enough time to join
contributing to Spark) has done excellent job. So why are some people
saying that Flink (or other framework) is better, like it was posted in
this mailing list? No, not because that framework is better in all
cases.. In my opinion, many of these discussions where started after
Flink marketing-like posts. Please look at StackOverflow ""Flink vs ....""
posts, almost every post in ""winned"" by Flink. Answers are sometimes
saying nothing about other frameworks, Flink's users (often PMC's) are
just posting same information about real-time streaming, about delta
iterations, etc. It look smart and very often it is marked as an aswer,
even if - in my opinion - there wasn't told all the truth.


My suggestion: I don't have enough money and knowledgle to perform huge
performance test. Maybe some company, that supports Spark (Databricks,
Cloudera? - just saying you're most visible in community :) ) could
perform performance test of:

- streaming engine - probably Spark will loose because of mini-batch
model, however currently the difference should be much lower that in
previous versions

- Machine Learning models

- batch jobs

- Graph jobs

- SQL queries

People will see that Spark is envolving and is also a modern framework,
because after reading posts mentioned above people may think ""it is
outdated, future is in framework X"".

Matei Zaharia posted excellent blog post about how Spark Structured
Streaming beats every other framework in terms of easy-of-use and
reliability. Performance tests, done in various environments (in
example: laptop, small 2 node cluster, 10-node cluster, 20-node
cluster), could be also very good marketing stuff to say ""hey, you're
telling that you're better, but Spark is still faster and is still
getting even more fast!"". This would be based on facts (just numbers),
not opinions. It would be good for companies, for marketing puproses and
for every Spark developer


Second: real-time streaming. I've written some time ago about real-time
streaming support in Spark Structured Streaming. Some work should be
done to make SSS more low-latency, but I think it's possible. Maybe
Spark may look at Gearpump, which is also built on top of Akka? I don't
know yet, it is good topic for SIP. However I think that Spark should
have real-time streaming support. Currently I see many posts/comments
that ""Spark has too big latency"". Spark Streaming is doing very good
jobs with micro-batches, however I think it is possible to add also more
real-time processing.

Other people said much more and I agree with proposal of SIP. I'm also
happy that PMC's are not saying that they will not listen to users, but
they really want to make Spark better for every user.


What do you think about these two topics? Especially I'm looking at Cody
(who has started this topic) and PMCs :)

Pozdrawiam / Best regards,

Tomasz


W dniu 2016-10-07 o 04:51, Cody Koeninger pisze:
> I love Spark.  3 or 4 years ago it was the first distributed computing
> environment that felt usable, and the community was welcoming.
>
> But I just got back from the Reactive Summit, and this is what I observed:
>
> - Industry leaders on stage making fun of Spark's streaming model
> - Open source project leaders saying they looked at Spark's governance
> as a model to avoid
> - Users saying they chose Flink because it was technically superior
> and they couldn't get any answers on the Spark mailing lists
>
> Whether you agree with the substance of any of this, when this stuff
> gets repeated enough people will believe it.
>
> Right now Spark is suffering from its own success, and I think
> something needs to change.
>
> - We need a clear process for planning significant changes to the codebase.
> I'm not saying you need to adopt Kafka Improvement Proposals exactly,
> but you need a documented process with a clear outcome (e.g. a vote).
> Passing around google docs after an implementation has largely been
> decided on doesn't cut it.
>
> - All technical communication needs to be public.
> Things getting decided in private chat, or when 1/3 of the committers
> work for the same company and can just talk to each other...
> Yes, it's convenient, but it's ultimately detrimental to the health of
> the project.
> The way structured streaming has played out has shown that there are
> significant technical blind spots (myself included).
> One way to address that is to get the people who have domain knowledge
> involved, and listen to them.
>
> - We need more committers, and more committer diversity.
> Per committer there are, what, more than 20 contributors and 10 new
> jira tickets a month?  It's too much.
> There are people (I am _not_ referring to myself) who have been around
> for years, contributed thousands of lines of code, helped educate the
> public around Spark... and yet are never going to be voted in.
>
> - We need a clear process for managing volunteer work.
> Too many tickets sit around unowned, unclosed, uncertain.
> If someone proposed something and it isn't up to snuff, tell them and
> close it.  It may be blunt, but it's clearer than ""silent no"".
> If someone wants to work on something, let them own the ticket and set
> a deadline. If they don't meet it, close it or reassign it.
>
> This is not me putting on an Apache Bureaucracy hat.  This is me
> saying, as a fellow hacker and loyal dissenter, something is wrong
> with the culture and process.
>
> Please, let's change it.
>
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
>

"
Prasun Ratn <prasun.ratn@gmail.com>,"Mon, 17 Oct 2016 14:32:43 +0530",trying to use Spark applications with modified Kryo,dev@spark.apache.org,"Hi

I want to run some Spark applications with some changes in Kryo serializer.

Please correct me, but I think I need to recompile spark (instead of
just the Spark applications) in order to use the newly built Kryo
serializer?

I obtained Kryo 3.0.3 source and built it (mvn package install).

Next, I took the source code for Spark 2.0.1 and built it (build/mvn
-X -DskipTests -Dhadoop.version=2.6.0 clean package)

I then compiled the Spark applications.

However, I am not seeing my Kryo changes when I run the Spark applications.

Please let me know if my assumptions and steps are correct.

Thank you
Prasun

---------------------------------------------------------------------


"
Nicolae Rosca <rosca.nicolae1985@gmail.com>,"Mon, 17 Oct 2016 12:20:47 +0200",Custom Monitoring of Spark applications,dev@spark.apache.org,"Hi all,

I am trying to write a custom Source for counting errors and output that
with Spark sink mechanism ( CSV or JMX ) and having some problems
understanding how this works.

1. I defined the Source, added counters created with MetricRegistry and
registered the Source



2. Used that counter ( I could printout in driver the value )

3. With CsvSink my counter is reported but value is 0. !!

I have following questions:
 - I expect that codehale's Counter is serialised and registered but
because objects are different is not the right counter. I have a version
with accumulator and is working fine just little worried about performance.
( and design ) Is there another way of doing this ? maybe static fields ?

- When running on YARN how many sink objects will be created ?

- If I will create some singleton object and register that counter in
Spark, counting is right but will never report from executor. How to enable
reporting from executors when running on YARN ?

My custom Source:

public class CustomMonitoring implements Source {


metrics.properties


Thanks you,

Nicolae  R.
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 17 Oct 2016 11:29:44 +0000",Re: trying to use Spark applications with modified Kryo,Prasun Ratn <prasun.ratn@gmail.com>,"

Hi

I want to run some Spark applications with some changes in Kryo serializer.

Please correct me, but I think I need to recompile spark (instead of
just the Spark applications) in order to use the newly built Kryo
serializer?

I obtained Kryo 3.0.3 source and built it (mvn package install).

Next, I took the source code for Spark 2.0.1 and built it (build/mvn
-X -DskipTests -Dhadoop.version=2.6.0 clean package)

I then compiled the Spark applications.

However, I am not seeing my Kryo changes when I run the Spark applications.


Kryo versions are very brittle.

You'll

-need to get an up to date/consistent version of Chill, which is where the transitive dependency on Kryo originates
-rebuild spark depending on that chill release

if you want hive integration, probably also rebuild Hive to be consistent too; the main reason Spark has its own Hive version is that
Kryo version sharing.

https://github.com/JoshRosen/hive/commits/release-1.2.1-spark2

Kryo has repackaged their class locations between versions. This lets the versions co-exist, but probably also explains why your apps aren't picking up the diffs.

Finally, keep an eye on this github PR

https://github.com/twitter/chill/issues/252

"
Prasun Ratn <prasun.ratn@gmail.com>,"Mon, 17 Oct 2016 19:29:26 +0530",Re: trying to use Spark applications with modified Kryo,Steve Loughran <stevel@hortonworks.com>,"Thanks a lot Steve!


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 17 Oct 2016 09:46:56 -0500",Re: Spark Improvement Proposals,Debasish Das <debasish.das83@gmail.com>,"I think narrowly focusing on Flink or benchmarks is missing my point.

My point is evolve or die.  Spark's governance and organization is
hampering its ability to evolve technologically, and it needs to
change.

ote:
ent
o
k
art
nd
y
d
k.com>
s""

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 17 Oct 2016 09:54:04 -0500",Re: cutting 2.0.2?,Reynold Xin <rxin@databricks.com>,"SPARK-17841  three line bugfix that has a week old PR
SPARK-17812  being able to specify starting offsets is a must have for
a Kafka mvp in my opinion, already has a PR
SPARK-17813  I can put in a PR for this tonight if it'll be considered


---------------------------------------------------------------------


"
=?iso-8859-2?Q?Tomasz_Gaw=EAda?= <tomasz.gaweda@outlook.com>,"Mon, 17 Oct 2016 15:05:05 +0000",Odp.: Spark Improvement Proposals,"Cody Koeninger <cody@koeninger.org>, Debasish Das
	<debasish.das83@gmail.com>","Maybe my mail was not clear enough.


I didn't want to write ""lets focus on Flink"" or any other framework. The idea with benchmarks was to show two things:

- why some people are doing bad PR for Spark

- how - in easy way - we can change it and show that Spark is still on the top


No more, no less. Benchmarks will be helpful, but I don't think they're thert ""Spark vs Hadoop"". It is important to show that framework is not the same Spark with other API, but much faster and optimized, comparable or even faster than other frameworks.


About real-time streaming, I think it would be just good to see it in Spark. I very like current Spark model, but many voices that says ""we need more"" - community should listen also them and try to help them. With SIPs it would be easier, I've just posted this example as ""thing that may be changed with SIP"".


I very like unification via Datasets, but there is a lot of algorithms inside - let's make easy API, but with strong background (articles, benchmarks, descriptions, etc) that shows that Spark is still modern framework.


Maybe now my intention will be clearer :) As I said organizational ideas were already mentioned and I agree with them, my mail was just to show some aspects from my side, so from theside of developer and person who is trying to help others with Spark (via StackOverflow or other ways)


Pozdrawiam / Best regards,

Tomasz


________________________________
Od: Cody Koeninger <cody@koeninger.org>
Wys³ane: 17 pa¼dziernika 2016 16:46
Do: Debasish Das
DW: Tomasz Gawêda; dev@spark.apache.org
Temat: Re: Spark Improvement Proposals

I think narrowly focusing on Flink or benchmarks is missing my point.

My point is evolve or die.  Spark's governance and organization is
hampering its ability to evolve technologically, and it needs to
change.

ote:
ent
o
k
art
nd
y
d
om>
s""

---------------------------------------------------------------------

"
Erik O'Shaughnessy <erik.oshaughnessy@oracle.com>,"Mon, 17 Oct 2016 08:09:35 -0700 (MST)",Re: cutting 2.0.2?,dev@spark.apache.org,"I would very much like to see SPARK-16962 included in 2.0.2 as it addresses
unaligned memory access patterns that crash non-x86 platforms.  I believe
this falls in the category of ""correctness fix"".  We (Oracle SAE) have
applied the fixes for SPARK-16962 to branch-2.0 and have not encountered any
problems on SPARC or x86 architectures attributable to unaligned accesses.
Including this fix will allow Oracle SPARC customers to run Apache Spark
without fear of crashing, expanding the reach of Apache Spark and making my
life a little easier :)

erik.oshaughnessy@oracle.com



--

---------------------------------------------------------------------


"
Pulasthi Supun Wickramasinghe <pulasthi911@gmail.com>,"Mon, 17 Oct 2016 12:05:36 -0400",Fwd: Large variation in spark in Task Deserialization Time,dev@spark.apache.org,"Hi Devs/All,

I am seeing a huge variation on spark Task Deserialization Time for my
collect and reduce operations. while most tasks complete within 100ms a few
take mote than a couple of seconds which slows the entire program down. I
have attached a screen shot of the web UI where you can see the variation


As you can see the Task Deserialization Time time has a Max of 7s and 75th
percentile at 0.3 seconds.

Does anyone know the reasons that may cause these kind of numbers. Any help
would be greatly appreciated.

Best Regards,
Pulasthi
-- 
Pulasthi S. Wickramasinghe
Graduate Student  | Research Assistant
School of Informatics and Computing | Digital Science Center
Indiana University, Bloomington
cell: 224-386-9035
"
Michael Segel <msegel_hadoop@hotmail.com>,"Mon, 17 Oct 2016 16:49:26 +0000",Indexing w spark joins? ,"Spark dev list <dev@spark.apache.org>, ""user @spark""
	<user@spark.apache.org>","Hi,

Apologies if Iâ€™ve asked this question before but I didnâ€™t see it in the list and Iâ€™m certain that my last surviving brain cell has gone on strike over my attempt to reduce my caffeine intakeâ€¦

Posting this to both user and dev because I think the question / topic jumps in to both camps.


Again since Iâ€™m a relative newbie on sparkâ€¦ I may be missing something so apologies up frontâ€¦


With respect to Spark SQL,  in pre 2.0.x,  there were only hash joins?  In post 2.0.x you have hash, semi-hash , and sorted list merge.

For the sake of simplicityâ€¦ lets forget about cross product joinsâ€¦

Has anyone looked at how we could use inverted tables to improve query performance?

The issue is that when you have a data sewer (lake) , what happens when your use case query is orthogonal to how your data is stored? This means full table scans.
By using secondary indexes, we can reduce this albeit at a cost of increasing your storage footprint by the size of the index.

Are there any JIRAs open that discuss this?

Indexes to assist in terms of â€˜predicate push downsâ€™ (using the index when a field in a where clause is indexed) rather than performing a full table scan.
Indexes to assist in the actual join if the join column is on an indexed column?

In the first, using an inverted table to produce a sort ordered set of row keys that you would then use in the join process (same as if you produced the subset based on the filter.)

To put this in perspectiveâ€¦ hereâ€™s a dummy use caseâ€¦

CCCis (CCC) is the middle man in the insurance industry. They have a piece of software that sits in the repair shop (e.g Joeâ€™s Auto Body) and works with multiple insurance carriers.
The primary key in their data is going to be Insurance Company | Claim ID.  This makes it very easy to find a specific claim for further processing.

Now lets say I want to do some analysis on determining the average cost of repairing a front end collision of a Volvo S80?
Or
Break down the number and types of accidents by car manufacturer , model and color.  (Then see if there is any correlation between car color and # and type of accidents)


As you can see, all of these queries are orthogonal to my storage.  So I need to create secondary indexes to help sift thru the data efficiently.

Does this make sense?

Please Note: I did some work for CCC back in the late 90â€™s. Any resemblance to their big data efforts is purely coincidence  and you can replace CCC with Allstate, Progressive, StateFarm or some other auto insurance company â€¦

Thx

-Mike


"
shane knapp <sknapp@berkeley.edu>,"Mon, 17 Oct 2016 10:15:18 -0700",[build system] jenkins downtime for backups delayed by a hung build,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","i just noticed that jenkins was still in quiet mode this morning due
to a hung build.  i killed the build, backups happened, and the queue
is now happily building.

sorry for any delay!

shane

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 17 Oct 2016 10:26:13 -0700",Re: source for org.spark-project.hive:1.2.1.spark2,Steve Loughran <stevel@hortonworks.com>,"Are these changes that the Hive community has rejected? I don't see a
compelling reason to have a long-term Spark fork of Hive.

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
Sean Owen <sowen@cloudera.com>,"Mon, 17 Oct 2016 17:34:36 +0000",Re: source for org.spark-project.hive:1.2.1.spark2,"Ryan Blue <rblue@netflix.com.invalid>, Steve Loughran <stevel@hortonworks.com>","IIRC this was all about shading of dependencies, not changes to the source.


"
Sean Owen <sowen@cloudera.com>,"Mon, 17 Oct 2016 17:48:37 +0000",Re: cutting 2.0.2?,"""Erik O'Shaughnessy"" <erik.oshaughnessy@oracle.com>, dev@spark.apache.org","(I don't think 2.0.2 will be released for a while if at all but that's not
what you're asking I think)

It's a fairly safe change, but also isn't exactly a fix in my opinion.
Because there are some other changes to make it all work for SPARC, I think
it's more realistic to look to the 2.1.0 release anyway, which is likely to
come first.




"
Reynold Xin <rxin@databricks.com>,"Mon, 17 Oct 2016 17:18:50 -0700",[VOTE] Release Apache Spark 1.6.3 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.3. The vote is open until Thursday, Oct 20, 2016 at 18:00 PDT and
passes if a majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.3
[ ] -1 Do not release this package because ...


The tag to be voted on is v1.6.3-rc1
(7375bb0c825408ea010dcef31c0759cf94ffe5c2)

This release candidate addresses 50 JIRA tickets:
https://s.apache.org/spark-1.6.3-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1205/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc1-docs/


=======================================
== How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.2.

================================================
== What justifies a -1 vote for this release?
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.2, missing features, or bugs related to new features will not
necessarily block this release.
"
Ofir Manor <ofir.manor@equalum.io>,"Tue, 18 Oct 2016 11:51:16 +0300",StructuredStreaming status,dev <dev@spark.apache.org>,"Hi,
I hope it is the right forum.
I am looking for some information of what to expect from
StructuredStreaming in its next releases to help me choose when / where to
start using it more seriously (or where to invest in workarounds and where
to wait). I couldn't find a good place where such planning discussed for
2.1  (like, for example ML and SPARK-15581).
I'm aware of the 2.0 documented limits (
http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations),
like no support for multiple aggregations levels, joins are strictly to a
static dataset (no SCD or stream-stream) etc, limited sources / sinks (like
no sink for interactive queries) etc etc
I'm also aware of some changes that have landed in master, like the new
Kafka 0.10 source (and its on-going improvements) in SPARK-15406, the
metrics in SPARK-17731, and some improvements for the file source.
If I remember correctly, the discussion on Spark release cadence concluded
with a preference to a four-month cycles, with likely code freeze pretty
soon (end of October). So I believe the scope for 2.1 should likely quite
clear to some, and that 2.2 planning should likely be starting about now.
Any visibility / sharing will be highly appreciated!
thanks in advance,

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io
"
Krishna Kalyan <krishnakalyan3@gmail.com>,"Tue, 18 Oct 2016 11:16:55 +0200",Contributing to PySpark,"dev <dev@spark.apache.org>, user <user@spark.apache.org>","Hello,
I am a masters student. Could someone please let me know how set up my dev
working environment to contribute to pyspark.
Questions I had were:
a) Should I use Intellij Idea or PyCharm?.
b) How do I test my changes?.

Regards,
Krishna
"
Holden Karau <holden@pigscanfly.ca>,"Tue, 18 Oct 2016 04:05:13 -0700",Re: Contributing to PySpark,Krishna Kalyan <krishnakalyan3@gmail.com>,"Hi Krishna,

Thanks for your interest contributing to PySpark! I don't personally use
either of those IDEs so I'll leave that part for someone else to answer -
but in general you can find the building spark documentation at
http://spark.apache.org/docs/latest/building-spark.html which includes
notes on how to run the Python tests as well. You will also probably want
to check out the contributing to Spark guide at
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark.

Cheers,

Holden :)





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Holden Karau <holden@pigscanfly.ca>,"Tue, 18 Oct 2016 04:25:42 -0700","Mini-Proposal: Make it easier to contribute to the contributing to
 Spark Guide","""dev@spark.apache.org"" <dev@spark.apache.org>","Right now the wiki isn't particularly accessible to updates by external
contributors. We've already got a contributing to spark page which just
links to the wiki - how about if we just move the wiki contents over? This
way contributors can contribute to our documentation about how to
contribute probably helping clear up points of confusion for new
contributors which the rest of us may be blind to.

If we do this we would probably want to update the wiki page to point to
the documentation generated from markdown. It would also mean that the
results of any update to the contributing guide take a full release cycle
to be visible. Another alternative would be opening up the wiki to a
broader set of people.

I know a lot of people are probably getting ready for Spark Summit EU (and
I hope to catch up with some of y'all there) but I figured this a
relatively minor proposal.
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Sean Owen <sowen@cloudera.com>,"Tue, 18 Oct 2016 11:40:13 +0000","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide","Holden Karau <holden@pigscanfly.ca>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I'm OK with that. The upside to the wiki is that it can be edited directly
outside of a release cycle. However, in practice I find that the wiki is
rarely changed. To me it also serves as a place for information that isn't
exactly project documentation like ""powered by"" listings.

In a way I'd like to get rid of the wiki to have one less place for docs,
that doesn't have the same accessibility (I don't know who can give edit
access), and doesn't have a review process.

For now I'd settle for bringing over a few key docs like the one you
mention. I spent a little time a while ago removing some duplication across
the wiki and project docs and think there's a bit more than could be done.



"
Cody Koeninger <cody@koeninger.org>,"Tue, 18 Oct 2016 08:19:59 -0500","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide",Sean Owen <sowen@cloudera.com>,"+1 to putting docs in one clear place.


"
roehst <ro.stevaux@gmail.com>,"Tue, 18 Oct 2016 07:03:28 -0700 (MST)",,dev@spark.apache.org,"Sorry, by API you mean by use of 3rd party libraries or user code or
something else?

Thanks



--

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 18 Oct 2016 07:32:52 -0700",,roehst <ro.stevaux@gmail.com>,"I think what Reynold means is that if its easy for a developer to build
this convenience function using the current Spark API it probably doesn't
need to go into Spark unless its being done to provide a similar API to a
system we are attempting to be semi-compatible with (e.g. if a
corresponding convenience function existed in the pandas API).




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 18 Oct 2016 09:59:40 -0700",Re: Mini-Proposal: Make it easier to contribute to the contributing to Spark Guide,Cody Koeninger <cody@koeninger.org>,"Is there any way to tie wiki accounts with JIRA accounts? I found it weird that they're not tied at the ASF.

Otherwise, moving this into the docs might make sense.

Matei

directly outside of a release cycle. However, in practice I find that the wiki is rarely changed. To me it also serves as a place for information that isn't exactly project documentation like ""powered by"" listings.
docs, that doesn't have the same accessibility (I don't know who can give edit access), and doesn't have a review process.
mention. I spent a little time a while ago removing some duplication across the wiki and project docs and think there's a bit more than could be done.
external contributors. We've already got a contributing to spark page which just links to the wiki - how about if we just move the wiki contents over? This way contributors can contribute to our documentation about how to contribute probably helping clear up points of confusion for new contributors which the rest of us may be blind to.
to the documentation generated from markdown. It would also mean that the results of any update to the contributing guide take a full release cycle to be visible. Another alternative would be opening up the wiki to a broader set of people.
(and I hope to catch up with some of y'all there) but I figured this a relatively minor proposal.
<https://twitter.com/holdenkarau>
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 18 Oct 2016 10:03:39 -0700","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide",Matei Zaharia <matei.zaharia@gmail.com>,"+1 - Given that our website is now on github
(https://github.com/apache/spark-website), I think we can move most of
our wiki into the main website. That way we'll only have two sources
of documentation to maintain: A release specific one in the main repo
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 18 Oct 2016 19:53:57 +0000",Re: source for org.spark-project.hive:1.2.1.spark2,Ryan Blue <rblue@netflix.com>,"

Are these changes that the Hive community has rejected? I don't see a compelling reason to have a long-term Spark fork of Hive.

More changes in hive that haven't been picked up

HIVE-11720 is needed to handle very long HTTP headers, which is exactly the kind of header Active Directory likes to generate

the other one fixes pom dependencies to stop groovy-all getting into to spark. That's been fixed in sparks own POMs; pushing into the spark hive fork ensures that nobody else gets it. This matters as you can abuse serialisation and have SparkContext.objectFile() run arbitrary code in the loader's process. Not ideal.



rb



The Spark 2 branch is based on this one: https://github.com/JoshRosen/hive/commits/release-1.2.1-spark2


Didn't know this had moved.... I had an outstanding PR against patricks which should really go in, if not already taken up ( HIVE-11720 ; https://github.com/pwendell/hive/pull/2 )


IMO I think it would make sense if -somehow- that hive fork were in the ASF; it's got to be in sync with Spark releases, and its not been ideal for me in terms of getting one or two fixes in, the other one being culling
groovy 2.4.4 as an export ( https://github.com/steveloughran/hive/tree/stevel/SPARK-13471-groovy-2.4.4 )

I don't know if the hive team themselves would be up to having it in their repo, or if committership logistics would suit it anyway. Otherwise, approaching infra@ and asking for a forked repo is likely to work with a bit of prodding

rb

In an email thread [1] from Aug 2015, it was mentioned that the source
to org.spark-project.hive was at
https://github.com/pwendell/hive/commits/release-1.2.1-spark .
That branch has a 1.2.1.spark version but spark 2.0.1 uses
1.2.1.spark2. Could anyone point me to the repo for 1.2.1.spark2?
Thanks --Ethan

[https://mail-archives.apache.org/mod_mbox/spark-dev/201508.mbox/%3CA0AA8B38-DEEE-476A-93FF-92FEAD06E404@hortonworks.com%3E]

---------------------------------------------------------------------
ibe@spark.apache.org>




--
Ryan Blue
Software Engineer
Netflix




--
Ryan Blue
Software Engineer
Netflix

"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 19 Oct 2016 08:46:17 +0900","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide",shivaram@eecs.berkeley.edu,"+1 if the docs can be exposed more.


"
Zhan Zhang <zhazhan@gmail.com>,"Tue, 18 Oct 2016 18:04:32 -0700 (MST)",SparkPlan/Shuffle stage reuse with Dataset/DataFrame,dev@spark.apache.org,"Hi Folks,

We have some Dataset/Dataframe use cases that will benefit from reuse the
SparkPlan and shuffle stage. 

For example, the following cases. Because the query optimization and
sparkplan is generated by catalyst when it is executed, as a result, the
underlying RDD lineage is regenerated for dataset1. Thus, the shuffle stage
will be executed multiple times.

val dataset1 = dataset.groupby.agg
df.registerTempTable(""tmpTable"")
spark.sql(""select * from tmpTable where condition"").collect
spark.sql(""select * from tmpTable where condition1"").cllect

cannot reuse the data generated by shuffle stage.

Currently, to reuse the dataset1, we have to use persist to cache the data.
It is helpful but sometimes is not what we want, as it has some side effect.
For example, we cannot release the executor that has active cache in it even
it is idle and dynamic allocator is enabled.

In other words, we only want to reuse the shuffle data as much as possible
without caching in a long pipeline with multiple shuffle stages.

I am wondering does it make sense to add a new feature to Dataset/Dataframe
to work as barrier and prevent the query optimization happens across the
barrier.

For example, in the above case, we want catalyst take tmpTable as a barrier,
and stop optimization across it, so that we can reuse the underlying rdd
lineage of dataset1.

The prototype code to make it work is quite small, and we tried in house
with a new API as Dataset.cacheShuffle to make this happen.

But I want some feedback from community before opening a JIRA, as in some
sense, it does stop the optimization earlier. Any comments?





--

---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 19 Oct 2016 07:58:19 +0000",Re: [VOTE] Release Apache Spark 1.6.3 (RC1),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","We see a regression since 1.6.2. I think this PR needs to be backported
https://github.com/apache/spark/pull/13784 which resolves SPARK-16078. The
PR that causes the issue (for SPARK-15613) was reverted just before 1.6.2
release then re-applied afterwards but this fix was only backported to 2.0.

Test failure: org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite.to UTC
timestamp


"
Sean Owen <sowen@cloudera.com>,"Wed, 19 Oct 2016 11:32:58 +0000",Re: [VOTE] Release Apache Spark 1.6.3 (RC1),"Pete Robbins <robbinspg@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah I see that too. I'll work on back-porting it.
The release otherwise looks good to me, but let's keep testing please to
identify anything else in the meantime.


"
Michael Armbrust <michael@databricks.com>,"Wed, 19 Oct 2016 12:45:10 -0700",Re: StructuredStreaming status,Ofir Manor <ofir.manor@equalum.io>,"Anything that is actively being designed should be in JIRA, and it seems
like you found most of it.  In general, release windows can be found on the
wiki <https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage>.

2.1 has a lot of stability fixes as well as the kafka support you
mentioned.  It may also include some of the following.

The items I'd like to start thinking about next are:
 - Evicting state from the store based on event time watermarks
 - Sessionization (grouping together related events by key / eventTime)
 - Improvements to the query planner (remove some of the restrictions on
what queries can be run).

This is roughly in order based on what I've been hearing users hit the
most.  Would love more feedback on what is blocking real use cases.


"
Cody Koeninger <cody@koeninger.org>,"Wed, 19 Oct 2016 15:11:52 -0500",Re: StructuredStreaming status,Michael Armbrust <michael@databricks.com>,"Is anyone seriously thinking about alternatives to microbatches?


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 19 Oct 2016 13:29:51 -0700",Re: StructuredStreaming status,Cody Koeninger <cody@koeninger.org>,"I know people are seriously thinking about latency.  So far that has not
been the limiting factor in the users I've been working with.


"
Amit Sela <amitsela33@gmail.com>,"Wed, 19 Oct 2016 20:49:11 +0000",Re: StructuredStreaming status,"Michael Armbrust <michael@databricks.com>, Cody Koeninger <cody@koeninger.org>","I've been working on the Apache Beam Spark runner which is (in this
context) basically running a streaming model that focuses on event-time and
correctness with Spark, and as I see it (even in spark 1.6.x) the
micro-batches are really just added latency, which will work-out for some
users, and not for others and that's OK. Structured Streaming triggers make
it even better with computing on trigger (other systems do it constantly,
but output only on trigger so no much difference there).

I'm actually curious about a couple of things:

   - State store API - having a state API available is extremely useful for
   streaming in many fronts:
      - Available for sources (and sinks ?) to avoid immortalizing
      micro-batch reads (simply let tasks pick-off where they left the previous
      micro-batch).
      - Can help rid of resuming from checkpoint, you can simply restart -
      that goes for upgrading Spark jobs as well as wrapping accumulators and
      broadcasts in getOrCreate methods (an of not resuming from checkpoint you
      can avoid wrapping you DAG construction in getOrCreate as well).
      - The fact that it aims to be pluggable enabling building platforms
      (not just frameworks) with Spark.
      - Finally, it is basically the basis for any stateful computation
      spark will support.
   - Evicting state as Michael pointed out, which currently, if using for
   example overlapping windows grows the Dataset really quickly.
   - Encoders API - Where does it stand ? will developers/users be able to
   define a custom schema for say a generically typed class ? will it get
   along with inner classes, static classes etc. ?

Thanks,
Amit


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 19 Oct 2016 14:01:47 -0700",Re: StructuredStreaming status,Michael Armbrust <michael@databricks.com>,"At the AMPLab we've been working on a research project that looks at
just the scheduling latencies and on techniques to get lower
scheduling latency. It moves away from the micro-batch model, but
reuses the fault tolerance etc. in Spark. However we haven't yet
figure out all the parts in integrating this with the rest of
structured streaming. I'll try to post a design doc / SIP about this
soon.

micro-batch other than latency ?

Thanks
Shivaram


---------------------------------------------------------------------


"
Amit Sela <amitsela33@gmail.com>,"Wed, 19 Oct 2016 21:18:22 +0000",Re: StructuredStreaming status,"shivaram@eecs.berkeley.edu, Michael Armbrust <michael@databricks.com>","
I think that the fact that they serve as an output trigger is a problem,
but Structured Streaming seems to resolve this now.

"
Michael Armbrust <michael@databricks.com>,"Wed, 19 Oct 2016 15:08:42 -0700","Re: Why the json file used by sparkSession.read.json must be a valid
 json object per line",trsell@gmail.com,"

That link is awesome.  I think it would be great if someone could open a PR
to add this to our documentation.

I'd also be happy to add a flag to support multiline objects at file
boundaries, but someone needs to propose a scalable way to it.  While that
blog post it a good resource, it would easily cause OOMs on large files.
"
Ofir Manor <ofir.manor@equalum.io>,"Thu, 20 Oct 2016 02:46:41 +0300",Re: StructuredStreaming status,Michael Armbrust <michael@databricks.com>,"Thanks a lot Michael! I really appreciate your sharing.
Logistically, I suggest to find a way to tag all structured streaming
JIRAs, so it wouldn't so hard to look for them, for anyone wanting to
participate, and also have something like the ML roadmap JIRA.
regarding your list, evicting space seems very important. If I understand
correctly, currently state grows forever (when using windows), so it is
impractical to run a long-running streaming job with decent state. It would
be great if user could bound the state by event time (it is also very
natural).
I personally see sessionization as lower priority (seems like a niche
requirement). To me, supporting only a single stream of events that can
only be joined to static datasets makes building anything but the simplest
of short-running streaming jobs problematic (all interesting datasets
change over time). Also, the promise of interactive queries on top of a
computed, live dataset likely has a wider appeal (as it was presented since
early this year as one of the goals of structured streaming). Also making
the sources and sinks API nicer to third-party developers to encourage
adoption and plugins, or beefing up the list of builtin exactly-once
sources and sinks (maybe also have a pluggable state store, as I've seen
some wanting, which may better enable interactive queries).
In addition, I think you should really identify what needs to be done to
make this API stable and focus on that. I think that for adoption, you'll
need to be clear on the full list of gaps / gotchas, and clearly
communicate the project priorities / target timeline (again, just like ML
does it), hopefully after some community discussion...

Structured Streaming over the last three months since 2.0 was released. I
was under the impression that this was one of the biggest things that the
Spark community actively works on, but that is clearly not the case, given
that most of the activity is a couple of (very important) JIRAs from the
last several weeks. Not really sure how to parse that yet...
I think having some clearer, prioritized roadmap going forward will be a
good first to recalibrate expectations for 2.2 and for graduating from an
alpha state. But especially, I think you guys seriously needs to figure out
what's the bottleneck here (lack of dedicated owner? lack of commiters
focusing on it?) and just fix it (recruit new commiters to work on it?) to
have a competitive streaming offering in a few quarters.

Just my two cents,

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 19 Oct 2016 17:36:05 -0700",Re: StructuredStreaming status,Amit Sela <amitsela33@gmail.com>,"I'm also curious whether there are concerns other than latency with the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.

The stateful operator designs for streaming systems aren't inherently ""better"" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.

Matei

problem, but Structured Streaming seems to resolve this now.  
not
it seems
found on
eventTime)
restrictions on
hit the
<ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>>
/ where
and
discussed
(http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations <http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations>),
strictly to
sinks
the new
SPARK-15406, the
source.
freeze
likely
about
ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>
<mailto:dev-unsubscribe@spark.apache.org>

"
Reynold Xin <rxin@databricks.com>,"Wed, 19 Oct 2016 17:46:43 -0700","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide",Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"For the contributing guide I think it makes more sense to put it in
apache/spark github, since that's where contributors start. I'd also link
to it from the website ...



"
Cody Koeninger <cody@koeninger.org>,"Wed, 19 Oct 2016 23:20:07 -0500",Re: StructuredStreaming status,Matei Zaharia <matei.zaharia@gmail.com>,"I don't think it's just about what to target - if you could target 1ms
batches, without harming 1 second or 1 minute batches.... why wouldn't you?
I think it's about having a clear strategy and dedicating resources to it.
If  scheduling batches at an order of magnitude or two lower latency is the
strategy, and that's actually feasible, that's great. But I haven't seen
that clear direction, and this is by no means a recent issue.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 19 Oct 2016 21:40:40 -0700",Re: StructuredStreaming status,Cody Koeninger <cody@koeninger.org>,"Yeah, as Shivaram pointed out, there have been research projects that looked at it. Also, Structured Streaming was explicitly designed to not make microbatching part of the API or part of the output behavior (tying triggers to it). However, when people begin working on that is a function of demand relative to other features. I don't think we can commit to one plan before exploring more options, but basically there is Shivaram's project, which adds a few new concepts to the scheduler, and there's the option to reduce control plane latency in the current system, which hasn't been heavily optimized yet but should be doable (lots of systems can handle 10,000s of RPCs per second).

Matei

batches, without harming 1 second or 1 minute batches.... why wouldn't you?
it. If  scheduling batches at an order of magnitude or two lower latency is the strategy, and that's actually feasible, that's great. But I haven't seen that clear direction, and this is by no means a recent issue.
the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.
""better"" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.
problem, but Structured Streaming seems to resolve this now.  
has not
it seems
found on
you
eventTime)
restrictions on
hit the
<ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>>
/ where
workarounds and
discussed
(http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations <http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations>),
strictly to
sinks
the new
SPARK-15406, the
source.
cadence
freeze
likely
about
ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>
<mailto:dev-unsubscribe@spark.apache.org>

"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 19 Oct 2016 21:41:43 -0700",Re: StructuredStreaming status,"""Abhishek R. Singh"" <abhishsi@tetrationanalytics.com>","Both Spark Streaming and Structured Streaming preserve locality for operator state actually. They only reshuffle state if a cluster node fails or if the load becomes heavily imbalanced and it's better to launch a task on another node and load the state remotely.

Matei

the state has to be reshuffled every micro/mini-batch (unless I am not understanding it right - spark 2.0 state model i.e.).
processing and state purging are the other essentials (which are thankfully getting addressed).
also be greatly appreciated.
the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.
""better"" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.
problem, but Structured Streaming seems to resolve this now.  
has not
it seems
found on
you
eventTime)
restrictions on
hit the
<ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>>
when / where
workarounds and
discussed
(http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations <http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations>),
strictly to
/ sinks
like the new
SPARK-15406, the
source.
cadence
freeze
likely
about
ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>

"
"""Abhishek R. Singh"" <abhishsi@tetrationanalytics.com>","Wed, 19 Oct 2016 21:38:56 -0700",Re: StructuredStreaming status,Matei Zaharia <matei.zaharia@gmail.com>,"Its not so much about latency actually. The bigger rub for me is that the state has to be reshuffled every micro/mini-batch (unless I am not understanding it right - spark 2.0 state model i.e.).

Operator model avoids it by preserving state locality. Event time processing and state purging are the other essentials (which are thankfully getting addressed).

Any guidance on (timelines for) expected exit from alpha state would also be greatly appreciated.

-Abhishek-

the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.
""better"" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.
problem, but Structured Streaming seems to resolve this now.  
has not
it seems
found on
you
eventTime)
restrictions on
hit the
<ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>>
/ where
workarounds and
discussed
(http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations <http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations>),
strictly to
sinks
the new
SPARK-15406, the
source.
cadence
freeze
likely
about
ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>
<mailto:dev-unsubscribe@spark.apache.org>

"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 19 Oct 2016 22:35:46 -0700 (MST)",RE: StructuredStreaming status,dev@spark.apache.org,"There is one issue I was thinking of.
If I understand correctly, structured streaming basically groups by a bucket for time in sliding window (of the step). My problem is that in some cases (e.g. distinct count and any other case where the buffer is relatively large) this would mean copying the buffer for each step. The can have a very large memory overhead.
There are other solutions for this. For example, let's say we would have implemented distinct count by saving a map with the key being the distinct value and the value being the last time we saw this value. This would mean that we wouldn't really need to save all the steps in the middle and copy the data, we could only save the last portion.
This is just an idea for optimization though, certainly nothing of high priority.


From: Matei Zaharia [via Apache Spark Developers List] [mailto:ml-node+s1001551n19513h92@n3.nabble.com]
Sent: Thursday, October 20, 2016 3:42 AM
To: Mendelson, Assaf
Subject: Re: StructuredStreaming status

I'm also curious whether there are concerns other than latency with the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.

The stateful operator designs for streaming systems aren't inherently ""better"" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.

Matei



At the AMPLab we've been working on a research project that looks at
just the scheduling latencies and on techniques to get lower
scheduling latency. It moves away from the micro-batch model, but
reuses the fault tolerance etc. in Spark. However we haven't yet
figure out all the parts in integrating this with the rest of
structured streaming. I'll try to post a design doc / SIP about this
soon.

micro-batch other than latency ?
I think that the fact that they serve as an output trigger is a problem, but Structured Streaming seems to resolve this now.

Thanks
Shivaram


---------------------------------------------------------------------


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/StructuredStreaming-status-tp19490p19513.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
Michael Armbrust <michael@databricks.com>,"Thu, 20 Oct 2016 01:15:05 -0700",Re: StructuredStreaming status,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","p with
all the
n.

I don't think you can calculate count distinct in each event time window
correctly using this map if there is late data, which is one of the key
problems we are trying to solve with this API.  If you are only tracking
the last time you saw this value, how do you know if a late data item was
already accounted for in any given window that is earlier than this ""last
time""?

We would currently need to track the items seen in each window (though much
less space is required for approx count distinct).  However, the state
eviction I mentioned above should also let you give us a boundary on how
late data can be, and thus how many windows we need retain state for.  You
should also be able to group by processing time instead of event time if
you want something closer to the semantics of DStreams.

Finally, you can already construct the map you describe using structured
streaming and use its result to output statistics at each trigger window:

df.groupBy($""value"")
  .select(max($""eventTime"") as 'lastSeen)
  .writeStream
  .outputMode(""complete"")
  .trigger(ProcessingTime(""5 minutes""))
  .foreach( <emit results using values from the map> )
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 20 Oct 2016 01:27:08 -0700 (MST)",RE: StructuredStreaming status,dev@spark.apache.org,"My thoughts were of handling just the â€œcurrentâ€ state of the sliding window (i.e. the â€œlastâ€ window). The idea is that at least in cases which I encountered, the sliding window is used to â€œforgetâ€ irrelevant information and therefore when a step goes out of  date for the â€œcurrentâ€ window it becomes irrelevant.
I agree that this use case is just an example and will also have issues if there is a combination of windows. My main issue was that if we need to have a relatively large buffer (such as full distinct count) then the memory overhead of this can be very high.

As for the example of the map you gave, If I understand correctly how this would occur behind the scenes, this just provides the map but the memory cost of having multiple versions of the data remain. As I said, my issue is with the high memory overhead.

Consider a simple example: I do a sliding window of 1 day with a 1 minute step. There are 1440 minutes per day which means the groupby has a cost of multiplying all aggregations by 1440. For something such as a count or sum, this might not be a big issue but if we have an array of say 100 elements then this can quickly become very costly.

As I said, it is just an idea for optimization for specific use cases.


From: Michael Armbrust [via Apache Spark Developers List] [mailto:ml-node+s1001551n19520h9@n3.nabble.com]
Sent: Thursday, October 20, 2016 11:16 AM
To: Mendelson, Assaf
Subject: Re: StructuredStreaming status

letâ€™s say we would have implemented distinct count by saving a map with the key being the distinct value and the value being the last time we saw this value. This would mean that we wouldnâ€™t really need to save all the steps in the middle and copy the data, we could only save the last portion.

I don't think you can calculate count distinct in each event time window correctly using this map if there is late data, which is one of the key problems we are trying to solve with this API.  If you are only tracking the last time you saw this value, how do you know if a late data item was already accounted for in any given window that is earlier than this ""last time""?

We would currently need to track the items seen in each window (though much less space is required for approx count distinct).  However, the state eviction I mentioned above should also let you give us a boundary on how late data can be, and thus how many windows we need retain state for.  You should also be able to group by processing time instead of event time if you want something closer to the semantics of DStreams.

Finally, you can already construct the map you describe using structured streaming and use its result to output statistics at each trigger window:

df.groupBy($""value"")
  .select(max($""eventTime"") as 'lastSeen)
  .writeStream
  .outputMode(""complete"")
  .trigger(ProcessingTime(""5 minutes""))
  .foreach( <emit results using values from the map> )

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/StructuredStreaming-status-tp19490p19520.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/StructuredStreaming-status-tp19490p19521.html
om."
Andreas Hechenberger <internet@hechenberger.me>,"Thu, 20 Oct 2016 11:17:46 +0200",Get size of intermediate results,dev@spark.apache.org,"Hey awesome Spark-Dev's :)

i am new to spark and i read a lot but now i am stuck :( so please be
kind, if i ask silly questions.

I want to analyze some algorithms and strategies in spark and for one
experiment i want to know the size of the intermediate results between
iterations/jobs. Some of them are written to disk and some are in the
cache, i guess. I am not afraid of looking into the code (i already did)
but its complex and have no clue where to start :( It would be nice if
someone can point me in the right direction or where i can find more
information about the structure of spark core devel :)

I already setup the devel environment and i can compile spark. It was
really awesome how smoothly the setup was :) Thx for that.

Servus
Andy

---------------------------------------------------------------------


"
Amit Sela <amitsela33@gmail.com>,"Thu, 20 Oct 2016 10:30:17 +0000",Re: StructuredStreaming status,Matei Zaharia <matei.zaharia@gmail.com>,"
But Streaming Query sources
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala#L41>
are
still designed with microbatches in mind, can this be removed and leave
offset tracking to the executors ?

"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Oct 2016 10:52:35 -0700",[PSA] TaskContext.partitionId != the actual logical partition index,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI - Xiangrui submitted an amazing pull request to fix a long standing
issue with a lot of the nondeterministic expressions (rand, randn,
monotonically_increasing_id): https://github.com/apache/spark/pull/15567

Prior to this PR, we were using TaskContext.partitionId as the partition
index in initializing expressions. However, that is actually not a good
index to use in most cases, because it is the physical task's partition id
and does not always reflect the partition index at the time the RDD is
created (or in the Spark SQL physical plan). This makes a big difference
once there is a union or coalesce operation.

The ""index"" given by mapPartitionsWithIndex, on the other hand, does not
have this problem because it actually reflects the logical partition index
at the time the RDD is created.

When is it safe to use TaskContext.partitionId? It is safe at the very end
of a query plan (the root node), because there partitionId is guaranteed
based on the current implementation to be the same as the physical task
partition id.


For example, prior to Xiangrui's PR, the following query would return 2
rows, whereas the correct behavior should be 1 entry:

spark.range(1).selectExpr(""rand(1)"").union(spark.range(1)
.selectExpr(""rand(1)"")).distinct.show()

The reason it'd return 2 rows is because rand was using
TaskContext.partitionId as the per-partition seed, and as a result the two
sides of the union are using different seeds.


I'm starting to think we should deprecate the API and ban the use of it
within the project to be safe ...
"
Cody Koeninger <cody@koeninger.org>,"Thu, 20 Oct 2016 13:14:13 -0500",Re: [PSA] TaskContext.partitionId != the actual logical partition index,Reynold Xin <rxin@databricks.com>,"Access to the partition ID is necessary for basically every single one
of my jobs, and there isn't a foreachPartiionWithIndex equivalent.
You can kind of work around it with empty foreach after the map, but
it's really awkward to explain to people.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Oct 2016 11:16:35 -0700",Re: [PSA] TaskContext.partitionId != the actual logical partition index,Cody Koeninger <cody@koeninger.org>,"Seems like a good new API to add?



"
Cody Koeninger <cody@koeninger.org>,"Thu, 20 Oct 2016 13:30:23 -0500",Re: [PSA] TaskContext.partitionId != the actual logical partition index,Reynold Xin <rxin@databricks.com>,"Yep, I had submitted a PR that included it way back in the original
direct stream for kafka, but it got nixed in favor of
TaskContext.partitionId ;)  The concern then was about too many
xWithBlah apis on rdd.

If we do want to deprecate taskcontext.partitionId and add
foreachPartitionWithIndex, I think that makes sense, I can start a
ticket.


---------------------------------------------------------------------


"
Fred Reiss <freiss.oss@gmail.com>,"Thu, 20 Oct 2016 11:33:06 -0700","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide","""dev@spark.apache.org"" <dev@spark.apache.org>","Great idea! If the developer docs are in github, then new contributors who
find errors or omissions can update the docs as an introduction to the PR
process.

Fred


"
Michael Armbrust <michael@databricks.com>,"Thu, 20 Oct 2016 12:17:49 -0700",Re: StructuredStreaming status,Ofir Manor <ofir.manor@equalum.io>,"
I totally agree we should spend more time making sure the roadmap is clear
to everyone, but I disagree with this characterization.  There is a lot of
work happening in Structured Streaming. In this next release (2.1 as well
as 2.0.1 and 2.0.2) it has been more about stability and scalability rather
than user visible features.  We are running it for real on production jobs
and working to make it rock solid (Everyone can help here!). Just look at the
list of commits
<https://github.com/apache/spark/commits/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming>
.

Regarding the timeline to graduation, I think its instructive to look at
what happened with Spark SQL.

 - Spark 1.0 - added to Spark
 - Spark 1.1 - basic apis, and stability
 - Spark 1.2 - stabilization of Data Source APIs for plugging in external
sources
 - Spark 1.3 - GA
 - Spark 1.4-1.5 - Tungsten
 - Spark 1.6 - Fully-codegened / memory managed
 - Spark 2.0 - Whole stage codegen, experimental streaming support

We probably won't follow that exactly, and we clearly are not done yet.
However, I think the trajectory is good.

But Streaming Query sources


It certainly could be, but what Matei is saying is that user code should be
able to seamlessly upgrade.  A lot of early focus and thought was towards
this goal.  However, these kinds of concerns are exactly why I think it is
premature to expose these internal APIs to end users. Lets build several
Sources and Sinks internally, and figure out what works and what doesn't.
Spark SQL had JSON, Hive, Parquet, and RDDs before we opened up the APIs.
This experience allowed us keep the Data Source API stable into 2.x and
build a large library of connectors.
"
Egor Pahomov <pahomov.egor@gmail.com>,"Thu, 20 Oct 2016 18:18:50 -0700",Re: Get size of intermediate results,Andreas Hechenberger <internet@hechenberger.me>,"I needed the same for debugging and I just added ""count"" action in debug
mode for every step I was interested in. It's very time-consuming, but I
debug not very often.

2016-10-20 2:17 GMT-07:00 Andreas Hechenberger <internet@hechenberger.me>:



-- 


*Sincerely yoursEgor Pakhomov*
"
Jeremy Davis <jerdavis@speakeasy.net>,"Sat, 22 Oct 2016 14:14:05 -0700",Ran in to a bug in Broadcast Hash Join,dev@spark.apache.org,"
Hello, I ran in to a bug with Broadcast Hash Join in Spark 2.0. (Running on EMR)
If I just toggle spark.sql.autoBroadcastJoinThreshold=-1 then the join works, if I leave it as default it does not work.
When it doesnâ€™t work, then one of my joined columns is filled with very small Doubles.

Iâ€™m joining two small tables: (datetime,spx) and (datetime,vix)
Attached are the plans and debug.

================================================================
The (Default) Broken case:

+-------------+-----------+
|     datetime|        spx|
+-------------+-----------+
|1476907200000|2144.290039|
|1476820800000|2139.600098|
|1476734400000|     2126.5|
|1476475200000| 2132.97998|
|1476388800000|2132.550049|
|1476302400000|2139.179932|
|1476216000000| 2136.72998|
|1476129600000|2163.659912|
|1475870400000| 2153.73999|
|1475784000000| 2160.77002|
|1475697600000| 2159.72998|
|1475611200000| 2150.48999|
|1475524800000|2161.199951|
|1475265600000| 2168.27002|
|1475179200000|2151.129883|
|1475092800000|2171.370117|
|1475006400000|2159.929932|
|1474920000000|2146.100098|
|1474660800000|2164.689941|
|1474574400000|2177.179932|
+-------------+-----------+
only showing top 20 rows

+-------------+---------+
|     datetime|      vix|
+-------------+---------+
|1476907200000|    14.41|
|1476820800000|    15.28|
|1476734400000|16.209999|
|1476475200000|16.120001|
|1476388800000|16.690001|
|1476302400000|    15.91|
|1476216000000|    15.36|
|1476129600000|    13.38|
|1475870400000|    13.48|
|1475784000000|    12.84|
|1475697600000|    12.99|
|1475611200000|    13.63|
|1475524800000|    13.57|
|1475265600000|    13.29|
|1475179200000|    14.02|
|1475092800000|    12.39|
|1475006400000|     13.1|
|1474920000000|     14.5|
|1474660800000|    12.29|
|1474574400000|    12.02|
+-------------+---------+
only showing top 20 rows

2016-10-22T20:50:31.345+0000: [GC (Allocation Failure) [PSYoungGen: 704134K->79382K(945664K)] 823872K->199145K(3089408K), 0.0285894 secs] [Times: user=0.29 sys=0.04, real=0.03 secs]
== Physical Plan ==
*Project [datetime#34L, spx#25, vix#72]
+- *BroadcastHashJoin [datetime#34L], [datetime#81L], Inner, BuildRight
   :- *Project [if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
   :  +- *Filter ((isnotnull(Date#0) && NOT (Date#0 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
   :     +- *Scan csv [Date#0,Close#4] Format: CSV, InputPaths: s3n://dataproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))
      +- *Project [if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
         +- *Filter ((isnotnull(Date#47) && NOT (Date#47 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
            +- *Scan csv [Date#47,Close#51] Format: CSV, InputPaths: s3n://dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
########
== Parsed Logical Plan ==
'Join UsingJoin(Inner,List('datetime))
:- Project [datetime#34L, spx#25]
:  +- Project [Date#0, Open#1, High#2, Low#3, spx#25, Volume#5, Adj Close#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
:     +- Project [Date#0, Open#1, High#2, Low#3, cast(spx#16 as double) AS spx#25, Volume#5, Adj Close#6]
:        +- Project [Date#0, Open#1, High#2, Low#3, Close#4 AS spx#16, Volume#5, Adj Close#6]
:           +- Filter NOT (Date#0 = Date)
:              +- Relation[Date#0,Open#1,High#2,Low#3,Close#4,Volume#5,Adj Close#6] csv
+- Project [datetime#81L, vix#72]
   +- Project [Date#47, Open#48, High#49, Low#50, vix#72, Volume#52, Adj Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
      +- Project [Date#47, Open#48, High#49, Low#50, cast(vix#63 as double) AS vix#72, Volume#52, Adj Close#53]
         +- Project [Date#47, Open#48, High#49, Low#50, Close#51 AS vix#63, Volume#52, Adj Close#53]
            +- Filter NOT (Date#47 = Date)
               +- Relation[Date#47,Open#48,High#49,Low#50,Close#51,Volume#52,Adj Close#53] csv

== Analyzed Logical Plan ==
datetime: bigint, spx: double, vix: double
Project [datetime#34L, spx#25, vix#72]
+- Join Inner, (datetime#34L = datetime#81L)
   :- Project [datetime#34L, spx#25]
   :  +- Project [Date#0, Open#1, High#2, Low#3, spx#25, Volume#5, Adj Close#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
   :     +- Project [Date#0, Open#1, High#2, Low#3, cast(spx#16 as double) AS spx#25, Volume#5, Adj Close#6]
   :        +- Project [Date#0, Open#1, High#2, Low#3, Close#4 AS spx#16, Volume#5, Adj Close#6]
   :           +- Filter NOT (Date#0 = Date)
   :              +- Relation[Date#0,Open#1,High#2,Low#3,Close#4,Volume#5,Adj Close#6] csv
   +- Project [datetime#81L, vix#72]
      +- Project [Date#47, Open#48, High#49, Low#50, vix#72, Volume#52, Adj Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
         +- Project [Date#47, Open#48, High#49, Low#50, cast(vix#63 as double) AS vix#72, Volume#52, Adj Close#53]
            +- Project [Date#47, Open#48, High#49, Low#50, Close#51 AS vix#63, Volume#52, Adj Close#53]
               +- Filter NOT (Date#47 = Date)
                  +- Relation[Date#47,Open#48,High#49,Low#50,Close#51,Volume#52,Adj Close#53] csv

== Optimized Logical Plan ==
Project [datetime#34L, spx#25, vix#72]
+- Join Inner, (datetime#34L = datetime#81L)
   :- Project [if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
   :  +- Filter ((isnotnull(Date#0) && NOT (Date#0 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
   :     +- Relation[Date#0,Open#1,High#2,Low#3,Close#4,Volume#5,Adj Close#6] csv
   +- Project [if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
      +- Filter ((isnotnull(Date#47) && NOT (Date#47 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
         +- Relation[Date#47,Open#48,High#49,Low#50,Close#51,Volume#52,Adj Close#53] csv

== Physical Plan ==
*Project [datetime#34L, spx#25, vix#72]
+- *BroadcastHashJoin [datetime#34L], [datetime#81L], Inner, BuildRight
   :- *Project [if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
   :  +- *Filter ((isnotnull(Date#0) && NOT (Date#0 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
   :     +- *Scan csv [Date#0,Close#4] Format: CSV, InputPaths: s3n://dataproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))
      +- *Project [if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
         +- *Filter ((isnotnull(Date#47) && NOT (Date#47 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
            +- *Scan csv [Date#47,Close#51] Format: CSV, InputPaths: s3n://dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
########
+-------------+-----------+-------------------+
|     datetime|        spx|                vix|
+-------------+-----------+-------------------+
|1476907200000|2144.290039|7.296891096156E-312|
|1476820800000|2139.600098| 7.29646422344E-312|
|1476734400000|     2126.5| 7.29603735072E-312|
|1476475200000| 2132.97998|7.294756732566E-312|
|1476388800000|2132.550049| 7.29432985985E-312|
|1476302400000|2139.179932| 7.29390298713E-312|
|1476216000000| 2136.72998| 7.29347611441E-312|
|1476129600000|2163.659912|7.293049241694E-312|
|1475870400000| 2153.73999| 7.29176862354E-312|
|1475784000000| 2160.77002| 7.29134175082E-312|
|1475697600000| 2159.72998|7.290914878104E-312|
|1475611200000| 2150.48999|7.290488005386E-312|
|1475524800000|2161.199951| 7.29006113267E-312|
|1475265600000| 2168.27002|7.288780514514E-312|
|1475179200000|2151.129883|7.288353641796E-312|
|1475092800000|2171.370117| 7.28792676908E-312|
|1475006400000|2159.929932| 7.28749989636E-312|
|1474920000000|2146.100098| 7.28707302364E-312|
|1474660800000|2164.689941| 7.28579240549E-312|
|1474574400000|2177.179932| 7.28536553277E-312|
+-------------+-----------+-------------------+




=============================================================================
=============================================================================
=============================================================================
--conf,spark.sql.autoBroadcastJoinThreshold=-1,



+-------------+-----------+
|     datetime|        spx|
+-------------+-----------+
|1476907200000|2144.290039|
|1476820800000|2139.600098|
|1476734400000|     2126.5|
|1476475200000| 2132.97998|
|1476388800000|2132.550049|
|1476302400000|2139.179932|
|1476216000000| 2136.72998|
|1476129600000|2163.659912|
|1475870400000| 2153.73999|
|1475784000000| 2160.77002|
|1475697600000| 2159.72998|
|1475611200000| 2150.48999|
|1475524800000|2161.199951|
|1475265600000| 2168.27002|
|1475179200000|2151.129883|
|1475092800000|2171.370117|
|1475006400000|2159.929932|
|1474920000000|2146.100098|
|1474660800000|2164.689941|
|1474574400000|2177.179932|
+-------------+-----------+
only showing top 20 rows

+-------------+---------+
|     datetime|      vix|
+-------------+---------+
|1476907200000|    14.41|
|1476820800000|    15.28|
|1476734400000|16.209999|
|1476475200000|16.120001|
|1476388800000|16.690001|
|1476302400000|    15.91|
|1476216000000|    15.36|
|1476129600000|    13.38|
|1475870400000|    13.48|
|1475784000000|    12.84|
|1475697600000|    12.99|
|1475611200000|    13.63|
|1475524800000|    13.57|
|1475265600000|    13.29|
|1475179200000|    14.02|
|1475092800000|    12.39|
|1475006400000|     13.1|
|1474920000000|     14.5|
|1474660800000|    12.29|
|1474574400000|    12.02|
+-------------+---------+
only showing top 20 rows

== Physical Plan ==
*Project [datetime#34L, spx#25, vix#72]
+- *SortMergeJoin [datetime#34L], [datetime#81L], Inner
   :- *Sort [datetime#34L ASC], false, 0
   :  +- Exchange hashpartitioning(datetime#34L, 200)
   :     +- *Project [if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
   :        +- *Filter ((isnotnull(Date#0) && NOT (Date#0 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
   :           +- *Scan csv [Date#0,Close#4] Format: CSV, InputPaths: s3n://dataproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
   +- *Sort [datetime#81L ASC], false, 0
      +- Exchange hashpartitioning(datetime#81L, 200)
         +- *Project [if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
            +- *Filter ((isnotnull(Date#47) && NOT (Date#47 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
               +- *Scan csv [Date#47,Close#51] Format: CSV, InputPaths: s3n://dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
########
2016-10-22T20:58:15.994+0000: [GC (Allocation Failure) [PSYoungGen: 705910K->79079K(999936K)] 824644K->197829K(2974208K), 0.0294130 secs] [Times: user=0.26 sys=0.04, real=0.03 secs]
== Parsed Logical Plan ==
'Join UsingJoin(Inner,List('datetime))
:- Project [datetime#34L, spx#25]
:  +- Project [Date#0, Open#1, High#2, Low#3, spx#25, Volume#5, Adj Close#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
:     +- Project [Date#0, Open#1, High#2, Low#3, cast(spx#16 as double) AS spx#25, Volume#5, Adj Close#6]
:        +- Project [Date#0, Open#1, High#2, Low#3, Close#4 AS spx#16, Volume#5, Adj Close#6]
:           +- Filter NOT (Date#0 = Date)
:              +- Relation[Date#0,Open#1,High#2,Low#3,Close#4,Volume#5,Adj Close#6] csv
+- Project [datetime#81L, vix#72]
   +- Project [Date#47, Open#48, High#49, Low#50, vix#72, Volume#52, Adj Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
      +- Project [Date#47, Open#48, High#49, Low#50, cast(vix#63 as double) AS vix#72, Volume#52, Adj Close#53]
         +- Project [Date#47, Open#48, High#49, Low#50, Close#51 AS vix#63, Volume#52, Adj Close#53]
            +- Filter NOT (Date#47 = Date)
               +- Relation[Date#47,Open#48,High#49,Low#50,Close#51,Volume#52,Adj Close#53] csv

== Analyzed Logical Plan ==
datetime: bigint, spx: double, vix: double
Project [datetime#34L, spx#25, vix#72]
+- Join Inner, (datetime#34L = datetime#81L)
   :- Project [datetime#34L, spx#25]
   :  +- Project [Date#0, Open#1, High#2, Low#3, spx#25, Volume#5, Adj Close#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
   :     +- Project [Date#0, Open#1, High#2, Low#3, cast(spx#16 as double) AS spx#25, Volume#5, Adj Close#6]
   :        +- Project [Date#0, Open#1, High#2, Low#3, Close#4 AS spx#16, Volume#5, Adj Close#6]
   :           +- Filter NOT (Date#0 = Date)
   :              +- Relation[Date#0,Open#1,High#2,Low#3,Close#4,Volume#5,Adj Close#6] csv
   +- Project [datetime#81L, vix#72]
      +- Project [Date#47, Open#48, High#49, Low#50, vix#72, Volume#52, Adj Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
         +- Project [Date#47, Open#48, High#49, Low#50, cast(vix#63 as double) AS vix#72, Volume#52, Adj Close#53]
            +- Project [Date#47, Open#48, High#49, Low#50, Close#51 AS vix#63, Volume#52, Adj Close#53]
               +- Filter NOT (Date#47 = Date)
                  +- Relation[Date#47,Open#48,High#49,Low#50,Close#51,Volume#52,Adj Close#53] csv

== Optimized Logical Plan ==
Project [datetime#34L, spx#25, vix#72]
+- Join Inner, (datetime#34L = datetime#81L)
   :- Project [if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
   :  +- Filter ((isnotnull(Date#0) && NOT (Date#0 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
   :     +- Relation[Date#0,Open#1,High#2,Low#3,Close#4,Volume#5,Adj Close#6] csv
   +- Project [if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
      +- Filter ((isnotnull(Date#47) && NOT (Date#47 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
         +- Relation[Date#47,Open#48,High#49,Low#50,Close#51,Volume#52,Adj Close#53] csv

== Physical Plan ==
*Project [datetime#34L, spx#25, vix#72]
+- *SortMergeJoin [datetime#34L], [datetime#81L], Inner
   :- *Sort [datetime#34L ASC], false, 0
   :  +- Exchange hashpartitioning(datetime#34L, 200)
   :     +- *Project [if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
   :        +- *Filter ((isnotnull(Date#0) && NOT (Date#0 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
   :           +- *Scan csv [Date#0,Close#4] Format: CSV, InputPaths: s3n://dataproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
   +- *Sort [datetime#81L ASC], false, 0
      +- Exchange hashpartitioning(datetime#81L, 200)
         +- *Project [if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
            +- *Filter ((isnotnull(Date#47) && NOT (Date#47 = Date)) && isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
               +- *Scan csv [Date#47,Close#51] Format: CSV, InputPaths: s3n://dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
########
+-------------+-----------+---------+
|     datetime|        spx|      vix|
+-------------+-----------+---------+
| 931550400000|1403.280029|17.959999|
| 955742400000|1356.560059|33.490002|
| 962308800000|1442.390015|19.700001|
| 967752000000|1517.680054|    16.84|
| 995054400000|1215.680054|21.139999|
|1028145600000| 911.619995|32.029999|
|1049832000000| 878.289978|27.129999|
|1088452800000|1133.349976|    16.07|
|1097265600000|1122.140015|    15.05|
|1102539600000|1182.810059|    13.19|
|1147809600000|1292.079956|    13.35|
|1162414800000|1367.810059|    11.51|
|1266526800000|    1106.75|20.629999|
|1314043200000|1123.819946|42.439999|
|1319227200000|    1238.25|    31.32|
|1331928000000|1404.170044|    14.43|
|1377201600000|1656.959961|    14.76|
|1378756800000|1671.709961|    15.63|
|1390597200000|1790.290039|17.879999|
|1400616000000|1872.829956|    12.96|
+-------------+-----------+---------+






"
Michael Armbrust <michael@databricks.com>,"Sat, 22 Oct 2016 19:34:35 -0700",Re: Ran in to a bug in Broadcast Hash Join,Jeremy Davis <jerdavis@speakeasy.net>,"2.0.0 or 2.0.1?  There are several correctness fixes in the latter.


h very
========================================
4K->79382K(945664K)] 823872K->199145K(3089408K), 0.0285894 secs] [Times: user=0.29 sys=0.04, real=0.03 secs]
(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
ll(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
aproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
 true]))
imal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
notnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
//dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
S spx#25, Volume#5, Adj Close#6]
lume#5, Adj Close#6]
j Close#6] csv
Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
e) AS vix#72, Volume#52, Adj Close#53]
3, Volume#52, Adj Close#53]
#52,Adj Close#53] csv
ose#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
) AS spx#25, Volume#5, Adj Close#6]
 Volume#5, Adj Close#6]
,Adj Close#6] csv
dj Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
uble) AS vix#72, Volume#52, Adj Close#53]
x#63, Volume#52, Adj Close#53]
ume#52,Adj Close#53] csv
20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
l(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
e#6] csv
(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
ull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
j Close#53] csv
(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
ll(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
aproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
 true]))
imal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
notnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
//dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
====================================
====================================
====================================
ecimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
snotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
://dataproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
 isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
3n://dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
0K->79079K(999936K)] 824644K->197829K(2974208K), 0.0294130 secs] [Times: user=0.26 sys=0.04, real=0.03 secs]
#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
S spx#25, Volume#5, Adj Close#6]
lume#5, Adj Close#6]
j Close#6] csv
Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
e) AS vix#72, Volume#52, Adj Close#53]
3, Volume#52, Adj Close#53]
#52,Adj Close#53] csv
ose#6, if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L]
) AS spx#25, Volume#5, Adj Close#6]
 Volume#5, Adj Close#6]
,Adj Close#6] csv
dj Close#53, if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L]
uble) AS vix#72, Volume#52, Adj Close#53]
x#63, Volume#52, Adj Close#53]
ume#52,Adj Close#53] csv
20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
l(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
e#6] csv
(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
ull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
j Close#53] csv
ecimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)) AS datetime#34L, cast(Close#4 as double) AS spx#25]
snotnull(if (((isnull(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#0, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#0, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#0, 9, 2) as decimal(20,0)) as int))))
://dataproc-data/data/index/spx, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)) AS datetime#81L, cast(Close#51 as double) AS vix#72]
 isnotnull(if (((isnull(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int)) || isnull(cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int))) || isnull(cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int)))) null else UDF(cast(cast(substring(date#47, 1, 4) as decimal(20,0)) as int), cast(cast(substring(date#47, 6, 2) as decimal(20,0)) as int), cast(cast(substring(date#47, 9, 2) as decimal(20,0)) as int))))
3n://dataproc-data/data/index/vix, PushedFilters: [IsNotNull(Date), Not(EqualTo(Date,Date))], ReadSchema: struct<Date:string,Close:string>
"
Ran Bai <lizbai@icloud.com>,"Sun, 23 Oct 2016 20:21:31 +0800",LIMIT issue of SparkSQL,dev@spark.apache.org,"Hi all,

I found the runtime for query with or without ¡°LIMIT¡± keyword is the same. We looked into it and found actually there is ¡°GlobalLimit / LocalLimit¡± in logical plan, however no relevant physical plan there. Is this a bug or something else? Attached are the logical and physical plans when running ""SELECT * FROM seq LIMIT 1"".

More specifically, We expected a early stop upon getting adequate results.
Thanks so much.

Best,
Liz



---------------------------------------------------------------------"
Xiao Li <gatorsmile@gmail.com>,"Sun, 23 Oct 2016 07:11:16 -0700",Re: LIMIT issue of SparkSQL,Ran Bai <lizbai@icloud.com>,"Hi, Liz,

CollectLimit means `Take the first `limit` elements and collect them to a
single partition.`

Thanks,

Xiao

2016-10-23 5:21 GMT-07:00 Ran Bai <lizbai@icloud.com>:

word is the same.
lLimitâ€ in
.
"
Jacek Laskowski <jacek@japila.pl>,"Sun, 23 Oct 2016 21:56:26 +0200",Redundant method in SparkUI and entire SparkUITab?,dev <dev@spark.apache.org>,"Hi,

While reviewing SparkUI I found two artifacts -- appUIAddress +
appName (with the entire SparkUITab) -- that I believe are not needed
at all as they seem to introduce nothing.

Please have a look at https://github.com/apache/spark/pull/15603 and
let me know your thoughts.

I'd appreciate your comments to learn Spark better. Thanks.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Liz Bai <lizbai@icloud.com>,"Mon, 24 Oct 2016 11:40:09 +0800",Re: LIMIT issue of SparkSQL,Xiao Li <gatorsmile@gmail.com>,"Hi all,

Let me clarify the problem: 

Suppose we have a simple table `A` with 100 000 000 records

Problem:
When we execute sql query â€˜select * from A Limit 500`,
It scan through all 100 000 000 records. 
Normal behaviour should be that once 500 records is found, engine stop scanning.

Detailed observation:
We found that there are â€œGlobalLimit / LocalLimitâ€ physical operators
https://github.com/apache/spark/blob/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala <https://github.com/apache/spark/blob/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala>
But during query plan generation, GlobalLimit / LocalLimit is not applied to the query plan.

Could you please help us to inspect LIMIT problem? 
Thanks.

Best,
Liz
to a single partition.`
<mailto:lizbai@icloud.com>>:
keyword is the same. We looked into it and found actually there is â€œGlobalLimit / LocalLimitâ€ in logical plan, however no relevant physical plan there. Is this a bug or something else? Attached are the logical and physical plans when running ""SELECT * FROM seq LIMIT 1"".
results.
<mailto:dev-unsubscribe@spark.apache.org>

"
Xiao Li <gatorsmile@gmail.com>,"Sun, 23 Oct 2016 21:37:14 -0700",Re: LIMIT issue of SparkSQL,Liz Bai <lizbai@icloud.com>,"The rule SpecialLimits converted GlobalLimit / LocalLimit
to CollectLimitExec.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala#L74-L75

Spark will not scan all the records based on your plan. CollectLimitExec
should behave as you expected.

Thanks,

Xiao





2016-10-23 20:40 GMT-07:00 Liz Bai <lizbai@icloud.com>:

al operators
yword is the
 /
here. Is
s
s.
"
Michael Armbrust <michael@databricks.com>,"Mon, 24 Oct 2016 07:48:26 +0200",Re: LIMIT issue of SparkSQL,"Liz Bai <lizbai@icloud.com>, user <user@spark.apache.org>","- dev + user

Can you give more info about the query?  Maybe a full explain()?  Are you
using a datasource like JDBC?  The API does not currently push down limits,
but the documentation talks about how you can use a query instead of a
table if that is wha"
Chen Qiming <qimingch@usc.edu>,"Sun, 23 Oct 2016 23:52:36 -0700",unsubscribe,dev@spark.apache.org,unsubscribe
Sean Owen <sowen@cloudera.com>,"Mon, 24 Oct 2016 12:51:36 +0000","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide","Holden Karau <holden@pigscanfly.ca>, ""dev@spark.apache.org"" <dev@spark.apache.org>","BTW I wrote up a straw-man proposal for migrating the wiki content:

https://issues.apache.org/jira/browse/SPARK-18073


"
Marco <rocchi.1407763@studenti.uniroma1.it>,"Mon, 24 Oct 2016 16:41:19 +0200",Dynamic Graph Handling,dev@spark.apache.org,"Hi,

I'm a student in Computer Science and I'm working for my master thesis=20
on Graph Partitioning problem, focusing on dynamic graph.

I'm searching for a framework to manage Dynamic Graph, with possible=20
disappearing of edges/nodes. Now the problem is: GraphX alone cannot=20
provide solution to manages this kind of graph, and searching throw=20
internet I didn't found nothing of relevant. There is a framework or a=20
way to handle dynamic graphs?

Thanks in advance

Marco Rocchi


---------------------------------------------------------------------


"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Mon, 24 Oct 2016 16:48:52 +0200",Re: Dynamic Graph Handling,Marco <rocchi.1407763@studenti.uniroma1.it>,"Maybe titandb ?! It uses Hbase to store graphs and solr (on HDFS) to index graphs. I am not 100% sure it supports it, but probably.
It can also integrate Spark, but analytics on a given graph only.
Otherwise you need to go for dedicated graph system.

e:
0
0

---------------------------------------------------------------------


"
"""Joseph E. Gonzalez"" <joseph.e.gonzalez@gmail.com>","Mon, 24 Oct 2016 08:53:55 -0700",Re: Dynamic Graph Handling,=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"What kind of partitioning are you exploring?  GraphX actually has some built in partitioning algorithms but if you are interested in spectral or hierarchical methods you might want to look at Metis/Zoltan?   There was some interest in integrating Metis style algorithms in Spark (GraphX or GraphFrames) so you may consider porting those algorithms.  As far as dynamic graphs go, there is an ongoing effort at Berkeley to study dynamic graphs in Spark. I have ccâ€™ed one of the key graduate students in that effort.  I hope that helps.

Good Luck!
Joey




index graphs. I am not 100% sure it supports it, but probably.
thesis=20
possible=20
cannot=20
throw=20
a=20


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 24 Oct 2016 10:11:35 -0700","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide",Sean Owen <sowen@cloudera.com>,"<rant>Alright, that does it!  Who is responsible for this ""straw-man"" abuse
that is becoming too commonplace in the Spark community?  ""Straw-man"" does
not mean something like ""trial balloon"" or ""run it up the flagpole and see
if anyone salutes"", and I would really appreciate it if Spark developers
would stop using ""straw-man"" to mean anything other than its established
meaning: The logical fallacy of declaring victory by knocking down an
easily defeated argument or position that the opposition has never actually
made.</rant>


"
Sean Owen <sowen@cloudera.com>,"Mon, 24 Oct 2016 17:38:09 +0000","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide",Mark Hamstra <mark@clearstorydata.com>,"Well, it's more of a reference to the fallacy than anything. Writing down a
proposed action implicitly claims it's what others are arguing for. It's
self-deprecating to call it a ""straw man"", suggesting that it may not at
all be what others are arguing for, and is done to openly invite criticism
and feedback. The logical fallacy is ""attacking a straw man"", and that's
not what was written here.

Really, the important thing is that we understand each other, and I'm
guessing you did. Although I think the usage here is fine, casually,
avoiding idioms is best, where plain language suffices, especially given we
have people from lots of language backgrounds here.



"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 24 Oct 2016 11:05:54 -0700","Re: Mini-Proposal: Make it easier to contribute to the contributing
 to Spark Guide",Sean Owen <sowen@cloudera.com>,"The advice to avoid idioms that may not be universally understood is good.
My further issue with the misuse of ""straw-man"" (which really is not, or
should not be, separable from ""straw-man argument"") is that a ""straw-man""
in the established usage is something that is always intended to be a
failure or designed to be obviously and fatally flawed.  That's what makes
it fundamentally different from a trial balloon or a first crack at
something or a prototype or an initial design proposal -- these are all
intended, despite any remaining flaws, to have merits that are likely worth
pursuing further, whereas a straw-man is only intended to be knocked apart
as a way to preclude and put an end to further consideration of something.



"
Matt Smith <matt.smith123@gmail.com>,"Tue, 25 Oct 2016 04:16:03 +0000",collect_list alternative for SQLContext?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is there an alternative function or design pattern for the collect_list
UDAF that can used without taking a dependency on HiveContext?  How does
one typically roll things up into an array when outputting JSON?
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 25 Oct 2016 08:33:38 +0200",Re: collect_list alternative for SQLContext?,Matt Smith <matt.smith123@gmail.com>,"What version of Spark are you using? We introduced a Spark native
collect_list in 2.0.

It still has the usual caveats, but it should quite a bit faster.


"
Reynold Xin <rxin@databricks.com>,"Tue, 25 Oct 2016 08:33:41 +0200",Re: collect_list alternative for SQLContext?,Matt Smith <matt.smith123@gmail.com>,"This shouldn't be required anymore since Spark 2.0.



"
Sean Owen <sowen@cloudera.com>,"Tue, 25 Oct 2016 14:02:14 +0000",PSA: watch the apache/spark-website repo if interested,dev <dev@spark.apache.org>,"I don't believe emails about the spark-website repo are forwarded to the
project mailing lists. If you want to watch for them, go star/watch the
repo to be sure. I just opened a PR, for example.

https://github.com/apache/spark-website
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 25 Oct 2016 08:11:30 -0700 (MST)",Converting spark types and standard scala types,dev@spark.apache.org,"Hi,
I am trying to write a new aggregate function (https://issues.apache.org/jira/browse/SPARK-17691) and I wanted it to support all ordered types.
I have several  issues though:

1.       How to convert the type of the child expression to a Scala standard type (e.g. I need an Array[Int] for IntegerType and an Array[Double] for DoubleType). The only method I found so far is to do a match for each of the types. Is there a better way?

2.       What would be the corresponding scala type for DecimalType, TimestampType, DateType and BinaryType?

3.       Should BinaryType be a legal type for such a function?

4.       I need to serialize the relevant array of type (i.e. turn it into an Array[Byte] for working with TypedImperativeAggregate). I can do a match for standard types such as Int and Double but I do not know of a generic way to do it.
Thanks,
                Assaf.




--"
Sean Owen <sowen@cloudera.com>,"Tue, 25 Oct 2016 15:36:45 +0000",Straw poll: dropping support for things like Scala 2.10,dev <dev@spark.apache.org>,"I'd like to gauge where people stand on the issue of dropping support for a
few things that were considered for 2.0.

First: Scala 2.10. We've seen a number of build breakages this week because
the PR builder only tests 2.11. No big deal at this stage, but, it did
cause me to wonder whether it's time to plan to drop 2.10 support,
especially with 2.12 coming soon.

Next, Java 7. It's reasonably old and out of public updates at this stage.
It's not that painful to keep supporting, to be honest. It would simplify
some bits of code, some scripts, some testing.

Hadoop versions: I think the the general argument is that most anyone would
be using, at the least, 2.6, and it would simplify some code that has to
reflect to use not-even-that-new APIs. It would remove some moderate
complexity in the build.


""When"" is a tricky question. Although it's a little aggressive for minor
releases, I think these will all happen before 3.x regardless. 2.1.0 is not
out of the question, though coming soon. What about ... 2.2.0?


Although I tend to favor dropping support, I'm mostly asking for current
opinions.
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 25 Oct 2016 08:47:40 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Sean Owen <sowen@cloudera.com>,"What's changed since the last time we discussed these issues, about 7
months ago?  Or, another way to formulate the question: What are the
threshold criteria that we should use to decide when to end Scala 2.10
and/or Java 7 support?


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 25 Oct 2016 08:45:07 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Sean Owen <sowen@cloudera.com>,"I'd also like to add Python 2.6 to the list of things. We've considered
dropping it before but never followed through to the best of my knowledge
(although on mobile right now so can't double check).




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Sean Owen <sowen@cloudera.com>,"Tue, 25 Oct 2016 15:56:20 +0000",Re: Straw poll: dropping support for things like Scala 2.10,Mark Hamstra <mark@clearstorydata.com>,"The general forces are that new versions of things to support emerge, and
are valuable to support, but have some cost to support in addition to old
versions. And the old versions become less used and therefore less valuable
to support, and at some point it tips to being more cost than value. It's
hard to judge these costs and benefits.

Scala is perhaps the trickiest one because of the general mutual
incompatibilities across minor versions. The cost of supporting multiple
versions is high, and a third version is about to arrive. That's probably
the most pressing question. It's actually biting with some regularity now,
with compile errors on 2.10.

(Python I confess I don't have an informed opinion about.)

Java, Hadoop are not as urgent because they're more backwards-compatible.
Anecdotally, I'd be surprised if anyone today would ""upgrade"" to Java 7 or
an old Hadoop version. And I think that's really the question. Even if one
decided to drop support for all this in 2.1.0, it would not mean people
can't use Spark with these things. It merely means they can't necessarily
use Spark 2.1.x. This is why we have maintenance branches for 1.6.x, 2.0.x.

Tying Scala 2.11/12 support to Java 8 might make sense.

In fact, I think that's part of the reason that an update in master,
perhaps 2.1.x, could be overdue, because it actually is just the beginning
of the end of the support burden. If you want to stop dealing with these in
~6 months they need to stop being supported in minor branches by right
about now.





"
Cody Koeninger <cody@koeninger.org>,"Tue, 25 Oct 2016 11:28:05 -0500",Re: Straw poll: dropping support for things like Scala 2.10,Sean Owen <sowen@cloudera.com>,"I think only supporting 1 version of scala at any given time is not
sufficient, 2 probably is ok.

I.e. don't drop 2.10 before 2.12 is out + supported


---------------------------------------------------------------------


"
Ofir Manor <ofir.manor@equalum.io>,"Tue, 25 Oct 2016 19:31:42 +0300",Re: Straw poll: dropping support for things like Scala 2.10,Cody Koeninger <cody@koeninger.org>,"I think that 2.1 should include a visible deprecation message about Java 7,
Scala 2.10 and older Hadoop versions (plus python if there is a consensus
on that), to give users / admins early warning, followed by dropping them
from trunk for 2.2 once 2.1 is released.
Personally, we use only Scala 2.11 on JDK8.
Cody - Scala 2.12 will likely be released before Spark 2.1, maybe even
later this week: http://scala-lang.org/news/2.12.0-RC2

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
Daniel Siegmann <dsiegmann@securityscorecard.io>,"Tue, 25 Oct 2016 13:03:58 -0400",Re: Straw poll: dropping support for things like Scala 2.10,Ofir Manor <ofir.manor@equalum.io>,"After support is dropped for Java 7, can we have encoders for java.time
classes (e.g. LocalDate)? If so, then please drop support for Java 7 ASAP.
:-)
"
Koert Kuipers <koert@tresata.com>,"Tue, 25 Oct 2016 13:09:10 -0400",Re: Straw poll: dropping support for things like Scala 2.10,Ofir Manor <ofir.manor@equalum.io>,"it will take time before all libraries that spark depends on are available
for scala 2.12, so we are not talking spark 2.1.x and probably also not
2.2.x for scala 2.12

it technically makes sense to drop java 7 and scala 2.10 around the same
time as scala 2.12 is introduced

we are still heavily dependent on java 7 (and python 2.6 if we used python
but we dont). i am surprised to see new clusters installed in last few
months (CDH and HDP latest versions) to still be running on java 7. even
getting java 8 installed on these clusters so we can use them in yarn is
often not an option. it beats me as to why this is still happening.

we do not use scala 2.10 at all anymore.


"
Koert Kuipers <koert@tresata.com>,"Tue, 25 Oct 2016 14:52:49 -0400",getting encoder implicits to be more accurate,"""dev@spark.apache.org"" <dev@spark.apache.org>","i am trying to use encoders as a typeclass where if it fails to find an
ExpressionEncoder it falls back to KryoEncoder.

the issue seems to be that ExpressionEncoder claims a little more than it
can handle here:
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] =
Encoders.product[T]

this ""claims"" to handle for example Option[Set[Int]], but it really cannot
handle Set so it leads to a runtime exception.

would it be useful to make this a little more specific? i guess the
challenge is going to be case classes which unfortunately dont extend
Product1, Product2, etc.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 25 Oct 2016 19:13:27 +0000",Re: Straw poll: dropping support for things like Scala 2.10,"Holden Karau <holden@pigscanfly.ca>, Sean Owen <sowen@cloudera.com>","FYI: Support for both Python 2.6 and Java 7 was deprecated in 2.0 (see release
notes <http://spark.apache.org/releases/spark-release-2-0-0.html> under
Deprecations). The deprecation notice didn't offer a specific timeline for
completely dropping support other than to say they ""might be removed in
future versions of Spark 2.x"".

Not sure what the distinction between deprecating and dropping support is
for language versions, since in both cases it seems like it's OK to do
things not compatible with the deprecated versions.

Nick



"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 25 Oct 2016 12:19:10 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Nicholas Chammas <nicholas.chammas@gmail.com>,"No, I think our intent is that using a deprecated language version can
generate warnings, but that it should still work; whereas once we remove
support for a language version, then it really is ok for Spark developers to
do things not compatible with that version and for users attempting to use
that version to encounter errors.

With that understanding, the first steps toward removing support for Scala
2.10 and/or Java 7 would be to deprecate them in 2.1.0.  Actual removal of
support could then occur at the earliest in 2.2.0.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 25 Oct 2016 19:59:48 +0000",Re: Straw poll: dropping support for things like Scala 2.10,Mark Hamstra <mark@clearstorydata.com>,"No, I think our intent is that using a deprecated language version can
generate warnings, but that it should still work; whereas once we remove
support for a language version, then it really is ok for Spark developers
to do things not compatible with that version and for users attempting to
use that version to encounter errors.

OK, understood.

With that understanding, the first steps toward removing support for Scala
2.10 and/or Java 7 would be to deprecate them in 2.1.0. Actual removal of
support could then occur at the earliest in 2.2.0.

Java 7 is already deprecated per the 2.0 release notes which I linked to. Here
they are
<http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations>
again.
â€‹


 to
e
a
f
lease
r
t
.
as
ot
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 25 Oct 2016 14:32:48 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Nicholas Chammas <nicholas.chammas@gmail.com>,"You're right; so we could remove Java 7 support in 2.1.0.

Both Holden and I not having the facts immediately to mind does suggest,
however, that we should be doing a better job of making sure that
information about deprecated language versions is inescapably public.
That's harder to do with a language version deprecation since using such a
version doesn't really give you the same kind of repeated warnings that
using a deprecated API does.


a
 Here
s to
se
elease
or
s
e
r
it
has
not
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 25 Oct 2016 22:15:31 +0000",Re: Straw poll: dropping support for things like Scala 2.10,Mark Hamstra <mark@clearstorydata.com>,"Agreed. Would an announcement/reminder on the dev and user lists suffice in
this case? Basically, just point out what's already been mentioned in the
2.0 release notes, and include a link there so people know what we're
referencing.
2016ë…„ 10ì›” 25ì¼ (í™”) ì˜¤í›„ 5:32, Mark Hamstra <mark@clearstorydata.com>ë‹˜ì´ ìž‘ì„±:

a
a
 Here
 to
e
a
f
lease
r
t
.
as
ot
"
Michael Hall <mhall119@gmail.com>,"Tue, 25 Oct 2016 18:46:18 -0400",Spark deployed as a Snap package,dev@spark.apache.org,"Iâ€™m Michael Hall. I work at Canonical as part of the engineering team
around Ubuntu and Snapcraft [1].


Weâ€™re working on snaps, a platform to enable ISVs to directly control
delivery of software updates to their users, and make their software
available to a considerably wider audience.


There has already been some interest from Spark users on making it
available as a Snap[2]. I was wondering if someone here would like to
discuss how to make that happen.


[1] http://snapcraft.io/
[2]
https://stackoverflow.com/questions/38526154/how-can-i-put-apache-spark-into-snapcraft

-- 
Michael Hall
mhall119@gmail.com

---------------------------------------------------------------------


"
shirisht <shirish.tatikonda@gmail.com>,"Tue, 25 Oct 2016 23:51:09 -0700 (MST)",SparkR issue with array types in gapply(),dev@spark.apache.org,"Hello,

I am getting an exception from catalyst when array types are used in the
return schema of gapply() function. 

Following is a (made-up) example:

------------------------------------------------------------
iris$flag = base::sample(1:2, nrow(iris), T, prob = c(0.5,0.5))
irisdf = createDataFrame(iris)

foo = function(key, x) {
  nr = nrow(x)
  nc = ncol(x)
  arr = c(  paste(""rows="", nr), paste(""cols="",nc) )
  data.frame(key, strings = arr, stringsAsFactors = F)
}

outSchema = structType(  structField('key', 'integer'),
                         structField('strings', 'array<string>')  )
result = SparkR::gapply(irisdf, ""flag"", foo, outSchema)
d = SparkR::collect(result)
------------------------------------------------------------

This code throws up the following error:

java.lang.RuntimeException: java.lang.String is not a valid external type
for schema of array<string>
	at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown
Source)
	at
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at
org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
	at
org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Any thoughts?

Thank you,
Shirish



--

---------------------------------------------------------------------


"
Manoj Shah <manojshah360@gmail.com>,"Wed, 26 Oct 2016 07:59:55 +0100",Unsubscribe,dev@spark.apache.org,"Unsubscribe
"
Liz Bai <lizbai@icloud.com>,"Wed, 26 Oct 2016 19:25:13 +0800",LIMIT statement on SparkSQL,dev@spark.apache.org,"Hi all,

We used Parquet and Spark 2.0 to do the testing. The table below is the summary of what we have found about `Limit` keyword. Query-2 reveals that SparkSQL does early stop upon getting adequate results. But we are curious of Query-1 and Query-2. It seems that, either writing result RDD as Parquet or filtering on columns will lead to scanning much more data.
No.
SQL statement
Filter
Method of saving result
Runtime(s)
Input data size
1
select ColA from Table limit 1
no
writeParquet
216
205MB
2
select ColA from Table limit 1
no
Collect
22
38.3KB
3
select ColA from Table where ColB = 50 limit 1
yes
Collect
229
1776.4MB
We are wondering if this is a bug or something else. Could you please help on it?
Thanks.

Best regards,
Liz"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 26 Oct 2016 04:30:09 -0700 (MST)",Using SPARK_WORKER_INSTANCES and SPARK-15781,dev@spark.apache.org,"As of applying SPARK-15781 the documentation of SPARK_WORKER_INSTANCES have been removed. This was due to a warning in spark-submit which suggested:
WARN SparkConf:
SPARK_WORKER_INSTANCES was detected (set to '4').
This is deprecated in Spark 1.0+.

Please instead use:
- ./spark-submit with --num-executors to specify the number of executors
- Or set SPARK_EXECUTOR_INSTANCES
- spark.executor.instances to configure the number of instances in the spark config.



The problem is that there is no replacement method to launch spark standalone with multiple workers per node. The options -num-executors and SPARK_EXECUTOR_INSTANCES configure the job rather than the resource manager behavior.

If I look at the spark standalone scripts, the only way to set multiple workers per node is the use of SPARK_WORKER_INSTANCES. The fixed in SPARK-15781 fixed the documentation without solving the problem.
A possible simple solution would be to add a SPARK_STANDALONE_WORKERS variable and add it to the start-slave.sh script and update the documentation accordingly.

Am I missing something here? Should I open a new JIRA issue?
Thanks,
                Assaf




--"
Liz Bai <lizbai@icloud.com>,"Wed, 26 Oct 2016 21:37:33 +0800",Re: LIMIT statement on SparkSQL,dev@spark.apache.org,"Sorry for the typo in last mail.
Compared with the Query-2, we have questions in Query-1 and Query-3. 
Also, may I know the difference between CollectLimit and BaseLimit?
Thanks so much.

Best,
Liz
the summary of what we have found about `Limit` keyword. Query-2 reveals that SparkSQL does early stop upon getting adequate results. But we are curious of Query-1 and Query-2.
*But we are curious of Query-1 and Query-3.
columns will lead to scanning much more data.
help on it?

"
"""Dongjoon Hyun""<dongjoon@apache.org>","Wed, 26 Oct 2016 17:26:50 -0000",Re: Straw poll: dropping support for things like Scala 2.10,<dev@spark.apache.org>,"Hi, All.

It's great since it's a progress.

Then, at least, in 2017, Spark 2.2.0 will be out with JDK8 and Scala 2.11/2.12, right?

Bests,
Dongjoon.

---------------------------------------------------------------------


"
Daniel Siegmann <dsiegmann@securityscorecard.io>,"Wed, 26 Oct 2016 13:45:38 -0400",Re: Straw poll: dropping support for things like Scala 2.10,dev <dev@spark.apache.org>,"Is the deprecation of JDK 7 and Scala 2.10 documented anywhere outside the
release notes for Spark 2.0.0? I do not consider release notes to be
sufficient public notice for deprecation of supported platforms - this
should be noted in the documentation somewhere. Here are on the only
mentions I could find:

At http://spark.apache.org/downloads.html it says:

""*Note: Starting version 2.0, Spark is built with Scala 2.11 by default.
Scala 2.10 users should download the Spark source package and build with
Scala 2.10 support
<http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-210>.""*

At http://spark.apache.org/docs/latest/#downloading it says:

""Spark runs on Java 7+, Python 2.6+/3.4+ and R 3.1+. For the Scala API,
Spark 2.0.1 uses Scala 2.11. You will need to use a compatible Scala
version (2.11.x).""

At
http://spark.apache.org/docs/latest/programming-guide.html#linking-with-spark
it says:

   - ""Spark 2.0.1 is built and distributed to work with Scala 2.11 by
   default. (Spark can be built to work with other versions of Scala, too.) To
   write applications in Scala, you will need to use a compatible Scala
   version (e.g. 2.11.X).""
   - ""Spark 2.0.1 works with Java 7 and higher. If you are using Java 8,
   Spark supports lambda expressions
   <http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html>
   for concisely writing functions, otherwise you can use the classes in the
   org.apache.spark.api.java.function
   <http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/function/package-summary.html>
   package.""
   - ""Spark 2.0.1 works with Python 2.6+ or Python 3.4+. It can use the
   standard CPython interpreter, so C libraries like NumPy can be used. It
   also works with PyPy 2.3+.""
"
Dongjoon Hyun <dongjoon@apache.org>,"Wed, 26 Oct 2016 10:50:52 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Daniel Siegmann <dsiegmann@securityscorecard.io>,"Hi, Daniel.

I guess that kind of works will start sufficiently in 2.1.0 after PMC's
annoucement/reminder on mailing list.

Bests,
Dongjoon.



"
Michael Armbrust <michael@databricks.com>,"Wed, 26 Oct 2016 20:18:09 +0200",Re: getting encoder implicits to be more accurate,Koert Kuipers <koert@tresata.com>,"Hmm, that is unfortunate.  Maybe the best solution is to add support for
sets?  I don't think that would be super hard.


"
Reynold Xin <rxin@databricks.com>,"Wed, 26 Oct 2016 20:26:11 +0200",Re: Straw poll: dropping support for things like Scala 2.10,Dongjoon Hyun <dongjoon@apache.org>,"We can do the following concrete proposal:

1. Plan to remove support for Java 7 / Scala 2.10 in Spark 2.2.0 (Mar/Apr
2017).

2. In Spark 2.1.0 release, aggressively and explicitly announce the
deprecation of Java 7 / Scala 2.10 support.

(a) It should appear in release notes, documentations that mention how to
build Spark

(b) and a warning should be shown every time SparkContext is started using
Scala 2.10 or Java 7.




"
Ryan Blue <rblue@netflix.com.INVALID>,"Wed, 26 Oct 2016 11:33:14 -0700",Re: getting encoder implicits to be more accurate,Michael Armbrust <michael@databricks.com>,"Isn't the problem that Option is a Product and the class it contains isn't
checked? Adding support for Set fixes the example, but the problem would
happen with any class there isn't an encoder for, right?




-- 
Ryan Blue
Software Engineer
Netflix
"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Oct 2016 14:47:23 -0400",Re: getting encoder implicits to be more accurate,Ryan Blue <rblue@netflix.com>,"yup, it doesnt really solve the underlying issue.

we fixed it internally by having our own typeclass that produces encoders
and that does check the contents of the products, but we did this by simply
supporting Tuple1 - Tuple22 and Option explicitly, and not supporting
Product, since we dont have a need for case classes

if case classes extended ProductN (which they will i think in scala 2.12?)
then we could drop Product and support Product1 - Product22 and Option
explicitly while checking the classes they contain. that would be the
cleanest.



"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Oct 2016 14:49:01 -0400",Re: Straw poll: dropping support for things like Scala 2.10,Reynold Xin <rxin@databricks.com>,"that sounds good to me


"
Michael Armbrust <michael@databricks.com>,"Wed, 26 Oct 2016 12:35:04 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Reynold Xin <rxin@databricks.com>,1
Michael Armbrust <michael@databricks.com>,"Wed, 26 Oct 2016 12:50:23 -0700",Re: getting encoder implicits to be more accurate,Koert Kuipers <koert@tresata.com>,"Sorry, I realize that set is only one example here, but I don't think that
making the type of the implicit more narrow to include only ProductN or
something eliminates the issue.  Even with that change, we will fail to
generate an encoder with the same error if you, for example, have a field
of your case class that is an unsupported type.

Short of changing this to compile-time macros, I think we are stuck with
this class of errors at runtime.  The simplest solution seems to be to
expand the set of thing we can handle as much as possible and allow users
to turn on a kryo fallback for expression encoders.  I'd be hesitant to
make this the default though, as behavior would change with each release
that adds support for more types.  I would be very supportive of making
this fallback a built-in option though.


"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Oct 2016 16:00:44 -0400",Re: getting encoder implicits to be more accurate,Michael Armbrust <michael@databricks.com>,"why would generating implicits for ProductN where you also require the
elements in the Product to have an expression encoder not work?

we do this. and then we have a generic fallback where it produces a kryo
encoder.

for us the result is that say an implicit for Seq[(Int, Seq[(String,
Int)])] will create a new ExpressionEncoder(), while an implicit for
Seq[(Int, Set[(String, Int)])] produces a Encoders.kryoEncoder()


"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Oct 2016 16:07:21 -0400",Re: getting encoder implicits to be more accurate,Michael Armbrust <michael@databricks.com>,"for example (the log shows when it creates a kryo encoder):

scala> implicitly[EncoderEvidence[Option[Seq[String]]]].encoder
res5: org.apache.spark.sql.Encoder[Option[Seq[String]]] = class[value[0]:
array<string>]

scala> implicitly[EncoderEvidence[Option[Set[String]]]].encoder
dataframe.EncoderEvidence$: using kryo encoder for scala.Option[Set[String]]
res6: org.apache.spark.sql.Encoder[Option[Set[String]]] = class[value[0]:
binary]





"
Michael Armbrust <michael@databricks.com>,"Wed, 26 Oct 2016 14:06:59 -0700",Re: getting encoder implicits to be more accurate,Koert Kuipers <koert@tresata.com>,"You use kryo encoder for the whole thing?  Or just the subtree that we
don't have specific encoders for?

Also, I'm saying I like the idea of having a kryo fallback.  I don't see
the point of narrowing the the definition of the implicit.


"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Oct 2016 17:10:39 -0400",Re: getting encoder implicits to be more accurate,Michael Armbrust <michael@databricks.com>,"i use kryo for the whole thing currently

it would be better to use it for the subtree


"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Oct 2016 17:11:57 -0400",Re: getting encoder implicits to be more accurate,Michael Armbrust <michael@databricks.com>,"if kryo could transparently be used for subtrees without narrowing the
implicit that would be great


"
Michael Armbrust <michael@databricks.com>,"Wed, 26 Oct 2016 14:16:37 -0700",Re: getting encoder implicits to be more accurate,Koert Kuipers <koert@tresata.com>,"Awesome, this is a great idea.  I opened SPARK-18122
<https://issues.apache.org/jira/browse/SPARK-18122>.


"
Tathagata Das <tdas@databricks.com>,"Wed, 26 Oct 2016 16:56:42 -0700",Watermarking in Structured Streaming to drop late data,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

We are planning implement watermarking in Structured Streaming that would
allow us handle late, out-of-order data better. Specially, when we are
aggregating over windows on event-time, we currently can end up keeping
unbounded amount data as state. We want to define watermarks on the event
time in order mark and drop data that are ""too late"" and accordingly age
out old aggregates that will not be updated any more.

To enable the user to specify details like lateness threshold, we are
considering adding a new method to Dataset. We would like to get more
feedback on this API. Here is the design doc

https://docs.google.com/document/d/1z-Pazs5v4rA31azvmYhu4I5xwqaNQl6Z
LIS03xhkfCQ/

Please comment on the design and proposed APIs.

Thank you very much!

TD
"
Michael Armbrust <michael@databricks.com>,"Wed, 26 Oct 2016 17:03:46 -0700",Re: Watermarking in Structured Streaming to drop late data,Tathagata Das <tdas@databricks.com>,"And the JIRA: https://issues.apache.org/jira/browse/SPARK-18124


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 26 Oct 2016 23:46:28 -0700 (MST)",RE: Watermarking in Structured Streaming to drop late data,dev@spark.apache.org,"Hi,
Should comments come here or in the JIRA?
Any, I am a little confused on the need to expose this as an API to begin with.
Letâ€™s consider for a second the most basic behavior: We have some input stream and we want to aggregate a sum over a time window.
This means that the window we should be looking at would be the maximum time across our data and back by the window interval. Everything older can be dropped.
When new data arrives, the maximum time cannot move back so we generally drop everything tool old.
This basically means we save only the latest time window.
This simpler model would only break if we have a secondary aggregation which needs the results of multiple windows.
Is this the use case we are trying to solve?
If so, wouldnâ€™t just calculating the bigger time window across the entire aggregation solve this?
Am I missing something here?

From: Michael Armbrust [via Apache Spark Developers List] [mailto:ml-node+s1001551n19590h79@n3.nabble.com]
Sent: Thursday, October 27, 2016 3:04 AM
To: Mendelson, Assaf
Subject: Re: Watermarking in Structured Streaming to drop late data

And the JIRA: https://issues.apache.org/jira/browse/SPARK-18124

Hey all,

We are planning implement watermarking in Structured Streaming that would allow us handle late, out-of-order data better. Specially, when we are aggregating over windows on event-time, we currently can end up keeping unbounded amount data as state. We want to define watermarks on the event time in order mark and drop data that are ""too late"" and accordingly age out old aggregates that will not be updated any more.

To enable the user to specify details like lateness threshold, we are considering adding a new method to Dataset. We would like to get more feedback on this API. Here is the design doc

https://docs.google.com/document/d/1z-Pazs5v4rA31azvmYhu4I5xwqaNQl6ZLIS03xhkfCQ/

Please comment on the design and proposed APIs.

Thank you very much!

TD


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Watermarking-in-Structured-Streaming-to-drop-late-data-tp19589p19590.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Watermarking-in-Structured-Streaming-to-drop-late-data-tp19589p19591.html
om."
kostas papageorgopoylos <p02096@gmail.com>,"Thu, 27 Oct 2016 10:16:55 +0300",Re: Watermarking in Structured Streaming to drop late data,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Hi all

I would highly recommend to all users-devs interested in the design
suggestions / discussions for Structured Streaming Spark API watermarking
to take a look on the following links along with the design document. It
would help to understand the notions of watermark , out of order data and
possible use cases.

https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101
https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102

Kind Regards


2016-10-27 9:46 GMT+03:00 assaf.mendelson <assaf.mendelson@rsa.com>:

 input
n
e entire
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
n-Structured-Streaming-to-drop-late-data-tp19589p19591.html>
"
Reynold Xin <rxin@databricks.com>,"Thu, 27 Oct 2016 09:18:34 +0200",[VOTE] Release Apache Spark 2.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Greetings from Spark Summit Europe at Brussels.

Please vote on releasing the following candidate as Apache Spark version
2.0.2. The vote is open until Sun, Oct 30, 2016 at 00:30 PDT and passes if
a majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.2
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.2-rc1
(1c2908eeb8890fdc91413a3f5bad2bb3d114db6c)

This release candidate resolves 75 issues:
https://s.apache.org/spark-2.0.2-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1208/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc1-docs/


Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.1.

Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series. Bugs already present
in 2.0.1, missing features, or bugs related to new features will not
necessarily block this release.

Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.3, rather than 2.0.2. If a new RC
(i.e. RC2) is cut, I will change the fix version of those patches to 2.0.2.
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 27 Oct 2016 09:46:50 +0200",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,1
Sean Owen <sowen@cloudera.com>,"Thu, 27 Oct 2016 08:03:14 +0000",Re: Straw poll: dropping support for things like Scala 2.10,"Koert Kuipers <koert@tresata.com>, Reynold Xin <rxin@databricks.com>","Seems OK by me.
How about Hadoop < 2.6, Python 2.6? Those seem more removeable. I'd like to
add that to a list of things that will begin to be unsupported 6 months
from now.


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 27 Oct 2016 08:19:00 +0000",Re: Straw poll: dropping support for things like Scala 2.10,Sean Owen <sowen@cloudera.com>,"

Seems OK by me.
How about Hadoop < 2.6, Python 2.6? Those seem more removeable. I'd like to add that to a list of things that will begin to be unsupported 6 months from now.


If you go to java 8 only, then hadoop 2.6+ is mandatory.


that sounds good to me

We can do the following concrete proposal:

1. Plan to remove support for Java 7 / Scala 2.10 in Spark 2.2.0 (Mar/Apr 2017).

2. In Spark 2.1.0 release, aggressively and explicitly announce the deprecation of Java 7 / Scala 2.10 support.

(a) It should appear in release notes, documentations that mention how to build Spark

(b) and a warning should be shown every time SparkContext is started using Scala 2.10 or Java 7.


"
Ofir Manor <ofir.manor@equalum.io>,"Thu, 27 Oct 2016 11:52:57 +0300",Re: Watermarking in Structured Streaming to drop late data,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Assaf,
I think you are using the term ""window"" differently than Structured
Streaming,... Also, you didn't consider groupBy. Here is an example:
I want to maintain, for every minute over the last six hours, a computation
(trend or average or stddev) on a five-minute window (from t-4 to t). So,
1. My window size is 5 minutes
2. The window slides every 1 minute (so, there is a new 5-minute window for
every minute)
3. Old windows should be purged if they are 6 hours old (based on event
time vs. clock?)
Option 3 is currently missing - the streaming job keeps all windows
forever, as the app may want to access very old windows, unless it would
explicitly say otherwise.


Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


 input
n
e entire
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
n-Structured-Streaming-to-drop-late-data-tp19589p19591.html>
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 27 Oct 2016 03:14:49 -0700",Re: Watermarking in Structured Streaming to drop late data,Ofir Manor <ofir.manor@equalum.io>,"Hello Assaf,

I think you are missing the fact that we want to compute over event-time of
the data (e.g. data generation time), which may arrive at Spark
out-of-order and late. And we want to aggregate over late data. The
watermark is an estimate made by the system that there wont be any data
later than the watermark time arriving after now.

If this basic context is clear, then please read the design doc for further
details. Please comments in the doc for more specific design discussions.


4
n
e input
an
he entire
d
t
n
l
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
in-Structured-Streaming-to-drop-late-data-tp19589p19591.html>
"
Reynold Xin <rxin@databricks.com>,"Thu, 27 Oct 2016 12:15:22 +0200",Re: Straw poll: dropping support for things like Scala 2.10,Steve Loughran <stevel@hortonworks.com>,"I created a JIRA ticket to track this:
https://issues.apache.org/jira/browse/SPARK-18138




"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 27 Oct 2016 03:28:33 -0700 (MST)",RE: Watermarking in Structured Streaming to drop late data,dev@spark.apache.org,"Thanks.
This article is excellent. It completely explains everything.
I would add it as a reference to any and all explanations of structured streaming (and in the case of watermarking, I simply didnâ€™t understand the definition before reading this).

Thanks,
                Assaf.


From: kostas papageorgopoylos [via Apache Spark Developers List] [mailto:ml-node+s1001551n19592h41@n3.nabble.com]
Sent: Thursday, October 27, 2016 10:17 AM
To: Mendelson, Assaf
Subject: Re: Watermarking in Structured Streaming to drop late data

Hi all

I would highly recommend to all users-devs interested in the design suggestions / discussions for Structured Streaming Spark API watermarking
to take a look on the following links along with the design document. It would help to understand the notions of watermark , out of order data and possible use cases.

https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101
https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102

Kind Regards


2016-10-27 9:46 GMT+03:00 assaf.mendelson <[hidden email]</user/SendEmail.jtp?type=node&node=19592&i=0>>:
Hi,
Should comments come here or in the JIRA?
Any, I am a little confused on the need to expose this as an API to begin with.
Letâ€™s consider for a second the most basic behavior: We have some input stream and we want to aggregate a sum over a time window.
This means that the window we should be looking at would be the maximum time across our data and back by the window interval. Everything older can be dropped.
When new data arrives, the maximum time cannot move back so we generally drop everything tool old.
This basically means we save only the latest time window.
This simpler model would only break if we have a secondary aggregation which needs the results of multiple windows.
Is this the use case we are trying to solve?
If so, wouldnâ€™t just calculating the bigger time window across the entire aggregation solve this?
Am I missing something here?

From: Michael Armbrust [via Apache Spark Developers List] [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=19592&i=1>[hidden email]<http://user/SendEmail.jtp?type=node&node=19591&i=0>]
Sent: Thursday, October 27, 2016 3:04 AM
To: Mendelson, Assaf
Subject: Re: Watermarking in Structured Streaming to drop late data

And the JIRA: https://issues.apache.org/jira/browse/SPARK-18124

Hey all,

We are planning implement watermarking in Structured Streaming that would allow us handle late, out-of-order data better. Specially, when we are aggregating over windows on event-time, we currently can end up keeping unbounded amount data as state. We want to define watermarks on the event time in order mark and drop data that are ""too late"" and accordingly age out old aggregates that will not be updated any more.

To enable the user to specify details like lateness threshold, we are considering adding a new method to Dataset. We would like to get more feedback on this API. Here is the design doc

https://docs.google.com/document/d/1z-Pazs5v4rA31azvmYhu4I5xwqaNQl6ZLIS03xhkfCQ/

Please comment on the design and proposed APIs.

Thank you very much!

TD


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Watermarking-in-Structured-Streaming-to-drop-late-data-tp19589p19590.html
To start a new topic under Apache Spark Developers List, email [hidden email]<http://user/SendEmail.jtp?type=node&node=19591&i=1>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: RE: Watermarking in Structured Streaming to drop late data<http://apache-spark-developers-list.1001551.n3.nabble.com/Watermarking-in-Structured-Streaming-to-drop-late-data-tp19589p19591.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Watermarking-in-Structured-Streaming-to-drop-late-data-tp19589p19592.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Watermarking-in-Structured-Streaming-to-drop-late-data-tp19589p19600.html
om."
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 27 Oct 2016 03:50:25 -0700",Re: Watermarking in Structured Streaming to drop late data,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Assaf, thanks for the feedback!


stand the
 input
n
e entire
en
 email]
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
n-Structured-Streaming-to-drop-late-data-tp19589p19591.html>
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
n-Structured-Streaming-to-drop-late-data-tp19589p19600.html>
"
Deenar Toraskar <deenar.toraskar@gmail.com>,"Thu, 27 Oct 2016 14:48:47 +0100",Spark 2.0 on HDP,dev <dev@spark.apache.org>,"Hi

Has anyone tried running Spark 2.0 on HDP. I have managed to get around the
issues with the timeline service (by turning it off), but now am stuck when
the YARN cannot find org.apache.spark.deploy.yarn.ExecutorLauncher.

Error: Could not find or load main class
org.apache.spark.deploy.yarn.ExecutorLauncher

I have verified that both spark.driver.extraJavaOptions and
spark.yarn.am.extraJavaOptions have the hdp.version set correctly. Anything
else I am missing?

Regards
Deenar




"
Sean Owen <sowen@cloudera.com>,"Thu, 27 Oct 2016 14:03:18 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),"=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>, 
	Reynold Xin <rxin@databricks.com>","+1 from me. All the sigs and licenses and hashes check out. It builds and
passes tests with -Phadoop-2.7 -Pyarn -Phive -Phive-thriftserver on Ubuntu
16 + Java 8.

Here are the open issues for 2.0.2 BTW. No blockers, but some marked
Critical FYI. Just maki"
Koert Kuipers <koert@tresata.com>,"Thu, 27 Oct 2016 11:24:38 -0400",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,"+1 non binding

compiled and unit tested in-house libraries against 2.0.2-rc1 successfully

was able to build, deploy and launch on cdh 5.7 yarn cluster

on a side note... these artifacts on staging repo having version 2.0.2
instead of 2.0.2-rc1 makes it "
Yanbo Liang <ybliang8@gmail.com>,"Thu, 27 Oct 2016 08:24:29 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Reynold Xin <rxin@databricks.com>,1
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 27 Oct 2016 17:36:31 +0200",Re: Straw poll: dropping support for things like Scala 2.10,Reynold Xin <rxin@databricks.com>,"Just to comment on this, I'm generally against removing these types of things unless they create a substantial burden on project contributors. It doesn't sound like Python 2.6 and Java 7 do that yet -- Scala 2.10 might, but then of course we need to wait for 2.12 to be out and stable.

In general, this type of stuff only hurts users, and doesn't have a huge impact on Spark contributors' productivity (sure, it's a bit unpleasant, but that's life). If we break compatibility this way too quickly, we fragment the user community, and then either people have a crappy experience with Spark because their corporate IT doesn't yet have an environment that can run the latest version, or worse, they create more maintenance burden for us because they ask for more patches to be backported to old Spark versions (1.6.x, 2.0.x, etc). Python in particular is pretty fundamental to many Linux distros.

In the future, rather than just looking at when some software came out, it may be good to have some criteria for when to drop support for something. For example, if there are really nice libraries in Python 2.7 or Java 8 that we're missing out on, that may be a good reason. The maintenance burden for multiple Scala versions is definitely painful but I also think we should always support the latest two Scala releases.

Matei

https://issues.apache.org/jira/browse/SPARK-18138 <https://issues.apache.org/jira/browse/SPARK-18138>
like to add that to a list of things that will begin to be unsupported 6 months from now.
(Mar/Apr 2017).
deprecation of Java 7 / Scala 2.10 support.
how to build Spark
using Scala 2.10 or Java 7.

"
Amit Tank <amittankopensource@gmail.com>,"Thu, 27 Oct 2016 08:49:53 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Matei Zaharia <matei.zaharia@gmail.com>,"+1 for Matei's point.


"
Michael Armbrust <michael@databricks.com>,"Thu, 27 Oct 2016 09:05:05 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,1
Davies Liu <davies@databricks.com>,"Thu, 27 Oct 2016 09:56:32 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,"+1


---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Thu, 27 Oct 2016 09:58:27 -0700",Re: Straw poll: dropping support for things like Scala 2.10,Matei Zaharia <matei.zaharia@gmail.com>,"+1 for Matei's point.


---------------------------------------------------------------------


"
vaquar khan <vaquar.khan@gmail.com>,"Thu, 27 Oct 2016 12:07:10 -0500",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Davies Liu <davies@databricks.com>,"+1






-- 
Regards,
Vaquar Khan
+1 -224-436-0783

IT Architect / Lead Consultant
Greater Chicago
"
Koert Kuipers <koert@tresata.com>,"Thu, 27 Oct 2016 15:57:30 -0400",encoders for more complex types,"""dev@spark.apache.org"" <dev@spark.apache.org>","i have been pushing my luck a bit and started using ExpressionEncoder for
more complex types like sequences of case classes etc. (where the case
classes only had primitives and Strings).

it all seems to work but i think the wheels come off in certain cases in
the code generation. i guess this is not unexpected, after all what i am
doing is not yet supported.

is there a planned path forward to support more complex types with
encoders? it would be nice if we can at least support all types that
spark-sql supports in general for DataFrame.

best, koert
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 27 Oct 2016 22:02:18 +0200",Re: encoders for more complex types,Koert Kuipers <koert@tresata.com>,"What kind of difficulties are you experiencing?


"
Koert Kuipers <koert@tresata.com>,"Thu, 27 Oct 2016 16:05:34 -0400",Re: encoders for more complex types,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"well i was using Aggregators that returned sequences of structs, or structs
with sequences inside etc. and got compilation errors on the codegen.

i didnt bother trying to reproduce it so far, or post it, since what i did
was beyond the supposed usage anyhow.



r
"
Felix Cheung <felixcheung_m@hotmail.com>,"Thu, 27 Oct 2016 20:36:36 +0000",Re: Straw poll: dropping support for things like Scala 2.10,"Matei Zaharia <matei.zaharia@gmail.com>, Davies Liu
	<davies@databricks.com>","+1 on Matei's.


_____________________________
From: Davies Liu <davies@databricks.com<mailto:davies@databricks.com>>
Sent: Thursday, October 27, 2016 9:58 AM
Subject: Re: Straw poll: dropping support for things like Scala 2.10
To: Matei Zaharia <matei.za"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Fri, 28 Oct 2016 05:39:59 +0900",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,#NAME?
Michael Armbrust <michael@databricks.com>,"Thu, 27 Oct 2016 13:51:45 -0700",Re: encoders for more complex types,Koert Kuipers <koert@tresata.com>,"I would categorize these as bugs.  We should (but probably don't fully yet)
support arbitrary nesting as long as you use basic collections / case
classes / primitives.  Please do open JIRAs as you find problems.


d
:
ase
n
m
"
Koert Kuipers <koert@tresata.com>,"Thu, 27 Oct 2016 16:55:35 -0400",Re: encoders for more complex types,Michael Armbrust <michael@databricks.com>,"ok will do


e
<
case
 i am
"
Sean Owen <sowen@cloudera.com>,"Thu, 27 Oct 2016 21:13:25 +0000",Re: Straw poll: dropping support for things like Scala 2.10,"Matei Zaharia <matei.zaharia@gmail.com>, Reynold Xin <rxin@databricks.com>","The burden may be a little more apparent when dealing with the day to day
merging and fixing of breaks. The upside is maybe the more compelling
argument though. For example, lambda-fying all the Java code, supporting
java.time, and taking advantage of some newer Hadoop/YARN APIs is a
moderate win for users too, and there's also a cost to not doing that.

I must say I don't see a risk of fragmentation as nearly the problem it's
made out to be here. We are, after all, here discussing _beginning_ to
remove support _in 6 months_, for long since non-current versions of
things. An org's decision to not, say, use Java 8 is a decision to not use
the new version of lots of things. It's not clear this is a constituency
that is either large or one to reasonably serve indefinitely.

In the end, the Scala issue may be decisive. Supporting 2.10 - 2.12
simultaneously is a bridge too far, and if 2.12 requires Java 8, it's a
good reason to for Spark to require Java 8. And Steve suggests that means a
minimum of Hadoop 2.6 too. (I still profess ignorance of the Python part of
the issue.)

Put another way I am not sure what the criteria is, if not the above?

I support deprecating all of these things, at the least, in 2.1.0. Although
it's a separate question, I believe it's going to be necessary to remove
support in ~6 months in 2.2.0.



"
"""Dongjoon Hyun""<dongjoon@apache.org>","Thu, 27 Oct 2016 21:47:58 -0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),<dev@spark.apache.org>,"+1 non-binding.

Built and tested CentOS 6.6 / OpenJDK 1.8.0_111.

Cheers,
Dongjoon.

---------------------------------------------------------------------


"
Ofir Manor <ofir.manor@equalum.io>,"Fri, 28 Oct 2016 00:51:12 +0300",Re: Straw poll: dropping support for things like Scala 2.10,Sean Owen <sowen@cloudera.com>,"I totally agree with Sean, just a small correction:
Java 7 and Python 2.6 are already deprecated since Spark 2.0 (after a
lengthy discussion), so there is no need to discuss whether they should
become deprecated in 2.1
  http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations
The discussion is whether Scala 2.10 should also be marked as deprecated
(no one is objecting that), and more importantly, when to actually move
from deprecation to actually dropping support for any combination of JDK /
Scala / Hadoop / Python.

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Thu, 27 Oct 2016 21:52:52 -0000",Re: [VOTE] Release Apache Spark 1.6.3 (RC1),<dev@spark.apache.org>,"Hi, All.

Last time, RC1 passed the tests with only the timezone testcase failure. Now, it's backported, too.
I'm wondering if we have other issues to block releasing Apache Spark 1.6.3.

Bests,
Dongjoon.

---------------------------------------------------------------------


"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Fri, 28 Oct 2016 00:35:53 +0200",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

built and tested without regressions from 2.0.1.




"
Koert Kuipers <koert@tresata.com>,"Thu, 27 Oct 2016 18:41:34 -0400",Re: encoders for more complex types,Michael Armbrust <michael@databricks.com>,"https://issues.apache.org/jira/browse/SPARK-18147


se
:
 <
 case
t i am
"
Denny Lee <denny.g.lee@gmail.com>,"Fri, 28 Oct 2016 00:18:44 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),"Ricardo Almeida <ricardo.almeida@actnowib.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)




"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 28 Oct 2016 02:29:19 +0200",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Felix Cheung <felixcheung_m@hotmail.com>,"Fri, 28 Oct 2016 03:16:09 +0000",Re: SparkR issue with array types in gapply(),"shirisht <shirish.tatikonda@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","This is a R native data.frame behavior.

While arr is a character vector of length = 2,
[1] ""rows= 50"" ""cols= 2""
[1] 2


when it is set as R data.frame the character vector is splitted into 2 rows


  key strings
1 a rows= 50
2 a cols= 2


        key strings
""character"" ""character""
[1] ""a""
[1] ""rows= 50""
[1] ""cols= 2""


And each is separate in the character column. This causes a schema mismatch when it is expecting a string array, not just string when you set schema to have  structField('strings', 'array<string>')


_____________________________
From: shirisht <shirish.tatikonda@gmail.com<mailto:shirish.tatikonda@gmail.com>>
Sent: Tuesday, October 25, 2016 11:51 PM
Subject: SparkR issue with array types in gapply()
To: <dev@spark.apache.org<mailto:dev@spark.apache.org>>


Hello,

I am getting an exception from catalyst when array types are used in the
return schema of gapply() function.

Following is a (made-up) example:

------------------------------------------------------------
iris$flag = base::sample(1:2, nrow(iris), T, prob = c(0.5,0.5))
irisdf = createDataFrame(iris)

foo = function(key, x) {
nr = nrow(x)
nc = ncol(x)
arr = c( paste(""rows="", nr), paste(""cols="",nc) )
data.frame(key, strings = arr, stringsAsFactors = F)
}

outSchema = structType( structField('key', 'integer'),
structField('strings', 'array<string>') )
result = SparkR::gapply(irisdf, ""flag"", foo, outSchema)
d = SparkR::collect(result)
------------------------------------------------------------

This code throws up the following error:

java.lang.RuntimeException: java.lang.String is not a valid external type
for schema of array<string>
at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown
Source)
at
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
at
org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
at
org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

Any thoughts?

Thank you,
Shirish



--
3.nabble.com/SparkR-issue-with-array-types-in-gapply-tp19568.html
om<http://Nabble.com>.

---------------------------------------------------------------------
ibe@spark.apache.org>



"
"""Jagadeesan As"" <as2@us.ibm.com>","Fri, 28 Oct 2016 09:52:51 +0530",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,"+1 (non binding)

Ubuntu 14.04.2/openjdk  ""1.8.0_72""
(-Pyarn -Phadoop-2.7 -Psparkr -Pkinesis-asl -Phive-thriftserver)
 
Cheers,
Jagadeesan A S



From:   Reynold Xin <rxin@databricks.com>
To:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   27-10"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 28 Oct 2016 09:58:13 +0200",Re: Straw poll: dropping support for things like Scala 2.10,Ofir Manor <ofir.manor@equalum.io>,"Deprecating them is fine (and I know they're already deprecated), the question is just whether to remove them. For example, what exactly is the downside of having Python 2.6 or Java 7 right now? If it's high, then we can remove them, but I just haven't seen a ton of details. It also sounded like fairly recent versions of CDH, HDP, RHEL, etc still have old versions of these.

Just talking with users, I've seen many of people who say ""we have a Hadoop cluster from $VENDOR, but we just download Spark from Apache and run newer versions of that"". That's great for Spark IMO, and we need to stay compatible even with somewhat older Hadoop installs because they are time-consuming to update. Having the whole community on a small set of versions leads to a better experience for everyone and also to more of a ""network effect"": more people can battle-test new versions, answer questions about them online, write libraries that easily reach the majority of Spark users, etc.

Matei

lengthy discussion), so there is no need to discuss whether they should become deprecated in 2.1
http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations <http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations>
deprecated (no one is objecting that), and more importantly, when to actually move from deprecation to actually dropping support for any combination of JDK / Scala / Hadoop / Python.
ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>
day merging and fixing of breaks. The upside is maybe the more compelling argument though. For example, lambda-fying all the Java code, supporting java.time, and taking advantage of some newer Hadoop/YARN APIs is a moderate win for users too, and there's also a cost to not doing that.
it's made out to be here. We are, after all, here discussing _beginning_ to remove support _in 6 months_, for long since non-current versions of things. An org's decision to not, say, use Java 8 is a decision to not use the new version of lots of things. It's not clear this is a constituency that is either large or one to reasonably serve indefinitely.
simultaneously is a bridge too far, and if 2.12 requires Java 8, it's a good reason to for Spark to require Java 8. And Steve suggests that means a minimum of Hadoop 2.6 too. (I still profess ignorance of the Python part of the issue.)
Although it's a separate question, I believe it's going to be necessary to remove support in ~6 months in 2.2.0.
things unless they create a substantial burden on project contributors. It doesn't sound like Python 2.6 and Java 7 do that yet -- Scala 2.10 might, but then of course we need to wait for 2.12 to be out and stable.
huge impact on Spark contributors' productivity (sure, it's a bit unpleasant, but that's life). If we break compatibility this way too quickly, we fragment the user community, and then either people have a crappy experience with Spark because their corporate IT doesn't yet have an environment that can run the latest version, or worse, they create more maintenance burden for us because they ask for more patches to be backported to old Spark versions (1.6.x, 2.0.x, etc). Python in particular is pretty fundamental to many Linux distros.
out, it may be good to have some criteria for when to drop support for something. For example, if there are really nice libraries in Python 2.7 or Java 8 that we're missing out on, that may be a good reason. The maintenance burden for multiple Scala versions is definitely painful but I also think we should always support the latest two Scala releases.
https://issues.apache.org/jira/browse/SPARK-18138 <https://issues.apache.org/jira/browse/SPARK-18138>
like to add that to a list of things that will begin to be unsupported 6 months from now.
(Mar/Apr 2017).
deprecation of Java 7 / Scala 2.10 support.
how to build Spark
using Scala 2.10 or Java 7.

"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 28 Oct 2016 10:04:05 +0200",Re: Straw poll: dropping support for things like Scala 2.10,Ofir Manor <ofir.manor@equalum.io>,"BTW maybe one key point that isn't obvious is that with YARN and Mesos, the version of Spark used can be solely up to the developer who writes an app, not to the cluster administrator. So even in very conservative orgs, developers can download a new version of Spark, run it, and the other hand, if they were stuck with, say, Spark 1.3, they'd have a much worse experience and perhaps get a worse impression of the project.

Matei

question is just whether to remove them. For example, what exactly is the downside of having Python 2.6 or Java 7 right now? If it's high, then we can remove them, but I just haven't seen a ton of details. It also sounded like fairly recent versions of CDH, HDP, RHEL, etc still have old versions of these.
Hadoop cluster from $VENDOR, but we just download Spark from Apache and run newer versions of that"". That's great for Spark IMO, and we need to stay compatible even with somewhat older Hadoop installs because they are time-consuming to update. Having the whole community on a small set of versions leads to a better experience for everyone and also to more of a ""network effect"": more people can battle-test new versions, answer questions about them online, write libraries that easily reach the majority of Spark users, etc.
lengthy discussion), so there is no need to discuss whether they should become deprecated in 2.1
http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations <http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations>
deprecated (no one is objecting that), and more importantly, when to actually move from deprecation to actually dropping support for any combination of JDK / Scala / Hadoop / Python.
ofir.manor@equalum.io <mailto:ofir.manor@equalum.io>
day merging and fixing of breaks. The upside is maybe the more compelling argument though. For example, lambda-fying all the Java code, supporting java.time, and taking advantage of some newer Hadoop/YARN APIs is a moderate win for users too, and there's also a cost to not doing that.
it's made out to be here. We are, after all, here discussing _beginning_ to remove support _in 6 months_, for long since non-current versions of things. An org's decision to not, say, use Java 8 is a decision to not use the new version of lots of things. It's not clear this is a constituency that is either large or one to reasonably serve indefinitely.
simultaneously is a bridge too far, and if 2.12 requires Java 8, it's a good reason to for Spark to require Java 8. And Steve suggests that means a minimum of Hadoop 2.6 too. (I still profess ignorance of the Python part of the issue.)
Although it's a separate question, I believe it's going to be necessary to remove support in ~6 months in 2.2.0.
of things unless they create a substantial burden on project contributors. It doesn't sound like Python 2.6 and Java 7 do that yet -- Scala 2.10 might, but then of course we need to wait for 2.12 to be out and stable.
huge impact on Spark contributors' productivity (sure, it's a bit unpleasant, but that's life). If we break compatibility this way too quickly, we fragment the user community, and then either people have a crappy experience with Spark because their corporate IT doesn't yet have an environment that can run the latest version, or worse, they create more maintenance burden for us because they ask for more patches to be backported to old Spark versions (1.6.x, 2.0.x, etc). Python in particular is pretty fundamental to many Linux distros.
out, it may be good to have some criteria for when to drop support for something. For example, if there are really nice libraries in Python 2.7 or Java 8 that we're missing out on, that may be a good reason. The maintenance burden for multiple Scala versions is definitely painful but I also think we should always support the latest two Scala releases.
https://issues.apache.org/jira/browse/SPARK-18138 <https://issues.apache.org/jira/browse/SPARK-18138>
like to add that to a list of things that will begin to be unsupported 6 months from now.
(Mar/Apr 2017).
deprecation of Java 7 / Scala 2.10 support.
how to build Spark
started using Scala 2.10 or Java 7.

"
Sean Owen <sowen@cloudera.com>,"Fri, 28 Oct 2016 09:43:10 +0000",Re: Straw poll: dropping support for things like Scala 2.10,Matei Zaharia <matei.zaharia@gmail.com>,"If the subtext is vendors, then I'd have a look at what recent distros look
like. I'll write about CDH as a representative example, but I think other
distros are naturally similar.

CDH has been on Java 8, Hadoop 2.6, Python 2.7 for almost two years (CDH
5.3 / Dec 2014). Granted, this depends on installing on an OS with that
Java / Python version. But Java 8 / Python 2.7 is available for all of the
supported OSes. The population that isn't on CDH 4, because that supported
was dropped a long time ago in Spark, and who is on a version released
2-2.5 years ago, and won't update, is a couple percent of the installed
base. They do not in general want anything to change at all.

I assure everyone that vendors too are aligned in wanting to cater to the
crowd that wants the most recent version of everything. For example, CDH
offers both Spark 2.0.1 and 1.6 at the same time.

I wouldn't dismiss support for these supporting components as a relevant
proxy for whether they are worth supporting in Spark. Java 7 is long since
EOL (no, I don't count paying Oracle for support). No vendor is supporting
Hadoop < 2.6. Scala 2.10 was EOL at the end of 2014. Is there a criteria
here that reaches a different conclusion about these things just for Spark?
This was roughly the same conversation that happened 6 months ago.

I imagine we're going to find that in about 6 months it'll make more sense
all around to remove these. If we can just give a heads up with deprecation
and then kick the can down the road a bit more, that sounds like enough for
now.


"
Chris Fregly <chris@fregly.com>,"Fri, 28 Oct 2016 11:47:03 +0100",Re: Straw poll: dropping support for things like Scala 2.10,Sean Owen <sowen@cloudera.com>,"i seem to remember a large spark user (tencent, i believe) chiming in late during these discussions 6-12 months ago and squashing any sort of deprecation given the massive effort that would be required to upgrade their environment.

i just want to make sure these convos take into consideration large spark users - and reflect the real world versus ideal world.

otherwise, this is all for naught like last time.

k like. I'll write about CDH as a representative example, but I think other distros are naturally similar.
.3 / Dec 2014). Granted, this depends on installing on an OS with that Java / Python version. But Java 8 / Python 2.7 is available for all of the supported OSes. The population that isn't on CDH 4, because that supported was dropped a long time ago in Spark, and who is on a version released 2-2.5 years ago, and won't update, is a couple percent of the installed base. They do not in general want anything to change at all.
rowd that wants the most recent version of everything. For example, CDH offers both Spark 2.0.1 and 1.6 at the same time.
roxy for whether they are worth supporting in Spark. Java 7 is long since EOL (no, I don't count paying Oracle for support). No vendor is supporting Hadoop < 2.6. Scala 2.10 was EOL at the end of 2014. Is there a criteria here that reaches a different conclusion about these things just for Spark? This was roughly the same conversation that happened 6 months ago.
 all around to remove these. If we can just give a heads up with deprecation and then kick the can down the road a bit more, that sounds like enough for now.
rote:
stion is just whether to remove them. For example, what exactly is the downside of having Python 2.6 or Java 7 right now? If it's high, then we can remove them, but I just haven't seen a ton of details. It also sounded like fairly recent versions of CDH, HDP, RHEL, etc still have old versions of these.
op cluster from $VENDOR, but we just download Spark from Apache and run newer versions of that"". That's great for Spark IMO, and we need to stay compatible even with somewhat older Hadoop installs because they are time-consuming to update. Having the whole community on a small set of versions leads to a better experience for everyone and also to more of a ""network effect"": more people can battle-test new versions, answer questions about them online, write libraries that easily reach the majority of Spark users, etc.
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 28 Oct 2016 13:44:33 +0000",Re: Straw poll: dropping support for things like Scala 2.10,Chris Fregly <chris@fregly.com>,"Twitter just led the release of Hadoop 2.6.5 precisely because they wanted to keep a Java 6 cluster up: the bigger your cluster, the less of a rush to upgrade.

HDP? I believe we install & prefer (openjdk) Java 8, but the Hadoop branch-2 line is intended to build/run on Java 7 too. There's always a conflict between us developers ""shiny new features"" and ops ""keep cluster alive"". That's actually where Scala has an edge: no need to upgrade the cluster-wide JVM just for an update, or play games configuring your deployed application to use a different JVM from the Hadoop services (which you can do, after all: it's just path setup). Thinking about it, knowing what can be done there â€”including documenting it in the spark docs, could be a good migration strategy.

me? I look forward to when we can use Java 9 to isolate transitive dependencies; the bane of everyone's life. Someone needs to start on preparing everything for that to work though.

On 28 Oct 2016, at 11:47, Chris Fregly <chris@fregly.com<mailto:chris@fregly.com>> wrote:

i seem to remember a large spark user (tencent, i believe) chiming in late during these discussions 6-12 months ago and squashing any sort of deprecation given the massive effort that would be required to upgrade their environment.

i just want to make sure these convos take into consideration large spark users - and reflect the real world versus ideal world.

otherwise, this is all for naught like last time.

On Oct 28, 2016, at 10:43 AM, Sean Owen <sowen@cloudera.com<mailto:sowen@cloudera.com>> wrote:

If the subtext is vendors, then I'd have a look at what recent distros look like. I'll write about CDH as a representative example, but I think other distros are naturally similar.

CDH has been on Java 8, Hadoop 2.6, Python 2.7 for almost two years (CDH 5.3 / Dec 2014). Granted, this depends on installing on an OS with that Java / Python version. But Java 8 / Python 2.7 is available for all of the supported OSes. The population that isn't on CDH 4, because that supported was dropped a long time ago in Spark, and who is on a version released 2-2.5 years ago, and won't update, is a couple percent of the installed base. They do not in general want anything to change at all.

I assure everyone that vendors too are aligned in wanting to cater to the crowd that wants the most recent version of everything. For example, CDH offers both Spark 2.0.1 and 1.6 at the same time.

I wouldn't dismiss support for these supporting components as a relevant proxy for whether they are worth supporting in Spark. Java 7 is long since EOL (no, I don't count paying Oracle for support). No vendor is supporting Hadoop < 2.6. Scala 2.10 was EOL at the end of 2014. Is there a criteria here that reaches a different conclusion about these things just for Spark? This was roughly the same conversation that happened 6 months ago.

I imagine we're going to find that in about 6 months it'll make more sense all around to remove these. If we can just give a heads up with deprecation and then kick the can down the road a bit more, that sounds like enough for now.

On Fri, Oct 28, 2016 at 8:58 AM Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.com>> wrote:
Deprecating them is fine (and I know they're already deprecated), the question is just whether to remove them. For example, what exactly is the downside of having Python 2.6 or Java 7 right now? If it's high, then we can remove them, but I just haven't seen a ton of details. It also sounded like fairly recent versions of CDH, HDP, RHEL, etc still have old versions of these.

Just talking with users, I've seen many of people who say ""we have a Hadoop cluster from $VENDOR, but we just download Spark from Apache and run newer versions of that"". That's great for Spark IMO, and we need to stay compatible even with somewhat older Hadoop installs because they are time-consuming to update. Having the whole community on a small set of versions leads to a better experience for everyone and also to more of a ""network effect"": more people can battle-test new versions, answer questions about them online, write libraries that easily reach the majority of Spark users, etc.

"
Sushrut Ikhar <sushrutikhar94@gmail.com>,"Fri, 28 Oct 2016 19:57:08 +0530",MemoryStore reporting wrong free memory in spark 1.6.2,"""user@spark.apache.org"" <dev@spark.apache.org>","Hi,
I am seeing wrong computation of storage memory available which is leading
to executor failures. I have allocated 8g memory with params:
spark.memory.fraction=0.7
spark.memory.storageFraction=0.4
As expected, I was able to see 5.2 GB storage memory in UI.
However, as per memory store logs I am seeing that free memory is
increasing as the rdds are getting cached; which should have been ideally
decreasing from 5.2 to 0.

Eventually, the executor runs OOM whereas the Memory store is reporting it
has 5G available.


Attached executor logs -

2016-10-28 12:36:51,996 INFO  [Executor task launch worker-10]
executor.Executor (Logging.scala:logInfo(58)) - Running task 324.0 in stage
1.0 (TID 760)
2016-10-28 12:36:52,019 INFO  [Executor task launch worker-10]
spark.CacheManager (Logging.scala:logInfo(58)) - Partition rdd_10_324 not
found, computing it
2016-10-28 12:36:52,031 INFO  [Executor task launch worker-8]
storage.MemoryStore (Logging.scala:logInfo(58)) - *Block rdd_10_145 stored
as values in memory (estimated size 65.0 MB, free 260.7 MB)*
2016-10-28 12:36:52,062 INFO  [Executor task launch worker-10]
storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(58)) - Getting
426 non-empty blocks out of 426 blocks
2016-10-28 12:36:52,109 INFO  [Executor task launch worker-10]
storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(58)) - Started
37 remote fetches in 47 ms
2016-10-28 12:36:52,130 INFO  [Executor task launch worker-8]
executor.Executor (Logging.scala:logInfo(58)) - Finished task 145.0 in
stage 1.0 (TID 581). 268313 bytes result sent to driver
2016-10-28 12:36:52,150 INFO  [dispatcher-event-loop-7]
executor.CoarseGrainedExecutorBackend (Logging.scala:logInfo(58)) - Got
assigned task 761
2016-10-28 12:36:52,150 INFO  [Executor task launch worker-8]
executor.Executor (Logging.scala:logInfo(58)) - Running task 325.0 in stage
1.0 (TID 761)
2016-10-28 12:36:52,164 INFO  [Executor task launch worker-8]
spark.CacheManager (Logging.scala:logInfo(58)) - Partition rdd_10_325 not
found, computing it
2016-10-28 12:36:52,247 INFO  [Executor task launch worker-8]
storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(58)) - Getting
426 non-empty blocks out of 426 blocks
2016-10-28 12:36:52,264 INFO  [Executor task launch worker-8]
storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(58)) - Started
37 remote fetches in 18 ms
2016-10-28 12:36:52,591 INFO  [Executor task launch worker-6]
storage.MemoryStore (Logging.scala:logInfo(58)) -* Block rdd_10_45 stored
as values in memory (estimated size 65.0 MB, free 325.7 MB)*
2016-10-28 12:36:52,646 INFO  [Executor task launch worker-6]
executor.Executor (Logging.scala:logInfo(58)) - Finished task 45.0 in stage
1.0 (TID 481). 266368 bytes result sent to driver

Eventual Failures logs-

2016-10-28 12:53:06,718 WARN  [Executor task launch worker-13]
storage.MemoryStore (Logging.scala:logWarning(70)) - Not enough space to
cache rdd_10_656 in memory! (computed 45.2 MB so far)
2016-10-28 12:53:06,718 INFO  [Executor task launch worker-13]
storage.MemoryStore (Logging.scala:logInfo(58)) - Memory use = 5.0 GB
(blocks) + 211.4 MB (scratch space shared across 103 tasks(s)) = 5.2 GB.
Storage limit = 5.2 GB.
2016-10-28 12:53:06,718 INFO  [Executor task launch worker-13]
storage.BlockManager (Logging.scala:logInfo(58)) - Found block rdd_10_656
locally
2016-10-28 12:53:06,719 INFO  [Executor task launch worker-12]
storage.MemoryStore (Logging.scala:logInfo(58)) - 1 blocks selected for
dropping
2016-10-28 12:53:06,720 INFO  [Executor task launch worker-12]
storage.BlockManager (Logging.scala:logInfo(58)) - Dropping block
rdd_10_719 from memory
2016-10-28 12:53:06,720 INFO  [Executor task launch worker-12]
storage.BlockManager (Logging.scala:logInfo(58)) - Writing block rdd_10_719
to disk
2016-10-28 12:53:06,736 ERROR [Executor task launch worker-15]
executor.Executor (Logging.scala:logError(95)) - Exception in task 657.0 in
stage 4.0 (TID 4565)
java.lang.OutOfMemoryError: Unable to acquire 262144 bytes of memory, got
85027
at
org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:91)


Is this a bug or I am setting something wrong?

Regards,

Sushrut Ikhar
[image: https://]about.me/sushrutikhar
<https://about.me/sushrutikhar?promo=email_sig>
"
Sushrut Ikhar <sushrutikhar94@gmail.com>,"Fri, 28 Oct 2016 21:25:05 +0530",Re: MemoryStore reporting wrong free memory in spark 1.6.2,"""user@spark.apache.org"" <dev@spark.apache.org>","found the reporting bug in 1.6.2-
Utils.bytesToString(maxMemory - blocksMemoryUsed)) should've been used

https://github.com/apache/spark/blob/v2.0.1/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala#L154

https://github.com/apache/spark/blob/v1.6.2/core/src/main/scala/org/apache/spark/storage/MemoryStore.scala#L396

Regards,

Sushrut Ikhar
[image: https://]about.me/sushrutikhar
<https://about.me/sushrutikhar?promo=email_sig>



"
Koert Kuipers <koert@tresata.com>,"Fri, 28 Oct 2016 12:02:15 -0400",Re: Straw poll: dropping support for things like Scala 2.10,Steve Loughran <stevel@hortonworks.com>,"thats correct in my experience: we have found a scala update to be
straightforward and basically somewhat invisible to ops, but a java upgrade
a pain because it is managed and ""certified"" by ops.


d
to
ch
s, could
e
e
d
e
g
k?
e
on
or
e
ed
ns
run
ity
"
Ryan Blue <rblue@netflix.com.INVALID>,"Fri, 28 Oct 2016 10:06:37 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)

Checksums and build are fine. The tarball matches the release tag except
that .gitignore is missing. It would be nice if the tarball were created
using git archive so that the commit ref is present, but otherwise
everything looks fine.
â"
Jeremy Smith <jeremy.smith@acorns.com>,"Fri, 28 Oct 2016 12:18:36 -0700",Spark has a compile dependency on scalatest,dev@spark.apache.org,"Hey everybody,

Just a heads up that currently Spark 2.0.1 has a compile dependency on
Scalatest 2.2.6. It comes from spark-core's dependency on spark-launcher,
which has a transitive dependency on spark-tags, which has a compile
dependency on Scalatest.

This makes it impossible to use any other version of Scalatest for testing
your app if you declare a dependency on any Spark 2.0.1 module; you'll get
a bunch of runtime errors during testing (unless you figure out the reason
like I did and explicitly exclude Scalatest from the spark dependency).

I think that dependency should probably be moved to a test dependency
instead.

Thanks,
Jeremy
"
Sean Owen <sowen@cloudera.com>,"Fri, 28 Oct 2016 19:27:18 +0000",Re: Spark has a compile dependency on scalatest,"Jeremy Smith <jeremy.smith@acorns.com>, dev@spark.apache.org","It's required because the tags module uses it to define annotations for
tests. I don't see it in compile scope for anything but the tags module,
which is then in test scope for other modules. What are you seeing that
makes you say it's in compile scope?


"
Jeremy Smith <jeremy.smith@acorns.com>,"Fri, 28 Oct 2016 12:51:48 -0700",Re: Spark has a compile dependency on scalatest,Sean Owen <sowen@cloudera.com>,"spark-core depends on spark-launcher (compile)
spark-launcher depends on spark-tags (compile)
spark-tags depends on scalatest (compile)

To be honest I'm not all that familiar with the project structure - should
I just exclude spark-launcher if I'm not using it?


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Fri, 28 Oct 2016 12:52:54 -0700",Re: Spark has a compile dependency on scalatest,Sean Owen <sowen@cloudera.com>,"spark-tags is in the compile scope of spark-core...


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Fri, 28 Oct 2016 12:54:11 -0700",Re: Spark has a compile dependency on scalatest,Jeremy Smith <jeremy.smith@acorns.com>,"You can just exclude scalatest from Spark.


"
Weiqing Yang <yangweiqing001@gmail.com>,"Fri, 28 Oct 2016 12:58:09 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),Ryan Blue <rblue@netflix.com.invalid>,"+1 (non binding)



Environment: CentOS Linux release 7.0.1406 / openjdk version ""1.8.0_111""/ R
version 3.3.1


./build/mvn -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver
-Dpyspark -Dsparkr -DskipTests clean package

./build/mvn -Pyarn -Phad"
Sean Owen <sowen@cloudera.com>,"Fri, 28 Oct 2016 20:04:27 +0000",Re: Spark has a compile dependency on scalatest,Jeremy Smith <jeremy.smith@acorns.com>,"Yes, but scalatest doesn't end up in compile scope, says Maven?

...

[INFO] +- org.apache.spark:spark-tags_2.11:jar:2.1.0-SNAPSHOT:compile

[INFO] |  +- (org.scalatest:scalatest_2.11:jar:2.2.6:test - scope managed
from compile; omitted for duplicate)

[INFO] |  \- (org.spark-project.spark:unused:jar:1.0.0:compile - omitted
for duplicate)

[INFO] +- org.apache.commons:commons-crypto:jar:1.0.0:compile

[INFO] +- org.spark-project.spark:unused:jar:1.0.0:compile

[INFO] +- org.scalatest:scalatest_2.11:jar:2.2.6:test

...


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 28 Oct 2016 13:09:52 -0700",Re: Spark has a compile dependency on scalatest,Sean Owen <sowen@cloudera.com>,"The root pom declares scalatest explicitly with test scope. It's added
by default to all sub-modules, so every one should get it in test
scope unless the module explicitly overrides that, like the tags
module does.

If you look at the ""blessed"" dependency list in dev/deps, there's no scalatest.

That being said, there are two things:
- sbt seems to get confused with that; if I look at the assembly jars
dir created by sbt, it includes scalatest.
- there's always a chance that the published pom did something wrong
and promoted things when it shouldn't because of a bug. But that
doesn't seem to be the case:

http://repo1.maven.org/maven2/org/apache/spark/spark-core_2.11/2.0.1/spark-core_2.11-2.0.1.pom

Has:

<dependency>
<groupId>org.scalatest</groupId>
<artifactId>scalatest_2.11</artifactId>
<version>2.2.6</version>
<scope>test</scope>
</dependency>





-- 
Marcelo

---------------------------------------------------------------------


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Fri, 28 Oct 2016 13:22:54 -0700",Re: Spark has a compile dependency on scalatest,Sean Owen <sowen@cloudera.com>,"This is my test pom:

<project>
<modelVersion>4.0.0</modelVersion>
  <artifactId>foo</artifactId>
<groupId>bar</groupId>
<version>1.0</version>
<dependencies>

<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.10</artifactId>
    <version>2.0.1</version>
</dependency>
 </dependencies>

</project>

scalatest is in the compile scope:

[INFO] bar:foo:jar:1.0
[INFO] \- org.apache.spark:spark-core_2.10:jar:2.0.1:compile
[INFO]    +- org.apache.avro:avro-mapred:jar:hadoop2:1.7.7:compile
[INFO]    |  +- org.apache.avro:avro-ipc:jar:1.7.7:compile
[INFO]    |  |  \- org.apache.avro:avro:jar:1.7.7:compile
[INFO]    |  +- org.apache.avro:avro-ipc:jar:tests:1.7.7:compile
[INFO]    |  +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile
[INFO]    |  \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile
[INFO]    +- com.twitter:chill_2.10:jar:0.8.0:compile
[INFO]    |  \- com.esotericsoftware:kryo-shaded:jar:3.0.3:compile
[INFO]    |     +- com.esotericsoftware:minlog:jar:1.3.0:compile
[INFO]    |     \- org.objenesis:objenesis:jar:2.1:compile
[INFO]    +- com.twitter:chill-java:jar:0.8.0:compile
[INFO]    +- org.apache.xbean:xbean-asm5-shaded:jar:4.4:compile
[INFO]    +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-common:jar:2.2.0:compile
[INFO]    |  |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO]    |  |  +- org.apache.commons:commons-math:jar:2.1:compile
[INFO]    |  |  +- xmlenc:xmlenc:jar:0.52:compile
[INFO]    |  |  +- commons-io:commons-io:jar:2.1:compile
[INFO]    |  |  +- commons-lang:commons-lang:jar:2.5:compile
[INFO]    |  |  +-
commons-configuration:commons-configuration:jar:1.6:compile
[INFO]    |  |  |  +-
commons-collections:commons-collections:jar:3.2.1:compile
[INFO]    |  |  |  +- commons-digester:commons-digester:jar:1.8:compile
[INFO]    |  |  |  |  \-
commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO]    |  |  |  \-
commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO]    |  |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-auth:jar:2.2.0:compile
[INFO]    |  |  \- org.apache.commons:commons-compress:jar:1.4.1:compile
[INFO]    |  |     \- org.tukaani:xz:jar:1.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-hdfs:jar:2.2.0:compile
[INFO]    |  |  \- org.mortbay.jetty:jetty-util:jar:6.1.26:compile
[INFO]    |  +-
org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.2.0:compile
[INFO]    |  |  +-
org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.2.0:compile
[INFO]    |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.2.0:compile
[INFO]    |  |  |  |  \- com.google.inject:guice:jar:3.0:compile
[INFO]    |  |  |  |     +- javax.inject:javax.inject:jar:1:compile
[INFO]    |  |  |  |     \- aopalliance:aopalliance:jar:1.0:compile
[INFO]    |  |  |  \-
org.apache.hadoop:hadoop-yarn-server-common:jar:2.2.0:compile
[INFO]    |  |  \-
org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.2.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.2.0:compile
[INFO]    |  +-
org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.2.0:compile
[INFO]    |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.2.0:compile
[INFO]    |  +-
org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.2.0:compile
[INFO]    |  \- org.apache.hadoop:hadoop-annotations:jar:2.2.0:compile
[INFO]    +- org.apache.spark:spark-launcher_2.10:jar:2.0.1:compile
[INFO]    +- org.apache.spark:spark-network-common_2.10:jar:2.0.1:compile
[INFO]    |  +- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:compile
[INFO]    |  \-
com.fasterxml.jackson.core:jackson-annotations:jar:2.6.5:compile
[INFO]    +- org.apache.spark:spark-network-shuffle_2.10:jar:2.0.1:compile
[INFO]    +- org.apache.spark:spark-unsafe_2.10:jar:2.0.1:compile
[INFO]    +- net.java.dev.jets3t:jets3t:jar:0.7.1:compile
[INFO]    |  +- commons-codec:commons-codec:jar:1.3:compile
[INFO]    |  \- commons-httpclient:commons-httpclient:jar:3.1:compile
[INFO]    +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO]    |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO]    |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO]    |  +- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO]    |  \- com.google.guava:guava:jar:14.0.1:compile
[INFO]    +- javax.servlet:javax.servlet-api:jar:3.1.0:compile
[INFO]    +- org.apache.commons:commons-lang3:jar:3.3.2:compile
[INFO]    +- org.apache.commons:commons-math3:jar:3.4.1:compile
[INFO]    +- com.google.code.findbugs:jsr305:jar:1.3.9:compile
[INFO]    +- org.slf4j:slf4j-api:jar:1.7.16:compile
[INFO]    +- org.slf4j:jul-to-slf4j:jar:1.7.16:compile
[INFO]    +- org.slf4j:jcl-over-slf4j:jar:1.7.16:compile
[INFO]    +- log4j:log4j:jar:1.2.17:compile
[INFO]    +- org.slf4j:slf4j-log4j12:jar:1.7.16:compile
[INFO]    +- com.ning:compress-lzf:jar:1.0.3:compile
[INFO]    +- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile
[INFO]    +- net.jpountz.lz4:lz4:jar:1.3.0:compile
[INFO]    +- org.roaringbitmap:RoaringBitmap:jar:0.5.11:compile
[INFO]    +- commons-net:commons-net:jar:2.2:compile
[INFO]    +- org.scala-lang:scala-library:jar:2.10.6:compile
[INFO]    +- org.json4s:json4s-jackson_2.10:jar:3.2.11:compile
[INFO]    |  \- org.json4s:json4s-core_2.10:jar:3.2.11:compile
[INFO]    |     +- org.json4s:json4s-ast_2.10:jar:3.2.11:compile
[INFO]    |     +- com.thoughtworks.paranamer:paranamer:jar:2.6:compile
[INFO]    |     \- org.scala-lang:scalap:jar:2.10.0:compile
[INFO]    |        \- org.scala-lang:scala-compiler:jar:2.10.0:compile
[INFO]    +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile
[INFO]    |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile
[INFO]    |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile
[INFO]    |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile
[INFO]    |  |  \-
org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile
[INFO]    |  +-
org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile
[INFO]    |  \- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile
[INFO]    |     \- org.javassist:javassist:jar:3.18.1-GA:compile
[INFO]    +- org.glassfish.jersey.core:jersey-common:jar:2.22.2:compile
[INFO]    |  +- javax.annotation:javax.annotation-api:jar:1.2:compile
[INFO]    |  +-
org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.22.2:compile
[INFO]    |  \- org.glassfish.hk2:osgi-resource-locator:jar:1.0.1:compile
[INFO]    +- org.glassfish.jersey.core:jersey-server:jar:2.22.2:compile
[INFO]    |  +-
org.glassfish.jersey.media:jersey-media-jaxb:jar:2.22.2:compile
[INFO]    |  \- javax.validation:validation-api:jar:1.1.0.Final:compile
[INFO]    +-
org.glassfish.jersey.containers:jersey-container-servlet:jar:2.22.2:compile
[INFO]    +-
org.glassfish.jersey.containers:jersey-container-servlet-core:jar:2.22.2:compile
[INFO]    +- org.apache.mesos:mesos:jar:shaded-protobuf:0.21.1:compile
[INFO]    +- io.netty:netty-all:jar:4.0.29.Final:compile
[INFO]    +- io.netty:netty:jar:3.8.0.Final:compile
[INFO]    +- com.clearspring.analytics:stream:jar:2.7.0:compile
[INFO]    +- io.dropwizard.metrics:metrics-core:jar:3.1.2:compile
[INFO]    +- io.dropwizard.metrics:metrics-jvm:jar:3.1.2:compile
[INFO]    +- io.dropwizard.metrics:metrics-json:jar:3.1.2:compile
[INFO]    +- io.dropwizard.metrics:metrics-graphite:jar:3.1.2:compile
[INFO]    +- com.fasterxml.jackson.core:jackson-databind:jar:2.6.5:compile
[INFO]    |  \- com.fasterxml.jackson.core:jackson-core:jar:2.6.5:compile
[INFO]    +-
com.fasterxml.jackson.module:jackson-module-scala_2.10:jar:2.6.5:compile
[INFO]    |  +- org.scala-lang:scala-reflect:jar:2.10.6:compile
[INFO]    |  \-
com.fasterxml.jackson.module:jackson-module-paranamer:jar:2.6.5:compile
[INFO]    +- org.apache.ivy:ivy:jar:2.4.0:compile
[INFO]    +- oro:oro:jar:2.0.8:compile
[INFO]    +- net.razorvine:pyrolite:jar:4.9:compile
[INFO]    +- net.sf.py4j:py4j:jar:0.10.3:compile
[INFO]    +- org.apache.spark:spark-tags_2.10:jar:2.0.1:compile
[INFO]    |  \- org.scalatest:scalatest_2.10:jar:2.2.6:compile
[INFO]    \- org.spark-project.spark:unused:jar:1.0.0:compile


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 28 Oct 2016 13:30:11 -0700",Re: Spark has a compile dependency on scalatest,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Hmm. Yes, that makes sense. Spark's root pom does not affect your
application's pom, in which case it will pick compile over test if
there are conflicting dependencies.

Perhaps spark-tags should override it to provided instead of compile...




-- 
Marcelo

---------------------------------------------------------------------


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Fri, 28 Oct 2016 14:58:26 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","-1.

The history server is broken because of some refactoring work in Structured
Streaming: https://issues.apache.org/jira/browse/SPARK-18143


n
 if
l
0.2.
"
Sean Owen <sowen@cloudera.com>,"Sat, 29 Oct 2016 09:50:25 +0000",Re: Spark has a compile dependency on scalatest,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Declare your scalatest dependency as test scope (which is correct anyway).
That would override it I think as desired?


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 29 Oct 2016 12:55:05 +0000",Re: Spark has a compile dependency on scalatest,Sean Owen <sowen@cloudera.com>,"

Declare your scalatest dependency as test scope (which is correct anyway). That would override it I think as desired?


not sure about that, but then mvn dependencies are one of those graph-theory problems. It may just add it to the test CP alongside the compile one, so: no change. Someone would need to do an experiment there.

If the scalatags dependency is marked as provided, then maven won't put it on the transitive CP. And I assume, Ivy will work similarly


This is my test pom:

<project>
<modelVersion>4.0.0</modelVersion>
  <artifactId>foo</artifactId>
<groupId>bar</groupId>
<version>1.0</version>
<dependencies>

<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.10</artifactId>
    <version>2.0.1</version>
</dependency>
 </dependencies>

</project>

scalatest is in the compile scope:

[INFO] bar:foo:jar:1.0
[INFO] \- org.apache.spark:spark-core_2.10:jar:2.0.1:compile
[INFO]    +- org.apache.avro:avro-mapred:jar:hadoop2:1.7.7:compile
[INFO]    |  +- org.apache.avro:avro-ipc:jar:1.7.7:compile
[INFO]    |  |  \- org.apache.avro:avro:jar:1.7.7:compile
[INFO]    |  +- org.apache.avro:avro-ipc:jar:tests:1.7.7:compile
[INFO]    |  +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile
[INFO]    |  \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile
[INFO]    +- com.twitter:chill_2.10:jar:0.8.0:compile
[INFO]    |  \- com.esotericsoftware:kryo-shaded:jar:3.0.3:compile
[INFO]    |     +- com.esotericsoftware:minlog:jar:1.3.0:compile
[INFO]    |     \- org.objenesis:objenesis:jar:2.1:compile
[INFO]    +- com.twitter:chill-java:jar:0.8.0:compile
[INFO]    +- org.apache.xbean:xbean-asm5-shaded:jar:4.4:compile
[INFO]    +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-common:jar:2.2.0:compile
[INFO]    |  |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO]    |  |  +- org.apache.commons:commons-math:jar:2.1:compile
[INFO]    |  |  +- xmlenc:xmlenc:jar:0.52:compile
[INFO]    |  |  +- commons-io:commons-io:jar:2.1:compile
[INFO]    |  |  +- commons-lang:commons-lang:jar:2.5:compile
[INFO]    |  |  +- commons-configuration:commons-configuration:jar:1.6:compile
[INFO]    |  |  |  +- commons-collections:commons-collections:jar:3.2.1:compile
[INFO]    |  |  |  +- commons-digester:commons-digester:jar:1.8:compile
[INFO]    |  |  |  |  \- commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO]    |  |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO]    |  |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-auth:jar:2.2.0:compile
[INFO]    |  |  \- org.apache.commons:commons-compress:jar:1.4.1:compile
[INFO]    |  |     \- org.tukaani:xz:jar:1.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-hdfs:jar:2.2.0:compile
[INFO]    |  |  \- org.mortbay.jetty:jetty-util:jar:6.1.26:compile
[INFO]    |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.2.0:compile
[INFO]    |  |  +- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.2.0:compile
[INFO]    |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.2.0:compile
[INFO]    |  |  |  |  \- com.google.inject:guice:jar:3.0:compile
[INFO]    |  |  |  |     +- javax.inject:javax.inject:jar:1:compile
[INFO]    |  |  |  |     \- aopalliance:aopalliance:jar:1.0:compile
[INFO]    |  |  |  \- org.apache.hadoop:hadoop-yarn-server-common:jar:2.2.0:compile
[INFO]    |  |  \- org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.2.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.2.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.2.0:compile
[INFO]    |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.2.0:compile
[INFO]    |  +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.2.0:compile
[INFO]    |  \- org.apache.hadoop:hadoop-annotations:jar:2.2.0:compile
[INFO]    +- org.apache.spark:spark-launcher_2.10:jar:2.0.1:compile
[INFO]    +- org.apache.spark:spark-network-common_2.10:jar:2.0.1:compile
[INFO]    |  +- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:compile
[INFO]    |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.6.5:compile
[INFO]    +- org.apache.spark:spark-network-shuffle_2.10:jar:2.0.1:compile
[INFO]    +- org.apache.spark:spark-unsafe_2.10:jar:2.0.1:compile
[INFO]    +- net.java.dev.jets3t:jets3t:jar:0.7.1:compile
[INFO]    |  +- commons-codec:commons-codec:jar:1.3:compile
[INFO]    |  \- commons-httpclient:commons-httpclient:jar:3.1:compile
[INFO]    +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO]    |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO]    |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO]    |  +- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO]    |  \- com.google.guava:guava:jar:14.0.1:compile
[INFO]    +- javax.servlet:javax.servlet-api:jar:3.1.0:compile
[INFO]    +- org.apache.commons:commons-lang3:jar:3.3.2:compile
[INFO]    +- org.apache.commons:commons-math3:jar:3.4.1:compile
[INFO]    +- com.google.code.findbugs:jsr305:jar:1.3.9:compile
[INFO]    +- org.slf4j:slf4j-api:jar:1.7.16:compile
[INFO]    +- org.slf4j:jul-to-slf4j:jar:1.7.16:compile
[INFO]    +- org.slf4j:jcl-over-slf4j:jar:1.7.16:compile
[INFO]    +- log4j:log4j:jar:1.2.17:compile
[INFO]    +- org.slf4j:slf4j-log4j12:jar:1.7.16:compile
[INFO]    +- com.ning:compress-lzf:jar:1.0.3:compile
[INFO]    +- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile
[INFO]    +- net.jpountz.lz4:lz4:jar:1.3.0:compile
[INFO]    +- org.roaringbitmap:RoaringBitmap:jar:0.5.11:compile
[INFO]    +- commons-net:commons-net:jar:2.2:compile
[INFO]    +- org.scala-lang:scala-library:jar:2.10.6:compile
[INFO]    +- org.json4s:json4s-jackson_2.10:jar:3.2.11:compile
[INFO]    |  \- org.json4s:json4s-core_2.10:jar:3.2.11:compile
[INFO]    |     +- org.json4s:json4s-ast_2.10:jar:3.2.11:compile
[INFO]    |     +- com.thoughtworks.paranamer:paranamer:jar:2.6:compile
[INFO]    |     \- org.scala-lang:scalap:jar:2.10.0:compile
[INFO]    |        \- org.scala-lang:scala-compiler:jar:2.10.0:compile
[INFO]    +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile
[INFO]    |  +- javax.ws.rs<http://javax.ws.rs>:javax.ws.rs<http://javax.ws.rs>-api:jar:2.0.1:compile
[INFO]    |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile
[INFO]    |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile
[INFO]    |  |  \- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile
[INFO]    |  +- org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile
[INFO]    |  \- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile
[INFO]    |     \- org.javassist:javassist:jar:3.18.1-GA:compile
[INFO]    +- org.glassfish.jersey.core:jersey-common:jar:2.22.2:compile
[INFO]    |  +- javax.annotation:javax.annotation-api:jar:1.2:compile
[INFO]    |  +- org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.22.2:compile
[INFO]    |  \- org.glassfish.hk2:osgi-resource-locator:jar:1.0.1:compile
[INFO]    +- org.glassfish.jersey.core:jersey-server:jar:2.22.2:compile
[INFO]    |  +- org.glassfish.jersey.media:jersey-media-jaxb:jar:2.22.2:compile
[INFO]    |  \- javax.validation:validation-api:jar:1.1.0.Final:compile
[INFO]    +- org.glassfish.jersey.containers:jersey-container-servlet:jar:2.22.2:compile
[INFO]    +- org.glassfish.jersey.containers:jersey-container-servlet-core:jar:2.22.2:compile
[INFO]    +- org.apache.mesos:mesos:jar:shaded-protobuf:0.21.1:compile
[INFO]    +- io.netty:netty-all:jar:4.0.29.Final:compile
[INFO]    +- io.netty:netty:jar:3.8.0.Final:compile
[INFO]    +- com.clearspring.analytics:stream:jar:2.7.0:compile
[INFO]    +- io.dropwizard.metrics:metrics-core:jar:3.1.2:compile
[INFO]    +- io.dropwizard.metrics:metrics-jvm:jar:3.1.2:compile
[INFO]    +- io.dropwizard.metrics:metrics-json:jar:3.1.2:compile
[INFO]    +- io.dropwizard.metrics:metrics-graphite:jar:3.1.2:compile
[INFO]    +- com.fasterxml.jackson.core:jackson-databind:jar:2.6.5:compile
[INFO]    |  \- com.fasterxml.jackson.core:jackson-core:jar:2.6.5:compile
[INFO]    +- com.fasterxml.jackson.module:jackson-module-scala_2.10:jar:2.6.5:compile
[INFO]    |  +- org.scala-lang:scala-reflect:jar:2.10.6:compile
[INFO]    |  \- com.fasterxml.jackson.module:jackson-module-paranamer:jar:2.6.5:compile
[INFO]    +- org.apache.ivy:ivy:jar:2.4.0:compile
[INFO]    +- oro:oro:jar:2.0.8:compile
[INFO]    +- net.razorvine:pyrolite:jar:4.9:compile
[INFO]    +- net.sf.py4j:py4j:jar:0.10.3:compile
[INFO]    +- org.apache.spark:spark-tags_2.10:jar:2.0.1:compile
[INFO]    |  \- org.scalatest:scalatest_2.10:jar:2.2.6:compile
[INFO]    \- org.spark-project.spark:unused:jar:1.0.0:compile

Yes, but scalatest doesn't end up in compile scope, says Maven?

...

[INFO] +- org.apache.spark:spark-tags_2.11:jar:2.1.0-SNAPSHOT:compile

[INFO] |  +- (org.scalatest:scalatest_2.11:jar:2.2.6:test - scope managed from compile; omitted for duplicate)

[INFO] |  \- (org.spark-project.spark:unused:jar:1.0.0:compile - omitted for duplicate)

[INFO] +- org.apache.commons:commons-crypto:jar:1.0.0:compile

[INFO] +- org.spark-project.spark:unused:jar:1.0.0:compile

[INFO] +- org.scalatest:scalatest_2.11:jar:2.2.6:test

...

spark-core depends on spark-launcher (compile)
spark-launcher depends on spark-tags (compile)
spark-tags depends on scalatest (compile)

To be honest I'm not all that familiar with the project structure - should I just exclude spark-launcher if I'm not using it?

It's required because the tags module uses it to define annotations for tests. I don't see it in compile scope for anything but the tags module, which is then in test scope for other modules. What are you seeing that makes you say it's in compile scope?

Hey everybody,

Just a heads up that currently Spark 2.0.1 has a compile dependency on Scalatest 2.2.6. It comes from spark-core's dependency on spark-launcher, which has a transitive dependency on spark-tags, which has a compile dependency on Scalatest.

This makes it impossible to use any other version of Scalatest for testing your app if you declare a dependency on any Spark 2.0.1 module; you'll get a bunch of runtime errors during testing (unless you figure out the reason like I did and explicitly exclude Scalatest from the spark dependency).

I think that dependency should probably be moved to a test dependency instead.

Thanks,
Jeremy



"
aditya1702 <adityavyas17@gmail.com>,"Sat, 29 Oct 2016 09:08:55 -0700 (MST)",GSoC projects related to Spark,dev@spark.apache.org,"Hello all,
I am really interested in Spark. I have been doing small projects in machine
learning using spark and would love to do a project in this year's GSoC. Can
anyone tell me are there any projects related to Spark for this year's GSoC?



--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 29 Oct 2016 18:29:49 +0200",[info] Warning: Unknown ScalaCheck args provided: -oDF,dev <dev@spark.apache.org>,"Hi,

Just noticed the messages from the recent build of my pull request in Jenkins:

[info] Warning: Unknown ScalaCheck args provided: -oDF

I think we should fix it, right?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jeremy Smith <jeremy.smith@acorns.com>,"Sun, 30 Oct 2016 16:39:50 -0700",Re: Spark has a compile dependency on scalatest,Sean Owen <sowen@cloudera.com>,"Well, recent experience tells me that at least SBT does not behave this
way. Given a `test` dependency on scalatest 3.0.0, my tests were still
being compiled against 2.2.6, which caused a combination of compile errors
and runtime errors (given that apparently 3.0.0 was still present at
runtime during the test run.) Actually, it could be that they were being
compiled against 3.0.0 but 2.2.6 was present at runtime. I'm not sure which
is accurate, but adding an exclusion rule when bringing in Spark in order
to exclude both scalatest and scalactic resolved the problem.

This may not be a huge issue in real practice, since people will typically
be marking spark in its entirety as `provided,test`. In any case, it
doesn't seem like it's intentional that scalatest makes its way into
compile scope when using any spark dependency in SBT. It's not clear to me
whether this is an issue with SBT or with the spark build, but it's a thing
that does happen when using SBT (which I imagine a great many people do
when building Spark applications).

Jeremy


"
Sean Owen <sowen@cloudera.com>,"Mon, 31 Oct 2016 09:01:35 +0000",Re: Spark has a compile dependency on scalatest,Jeremy Smith <jeremy.smith@acorns.com>,"SBT and Maven resolution rules do differ. I thought SBT was generally
latest-first though, which should make 3.0 take priority. Maven is more
like closest-first, which means you can pretty much always override things
in your own build. An exclusion is the right way to go in this case because
the deployed POM does look like it says what we all want it to say, at
least.


"
Zak H <zak.hassan1010@gmail.com>,"Mon, 31 Oct 2016 08:09:50 -0400",Interesting in contributing to spark,dev@spark.apache.org,"Hi,

I'd like to introduce myself. My name is Zak and I'm a software engineer.
I'm interested in contributing to spark as a way to learn more. I've signed
up to the mailing list and hope to learn more about spark. What do you
recommend I start on as my first bug ? I have a working knowledge of
scala/java/maven ?

Thanks,
Zak Hassan
https://github.com/zmhassan
"
Reynold Xin <rxin@databricks.com>,"Mon, 31 Oct 2016 10:08:45 -0700",Re: Interesting in contributing to spark,Zak H <zak.hassan1010@gmail.com>,"Welcome!

This is the best guide to get started:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 31 Oct 2016 10:28:28 -0700",Re: Python Spark Improvements (forked from Spark Improvement Proposals),Tobi Bosede <ani.tobib@gmail.com>,"I've been working on some of the issues and I have an PR to help provide
pip installable PySpark - https://github.com/apache/spark/pull/15659 .

Also if anyone is interested in PySpark UDF performance there is a closed
PR that if we could maybe figure out a way to make it a little more light
weight I'd love to hear people's ideas (
https://github.com/apache/spark/pull/13571 ).


 if
ng
s
ment-Proposals-td19268.html#none>
s
&ie=UTF-8#q=holden+karau+pydata&start=0>
 and dev@
out
™ve been
I
 or
d try and
 on
o a
ran
ld
ch about
very loose term)
ure what
er
 around supporting custom
_
her
€™t
s.
t
ent set of
e
€ profile
s
€™m
ed 0.1
s
nt
s.
th
for
JVM
omplaint some people
e/Dataset
oices with
l
and
did
n more
ew
l
 of work -
how
re
g through
or
ark
he
 PR to add
t I
ty
seen develop in
. medium sized data
ing, but if
ften
see Spark
missed important
ng
o
s
 a
.
ed
œnormalâ€
se
ng
e
 of my


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Cody Koeninger <cody@koeninger.org>,"Mon, 31 Oct 2016 12:34:09 -0500",Re: Odp.: Spark Improvement Proposals,"""dev@spark.apache.org"" <dev@spark.apache.org>","Now that spark summit europe is over, are any committers interested in
moving forward with this?

https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md

Or are we going to let this discussion die on the vine?

e
he
art
rk.
 -
d
th
me
ng
I
to
L
n
rk
ry
ed
ok.com>
""
""
d
e
y
g
e
f
e
d
t

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 31 Oct 2016 11:37:40 -0700",JIRA Components for Streaming,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm planning to do a little maintenance on JIRA to hopefully improve the
visibility into the progress / gaps in Structured Streaming.  In
particular, while we share a lot of optimization / execution logic with
SQL, the set of desired features and bugs is fairly different.

Proposal:
  - Structured Streaming (new component, move existing tickets here)
  - Streaming -> DStreams

Thoughts, objections?

Michael
"
Cody Koeninger <cody@koeninger.org>,"Mon, 31 Oct 2016 14:41:23 -0500",Re: JIRA Components for Streaming,Michael Armbrust <michael@databricks.com>,"Makes sense to me.

I do wonder if e.g.

[SPARK-12345][STRUCTUREDSTREAMING][KAFKA]

is going to leave any room in the Github PR form for actual title content?


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 31 Oct 2016 12:42:08 -0700",Re: JIRA Components for Streaming,Cody Koeninger <cody@koeninger.org>,"Maybe just streaming or SS in GitHub?


"
ankits <ankitsoni9@gmail.com>,"Mon, 31 Oct 2016 14:05:55 -0700 (MST)",Re: Issue with repartition and cache,dev@spark.apache.org,"Hi,

Did you ever figure this one out? I'm seeing the same behavior:

Calling cache() after a repartition() makes Spark cache the version of the
RDD BEFORE the repartition, which means a shuffle everytime it is accessed..

However, calling cache before the repartition() seems to work fine, the
cached version has the new partitioning applied.


In summary, these 2 patterns dont seem to work as expected:

-------
repartition()
cache()
--------
repartition()
cache()
count()
---------



But this works:

cache()
repartition()

Very strange..



--

---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Mon, 31 Oct 2016 14:07:25 -0700",Updating Parquet dep to 1.9,dev@spark.apache.org,"Hi All,

Is anyone working on updating Spark's Parquet library dep to 1.9? If not, I can at least get started on it and publish a PR.

Cheers,

Michael
---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 31 Oct 2016 14:54:25 -0700",Re: Odp.: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"I agree, we should push forward on this. I think there is enough consensus
to call a vote, unless someone else thinks that there is more to discuss?

rb

:

e
me
s
as
e
,
n
th
d,
in
e
r,
ge
,
k,
,
me
't
o
ut
f
y,
).
rs
e
he
nd


-- 
Ryan Blue
Software Engineer
Netflix
"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 31 Oct 2016 15:12:24 -0700",Re: Odp.: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"The proposal looks OK to me. I assume, even though it's not explicitly
called, that voting would happen by e-mail? A template for the
proposal document (instead of just a bullet nice) would also be nice,
but that can be done at any time.

BTW, shameless plug: I filed SPARK-18085 which I consider a candidate
for a SIP, given the scope of the work. The document attached even
somewhat matches the proposed format. So if anyone wants to try out
the process...

:
oposals.md
he
the
hart
e
ark.
"" -
ld
ith
ome
ing
s
 I
 to
h
QL
on
ork
ery
eed
ook.com>
,
t""
n
.""
,
e
,
nd
e
t
re
t
dy
ng
ce
,
.
s
of
ge
nd
e
d
et
-



-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 31 Oct 2016 16:35:30 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","OK I will cut a new RC tomorrow. Any other issues people have seen?


m

/
t
d
nd
:
/
/
g
ll
C
.0.2.
"
Denny Lee <denny.g.lee@gmail.com>,"Mon, 31 Oct 2016 23:52:11 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),"Reynold Xin <rxin@databricks.com>, ""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Oh, I forgot to note that when downloading and running against the Spark
2.0.2 without Hadoop binaries, I got a JNI error due to an exception with
org / slf4j / logger  (i.e. org.slf4j.Logger class is not found).



:
f
n
t
2.
"
