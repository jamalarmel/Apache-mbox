khyati <khyati.shah@guavus.com>,"Sat, 31 Dec 2016 21:06:17 -0700 (MST)",Task not Serializable Exception,dev@spark.apache.org,"Getting error for the following code snippet:

object SparkTaskTry extends Logging {
  63   /**
  64    * Extends the normal Try constructor to allow TaskKilledExceptions
to propagate
  65    */
  66   def apply[T](r: => T): Try[T] =
  67     try scala.util.Success(r) catch {
  68       case e: TaskKilledException => throw e
  69       case NonFatal(e) =>
  70         logInfo(""Caught and Ignored Exception: "" + e.toString)
  71         e.printStackTrace()
  72         Failure(e)
  73     }
  74 }

override def buildScan(
 349       requiredColumns: Array[String],
 350       filters: Array[Filter],
 351       inputFiles: Array[FileStatus],
 352       broadcastedConf: Broadcast[SerializableConfiguration]): RDD[Row]
= {
 353     val useMetadataCache =
sqlContext.getConf(SQLConf.PARQUET_CACHE_METADATA)
 354     val parquetFilterPushDown = sqlContext.conf.parquetFilterPushDown
 355     val assumeBinaryIsString = sqlContext.conf.isParquetBinaryAsString
 356     val assumeInt96IsTimestamp =
sqlContext.conf.isParquetINT96AsTimestamp
 357     val followParquetFormatSpec =
sqlContext.conf.followParquetFormatSpec
 358 
 359     // When merging schemas is enabled and the column of the given
filter does not exist,
 360     // Parquet emits an exception which is an issue of Parquet
(PARQUET-389).
 361     val safeParquetFilterPushDown = !shouldMergeSchemas &&
parquetFilterPushDown
 362 
 363     // Parquet row group size. We will use this value as the value for
 364     // mapreduce.input.fileinputformat.split.minsize and
mapred.min.split.size if the value
 365     // of these flags are smaller than the parquet row group size.
 366     val parquetBlockSize =
ParquetOutputFormat.getLongBlockSize(broadcastedConf.value.value)
 367 
 368     // Create the function to set variable Parquet confs at both driver
and executor side.
 369     val initLocalJobFuncOpt =
 370       ParquetRelation.initializeLocalJobFunc(
 371         requiredColumns,
 372         filters,
 373         dataSchema,
 374         parquetBlockSize,
 375         useMetadataCache,
 376         safeParquetFilterPushDown,
 377         assumeBinaryIsString,
 378         assumeInt96IsTimestamp,
 379         followParquetFormatSpec) _
 380 
 381     // Create the function to set input paths at the driver side.
 382     val setInputPaths =
 383       ParquetRelation.initializeDriverSideJobFunc(inputFiles,
parquetBlockSize) _
 384 
 385     Utils.withDummyCallSite(sqlContext.sparkContext) {
 386       new RDD[Try[InternalRow]](sqlContext.sparkContext, Nil) with
Logging {
 387 
 388         override def getPartitions: Array[SparkPartition] =
internalRDD.getPartitions
 389 
 390         override def getPreferredLocations(split: SparkPartition):
Seq[String] =
 391           internalRDD.getPreferredLocations(split)
 392 
 393         override def checkpoint() {
 394           // Do nothing. Hadoop RDD should not be checkpointed.
 395         }
 396 
 397         override def persist(storageLevel: StorageLevel): this.type = {
 398           super.persist(storageLevel)
 399         }
 400 
 401         val internalRDD: SqlNewHadoopRDD[InternalRow] = new
SqlNewHadoopRDD(
 402         sc = sqlContext.sparkContext,
 403         broadcastedConf = broadcastedConf,
 404         initDriverSideJobFuncOpt = Some(setInputPaths),
 405         initLocalJobFuncOpt = Some(initLocalJobFuncOpt),
 406         inputFormatClass = if (isSplittable) {
 407           classOf[ParquetInputFormat[InternalRow]]
 408         } else {
 409           classOf[ParquetRowInputFormatIndivisible]
 410         },
 411         valueClass = classOf[InternalRow]) {
 412 
 413         val cacheMetadata = useMetadataCache
 414 
 415         @transient val cachedStatuses = inputFiles.map { f =>
 416           // In order to encode the authority of a Path containing
special characters such as '/'
 417           // (which does happen in some S3N credentials), we need to
use the string returned by the
 418           // URI of the path to create a new Path.
 419           val pathWithEscapedAuthority = escapePathUserInfo(f.getPath)
 420           new FileStatus(
 421             f.getLen, f.isDirectory, f.getReplication, f.getBlockSize,
f.getModificationTime,
 422             f.getAccessTime, f.getPermission, f.getOwner, f.getGroup,
pathWithEscapedAuthority)
 423         }.toSeq
 424 
 425         private def escapePathUserInfo(path: Path): Path = {
 426           val uri = path.toUri
 427           new Path(new URI(
 428             uri.getScheme, uri.getRawUserInfo, uri.getHost,
uri.getPort, uri.getPath,
 429             uri.getQuery, uri.getFragment))
 430         }
 431 
 432         // Overridden so we can inject our own cached files statuses.
 433         override def getPartitions: Array[SparkPartition] = {
 434           val inputFormat = new ParquetInputFormat[InternalRow] {
 435             override def listStatus(jobContext: JobContext):
JList[FileStatus] = {
 436               if (cacheMetadata) cachedStatuses else
super.listStatus(jobContext)
 437             }
 438           }
 439 
 440           val jobContext = newJobContext(getConf(isDriverSide = true),
jobId)
 441           val rawSplits = inputFormat.getSplits(jobContext)
 442 
 443           Array.tabulate[SparkPartition](rawSplits.size) { i =>
 444             new SqlNewHadoopPartition(id, i,
rawSplits(i).asInstanceOf[InputSplit with Writable])
 445           }
 446         }
 447       }
 448 
 449         override def compute(part: SparkPartition, context:
TaskContext):
 450         InterruptibleIterator[Try[InternalRow]] = {
 451           val iter: Iterator[InternalRow] =
internalRDD.constructIter(part, context)
 452           val tryIter = new Iterator[Try[InternalRow]] {
 453             override def next(): Try[InternalRow] = {
 454               val readAttempt = SparkTaskTry(iter.next())
 455               readAttempt
 456             }
 457 
 458             override def hasNext: Boolean = {
 459               SparkTaskTry[Boolean](iter.hasNext) match {
 460                 case scala.util.Success(r) => r
 461                 case _ => false
 462               }
 463             }
 464           }
 465           new InterruptibleIterator[Try[InternalRow]](context, tryIter)
 466         }
 467 
 468       }.filter(_.isSuccess).map(_.get)
 469         .asInstanceOf[RDD[Row]]  // type erasure hack to pass
RDD[InternalRow] as RDD[Row]
 470     }
 471   }

Error StackTrace :
Caused by: java.io.NotSerializableException:
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation
Serialization stack:
	- object not serializable (class:
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation, value:
ParquetRelation)
	- field (class:
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$buildInternalScan$1,
name: $outer, type: class
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation)
	- object (class
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$buildInternalScan$1,
<function0>)
	- field (class:
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$buildInternalScan$1$$anon$2,
name: $outer, type: class
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$buildInternalScan$1)
	- object (class
org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$buildInternalScan$1$$anon$2,
$anonfun$buildInternalScan$1$$anon$2[2] at )
	- field (class: org.apache.spark.NarrowDependency, name: _rdd, type: class
org.apache.spark.rdd.RDD)
	- writeObject data (class: scala.collection.immutable.$colon$colon)
	- object (class scala.collection.immutable.$colon$colon,
	- field (class: org.apache.spark.rdd.RDD, name:
org$apache$spark$rdd$RDD$$dependencies_, type: interface
scala.collection.Seq)
	- object (class org.apache.spark.rdd.MapPartitionsRDD, MapPartitionsRDD[4]
at )
	- field (class: org.apache.spark.rdd.MapPartitionsRDD, name: prev, type:
class org.apache.spark.rdd.RDD)
	- object (class org.apache.spark.rdd.MapPartitionsRDD, MapPartitionsRDD[5]
at )
	- field (class: org.apache.spark.sql.execution.PhysicalRDD, name: rdd,
type: class org.apache.spark.rdd.RDD)
	- object (class org.apache.spark.sql.execution.PhysicalRDD, Scan
ParquetRelation[_1#0] InputPaths:
hdfs://CRUX2-SETUP:9000/data/testdir/data1.parquet
)
	- field (class: org.apache.spark.sql.execution.ConvertToSafe, name: child,
type: class org.apache.spark.sql.execution.SparkPlan)
	- object (class org.apache.spark.sql.execution.ConvertToSafe, ConvertToSafe
+- Scan ParquetRelation[_1#0] InputPaths:
hdfs://CRUX2-SETUP:9000/data/testdir/data1.parquet
)
	- field (class: org.apache.spark.sql.execution.ConvertToSafe$$anonfun$2,
name: $outer, type: class org.apache.spark.sql.execution.ConvertToSafe)
	- object (class org.apache.spark.sql.execution.ConvertToSafe$$anonfun$2,
<function1>)
	at
org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
	at
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
	at
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
	... 78 more

Please help!



--

---------------------------------------------------------------------


"
khyati <khyati.shah@guavus.com>,"Sat, 31 Dec 2016 21:11:17 -0700 (MST)",Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"Hi,

I am trying to read the multiple parquet files in sparksql. In one dir there
are two files, of which one is corrupted. While trying to read these files,
sparksql throws Exception for the corrupted file.

val newDataDF =
sqlContext.read.parquet(""/data/testdir/data1.parquet"",""/data/testdir/corruptblock.0"")
newDataDF.show 

throws Exception.

Is there any way to just skip the file having corrupted block/footer and
just read the file/files which are proper? 

Thanks



--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 1 Jan 2017 05:34:41 -0700 (MST)","Re: context.runJob() was suspended in getPreferredLocations()
 function",dev@spark.apache.org,"
Hi,

Simply said, you submit another Job in the event thread which will be
blocked and can't receive the this job submission event. So your second job
submission is never processed, and the getPreferredLocations method is never
returned.



Fei Hu wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Abhishek <smartshobhu@gmail.com>,"Sun, 1 Jan 2017 12:16:06 -0700 (MST)",Re: Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"You will have to change the metadata file under _spark_metadata folder to remove the listing of corrupt files.

Thanks,
Shobhit G 

Sent from my iPhone





-----
Regards, 
Abhi
--"
Reynold Xin <rxin@databricks.com>,"Mon, 2 Jan 2017 07:49:57 +0900",Re: Skip Corrupted Parquet blocks / footer.,khyati <khyati.shah@guavus.com>,"In Spark 2.1, set spark.sql.files.ignoreCorruptFiles to true.


"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 2 Jan 2017 16:45:20 +0530",Re: Task not Serializable Exception,khyati <khyati.shah@guavus.com>,"Can you minimize the code snippet with which we can get this
`NotSerializableException`
exception?

Thanks,


-----
Prashant Sharma
Spark Technology Center
http://www.spark.tc/
--



"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 2 Jan 2017 16:49:05 +0530",Re: Kafka Spark structured streaming latency benchmark.,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","This issue was fixed in https://issues.apache.org/jira/browse/SPARK-18991.

--Prashant



"
Cody Koeninger <cody@koeninger.org>,"Mon, 2 Jan 2017 09:45:55 -0600",Re: Spark Improvement Proposals,Ryan Blue <rblue@netflix.com>,"I'm bumping this one more time for the new year, and then I'm giving up.

Please, fix your process, even if it isn't exactly the way I suggested.

ast
hy
:
,
e:
nt
ue
e
a
k
s
ve
ld
lt
mnZ7SUi4qMljg/edit#heading=h.36ut37zh7w2b
a
"".
r
en
rovement-proposals.md
nk
le
ee
e
e
t
y
on
k
t
t
nt
rnals
rnals
ay
e
y
e
as
as
 )
r
n
k
se
s
s
t
ut
e.
P.
o

---------------------------------------------------------------------


"
Shuai Lin <linshuai2012@gmail.com>,"Tue, 3 Jan 2017 01:30:27 +0800","Re: What is mainly different from a UDT and a spark internal type
 that ExpressionEncoder recognized?",dragonly <liyilongko@gmail.com>,"Disclaimer: I'm not a spark guru, and what's written below are some notes I
took when reading spark source code, so I could be wrong, in which case I'd
appreciate a lot if someone could correct me.




IIUC, physical operators like `ProjectExec` implements doProduce/doConsume
to support codegen, and when whole-stage codegen is enabled, a subtree
would be collapsed into a WholeStageCodegenExec wrapper tree, and the root
node of the wrapper tree would call the doProduce/doConsume method of each
operator to generate the java source code to be compiled into java byte
code by janino.

In contrast, when whole stage code gen is disabled (e.g. by passing ""--conf
spark.sql.codegen.wholeStage=false"" to spark submit), the doExecute method
of the physical operators are called so no code generation would happen.

The producing of the RDDs is some post-order SparkPlan tree evaluation. The
leaf node would be some data source: either some file-based
HadoopFsRelation, or some external data sources like JdbcRelation, or
in-memory LocalRelation created by ""spark.range(100)"". Above all, the leaf
nodes could produce rows on their own. Then the evaluation goes in a bottom
up manner, applying filter/limit/project etc. along the way. The generated
code or the various doExecute method would be called, depending on whether
codegen is enabled (the default) or not.



AFAIK the `eval` method of Expression is used to do static evaluation when
the expression is foldable, e.g.:

   select map('a', 1, 'b', 2, 'a', 3) as m

Regards,
Shuai



"
Joseph Bradley <joseph@databricks.com>,"Mon, 2 Jan 2017 12:28:03 -0800",Re: mllib metrics vs ml evaluators and how to improve apis for users,Ilya Matiach <ilmat@microsoft.com>,"Hi Ilya,

Thanks for your thoughts.  Here's my understanding of where we are headed:
* We will want to move the *Metrics functionality to the spark.ml package,
as part of *Evaluator or related classes such as model/result summaries.
* It has not yet been decided if or when the spark.mllib package will be
removed.  This cannot happen until spark.ml has complete feature parity and
has been separated from spark.mllib internally for a few releases, and it
will require a community vote and significant QA.
* You're correct that Evaluators are meant for model tuning.  IMO, multiple
metrics are more naturally handled by model/result summaries, though I
could see good arguments for separating the metric computation from
models.  This is an issue which has not yet been discussed properly.  There
have also been questions about Evaluators maintaining multiple metrics
along the way during model tuning (SPARK-18704).

I created a JIRA for discussing this further:
https://issues.apache.org/jira/browse/SPARK-19053

Thanks!
Joseph


?
The mllib metrics seem very useful because they are able to compute/expose
c.
Also, the ml evaluators expose a lot fewer metrics than the mllib metrics
e sense to
That would solve my issue if that is what is planned in the future.
 confusion
t the
r.



-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Jeremy Smith <jeremy.smith@acorns.com>,"Mon, 2 Jan 2017 14:05:02 -0800",StateStoreSaveExec / StateStoreRestoreExec,dev <dev@spark.apache.org>,"I have a question about state tracking in Structured Streaming.

First let me briefly explain my use case: Given a mutable data source (i.e.
an RDBMS) in which we assume we can retrieve a set of newly created row
versions (being a row that was created or updated between two given
`Offset`s, whatever those are), we can create a Structured Streaming
`Source` which retrieves the new row versions. Further assuming that every
logical row has some primary key, then as long as we can track the current
offset for each primary key, we can differentiate between new and updated
rows. Then, when a row is updated, we can record that the previous version
of that row expired at some particular time. That's essentially what I'm
trying to do. This would effectively give you an ""event-sourcing"" type of
historical/immutable log of changes out of a mutable data source.

I noticed that in Spark 2.0.1 there was a concept of a StateStore, which
seemed like it would allow me to do exactly the tracking that I needed, so
I decided to try and use that built-in functionality rather than some
external key/value store for storing the current ""version number"" of each
primary key. There were a lot of hard-coded hoops I had to jump through,
but I eventually made it work by implementing some custom LogicalPlans and
SparkPlans around StateStore[Save/Restore]Exec.

Now, in Spark 2.1.0 it seems to have gotten even further away from what I
was using it for - the keyExpressions of StateStoreSaveExec must include a
timestamp column, which means that those expressions are not really keys
(at least not for a logical row). So it appears I can't use it that way
anymore (I can't blame Spark for this, as I knew what I was getting into
when leveraging developer APIs). There are also several hard-coded checks
which now make it clear that StateStore functionality is only to be used
for streaming aggregates, which is not really what I'm doing.

My question is - is there a good way to accomplish the above use case
within Structured Streaming? Or is this the wrong use case for the state
tracking functionality (which increasingly seems to be targeted toward
aggregates only)? Is there a plan for any kind of generalized
`mapWithState`-type functionality for Structured Streaming, or should I
just give up on that and use an external key/value store for my state
tracking?

Thanks,
Jeremy
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 3 Jan 2017 12:27:00 +0100",Why ShuffleMapTask has transient locs and preferredLocs?!,dev <dev@spark.apache.org>,"Hi,

Just found out that ShuffleMapTask has transient locs and
preferredLocs attributes which means that when ShuffleMapTask is
serialized (as a broadcast variable) the information is gone.

Does this mean that the attributes could have not been defined at all
since Spark uses SortShuffleManager (and BlockManagerMaster on the
driver) to track the shuffle locations (MapStatuses)?

Is my understanding correct? What am I missing? (I'm exploring shuffle
system currently and would appreciate comments a lot!) Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 3 Jan 2017 13:55:51 +0100","Re: What is mainly different from a UDT and a spark internal type
 that ExpressionEncoder recognized?",Shuai Lin <linshuai2012@gmail.com>,"Hi Shuai,

Disclaimer: I'm not a spark guru, and what's written below are some
notes I took when reading spark source code, so I could be wrong, in
which case I'd appreciate a lot if someone could correct me.

(Yes, I did copy your disclaimer since it applies to me too. Sorry for
duplication :))

I'd say that the description is very well-written and clear. I'd only add that:

1. CodegenSupport allows custom implementations to optionally disable
codegen using supportCodegen predicate (that is enabled by default,
i.e. true)
2. CollapseCodegenStages is a Rule[SparkPlan], i.e. a transformation
of SparkPlan into another SparkPlan, that searches for sub-plans (aka
stages) that support codegen and collapse them together as a
WholeStageCodegen for which supportCodegen is enabled.
3. It is assumed that all Expression instances except CodegenFallback
support codegen.
4. CollapseCodegenStages uses the internal setting
spark.sql.codegen.maxFields (default: 200) to control the number of
fields in input and output schemas before deactivating whole-stage
codegen. See https://issues.apache.org/jira/browse/SPARK-14554.

NOTE: The magic number 200 (!) again. I asked about it few days ago
and in http://stackoverflow.com/questions/41359344/why-is-the-number-of-partitions-after-groupby-200

5. There are side-effecting logical commands that are executed for
their side-effects that are translated to ExecutedCommandExec in
BasicOperators strategy and won't take part in codegen.

Thanks for sharing your notes! Gonna merge yours with mine! Thanks.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


 I
'd
Is
e
uld
 of
or
nf
hod
he
f
om
d
r
n
ch
d
he
-different-from-a-UDT-and-a-spark-internal-type-that-ExpressionEncoder-recognized-tp20370p20376.html

---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 3 Jan 2017 15:06:45 +0100","Re: What is mainly different from a UDT and a spark internal type
 that ExpressionEncoder recognized?",Jacek Laskowski <jacek@japila.pl>,"@Jacek The maximum output of 200 fields for whole stage code generation has
been chosen to prevent the code generated method from exceeding the 64kb
code limit. There absolutely no relation between this value and the number
of partitions after a shuffle ("
Jacek Laskowski <jacek@japila.pl>,"Tue, 3 Jan 2017 15:27:44 +0100","Re: What is mainly different from a UDT and a spark internal type
 that ExpressionEncoder recognized?",=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thanks Herman for the explanation.

I silently assume that the other points were ok since you did not object?
Correct?


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


d
:
e
n.
.
y
:
n
in
y
h
"
khyati <khyati.shah@guavus.com>,"Tue, 3 Jan 2017 12:03:36 -0700 (MST)",Re: Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"Hi Reynold Xin,

I tried setting spark.sql.files.ignoreCorruptFiles = true by using commands,

val sqlContext =new org.apache.spark.sql.hive.HiveContext(sc)

sqlContext.setConf(""spark.sql.files.ignoreCorruptFiles"",""true"") /
sqlContext.sql(""set spark.sql.files.ignoreCorruptFiles=true"")

but still getting error while reading parquet files using 
val newDataDF =
sqlContext.read.parquet(""/data/tempparquetdata/corruptblock.0"",""/data/tempparquetdata/data1.parquet"")

Error: ERROR executor.Executor: Exception in task 0.0 in stage 4.0 (TID 4)
java.io.IOException: Could not read footer: java.lang.RuntimeException:
hdfs://192.168.1.53:9000/data/tempparquetdata/corruptblock.0 is not a
Parquet file. expected magic number at tail [80, 65, 82, 49] but found [65,
82, 49, 10]
	at
org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:248)


Please let me know if I am missing anything.




--

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 3 Jan 2017 12:13:25 -0800",Re: Skip Corrupted Parquet blocks / footer.,khyati <khyati.shah@guavus.com>,"Khyati,

Are you using Spark 2.1? The usual entry point for Spark 2.x is spark
rather than sqlContext.

rb
​


)
5,


-- 
Ryan Blue
Software Engineer
Netflix
"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 3 Jan 2017 12:32:37 -0800",Re: Apache Hive with Spark Configuration,Chetan Khatri <chetan.opensource@gmail.com>,"Chetan,

Spark is currently using Hive 1.2.1 to interact with the Metastore. Using
that version for Hive is going to be the most reliable, but the metastore
API doesn't change very often and we've found (from having different
versions as well) that older versions are mostly compatible. Some things
fail occasionally, but we haven't had too many problems running different
versions with the same metastore in practice.

rb





-- 
Ryan Blue
Software Engineer
Netflix
"
Imran Rashid <irashid@cloudera.com>,"Tue, 3 Jan 2017 15:08:35 -0600",Re: Why ShuffleMapTask has transient locs and preferredLocs?!,Jacek Laskowski <jacek@japila.pl>,"Hi Jacek,

I'm not entirely sure I understand your question, but the reason
preferredLocs can be transient is b/c that is used to define where the
scheduler (on the driver) should prefer to assign the task.  But no matter
the value, the task could still get assigned anywhere.  By the time that
task has been assigned a location, and its running on an executor, it
doesn't matter anymore.

preferredLocations are entirely independent of having the map task know
where to fetch its input shuffle data, and where the shuffle map task
writes it output data.  All of that info goes through MapOutputTracker.

hope that helps,
Imran


"
Joseph Bradley <joseph@databricks.com>,"Tue, 3 Jan 2017 13:30:12 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Hi Cody,

Thanks for being persistent about this.  I too would like to see this
happen.  Reviewing the thread, it sounds like the main things remaining are:
* Decide about a few issues
* Finalize the doc(s)
* Vote on this proposal

Issues & TODOs:

(1) The main issue I see above is voting vs. consensus.  I have little
preference here.  It sounds like something which could be tailored based on
whether we see too many or too few SIPs being approved.

(2) Design doc template  (This would be great to have for Spark regardless
of this SIP discussion.)
* Reynold, are you still putting this together?

(3) Template cleanups.  Listing some items mentioned above + a new one
w.r.t. Reynold's draft
<https://docs.google.com/document/d/1-Zdi_W-wtuxS9hTK0P9qb2x-nRanvXmnZ7SUi4qMljg/edit#>
:
* Reinstate the ""Where"" section with links to current and past SIPs
* Add field for stating explicit deadlines for approval
* Add field for stating Author & Committer shepherd

Thanks all!
Joseph


we
d
st
e
n
to
n,
s)
ue
is
us
e
om
st
m
e
y
is
re
s
h
ll
on
ds
re
s
k
om
d
e
nd
er
e
en
d
.
o
ng
k
g
o


-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Imran Rashid <irashid@cloudera.com>,"Tue, 3 Jan 2017 16:08:01 -0600",Re: Spark Improvement Proposals,Joseph Bradley <joseph@databricks.com>,"I'm also in favor of this.  Thanks for your persistence Cody.

My take on the specific issues Joseph mentioned:

1) voting vs. consensus -- I agree with the argument Ryan Blue made earlier
for consensus:

consider a proposal approved if it had objections serious enough that
committers down-voted (or PMC depending on who gets a vote). If these
proposals are like PEPs, then they represent a significant amount of
community effort and I wouldn't want to move forward if up to half of the
community thinks it's an untenable idea.

2) Design doc template -- agree this would be useful, but also seems
totally orthogonal to moving forward on the SIP proposal.

3) agree w/ Joseph's proposal for updating the template.


4) Deciding on a name -- minor, but I think its wroth disambiguating from
Scala's SIPs, and the best proposal I've heard is ""SPIP"".   At least, no
one has objected.  (don't care enough that I'd object to anything else,
though.)



re:
on
s
i4qMljg/edit#>
:
.
od
k
e
t
on
o
l
d
e
be
ry
i
is
o
th
y
f
)
g
n
ms
nk
e
nd
ne
s
e
t
re
h.
to
k
f
d
rk
f
ng
to
n
"
dstuck <david.e.stuck@gmail.com>,"Tue, 3 Jan 2017 16:15:16 -0700 (MST)",DataFrame Distinct Sample Bug?,dev@spark.apache.org,"I ran into an issue where I'm getting unstable results after sampling a
dataframe that has had the distinct function called on it. The following
code should print different answers each time.

from pyspark.sql import functions as F
d = sqlContext.createDataFrame(sc.parallelize([[x] for x in range(100000)]),
['t'])
sampled = d.distinct().sample(False, 0.01, 478)
print sampled.select(F.min('t').alias('t')).collect()
print sampled.select(F.min('t').alias('t')).collect()
print sampled.select(F.min('t').alias('t')).collect()

Removing distinct and caching after sampling fix the problem (as does using
a smaller dataframe). The spark bug reporting docs dissuaded me from
creating a JIRA issue without checking with this mailing list that this is
reproducible.

I'm not familiar enough with the spark code to fix this :\



--

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Tue, 3 Jan 2017 17:16:03 -0600",Re: Spark Improvement Proposals,Imran Rashid <irashid@cloudera.com>,"I don't have a concern about voting vs consensus.

I have a concern that whatever the decision making process is, it is
explicitly announced on the ticket for the given proposal, with an explicit
deadline, and an explicit outcome.



are:
 on
Ui4qMljg/edit#>
.
t
s
t
nk
ke
e
st
t
s
do
t
al
d
.
o
ed
he
d
k
s
of
s)
on
d
we
k
r
re
w
,
rk
y
of
k
y
of
t
f
m
-
"
Michael Armbrust <michael@databricks.com>,"Tue, 3 Jan 2017 15:36:44 -0800",Re: StateStoreSaveExec / StateStoreRestoreExec,Jeremy Smith <jeremy.smith@acorns.com>,"I think we should add something similar to mapWithState in 2.2.  It would
be great if you could add the description of your problem to this ticket:
https://issues.apache.org/jira/browse/SPARK-19067


"
Michael Armbrust <michael@databricks.com>,"Tue, 3 Jan 2017 15:39:23 -0800",Re: StateStoreSaveExec / StateStoreRestoreExec,Jeremy Smith <jeremy.smith@acorns.com>,"You might also be interested in this:
https://issues.apache.org/jira/browse/SPARK-19031


"
khyati.shah <khyati.shah@guavus.com>,"Wed, 4 Jan 2017 06:22:33 +0530",Re: Skip Corrupted Parquet blocks / footer.,Ryan Blue <rblue@netflix.com>,"Yes! Using spark 2.1 . I hope i am using right syntax for setting up conf.
sqlContext.setConf(""spark.sql.files.ignoreCorruptFiles"",""true"") /

sqlContext.sql(""set spark.sql.files.ignoreCorruptFiles=true"")
Sent from my Samsung Galaxy smartphone.
-------- Original message --------From: Ryan Blue <rblue@netflix.com> Date: 04/01/2017  1:43 a.m.  (GMT+05:30) To: khyati <khyati.shah@guavus.com> Cc: Spark Dev List <dev@spark.apache.org> Subject: Re: Skip Corrupted Parquet blocks / footer. 
Khyati,
Are you using Spark 2.1? The usual entry point for Spark 2.x is spark rather than sqlContext.
rb
​
On Tue, Jan 3, 2017 at 11:03 AM, khyati <khyati.shah@guavus.com> wrote:
Hi Reynold Xin,



I tried setting spark.sql.files.ignoreCorruptFiles = true by using commands,



val sqlContext =new org.apache.spark.sql.hive.HiveContext(sc)



sqlContext.setConf(""spark.sql.files.ignoreCorruptFiles"",""true"") /

sqlContext.sql(""set spark.sql.files.ignoreCorruptFiles=true"")



but still getting error while reading parquet files using

val newDataDF =

sqlContext.read.parquet(""/data/tempparquetdata/corruptblock.0"",""/data/tempparquetdata/data1.parquet"")



Error: ERROR executor.Executor: Exception in task 0.0 in stage 4.0 (TID 4)

java.io.IOException: Could not read footer: java.lang.RuntimeException:

hdfs://192.168.1.53:9000/data/tempparquetdata/corruptblock.0 is not a

Parquet file. expected magic number at tail [80, 65, 82, 49] but found [65,

82, 49, 10]

        at

org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:248)





Please let me know if I am missing anything.









--

View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Skip-Corrupted-Parquet-blocks-footer-tp20418p20433.html

Sent from the Apache Spark Developers List mailing list archive at Nabble.com.



---------------------------------------------------------------------

To unsubscribe e-mail: dev-unsubscribe@spark.apache.org






-- 
Ryan BlueSoftware EngineerNetflix

"
khyati <khyati.shah@guavus.com>,"Tue, 3 Jan 2017 18:06:20 -0700 (MST)",Re: Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"Yes! Using spark 2.1.0 . I hope the command used to set the conf is correct.

sqlContext.setConf(""spark.sql.files.ignoreCorruptFiles"",""true"") /
sqlContext.sql(""set spark.sql.files.ignoreCorruptFiles=true"")





--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 4 Jan 2017 10:34:33 +0900",Re: DataFrame Distinct Sample Bug?,dstuck <david.e.stuck@gmail.com>,"I get the same result every time on Spark 2.1:


Using Python version 2.7.12 (default, Jul  2 2016 17:43:17)
SparkSession available as 'spark'.
range(100000)]),
... ['t'])
[Row(t=4)]

[Row(t=4)]
[Row(t=4)]



"
khyati <khyati.shah@guavus.com>,"Tue, 3 Jan 2017 19:46:07 -0700 (MST)",Re: Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"Just attaching the screenshot to make it more clear..

<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n20446/Screen_Shot_2017-01-04_at_8.png> 

Thanks.. 



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 3 Jan 2017 18:49:11 -0800",Re: Tests failing with GC limit exceeded,Kay Ousterhout <keo@eecs.berkeley.edu>,"nope, no changes to jenkins in the past few months.  ganglia graphs
show higher, but not worrying, memory usage on the workers when the
jobs failed...

i'll take a closer look later tonite/first thing tomorrow morning.

shane


---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 4 Jan 2017 00:21:26 -0700 (MST)","Re: What is mainly different from a UDT and a spark internal type
 that ExpressionEncoder recognized?",dev@spark.apache.org,"
Actually, I think UDTs can directly translates an object into Spark's
internal format by ScalaReflection and encoder, without the intermediate
generic row. You can directly create a dataset of the objects of UDT.

If you don't convert the dataset to a dataframe, I think RowEncoder won't
step in.



Michael Armbrust wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Asher Krim <akrim@hubspot.com>,"Wed, 4 Jan 2017 09:58:22 +0200",Re: ml word2vec finSynonyms return type,Felix Cheung <felixcheung_m@hotmail.com>,"The jira: https://issues.apache.org/jira/browse/SPARK-17629

Adding new methods could result in method clutter. Changing behavior of
non-experimental classes is unfortunate (ml Word2Vec was marked
Experimental until Spark 2.0). Neither option is great. If I had to pick, I
would rather change the existing methods to keep the class simpler moving
forward.



"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 4 Jan 2017 01:06:43 -0700 (MST)",Re: Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"
Hi,

The method readAllFootersInParallel is implemented in Parquet's
ParquetFileReader. So the spark config ""spark.sql.files.ignoreCorruptFiles""
doesn't work for it.

Reading all footers in parallel can speed up the task. However, we can't
control if ignoring corrupt files or not.

Of course we can read this footers in sequence and ignore the corrupt ones.
But it might be inefficient. Since this is a relatively corner use case, I
don't expect we can have this.

Maybe Parquet can implement an option to ignore corrupt files. However, even
so, it can't be expected to have this updated Parquet implementation
available to Spark very soon.



khyati wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 4 Jan 2017 01:12:18 -0700 (MST)",Re: Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"

Forget to say, another option is we can replace readAllFootersInParallel
with our parallel reading logic, so we can ignore corrupt files.


Liang-Chi Hsieh wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 4 Jan 2017 10:12:56 +0100",Re: Why ShuffleMapTask has transient locs and preferredLocs?!,Imran Rashid <irashid@cloudera.com>,"Hi Imran,

Yes, you're right. I stand corrected! Thanks.

This is the part that opened my eyes:


That's why a task does not have to have it after deserialization (!)
Thanks a lot.


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 4 Jan 2017 15:05:24 +0530",Re: Apache Hive with Spark Configuration,Ryan Blue <rblue@netflix.com>,"Ryan,

I agree that Hive 1.2.1 work reliably with Spark 2.x , but i went through
with current stable version of Hive which is 2.0.1 and I am working with
that. seems good but i want to make sure the which version of Hive is more
reliable with Spark 2.x and i think @Ryan you replied the same which is
hive 1.2.1 .

Thanks.




"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 4 Jan 2017 17:04:48 +0530",Re: Dependency Injection and Microservice development with Spark,Lars Albertsson <lalle@mapflat.com>,"Lars,

Thank you, I want to use DI for configuring all the properties (wiring) for
below architectural approach.

Oracle -> Kafka Batch (Event Queuing) -> Spark Jobs( Incremental load from
HBase -> Hive with Transformation) -> Spark Transformation -> PostgreSQL

Thanks.


"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 4 Jan 2017 17:07:45 +0530",Re: Approach: Incremental data load from HBASE,Ted Yu <yuzhihong@gmail.com>,"Ted Yu,

You understood wrong, i said Incremental load from HBase to Hive,
individually you can say Incremental Import from HBase.


"
=?UTF-8?B?SmnFmcOtIFN5cm92w70=?= <syrovy.jiri@gmail.com>,"Wed, 4 Jan 2017 13:06:17 +0100",Re: Dependency Injection and Microservice development with Spark,Lars Albertsson <lalle@mapflat.com>,"Hi,

another nice approach is to use instead of it Reader monad and some
framework to support this approach (e.g. Grafter -
https://github.com/zalando/grafter). It's lightweight and helps a bit with
dependencies issues.

2016-12-28 22:55 GMT+01:00 Lars Albertsson <lalle@mapflat.com>:

"
Sean Owen <sowen@cloudera.com>,"Wed, 04 Jan 2017 12:35:48 +0000","Quick request: prolific PR openers, review your open PRs",dev <dev@spark.apache.org>,"Just saw that there are many people with >= 8 open PRs. Some are
legitimately in flight but many are probably stale. To set a good example,
would (everyone) mind flicking through what they've got open and see if
some PRs are stale and should be closed?

https://spark-prs.appspot.com/users

UsernameOpen PRs ▴
viirya <https://spark-prs.appspot.com/users/viirya> 13
hhbyyh <https://spark-prs.appspot.com/users/hhbyyh> 12
zhengruifeng <https://spark-prs.appspot.com/users/zhengruifeng> 12
HyukjinKwon <https://spark-prs.appspot.com/users/HyukjinKwon> 12
maropu <https://spark-prs.appspot.com/users/maropu> 10
kiszk <https://spark-prs.appspot.com/users/kiszk> 10
yanboliang <https://spark-prs.appspot.com/users/yanboliang> 10
cloud-fan <https://spark-prs.appspot.com/users/cloud-fan> 8
jerryshao <https://spark-prs.appspot.com/users/jerryshao> 8
"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 4 Jan 2017 05:57:01 -0700 (MST)","Re: Quick request: prolific PR openers, review your open PRs",dev@spark.apache.org,"
Ok. I will go through and check my open PRs.


Sean Owen wrote
,





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--
3.nabble.com/Quick-request-prolific-PR-openers-review-your-open-PRs-tp20457p20458.html
om.

---------------------------------------------------------------------


"
geoHeil <georg.kf.heiler@gmail.com>,"Wed, 4 Jan 2017 08:19:20 -0700 (MST)",Clarification about typesafe aggregations,dev@spark.apache.org,"Hi I would like to know more about typeface aggregations in spark.

http://stackoverflow.com/questions/40596638/inquiries-about-spark-2-0-dataset/40602882?noredirect=1#comment70139481_40602882
An example of these is
https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/
ds.groupByKey(body => body.color)

does
""myDataSet.map(foo.someVal) is type safe but as any Dataset operation uses
RDD and compared to DataFrame operations there is a significant overhead.
Let's take a look at a simple example:""
hold true e.g. will type safe aggregation require the deserialisation of the
full objects as displayed for 
ds.map(_.foo).explain ?

Kind regards,
Georg



--

---------------------------------------------------------------------


"
Andy Dang <namd88@gmail.com>,"Wed, 4 Jan 2017 19:27:15 +0000",Converting an InternalRow to a Row,"user <user@spark.apache.org>, dev@spark.apache.org","Hi all,
(cc-ing dev since I've hit some developer API corner)

What's the best way to convert an InternalRow to a Row if I've got an
InternalRow and the corresponding Schema.

Code snippet:
    @Test
    public void foo() throws Exception {
        Row row = RowFactory.create(1);
        StructType struct = new StructType().add(""id"",
DataTypes.IntegerType);
        ExpressionEncoder<Row> enconder = RowEncoder.apply(struct);
        InternalRow internalRow = enconder.toRow(row);
        System.out.println(""Internal row size: "" + internalRow.numFields());
        Row roundTrip = enconder.fromRow(internalRow);
        System.out.println(""Round trip: "" + roundTrip.size());
    }

The code fails at the line encoder.fromRow() with the exception:
expression: getcolumnbyordinal(0, IntegerType)

-------
Regards,
Andy
"
shane knapp <sknapp@berkeley.edu>,"Wed, 4 Jan 2017 13:22:09 -0800",Re: Tests failing with GC limit exceeded,Kay Ousterhout <keo@eecs.berkeley.edu>,"preliminary findings:  seems to be transient, and affecting 4% of
builds from late december until now (which is as far back as we keep
build records for the PRB builds).

 408 builds
  16 builds.gc   <--- failures

it's also happening across all workers at about the same rate.

and best of all, there seems to be no pattern to which tests are
failing (different each time).  i'll look a little deeper and decide
what to do next.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 4 Jan 2017 14:36:55 -0800",Re: Clarification about typesafe aggregations,geoHeil <georg.kf.heiler@gmail.com>,"Your understanding is correct - it is indeed slower due to extra
serialization. In some cases we can get rid of the serialization if the
value is already deserialized.



"
geoHeil <georg.kf.heiler@gmail.com>,"Wed, 4 Jan 2017 16:02:13 -0700 (MST)",Re: Clarification about typesafe aggregations,dev@spark.apache.org,"Thanks for the clarification.
rxin [via Apache Spark Developers List] <
ml-node+s1001551n20462h71@n3.nabble.com> schrieb am Mi. 4. Jan. 2017 um
23:37:





--"
Hyukjin Kwon <gurwls223@gmail.com>,"Thu, 5 Jan 2017 09:35:33 +0900","Re: Quick request: prolific PR openers, review your open PRs",Liang-Chi Hsieh <viirya@gmail.com>,"Let me double-check mind too.

2017-01-04 21:57 GMT+09:00 Liang-Chi Hsieh <viirya@gmail.com>:

2
"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 4 Jan 2017 19:53:09 -0700 (MST)",Re: Converting an InternalRow to a Row,dev@spark.apache.org,"
You need to resolve and bind the encoder.

ExpressionEncoder<Row> enconder = RowEncoder.apply(struct).resolveAndBind();


Andy Dang wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 4 Jan 2017 22:11:29 -0700 (MST)",Re: Skip Corrupted Parquet blocks / footer.,dev@spark.apache.org,"
After checking the codes, I think there are few issues regarding this
ignoreCorruptFiles config, so you can't actually use it with Parquet files
now.

I opened a JIRA https://issues.apache.org/jira/browse/SPARK-19082 and also
submitted a PR for it.


khyati wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
ayan guha <guha.ayan@gmail.com>,"Thu, 5 Jan 2017 07:32:12 +1100",Re: Approach: Incremental data load from HBASE,Chetan Khatri <chetan.opensource@gmail.com>,"Hi Chetan

What do you mean by incremental load from HBase? There is a timestamp
marker for each cell, but not at Row level.




-- 
Best Regards,
Ayan Guha
"
darren <darren@ontrenet.com>,"Wed, 04 Jan 2017 10:08:12 -0500",Re: Dependency Injection and Microservice development with Spark,"Chetan Khatri <chetan.opensource@gmail.com>, Lars Albertsson
 <lalle@mapflat.com>","We've been able to use ipopo dependency injection framework in our pyspark system and deploy .egg pyspark apps that resolve and wire up all the components (like a kernel architecture. Also similar to spring) during an initial bootstrap sequence; then invoke those components across spark.
Just replying for info since it's not identical to your request but in the same spirit.
Darren


Sent from my Verizon, Samsung Galaxy smartphone
-------- Original message --------From: Chetan Khatri <chetan.opensource@gmail.com> Date: 1/4/17  6:34 AM  (GMT-05:00) To: Lars Albertsson <lalle@mapflat.com> Cc: user <user@spark.apache.org>, Spark Dev List <dev@spark.apache.org> Subject: Re: Dependency Injection and Microservice development with Spark 
Lars,
Thank you, I want to use DI for configuring all the properties (wiring) for below architectural approach.
Oracle -> Kafka Batch (Event Queuing) -> Spark Jobs( Incremental load from HBase -> Hive with Transformation) -> Spark Transformation -> PostgreSQL
Thanks.
On Thu, Dec 29, 2016 at 3:25 AM, Lars Albertsson <lalle@mapflat.com> wrote:
Do you really need dependency injection?



DI is often used for testing purposes. Data processing jobs are easy

to test without DI, however, due to their functional and synchronous

nature. Hence, DI is often unnecessary for testing data processing

jobs, whether they are batch or streaming jobs.



Or do you want to use DI for other reasons?





Lars Albertsson

Data engineering consultant

www.mapflat.com

https://twitter.com/lalleal

+46 70 7687109

Calendar: https://goo.gl/6FBtlS, https://freebusy.io/lalle@mapflat.com





On Fri, Dec 23, 2016 at 11:56 AM, Chetan Khatri

<chetan.opensource@gmail.com> wrote:

> Hello Community,

>

> Current approach I am using for Spark Job Development with Scala + SBT and

> Uber Jar with yml properties file to pass configuration parameters. But If i

> would like to use Dependency Injection and MicroService Development like

> Spring Boot feature in Scala then what would be the standard approach.

>

> Thanks

>

> Chetan



"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Tue, 3 Jan 2017 16:35:38 -0800",Tests failing with GC limit exceeded,"""dev@spark.apache.org"" <dev@spark.apache.org>, shane knapp <sknapp@berkeley.edu>","I've noticed a bunch of the recent builds failing because of GC limits, for
seemingly unrelated changes (e.g. 70818, 70840, 70842).  Shane, have there
been any recent changes in the build configuration that might be causing
this?  Does anyone else have any ideas about what's going on here?

-Kay
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Tue, 3 Jan 2017 16:15:25 -0800",Re: Why is spark.shuffle.sort.bypassMergeThreshold 200?,Liang-Chi Hsieh <viirya@gmail.com>,"I believe that these two were indeed originally related.  In the old
hash-based shuffle, we wrote objects out immediately to disk as they were
version of the new sort-based shuffle, Spark buffered a bunch of objects
before writing them out to disk.  My vague memory is that this caused
issues for Spark SQL -- I think because SQL got a performance improvement
from re-using the same objects when generating data from the iterator (but
if it re-used objects, the sort-based shuffle didn't work, because all of
the buffered objects would incorrectly point to the same underlying
object).  So, the default configuration was 200 so that SQL wouldn't use
the sort-based shuffle.  My memory is that the issues around this have
since been fixed but Michael / Reynold / Andrew Or probably have a better
memory of this.

-Kay


"
Dino <dino@spam4.me>,"Mon, 2 Jan 2017 04:49:27 -0700 (MST)","Cannot pass broker list parameter from Scala to Kafka: Property
 bootstrap.servers is not valid",dev@spark.apache.org,"I have spent a lot of time trying to figure out the following problem. I need
to consume messages from the topic of remote Kafka queue using Scala and
Spark. By default the port of Kafka on remote machine is set to `7072`, not
`9092`. Also, on remote machine there are the following versions installed:

 1. Kafka 0.10.1.0  
 2. Scala 2.11

It means that I should pass the broker list (with the port `7072`) from
Scala to remote Kafka, because otherwise it will try to use the default
port.

The problem is that according to logs the parameter `bootstrap.servers`
cannot be recognized by the remote machine. I also tried to rename this
parameter to `metadata.broker.list`, `broker.list` and `listeners`, but all
the time the same error appears in logs, e.g. `Property bootstrap.servers is
not valid` and then the port `9092` is used by default (and the messages are
obviously not consumed).

Using telnet, I check that I have an access to remote Kafka from the EMR
machine on Amazon Cloud. I also checked that the name of Kafka topic is
correct (I can consume messages from terminal using `curl` and Rest API, but
just everything fails in Scala).

In POM file I use the following dependency for Kafka and Spark:

    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming_2.10</artifactId>
        <version>1.6.2</version>
    </dependency>

    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming-kafka_2.10</artifactId>
        <version>1.6.2</version>
    </dependency>

I use Scala 2.10, not 2.11.

This is my Scala code for Kafka consumer (*it works absolutely fine if I use
my own Kafka installed in Amazon Cloud where I have EMR machines (there I
have the port `9092` used for Kafka)*):

        val testTopicMap = testTopic.split("","").map((_,
kafkaNumThreads.toInt)).toMap
    
       val kafkaParams = Map[String, String](
          ""broker.list"" -> ""XXX.XX.XXX.XX:7072"",
          ""zookeeper.connect"" -> ""XXX.XX.XXX.XX:2181"",
          ""group.id"" -> ""test"",
          ""zookeeper.connection.timeout.ms"" -> ""10000"",
          ""auto.offset.reset"" -> ""smallest"")
    
        val testEvents: DStream[String] =
          KafkaUtils
            .createStream[String, String, StringDecoder, StringDecoder](
            ssc,
            kafkaParams,
            testTopicMap,
            StorageLevel.MEMORY_AND_DISK_SER_2
          ).map(_._2)

I was reading this Documentation
(/https://kafka.apache.org/documentation/#brokerconfigs/) but it looks like
everything I did is correct. Should I use some other Kafka client API (other
Maven dependency)?

*UPDATE #1:*

I also tried Direct Stream (without Zookeeper), but it runs into the error:

    val testTopicMap = testTopic.split("","").toSet
    val kafkaParams = Map[String, String](""metadata.broker.list"" ->
""XXX.XX.XXX.XX:7072,XXX.XX.XXX.XX:7072,XXX.XX.XXX.XX:7072"", 
""bootstrap.servers"" ->
""XXX.XX.XXX.XX:7072,XXX.XX.XXX.XX:7072,XXX.XX.XXX.XX:7072"",
                                          ""auto.offset.reset"" -> ""smallest"")
    val testEvents = KafkaUtils.createDirectStream[String, String,
StringDecoder, StringDecoder](ssc, kafkaParams, testTopicMap).map(_._2)
    
    testEvents.print()

    17/01/02 12:23:15 ERROR ApplicationMaster: User class threw exception:
org.apache.spark.SparkException: java.io.EOFException: Received -1 when
reading from channel, socket has likely been closed.
    java.io.EOFException: Received -1 when reading from channel, socket has
likely been closed.
    java.io.EOFException: Received -1 when reading from channel, socket has
likely been closed.


*UPDATE #2:*

Some of suggested solutions was to set `the property 'advertised.host.name'
as instructed by the comments in the kafka configuration
(config/server.properties)`. Do I understand correctly that
`config/server.properties` should be changed on the remote machine where
Kafka is installed?



ANY SUGGESTION WILL BE REALLY HIGHLY APPRECIATED.



--

---------------------------------------------------------------------


"
Andy Dang <namd88@gmail.com>,"Thu, 5 Jan 2017 10:55:07 +0000",Re: Converting an InternalRow to a Row,Liang-Chi Hsieh <viirya@gmail.com>,"Perfect. The API in Java is bit clumsy though

What I ended up doing in Java (the val is from lombok, if anyone's
wondering):
        val attributes =
JavaConversions.asJavaCollection(schema.toAttributes()).stream().map(Attribute::toAttribute).collect(Collectors.toList());
        val encoder =
RowEncoder.apply(schema).resolveAndBind(ScalaUtils.scalaSeq(attributes),
SimpleAnalyzer$.MODULE$);


-------
Regards,
Andy


"
Vincent Frochot <vincentfrochot@gmail.com>,"Thu, 5 Jan 2017 11:59:29 +0100",unsubscribe,"user@spark.apache.org, dev@spark.apache.org","Vincent
"
Andy Dang <namd88@gmail.com>,"Thu, 5 Jan 2017 11:58:06 +0000",Re: Converting an InternalRow to a Row,Liang-Chi Hsieh <viirya@gmail.com>,"
I have a UDAF that has ExpressionEncoder<Row> as a member variable.

However, if call resolveAndBind() eagerly on this encoder, it appears to
break the UDAF. Bascially somehow the deserialized row are all the same
during the merge step. Is it the expected behavior of Encoders?

-------
Regards,
Andy


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 05 Jan 2017 20:01:45 +0000","Re: Spark SQL - Applying transformation on a struct inside an
 array",Michael Armbrust <michael@databricks.com>,"So, it seems the only way I found for now is a recursive handling of the Row
instances directly, but to do that I have to go back to RDDs, i've put together
a simple test case demonstrating the problem :
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.scalatest.{FlatSpec, Matchers}

class extends with DFInPlaceTransform FlatSpec Matchers {
val spark = SparkSession.builder().appName(""local""""local[*]""
).master().getOrCreate()
it should ""access and mutate deeply nested arrays/structs"" in {

val df = spark.read.json(spark.sparkContext.parallelize(List(
""""""{""a"":[{""b"" : ""toto"" }]}"""""".stripMargin)))
df.show()
df.printSchema()

val result = transformInPlace(""a.b"", df)

result.printSchema()
result.show()

result.schema should be (df.schema)
val res = result.toJSON.take(1)
res should be(""""""{""a"":[{""b"" : TOTO"" }]}"""""")
}

def transformInPlace(path: String, df: DataFrame): DataFrame = {
val udf = spark.udf.register(""transform"", (s: String) => s.toUpperCase)
val paths = path.split('.')
val root = paths.head
import org.apache.spark.sql.functions._
df.withColumn(root, udf(df(path))) // does not work of course
}
}

the three other solutions I see are * to create a dedicated Expression for
   in-place modifications of nested arrays and structs,
 * to use heavy explode/lateral views/group
   by computations, but that's bound to be inefficient
 * or to generate bytecode using the schema
   to do the nested ""getRow,getSeq…"" and re-create the rows once transformation
   is applied

I'd like to open an issue regarding that use case because it's not the first or
last time it comes up and I still don't see any generic solution using
Dataframes.Thanks for your time,Regards,
Olivier
 





girardot@lateral-thoughts.com
Hi michael,Well for nested structs, I saw in the tests the behaviour defined by
SPARK-12512 for the ""a.b.c"" handling in withColumn, and even if it's not ideal
for me, I managed to make it work anyway like that :> df.withColumn(""a"",
struct(struct(myUDF(df(""a.b.c."")))) // I didn't put back the aliases but you see
what I mean.
What I'd like to make work in essence is something like that> val someFunc :
String => String = ???> val myUDF = udf(someFunc)> df.withColumn(""a.b[*].c"",
myUDF(df(""a.b[*].c""))) // the fact is that in order to be consistent with the
previous API, maybe I'd have to put something like a struct(array(struct(… which
would be troublesome because I'd have to parse the arbitrary input string  and
create something like ""a.b[*].c"" => struct(array(struct(
I realise the ambiguity implied in the kind of column expression, but it doesn't
seem for now available to cleanly update data inplace at an arbitrary depth.
I'll try to work on a PR that would make this possible, but any pointers would
be appreciated.
Regards,
Olivier.
 





Is what you are looking for a withColumn that support in place modification of
nested columns? or is it some other problem?
I tried to use the RowEncoder but got stuck along the way :The main issue really
is that even if it's possible (however tedious) to pattern match generically
Row(s) and target the nested field that you need to modify, Rows being immutable
data structure without a method like a case class's copy or any kind of lens to
create a brand new object, I ended up stuck at the step ""target and extract the
field to update"" without any way to update the original Row with the new value.
To sum up, I tried : * using only dataframe's API itself + my udf - which works
   for nested structs as long as no arrays are along the way
 * trying to create a udf the can apply on Row and pattern
   match recursively the path I needed to explore/modify
 * trying to create a UDT - but we seem to be stuck in a
   strange middle-ground with 2.0 because some parts of the API ended up private
   while some stayed public making it impossible to use it now (I'd be glad if
   I'm mistaken)

All of these failed for me and I ended up converting the rows to JSON and update
using JSONPath which is…. something I'd like to avoid 'pretty please' 





Hi Guys,
Have you tried org.apache.spark.sql.catalyst.encoders.RowEncoder? It's not a
public API, but it is publicly accessible. I used it recently to correct some
bad data in a few nested columns in a dataframe. It wasn't an easy job, but it
made it possible. In my particular case I was not working with arrays.
Olivier, I'm interested in seeing what you come up with.
Thanks,
Michael

+1 to this request. I talked last week with a product group within IBM that is
struggling with the same issue. It's pretty common in data cleaning applications
for data in the early stages to have nested lists or sets inconsistent or
incomplete schema information.
Fred
Hi everyone,I'm currently trying to create a generic transformation mecanism on
a Dataframe to modify an arbitrary column regardless of the underlying the
schema.
It's ""relatively"" straightforward for complex types like struct<struct<…>> to
apply an arbitrary UDF on the column and replace the data ""inside"" the struct,
however I'm struggling to make it work for complex types containing arrays along
the way like struct<array<struct<…>>>.
Michael Armbrust seemed to allude on the mailing list/forum to a way of using
Encoders to do that, I'd be interested in any pointers, especially considering
that it's not possible to output any Row or GenericRowWithSchema from a UDF
(thanks to https://github.com/apache/spark/blob/v2.0.0/sql/catalyst/
src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L657  it
seems).
To sum up, I'd like to find a way to apply a transformation on complex nested
datatypes (arrays and struct) on a Dataframe updating the value itself.
Regards,
Olivier Girardot

 



Olivier Girardot| Associé
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
 


Olivier Girardot| Associé
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94


 

Olivier Girardot| Associé
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
Felix Cheung <felixcheung_m@hotmail.com>,"Thu, 5 Jan 2017 20:44:46 +0000",Re: ml word2vec finSynonyms return type,Asher Krim <akrim@hubspot.com>,"Given how Word2Vec is used the pipeline model in the new ml implementation, we might need to keep the current behavior?


https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala


_____________________________
From: Asher Krim <akrim@hubspot.com<mailto:akrim@hubspot.com>>
Sent: Tuesday, January 3, 2017 11:58 PM
Subject: Re: ml word2vec finSynonyms return type
To: Felix Cheung <felixcheung_m@hotmail.com<mailto:felixcheung_m@hotmail.com>>
Cc: <manojkumarsivaraj334@gmail.com<mailto:manojkumarsivaraj334@gmail.com>>, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>>, <dev@spark.apache.org<mailto:dev@spark.apache.org>>


The jira: https://issues.apache.org/jira/browse/SPARK-17629

Adding new methods could result in method clutter. Changing behavior of non-experimental classes is unfortunate (ml Word2Vec was marked Experimental until Spark 2.0). Neither option is great. If I had to pick, I would rather change the existing methods to keep the class simpler moving forward.


Could you link to the JIRA here?

What you suggest makes sense to me. Though we might want to maintain compatibility and add a new method instead of changing the return type of the existing one.


_____________________________
From: Asher Krim <akrim@hubspot.com<mailto:akrim@hubspot.com>>
Sent: Wednesday, December 28, 2016 11:52 AM
Subject: ml word2vec finSynonyms return type
To: <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Cc: <manojkumarsivaraj334@gmail.com<mailto:manojkumarsivaraj334@gmail.com>>, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>>



Hey all,

I would like to propose changing the return type of `findSynonyms` in ml's Word2Vec<https://github.com/apache/spark/blob/branch-2.1/mllib/src/main/scala/org/apache/spark/ml/feature/Word2Vec.scala#L233-L248>:

def findSynonyms(word: String, num: Int): DataFrame = {
  val spark = SparkSession.builder().getOrCreate()
  spark.createDataFrame(wordVectors.findSynonyms(word, num)).toDF(""word"", ""similarity"")
}


I find it very strange that the results are parallelized before being returned to the user. The results are already on the driver to begin with, and I can imagine that for most usecases (and definitely for ours) the synonyms are collected right back to the driver. This incurs both an added cost of shipping data to and from the cluster, as well as a more cumbersome interface than needed.

Can we change it to just the following?

def findSynonyms(word: String, num: Int): Array[(String, Double)] = {
  wordVectors.findSynonyms(word, num)
}

If the user wants the results parallelized, they can still do so on their own.

(I had brought this up a while back in Jira. It was suggested that the mailing list would be a better forum to discuss it, so here we are.)

Thanks,
--
Asher Krim
Senior Software Engineer
[http://cdn2.hubspot.net/hub/137828/file-223457316-png/HubSpot_User_Group_Images/HUG_lrg_HS.png?t=1477096082917]



"
shane knapp <sknapp@berkeley.edu>,"Thu, 5 Jan 2017 13:19:33 -0800",Re: Tests failing with GC limit exceeded,Kay Ousterhout <keo@eecs.berkeley.edu>,"as of first thing this morning, here's the list of recent GC overhead
build failures:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70891/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70874/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70842/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70927/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70551/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70835/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70841/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70869/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70598/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70898/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70629/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70644/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70686/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70620/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70871/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70873/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70622/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70837/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70626/console

i haven't really found anything that jumps out at me except perhaps
auditing/upping the java memory limits across the build.  this seems
to be a massive shot in the dark, and time consuming, so let's just
call this a ""method of last resort"".

looking more closely at the systems themselves, it looked to me that
there was enough java ""garbage"" that had accumulated over the last 5
months (since the last reboot) that system reboots would be a good
first step.

https://www.youtube.com/watch?v=nn2FB1P_Mn8

over the course of this morning i've been sneaking in worker reboots
during quiet times...  the ganglia memory graphs look a lot better
(free memory up, cached memory down!), and i'll keep an eye on things
over the course of the next few days to see if the build failure
frequency is effected.

also, i might be scheduling quarterly system reboots if this indeed
fixes the problem.

shane


---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Thu, 5 Jan 2017 14:33:28 -0800",Re: ml word2vec finSynonyms return type,Felix Cheung <felixcheung_m@hotmail.com>,"We returned a DataFrame since it is a nicer API, but I agree forcing RDD
operations is not ideal.  I'd be OK with adding a new method, but I agree
with Felix that we cannot break the API for something like this.




-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Tim Hunter <timhunter@databricks.com>,"Thu, 5 Jan 2017 15:29:44 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Hi Cody,
thank you for bringing up this topic, I agree it is very important to keep
a cohesive community around some common, fluid goals. Here are a few
comments about the current document:

1. name: it should not overlap with an existing one such as SIP. Can you
imagine someone trying to discuss a scala spore proposal for spark?
""[Spark] SIP-3 is intended to evolve in tandem with [Scala] SIP-21"". SPIP
sounds great.

2. roles: at a high level, SPIPs are meant to reach consensus for technical
decisions with a lasting impact. As such, the template should emphasize the
role of the various parties during this process:

 - the SPIP author is responsible for building consensus. She is the
champion driving the process forward and is responsible for ensuring that
the SPIP follows the general guidelines. The author should be identified in
the SPIP. The authorship of a SPIP can be transferred if the current author
is not interested and someone else wants to move the SPIP forward. There
should probably be 2-3 authors at most for each SPIP.

 - someone with voting power should probably shepherd the SPIP (and be
recorded as such): ensuring that the final decision over the SPIP is
recorded (rejected, accepted, etc.), and advising about the technical
quality of the SPIP: this person need not be a champion for the SPIP or
contribute to it, but rather makes sure it stands a chance of being
approved when the vote happens. Also, if the author cannot find anyone who
would want to take this role, this proposal is likely to be rejected anyway.

 - users, committers, contributors have the roles already outlined in the
document

3. timeline: ideally, once a SPIP has been offered for voting, it should
move swiftly into either being accepted or rejected, so that we do not end
up with a distracting long tail of half-hearted proposals.

These rules are meant to be flexible, but the current document should be
clear about who is in charge of a SPIP, and the state it is currently in.

We have had long discussions over some very important questions such as
approval. I do not have an opinion on these, but why not make a pick and
reevaluate this decision later? This is not a binding process at this point.

Tim



it
:
e
m
 are:
d on
SUi4qMljg/edit#>
p.
.
es
o
.
n
m
is
y
nd
o
to
a
o
r
t
k
s
ed
rk
or
s
k
a
h
ow
g,
e
rk
,
ay
o
at
e
'm
"
Ankur Srivastava <ankur.srivastava@gmail.com>,"Thu, 5 Jan 2017 15:45:59 -0800",Re: Spark GraphFrame ConnectedComponents,"Felix Cheung <felixcheung_m@hotmail.com>, dev@spark.apache.org","Adding DEV mailing list to see if this is a defect with ConnectedComponent
or if they can recommend any solution.

Thanks
Ankur


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 5 Jan 2017 15:02:57 -0800",Re: Tests failing with GC limit exceeded,shane knapp <sknapp@berkeley.edu>,"Thanks for looking into this Shane!


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Jan 2017 16:38:49 -0800",Re: Tests failing with GC limit exceeded,shane knapp <sknapp@berkeley.edu>,"Seems like the OOM is coming from tests, which most probably means
it's not an infrastructure issue. Maybe tests just need more memory
these days and we need to update maven / sbt scripts.




-- 
Marcelo

---------------------------------------------------------------------


"
Felix Cheung <felixcheung_m@hotmail.com>,"Fri, 6 Jan 2017 00:57:04 +0000",Re: Spark GraphFrame ConnectedComponents,"Ankur Srivastava <ankur.srivastava@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","This is likely a factor of your hadoop config and Spark rather then anything specific with GraphFrames.

You might have better luck getting assistance if you could isolate the code to a simple case that manifests the problem (without GraphFrames), and repost.


________________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com>
Sent: Thursday, January 5, 2017 3:45:59 PM
To: Felix Cheung; dev@spark.apache.org
Cc: user@spark.apache.org
Subject: Re: Spark GraphFrame ConnectedComponents

Adding DEV mailing list to see if this is a defect with ConnectedComponent or if they can recommend any solution.

Thanks
Ankur

Yes I did try it out and it choses the local file system as my checkpoint location starts with s3n://

I am not sure how can I make it load the S3FileSystem.

Right, I'd agree, it seems to be only with delete.

Could you by chance run just the delete to see if it fails

FileSystem.get(sc.hadoopConfiguration)
.delete(new Path(somepath), true)
________________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Thursday, January 5, 2017 10:05:03 AM
To: Felix Cheung
Cc: user@spark.apache.org<mailto:user@spark.apache.org>

Subject: Re: Spark GraphFrame ConnectedComponents

Yes it works to read the vertices and edges data from S3 location and is also able to write the checkpoint files to S3. It only fails when deleting the data and that is because it tries to use the default file system. I tried looking up how to update the default file system but could not find anything in that regard.

Thanks
Ankur

FileSystem.

Is the URL scheme for s3n registered?
Does it work when you try to read from s3 from Spark?

_____________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Wednesday, January 4, 2017 9:23 PM
Subject: Re: Spark GraphFrame ConnectedComponents
To: Felix Cheung <felixcheung_m@hotmail.com<mailto:felixcheung_m@hotmail.com>>
Cc: <user@spark.apache.org<mailto:user@spark.apache.org>>



This is the exact trace from the driver logs

Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: s3n://<checkpoint-folder>/8ac233e4-10f9-4eb3-aa53-df6d9d7ea7be/connected-components-c1dbc2b0/3, expected: file:///
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)
at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:529)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
at org.apache.hadoop.fs.ChecksumFileSystem.delete(ChecksumFileSystem.java:534)
at org.graphframes.lib.ConnectedComponents$.org$graphframes$lib$ConnectedComponents$$run(ConnectedComponents.scala:340)
at org.graphframes.lib.ConnectedComponents.run(ConnectedComponents.scala:139)
at GraphTest.main(GraphTest.java:31) ----------- Application Class
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

And I am running spark v 1.6.2 and graphframes v 0.3.0-spark1.6-s_2.10

Thanks
Ankur

Hi

I am rerunning the pipeline to generate the exact trace, I have below part of trace from last run:

Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: s3n://<folder-path>, expected: file:///
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)
at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:516)
at org.apache.hadoop.fs.ChecksumFileSystem.delete(ChecksumFileSystem.java:528)

Also I think the error is happening in this part of the code ""ConnectedComponents.scala:339"" I am referring the code @https://github.com/graphframes/graphframes/blob/master/src/main/scala/org/graphframes/lib/ConnectedComponents.scala

      if (shouldCheckpoint && (iteration % checkpointInterval == 0)) {
        // TODO: remove this after DataFrame.checkpoint is implemented
        val out = s""${checkpointDir.get}/$iteration""
        ee.write.parquet(out)
        // may hit S3 eventually consistent issue
        ee = sqlContext.read.parquet(out)

        // remove previous checkpoint
        if (iteration > checkpointInterval) {
          FileSystem.get(sc.hadoopConfiguration)
            .delete(new Path(s""${checkpointDir.get}/${iteration - checkpointInterval}""), true)
        }

        System.gc() // hint Spark to clean shuffle directories
      }


Thanks
Ankur

Do you have more of the exception stack?


________________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Wednesday, January 4, 2017 4:40:02 PM
To: user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Spark GraphFrame ConnectedComponents

Hi,

I am trying to use the ConnectedComponent algorithm of GraphFrames but by default it needs a checkpoint directory. As I am running my spark cluster with S3 as the DFS and do not have access to HDFS file system I tried using a s3 directory as checkpoint directory but I run into below exception:


Exception in thread ""main""java.lang.IllegalArgumentException: Wrong FS: s3n://<folder-path>, expected: file:///

at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)

at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69)

If I set checkpoint interval to -1 to avoid checkpointing the driver just hangs after 3 or 4 iterations.

Is there some way I can set the default FileSystem to S3 for Spark or any other option?

Thanks
Ankur








"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 5 Jan 2017 17:02:47 -0800",Re: Tests failing with GC limit exceeded,Kay Ousterhout <keo@eecs.berkeley.edu>,"
It can always be some memory leak; if we increase the memory settings
and OOMs still happen, that would be a good indication. Also if the
same tests tend to fail (even if unreliably).

Normally some existing code / feature requiring more memory than
before, especially some non-trivial amount, is suspicious. But
sometimes new features / new tests require more memory than configured
in the build scripts, and sometimes blow up.

-- 
Marcelo

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Thu, 5 Jan 2017 17:44:25 -0800",Re: Spark GraphFrame ConnectedComponents,Felix Cheung <felixcheung_m@hotmail.com>,"Would it be more robust to use the Path when creating the FileSystem?
https://github.com/graphframes/graphframes/issues/160




-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 5 Jan 2017 16:58:34 -0800",Re: Tests failing with GC limit exceeded,Marcelo Vanzin <vanzin@cloudera.com>,"But is there any non-memory-leak reason why the tests should need more
memory?  In theory each test should be cleaning up it's own Spark Context
etc. right? My memory is that OOM issues in the tests in the past have been
indicative of memory leaks somewhere.

I do agree that it doesn't seem likely that it's an infrastructure issue /
I can't explain why re-booting would improve things.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 5 Jan 2017 20:48:19 -0700 (MST)",Re: Converting an InternalRow to a Row,dev@spark.apache.org,"
Can you show how you use the encoder in your UDAF?


Andy Dang wrote











-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
"""write2sivakumar@gmail"" <write2sivakumar@gmail.com>","Fri, 06 Jan 2017 12:01:18 +0800",Unsubscribe,dev@spark.apache.org,"
    




Sent from my Samsung device"
shane knapp <sknapp@berkeley.edu>,"Thu, 5 Jan 2017 21:00:01 -0800",Re: Tests failing with GC limit exceeded,Marcelo Vanzin <vanzin@cloudera.com>,"unsurprisingly, we had another GC:

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70949/console

so, definitely not the system (everything looks hunky dory on the build node).

yeah, this would be a great way to check for leaks. :)

yep...  i agree on both points.

---------------------------------------------------------------------


"
Kyle Kelley <rgbkrk@gmail.com>,"Fri, 06 Jan 2017 06:03:06 +0000",Re: Unsubscribe,"dev@spark.apache.org, ""write2sivakumar@gmail"" <write2sivakumar@gmail.com>","You are now in position 368 for unsubscription. If you wish for your
Unsubscription to occur immediately, please email
dev-unsubscribe@spark.apache.org
"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 6 Jan 2017 13:52:23 +0530",Re: Approach: Incremental data load from HBASE,ayan guha <guha.ayan@gmail.com>,"Hi Ayan,

I mean by Incremental load from HBase, weekly running batch jobs takes rows
from HBase table and dump it out to Hive. Now when next i run Job it only
takes newly arrived jobs.

Same as if we use Sqoop for incremental load from RDBMS to Hive with below
command,

sqoop job --create myssb1 -- import --connect
jdbc:mysql://<hostname>:<port>/sakila --username admin --password admin
--driver=com.mysql.jdbc.Driver --query ""SELECT address_id, address,
district, city_id, postal_code, alast_update, cityid, city, country_id,
clast_update FROM(SELECT a.address_id as address_id, a.address as address,
a.district as district, a.city_id as city_id, a.postal_code as postal_code,
a.last_update as alast_update, c.city_id as cityid, c.city as city,
c.country_id as country_id, c.last_update as clast_update FROM
sakila.address a INNER JOIN sakila.city c ON a.city_id=c.city_id) as sub
WHERE $CONDITIONS"" --incremental lastmodified --check-column alast_update
--last-value 1900-01-01 --target-dir /user/cloudera/ssb7 --hive-import
--hive-table test.sakila -m 1 --hive-drop-import-delims --map-column-java
address=String

Probably i am looking for any tool from HBase incubator family which does
the job for me, or other alternative approaches can be done through reading
Hbase tables in RDD and saving RDD to Hive.

Thanks.



"
Asher Krim <akrim@hubspot.com>,"Fri, 6 Jan 2017 11:03:53 +0200",Re: ml word2vec finSynonyms return type,Joseph Bradley <joseph@databricks.com>,"Felix - I'm not sure I understand your example about pipeline models, could
you elaborate? I'm talking about the `findSynonyms` methods, which AFAIK
have nothing to do with pipeline models.

Joseph - Cool, thanks, I'll PR something in the next few days (and reopen
SPARK-17629 <https://issues.apache.org/jira/browse/SPARK-17629>)





-- 
Asher Krim
Senior Software Engineer
"
Andy Dang <namd88@gmail.com>,"Fri, 6 Jan 2017 10:02:21 +0000",Re: Converting an InternalRow to a Row,Liang-Chi Hsieh <viirya@gmail.com>,"Hi Liang-Chi,

The snippet of code is below. If I bind the encoder early (the schema
doesn't change throughout the execution), the final result is a list of the
same entries.

@RequiredArgsConstructor
public class UDAF extends UserDefinedAggregateFunction {

    // Do not resolve and bind this expression encoder eagerly
    private final ExpressionEncoder<Row> unboundedEncoder;
    private final StructType schema;

    @Override
    public StructType inputSchema() {
        return schema;
    }

    @Override
    public StructType bufferSchema() {
        return new UserDefineType(schema, unboundedEncoder);
    }

    @Override
    public DataType dataType() {
        return DataTypes.createArrayType(schema);
    }

    @Override
    public void initialize(MutableAggregationBuffer buffer) {
        buffer.update(0, new InternalRow[0]);
    }

    @Override
    public void update(MutableAggregationBuffer buffer, Row input) {
        UserDefineType data = buffer.getAs(0);

        data.add(unboundedEncoder.toRow(input));

        buffer.update(0, data);
    }

    @Override
    public void merge(MutableAggregationBuffer buffer1, Row buffer2) {
        // merge
        buffer1.update(0, data1);
    }

    @Override
    public Object evaluate(Row buffer) {
        UserDefineType data = buffer.getAs(0);

       // need to return Row here instead of Internal Row
        return data.rows();
    }

    static ExpressionEncoder<Row> resolveAndBind(ExpressionEncoder<Row>
encoder) {
        val attributes =
JavaConversions.asJavaCollection(encoder.schema().toAttributes()).stream().map(Attribute::toAttribute).collect(Collectors.toList());
        return encoder.resolveAndBind(ScalaUtils.scalaSeq(attributes),
SimpleAnalyzer$.MODULE$);
    }
}

// Wrap around a list of InternalRow
class TopKDataType extends UserDefinedType<TopKDataType> {
    private final ExpressionEncoder<Row> unboundedEncoder;
    private final List<InternalRow> data;

   public Row[] rows() {
        val encoder = resolveAndBind(this.unboundedEncoder);

        return data.stream().map(encoder::fromRow).toArray(Row[]::new);
    }
}

-------
Regards,
Andy


"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 6 Jan 2017 16:37:50 +0530",Re: Approach: Incremental data load from HBASE,ayan guha <guha.ayan@gmail.com>,"Ayan, Thanks
Correct I am not thinking RDBMS terms, i am wearing NoSQL glasses !



"
ayan guha <guha.ayan@gmail.com>,"Fri, 6 Jan 2017 20:53:31 +1100",Re: Approach: Incremental data load from HBASE,Chetan Khatri <chetan.opensource@gmail.com>,"IMHO you should not ""think"" HBase in RDMBS terms, but you can use
ColumnFilters to filter out new records




-- 
Best Regards,
Ayan Guha
"
geoHeil <georg.kf.heiler@gmail.com>,"Fri, 6 Jan 2017 12:30:16 -0700 (MST)",handling of empty partitions,dev@spark.apache.org,"I am working on building a custom ML pipeline-model / estimator to impute
missing values, e.g. I want to fill with last good known value.
Using a window function is slow / will put the data into a single partition.
I built some sample code to use the RDD API however, it some None / null
problems with empty partitions.

How should this be implemented properly to handle such empty partitions?
http://stackoverflow.com/questions/41474175/spark-mappartitionswithindex-handling-empty-partitions

Kind regards,
Georg



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Jan 2017 12:20:43 -0800",Re: Tests failing with GC limit exceeded,Marcelo Vanzin <vanzin@cloudera.com>,"FYI, this is happening across all spark builds...  not just the PRB.
i'm compiling a report now and will email that out this afternoon.
:(


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Jan 2017 12:22:04 -0800",Re: Tests failing with GC limit exceeded,Marcelo Vanzin <vanzin@cloudera.com>,"
s/all/almost all/

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 6 Jan 2017 13:06:16 -0800",Re: Tests failing with GC limit exceeded,"Marcelo Vanzin <vanzin@cloudera.com>, Josh Rosen <joshrosen@databricks.com>, 
	Michael Armbrust <michael@databricks.com>","(adding michael armbrust and josh rosen for visibility)

ok.  roughly 9% of all spark tests builds (including both PRB builds
are failing due to GC overhead limits.

$ wc -l SPARK_TEST_BUILDS GC_FAIL
 1350 SPARK_TEST_BUILDS
  125 GC_FAIL

here are the affected builds (over the past ~2 weeks):
$ sort builds.raw | uniq -c
      6 NewSparkPullRequestBuilder
      1 spark-branch-2.0-test-sbt-hadoop-2.6
      6 spark-branch-2.1-test-maven-hadoop-2.7
      1 spark-master-test-maven-hadoop-2.4
     10 spark-master-test-maven-hadoop-2.6
     12 spark-master-test-maven-hadoop-2.7
      5 spark-master-test-sbt-hadoop-2.2
     15 spark-master-test-sbt-hadoop-2.3
     11 spark-master-test-sbt-hadoop-2.4
     16 spark-master-test-sbt-hadoop-2.6
     22 spark-master-test-sbt-hadoop-2.7
     20 SparkPullRequestBuilder

please note i also included the spark 1.6 test builds in there just to
check...  they last ran ~1 month ago, and had no GC overhead failures.
this leads me to believe that this behavior is quite recent.

so yeah...  looks like we (someone other than me?) needs to take a
look at the sbt and maven java opts.  :)

shane

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Fri, 6 Jan 2017 15:46:03 -0800",Parquet patch release,Spark Dev List <dev@spark.apache.org>,"Last month, there was interest in a Parquet patch release on PR #16281
<https://github.com/apache/spark/pull/16281>. I went ahead and reviewed
commits that should go into a Parquet patch release and started a 1.8.2
discussion
<http://mail-archives.apache.org/mod_mbox/parquet-dev/201701.mbox/%3CCAO4re1mnWJ3%3Di0NpUmPU%2BwD8G%3DsG_%2BAA2PsFBzZv%3DwrUR1529g%40mail.gmail.com%3E>
on the Parquet dev list. If you're interested in reviewing what goes into
1.8.2 or have suggestions, please follow that thread on the Parquet list.

Thanks!

rb

-- 
Ryan Blue
Software Engineer
Netflix
"
Reynold Xin <rxin@databricks.com>,"Fri, 6 Jan 2017 15:48:07 -0800",Re: Parquet patch release,Ryan Blue <rblue@netflix.com.invalid>,"Thanks for the heads up, Ryan!



"
Xiao Li <gatorsmile@gmail.com>,"Fri, 6 Jan 2017 15:49:02 -0800",Re: Parquet patch release,Ryan Blue <rblue@netflix.com.invalid>,"Hi, Ryan,

Really thank you for your help!

Happy New Year!

Xiao Li

2017-01-06 15:46 GMT-08:00 Ryan Blue <rblue@netflix.com.invalid>:

"
Dongjoon Hyun <dongjoon@apache.org>,"Sat, 07 Jan 2017 00:04:02 +0000",Re: Parquet patch release,"Ryan Blue <rblue@netflix.com.invalid>, Xiao Li <gatorsmile@gmail.com>","Great! Thank you, Ryan.

Bests,
Dongjoon.


"
Felix Cheung <felixcheung_m@hotmail.com>,"Sat, 7 Jan 2017 08:29:01 +0000",Spark checkpointing,Steve Loughran <stevel@hortonworks.com>,"Thanks Steve.

As you have pointed out, we have seen some issues related to cloud storage as ""file system"". I'm looking at checkpointing recently. What do you think would be the improvement we could make for ""non local"" (== reliable?) checkpointing?


________________________________
From: Steve Loughran <stevel@hortonworks.com>
Sent: Friday, January 6, 2017 9:57:05 AM
To: Ankur Srivastava
Cc: Felix Cheung; user@spark.apache.org
Subject: Re: Spark GraphFrame ConnectedComponents



Yes I did try it out and it choses the local file system as my checkpoint location starts with s3n://

I am not sure how can I make it load the S3FileSystem.

set fs.default.name to s3n://whatever , or, in spark context, spark.hadoop.fs.default.name

However

1. you should really use s3a, if you have the hadoop 2.7 JARs on your classpath.
2. neither s3n or s3a are real filesystems, and certain assumptions that checkpointing code tends to make ""renames being O(1) atomic calls"" do not hold. It may be that checkpointing to s3 isn't as robust as you'd like




Right, I'd agree, it seems to be only with delete.

Could you by chance run just the delete to see if it fails

FileSystem.get(sc.hadoopConfiguration)
.delete(new Path(somepath), true)
________________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Thursday, January 5, 2017 10:05:03 AM
To: Felix Cheung
Cc: user@spark.apache.org<mailto:user@spark.apache.org>

Subject: Re: Spark GraphFrame ConnectedComponents

Yes it works to read the vertices and edges data from S3 location and is also able to write the checkpoint files to S3. It only fails when deleting the data and that is because it tries to use the default file system. I tried looking up how to update the default file system but could not find anything in that regard.

Thanks
Ankur

FileSystem.

Is the URL scheme for s3n registered?
Does it work when you try to read from s3 from Spark?

_____________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Wednesday, January 4, 2017 9:23 PM
Subject: Re: Spark GraphFrame ConnectedComponents
To: Felix Cheung <felixcheung_m@hotmail.com<mailto:felixcheung_m@hotmail.com>>
Cc: <user@spark.apache.org<mailto:user@spark.apache.org>>



This is the exact trace from the driver logs

Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: s3n://<checkpoint-folder>/8ac233e4-10f9-4eb3-aa53-df6d9d7ea7be/connected-components-c1dbc2b0/3, expected: file:///
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)
at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:529)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
at org.apache.hadoop.fs.ChecksumFileSystem.delete(ChecksumFileSystem.java:534)
at org.graphframes.lib.ConnectedComponents$.org$graphframes$lib$ConnectedComponents$$run(ConnectedComponents.scala:340)
at org.graphframes.lib.ConnectedComponents.run(ConnectedComponents.scala:139)
at GraphTest.main(GraphTest.java:31) ----------- Application Class
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

And I am running spark v 1.6.2 and graphframes v 0.3.0-spark1.6-s_2.10

Thanks
Ankur

Hi

I am rerunning the pipeline to generate the exact trace, I have below part of trace from last run:

Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: s3n://<folder-path>, expected: file:///
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)
at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:516)
at org.apache.hadoop.fs.ChecksumFileSystem.delete(ChecksumFileSystem.java:528)

Also I think the error is happening in this part of the code ""ConnectedComponents.scala:339"" I am referring the code @https://github.com/graphframes/graphframes/blob/master/src/main/scala/org/graphframes/lib/ConnectedComponents.scala

      if (shouldCheckpoint && (iteration % checkpointInterval == 0)) {
        // TODO: remove this after DataFrame.checkpoint is implemented
        val out = s""${checkpointDir.get}/$iteration""
        ee.write.parquet(out)
        // may hit S3 eventually consistent issue
        ee = sqlContext.read.parquet(out)

        // remove previous checkpoint
        if (iteration > checkpointInterval) {
          FileSystem.get(sc.hadoopConfiguration)
            .delete(new Path(s""${checkpointDir.get}/${iteration - checkpointInterval}""), true)
        }

        System.gc() // hint Spark to clean shuffle directories
      }


Thanks
Ankur

Do you have more of the exception stack?


________________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Wednesday, January 4, 2017 4:40:02 PM
To: user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Spark GraphFrame ConnectedComponents

Hi,

I am trying to use the ConnectedComponent algorithm of GraphFrames but by default it needs a checkpoint directory. As I am running my spark cluster with S3 as the DFS and do not have access to HDFS file system I tried using a s3 directory as checkpoint directory but I run into below exception:


Exception in thread ""main""java.lang.IllegalArgumentException: Wrong FS: s3n://<folder-path>, expected: file:///

at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)

at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69)

If I set checkpoint interval to -1 to avoid checkpointing the driver just hangs after 3 or 4 iterations.

Is there some way I can set the default FileSystem to S3 for Spark or any other option?

Thanks
Ankur








"
Liang-Chi Hsieh <viirya@gmail.com>,"Sat, 7 Jan 2017 02:36:37 -0700 (MST)",Re: Parquet patch release,dev@spark.apache.org,"
Hi Ryan,

Great! Thanks for pushing this forward.


Ryan Blue wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sat, 7 Jan 2017 03:11:29 -0700 (MST)",Re: Converting an InternalRow to a Row,dev@spark.apache.org,"
Hi Andy,

Thanks for sharing the code snippet.

I am not sure if you miss something in the snippet, because some function
signature are not matched, e.g.,

    @Override
    public StructType bufferSchema() {
        return new UserDefineType(schema, unboundedEncoder);
    }


Maybe you define a class UserDefineType which extends StructType.

Anyway, I noticed that in this line:

        data.add(unboundedEncoder.toRow(input));

If you read the comment of ""toRow"", you will find it says:

Note that multiple calls to toRow are allowed to return the same actual
[[InternalRow]] object.  Thus, the caller should copy the result before
making another call if required.

I think it is why you get a list of the same entries.

So you may need to change it to:

        data.add(unboundedEncoder.toRow(input).copy());



Andy Dang wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Sat, 7 Jan 2017 21:39:21 +0100",[SQL][PYTHON] UDF improvements.,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I've been looking at the PySpark UserDefinedFunction and I have a couple
of suggestions how it could be improved including:

  * Full featured decorator syntax.
  * Docstring handling improvements.
  * Lazy initialization.

I summarized all suggestions with links to possible solutions in gist
(https://gist.github.com/zero323/88953975361dbb6afd639b35368a97b4) and
I'll be happy to open a JIRA and submit a PR if there is any interest in
that.

-- 
Best,
Maciej

"
Andy Dang <namd88@gmail.com>,"Sat, 7 Jan 2017 22:37:00 +0000",Re: Converting an InternalRow to a Row,Liang-Chi Hsieh <viirya@gmail.com>,"Ah, I missed that bit of documentation  my bad :). That totally explains
the behavior!

Thanks a lot!

-------
Regards,
Andy


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Sun, 8 Jan 2017 22:01:07 +0900","Re: Quick request: prolific PR openers, review your open PRs",Sean Owen <sowen@cloudera.com>,"Sure, I updated status of some PRs.

Regards,
Kazuaki Ishizaki



From:   Sean Owen <sowen@cloudera.com>
To:     dev <dev@spark.apache.org>
Date:   2017/01/04 21:37
Subject:        Quick request: prolific PR openers, review your open PRs



Just saw that there are many people with >= 8 open PRs. Some are 
legitimately in flight but many are probably stale. To set a good example, 
would (everyone) mind flicking through what they've got open and see if 
some PRs are stale and should be closed?

https://spark-prs.appspot.com/users

Username
Open PRs ▴
viirya
13
hhbyyh
12
zhengruifeng
12
HyukjinKwon
12
maropu
10
kiszk
10
yanboliang
10
cloud-fan
8
jerryshao
8












"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Sun, 8 Jan 2017 22:05:56 +0900","Re: Quick request: prolific PR openers, review your open PRs",dev <dev@spark.apache.org>,"Yea, I'll update soon, thanks


,


-- 
---
Takeshi Yamamuro
"
Jacek Laskowski <jacek@japila.pl>,"Sun, 8 Jan 2017 15:41:41 +0100","Re: Quick request: prolific PR openers, review your open PRs",Sean Owen <sowen@cloudera.com>,"+1

What an excellent way to offload some of your chores! I'm so much to learn
from you, Sean!

(Now since Sean seems to have a bit more time I'm gonna send few PRs hoping
he spares some time to find merits in them :))


Pozdrawiam,
Jacek Laskowski
----
h"
Jacek Laskowski <jacek@japila.pl>,"Sun, 8 Jan 2017 20:48:07 +0100","protected val mapStatuses is ConcurrentHashMap in both
 MapOutputTrackerMaster and MapOutputTrackerWorker?",dev <dev@spark.apache.org>,"Hi,

Just noticed that both MapOutputTrackerMaster [1] and
MapOutputTrackerWorker [2] use Java's ConcurrentHashMap for
mapStatuses [3] which makes this abstract mapStatuses attribute less
abstract. I think it was the outcome of some refactoring that led to a
small duplication (and makes the distinction between
MapOutputTrackerMaster and MapOutputTrackerWorker...spurious?).

Why do you think about removing the duplication?

(There's another change with post(message: GetMapOutputMessage) to use
consistently instead of mapOutputRequests.offer(message) that would
make the change initially small slightly bigger and perhaps more
acceptable).

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L292
[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L596
[3] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L84

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Gilad Barkan <gilad.barkan@gmail.com>,"Sun, 8 Jan 2017 22:06:55 +0200",A note about MLlib's StandardScaler,"user@spark.apache.org, dev@spark.apache.org","Hi

It seems that the output of MLlib's *StandardScaler*(*withMean=*True,
*withStd*=True)are not as expected.

The above configuration is expected to do the following transformation:

X -> Y = (X-Mean)/Std  - Eq.1

This transformation (a.k.a. Standardization) should result in a
""standardized"" vector with unit-variance and zero-mean.

I'll demonstrate my claim using the current documentation example:

True)>>> model = standardizer.fit(dataset)>>> result = model.transform(dataset)>>> for r in result.collect(): print r
    DenseVector([-0.7071, 0.7071, -0.7071])    DenseVector([0.7071,
-0.7071, 0.7071])

This result in std = sqrt(1/2) foreach column instead of std=1.

Applying Standardization transformation on the above 2 vectors result
in the following output

    DenseVector([-1.0, 1.0, -1.0])    DenseVector([1.0, -1.0, 1.0])


Another example:

Adding another DenseVector([2.4, 0.8, 3.5]) to the above we get a 3
rows of DenseVectors:
[DenseVector([-2.0, 2.3, 0.0]), DenseVector([3.8, 0.0, 1.9]),
DenseVector([2.4, 0.8, 3.5])]

The StandardScaler result the following scaled vectors:
[DenseVector([-1.12339, 1.084829, -1.02731]), DenseVector([0.792982,
-0.88499, 0.057073]), DenseVector([0.330409, 4
-0.19984, 0.970241])

This result has std=sqrt(2/3)

Instead it should have resulted other 3 vectors that form std=1 for each column.

Adding another vector (4 total) results in 4 scaled vectors that form
std= sqrt(3/4) instead of std=1

I hope all the examples help to make my point clear.

I hope I don't miss here something.

Thank you

Gilad Barkan
"
Holden Karau <holden@pigscanfly.ca>,"Sun, 8 Jan 2017 19:30:43 -0800",Re: A note about MLlib's StandardScaler,Gilad Barkan <gilad.barkan@gmail.com>,"Hi Gilad,

Spark uses the sample standard variance inside of the StandardScaler (see
https://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.mllib.feature.StandardScaler
) which I think would explain the results you are seeing you are seeing. I
believe the scalers are intended to be used on larger sized datasets You
can verify this yourself doing the same computation in Python and see the
scaling using the sample deviation result in the values you are seeing from
Spark.

Cheers,

Holden :)



]>>> dataset = sc.parallelize(vs)>>> standardizer = StandardScaler(True, True)>>> model = standardizer.fit(dataset)>>> result = model.transform(dataset)>>> for r in result.collect(): print r
71, 0.7071])
the following output
of DenseVectors:
([2.4, 0.8, 3.5])]
8499, 0.057073]), DenseVector([0.330409, 4
h column.
= sqrt(3/4) instead of std=1


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 8 Jan 2017 21:30:17 -0700 (MST)",Re: handling of empty partitions,dev@spark.apache.org,"
Hi Georg,

Can you describe your question more clear?

Actually, the example codes you posted in stackoverflow doesn't crash as you
said in the post.


geoHeil wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Sun, 8 Jan 2017 21:40:25 -0800",Re: handling of empty partitions,"Liang-Chi Hsieh <viirya@gmail.com>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi Georg,

Thanks for the question along with the code (as well as posting to stack
overflow). In general if a question is well suited for stackoverflow its
probably better suited to the user@ list instead of the dev@ list so I've
cc'd the user@ list for you.

As far as handling empty partitions when working mapPartitions (and
similar), the general approach is to return an empty iterator of the
correct type when you have an empty input iterator.

It looks like your code is doing this, however it seems like you likely
have a bug in your application logic (namely it assumes that if a partition
has a record missing a value it will either have had a previous row in the
same partition which is good OR that the previous partition is not empty
and has a good row - which need not necessarily be the case). You've
partially fixed this problem by going through and for each partition
collecting the last previous good value, and then if you don't have a good
value at the start of a partition look up the value in the collected array.

However, if this also happens at the same time the previous partition is
empty, you will need to go and lookup the previous previous partition value
until you find the one you are looking for. (Note this assumes that the
first record in your dataset is valid, if it isn't your code will still
fail).

Your solution is really close to working but just has some minor
assumptions which don't always necessarily hold.

Cheers,

Holden :)




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 8 Jan 2017 23:50:18 -0700 (MST)",Re: A note about MLlib's StandardScaler,dev@spark.apache.org,"
Actually I think it is possibly that an user/developer needs the
standardized features with population mean and std in some cases. It would
be better if StandardScaler can offer the option to do that.



Holden Karau wrote







-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
geoHeil <georg.kf.heiler@gmail.com>,"Sun, 8 Jan 2017 23:52:40 -0700 (MST)",Re: handling of empty partitions,dev@spark.apache.org,"Thanks a lot, Holden.

@Liang-Chi Hsieh did you try to run
https://gist.github.com/geoHeil/6a23d18ccec085d486165089f9f430f2 for me
that is crashing in either line 51 or 58. Holden described the problem
pretty well. Ist it clear for you now?

Cheers,
Georg

Holden Karau [via Apache Spark Developers List] <
ml-node+s1001551n20516h45@n3.nabble.com> schrieb am Mo., 9. Jan. 2017 um
06:40 Uhr:





--"
Liang-Chi Hsieh <viirya@gmail.com>,"Mon, 9 Jan 2017 01:08:35 -0700 (MST)",Re: handling of empty partitions,dev@spark.apache.org,"
The map ""toCarry"" will return you (partitionIndex, None) for empty
partition.

So I think line 51 won't fail. Line 58 can fail if ""lastNotNullRow"" is None.
You of course should check if an Option has value or not before you access
it.

As the ""toCarry"" returned is the following when I tested your codes:

Map(1 -> Some(FooBar(Some(2016-01-04),lastAssumingSameDate)), 0 ->
Some(FooBar(Some(2016-01-02),second)))

As you seen, there is no None, so the codes work without failure. But of
course it depends how your data partitions.

For empty partition, when you do mapPartitions, it just gives you an empty
iterator as input. You can do what you need. You already return a None when
you find an empty iterator in preparing ""toCarry"". So I was wondering what
you want to ask in the previous reply.



geoHeil wrote







-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 9 Jan 2017 09:16:59 +0100",scala.MatchError: scala.collection.immutable.Range.Inclusive from catalyst.ScalaReflection.serializerFor?,dev <dev@spark.apache.org>,"Hi,

Just got this this morning using the fresh build of Spark
2.2.0-SNAPSHOT (with a few local modifications):

scala> Seq(0 to 8).toDF
scala.MatchError: scala.collection.immutable.Range.Inclusive (of class
scala.reflect.internal.Types$ClassNoArgsTypeRef)
  at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:520)
  at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:463)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:71)
  at org.apache.spark.sql.SQLImplicits.newIntSequenceEncoder(SQLImplicits.scala:168)
  ... 48 elided

Is this something I've introduced, a known issue or a bug?

./bin/spark-shell --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.0-SNAPSHOT
      /_/

Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_112
Branch master
Compiled by user jacek on 2017-01-09T05:01:47Z
Revision 19d9d4c855eab8f647a5ec66b079172de81221d0
Url https://github.com/apache/spark.git
Type --help for more information.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Georg Heiler <georg.kf.heiler@gmail.com>,"Mon, 09 Jan 2017 08:26:08 +0000",Re: handling of empty partitions,"Liang-Chi Hsieh <viirya@gmail.com>, dev@spark.apache.org","Hi Liang-Chi Hsieh,

Strange:
As the ""toCarry"" returned is the following when I tested your codes:

Map(1 -> Some(FooBar(Some(2016-01-04),lastAssumingSameDate)), 0 ->
Some(FooBar(Some(2016-01-02),second)))
For me it always looked like:

###################### carry
Map(2 -> None, 5 -> None, 4 -> None, 7 ->
Some(FooBar(2016-01-04,lastAssumingSameDate)), 1 ->
Some(FooBar(2016-01-01,first)), 3 -> Some(FooBar(2016-01-02,second)),
6 -> None, 0 -> None)
(2,None)
(5,None)
(4,None)
(7,Some(FooBar(2016-01-04,lastAssumingSameDate)))
(1,Some(FooBar(2016-01-01,first)))
(3,Some(FooBar(2016-01-02,second)))
(6,None)
(0,None)
()
###################### carry


I updated the code to contain a fixed default parallelism
.set(""spark.default.parallelism"", ""12"")

Also:
I updated the sample code:
https://gist.github.com/geoHeil/6a23d18ccec085d486165089f9f430f2

To cope with ""empty/ none"" partitions I added

var lastNotNullRow: Option[FooBar] = toCarryBd.value.get(i).get
      if (lastNotNullRow == None) {
        lastNotNullRow = toCarryBd.value.get(i + 1).get
      }


But that will result in

+----------+--------------------+
|       foo|                 bar|
+----------+--------------------+
|2016-01-01|               first|
|2016-01-02|              second|
|      null|       noValidFormat|
|2016-01-04|lastAssumingSameDate|
+----------+--------------------+

+----------+--------------------+
|       foo|                 bar|
+----------+--------------------+
|2016-01-01|               first|
|2016-01-02|              second|
|2016-01-04|       noValidFormat|
|2016-01-04|lastAssumingSameDate|
+----------+--------------------+

You see that noValidFormat should have been filled with 2016-01-02 to be
filled with last good known value (forward fill)
Cheers,
Georg

Liang-Chi Hsieh <viirya@gmail.com> schrieb am Mo., 9. Jan. 2017 um
09:08 Uhr:

s
y
en
t
ck
ts
y
ty
is
e
l
gt;>
as
handling-empty-partitions
ty-partitions-tp20496p20515.html
gt;
on
ty-partitions-tp20496p20516.html
rvlet.jtp?macro=unsubscribe_by_code&amp;node=20496&amp;code=Z2Vvcmcua2YuaGVpbGVyQGdtYWlsLmNvbXwyMDQ5NnwtMTgzMzc4NTU4MQ==&gt
rvlet.jtp?macro=macro_viewer&amp;id=instant_html%21nabble%3Aemail.naml&amp;base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&amp;breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml&gt
ty-partitions-tp20496p20519.html
"
Liang-Chi Hsieh <viirya@gmail.com>,"Mon, 9 Jan 2017 02:01:06 -0700 (MST)","Re: scala.MatchError: scala.collection.immutable.Range.Inclusive
 from catalyst.ScalaReflection.serializerFor?",dev@spark.apache.org,"
Hi,

As Seq(0 to 8) is:

scala> Seq(0 to 8)
res1: Seq[scala.collection.immutable.Range.Inclusive] = List(Range(0, 1, 2,
3, 4, 5, 6, 7, 8))

Do you actually want to create a Dataset of Range? If so, I think currently
ScalaReflection which the encoder relies doesn't support Range.



Jacek Laskowski wrote






-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 09 Jan 2017 09:12:06 +0000",Re: scala.MatchError: scala.collection.immutable.Range.Inclusive from catalyst.ScalaReflection.serializerFor?,"Liang-Chi Hsieh <viirya@gmail.com>, dev@spark.apache.org","If you want to check if it's your modifications or just in mainline, you
can always just checkout mainline or stash your current changes to rebuild
(this is something I do pretty often when I run into bugs I don't think I
would have introduced).


"
Steve Loughran <stevel@hortonworks.com>,"Mon, 9 Jan 2017 09:56:15 +0000",Re: Spark checkpointing,Felix Cheung <felixcheung_m@hotmail.com>,"

Thanks Steve.

As you have pointed out, we have seen some issues related to cloud storage as ""file system"". I'm looking at checkpointing recently. What do you think would be the improvement we could make for ""non local"" (== reliable?) checkpointing?


right now? I wouldn't checkpoint to S3. Azure WASB works, S3: not reliably. Checkpoint to HDFS, use distCp to back up S3



________________________________
From: Steve Loughran <stevel@hortonworks.com<mailto:stevel@hortonworks.com>Sent: Friday, January 6, 2017 9:57:05 AM
To: Ankur Srivastava
Cc: Felix Cheung; user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Re: Spark GraphFrame ConnectedComponents



Yes I did try it out and it choses the local file system as my checkpoint location starts with s3n://

I am not sure how can I make it load the S3FileSystem.

set fs.default.name to s3n://whatever , or, in spark context, spark.hadoop.fs.default.name

However

1. you should really use s3a, if you have the hadoop 2.7 JARs on your classpath.
2. neither s3n or s3a are real filesystems, and certain assumptions that checkpointing code tends to make ""renames being O(1) atomic calls"" do not hold. It may be that checkpointing to s3 isn't as robust as you'd like




Right, I'd agree, it seems to be only with delete.

Could you by chance run just the delete to see if it fails

FileSystem.get(sc.hadoopConfiguration)
.delete(new Path(somepath), true)
________________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Thursday, January 5, 2017 10:05:03 AM
To: Felix Cheung
Cc: user@spark.apache.org<mailto:user@spark.apache.org>

Subject: Re: Spark GraphFrame ConnectedComponents

Yes it works to read the vertices and edges data from S3 location and is also able to write the checkpoint files to S3. It only fails when deleting the data and that is because it tries to use the default file system. I tried looking up how to update the default file system but could not find anything in that regard.

Thanks
Ankur

FileSystem.

Is the URL scheme for s3n registered?
Does it work when you try to read from s3 from Spark?

_____________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Wednesday, January 4, 2017 9:23 PM
Subject: Re: Spark GraphFrame ConnectedComponents
To: Felix Cheung <felixcheung_m@hotmail.com<mailto:felixcheung_m@hotmail.com>>
Cc: <user@spark.apache.org<mailto:user@spark.apache.org>>



This is the exact trace from the driver logs

Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: s3n://<checkpoint-folder>/8ac233e4-10f9-4eb3-aa53-df6d9d7ea7be/connected-components-c1dbc2b0/3, expected: file:///
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)
at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:529)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
at org.apache.hadoop.fs.ChecksumFileSystem.delete(ChecksumFileSystem.java:534)
at org.graphframes.lib.ConnectedComponents$.org$graphframes$lib$ConnectedComponents$$run(ConnectedComponents.scala:340)
at org.graphframes.lib.ConnectedComponents.run(ConnectedComponents.scala:139)
at GraphTest.main(GraphTest.java:31) ----------- Application Class
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

And I am running spark v 1.6.2 and graphframes v 0.3.0-spark1.6-s_2.10

Thanks
Ankur

Hi

I am rerunning the pipeline to generate the exact trace, I have below part of trace from last run:

Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: s3n://<folder-path>, expected: file:///
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)
at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69)
at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:516)
at org.apache.hadoop.fs.ChecksumFileSystem.delete(ChecksumFileSystem.java:528)

Also I think the error is happening in this part of the code ""ConnectedComponents.scala:339"" I am referring the code @https://github.com/graphframes/graphframes/blob/master/src/main/scala/org/graphframes/lib/ConnectedComponents.scala

      if (shouldCheckpoint && (iteration % checkpointInterval == 0)) {
        // TODO: remove this after DataFrame.checkpoint is implemented
        val out = s""${checkpointDir.get}/$iteration""
        ee.write.parquet(out)
        // may hit S3 eventually consistent issue
        ee = sqlContext.read.parquet(out)

        // remove previous checkpoint
        if (iteration > checkpointInterval) {
          FileSystem.get(sc.hadoopConfiguration)
            .delete(new Path(s""${checkpointDir.get}/${iteration - checkpointInterval}""), true)
        }

        System.gc() // hint Spark to clean shuffle directories
      }


Thanks
Ankur

Do you have more of the exception stack?


________________________________
From: Ankur Srivastava <ankur.srivastava@gmail.com<mailto:ankur.srivastava@gmail.com>>
Sent: Wednesday, January 4, 2017 4:40:02 PM
To: user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Spark GraphFrame ConnectedComponents

Hi,

I am trying to use the ConnectedComponent algorithm of GraphFrames but by default it needs a checkpoint directory. As I am running my spark cluster with S3 as the DFS and do not have access to HDFS file system I tried using a s3 directory as checkpoint directory but I run into below exception:


Exception in thread ""main""java.lang.IllegalArgumentException: Wrong FS: s3n://<folder-path>, expected: file:///

at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)

at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69)

If I set checkpoint interval to -1 to avoid checkpointing the driver just hangs after 3 or 4 iterations.

Is there some way I can set the default FileSystem to S3 for Spark or any other option?

Thanks
Ankur









"
Sean Owen <sowen@cloudera.com>,"Mon, 09 Jan 2017 11:23:36 +0000",Re: A note about MLlib's StandardScaler,"Liang-Chi Hsieh <viirya@gmail.com>, dev@spark.apache.org","This could be true if you knew you were just going to scale the input to
StandardScaler and nothing else. It's probably more typical you'd scale
some other data. The current behavior is therefore the sensible default,
because the input is a sample of some unknown larger population.

I think it doesn't matter much except for toy problems, because at any
scale, the difference between 1/n and 1/(n-1) is negligible, and for most
purposes for which the scaler is used, it won't matter anyway (faster
convergence of an optimizer for example). I'm neutral on whether it's worth
complicating the API to do both, therefore.


"
Andy Dang <namd88@gmail.com>,"Mon, 9 Jan 2017 13:52:42 +0000",How to hint Spark to use HashAggregate() for UDAF,"user <user@spark.apache.org>, dev@spark.apache.org","Hi all,

It appears to me that Dataset.groupBy().agg(udaf) requires a full sort,
which is very inefficient for certain aggration:

The code is very simple:
- I have a UDAF
- What I want to do is: dataset.groupBy(cols).agg(udaf).count()

The physical plan I got was:
*HashAggregate(keys=[], functions=[count(1)], output=[count#67L])
+- Exchange SinglePartition
   +- *HashAggregate(keys=[], functions=[partial_count(1)],
output=[count#71L])
      +- *Project
         +- Generate explode(internal_col#31), false, false,
[internal_col#42]
            +- SortAggregate(key=[key#0],
functions=[aggregatefunction(key#0, nested#1, nestedArray#2,
nestedObjectArray#3, value#4L, com.[...]uDf@108b121f, 0, 0)],
output=[internal_col#31])
               +- *Sort [key#0 ASC], false, 0
                  +- Exchange hashpartitioning(key#0, 200)
                     +- SortAggregate(key=[key#0],
functions=[partial_aggregatefunction(key#0, nested#1, nestedArray#2,
nestedObjectArray#3, value#4L, com.[...]uDf@108b121f, 0, 0)],
output=[key#0,internal_col#37])
                        +- *Sort [key#0 ASC], false, 0
                           +- Scan
ExistingRDD[key#0,nested#1,nestedArray#2,nestedObjectArray#3,value#4L]

How can I make Spark to use HashAggregate (like the count(*) expression)
instead of SortAggregate with my UDAF?

Is it intentional? Is there an issue tracking this?

-------
Regards,
Andy
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Mon, 9 Jan 2017 23:05:04 +0900",Re: How to hint Spark to use HashAggregate() for UDAF,Andy Dang <namd88@gmail.com>,"Hi,

Spark always uses hash-based aggregates if the types of aggregated data are
supported there;
otherwise, spark fails to use hash-based ones, then it uses sort-based ones.
See:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala#L38

So, I'm not sure about your query though, it seems the types of aggregated
data in your query
are not supported for hash-based aggregates.

// maropu







-- 
---
Takeshi Yamamuro
"
Andy Dang <namd88@gmail.com>,"Mon, 9 Jan 2017 14:13:56 +0000",Re: How to hint Spark to use HashAggregate() for UDAF,Takeshi Yamamuro <linguin.m.s@gmail.com>,"Hi Takeshi,

Thanks for the answer. My UDAF aggregates data into an array of rows.

Apparently this makes it ineligible to using Hash-based aggregate based on
the logic at:
https://github.com/apache/spark/blob/master/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java#L74
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java#L108

The list of support data type is VERY limited unfortunately.

It doesn't make sense to me that data type must be mutable for the UDAF to
use hash-based aggregate, but I could be missing something here :). I could
achieve hash-based aggregate by turning this query to RDD mode, but that is
counter intuitive IMO.

-------
Regards,
Andy


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 9 Jan 2017 10:30:18 -0800",Re: [SQL][PYTHON] UDF improvements.,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Maciej, this looks great.

Could you open a JIRA issue for improving the @udf decorator and possibly
sub-tasks for the specific features from the gist? Thanks!

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 9 Jan 2017 17:25:05 -0800",Re: Parquet patch release,Spark Dev List <dev@spark.apache.org>,"Finished reviewing the list and it LGTM now (left comments in the 
spreadsheet and Ryan already made corresponding changes).

Ryan - Thanks a lot for pushing this and making it happen!

Cheng



"
Liang-Chi Hsieh <viirya@gmail.com>,"Mon, 9 Jan 2017 19:01:03 -0700 (MST)",Re: How to hint Spark to use HashAggregate() for UDAF,dev@spark.apache.org,"
Hi Andy,

Because hash-based aggregate uses unsafe row as aggregation states, so the
aggregation buffer schema must be mutable types in unsafe row.

If you can use TypedImperativeAggregate to implement your aggregation
function, SparkSQL has ObjectHashAggregateExec which supports hash-based
aggregate using arbitrary JVM objects as aggregation states.



Andy Dang wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Prasun Ratn <prasun.ratn@gmail.com>,"Tue, 10 Jan 2017 09:20:31 +0530",Spark performance tests,Apache Spark Dev <dev@spark.apache.org>,"Hi

Are there performance tests or microbenchmarks for Spark - especially
directed towards the CPU specific parts? I looked at spark-perf but
that doesn't seem to have been updated recently.

Thanks
Prasun

---------------------------------------------------------------------


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Tue, 10 Jan 2017 18:21:29 +0900",Re: Spark performance tests,Prasun Ratn <prasun.ratn@gmail.com>,"Hi,
You may find several micro-benchmarks under 
https://github.com/apache/spark/tree/master/sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark
.

Regards,
Kazuaki Ishizaki



From:   Prasun Ratn <prasun.ratn@gmail.com>
To:     Apache Spark Dev <dev@spark.apache.org>
Date:   2017/01/10 12:52
Subject:        Spark performance tests



Hi

Are there performance tests or microbenchmarks for Spark - especially
directed towards the CPU specific parts? I looked at spark-perf but
that doesn't seem to have been updated recently.

Thanks
Prasun

---------------------------------------------------------------------




"
Adam Roberts <AROBERTS@uk.ibm.com>,"Tue, 10 Jan 2017 09:58:03 +0000",Re: Spark performance tests,"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Hi, I suggest HiBench and SparkSqlPerf, HiBench features many benchmarks 
within it that exercise several components of Spark (great for stressing 
core, sql, MLlib capabilities), SparkSqlPerf features 99 TPC-DS queries 
(stressing the DataFrame API and therefore the Catalyst optimiser), both 
work well with Spark 2

HiBench: https://github.com/intel-hadoop/HiBench
SparkSqlPerf: https://github.com/databricks/spark-sql-perf




From:   ""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>
To:     Prasun Ratn <prasun.ratn@gmail.com>
Cc:     Apache Spark Dev <dev@spark.apache.org>
Date:   10/01/2017 09:22
Subject:        Re: Spark performance tests



Hi,
You may find several micro-benchmarks under 
https://github.com/apache/spark/tree/master/sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark
.

Regards,
Kazuaki Ishizaki



From:        Prasun Ratn <prasun.ratn@gmail.com>
To:        Apache Spark Dev <dev@spark.apache.org>
Date:        2017/01/10 12:52
Subject:        Spark performance tests



Hi

Are there performance tests or microbenchmarks for Spark - especially
directed towards the CPU specific parts? I looked at spark-perf but
that doesn't seem to have been updated recently.

Thanks
Prasun

---------------------------------------------------------------------




Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
dragonly <liyilongko@gmail.com>,"Tue, 10 Jan 2017 04:21:59 -0700 (MST)","[SQL][CodeGen] Is there a way to set break point and debug the
 generated code?",dev@spark.apache.org,"I am recently hacking into the SparkSQL and trying to add some new udts and
functions, as well as some new Expression classes. I run into the problem of
the return type of nullSafeEval method. In one of the new Expression
classes, I want to return an array of my udt, and my code is like `return
new GenericArrayData(Array[udt](the array))`. my dataType of the new
Expression class is like `ArrayType(new MyUDT(), containsNull = false)`. And
I finally get an java object type conversion error.

So I tried to debug into the code and see where the conversion happened,
only to found that after some generated code execution, I stepped into the
GenericArrayData.getAs[T](ordinal: Int) method, and find the ordinal always
0. So here's the problem: SparkSQL is getting the 0th element out of the
GenericArrayData and treat it as a MyUDT, but I told it to treat the output
of the Expression class as ArrayType of MyUDT.

It's obscure to me how this ordinal variable comes in and is always 0. Is
there a way of debugging into the generated code?

PS: just reading the code generation part without jumping back and forth is
really not cool :/



--

---------------------------------------------------------------------


"
Prasun Ratn <prasun.ratn@gmail.com>,"Tue, 10 Jan 2017 17:55:34 +0530",Re: Spark performance tests,Adam Roberts <AROBERTS@uk.ibm.com>,"Thanks Adam, Kazuaki!


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Jan 2017 09:31:23 -0800","Re: [SQL][CodeGen] Is there a way to set break point and debug the
 generated code?",dragonly <liyilongko@gmail.com>,"It's unfortunately difficult to debug -- that's one downside of codegen.
You can dump all the code via ""explain codegen"" though. That's typically
enough for me to debug.



"
Andy Dang <namd88@gmail.com>,"Tue, 10 Jan 2017 17:35:40 +0000",Re: How to hint Spark to use HashAggregate() for UDAF,Liang-Chi Hsieh <viirya@gmail.com>,"Thanks. It appears that TypedImperativeAggregate won't be available till
2.2.x. I'm stuck with my RDD approach then :(

-------
Regards,
Andy


"
shane knapp <sknapp@berkeley.edu>,"Tue, 10 Jan 2017 10:44:53 -0800",Re: Tests failing with GC limit exceeded,"Marcelo Vanzin <vanzin@cloudera.com>, Josh Rosen <joshrosen@databricks.com>, 
	Michael Armbrust <michael@databricks.com>","quick update:

things are looking slightly...  better.  the number of failing builds
due to GC overhead has decreased slightly since the reboots last
week...  in fact, in the last three days the only builds to be
affected are spark-master-test-maven-hadoop-2.7 (three failures) and
spark-master-test-maven-hadoop-2.6 (five failures).

overall percentages (over two weeks) have also dropped from ~9% to
~7%, so at least the rate of failure is dropping.

so, the while we're still bleeding, it's slowed down a bit.  we'll
still need to audit the java heap size allocs in the various tests,
however.

shane


---------------------------------------------------------------------


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 10 Jan 2017 23:19:27 +0100",Re: [SQL][PYTHON] UDF improvements.,Ryan Blue <rblue@netflix.com>,"Thanks for your response Ryan. Here you are
https://issues.apache.org/jira/browse/SPARK-19159



-- 
Best,
Maciej

"
Reynold Xin <rxin@databricks.com>,"Wed, 11 Jan 2017 01:13:32 -0800",Re: Spark Improvement Proposals,Tim Hunter <timhunter@databricks.com>,"+1 on all counts (consensus, time bound, define roles)

I can update the doc in the next few days and share back. Then maybe we can
just officially vote on this. As Tim suggested, we might not get it 100%
right the first time and would need to re-iterate."
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 11 Jan 2017 13:18:55 +0100",[PYSPARK] Python tests organization,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I can't help but wonder if there is any practical reason for keeping
monolithic test modules. These things are already pretty large (1500 -
2200 LOCs) and can only grow. Development aside, I assume that many
users use tests the same way as me, to check the intended behavior, and
largish loosely coupled modules make it harder than it should be.

If there's no rationale for that it could be a good time start thinking
about moving tests to packages and separating into modules reflecting
project structure.

-- 
Best,
Maciej


---------------------------------------------------------------------


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Wed, 11 Jan 2017 17:44:11 +0000",Re: [PYSPARK] Python tests organization,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello Maciej,

If there's a jira available for this I'd like to help get this moving, let me know next steps.

Thanks in advance.


________________________________
From: Maciej Szymkiewicz <mszymkiewicz@gmail.com>
Sent: Wednesday, January 11, 2017 4:18 AM
To: dev@spark.apache.org
Subject: [PYSPARK] Python tests organization

Hi,

I can't help but wonder if there is any practical reason for keeping
monolithic test modules. These things are already pretty large (1500 -
2200 LOCs) and can only grow. Development aside, I assume that many
users use tests the same way as me, to check the intended behavior, and
largish loosely coupled modules make it harder than it should be.

If there's no rationale for that it could be a good time start thinking
about moving tests to packages and separating into modules reflecting
project structure.

--
Best,
Maciej


---------------------------------------------------------------------

"
Reynold Xin <rxin@databricks.com>,"Wed, 11 Jan 2017 17:47:30 +0000",Re: [PYSPARK] Python tests organization,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, Saikat Kanjilal <sxk1969@hotmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","It would be good to break them down a bit more, provided that we don't
increase for example total runtime due to extra setup.



"
Saikat Kanjilal <sxk1969@hotmail.com>,"Wed, 11 Jan 2017 17:54:18 +0000",Re: [PYSPARK] Python tests organization,"Reynold Xin <rxin@databricks.com>, Maciej Szymkiewicz
	<mszymkiewicz@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Is it worth to come up with a proposal for this and float to dev?


________________________________
From: Reynold Xin <rxin@databricks.com>
Sent: Wednesday, January 11, 2017 9:47 AM
To: Maciej Szymkiewicz; Saikat Kanjilal; dev@spark.apache.org
Subject: Re: [PYSPARK] Python tests organization

It would be good to break them down a bit more, provided that we don't increase for example total runtime due to extra setup.

















Hello Maciej,



If there's a jira available for this I'd like to help get this moving, let me know next steps.



Thanks in advance.












________________________________


From: Maciej Szymkiewicz <mszymkiewicz@gmail.com<mailto:mszymkiewicz@gmail.com>>


Sent: Wednesday, January 11, 2017 4:18 AM


To: dev@spark.apache.org<mailto:dev@spark.apache.org>


Subject: [PYSPARK] Python tests organization










Hi,





I can't help but wonder if there is any practical reason for keeping


monolithic test modules. These things are already pretty large (1500 -


2200 LOCs) and can only grow. Development aside, I assume that many


users use tests the same way as me, to check the intended behavior, and


largish loosely coupled modules make it harder than it should be.





If there's no rationale for that it could be a good time start thinking


about moving tests to packages and separating into modules reflecting


project structure.





--


Best,


Maciej








---------------------------------------------------------------------


ibe@spark.apache.org>







"
Reynold Xin <rxin@databricks.com>,"Wed, 11 Jan 2017 18:02:09 +0000",Re: [PYSPARK] Python tests organization,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, Saikat Kanjilal <sxk1969@hotmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Yes absolutely.

"
Saikat Kanjilal <sxk1969@hotmail.com>,"Wed, 11 Jan 2017 18:36:18 +0000",Re: [PYSPARK] Python tests organization,"""dev@spark.apache.org"" <dev@spark.apache.org>","Maciej/Reynolds,

If its ok with you guys I can start working on a proposal and create a JIRA, let me know next steps.

Thanks in advance.


________________________________
From: Maciej Szymkiewicz <mszymkiewicz@gmail.com>
Sent: Wednesday, January 11, 2017 10:14 AM
To: Saikat Kanjilal
Subject: Re: [PYSPARK] Python tests organization


Not yet, I want to see if there is any consensus about it. It is a lot of tedious work and I would be shame if someone started working on this just to get it dropped.


Hello Maciej,

If there's a jira available for this I'd like to help get this moving, let me know next steps.

Thanks in advance.


________________________________
From: Maciej Szymkiewicz <mszymkiewicz@gmail.com><mailto:mszymkiewicz@gmail.com>
Sent: Wednesday, January 11, 2017 4:18 AM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: [PYSPARK] Python tests organization

Hi,

I can't help but wonder if there is any practical reason for keeping
monolithic test modules. These things are already pretty large (1500 -
2200 LOCs) and can only grow. Development aside, I assume that many
users use tests the same way as me, to check the intended behavior, and
largish loosely coupled modules make it harder than it should be.

If there's no rationale for that it could be a good time start thinking
about moving tests to packages and separating into modules reflecting
project structure.

--
Best,
Maciej


---------------------------------------------------------------------
ibe@spark.apache.org>



--
Maciej Szymkiewicz
"
Kalvin Chau <kalvinnchau@gmail.com>,"Wed, 11 Jan 2017 22:38:43 +0000",[Streaming] ConcurrentModificationExceptions when Windowing,dev@spark.apache.org,"Hi,

We've been running into ConcurrentModificationExcpetions ""KafkaConsumer is
not safe for multi-threaded access"" with the CachedKafkaConsumer. I've been
working through debugging this issue and after looking through some of the
spark source code I think this is a bug.

Our set up is:
Spark 2.0.2, running in Mesos 0.28.0-2 in client mode, using
Spark-Streaming-Kafka-010
spark.executor.cores 1
spark.mesos.extra.cores 1

Batch interval: 10s, window interval: 180s, and slide interval: 30s

We would see the exception when in one executor there are two task worker
threads assigned the same Topic+Partition, but a different set of offsets.

They would both get the same CachedKafkaConsumer, and whichever task thread
went first would seek and poll for all the records, and at the same time
the second thread would try to seek to its offset but fail because it is
unable to acquire the lock.

Time0 E0 Task0 - TopicPartition(""abc"", 0) X to Y
Time0 E0 Task1 - TopicPartition(""abc"", 0) Y to Z

Time1 E0 Task0 - Seeks and starts to poll
Time1 E0 Task1 - Attempts to seek, but fails

Here are some relevant logs:
17/01/06 03:10:01 Executor task launch worker-1 INFO KafkaRDD: Computing
topic test-topic, partition 2 offsets 4394204414 -> 4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO KafkaRDD: Computing
topic test-topic, partition 2 offsets 4394238058 -> 4394257712
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested
4394204414
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested
4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO CachedKafkaConsumer:
Initial fetch for spark-executor-consumer test-topic 2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer:
Seeking to test-topic-2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Putting
block rdd_199_2 failed due to an exception
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Block
rdd_199_2 could not be removed as it was not found on disk or in memory
17/01/06 03:10:01 Executor task launch worker-0 ERROR Executor: Exception
in task 49.0 in stage 45.0 (TID 3201)
java.util.ConcurrentModificationException: KafkaConsumer is not safe for
multi-threaded access
at
org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1431)
at
org.apache.kafka.clients.consumer.KafkaConsumer.seek(KafkaConsumer.java:1132)
at
org.apache.spark.streaming.kafka010.CachedKafkaConsumer.seek(CachedKafkaConsumer.scala:95)
at
org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:69)
at
org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
at
org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at
org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:360)
at
org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:951)
at
org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
at
org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
at
org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Polled [test-topic-2]  8237
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204415 requested
4394204415
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204416 requested
4394204416
...

It looks like when WindowedDStream does the getOrCompute call its computing
all the sets of of offsets it needs and tries to farm out the work in
parallel. So each available worker task gets each set of offsets that need
to be read.

After realizing what was going on I tested four states:

   - spark.executor.cores 1 and spark.mesos.extra.cores 0
      - No Exceptions
   - spark.executor.cores 1 and spark.mesos.extra.cores 1
      - ConcurrentModificationException
   - spark.executor.cores 2 and spark.mesos.extra.cores 0
      - ConcurrentModificationException
   - spark.executor.cores 2 and spark.mesos.extra.cores 1
      - ConcurrentModificationException


I'm not sure what the best solution to this is if we want to be able to
have N tasks threads read from the same TopicPartition to increase
parallelization. You could possibly allow N CachedKafkaConsumers for the
same TopicPartition.

Any thoughts on this?

Thanks,
Kalvin
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Wed, 11 Jan 2017 14:53:49 -0800",Re: [Streaming] ConcurrentModificationExceptions when Windowing,Kalvin Chau <kalvinnchau@gmail.com>,"I think you may reuse the kafka DStream (the DStream returned by
createDirectStream). If you need to read from the same Kafka source, you
need to create another DStream.


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Wed, 11 Jan 2017 15:27:41 -0800",Re: [Streaming] ConcurrentModificationExceptions when Windowing,Kalvin Chau <kalvinnchau@gmail.com>,"Do you change ""spark.streaming.concurrentJobs"" to more than 1? Kafka 0.10
connector requires it must be 1.


"
Kalvin Chau <kalvinnchau@gmail.com>,"Wed, 11 Jan 2017 23:25:15 +0000",Re: [Streaming] ConcurrentModificationExceptions when Windowing,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","I'm not re-using any InputDStreams actually, this is one InputDStream that
has a window applied to it.
 Then when Spark creates and assigns tasks to read from the Topic, one
executor gets assigned two tasks to read from the same TopicPartition, and
uses the same CachedKafkaConsumer to read from the TopicPartition causing
the ConcurrentModificationException in one of the worker threads.


I think you may reuse the kafka DStream (the DStream returned by
createDirectStream). If you need to read from the same Kafka source, you
need to create another DStream.


Hi,

We've been running into ConcurrentModificationExcpetions ""KafkaConsumer is
not safe for multi-threaded access"" with the CachedKafkaConsumer. I've been
working through debugging this issue and after looking through some of the
spark source code I think this is a bug.

Our set up is:
Spark 2.0.2, running in Mesos 0.28.0-2 in client mode, using
Spark-Streaming-Kafka-010
spark.executor.cores 1
spark.mesos.extra.cores 1

Batch interval: 10s, window interval: 180s, and slide interval: 30s

We would see the exception when in one executor there are two task worker
threads assigned the same Topic+Partition, but a different set of offsets.

They would both get the same CachedKafkaConsumer, and whichever task thread
went first would seek and poll for all the records, and at the same time
the second thread would try to seek to its offset but fail because it is
unable to acquire the lock.

Time0 E0 Task0 - TopicPartition(""abc"", 0) X to Y
Time0 E0 Task1 - TopicPartition(""abc"", 0) Y to Z

Time1 E0 Task0 - Seeks and starts to poll
Time1 E0 Task1 - Attempts to seek, but fails

Here are some relevant logs:
17/01/06 03:10:01 Executor task launch worker-1 INFO KafkaRDD: Computing
topic test-topic, partition 2 offsets 4394204414 -> 4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO KafkaRDD: Computing
topic test-topic, partition 2 offsets 4394238058 -> 4394257712
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested
4394204414
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested
4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO CachedKafkaConsumer:
Initial fetch for spark-executor-consumer test-topic 2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer:
Seeking to test-topic-2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Putting
block rdd_199_2 failed due to an exception
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Block
rdd_199_2 could not be removed as it was not found on disk or in memory
17/01/06 03:10:01 Executor task launch worker-0 ERROR Executor: Exception
in task 49.0 in stage 45.0 (TID 3201)
java.util.ConcurrentModificationException: KafkaConsumer is not safe for
multi-threaded access
at
org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1431)
at
org.apache.kafka.clients.consumer.KafkaConsumer.seek(KafkaConsumer.java:1132)
at
org.apache.spark.streaming.kafka010.CachedKafkaConsumer.seek(CachedKafkaConsumer.scala:95)
at
org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:69)
at
org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
at
org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at
org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:360)
at
org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:951)
at
org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
at
org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
at
org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Polled [test-topic-2]  8237
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204415 requested
4394204415
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer:
Get spark-executor-consumer test-topic 2 nextOffset 4394204416 requested
4394204416
...

It looks like when WindowedDStream does the getOrCompute call its computing
all the sets of of offsets it needs and tries to farm out the work in
parallel. So each available worker task gets each set of offsets that need
to be read.

After realizing what was going on I tested four states:

   - spark.executor.cores 1 and spark.mesos.extra.cores 0
      - No Exceptions
   - spark.executor.cores 1 and spark.mesos.extra.cores 1
      - ConcurrentModificationException
   - spark.executor.cores 2 and spark.mesos.extra.cores 0
      - ConcurrentModificationException
   - spark.executor.cores 2 and spark.mesos.extra.cores 1
      - ConcurrentModificationException


I'm not sure what the best solution to this is if we want to be able to
have N tasks threads read from the same TopicPartition to increase
parallelization. You could possibly allow N CachedKafkaConsumers for the
same TopicPartition.

Any thoughts on this?

Thanks,
Kalvin
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Wed, 11 Jan 2017 15:43:49 -0800",Re: [Streaming] ConcurrentModificationExceptions when Windowing,Kalvin Chau <kalvinnchau@gmail.com>,"Or do you enable ""spark.speculation""? If not, Spark Streaming should not
launch two tasks using the same TopicPartition.


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Wed, 11 Jan 2017 16:04:14 -0800",Re: [Streaming] ConcurrentModificationExceptions when Windowing,Kalvin Chau <kalvinnchau@gmail.com>,"Could you post your codes, please?


"
Kalvin Chau <kalvinnchau@gmail.com>,"Wed, 11 Jan 2017 23:53:05 +0000",Re: [Streaming] ConcurrentModificationExceptions when Windowing,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","""spark.speculation"" is not set, so it would be whatever the default is.



"
Kalvin Chau <kalvinnchau@gmail.com>,"Wed, 11 Jan 2017 23:33:49 +0000",Re: [Streaming] ConcurrentModificationExceptions when Windowing,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","I have not modified that configuration setting, and that doesn't seem to be
documented anywhere.

Does the Kafka 0.10 require the number of cores on an executor be set to 1?
I didn't see that documented anywhere either.


"
Kalvin Chau <kalvinnchau@gmail.com>,"Thu, 12 Jan 2017 01:20:26 +0000",Re: [Streaming] ConcurrentModificationExceptions when Windowing,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Here is the minimal code example where I was able to replicate:
Batch interval is set to 2 to get the exceptions to happen more often.

val kafkaParams = Map[String, Object](
  ""bootstrap.servers"" -> brokers,
  ""key.deserializer"" -> classOf[KafkaAvroDeserializer],
  ""value.deserializer"" -> classOf[KafkaAvroDeserializer],
  ""enable.auto.commit"" -> (false: java.lang.Boolean),
  ""group.id"" -> groupId,
  ""schema.registry.url"" -> schemaRegistryUrl,
  ""auto.offset.reset"" -> offset
)

val inputStream = KafkaUtils.createDirectStream[Object, Object](
  ssc,
  PreferConsistent,
  Subscribe[Object, Object]
    (kafkaTopic, kafkaParams)
)

val windowStream = inputStream.map(_.toString).window(Seconds(180), Seconds(30))

windowStream.foreachRDD{
  rdd => {
    val filtered = rdd.filter(_.contains(""idb""))

    filtered.foreach(
      message => {
        var i = 0
        if (i == 0) {
          logger.info(message)
          i = i + 1
        }
      }
    )
  }
}



"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Wed, 11 Jan 2017 17:43:25 -0800",Re: [Streaming] ConcurrentModificationExceptions when Windowing,Kalvin Chau <kalvinnchau@gmail.com>,"Thanks for reporting this. Finally I understood the root cause. Could you
file a JIRA on https://issues.apache.org/jira/browse/SPARK please?


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 11 Jan 2017 19:47:57 -0700 (MST)",Re: handling of empty partitions,dev@spark.apache.org,"
Hi Georg,

It is not strange. As I said before, it depends how the data is partitioned.

When you try to get the available value from next partition like this:

var lastNotNullRow: Option[FooBar] = toCarryBd.value.get(i).get
      if (lastNotNullRow == None) {
        lastNotNullRow = toCarryBd.value.get(i + 1).get
      }

You may need to make sure the next partition has a value too. Holden has
pointed out before, you need to deal with the case that the previous/next
partition is empty too and go next until you find a non-empty partition.



geoHeil wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Kalvin Chau <kalvinnchau@gmail.com>,"Thu, 12 Jan 2017 02:17:32 +0000",Re: [Streaming] ConcurrentModificationExceptions when Windowing,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","I've filed an issue here https://issues.apache.org/jira/browse/SPARK-19185,
let me know if I missed anything!

--Kalvin


"
Georg Heiler <georg.kf.heiler@gmail.com>,"Thu, 12 Jan 2017 07:08:57 +0000",Re: handling of empty partitions,"Liang-Chi Hsieh <viirya@gmail.com>, dev@spark.apache.org","I see that there is the possibility to improve and make the algorithm more
fault tolerant as outlined by both of you.
Could you explain a little bit more why

+----------+--------------------+
|       foo|                 bar|
+----------+--------------------+
|2016-01-01|               first|
|2016-01-02|              second|
|      null|       noValidFormat|
|2016-01-04|lastAssumingSameDate|
+----------+--------------------+

+----------+--------------------+
|       foo|                 bar|
+----------+--------------------+
|2016-01-01|               first|
|2016-01-02|              second|
|2016-01-04|       noValidFormat|
|2016-01-04|lastAssumingSameDate|
+----------+--------------------+

i.e. that the records are not filled with the last good known value but
rather ""the next one"" is so clear.
Why does it depend on the partitions?
As the broadcast map is available to all the partitions shouldn't this be
the same regardless of partitioning?

The (too simple fix to be applicable generally)
 if (lastNotNullRow == None) {
        lastNotNullRow = toCarryBd.value.get(i + 1).get
      }
as of choosing the next element is only applied when the partition does not
contain new values.

Kind regards,
Georg

Liang-Chi Hsieh <viirya@gmail.com> schrieb am Do., 12. Jan. 2017 um
03:48 Uhr:

e
of
em
w
o
e
e
n
 a
d
on
on
=0&gt;>
sh
o
 /
handling-empty-partitions
ty-partitions-tp20496p20515.html
--
=1&gt;
ty-partitions-tp20496p20516.html
rvlet.jtp?macro=unsubscribe_by_code&amp;node=20496&amp;code=Z2Vvcmcua2YuaGVpbGVyQGdtYWlsLmNvbXwyMDQ5NnwtMTgzMzc4NTU4MQ==&gt
rvlet.jtp?macro=macro_viewer&amp;id=instant_html%21nabble%3Aemail.naml&amp;base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&amp;breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml&gt
ty-partitions-tp20496p20519.html
ty-partitions-tp20496p20558.html
"
Sharan F <sharan@apache.org>,"Thu, 12 Jan 2017 13:12:10 +0100",FOSDEM 2017 Open Source Conference - Brussels,undisclosed-recipients: ;,"Hello Everyone

This email is to tell you about ASF participation at FOSDEM. The event 
will be held in Brussels on 4^th & 5^th February 2017 and we are hoping 
that many people from our ASF projects will be there.

https://fosdem.org/2017/

Attending FOSDEM is completely free and the ASF will again be running a 
booth there. Our main focus will on talking to people about the ASF, our 
projects and communities.

*_Why Attend FOSDEM?_*
Some reasons for attending FOSDEM are:

 1. Promoting your project: FOSDEM has up to 4-5000 attendees so is a
    great place to spread the word about your project
 2. Learning, participating and meeting up: FOSDEM is a developers
    conference so includes presentations covering a range of
    technologies and includes lots of topic specific devrooms

_*FOSDEM Wiki *_
A page on the Community Development wiki has been created with the main 
details about our involvement at conference, so please take a look

https://cwiki.apache.org/confluence/display/COMDEV/FOSDEM+2017

If you would like to spend some time on the ASF booth promoting your 
project then please sign up on the FOSDEM wiki page. Initially we would 
like to split this into slots of 3-4 hours but this will depend on the 
number of projects that are represented.

We are also looking for volunteers to help out on the booth over the 2 
days of the conference, so if you are going to be there and are willing 
to help then please add your name to the volunteer list.

_*Project Stickers*_
If you are going to be at FOSDEM and do not have any project stickers to 
give away then we may (budget permitting) be able to help you get some 
printed. Please contact me with your requirements.

_*Social Event*_
Some people have asked about organising an ASF social event / meetup 
during the conference. This is possible but we will need know how many 
people are interested and which date works best. The FOSDEM wiki page 
also contains an 'Arrival / Departure' section so so please add your 
details if you would like to participate.

I hope this helps people see some of the advantages of attending FOSDEM 
and we are looking forward to seeing lots of people there from our ASF 
communities.

Thanks
Sharan

Apache Community Development
http://community.apache.org/
"
<williamtellme123@gmail.com>,"Thu, 12 Jan 2017 11:40:21 -0600",Unsubscribe,"<dev@spark.apache.org>,
	<dev-unsubscribe@spark.apache.org>","Please unsubscribe me

 

From: Varanasi, Venkata [mailto:venkata.varanasi@bankofamerica.com] 
Sent: Thursday, April 28, 2016 1:35 PM
To: dev@spark.apache.org
Subject: Unsubscribe

 

 

  _____  

This message, and any attachments, is for the intended recipient(s) only,
may contain information that is privileged, confidential and/or proprietary
and subject to important terms and conditions available at
http://www.bankofamerica.com/emaildisclaimer. If you are not the intended
recipient, please delete this message.

"
Saikat Kanjilal <sxk1969@hotmail.com>,"Thu, 12 Jan 2017 18:20:35 +0000",Re: [PYSPARK] Python tests organization,"""dev@spark.apache.org"" <dev@spark.apache.org>","Following up, any thoughts on next steps for this?


________________________________
From: Maciej Szymkiewicz <mszymkiewicz@gmail.com>
Sent: Wednesday, January 11, 2017 10:14 AM
To: Saikat Kanjilal
Subject: Re: [PYSPARK] Python tests organization


Not yet, I want to see if there is any consensus about it. It is a lot of tedious work and I would be shame if someone started working on this just to get it dropped.


Hello Maciej,

If there's a jira available for this I'd like to help get this moving, let me know next steps.

Thanks in advance.


________________________________
From: Maciej Szymkiewicz <mszymkiewicz@gmail.com><mailto:mszymkiewicz@gmail.com>
Sent: Wednesday, January 11, 2017 4:18 AM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: [PYSPARK] Python tests organization

Hi,

I can't help but wonder if there is any practical reason for keeping
monolithic test modules. These things are already pretty large (1500 -
2200 LOCs) and can only grow. Development aside, I assume that many
users use tests the same way as me, to check the intended behavior, and
largish loosely coupled modules make it harder than it should be.

If there's no rationale for that it could be a good time start thinking
about moving tests to packages and separating into modules reflecting
project structure.

--
Best,
Maciej


---------------------------------------------------------------------
ibe@spark.apache.org>



--
Maciej Szymkiewicz
"
Holden Karau <holden@pigscanfly.ca>,"Thu, 12 Jan 2017 18:34:32 +0000",Re: [PYSPARK] Python tests organization,"Saikat Kanjilal <sxk1969@hotmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I'd be happy to help with reviewing Python test improvements. Maybe make an
umbrella JIRA and do one sub components at a time?


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Thu, 12 Jan 2017 18:36:27 +0000",Re: [PYSPARK] Python tests organization,"Holden Karau <holden@pigscanfly.ca>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Maciej? LGTM, what do you think?  I can create a JIRA and drive this.


________________________________
From: Holden Karau <holden@pigscanfly.ca>
Sent: Thursday, January 12, 2017 10:34 AM
To: Saikat Kanjilal; dev@spark.apache.org
Subject: Re: [PYSPARK] Python tests organization

I'd be happy to help with reviewing Python test improvements. Maybe make an umbrella JIRA and do one sub components at a time?
















Following up, any thoughts on next steps for this?










________________________________


From: Maciej Szymkiewicz <mszymkiewicz@gmail.com<mailto:mszymkiewicz@gmail.com>>



Sent: Wednesday, January 11, 2017 10:14 AM


To: Saikat Kanjilal


Subject: Re: [PYSPARK] Python tests organization



Not yet, I want to see if there is any consensus about it. It is a lot of tedious work and I would be shame if someone started working on this just to get it dropped.
















Hello Maciej,



If there's a jira available for this I'd like to help get this moving, let me know next steps.



Thanks in advance.












________________________________


From: Maciej Szymkiewicz

<mszymkiewicz@gmail.com><mailto:mszymkiewicz@gmail.com>


Sent: Wednesday, January 11, 2017 4:18 AM


To:

dev@spark.apache.org<mailto:dev@spark.apache.org>


Subject: [PYSPARK] Python tests organization










Hi,





I can't help but wonder if there is any practical reason for keeping


monolithic test modules. These things are already pretty large (1500 -


2200 LOCs) and can only grow. Development aside, I assume that many


users use tests the same way as me, to check the intended behavior, and


largish loosely coupled modules make it harder than it should be.





If there's no rationale for that it could be a good time start thinking


about moving tests to packages and separating into modules reflecting


project structure.





--


Best,


Maciej








---------------------------------------------------------------------



dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>

















--
Maciej Szymkiewicz


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 12 Jan 2017 19:44:48 +0100",Re: [PYSPARK] Python tests organization,dev@spark.apache.org,"Sounds good, but it looks like JIRA is still down.


Personally I can look at sql.tests and see what can be done there.
Depending on the resolution of
https://issues.apache.org/jira/browse/SPARK-19160 I may have to adjust
some tests anyway.



-- 
Maciej Szymkiewicz

"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 12 Jan 2017 19:49:26 +0100",Re: [PYSPARK] Python tests organization,dev@spark.apache.org,"Thanks Holden. If you have some spare time would you take a look at
https://github.com/apache/spark/pull/16534?

It is somewhat related to
https://issues.apache.org/jira/browse/SPARK-18777 (Return UDF objects
when registering from Python).



-- 
Maciej Szymkiewicz

"
anuj ojha <anujojha5@gmail.com>,"Thu, 12 Jan 2017 18:55:18 -0600",Unsubscribe,,"Unsubscribe
"
sujith chacko <sujithchacko.2010@gmail.com>,"Fri, 13 Jan 2017 12:49:24 +0530",Limit Query Performance Suggestion,dev@spark.apache.org,"When limit is being added in the terminal of the physical plan there will
be possibility of memory bottleneck
if the limit value is too large and system will try to aggregate all the
partition limit values as part of single partition.
Description:
Eg:
create table src_temp as select * from src limit n;
== Physical Plan ==
ExecutedCommand
   +- CreateHiveTableAsSelectCommand [Database:spark}, TableName: t2,
InsertIntoHiveTable]
         +- GlobalLimit 2
            +- LocalLimit 2
               +- Project [imei#101, age#102, task#103L, num#104,
level#105, productdate#106, name#107, point#108]
                  +- SubqueryAlias hive
                     +-
Relation[imei#101,age#102,task#103L,num#104,level#105,productdate#106,name#107,point#108]
csv  |

As shown in above plan when the limit comes in terminal ,there can be two
types of performance bottlenecks.
scenario 1: when the partition count is very high and limit value is small
scenario 2: when the limit value is very large

 protected override def doExecute(): RDD[InternalRow] = {
    val locallyLimited =
child.execute().mapPartitionsInternal(_.take(limit))
    val shuffled = new ShuffledRowRDD(
      ShuffleExchange.prepareShuffleDependency(
        locallyLimited, child.output, SinglePartition, serializer))
    shuffled.mapPartitionsInternal(_.take(limit))
  }
}

As per my understanding the current algorithm first creates the
MapPartitionsRDD by applying limit from each partition, then ShuffledRowRDD
will be created by grouping data from all partitions into single partition,
this can create overhead since all partitions will return limit n data , so
while grouping there will be N partition * limit N which can be very huge,
in both scenarios mentioned above this logic can be a bottle neck.

My suggestion for handling scenario 1 where large number of partition and
limit value is small, in this case driver can create an accumulator value
and try to send to all partitions, all executer will be updating the
accumulator value based on the data fetched ,
eg: number of partition = 100, number of cores =10
tasks will be launched in a group of 10(10*10 = 100), once the first group
finishes the tasks driver will check whether the accumulator value is been
reached the limit value
if its reached then no further task will be launched to executers and the
result will be returned.

Let me know for any furthur suggestions or solution.

Thanks in advance,
Sujith
"
Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Fri, 13 Jan 2017 13:24:12 +0200",Both Spark AM and Client are trying to delete Staging Directory,dev@spark.apache.org,"Hi all!

I am a bit confused why Spark AM and Client are both trying to delete
Staging Directory.

https://github.com/apache/spark/blob/branch-2.1/yarn/
src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1110
https://github.com/apache/spark/blob/branch-2.1/yarn/
src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L233

As you can see, in case if a job was running on YARN in Cluster deployment
mode, both AM and Client will try to delete Staging directory if job
succeeded and eventually one of them will fail to do this, because the
other one already deleted the directory.

Shouldn't we add some check to Client?


Thanks,
Rostyslav
"
Sean Owen <sowen@cloudera.com>,"Fri, 13 Jan 2017 13:27:41 +0000",Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,dev <dev@spark.apache.org>,"Looks like the JIRA maintenance left a bunch of duplicate JIRAs, from
SPARK-19191 to SPARK-19202. For some reason, I can't resolve these issues,
but I can resolve others. Does anyone else see the same?

I know SPARK-19190 was similarly borked but closed by its owner.
"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Fri, 13 Jan 2017 13:54:59 +0000",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","Hello Sean,

I can't resolve SPARK-19191 to SPARK-19202 too. I believe this is a bug.
Here is JIRA Documentation which states this or similar problems - How to
Edit the Resolution of an Issue
<https://confluence.atlassian.com/jirakb/how-to-edit-the-resolution-of-an-issue-313467778.html>




-- 
--
Artur Sukhenko
"
Sean Owen <sowen@cloudera.com>,"Fri, 13 Jan 2017 14:28:56 +0000",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,"Artur Sukhenko <artur.sukhenko@gmail.com>, dev <dev@spark.apache.org>","Do you see a button to resolve other issues? you may not be able to resolve
any of them. I am a JIRA admin though, like most other devs, so should be
able to resolve anything.

Yes, I certainly know how resolving issues works but it's suddenly today
only working for a subset of issues, and I bet it's related to the JIRA
problems this week.


"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Fri, 13 Jan 2017 14:32:26 +0000",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","Yes, I can resolve/close SPARK-19214 for example.


--
Artur Sukhenko
"
Sean Owen <sowen@cloudera.com>,"Fri, 13 Jan 2017 14:35:13 +0000",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,"Artur Sukhenko <artur.sukhenko@gmail.com>, dev <dev@spark.apache.org>","Yes, I'm asking about a specific range: 19191 - 19202. These seem to be the
ones created during the downtime. Most are duplicates or incomplete.


"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Fri, 13 Jan 2017 15:47:17 +0000",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","None of JIRA issues of range 19191 -19202 have Resolve/Close Issue button.

e.g. SPARK-19214 ""Resolve Issue"" button has link
https://issues.apache.org/jira/secure/WorkflowUIDispatcher.jspa?id=13034656&action=5&atl_token=
...
Meaning that this is something to do with *Workflow*  (action 5) (as it has
*WorkflowUIDispatcher*.*jspa* in the URI, which isn't present in any of the
SPARK-19202 links)
Thus I guess you or other Spark JIRA admin should tweak workflows e.g. Working
with workflows
<https://confluence.atlassian.com/adminjiraserver072/working-with-workflows-828787890.html>




--
Artur Sukhenko
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 13 Jan 2017 08:11:52 -0800",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,Sean Owen <sowen@cloudera.com>,"I can't see the resolve button either - Maybe we can forward this to
Apache Infra and see if they can close these issues ?

Shivaram


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 13 Jan 2017 08:15:19 -0800",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"FWIW there is an option to Delete the issue (in More -> Delete).

Shivaram


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 13 Jan 2017 16:18:14 +0000",Re: Can anyone edit JIRAs SPARK-19191 to SPARK-19202?,shivaram@eecs.berkeley.edu,"A-ha. I'll try to clean them up and ask for new JIRAs to be created in some
cases.


"
Asher Krim <akrim@hubspot.com>,"Fri, 13 Jan 2017 12:23:04 -0500",Why are ml models repartition(1)'d in save methods?,dev@spark.apache.org,"Hi,

I'm curious why it's common for data to be repartitioned to 1 partition
when saving ml models:

sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)

This shows up in most ml models I've seen (Word2Vec
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/Word2Vec.scala#L314>,
PCA
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/PCA.scala#L189>,
LDA
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/clustering/LDA.scala#L605>).
Am I missing some benefit of repartitioning like this?

Thanks,
-- 
Asher Krim
Senior Software Engineer
"
Sean Owen <sowen@cloudera.com>,"Fri, 13 Jan 2017 17:25:34 +0000",Re: Why are ml models repartition(1)'d in save methods?,"Asher Krim <akrim@hubspot.com>, dev@spark.apache.org","That is usually so the result comes out in one file, not partitioned over n
files.


"
Asher Krim <akrim@hubspot.com>,"Fri, 13 Jan 2017 12:29:11 -0500",Re: Why are ml models repartition(1)'d in save methods?,Sean Owen <sowen@cloudera.com>,"But why is that beneficial? The data is supposedly quite large,
distributing it across many partitions/files would seem to make sense.




-- 
Asher Krim
Senior Software Engineer
"
Sean Owen <sowen@cloudera.com>,"Fri, 13 Jan 2017 17:42:20 +0000",Re: Why are ml models repartition(1)'d in save methods?,Asher Krim <akrim@hubspot.com>,"You're referring to code that serializes models, which are quite small. For
example a PCA model consists of a few principal component vector. It's a
Dataset of just one element being saved here. It's re-using the code path
normally used to save big data sets, to output 1 file with 1 thing as
Parquet.


"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 13 Jan 2017 18:02:42 +0000",Re: Why are ml models repartition(1)'d in save methods?,"Sean Owen <sowen@cloudera.com>, Asher Krim <akrim@hubspot.com>","Yup - it's because almost all model data in spark ML (model coefficients)
is ""small"" - i.e. Non distributed.

If you look at ALS you'll see there is no repartitioning since the factor
dataframes can be large

"
Asher Krim <akrim@hubspot.com>,"Fri, 13 Jan 2017 15:16:55 -0500",Re: Why are ml models repartition(1)'d in save methods?,Nick Pentreath <nick.pentreath@gmail.com>,"I guess it depends on the definition of ""small"". A Word2vec model with
vectorSize=300 and vocabulary=3m takes nearly 4gb. While it does fit on a
single machine (so isn't really ""big"" data), I don't see the benefit in
would want the model to be distributed:
* avoids shuffling of data to one executor
* allows the whole cluster to participate in saving the model
* avoids rpc issues (http://stackoverflow.com/questions/40842736/spark-
word2vecmodel-exceeds-max-rpc-size-for-saving)
* ""feature parity"" with mllib (issues with one large model file already
solved for mllib in SPARK-11994
<https://issues.apache.org/jira/browse/SPARK-11994>)



"
Sean Owen <sowen@cloudera.com>,"Fri, 13 Jan 2017 20:55:29 +0000",Re: Why are ml models repartition(1)'d in save methods?,"Asher Krim <akrim@hubspot.com>, Nick Pentreath <nick.pentreath@gmail.com>","Yes, certainly debatable for word2vec. You have a good point that this
could overrun the 2GB limit if the model is one big datum, for large but
not crazy models. This model could probably easily be serialized as
individual vectors in this case. It would introduce a
backwards-compatibility issue but it's possible to read old and new
formats, I believe.


"
Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Fri, 13 Jan 2017 12:44:04 +0200",Both Spark AM and Client are trying to delete Staging Directory,dev@spark.apache.org,"Hi all!

I am a bit confused why Spark AM and Client are both trying to delete
Staging Directory.

https://github.com/apache/spark/blob/branch-2.1/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1110
https://github.com/apache/spark/blob/branch-2.1/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L233

As you can see, in case if a job was running on YARN in Cluster deployment
mode, both AM and Client will try to delete Staging directory if job
succeeded and eventually one of them will fail to do this, because the
other one already deleted the directory.

Shouldn't we add some check to Client?


Thanks,
Rostyslav
"
Jacek Laskowski <jacek@japila.pl>,"Sat, 14 Jan 2017 11:02:30 +0100",What about removing TaskContext#getPartitionId?,dev <dev@spark.apache.org>,"Hi,

Just noticed that TaskContext#getPartitionId [1] is not used and
moreover the scaladoc is incorrect:

""It will return 0 if there is no active TaskContext for cases like
local execution.""

since there are no local execution. (I've seen the comment in the code
before but can't find it now).

The reason to remove it is that Structured Streaming is giving new
birth to the method in ForeachSink [2] which may look like a
""resurrection"".

There's simply TaskContext.get.partitionId.

What do you think?

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/TaskContext.scala#L41
[2] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala#L50

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 14 Jan 2017 03:47:46 -0800",Re: What about removing TaskContext#getPartitionId?,Jacek Laskowski <jacek@japila.pl>,"Since TaskContext.getPartitionId is part of the public api, it cant be
removed as user code can be depending on it (unless we go through a
deprecation process for it).

Regards,
Mridul



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 14 Jan 2017 16:55:02 +0100",Re: What about removing TaskContext#getPartitionId?,Mridul Muralidharan <mridul@gmail.com>,"Hi,

Yes, correct. I was too forceful in discouraging people using it. I think
@deprecated would be a right direction.

What should be the next step? I think I should file an JIRA so it's in a
release notes. Correct?

I was very surprised to have noticed its resurrection in the very latest
module of Spark - Structured Streaming - that will be an inspiration for
others to learn Spark.

Jacek


"
Sean Owen <sowen@cloudera.com>,"Sat, 14 Jan 2017 17:03:14 +0000",Re: What about removing TaskContext#getPartitionId?,"Jacek Laskowski <jacek@japila.pl>, Mridul Muralidharan <mridul@gmail.com>","It doesn't strike me as something that's problematic to use. There are a
thousand things in the API that, maybe in hindsight, could have been done
differently, but unless something is bad practice or superseded by another
superior mechanism, it's probably not worth the bother for maintainers or
users. I don't see much problem with this, and it's actually used by Spark.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Sat, 14 Jan 2017 11:37:57 -0800",Re: Both Spark AM and Client are trying to delete Staging Directory,Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Are you actually seeing a problem or just questioning the code?

I have never seen a situation where there's a failure because of that
part of the current code.




-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Sat, 14 Jan 2017 11:42:04 -0800",Re: Both Spark AM and Client are trying to delete Staging Directory,Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"scala> org.apache.hadoop.fs.FileSystem.getLocal(sc.hadoopConfiguration)
res0: org.apache.hadoop.fs.LocalFileSystem =
org.apache.hadoop.fs.LocalFileSystem@3f84970b

scala> res0.delete(new org.apache.hadoop.fs.Path(""/tmp/does-not-exist""), true)
res3: Boolean = false

Does that explain your confusion?





-- 
Marcelo

---------------------------------------------------------------------


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Sat, 14 Jan 2017 20:01:27 +0000",Re: [PYSPARK] Python tests organization,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","https://issues.apache.org/jira/browse/SPARK-19224



Maciej/Holden,

If its ok for I can come up with a proposal for reorganization and add the proposal to the JIRA as next steps before we break up the work?

Thanks


________________________________
From: Maciej Szymkiewicz <mszymkiewicz@gmail.com>
Sent: Thursday, January 12, 2017 10:44 AM
To: dev@spark.apache.org
Subject: Re: [PYSPARK] Python tests organization


Sounds good, but it looks like JIRA is still down.


Personally I can look at sql.tests and see what can be done there. Depending on the resolution of https://issues.apache.org/jira/browse/SPARK-19160 I may have to adjust some tests anyway.


Maciej? LGTM, what do you think?  I can create a JIRA and drive this.


________________________________
From: Holden Karau <holden@pigscanfly.ca><mailto:holden@pigscanfly.ca>
Sent: Thursday, January 12, 2017 10:34 AM
To: Saikat Kanjilal; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: [PYSPARK] Python tests organization

I'd be happy to help with reviewing Python test improvements. Maybe make an umbrella JIRA and do one sub components at a time?
















Following up, any thoughts on next steps for this?










________________________________


From: Maciej Szymkiewicz <mszymkiewicz@gmail.com<mailto:mszymkiewicz@gmail.com>>



Sent: Wednesday, January 11, 2017 10:14 AM


To: Saikat Kanjilal


Subject: Re: [PYSPARK] Python tests organization



Not yet, I want to see if there is any consensus about it. It is a lot of tedious work and I would be shame if someone started working on this just to get it dropped.
















Hello Maciej,



If there's a jira available for this I'd like to help get this moving, let me know next steps.



Thanks in advance.












________________________________


From: Maciej Szymkiewicz

<mszymkiewicz@gmail.com><mailto:mszymkiewicz@gmail.com>


Sent: Wednesday, January 11, 2017 4:18 AM


To:

dev@spark.apache.org<mailto:dev@spark.apache.org>


Subject: [PYSPARK] Python tests organization










Hi,





I can't help but wonder if there is any practical reason for keeping


monolithic test modules. These things are already pretty large (1500 -


2200 LOCs) and can only grow. Development aside, I assume that many


users use tests the same way as me, to check the intended behavior, and


largish loosely coupled modules make it harder than it should be.





If there's no rationale for that it could be a good time start thinking


about moving tests to packages and separating into modules reflecting


project structure.





--


Best,


Maciej








---------------------------------------------------------------------



dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>

















--
Maciej Szymkiewicz




--
Maciej Szymkiewicz
"
Jacek Laskowski <jacek@japila.pl>,"Sat, 14 Jan 2017 21:05:58 +0100",Re: What about removing TaskContext#getPartitionId?,Sean Owen <sowen@cloudera.com>,"Hi Sean,

Can you elaborate on "" it's actually used by Spark""? Where exactly?
I'd like to be corrected.

What about the scaladoc? Since the method's a public API, I think it
should be fixed, shouldn't it?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Fei Hu <hufei68@gmail.com>,"Sat, 14 Jan 2017 18:58:54 -0500",Equally split a RDD partition into two partition at the same node,"user@spark.apache.org, dev@spark.apache.org, 
	""user@hadoop.apache.org"" <user@hadoop.apache.org>","Dear all,

I want to equally divide a RDD partition into two partitions. That means,
the first half of elements in the partition will create a new partition,
and the second half of elements in the partition will generate another new
partition. But the two new partitions are required to be at the same node
with their parent partition, which can help get high data locality.

Is there anyone who knows how to implement it or any hints for it?

Thanks in advance,
Fei
"
Anastasios Zouzias <zouzias@gmail.com>,"Sun, 15 Jan 2017 09:58:36 +0100",Re: Equally split a RDD partition into two partition at the same node,Fei Hu <hufei68@gmail.com>,"Hi Fei,

How you tried coalesce(numPartitions: Int, shuffle: Boolean = false) ?

https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L395

coalesce is mostly used for reducing the number of partitions before
writing to HDFS, but it might still be a narrow dependency (satisfying your
requirements) if you increase the # of partitions.

Best,
Anastasios




-- 
-- Anastasios Zouzias
<azo@zurich.ibm.com>
"
Sean Owen <sowen@cloudera.com>,"Sun, 15 Jan 2017 11:20:44 +0000",Re: What about removing TaskContext#getPartitionId?,Jacek Laskowski <jacek@japila.pl>,"As you mentioned, it's called in ForeachSink. I don't know that the
scaladoc is wrong. You're saying something else, that there's no such thing
as local execution. I confess I don't know if that's true? but the doc
isn't wrong in that case, really.

More broadly, I just don't think this type of thing is worth this small
amount of attention.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 15 Jan 2017 06:19:54 -0700 (MST)",Re: Both Spark AM and Client are trying to delete Staging Directory,dev@spark.apache.org,"
Hi,

Will it be a problem if the staging directory is already deleted? Because
even the directory doesn't exist, fs.delete(stagingDirPath, true) won't
cause failure but just return false.


Rostyslav Sotnychenko wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Fei Hu <hufei68@gmail.com>,"Sun, 15 Jan 2017 11:36:09 -0500",Re: Equally split a RDD partition into two partition at the same node,Rishi Yadav <rishi@infoobjects.com>,"Hi Rishi,

Thanks for your reply! The RDD has 24 partitions, and the cluster has a
master node + 24 computing nodes (12 cores per node). Each node will have a
partition, and I want to split each partition to two sub-partitions on the
same node to improve the parallelism and achieve high data locality.

Thanks,
Fei



"
Fei Hu <hufei68@gmail.com>,"Sun, 15 Jan 2017 11:39:58 -0500",Re: Equally split a RDD partition into two partition at the same node,zouzias@cs.toronto.edu,"Hi Anastasios,

Thanks for your reply. If I just increase the numPartitions to be twice
larger, how coalesce(numPartitions: Int, shuffle: Boolean = false) keeps
the data locality? Do I need to define my own Partitioner?

Thanks,
Fei


"
Anastasios Zouzias <zouzias@gmail.com>,"Sun, 15 Jan 2017 18:21:53 +0100",Re: Equally split a RDD partition into two partition at the same node,Fei Hu <hufei68@gmail.com>,"Hi Fei,

I looked at the code of CoalescedRDD and probably what I suggested will not
work.

Speaking of which, CoalescedRDD is private[spark]. If this was not the
case, you could set balanceSlack to 1, and get what you requested, see

https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala#L75

Maybe you could try to use the CoalescedRDD code to implement your
requirement.

Good luck!
Cheers,
Anastasios





-- 
-- Anastasios Zouzias
<azo@zurich.ibm.com>
"
Fei Hu <hufei68@gmail.com>,"Sun, 15 Jan 2017 13:37:51 -0500",Re: Equally split a RDD partition into two partition at the same node,zouzias@cs.toronto.edu,"Hi Anastasios,

Thanks for your information. I will look into the CoalescedRDD code.

Thanks,
Fei


"
Hosun Lee <enowys@gmail.com>,"Mon, 16 Jan 2017 09:35:19 +0900",unsubscribe,dev@spark.apache.org,"*unsubscribe*
"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 15 Jan 2017 20:36:21 -0700 (MST)",Re: Limit Query Performance Suggestion,dev@spark.apache.org,"
Hi Sujith,

Thanks for suggestion.

The codes you quoted are from `CollectLimitExec` which will be in the plan
if a logical `Limit` is the final operator in an logical plan. But in the
physical plan you showed, there are `GlobalLimit` and `LocalLimit` for the
logical `Limit` operation, so the `doExecute` method of `CollectLimitExec`
will not be executed.

In the case `CollectLimitExec` is the final physical operation, its
`executeCollect` will be executed and delegate to `SparkPlan.executeTake`
which is optimized to only retrieved required number of rows back to the
driver. So when using `limit n` with a huge partition number it should not
be a problem.

In the case `GlobalLimit` and `LocalLimit` are the final physical
operations, your concern is that when returning `n` rows from `N` partitions
and `N` is huge, the total `n * N` rows will cause heavy memory pressure on
the driver. I am not sure if you really observe this problem or you just
think it might be a problem. In this case, there will be a shuffle exchange
between `GlobalLimit` and `LocalLimit` to retrieve data from all partitions
to one partition. In `GlobalLimit` we will only take the required number of
rows from the input iterator which really pulls data from local blocks and
remote blocks. Due to the use of iterator approach, I think when we get the
enough rows in `GlobalLimit`, we won't continue to consume the input
iterator and pull more data back. So I don't think your concern will be a
problem.



sujith71955 wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 15 Jan 2017 21:01:52 -0700 (MST)","Re: Equally split a RDD partition into two partition at the same
 node",dev@spark.apache.org,"
Hi,

When calling `coalesce` with `shuffle = false`, it is going to produce at
most min(numPartitions, previous RDD's number of partitions). So I think it
can't be used to double the number of partitions.


Anastasios Zouzias wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Fei Hu <hufei68@gmail.com>,"Mon, 16 Jan 2017 00:20:22 -0500",Re: Equally split a RDD partition into two partition at the same node,Liang-Chi Hsieh <viirya@gmail.com>,"Hi Liang-Chi,

Yes, you are right. I implement the following solution for this problem,
and it works. But I am not sure if it is efficient:

I double the partitions of the parent RDD, and then use the new partitions
and parent RDD to construct the target RDD. In the compute() function of
the target RDD, I use the input partition to get the corresponding parent
partition, and get the half elements in the parent partitions as the output
of the computing function.

Thanks,
Fei


"
Chetan Khatri <chetan.opensource@gmail.com>,"Mon, 16 Jan 2017 11:01:56 +0530",Re: Error at starting Phoenix shell with HBase,"user@phoenix.apache.org, Spark Dev List <dev@spark.apache.org>","Any updates for the above error guys ?



"
<jasbir.sing@accenture.com>,"Mon, 16 Jan 2017 05:37:36 +0000",RE: Equally split a RDD partition into two partition at the same node,"<hufei68@gmail.com>, <zouzias@cs.toronto.edu>","Hi,

Coalesce is used to decrease the number of partitions. If you give the value of numPartitions greater than the current partition, I don’t think RDD number of partitions will be increased.

Thanks,
Jasbir

From: Fei Hu [mailto:hufei68@gmail.com]
Sent: Sunday, January 15, 2017 10:10 PM
To: zouzias@cs.toronto.edu
Cc: user @spark <user@spark.apache.org>; dev@spark.apache.org
Subject: Re: Equally split a RDD partition into two partition at the same node

Hi Anastasios,

Thanks for your reply. If I just increase the numPartitions to be twice larger, how coalesce(numPartitions: Int, shuffle: Boolean = false) keeps the data locality? Do I need to define my own Partitioner?

Thanks,
Fei

On Sun, Jan 15, 2017 at 3:as@gmail.com>> wrote:
Hi Fei,

How you tried coalesce(numPartitions: Int, shuffle: Boolean = false) ?

https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L395<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spark_blob_branch-2D1.6_core_src_main_scala_org_apache_spark_rdd_RDD.scala-23L395&d=DgMFaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=7scIIjM0jY9x3fjvY6a_yERLxMA2NwA8l0DnuyrL6yA&m=bFMBTBwSwMOFRd7Or6fF0sQOH87UIhmuUqEO9UkxPIY&s=qNa3MyvKhIDlXHtxm3s0DZJRZaSWIHpaNhcS86GEQow&e=>

coalesce is mostly used for reducing the number of partitions before writing to HDFS, but it might still be a narrow dependency (satisfying your requirements) if you increase the # of partitions.

Best,
Anastasios

On Sun, Jan 15, 2017 at 12:58 AM, Fei Hu <hufei68@gmail.com<mailto:hufei68@gmail.com>> wrote:
Dear all,

I want to equally divide a RDD partition into two partitions. That means, the first half of elements in the partition will create a new partition, and the second half of elements in the partition will generate another new partition. But the two new partitions are required to be at the same node with their parent partition, which can help get high data locality.

Is there anyone who knows how to implement it or any hints for it?

Thanks in advance,
Fei




--
-- Anastasios Zouzias


________________________________

This message is for the designated recipient only and may contain privileged, proprietary, or otherwise confidential information. If you have received it in error, please notify the sender immediately and delete the original. Any other use of the e-mail by you is prohibited. Where allowed by local law, electronic communications with Accenture and its affiliates, including e-mail and instant messaging (including content), may be scanned by our systems for the purposes of information security and assessment of internal compliance with Accenture policy.
______________________________________________________________________________________

www.accenture.com
"
Fei Hu <hufei68@gmail.com>,"Mon, 16 Jan 2017 00:42:35 -0500",Re: Equally split a RDD partition into two partition at the same node,jasbir.sing@accenture.com,"Hi Jasbir,

Yes, you are right. Do you have any idea about my question?

Thanks,
Fei


t think
s
_spark_blob_branch-2D1.6_core_src_main_scala_org_apache_spark_rdd_RDD.scala-23L395&d=DgMFaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=7scIIjM0jY9x3fjvY6a_yERLxMA2NwA8l0DnuyrL6yA&m=bFMBTBwSwMOFRd7Or6fF0sQOH87UIhmuUqEO9UkxPIY&s=qNa3MyvKhIDlXHtxm3s0DZJRZaSWIHpaNhcS86GEQow&e=>
ur
w
ve
,
d
"
Liang-Chi Hsieh <viirya@gmail.com>,"Mon, 16 Jan 2017 01:00:43 -0700 (MST)","Re: Equally split a RDD partition into two partition at the same
 node",dev@spark.apache.org,"
Hi Fei,

I think it should work. But you may need to add few logic in compute() to
decide which half of the parent partition is needed to output. And you need
to get the correct preferred locations for the partitions sharing the same
parent partition.


Fei Hu wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Mon, 16 Jan 2017 03:35:18 -0700 (MST)",spark support on windows,dev@spark.apache.org,"Hi,
In the documentation it says spark is supported on windows.
The problem, however, is that the documentation description on windows is lacking. There are sources (such as https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-tips-and-tricks-running-spark-windows.html and many more) which explain how to make spark run on windows, however, they all involve downloading a third party winutil.exe file.
Since this file is downloaded from a repository belonging to a private person, this can be an issue (e.g. getting approval to install on a company computer can be an issue).
There are tons of jira tickets on the subject (most are marked as duplicate or not a problem), however, I believe that if we say spark is supported on windows there should be a clear explanation on how to run it and one shouldn't have to use executable from a private person.

If indeed using winutil.exe is the correct solution, I believe it should be bundled to the spark binary distribution along with clear instructions on how to add it.
Assaf.




--"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 16 Jan 2017 20:06:01 +0900",Re: spark support on windows,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Hi,

I just looked through Jacek's page and I believe that is the correct way.

That seems to be a Hadoop library specific issue[1]. Up to my
knowledge, winutils and the binaries in the private repo
 are built by a Hadoop PMC member on a dedicated Windows VM which I believe
are pretty trustable.
This can be compile from the source. If you think it is not reliable and
not safe, you can go and build it by your self.

I agree it would be great if there are documentation about this as we have
a weak promise for Windows[2] and
I believe it always require some overhead to install Spark on Windows.
FWIW, In case of SparkR, there are some
documentation [3].

For bundling it, it seems even Hadoop itself does not include this in their
releases. I think documentation would be
enough.

For many JIRAs, at least I am resolving it one by one.

I hope my answer is helpful and makes sense.

Thanks.


[1] https://wiki.apache.org/hadoop/WindowsProblems
[2]
https://github.com/apache/spark/blob/f3a3fed76cb74ecd0f46031f337576ce60f54fb2/docs/index.md
[3] https://github.com/apache/spark/blob/master/R/WINDOWS.md


2017-01-16 19:35 GMT+09:00 assaf.mendelson <assaf.mendelson@rsa.com>:

ny
on-windows-tp20614.html>
"
Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Mon, 16 Jan 2017 14:51:24 +0200",Re: Both Spark AM and Client are trying to delete Staging Directory,Liang-Chi Hsieh <viirya@gmail.com>,"Thanks all!

I was using another DFS instead of HDFS, which was logging an error when
fs.delete got called on non-existing path.
In Spark 2.0.1 which I was using previously, everything was working fine
because existence of an additional check that was made prior to deleting.
However that check got removed in 2.1 (SPARK-16736
<https://issues.apache.org/jira/browse/SPARK-16736>, commit
<https://github.com/apache/spark/pull/14371/files#diff-b050df3f55b82065803d6e83453b9706>),
so I started seeing an error from my DFS.

Its not a problem in any way (i.e. it does not affect Spark job in any
way), so everything is fine. I just wanted to make sure its not a Spark
issue.


Thanks,
Rostyslav


"
Fei Hu <hufei68@gmail.com>,"Mon, 16 Jan 2017 10:45:46 -0500",Re: Equally split a RDD partition into two partition at the same node,Liang-Chi Hsieh <viirya@gmail.com>,"Hi Liang-Chi,

Yes, the logic split is needed in compute(). The preferred locations can be
derived from the customized Partition class.

Thanks for your help!

Cheers,
Fei



"
Asher Krim <akrim@hubspot.com>,"Mon, 16 Jan 2017 11:41:00 -0500",Re: Why are ml models repartition(1)'d in save methods?,Sean Owen <sowen@cloudera.com>,"Cool, thanks!

Jira: https://issues.apache.org/jira/browse/SPARK-19247
PR: https://github.com/apache/spark/pull/16607

I think the LDA model has the exact same issues - currently the
`topicsMatrix` (which is on order of numWords*k, 4GB for numWords=3m and
k=1000) is saved as a single element in a case class. We should probably
address this in another issue.


"
Chetan Khatri <chetan.opensource@gmail.com>,"Tue, 17 Jan 2017 00:48:23 +0530",About saving DataFrame to Hive 1.2.1 with Spark 2.0.1,"Spark Dev List <dev@spark.apache.org>, user <user@spark.apache.org>","Hello Community,

I am struggling to save Dataframe to Hive Table,

Versions:

Hive 1.2.1
Spark 2.0.1

*Working code:*

/*
@Author: Chetan Khatri
/* @Author: Chetan Khatri Description: This Scala script has written for
HBase to Hive module, which reads table from HBase and dump it out to Hive
*/ import it.nerdammer.spark.hbase._ import org.apache.spark.sql.Row import
org.apache.spark.sql.types.StructType import
org.apache.spark.sql.types.StructField import
org.apache.spark.sql.types.StringType import
org.apache.spark.sql.SparkSession // Approach 1: // Read HBase Table val
hBaseRDD = sc.hbaseTable[(Option[String], Option[String], Option[String],
Option[String], Option[String])](""university"").select(""stid"",
""name"",""subject"",""grade"",""city"").inColumnFamily(""emp"") // Iterate HBaseRDD
and generate RDD[Row] val rowRDD = hBaseRDD.map(i =>
Row(i._1.get,i._2.get,i._3.get,i._4.get,i._5.get)) // Create sqlContext for
createDataFrame method val sqlContext = new
org.apache.spark.sql.SQLContext(sc) // Create Schema Structure object
empSchema { val stid = StructField(""stid"", StringType) val name =
StructField(""name"", StringType) val subject = StructField(""subject"",
StringType) val grade = StructField(""grade"", StringType) val city =
StructField(""city"", StringType) val struct = StructType(Array(stid, name,
subject, grade, city)) } import sqlContext.implicits._ // Create DataFrame
with rowRDD and Schema structure val stdDf =
sqlContext.createDataFrame(rowRDD,empSchema.struct); // Importing Hive
import org.apache.spark.sql.hive // Enable Hive with Hive warehouse in
SparkSession val spark = SparkSession.builder().appName(""Spark Hive
Example"").config(""spark.sql.warehouse.dir"",
""/usr/local/hive/warehouse/"").enableHiveSupport().getOrCreate() // Saving
Dataframe to Hive Table Successfully.
stdDf.write.mode(""append"").saveAsTable(""employee"") // Approach 2 : Where
error comes import spark.implicits._ import spark.sql sql(""use default"")
sql(""create table employee(stid STRING, name STRING, subject STRING, grade
STRING, city STRING)"") scala> sql(""show TABLES"").show()
+---------+-----------+ |tableName|isTemporary| +---------+-----------+ |
employee| false| +---------+-----------+
stdDf.write.mode(""append"").saveAsTable(""employee"") ERROR Exception:
org.apache.spark.sql.AnalysisException: Saving data in MetastoreRelation
default, employee is not supported.; at
org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:221)
at
org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
at
org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
at
org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
at
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
at
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
at
org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at
org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114) at
org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
at
org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
at
org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:378)
at
org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
... 56 elided Questions: At Approach 1, It stores data where hive table is
not previously created, when i say saveAsTable it automatically creates for
me and next time it also appends data into that, How to store data in
previously created tables ?
It also gives warning WARN metastore.HiveMetaStore: Location:
file:/usr/local/spark/spark-warehouse/employee specified for non-external
table:employee but i have already provided path of HiveMetaStore then why
it is storing in spark's warehouse meta-store.

Hive-setup done with reference to:
http://mitu.co.in/wp-content/uploads/2015/12/Hive-Installation-on-Ubuntu-14.04-and-Hadoop-2.6.3.pdf
and it's working well, I could not change the Hive version, it must be 1.2.1

Thank you.
"
Fei Hu <hufei68@gmail.com>,"Mon, 16 Jan 2017 15:03:37 -0500",Re: Equally split a RDD partition into two partition at the same node,Pradeep Gollakota <pradeepg26@gmail.com>,"Hi Pradeep,

That is a good idea. My customized RDDs are similar to the NewHadoopRDD. If
we have billions of InputSplit, will it be bottlenecked for the
performance? That is, will too many data need to be transferred from master
node to computing nodes by networking?

Thanks,
Fei


t
t think
?
he_spark_blob_branch-2D1.6_core_src_main_scala_org_apache_spark_rdd_RDD.scala-23L395&d=DgMFaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=7scIIjM0jY9x3fjvY6a_yERLxMA2NwA8l0DnuyrL6yA&m=bFMBTBwSwMOFRd7Or6fF0sQOH87UIhmuUqEO9UkxPIY&s=qNa3MyvKhIDlXHtxm3s0DZJRZaSWIHpaNhcS86GEQow&e=>
your
te
 the
have
he
ed
es,
ned
of
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 16 Jan 2017 21:54:13 +0000",Re: spark support on windows,assaf.mendelson <assaf.mendelson@rsa.com>,"
On 16 Jan 2017, at 10:35, assaf.mendelson <assaf.mendelson@rsa.com<mailto:assaf.mendelson@rsa.com>> wrote:

Hi,
In the documentation it says spark is supported on windows.
The problem, however, is that the documentation description on windows is lacking. There are sources (such as https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-tips-and-tricks-running-spark-windows.html and many more) which explain how to make spark run on windows, however, they all involve downloading a third party winutil.exe file.
Since this file is downloaded from a repository belonging to a private person,

A repository belonging to me, stevel@apache.org<mailto:stevel@apache.org>

this can be an issue (e.g. getting approval to install on a company computer can be an issue).


An a committer on the Hadoop PMC, those signed artifacts are no less trustworthy than anything you get from the ASF itself. It's clean built off a windows VM that is only ever used for build/test of Hadoop code, no other use at all; the VM is powered off most of its life. This actually makes it less of a security risk than the main desktop. And you can check the GPG signature of the artifacts to see they've not been tampered with.

There are tons of jira tickets on the subject (most are marked as duplicate or not a problem), however, I believe that if we say spark is supported on windows there should be a clear explanation on how to run it and one shouldn’t have to use executable from a private person.

While I recognise your concerns, if I wanted to run code on your machines, rest assured, I wouldn't do it in such an obvious way.

I'd do it via transitive maven artifacts with a harmless name like ""org.example.xml-unit-diags"" which would so something useful except in the special case that is' running on code in your subnet, get a patch a pom.xml to pull it into org.apache.hadoop somewhere, release a version of hadoop with that dependency, then wait for it to propagate downstream into everything, including all those server farms running linux only.

Writing a malicious windows native excutable would require me to write C/C++ windows code, and I don't want to go there.

Of course, if I did any of these I'd be in trouble when caught, lose my job, never be trusted to submit a line of code to any OSS project, lose all my friends, etc, etc. I have nothing to gain by doing so.

If you really don't trust me the instructions for building it are up online; build  a windows system for compiuling hadoop, check out the branch and then go

  mvn -T 1C package -Pdist -Dmaven.javadoc.skip=true -DskipTests

Or go to hortonworks.com<http://hortonworks.com>, download the windows version and lift the windows binaries. Same thing, built by a colleague-managed release VM.


If indeed using winutil.exe is the correct solution, I believe it should be bundled to the spark binary distribution along with clear instructions on how to add it.
I recognise that it is good to question the provenance of every line of code executed on machines you care about. I am reasonably confident as so the quality of this code; given the fact it was a checkout & build of the ASF tagged release, then signed my me, it'd either need my VM corrupted, my VM's feed from the ASF HTTPS repo subverted by a fake SSL cert, or by someone getting hold of my GPG key and github keys and uploading something malicious in my name. Interestingly, that is a vulnerability, one I covered last year in my ""Household infosec in a post-sony era: talk: https://www.youtube.com/watch?v=tcRjG1CCrPs

You'll be pleased to know that the relevant keys now live on a yubikey, so even malicious code executed on my desktop cannot get the secrets off the (encrypted) local drive. It'd need physical access to the key, and I'd notice it was missing, revoke everything, etc, etc, making the risk of my keys being stolen low. That leaves the general problem of ""our entire build process is based on the assumption that we truest the maven repositories and the people who wrote the JARs""

That's a far more serious problem than the provenance of a single exe file on github

-Steve
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 16 Jan 2017 22:00:04 +0000",Re: Both Spark AM and Client are trying to delete Staging Directory,Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"

Thanks all!

I was using another DFS instead of HDFS, which was logging an error when fs.delete got called on non-existing path.


really? Whose DFS, if you don't mind me asking? I'm surprised they logged that delete() of a missing path, as it's not entirely uncommon to happen during cleanup

In Spark 2.0.1 which I was using previously, everything was working fine because existence of an additional check that was made prior to deleting. However that check got removed in 2.1 (SPARK-16736<https://issues.apache.org/jira/browse/SPARK-16736>, commit<https://github.com/apache/spark/pull/14371/files#diff-b050df3f55b82065803d6e83453b9706>), so I started seeing an error from my DFS.

Its not a problem in any way (i.e. it does not affect Spark job in any way), so everything is fine. I just wanted to make sure its not a Spark issue.


Thanks,
Rostyslav



No, not a problem. Really that whole idea of having delete() return true/false is pretty useless: nobody really knows what it means when it returns false. It should just have been void() wth an exception thrown if something actually failed. That's essentially what they all do, though I've never seen any which complains about the situation.

mkdirs(), now there's one to fear. Not even the java.io<http://java.io> API clearly defines what ""false"" coming back from there means, as it can mean both ""ther'es a directory there, so I didnt' do any work"", or ""there's a file/symlnk/mount point/device there which is a probably a serious problem""
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 16 Jan 2017 22:05:22 +0000",Re: spark support on windows,Hyukjin Kwon <gurwls223@gmail.com>,"
On 16 Jan 2017, at 11:06, Hyukjin Kwon <gurwls223@gmail.com<mailto:gurwls223@gmail.com>> wrote:

Hi,

I just looked through Jacek's page and I believe that is the correct way.

That seems to be a Hadoop library specific issue[1]. Up to my knowledge, winutils and the binaries in the private repo
 are built by a Hadoop PMC member on a dedicated Windows VM which I believe are pretty trustable.

thank you :)

I also check out and build the specific git commit SHA1 of the release, not any (moveable) tag, so we have identical sources for my builds as the matching releases.

This can be compile from the source. If you think it is not reliable and not safe, you can go and build it by your self.

I agree it would be great if there are documentation about this as we have a weak promise for Windows[2] and
I believe it always require some overhead to install Spark on Windows. FWIW, In case of SparkR, there are some
documentation [3].

For bundling it, it seems even Hadoop itself does not include this in their releases. I think documentation would be
enough.

Really, Hadoop itself should be doing the release of the windows binaries. It's just it complicates the release process as the linux build/test/release would have to be done, then somehow the windows stuff would need to be done on another machine and mixed in. That's the real barrier: extra work. That said, maybe it's time.




For many JIRAs, at least I am resolving it one by one.

I hope my answer is helpful and makes sense.

Thanks.


[1] https://wiki.apache.org/hadoop/WindowsProblems
[2] https://github.com/apache/spark/blob/f3a3fed76cb74ecd0f46031f337576ce60f54fb2/docs/index.md
[3] https://github.com/apache/spark/blob/master/R/WINDOWS.md


2017-01-16 19:35 GMT+09:00 assaf.mendelson <assaf.mendelson@rsa.com<mailto:assaf.mendelson@rsa.com>>:
Hi,
In the documentation it says spark is supported on windows.
The problem, however, is that the documentation description on windows is lacking. There are sources (such as https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-tips-and-tricks-running-spark-windows.html and many more) which explain how to make spark run on windows, however, they all involve downloading a third party winutil.exe file.
Since this file is downloaded from a repository belonging to a private person, this can be an issue (e.g. getting approval to install on a company computer can be an issue).
There are tons of jira tickets on the subject (most are marked as duplicate or not a problem), however, I believe that if we say spark is supported on windows there should be a clear explanation on how to run it and one shouldn’t have to use executable from a private person.

If indeed using winutil.exe is the correct solution, I believe it should be bundled to the spark binary distribution along with clear instructions on how to add it.
Assaf.

________________________________
View this message in context: spark support on windows<http://apache-spark-developers-list.1001551.n3.nabble.com/spark-support-on-windows-tp20614.html>
Sent from the Apache Spark Developers List mailing list archive<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com<http://Nabble.com>.


"
Pradeep Gollakota <pradeepg26@gmail.com>,"Mon, 16 Jan 2017 11:07:37 -0800",Re: Equally split a RDD partition into two partition at the same node,Fei Hu <hufei68@gmail.com>,"Usually this kind of thing can be done at a lower level in the InputFormat
usually by specifying the max split size. Have you looked into that
possibility with your InputFormat?


t think
ps
e_spark_blob_branch-2D1.6_core_src_main_scala_org_apache_spark_rdd_RDD.scala-23L395&d=DgMFaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=7scIIjM0jY9x3fjvY6a_yERLxMA2NwA8l0DnuyrL6yA&m=bFMBTBwSwMOFRd7Or6fF0sQOH87UIhmuUqEO9UkxPIY&s=qNa3MyvKhIDlXHtxm3s0DZJRZaSWIHpaNhcS86GEQow&e=>
our
,
ew
e
ave
e
d
s,
ed
f
"
Chetan Khatri <chetan.opensource@gmail.com>,"Tue, 17 Jan 2017 10:30:12 +0530",Re: About saving DataFrame to Hive 1.2.1 with Spark 2.0.1,"Spark Dev List <dev@spark.apache.org>, user <user@spark.apache.org>","Hello Spark Folks,

Other weird experience i have with Spark with SqlContext is when i created
Dataframe sometime this error throws exception and sometime not !

scala> import sqlContext.implicits._
import sqlContext.implicits._

scala> val stdDf = sqlContext.createDataFrame(rowRDD,empSchema.struct);
17/01/17 10:27:15 ERROR metastore.RetryingHMSHandler:
AlreadyExistsException(message:Database default already exists)
at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
at com.sun.proxy.$Proxy21.create_database(Unknown Source)
at
org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
at
org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
at
org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
at
org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
at
org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
at
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
at
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
at
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
at
org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
at
org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
at
org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
at
org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
at
org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
at
org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
at
org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
at
org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
at
org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
at
org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
at
org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:542)
at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:302)
at org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:337)
at
$line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:43)
at
$line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:48)
at
$line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:50)
at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:52)
at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:54)
at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:56)
at $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:58)
at $line28.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:60)
at $line28.$read$$iw$$iw$$iw$$iw.<init>(<console>:62)
at $line28.$read$$iw$$iw$$iw.<init>(<console>:64)
at $line28.$read$$iw$$iw.<init>(<console>:66)
at $line28.$read$$iw.<init>(<console>:68)
at $line28.$read.<init>(<console>:70)
at $line28.$read$.<init>(<console>:74)
at $line28.$read$.<clinit>(<console>)
at $line28.$eval$.$print$lzycompute(<console>:7)
at $line28.$eval$.$print(<console>:6)
at $line28.$eval.$print(<console>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
at
scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
at
scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
at
scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
at
scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
at
scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:807)
at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:681)
at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:395)
at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:415)
at
scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923)
at
scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
at
scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
at
scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)
at org.apache.spark.repl.Main$.doMain(Main.scala:68)
at org.apache.spark.repl.Main$.main(Main.scala:51)
at org.apache.spark.repl.Main.main(Main.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

stdDf: org.apache.spark.sql.DataFrame = [stid: string, name: string ... 3
more fields]

again same works without exception:

scala> import sqlContext.implicits._
import sqlContext.implicits._

scala> val stdDf = sqlContext.createDataFrame(rowRDD,empSchema.struct);
stdDf: org.apache.spark.sql.DataFrame = [stid: string, name: string ... 3
more fields]


Thanks.



"
Chetan Khatri <chetan.opensource@gmail.com>,"Tue, 17 Jan 2017 11:06:05 +0530",Weird experience Hive with Spark Transformations,"Spark Dev List <dev@spark.apache.org>, user <user@spark.apache.org>","Hello,

I have following services are configured and installed successfully:

Hadoop 2.7.x
Spark 2.0.x
HBase 1.2.4
Hive 1.2.1

*Installation Directories:*

/usr/local/hadoop
/usr/local/spark
/usr/local/hbase

*Hive Environment variables:*

#HIVE VARIABLES START
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
#HIVE VARIABLES END

So, I can access Hive from anywhere as environment variables are
configured. Now if if i start my spark-shell & hive from location
/usr/local/hive then both work good for hive-metastore other wise from
where i start spark-shell where spark creates own meta-store.

i.e I am reading from HBase and Writing to Hive using Spark. I dont know
why this is weird issue is.




Thanks.
"
Raju Bairishetti <raju@apache.org>,"Tue, 17 Jan 2017 16:01:59 +0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",dev@spark.apache.org,"Hello,

   Spark sql is generating query plan with all partitions information even
though if we apply filters on partitions in the query.  Due to this,
sparkdriver/hive metastore is
hitting with OOM as each table is with lots of partitions.

We can confirm from hive audit logs that it tries to *fetch all
partitions* from
hive metastore.

 2016-12-28 07:18:33,749 INFO  [pool-4-thread-184]: HiveMetaStore.audit
(HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x
cmd=get_partitions : db=xxxx tbl=xxxxx


Configured the following parameters in the spark conf to fix the above
issue(source: from spark-jira & github pullreq):

*spark.sql.hive.convertMetastoreParquet   false*
*    spark.sql.hive.metastorePartitionPruning   true*


*   plan:  rdf.explain*
*   == Physical Plan ==*
       HiveTableScan [rejection_reason#626], MetastoreRelation dbname,
tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 =
28),(hour#317 = 2),(venture#318 = DEFAULT)]

*    get_partitions_by_filter* method is called and fetching only required
partitions.

    But we are seeing parquetDecode errors in our applications frequently
after this. Looks like these decoding errors were because of changing
serde fromspark-builtin to hive serde.

I feel like,* fixing query plan generation in the spark-sql* is the right
approach instead of forcing users to use hive serde.

Is there any workaround/way to fix this issue? I would like to hear more
thoughts on this :)






-- 

------
Thanks,
Raju Bairishetti,
www.lazada.com
"
Raju Bairishetti <raju@apache.org>,"Tue, 17 Jan 2017 16:02:46 +0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",dev@spark.apache.org,"Had a high level look into the code. Seems getHiveQlPartitions  method from
HiveMetastoreCatalog is getting called irrespective of
metastorePartitionPruning
conf value.

 It should not fetch all partitions if we set metastorePartitionPruning to
true (Default value for this is false)

def getHiveQlPartitions(predicates: Seq[Expression] = Nil): Seq[Partition] = {
  val rawPartitions = if (sqlContext.conf.metastorePartitionPruning) {
    table.getPartitions(predicates)
  } else {
    allPartitions
  }

...

def getPartitions(predicates: Seq[Expression]): Seq[HivePartition] =
  client.getPartitionsByFilter(this, predicates)

lazy val allPartitions = table.getAllPartitions

But somehow getAllPartitions is getting called eventough after setting
metastorePartitionPruning to true.

Am I missing something or looking at wrong place?






-- 

------
Thanks,
Raju Bairishetti,
www.lazada.com
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 17 Jan 2017 11:58:06 +0000",Re: Both Spark AM and Client are trying to delete Staging Directory,Marcelo Vanzin <vanzin@cloudera.com>,"I think Rostyslav is using a DFS which logs at warn/error if you try to delete a directory that isn't there, so is seeing warning messages that nobody else does

Rostyslav —like I said, i'd be curious as to which DFS/object store you are working with, as it is behaving slightly differently from everyone else's.

It sounds like I may need to have a quick chat with them about the merits of running all the Hadoop FS specification tests, if they aren't already. Something like a warning printed on delete(nonexistent path) hints that there may be other differences, which makes me worry about what their rename() does. That's the thing that atomic and speculative work depends on, so everyone needs to understand what's expected there.



On 14 Jan 2017, at 19:42, Marcelo Vanzin <vanzin@cloudera.com<mailto:vanzin@cloudera.com>> wrote:

scala> org.apache.hadoop.fs.FileSystem.getLocal(sc.hadoopConfiguration)
res0: org.apache.hadoop.fs.LocalFileSystem =
org.apache.hadoop.fs.LocalFileSystem@3f84970b

scala> res0.delete(new org.apache.hadoop.fs.Path(""/tmp/does-not-exist""), true)
res3: Boolean = false

Does that explain your confusion?


On Sat, Jan 14, 2017 at 11:37 AM, Marcelo Vanzin <vanzin@cloudera.com<mailto:vanzin@cloudera.com>> wrote:
Are you actually seeing a problem or just questioning the code?

I have never seen a situation where there's a failure because of that
part of the current code.

On Fri, Jan 13, 2017 at 3:24 AM, Rostyslav Sotnychenko
<r.sotnychenko@gmail.com<mailto:r.sotnychenko@gmail.com>> wrote:
Hi all!

I am a bit confused why Spark AM and Client are both trying to delete
Staging Directory.

https://github.com/apache/spark/blob/branch-2.1/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1110
https://github.com/apache/spark/blob/branch-2.1/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L233

As you can see, in case if a job was running on YARN in Cluster deployment
mode, both AM and Client will try to delete Staging directory if job
succeeded and eventually one of them will fail to do this, because the other
one already deleted the directory.

Shouldn't we add some check to Client?


Thanks,
Rostyslav



--
Marcelo



--
Marcelo

---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>

"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Wed, 18 Jan 2017 01:11:13 +0900","GraphX-related ""open"" issues","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, devs

Sorry to bother you, but plz let me check in advance;
in JIRA, there are some open (and inactive) issues about GraphX features.
IIUC the current GraphX features become almost freeze and
they possibly get no modification except for critical bugs.
So, IMO it seems okay to close tickets about ""Improvement"" and ""New
Feature"" for now.
Thought?

If any problem, please let me know; otherwise, I'll close the tickets below
this weekend;
---
New Feature SPARK-15880
 - PREGEL Based Semi-Clustering Algorithm Implementation using Spark GraphX
API
New Feature SPARK-13733
 - Support initial weight distribution in personalized PageRank
Improvement SPARK-13460
 - Applying Encoding methods to GraphX's Internal storage structure
New Feature SPARK-10758
 - approximation algorithms to speedup triangle count and clustering
coefficient computation in GraphX
Improvement SPARK-10335
 - GraphX Connected Components fail with large number of iterations
New Feature SPARK-8497
 - Graph Clique(Complete Connected Sub-graph) Discovery Algorithm
New Feature SPARK-7258
 - spark.ml API taking Graph instead of DataFrame
New Feature SPARK-7257
 - Find nearest neighbor satisfying predicate
New Feature SPARK-7244
 - Find vertex sequences satisfying predicates
New Feature SPARK-4763
 - All-pairs shortest paths algorithm
Improvement SPARK-3373
 - Filtering operations should optionally rebuild routing tables


-- 
---
Takeshi Yamamuro (maropu)
"
"""Dongjoon Hyun""<dongjoon@apache.org>","Tue, 17 Jan 2017 16:31:46 -0000",Re: Weird experience Hive with Spark Transformations,<dev@spark.apache.org>,"Hi, Chetan.

Did you copy your `hive-site.xml` into Spark conf directory? For example,

cp /usr/local/hive/conf/hive-site.xml /usr/local/spark/conf

If you want to use the existing Hive metastore, you need to provide that information to Spark.

Bests,
Dongjoon.


---------------------------------------------------------------------


"
"""John Fang"" <xiaojian.fxj@alibaba-inc.com>","Wed, 18 Jan 2017 00:32:49 +0800","=?UTF-8?B?c3BhcmsgbWFpbiB0aHJlYWQgcXVpdCwgYnV0IHRoZSBKdm0gb2YgZHJpdmVyIGRvbid0IGNy?=
  =?UTF-8?B?YXNo?=","""spark-user"" <user@spark.apache.org>,
  ""spark-dev"" <dev@spark.apache.org>","My spark main thread create some daemon thread. Then the spark application throw some exceptions, and the main thread will quit. But the jvm of driver don't crash, so How can i do?
for example:
val sparkConf = new SparkConf().setAppName(""NetworkWordCount"")
sparkConf.set(""spark.streaming.blockInterval"", ""1000ms"")
val ssc = new StreamingContext(sparkConf, Seconds(10))

//daemon thread
val scheduledExecutorService = Executors.newSingleThreadScheduledExecutor(
  new ThreadFactory() {
    def newThread(r: Runnable): Thread = new Thread(r, ""Driver-Commit-Thread"")
  })

scheduledExecutorService.scheduleAtFixedRate(
  new Runnable() {
    def run() {
      try {
        System.out.println(""runable"")
      } catch {
        case e: Exception => {
          System.out.println(""ScheduledTask persistAllConsumerOffset exception"", e)
        }
      }
    }
  }, 1000, 1000 * 5, TimeUnit.MILLISECONDS)

Thread.sleep(1005)


val lines = ssc.receiverStream(new WordReceiver(StorageLevel.MEMORY_AND_DISK_2))
val words = lines.flatMap(_.split("" ""))

val wordCounts = words.map(x => (x, 1)).reduceByKey((x: Int, y: Int) => x + y, 10)
wordCounts.foreachRDD{rdd =>
  rdd.collect().foreach(println)
  throw new RuntimeException
}
ssc.start()
try {
  ssc.awaitTermination()
} catch {
  case e: Exception => {
    System.out.println(""end!!!!!"")
    throw e
  }
}

"
"""Dongjoon Hyun""<dongjoon@apache.org>","Tue, 17 Jan 2017 16:48:44 -0000","Re: GraphX-related ""open"" issues",<dev@spark.apache.org>,"Hi, Takeshi.


I'm just wondering about what kind of field value you want to fill in the `Resolution` field for those issues.

Maybe, 'Later'? Or, 'Won't Fix'?

Bests,
Dongjoon.

---------------------------------------------------------------------


"
"""John Fang"" <xiaojian.fxj@alibaba-inc.com>","Wed, 18 Jan 2017 00:58:14 +0800","=?UTF-8?B?c3BhcmsgbWFpbiB0aHJlYWQgcXVpdCwgYnV0IHRoZSBkcml2ZXIgZG9uJ3QgY3Jhc2ggYXQg?=
  =?UTF-8?B?c3RhbmRhbG9uZSBjbHVzdGVy?=","""spark-user"" <user@spark.apache.org>,
  ""spark-dev"" <dev@spark.apache.org>","My spark main thread create some daemon threads which maybe timer thread. Then the spark application throw some exceptions, and the main thread will quit. But the jvm of driver don't crash for standalone cluster. Of course the question don't happen at yarn cluster. Because the application master will monitor the main thread of applicaiton, but the stanalone cluster can't. for example:val sparkConf = new SparkConf().setAppName(""NetworkWordCount"")
sparkConf.set(""spark.streaming.blockInterval"", ""1000ms"")
val ssc = new StreamingContext(sparkConf, Seconds(10))

//daemon thread
val scheduledExecutorService = Executors.newSingleThreadScheduledExecutor(
  new ThreadFactory() {
    def newThread(r: Runnable): Thread = new Thread(r, ""Driver-Commit-Thread"")
  })

scheduledExecutorService.scheduleAtFixedRate(
  new Runnable() {
    def run() {
      try {
        System.out.println(""runable"")
      } catch {
        case e: Exception => {
          System.out.println(""ScheduledTask persistAllConsumerOffset exception"", e)
        }
      }
    }
  }, 1000, 1000 * 5, TimeUnit.MILLISECONDS)

Thread.sleep(1005)


val lines = ssc.receiverStream(new WordReceiver(StorageLevel.MEMORY_AND_DISK_2))
val words = lines.flatMap(_.split("" ""))

val wordCounts = words.map(x => (x, 1)).reduceByKey((x: Int, y: Int) => x + y, 10)
wordCounts.foreachRDD{rdd =>
  rdd.collect().foreach(println)
  throw new RuntimeException  //exception
}
ssc.start()
try {
  ssc.awaitTermination()
} catch {
  case e: Exception => {
    System.out.println(""end!!!!!"")
    throw e
  }
}


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Wed, 18 Jan 2017 02:30:16 +0900","Re: GraphX-related ""open"" issues",Dongjoon Hyun <dongjoon@apache.org>,"Thank for your comment!
I'm just thinking I'll set ""Won't Fix"" though, ""Later"" is also okay.
But, I re-checked ""Contributing to JIRA Maintenance"" in the contribution
guide (http://spark.apache.org/contributing.html) and
I couldn't find any setting policy about ""Later"".
So, IMO it's okay to set ""Won't Fix"" for now and those who'd like to make
prs feel free to (re?-)open tickets.





-- 
---
Takeshi Yamamuro
"
Sean Owen <sowen@cloudera.com>,"Tue, 17 Jan 2017 17:40:05 +0000","Re: GraphX-related ""open"" issues","Takeshi Yamamuro <linguin.m.s@gmail.com>, Dongjoon Hyun <dongjoon@apache.org>","WontFix or Later is fine. There's not really any practical distinction. I
figure that if something times out and is closed, it's very unlikely to be
looked at again. Therefore marking it as something to do 'later' seemed
less accurate.


"
Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Tue, 17 Jan 2017 19:45:44 +0200",Re: Both Spark AM and Client are trying to delete Staging Directory,Steve Loughran <stevel@hortonworks.com>,"delete a directory that isn't there, so is seeing warning messages that
nobody else does

Yep, you are correct.

re you
are working with

Unfortunately, I am not able to share this information :(

of running all the Hadoop FS specification tests

I already send the link to Hadoop docs you mentioned to appropriate people,
so the difference could be fixed.


Thanks,
Rostyslav



re you
t
"
Michael Allman <michael@videoamp.com>,"Tue, 17 Jan 2017 10:45:47 -0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Raju Bairishetti <raju@apache.org>,"Hi Raju,

I'm sorry this isn't working for you. I helped author this functionality and will try my best to help.

First, I'm curious why you set spark.sql.hive.convertMetastoreParquet to false? Can you link specifically to the jira issue or spark pr you referred to? The first thing I would try is setting spark.sql.hive.convertMetastoreParquet to true. Setting that to false might also explain why you're getting parquet decode errors. If you're writing your table data with Spark's parquet file writer and reading with Hive's parquet file reader, there may be an incompatibility accounting for the decode errors you're seeing. 

Can you reply with your table's Hive metastore schema, including partition schema? Where are the table's files located? If you do a ""show partitions <dbname>.<tablename>"" in the spark-sql shell, does it show the partitions you expect to see? If not, run ""msck repair table <dbname>.<tablename>"".

Cheers,

Michael


from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
{
metastorePartitionPruning to true.
even though if we apply filters on partitions in the query.  Due to this, sparkdriver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
issue(source: from spark-jira & github pullreq):
tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde fromspark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
{
metastorePartitionPruning to true.
even though if we apply filters on partitions in the query.  Due to this, spark driver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
issue(source: from spark-jira & github pullreq):
tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde from spark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
"
Joseph Bradley <joseph@databricks.com>,"Tue, 17 Jan 2017 15:38:06 -0800",Feedback on MLlib roadmap process proposal,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

This is a general call for thoughts about the process for the MLlib roadmap
proposed in SPARK-18813.  See the section called ""Roadmap process.""

Summary:
* This process is about committers indicating intention to shepherd and
review.
* The goal is to improve visibility and communication.
* This is fairly orthogonal to the SIP discussion since this proposal is
more about setting release targets than about proposing future plans.

Thanks!
Joseph

-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
sujith71955 <sujithchacko.2010@gmail.com>,"Tue, 17 Jan 2017 19:40:03 -0700 (MST)",Re: Limit Query Performance Suggestion,dev@spark.apache.org,"Dear Liang,

Thanks for your valuable feedback.

There was a mistake in the previous post  i corrected it, as you mentioned
the  `GlobalLimit` we will only take the required number of rows from the
input iterator which really pulls data from local blocks and remote blocks.
but if the limit value is very high >= 10000000,  and when there will be a
shuffle exchange happens  between `GlobalLimit` and `LocalLimit` to retrieve
data from all partitions to one partition, since the limit value is very
large the performance bottleneck still exists.
 
soon in next  post i will publish a test report with sample data and also
figuring out a solution for this problem. 

Please let me know for any clarifications or suggestions regarding this
issue.

Regards,
Sujith



--

---------------------------------------------------------------------


"
Raju Bairishetti <raju@apache.org>,"Wed, 18 Jan 2017 10:51:56 +0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Michael Allman <michael@videoamp.com>,"Thanks Michael for the respopnse.



I had set as suggested in SPARK-6910 and corresponsing pull reqs. It did
not work for me without  setting *spark.sql.hive.convertMetastoreParquet*
property.

Can you link specifically to the jira issue or spark pr you referred to?
to avoid fetching all the partitions. We reverted
spark.sql.hive.convertMetastoreParquet
 setting to true to decoding errors. After reverting this it is fetching
all partiitons from the table.

Can you reply with your table's Hive metastore schema, including partition
     col1 string
     col2 string
     year int
     month int
     day int
     hour int

# Partition Information

# col_name            data_type           comment

year  int

month int

day int

hour int

venture string

Where are the table's files located?
In hadoop. Under some user directory.

Yes. It is listing the partitions



-- 

------
Thanks,
Raju Bairishetti,
www.lazada.com
"
Michael Allman <michael@videoamp.com>,"Tue, 17 Jan 2017 19:13:13 -0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Raju Bairishetti <raju@apache.org>,"What is the physical query plan after you set spark.sql.hive.convertMetastoreParquet to true?

Michael

functionality and will try my best to help.
to false? 
did not work for me without  setting spark.sql.hive.convertMetastoreParquet property. 
to? The first thing I would try is setting spark.sql.hive.convertMetastoreParquet to true. Setting that to false might also explain why you're getting parquet decode errors. If you're writing your table data with Spark's parquet file writer and reading with Hive's parquet file reader, there may be an incompatibility accounting for the decode errors you're seeing. 
<https://issues.apache.org/jira/browse/SPARK-6910> . My main motivation is to avoid fetching all the partitions. We reverted spark.sql.hive.convertMetastoreParquet  setting to true to decoding errors. After reverting this it is fetching all partiitons from the table.
partition schema?
shell, does it show the partitions you expect to see? If not, run ""msck repair table <dbname>.<tablename>"".
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
setting metastorePartitionPruning to true.
even though if we apply filters on partitions in the query.  Due to this, sparkdriver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde fromspark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
setting metastorePartitionPruning to true.
even though if we apply filters on partitions in the query.  Due to this, spark driver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde from spark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
"
Raju Bairishetti <raju@apache.org>,"Wed, 18 Jan 2017 11:38:32 +0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Michael Allman <michael@videoamp.com>,"
Physical plan continas all the partition locations



-- 

------
Thanks,
Raju Bairishetti,
www.lazada.com
"
Michael Allman <michael@videoamp.com>,"Tue, 17 Jan 2017 20:25:56 -0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Raju Bairishetti <raju@apache.org>,"Can you paste the actual query plan here, please?

spark.sql.hive.convertMetastoreParquet to true?
functionality and will try my best to help.
to false? 
did not work for me without  setting spark.sql.hive.convertMetastoreParquet property. 
to? The first thing I would try is setting spark.sql.hive.convertMetastoreParquet to true. Setting that to false might also explain why you're getting parquet decode errors. If you're writing your table data with Spark's parquet file writer and reading with Hive's parquet file reader, there may be an incompatibility accounting for the decode errors you're seeing. 
<https://issues.apache.org/jira/browse/SPARK-6910> . My main motivation is to avoid fetching all the partitions. We reverted spark.sql.hive.convertMetastoreParquet  setting to true to decoding errors. After reverting this it is fetching all partiitons from the table.
partition schema?
shell, does it show the partitions you expect to see? If not, run ""msck repair table <dbname>.<tablename>"".
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {

setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, sparkdriver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde fromspark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {

setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, spark driver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde from spark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
"
Raju Bairishetti <raju@apache.org>,"Wed, 18 Jan 2017 12:42:29 +0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Michael Allman <michael@videoamp.com>," describe dummy;

OK

sample              string

year                string

month               string

# Partition Information

# col_name            data_type           comment

year                string

month               string


val df = sqlContext.sql(""select count(1) from rajub.dummy where year='*2017*
'"")

df: org.apache.spark.sql.DataFrame = [_c0: bigint]


*scala> df.explain*

== Physical Plan ==

TungstenAggregate(key=[],
functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#3070L])

+- TungstenExchange SinglePartition, None

   +- TungstenAggregate(key=[],
functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#3076L])

      +- Scan ParquetRelation: rajub.dummy[] InputPaths:
maprfs:/user/rajub/dummy/sample/year=2016/month=10,
maprfs:/user/rajub/dummy/sample/year=*2016*/month=11,
maprfs:/user/rajub/dummy/sample/year=*2016*/month=9,
maprfs:/user/rajub/dummy/sample/year=2017/month=10,
maprfs:/user/rajub/dummy/sample/year=2017/month=11,
maprfs:/user/rajub/dummy/sample/year=2017/month=9




-- 

------
Thanks,
Raju Bairishetti,
www.lazada.com
"
Michael Allman <michael@videoamp.com>,"Tue, 17 Jan 2017 20:52:37 -0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Raju Bairishetti <raju@apache.org>,"What version of Spark are you running?

year='2017'"")
functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#3070L])
functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#3076L])
maprfs:/user/rajub/dummy/sample/year=2016/month=10, maprfs:/user/rajub/dummy/sample/year=2016/month=11, maprfs:/user/rajub/dummy/sample/year=2016/month=9, maprfs:/user/rajub/dummy/sample/year=2017/month=10, maprfs:/user/rajub/dummy/sample/year=2017/month=11, maprfs:/user/rajub/dummy/sample/year=2017/month=9
spark.sql.hive.convertMetastoreParquet to true?
functionality and will try my best to help.
spark.sql.hive.convertMetastoreParquet to false? 
did not work for me without  setting spark.sql.hive.convertMetastoreParquet property. 
to? The first thing I would try is setting spark.sql.hive.convertMetastoreParquet to true. Setting that to false might also explain why you're getting parquet decode errors. If you're writing your table data with Spark's parquet file writer and reading with Hive's parquet file reader, there may be an incompatibility accounting for the decode errors you're seeing. 
<https://issues.apache.org/jira/browse/SPARK-6910> . My main motivation is to avoid fetching all the partitions. We reverted spark.sql.hive.convertMetastoreParquet  setting to true to decoding errors. After reverting this it is fetching all partiitons from the table.
partition schema?
shell, does it show the partitions you expect to see? If not, run ""msck repair table <dbname>.<tablename>"".
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
=
setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, sparkdriver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde fromspark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
=
setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, spark driver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde from spark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
more thoughts on this :)
"
Raju Bairishetti <raju@apache.org>,"Wed, 18 Jan 2017 12:59:18 +0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Michael Allman <michael@videoamp.com>,"Tested on both 1.5.2 and 1.61.




-- 

------
Thanks,
Raju Bairishetti,
www.lazada.com
"
Michael Allman <michael@videoamp.com>,"Tue, 17 Jan 2017 22:09:45 -0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Raju Bairishetti <raju@apache.org>,"I think I understand. Partition pruning for the case where spark.sql.hive.convertMetastoreParquet is true was not added to Spark until 2.1.0. I think that in previous versions it only worked when spark.sql.hive.convertMetastoreParquet is false. Unfortunately, that configuration gives you data decoding errors. If it's possible for you to write all of your data with Hive, then you should be able to read it without decoding errors and with partition pruning turned on. Another possibility is running your Spark app with a very large maximum heap configuration, like 8g or even 16g. However, loading all of that partition metadata can be quite slow for very large tables. I'm sorry I can't think of a better solution for you.

Michael



year='2017'"")
functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#3070L])
functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#3076L])
maprfs:/user/rajub/dummy/sample/year=2016/month=10, maprfs:/user/rajub/dummy/sample/year=2016/month=11, maprfs:/user/rajub/dummy/sample/year=2016/month=9, maprfs:/user/rajub/dummy/sample/year=2017/month=10, maprfs:/user/rajub/dummy/sample/year=2017/month=11, maprfs:/user/rajub/dummy/sample/year=2017/month=9
spark.sql.hive.convertMetastoreParquet to true?
functionality and will try my best to help.
spark.sql.hive.convertMetastoreParquet to false? 
It did not work for me without  setting spark.sql.hive.convertMetastoreParquet property. 
referred to? The first thing I would try is setting spark.sql.hive.convertMetastoreParquet to true. Setting that to false might also explain why you're getting parquet decode errors. If you're writing your table data with Spark's parquet file writer and reading with Hive's parquet file reader, there may be an incompatibility accounting for the decode errors you're seeing. 
<https://issues.apache.org/jira/browse/SPARK-6910> . My main motivation is to avoid fetching all the partitions. We reverted spark.sql.hive.convertMetastoreParquet  setting to true to decoding errors. After reverting this it is fetching all partiitons from the table.
partition schema?
shell, does it show the partitions you expect to see? If not, run ""msck repair table <dbname>.<tablename>"".
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
=
setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, sparkdriver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde fromspark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
hear more thoughts on this :)
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
=
setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, spark driver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde from spark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
hear more thoughts on this :)
"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 18 Jan 2017 11:59:45 +0530",Re: Weird experience Hive with Spark Transformations,Dongjoon Hyun <dongjoon@apache.org>,"But Hive 1.2.1 do not have hive-site.xml, I tried to add my own which
 Hive 2.0.1 where hive-site.xml content were as below and copied to
spark/conf too. it worked.

*5. hive-site.xml configuration setup*


Add below at conf/hive-site.xml , if not there then create it.


<property>

<name>javax.jdo.option.ConnectionURL</name>

<value>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true</value>

<description>metadata is stored in a MySQL server</description>

</property>

<property>

<name>javax.jdo.option.ConnectionDriverName</name>

<value>com.mysql.jdbc.Driver</value>

<description>MySQL JDBC driver class</description>

</property>

<property>

<name>javax.jdo.option.ConnectionUserName</name>

<value>hiveuser</value>

<description>user name for connecting to mysql server</description>

</property>

<property>

<name>javax.jdo.option.ConnectionPassword</name>

<value>hivepassword</value>

<description>password for connecting to mysql server</description>

</property>


Replace below 3 properties tag with whatever already exist by default.
otherwise it will throw an error


""java.net.URISyntaxException: Relative path in absolute URI:
${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D""


<property>

<name>hive.querylog.location</name>

<value>$HIVE_HOME/iotmp</value>

<description>Location of Hive run time structured log file</description>

</property>


<property>

<name>hive.exec.local.scratchdir</name>

<value>$HIVE_HOME/iotmp</value>

<description>Local scratch space for Hive jobs</description>

</property>


<property>

<name>hive.downloaded.resources.dir</name>

<value>$HIVE_HOME/iotmp</value>

<description>Temporary local directory for added resources in the remote
file system.</description>

</property>




"
Raju Bairishetti <raju@apache.org>,"Wed, 18 Jan 2017 14:44:48 +0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Michael Allman <michael@videoamp.com>,"Thanks for the detailed explanation. Is it completely fixed in spark-2.1.0?

  We are giving very high memory to spark-driver to avoid the OOM(heap
space/ GC overhead limit) errors in spark-app. But when we run two-three
jobs together, these are bringing down the Hive metastore. We had to
forcefully drop older partitions to avoid frequent downs of Hive Metastore.





-- 

------
Thanks,
Raju Bairishetti,
www.lazada.com
"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 18 Jan 2017 00:48:09 -0700 (MST)",Re: Limit Query Performance Suggestion,dev@spark.apache.org,"
Hi Sujith,

I saw your updated post. Seems it makes sense to me now.

If you use a very big limit number, the shuffling before `GlobalLimit` would
be a bottleneck for performance, of course, even it can eventually shuffle
enough data to the single partition.

Unlike `CollectLimit`, actually I think there is no reason `GlobalLimit`
must shuffle all limited data from all partitions to one single machine with
respect to query execution. In other words, I think we can avoid shuffling
data in `GlobalLimit`.

I have an idea to improve this and may update here later if I can make it
work.


sujith71955 wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 18 Jan 2017 08:57:29 +0100","RpcEnv(Factory) is no longer pluggable? spark.rpc is gone, isn't it?",dev <dev@spark.apache.org>,"Hi,

Given [1]:


I believe the comment in [2]:


Correct? Deserves a pull request to get rid of the seemingly incorrect scaladoc?

p.s. How to know when the deprecation was introduced? The last change
is for executor blacklisting so git blame does not show what I want :(
Any ideas?

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L641
[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala#L32

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 18 Jan 2017 09:09:02 +0100","Re: RpcEnv(Factory) is no longer pluggable? spark.rpc is gone, isn't it?",dev <dev@spark.apache.org>,"

Figured that out myself!

$ git log --topo-order --graph -u -L
641,641:core/src/main/scala/org/apache/spark/SparkConf.scala

which gave me https://github.com/apache/spark/commit/4f5a24d7e73104771f233af041eeba4f41675974.

If you knew it, you're the git pro!

Jacek

---------------------------------------------------------------------


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 18 Jan 2017 09:18:59 +0100",[SQL][SPARK-14160] Maximum interval for o.a.s.sql.functions.window,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Can I ask for some clarifications regarding intended behavior of window
/ TimeWindow?

PySpark documentation states that ""Windows in the order of months are
not supported"". This is further confirmed by the checks in
TimeWindow.getIntervalInMicroseconds (https://git.io/vMP5l).

Surprisingly enough we can pass interval much larger than a month by
expressing interval in days or another unit of a higher precision. So
this fails:

Seq(""2017-01-01"").toDF(""date"").groupBy(window($""date"", ""1 month""))

while following is accepted:

Seq(""2017-01-01"").toDF(""date"").groupBy(window($""date"", ""999 days""))

with results which look sensible at first glance.

Is it a matter of a faulty validation logic (months will be assigned
only if there is a match against years or months https://git.io/vMPdi)
or expected behavior and I simply misunderstood the intentions?

-- 
Best,
Maciej

"
"""wangzhenhua (G)"" <wangzhenhua@huawei.com>","Wed, 18 Jan 2017 08:53:52 +0000",=?gb2312?B?tPC4tDogTGltaXQgUXVlcnkgUGVyZm9ybWFuY2UgU3VnZ2VzdGlvbg==?=,"Liang-Chi Hsieh <viirya@gmail.com>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","How about this:
1. we can make LocalLimit shuffle to mutiple partitions, i.e. create a new partitioner to uniformly dispatch the data

class LimitUniformPartitioner(partitions: Int) extends Partitioner {

  def numPartitions: Int = partitions
  
  var num = 0

  def getPartition(key: Any): Int = {
    num = num + 1
    num % partitions
  }

  override def equals(other: Any): Boolean = other match {
    case h: HashPartitioner =>
      h.numPartitions == numPartitions
    case _ =>
      false
  }

  override def hashCode: Int = numPartitions
}

2. then in GlobalLimit, we only take the first limit_number/num_of_shufflepartitions elements in each partition.

One issue left is how to decide shuffle partition number. 
We can have a config of the maximum number of elements for each GlobalLimit task to process,
then do a factorization to get a number most close to that config.
E.g. the config is 2000:
if limit=10000,  10000 = 2000 * 5, we shuffle to 5 partitions
if limit=9999, 9999 = 1111 * 9, we shuffle to 9 partitions
if limit is a prime number, we just fall back to single partition

best regards,
-zhenhua


-----ʼԭ-----
: Liang-Chi Hsieh [mailto:viirya@gmail.com] 
ʱ: 2017118 15:48
ռ: dev@spark.apache.org
: Re: Limit Query Performance Suggestion


Hi Sujith,

I saw your updated post. Seems it makes sense to me now.

If you use a very big limit number, the shuffling before `GlobalLimit` would be a bottleneck for performance, of course, even it can eventually shuffle enough data to the single partition.

Unlike `CollectLimit`, actually I think there is no reason `GlobalLimit` must shuffle all limited data from all partitions to one single machine with respect to query execution. In other words, I think we can avoid shuffling data in `GlobalLimit`.

I have an idea to improve this and may update here later if I can make it work.


sujith71955 wrote
> Dear Liang,
> 
> Thanks for your valuable feedback.
> 
> There was a mistake in the previous post  i corrected it, as you 
> mentioned the  `GlobalLimit` we will only take the required number of 
> rows from the input iterator which really pulls data from local blocks 
> and remote blocks.
> but if the limit value is very high >= 10000000,  and when there will 
> be a shuffle exchange happens  between `GlobalLimit` and `LocalLimit` 
> to retrieve data from all partitions to one partition, since the limit 
> value is very large the performance bottleneck still exists.
>  
> soon in next  post i will publish a test report with sample data and 
> also figuring out a solution for this problem.
> 
> Please let me know for any clarifications or suggestions regarding 
> this issue.
> 
> Regards,
> Sujith





-----
Liang-Chi Hsieh | @viirya
Spark Technology Center
http://www.spark.tc/
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Limit-Query-Performance-Suggestion-tp20570p20652.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org

"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 18 Jan 2017 01:57:42 -0700 (MST)",=?UTF-8?Q?Re:_=E7=AD=94=E5=A4=8D:_Limit_Query_Performance_Suggestion?=,dev@spark.apache.org,"
Hi zhenhua,

Thanks for the idea.

Actually, I think we can completely avoid shuffling the data in a limit
operation, no matter LocalLimit or GlobalLimit.



wangzhenhua (G) wrote
w


15:48


 
 
 
formance-Suggestion-tp20570p20652.html








-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--
3.nabble.com/Limit-Query-Performance-Suggestion-tp20570p20657.html
om.

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 18 Jan 2017 10:29:41 +0100","clientMode in RpcEnv.create in Spark on YARN vs general case (driver
 vs executors)?",dev <dev@spark.apache.org>,"Hi,

I'm trying to get the gist of clientMode input parameter for
RpcEnv.create [1]. It is disabled (i.e. false) by default.

I've managed to find out that, in the ""general"" case, it's enabled for
executors and disabled for the driver.

(it's also used for Spark Standalone's master and workers but it's
infra and I'm not interested in exploring it currently).

I've however noticed that in YARN the clientMode parameter means
something more, i.e. whether the Spark application runs in client or
cluster deploy mode.

In YARN my understanding of the parameter is that clientMode is
enabled explicitly when Spark on YARN's ApplicationMaster creates the
`sparkYarnAM` RPC Environment [2] (when executed for client deploy
mode [3])

This is because in client deploy mode the driver runs on some other
node and the AM acts simply as a proxy to Spark executors that run in
their own YARN containers. This is (also?) because in client deploy
mode in Spark on YARN we have separate JVM processes for the driver,
the AM and Spark executors. The distinction is

Is the last two paragraphs correct?

I'd appreciate if you could fix and fill out the gaps where necessary.
Thanks a lot to make it so much easier for me and...participants of my
Spark workshops ;-)

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala#L42

[2] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L434

[3] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L254

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Wed, 18 Jan 2017 11:10:16 +0000",can someone review my PR?,Apache Spark Dev <dev@spark.apache.org>,"I've had a PR outstanding on spark/object store integration, works for both maven and sbt builds

https://issues.apache.org/jira/browse/SPARK-7481
https://github.com/apache/spark/pull/12004

Can I get someone to review this as it appears to be being overlooked amongst all the PRs

thanks

-steve
"
Sean Owen <sowen@cloudera.com>,"Wed, 18 Jan 2017 11:18:41 +0000",Re: can someone review my PR?,"Steve Loughran <stevel@hortonworks.com>, Apache Spark Dev <dev@spark.apache.org>","It still doesn't pass tests -- I'd usually not look until that point.


"
Brian Hong <sungjinhong@devsisters.com>,"Wed, 18 Jan 2017 13:51:43 +0000",[Spark SQL] Making InferSchema and JacksonParser public,dev@spark.apache.org,"I work for a mobile game company. I'm solving a simple question: ""Can we
efficiently/cheaply query for the log of a particular user within given
date period?""

I've created a special JSON text-based file format that has these traits:
 - Snappy compressed, saved in AWS S3
 - Partitioned by date. ie. 2017-01-01.sz, 2017-01-02.sz, ...
 - Sorted by a primary key (log_type) and a secondary key (user_id), Snappy
block compressed by 5MB blocks
 - Blocks are indexed with primary/secondary key in file 2017-01-01.json
 - Efficient block based random access on primary key (log_type) and
secondary key (user_id) using the index

I've created a Spark SQL DataFrame relation that can query this file
format.  Since the schema of each log type is fairly consistent, I've
reused the `InferSchema.inferSchema` method and `JacksonParser`in the Spark
SQL code to support structured querying.  I've also implemented filter
push-down to optimize the file access.

It is very fast when querying for a single user or querying for a single
log type with a sampling ratio of 10000 to 1 compared to parquet file
format.  (We do use parquet for some log types when we need batch analysis.)

API.  So we are forced to use hacks to use these methods.  (Things like
copying the code or using the org.apache.spark.sql package namespace)

I've been following Spark SQL code since 1.4, and the JSON schema
inferencing code and JacksonParser seem to be relatively stable recently.
Can the core-devs make these APIs public?

We are willing to open source this file format because it is very excellent
for archiving user related logs in S3.  The key dependency of private APIs
in Spark SQL is the main hurdle in making this a reality.

Thank you for reading!
"
Steve Loughran <stevel@hortonworks.com>,"Wed, 18 Jan 2017 14:16:26 +0000",Re: can someone review my PR?,Sean Owen <sowen@cloudera.com>,"

It still doesn't pass tests -- I'd usually not look until that point.

it's failing on the dependency check as the dependencies have changed. that's what it's meant to do. should I explicitly be changing the values so that the build doesn't notice the change?

I've had a PR outstanding on spark/object store integration, works for both maven and sbt builds

https://issues.apache.org/jira/browse/SPARK-7481
https://github.com/apache/spark/pull/12004

Can I get someone to review this as it appears to be being overlooked amongst all the PRs

thanks

-steve

"
marco rocchi <rocchi.1407763@studenti.uniroma1.it>,"Wed, 18 Jan 2017 15:22:51 +0100",GC limit exceed,dev@spark.apache.org,"I have a spark code that works well over a sample of data in local mode,
but when I pass the same code on a cluster with the entire dataset I
receive GC limited exceed error.
In that section is possible to submit the code and have some hints in order
to solve my problem?
Thanks a lot for the attention

Marco
"
Daniel van der Ende <daniel.vanderende@gmail.com>,"Wed, 18 Jan 2017 15:30:29 +0100",Re: GC limit exceed,marco rocchi <rocchi.1407763@studenti.uniroma1.it>,"Hi Marco,

What kind of scheduler are you using on your cluster? Yarn?

Also, are you running in client mode or cluster mode on the cluster?

Daniel





-- 
Daniel
"
Rich Bowen <rbowen@apache.org>,"Wed, 18 Jan 2017 11:45:41 -0500",ApacheCon CFP closing soon (11 February),comdev <dev@community.apache.org>,"Hello, fellow Apache enthusiast. Thanks for your participation, and
interest in, the projects of the Apache Software Foundation.

I wanted to remind you that the Call For Papers (CFP) for ApacheCon
North America, and Apache: Big Data North America, closes in less than a
month. If you've been putting it off because there was lots of time
left, it's time to dig for that inspiration and get those talk proposals in.

It's also time to discuss with your developer and user community whether
there's a track of talks that you might want to propose, so that you
have more complete coverage of your project than a talk or two.

We're looking for talks directly, and indirectly, related to projects at
the Apache Software Foundation. These can be anything from in-depth
technical discussions of the projects you work with, to talks about
community, documentation, legal issues, marketing, and so on. We're also
very interested in talks about projects and services built on top of
Apache projects, and case studies of how you use Apache projects to
solve real-world problems.

We are particularly interested in presentations from Apache projects
either in the Incubator, or recently graduated. ApacheCon is where
people come to find out what technology they'll be using this time next
year.

Important URLs are:

To submit a talk for Apache: Big Data -
http://events.linuxfoundation.org/events/apache-big-data-north-america/program/cfp
To submit a talk for ApacheCon -
http://events.linuxfoundation.org/events/apachecon-north-america/program/cfp

To register for Apache: Big Data -
http://events.linuxfoundation.org/events/apache-big-data-north-america/attend/register-
To register for ApacheCon -
http://events.linuxfoundation.org/events/apachecon-north-america/attend/register-

Early Bird registration rates end March 12th, but if you're a committer
on an Apache project, you get the low committer rate, which is less than
half of the early bird rate!

For further updated about ApacheCon, follow us on Twitter, @ApacheCon,
or drop by our IRC channel, #apachecon on the Freenode IRC network. Or
contact me - rbowen@apache.org - with any questions or concerns.

Thanks!

Rich Bowen, VP Conferences, Apache Software Foundation

-- 
(You've received this email because you're on a dev@ or users@ mailing
list of an Apache Software Foundation project. For subscription and
unsubscription information, consult the headers of this email message,
as this varies from one list to another.)

---------------------------------------------------------------------


"
Burak Yavuz <brkyvz@gmail.com>,"Wed, 18 Jan 2017 18:52:09 +0200",Re: [SQL][SPARK-14160] Maximum interval for o.a.s.sql.functions.window,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Hi Maciej,

I believe it would be useful to either fix the documentation or fix the
implementation. I'll leave it to the community to comment on. The code
right now disallows intervals provided in months and years, because they
are not a ""consistently"" fixed amount of time. A month can be 28, 29, 30,
or 31 days. A year is 12 months for sure, but is it 360 days (sometimes
used in finance), 365 days or 366 days?

Therefore we could either:
  1) Allow windowing when intervals are given in days and less, even though
it could be 365 days, and fix the documentation.
  2) Explicitly disallow it as there may be a lot of data for a given
window, but partial aggregations should help with that.

My thoughts are to go with 1. What do you think?

Best,
Burak


"
Michael Allman <michael@videoamp.com>,"Wed, 18 Jan 2017 09:18:46 -0800","Re: Spark sql query plan contains all the partitions from hive table
 even though filtering of partitions is provided",Raju Bairishetti <raju@apache.org>,"Based on what you've described, I think you should be able to use Spark's parquet reader plus partition pruning in 2.1.

spark-2.1.0?
space/ GC overhead limit) errors in spark-app. But when we run two-three jobs together, these are bringing down the Hive metastore. We had to forcefully drop older partitions to avoid frequent downs of Hive Metastore.
spark.sql.hive.convertMetastoreParquet is true was not added to Spark until 2.1.0. I think that in previous versions it only worked when spark.sql.hive.convertMetastoreParquet is false. Unfortunately, that configuration gives you data decoding errors. If it's possible for you to write all of your data with Hive, then you should be able to read it without decoding errors and with partition pruning turned on. Another possibility is running your Spark app with a very large maximum heap configuration, like 8g or even 16g. However, loading all of that partition metadata can be quite slow for very large tables. I'm sorry I can't think of a better solution for you.
year='2017'"")
functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#3070L])
functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#3076L])
maprfs:/user/rajub/dummy/sample/year=2016/month=10, maprfs:/user/rajub/dummy/sample/year=2016/month=11, maprfs:/user/rajub/dummy/sample/year=2016/month=9, maprfs:/user/rajub/dummy/sample/year=2017/month=10, maprfs:/user/rajub/dummy/sample/year=2017/month=11, maprfs:/user/rajub/dummy/sample/year=2017/month=9
spark.sql.hive.convertMetastoreParquet to true?
functionality and will try my best to help.
spark.sql.hive.convertMetastoreParquet to false? 
It did not work for me without  setting spark.sql.hive.convertMetastoreParquet property. 
referred to? The first thing I would try is setting spark.sql.hive.convertMetastoreParquet to true. Setting that to false might also explain why you're getting parquet decode errors. If you're writing your table data with Spark's parquet file writer and reading with Hive's parquet file reader, there may be an incompatibility accounting for the decode errors you're seeing. 
<https://issues.apache.org/jira/browse/SPARK-6910> . My main motivation is to avoid fetching all the partitions. We reverted spark.sql.hive.convertMetastoreParquet  setting to true to decoding errors. After reverting this it is fetching all partiitons from the table.
partition schema?
spark-sql shell, does it show the partitions you expect to see? If not, run ""msck repair table <dbname>.<tablename>"".
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
Seq[HivePartition] =
setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, sparkdriver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde fromspark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
hear more thoughts on this :)
method from HiveMetastoreCatalog is getting called irrespective of metastorePartitionPruning conf value.
metastorePartitionPruning to true (Default value for this is false) 
Seq[Partition] = {
(sqlContext.conf.metastorePartitionPruning) {
Seq[HivePartition] =
setting metastorePartitionPruning to true.
information even though if we apply filters on partitions in the query.  Due to this, spark driver/hive metastore is hitting with OOM as each table is with lots of partitions.
partitions from hive metastore.
HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(371)) - ugi=rajub    ip=/x.x.x.x   cmd=get_partitions : db=xxxx tbl=xxxxx
above issue(source: from spark-jira & github pullreq):
dbname, tablename, None,   [(year#314 = 2016),(month#315 = 12),(day#316 = 28),(hour#317 = 2),(venture#318 = DEFAULT)]
required partitions.
frequently after this. Looks like these decoding errors were because of changing serde from spark-builtin to hive serde.
right approach instead of forcing users to use hive serde.
hear more thoughts on this :)
"
Michael Allman <michael@videoamp.com>,"Wed, 18 Jan 2017 09:21:19 -0800",Re: [Spark SQL] Making InferSchema and JacksonParser public,Brian Hong <sungjinhong@devsisters.com>,"Personally I'd love to see some kind of pluggability, configurability in the JSON schema parsing, maybe as an option in the DataFrameReader. Perhaps you can propose an API?

we efficiently/cheaply query for the log of a particular user within given date period?""
traits:
2017-01-02.sz <http://2017-01-02.sz/>, ...
Snappy block compressed by 5MB blocks
2017-01-01.json
secondary key (user_id) using the index
format.  Since the schema of each log type is fairly consistent, I've reused the `InferSchema.inferSchema` method and `JacksonParser`in the Spark SQL code to support structured querying.  I've also implemented filter push-down to optimize the file access.
single log type with a sampling ratio of 10000 to 1 compared to parquet file format.  (We do use parquet for some log types when we need batch analysis.)
private API.  So we are forced to use hacks to use these methods.  (Things like copying the code or using the org.apache.spark.sql package namespace)
inferencing code and JacksonParser seem to be relatively stable recently.  Can the core-devs make these APIs public?
excellent for archiving user related logs in S3.  The key dependency of private APIs in Spark SQL is the main hurdle in making this a reality.

"
Reynold Xin <rxin@databricks.com>,"Wed, 18 Jan 2017 17:41:37 +0000",Re: [Spark SQL] Making InferSchema and JacksonParser public,"Brian Hong <sungjinhong@devsisters.com>, dev@spark.apache.org","That is internal, but the amount of code is not a lot. Can you just copy
the relevant classes over to your project?


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 18 Jan 2017 18:52:19 +0100","Re: [SQL][SPARK-14160] Maximum interval for
 o.a.s.sql.functions.window",Burak Yavuz <brkyvz@gmail.com>,"Thanks for the response Burak,

As any sane person I try to steer away from the objects which have both
calendar and unsafe in their fully qualified names but if there is no
bigger picture I missed here I would go with 1 as well. And of course
fix the error message. I understand this has been introduced with
structured streaming in mind, but it is an useful feature in general,
not only in high precision scale. To be honest I would love to see some
generalized version which could be used (I mean without hacking) with
arbitrary numeric sequence. It could address at least some scenarios in
which people try to use window functions without PARTITION BY clause and
fail miserably.

Regarding ambiguity... Sticking with days doesn't really resolve the
problem, does it? If one were to nitpick it doesn't look like this
implementation even touches all the subtleties of DST or leap second.




"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 18 Jan 2017 10:05:15 -0800",Re: can someone review my PR?,Steve Loughran <stevel@hortonworks.com>,"
Yes. There's no automated way to do that, intentionally.

-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 18 Jan 2017 10:09:03 -0800","Re: clientMode in RpcEnv.create in Spark on YARN vs general case
 (driver vs executors)?",Jacek Laskowski <jacek@japila.pl>,"
""clientMode"" means whether the RpcEnv only opens external connections
(client) or also accepts incoming connections. From that you should be
able to understand why it's true or false in the cases you looked at.


-- 
Marcelo

---------------------------------------------------------------------


"
Asher Krim <akrim@hubspot.com>,"Wed, 18 Jan 2017 15:50:21 -0500",Possible bug - Java iterator/iterable inconsistency,dev@spark.apache.org,"In Spark 2 + Java + RDD api, the use of iterables was replaced with
iterators. I just encountered an inconsistency in `flatMapValues` that may
be a bug:

`flatMapValues` (https://github.com/apache/spark/blob/master/core/src/
main/scala/org/apache/spark/api/java/JavaPairRDD.scala#L677) takes
a `FlatMapFunction` (
https://github.com/apache/spark/blob/39e2bad6a866d27c3ca594d15e574a1da3ee84cc/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java
)

The problem is that `FlatMapFunction` was changed to return an iterator,
but `rdd.flatMapValues` still expects an iterable. Am I using these
constructs correctly? Is there a workaround other than converting the
iterator to an iterable outside of the function?

Thanks,
-- 
Asher Krim
Senior Software Engineer
"
Sean Owen <sowen@cloudera.com>,"Wed, 18 Jan 2017 21:00:38 +0000",Re: Possible bug - Java iterator/iterable inconsistency,"Asher Krim <akrim@hubspot.com>, dev@spark.apache.org","Hm. Unless I am also totally missing or forgetting something, I think
you're right. The equivalent in PairRDDFunctions.scala operations on a
java.util.Iterator.

You can work around it by wrapping it in a faked IteratorIterable.

I think this is fixable in the API by deprecating this method and adding a
new one that takes a FlatMapFunction. We'd have to triple-check in a test
that this doesn't cause an API compatibility problem with respect to Java 8
lambdas, but if that's settled, I think this could be fixed without
breaking the API.


"
Michael Armbrust <michael@databricks.com>,"Wed, 18 Jan 2017 18:34:45 -0800",Re: [SQL][SPARK-14160] Maximum interval for o.a.s.sql.functions.window,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"+1, we should just fix the error to explain why months aren't allowed and
suggest that you manually specify some number of days.


"
Dongjin Lee <dongjin@apache.org>,"Thu, 19 Jan 2017 12:21:23 +0900","Re: GraphX-related ""open"" issues","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I am currently working on SPARK-15880[^1] and also have some interest
on SPARK-7244[^2] and SPARK-7257[^3]. In fact, SPARK-7244 and SPARK-7257
have some importance on graph analysis field.
Could you make them an exception? Since I am working on graph analysis, I
hope to take them.

If needed, I can take SPARK-10335 and SPARK-8497 after them.

Thanks,
Dongjin




-- 
*Dongjin Lee*


*Software developer in Line+.So interested in massive-scale machine
learning.facebook: www.facebook.com/dongjin.lee.kr
<http://www.facebook.com/dongjin.lee.kr>linkedin:
kr.linkedin.com/in/dongjinleekr
<http://kr.linkedin.com/in/dongjinleekr>github:
<http://goog_969573159/>github.com/dongjinleekr
<http://github.com/dongjinleekr>twitter: www.twitter.com/dongjinleekr
<http://www.twitter.com/dongjinleekr>*
"
Brian Hong <sungjinhong@devsisters.com>,"Thu, 19 Jan 2017 03:40:18 +0000",Re: [Spark SQL] Making InferSchema and JacksonParser public,"Reynold Xin <rxin@databricks.com>, dev@spark.apache.org","Yes that is the option I took while implementing this under Spark 1.4.  But
every time there is a major update in Spark, I needed to re-copy the needed
parts, which is very time consuming.

The reason is that InferSchema and JacksonParser uses many more Spark
internal methods, which makes this very hard to copy and maintain.

Thanks!


"
Sean Owen <sowen@cloudera.com>,"Thu, 19 Jan 2017 11:09:46 +0000",Re: Possible bug - Java iterator/iterable inconsistency,"Asher Krim <akrim@hubspot.com>, dev@spark.apache.org","Yes, confirmed that fixing it unfortunately causes trouble in Java 8. See
https://issues.apache.org/jira/browse/SPARK-19287 for further discussion.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 19 Jan 2017 06:50:07 -0700 (MST)","Re: spark main thread quit, but the driver don't crash at
 standalone cluster",dev@spark.apache.org,"
Can you just cancel the `ScheduledFuture` returned by `scheduleAtFixedRate`
when catching the exception in the main thread of the spark driver?


John Fang wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 20 Jan 2017 01:09:05 +0900","Re: GraphX-related ""open"" issues",Dongjin Lee <dongjin@apache.org>,"Thanks for your comment, Dongjin!
I have a pretty basic and also important question; why do you implement
these features as  a third-party library (and then upload them to the spark
packages https://spark-packages.org/)? ISTM graphx has already necessary
and sufficient APIs for these third-party ones.





-- 
---
Takeshi Yamamuro
"
Seth Hendrickson <seth.hendrickson16@gmail.com>,"Thu, 19 Jan 2017 08:54:00 -0800",Re: Feedback on MLlib roadmap process proposal,Joseph Bradley <joseph@databricks.com>,"I think the proposal laid out in SPARK-18813 is well done, and I do think
it is going to improve the process going forward. I also really like the
idea of getting the community to vote on JIRAs to give some of them
priority - provided that we listen to those votes, of course. The biggest
problem I see is that we do have several active contributors and those who
want to help implement these changes, but PRs are reviewed rather
sporadically and I imagine it is very difficult for contributors to
understand why some get reviewed and some do not. The most important thing
we can do, given that MLlib currently has a very limited committer review
bandwidth, is to make clear issues that, if worked on, will definitely get
reviewed. A hard thing to do in open source, no doubt, but even if we have
to limit the scope of such issues to a very small subset, it's a gain for
all I think.

goal of Spark MLlib (if this derails the original discussion, please let me
know and we can discuss in another thread). The roadmap does contain
specific items that help to convey some of this (ML parity with MLlib,
model persistence, etc...), but I'm interested in what the ""mission"" of
Spark MLlib is. We often see PRs for brand new algorithms which are
sometimes rejected and sometimes not. Do we aim to keep implementing more
and more algorithms? Or is our focus really, now that we have a reasonable
library of algorithms, to simply make the existing ones faster/better/more
robust? Should we aim to make interfaces that are easily extended for
developers to easily implement their own custom code (e.g. custom
optimization libraries), or do we want to restrict things to out-of-the box
algorithms? Should we focus on more flexible, general abstractions like
distributed linear algebra?

I was not involved in the project in the early days of MLlib when this
discussion may have happened, but I think it would be useful to either
revisit it or restate it here for some of the newer developers.


"
Michael Allman <michael@videoamp.com>,"Thu, 19 Jan 2017 09:48:57 -0800","Re: GraphX-related ""open"" issues","Takeshi Yamamuro <linguin.m.s@gmail.com>
 <pony-6ff48b33cbb828151035af58a92f18a39f1d1036-424de1f36582e993d75c4826d9a55738432c6d61@dev.spark.apache.org>
 <CACVFzXBQynt97bH4ekegR-CtPUuU1T8oixkVqLf00Sd9CLV=eQ@mail.gmail.com>
 <CAMAsSdJ69kSu7c1rRzbogxcLhT3g8JzxZSk35z7oV825E+=s7g@mail.gmail.com>
 <CAJWPb4LDcbad0+3CQ6jw_NtSp8x=SWNxb-hPQ7GsyM8OXLFDDQ@mail.gmail.com>
 <CACVFzXAWXJSiT5zxRSrgjEe0g0RPZf6Pwc72PLaq3s+MNdNN_Q@mail.gmail.com>","Regarding new GraphX algorithms, I am in agreement with the idea of publishing algorithms which are implemented using the existing API as outside packages.

Regarding SPARK-10335, we have a PR for SPARK-5484 which should address the problem described in that ticket. I've reviewed that PR, but because it touches the ML codebase I'd like to get an ML committer to review that PR. It's a relatively simple change and fixes an significant barrier to scaling in GraphX.

https://github.com/apache/spark/pull/15125

Cheers,

Michael


implement these features as  a third-party library (and then upload them to the spark packages https://spark-packages.org/ <https://spark-packages.org/>)? ISTM graphx has already necessary and sufficient APIs for these third-party ones.
on SPARK-7244[^2] and SPARK-7257[^3]. In fact, SPARK-7244 and SPARK-7257 have some importance on graph analysis field.
analysis, I hope to take them.
distinction. I figure that if something times out and is closed, it's very unlikely to be looked at again. Therefore marking it as something to do 'later' seemed less accurate.
contribution guide (http://spark.apache.org/contributing.html <http://spark.apache.org/contributing.html>) and
make prs feel free to (re?-)open tickets.
Feature"" for now.
the `Resolution` field for those issues.
<mailto:dev-unsubscribe@spark.apache.org>
<http://www.facebook.com/dongjin.lee.kr>
<http://kr.linkedin.com/in/dongjinleekr>
<http://github.com/dongjinleekr>
<http://www.twitter.com/dongjinleekr>

"
Mingjie Tang <tangrock@gmail.com>,"Thu, 19 Jan 2017 10:30:26 -0800",Re: Feedback on MLlib roadmap process proposal,Seth Hendrickson <seth.hendrickson16@gmail.com>,"+1 general abstractions like distributed linear algebra.


"
Asher Krim <akrim@hubspot.com>,"Thu, 19 Jan 2017 14:32:43 -0500",Re: Possible bug - Java iterator/iterable inconsistency,Sean Owen <sowen@cloudera.com>,"Thanks Sean!




-- 
Asher Krim
Senior Software Engineer
"
Felix Cheung <felixcheung_m@hotmail.com>,"Thu, 19 Jan 2017 21:51:12 +0000",Re: Feedback on MLlib roadmap process proposal,"Mingjie Tang <tangrock@gmail.com>, Seth Hendrickson
	<seth.hendrickson16@gmail.com>","Hi Seth

Re: ""The most important thing we can do, given that MLlib currently has a very limited committer review bandwidth, is to make clear issues that, if worked on, will definitely get reviewed. ""

We are adopting a Shepherd model, as described in the JIRA Joseph has, in which, when assigned, the Shepherd will see it through with the contributor to make sure it lands with the target release.

I'm sure Joseph can explain it better than I do ;)


_____________________________
From: Mingjie Tang <tangrock@gmail.com<mailto:tangrock@gmail.com>>
Sent: Thursday, January 19, 2017 10:30 AM
Subject: Re: Feedback on MLlib roadmap process proposal
To: Seth Hendrickson <seth.hendrickson16@gmail.com<mailto:seth.hendrickson16@gmail.com>>
Cc: Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>>, <dev@spark.apache.org<mailto:dev@spark.apache.org>>


+1 general abstractions like distributed linear algebra.

I think the proposal laid out in SPARK-18813 is well done, and I do think it is going to improve the process going forward. I also really like the idea of getting the community to vote on JIRAs to give some of them priority - provided that we listen to those votes, of course. The biggest problem I see is that we do have several active contributors and those who want to help implement these changes, but PRs are reviewed rather sporadically and I imagine it is very difficult for contributors to understand why some get reviewed and some do not. The most important thing we can do, given that MLlib currently has a very limited committer review bandwidth, is to make clear issues that, if worked on, will definitely get reviewed. A hard thing to do in open source, no doubt, but even if we have to limit the scope of such issues to a very small subset, it's a gain for all I think.

 goal of Spark MLlib (if this derails the original discussion, please let me know and we can discuss in another thread). The roadmap does contain specific items that help to convey some of this (ML parity with MLlib, model persistence, etc...), but I'm interested in what the ""mission"" of Spark MLlib is. We often see PRs for brand new algorithms which are sometimes rejected and sometimes not. Do we aim to keep implementing more and more algorithms? Or is our focus really, now that we have a reasonable library of algorithms, to simply make the existing ones faster/better/more robust? Should we aim to make interfaces that are easily extended for developers to easily implement their own custom code (e.g. custom optimization libraries), or do we want to restrict things to out-of-the box algorithms? Should we focus on more flexible, general abstractions like distributed linear algebra?

I was not involved in the project in the early days of MLlib when this discussion may have happened, but I think it would be useful to either revisit it or restate it here for some of the newer developers.

Hi all,

This is a general call for thoughts about the process for the MLlib roadmap proposed in SPARK-18813.  See the section called ""Roadmap process.""

Summary:
* This process is about committers indicating intention to shepherd and review.
* The goal is to improve visibility and communication.
* This is fairly orthogonal to the SIP discussion since this proposal is more about setting release targets than about proposing future plans.

Thanks!
Joseph

--

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[http://databricks.com]<http://databricks.com/>




"
Deepu Raj <deepuraj_tech@outlook.com>,"Thu, 19 Jan 2017 22:49:53 +0000",Spark Source Code Configuration,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi,

Is there any article/Docs/support to set up Apache Spark source code on  
Eclipse/InteliJ.

I have tried setting up the source code, by importing into Git & using  
maven. I am getting lot of compilation errors.

#suggestions

Regards,
Deepu Raj
+61 414 707 319

---------------------------------------------------------------------


"
Kai Jiang <jiangkai@gmail.com>,"Thu, 19 Jan 2017 23:27:16 +0000",Re: Spark Source Code Configuration,"Deepu Raj <deepuraj_tech@outlook.com>, 
	""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Deepu,

Hope this page can give you some help.
http://spark.apache.org/developer-tools.html

Best,
Kai


"
Dongjin Lee <dongjin@apache.org>,"Fri, 20 Jan 2017 10:34:04 +0900","Re: GraphX-related ""open"" issues","Michael Allman <michael@videoamp.com>, Takeshi Yamamuro <linguin.m.s@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for your comments. Then, How about change following issues (see
below) into 'won't fix'? After Implementing & uploading them as Spark
Packages, commenting on those issues would be a reasonable solution. It
would also be better for the potential users of those graph algorithms.

- SPARK-15880: PREGEL Based Semi-Clustering Algorithm Implementation using
Spark GraphX API <https://issues.apache.org/jira/browse/SPARK-15880>
- SPARK-7244: Find vertex sequences satisfying predicates
<https://issues.apache.org/jira/browse/SPARK-7244>
- SPARK-7257: Find nearest neighbor satisfying predicate
<https://issues.apache.org/jira/browse/SPARK-7257>
- SPARK-8497: Graph Clique(Complete Connected Sub-graph) Discovery Algorithm
<https://issues.apache.org/jira/browse/SPARK-8497>

Best,
Dongjin




-- 
*Dongjin Lee*


*Software developer in Line+.So interested in massive-scale machine
learning.facebook: www.facebook.com/dongjin.lee.kr
<http://www.facebook.com/dongjin.lee.kr>linkedin:
kr.linkedin.com/in/dongjinleekr
<http://kr.linkedin.com/in/dongjinleekr>github:
<http://goog_969573159/>github.com/dongjinleekr
<http://github.com/dongjinleekr>twitter: www.twitter.com/dongjinleekr
<http://www.twitter.com/dongjinleekr>*
"
Keith Chapman <keithgchapman@gmail.com>,"Thu, 19 Jan 2017 18:57:42 -0800",,dev@spark.apache.org,"Hi ,

Is it possible for an executor (or slave) to know when an actual job ends?
I'm running spark on a cluster (with yarn) and my workers create some
temporary files that I would like to clean up once the job ends. Is there a
way for the worker to detect that a job has finished? I tried doing it in
the JobProgressListener but it does not seem to work in a cluster. The
event is not triggered in the worker.

Regards,
Keith.

http://keith-chapman.com
"
Michael Allman <michael@videoamp.com>,"Thu, 19 Jan 2017 20:27:17 -0800","Re: GraphX-related ""open"" issues","Dongjin Lee <dongjin@apache.org>
 <pony-6ff48b33cbb828151035af58a92f18a39f1d1036-424de1f36582e993d75c4826d9a55738432c6d61@dev.spark.apache.org>
 <CACVFzXBQynt97bH4ekegR-CtPUuU1T8oixkVqLf00Sd9CLV=eQ@mail.gmail.com>
 <CAMAsSdJ69kSu7c1rRzbogxcLhT3g8JzxZSk35z7oV825E+=s7g@mail.gmail.com>
 <CAJWPb4LDcbad0+3CQ6jw_NtSp8x=SWNxb-hPQ7GsyM8OXLFDDQ@mail.gmail.com>
 <CACVFzXAWXJSiT5zxRSrgjEe0g0RPZf6Pwc72PLaq3s+MNdNN_Q@mail.gmail.com>
 <7ADCF1A3-D282-41A0-9AA3-B7F6B23BCF9C@videoamp.com>
 <CAJWPb4+WpiLx-BF4uPVrSVymZ2Ku7fgBkq=BW_hWYxTNJtiFKA@mail.gmail.com>","That sounds fine to me. I think that in closing the issues, we should mention that we're closing them because these algorithms can be implemented using the existing API.

Michael


below) into 'won't fix'? After Implementing & uploading them as Spark Packages, commenting on those issues would be a reasonable solution. It would also be better for the potential users of those graph algorithms.
using Spark GraphX API <https://issues.apache.org/jira/browse/SPARK-15880>
<https://issues.apache.org/jira/browse/SPARK-7244>
<https://issues.apache.org/jira/browse/SPARK-7257>
Algorithm <https://issues.apache.org/jira/browse/SPARK-8497>
publishing algorithms which are implemented using the existing API as outside packages.
address the problem described in that ticket. I've reviewed that PR, but because it touches the ML codebase I'd like to get an ML committer to review that PR. It's a relatively simple change and fixes an significant barrier to scaling in GraphX.
<https://github.com/apache/spark/pull/15125>
implement these features as  a third-party library (and then upload them to the spark packages https://spark-packages.org/ <https://spark-packages.org/>)? ISTM graphx has already necessary and sufficient APIs for these third-party ones.
on SPARK-7244[^2] and SPARK-7257[^3]. In fact, SPARK-7244 and SPARK-7257 have some importance on graph analysis field.
analysis, I hope to take them.
distinction. I figure that if something times out and is closed, it's very unlikely to be looked at again. Therefore marking it as something to do 'later' seemed less accurate.
contribution guide (http://spark.apache.org/contributing.html <http://spark.apache.org/contributing.html>) and
make prs feel free to (re?-)open tickets.
Feature"" for now.
the `Resolution` field for those issues.
<mailto:dev-unsubscribe@spark.apache.org>
<http://www.facebook.com/dongjin.lee.kr>
<http://kr.linkedin.com/in/dongjinleekr>
<http://github.com/dongjinleekr>
<http://www.twitter.com/dongjinleekr>
<http://www.facebook.com/dongjin.lee.kr>
<http://kr.linkedin.com/in/dongjinleekr>
<http://github.com/dongjinleekr>
<http://www.twitter.com/dongjinleekr>
"
Keith Chapman <keithgchapman@gmail.com>,"Thu, 19 Jan 2017 21:31:38 -0800",Is it possible to get a job end kind of notification on the executor (slave),dev@spark.apache.org,"Hi ,

Is it possible for an executor (or slave) to know when an actual job ends?
I'm running spark on a cluster (with yarn) and my workers create some
temporary files that I would like to clean up once the job ends. Is there a
way for the worker to detect that a job has finished? I tried doing it in
the JobProgressListener but it does not seem to work in a cluster. The
event is not triggered in the worker.

Regards,
Keith.

http://keith-chapman.com
"
Deepu Raj <deepuraj_tech@outlook.com>,"Fri, 20 Jan 2017 05:42:19 +0000",Re: Spark Source Code Configuration,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, Kai Jiang <jiangkai@gmail.com>","Thanks Kai,

I am getting the message and its stuck when I run sbt any idea:-

""Set current project to spark-parent (in build file:/home/cloudera/spark/)""

Details attached.

Regards,
Deepu Raj
+61 414 707 319






Hi Deepu,

Hope this page can give you some help. http://spark.apache.org/developer-tools.html

Best,
Kai

Hi,

Is there any article/Docs/support to set up Apache Spark source code on
Eclipse/InteliJ.

I have tried setting up the source code, by importing into Git & using
maven. I am getting lot of compilation errors.

#suggestions

Regards,
Deepu Raj
+61 414 707 319

---------------------------------------------------------------------
ibe@spark.apache.org>




--
Using Opera's mail client: http://www.opera.com/mail/
[cloudera@quickstart spark]$ sudo yum install sbt
Loaded plugins: fastestmirror, security
Setting up Install Process
Loading mirror speeds from cached hostfile
epel/metalink                                                      | 2.7 kB     00:00
 * base: centos.uberglobalmirror.com
 * epel: mirror.intergrid.com.au
 * extras: centos.uberglobalmirror.com
 * updates: mirror.ventraip.net.au
base                                                               | 3.7 kB     00:00
base/primary_db                                                    | 4.7 MB     00:04
bintray--sbt-rpm                                                   | 1.3 kB     00:00
bintray--sbt-rpm/primary                                           | 2.0 kB     00:00
bintray--sbt-rpm                                                                    13/13
cloudera-cdh5                                                      |  951 B     00:00
cloudera-cdh5/primary                                              |  43 kB     00:00
cloudera-cdh5                                                                     146/146
cloudera-gplextras5                                                |  951 B     00:00
cloudera-gplextras5/primary                                        | 2.4 kB     00:00
cloudera-gplextras5                                                                   9/9
cloudera-kafka                                                     |  951 B     00:00
cloudera-kafka/primary                                             | 1.8 kB     00:00
cloudera-kafka                                                                        4/4
cloudera-manager                                                   |  951 B     00:00
cloudera-manager/primary                                           | 4.3 kB     00:00
cloudera-manager                                                                      7/7
http://mirror.intergrid.com.au/epel/6/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR                                 6 - ""Couldn't resolve host 'mirror.intergrid.com.au'""
Trying other mirror.
http://epel.mirror.web24.net.au/6/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 6 -                                 ""Couldn't resolve host 'epel.mirror.web24.net.au'""
Trying other mirror.
epel                                                               | 4.3 kB     00:00
epel/primary_db                                                    | 5.9 MB     00:04
extras                                                             | 3.4 kB     00:00
extras/primary_db                                                  |  37 kB     00:00
updates                                                            | 3.4 kB     00:00
updates/primary_db                                                 | 4.3 MB     00:04
Resolving Dependencies
--> Running transaction check
---> Package sbt.noarch 0:0.13.13.1-1 will be installed
--> Processing Dependency: jpackage-utils for package: sbt-0.13.13.1-1.noarch
--> Processing Dependency: java-devel for package: sbt-0.13.13.1-1.noarch
--> Running transaction check
---> Package java-1.7.0-openjdk-devel.x86_64 1:1.7.0.121-2.6.8.1.el6_8 will be installed
--> Processing Dependency: java-1.7.0-openjdk = 1:1.7.0.121-2.6.8.1.el6_8 for package: 1:j                                ava-1.7.0-openjdk-devel-1.7.0.121-2.6.8.1.el6_8.x86_64
---> Package jpackage-utils.noarch 0:1.7.5-3.16.el6 will be installed
--> Running transaction check
---> Package java-1.7.0-openjdk.x86_64 1:1.7.0.121-2.6.8.1.el6_8 will be installed
--> Processing Dependency: nss(x86-64) >= 3.21.0 for package: 1:java-1.7.0-openjdk-1.7.0.1                                21-2.6.8.1.el6_8.x86_64
--> Processing Dependency: xorg-x11-fonts-Type1 for package: 1:java-1.7.0-openjdk-1.7.0.12                                1-2.6.8.1.el6_8.x86_64
--> Processing Dependency: tzdata-java for package: 1:java-1.7.0-openjdk-1.7.0.121-2.6.8.1                                .el6_8.x86_64
--> Processing Dependency: libsctp.so.1(VERS_1)(64bit) for package: 1:java-1.7.0-openjdk-1                                .7.0.121-2.6.8.1.el6_8.x86_64
--> Processing Dependency: libsctp.so.1()(64bit) for package: 1:java-1.7.0-openjdk-1.7.0.1                                21-2.6.8.1.el6_8.x86_64
--> Processing Dependency: libpcsclite.so.1()(64bit) for package: 1:java-1.7.0-openjdk-1.7                                .0.121-2.6.8.1.el6_8.x86_64
--> Processing Dependency: libgif.so.4()(64bit) for package: 1:java-1.7.0-openjdk-1.7.0.12                                1-2.6.8.1.el6_8.x86_64
--> Running transaction check
---> Package giflib.x86_64 0:4.1.6-3.1.el6 will be installed
---> Package lksctp-tools.x86_64 0:1.0.10-7.el6 will be installed
---> Package nss.x86_64 0:3.18.0-5.3.el6_6 will be updated
--> Processing Dependency: nss = 3.18.0-5.3.el6_6 for package: nss-sysinit-3.18.0-5.3.el6_                                6.x86_64
--> Processing Dependency: nss(x86-64) = 3.18.0-5.3.el6_6 for package: nss-tools-3.18.0-5.                                3.el6_6.x86_64
---> Package nss.x86_64 0:3.21.3-2.el6_8 will be an update
--> Processing Dependency: nss-util >= 3.21.0 for package: nss-3.21.3-2.el6_8.x86_64
--> Processing Dependency: nspr >= 4.11.0 for package: nss-3.21.3-2.el6_8.x86_64
--> Processing Dependency: libnssutil3.so(NSSUTIL_3.21)(64bit) for package: nss-3.21.3-2.e                                l6_8.x86_64
---> Package pcsc-lite-libs.x86_64 0:1.5.2-15.el6 will be installed
---> Package tzdata-java.noarch 0:2016j-1.el6 will be installed
---> Package xorg-x11-fonts-Type1.noarch 0:7.2-11.el6 will be installed
--> Processing Dependency: ttmkfdir for package: xorg-x11-fonts-Type1-7.2-11.el6.noarch
--> Processing Dependency: ttmkfdir for package: xorg-x11-fonts-Type1-7.2-11.el6.noarch
--> Running transaction check
---> Package nspr.x86_64 0:4.10.8-1.el6_6 will be updated
---> Package nspr.x86_64 0:4.11.0-1.el6 will be an update
---> Package nss-sysinit.x86_64 0:3.18.0-5.3.el6_6 will be updated
---> Package nss-sysinit.x86_64 0:3.21.3-2.el6_8 will be an update
---> Package nss-tools.x86_64 0:3.18.0-5.3.el6_6 will be updated
---> Package nss-tools.x86_64 0:3.21.3-2.el6_8 will be an update
---> Package nss-util.x86_64 0:3.18.0-1.el6_6 will be updated
---> Package nss-util.x86_64 0:3.21.3-1.el6_8 will be an update
---> Package ttmkfdir.x86_64 0:3.0.9-32.1.el6 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==========================================================================================
 Package                    Arch     Version                     Repository          Size
==========================================================================================
Installing:
 sbt                        noarch   0.13.13.1-1                 bintray--sbt-rpm   1.0 M
Installing for dependencies:
 giflib                     x86_64   4.1.6-3.1.el6               base                37 k
 java-1.7.0-openjdk         x86_64   1:1.7.0.121-2.6.8.1.el6_8   updates             26 M
 java-1.7.0-openjdk-devel   x86_64   1:1.7.0.121-2.6.8.1.el6_8   updates            9.4 M
 jpackage-utils             noarch   1.7.5-3.16.el6              base                60 k
 lksctp-tools               x86_64   1.0.10-7.el6                base                79 k
 pcsc-lite-libs             x86_64   1.5.2-15.el6                base                28 k
 ttmkfdir                   x86_64   3.0.9-32.1.el6              base                43 k
 tzdata-java                noarch   2016j-1.el6                 updates            182 k
 xorg-x11-fonts-Type1       noarch   7.2-11.el6                  base               520 k
Updating for dependencies:
 nspr                       x86_64   4.11.0-1.el6                base               114 k
 nss                        x86_64   3.21.3-2.el6_8              updates            859 k
 nss-sysinit                x86_64   3.21.3-2.el6_8              updates             47 k
 nss-tools                  x86_64   3.21.3-2.el6_8              updates            437 k
 nss-util                   x86_64   3.21.3-1.el6_8              updates             67 k

Transaction Summary
==========================================================================================
Install      10 Package(s)
Upgrade       5 Package(s)

Total download size: 39 M
Is this ok [y/N]: y
Downloading Packages:
(1/15): giflib-4.1.6-3.1.el6.x86_64.rpm                            |  37 kB     00:00
http://mirror.ventraip.net.au/CentOS/6.8/updates/x86_64/Packages/java-1.7.0-openjdk-1.7.0.                                121-2.6.8.1.el6_8.x86_64.rpm: [Errno 12] Timeout on http://mirror.ventraip.net.au/CentOS/6                                .8/updates/x86_64/Packages/java-1.7.0-openjdk-1.7.0.121-2.6.8.1.el6_8.x86_64.rpm: (28, 'Op                                eration too slow. Less than 1 bytes/sec transfered the last 30 seconds')
Trying other mirror.
(2/15): java-1.7.0-openjdk-1.7.0.121-2.6.8.1.el6_8.x86_64.rpm      |  26 MB     00:00
(3/15): java-1.7.0-openjdk-devel-1.7.0.121-2.6.8.1.el6_8.x86_64.rp | 9.4 MB     00:07
(4/15): jpackage-utils-1.7.5-3.16.el6.noarch.rpm                   |  60 kB     00:00
(5/15): lksctp-tools-1.0.10-7.el6.x86_64.rpm                       |  79 kB     00:00
(6/15): nspr-4.11.0-1.el6.x86_64.rpm                               | 114 kB     00:00
(7/15): nss-3.21.3-2.el6_8.x86_64.rpm                              | 859 kB     00:00
(8/15): nss-sysinit-3.21.3-2.el6_8.x86_64.rpm                      |  47 kB     00:00
(9/15): nss-tools-3.21.3-2.el6_8.x86_64.rpm                        | 437 kB     00:00
(10/15): nss-util-3.21.3-1.el6_8.x86_64.rpm                        |  67 kB     00:00
(11/15): pcsc-lite-libs-1.5.2-15.el6.x86_64.rpm                    |  28 kB     00:00
(12/15): sbt-0.13.13.1.rpm                                         | 1.0 MB     00:01
(13/15): ttmkfdir-3.0.9-32.1.el6.x86_64.rpm                        |  43 kB     00:00
(14/15): tzdata-java-2016j-1.el6.noarch.rpm                        | 182 kB     00:00
(15/15): xorg-x11-fonts-Type1-7.2-11.el6.noarch.rpm                | 520 kB     00:00
------------------------------------------------------------------------------------------
Total                                                     519 kB/s |  39 MB     01:17
warning: rpmts_HdrFromFdno: Header V3 RSA/SHA256 Signature, key ID c105b9de: NOKEY
Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
Importing GPG key 0xC105B9DE:
 Userid : CentOS-6 Key (CentOS 6 Official Signing Key) <centos-6-key@centos.org>
 Package: centos-release-6-7.el6.centos.12.3.x86_64 (@CentOS 6.7 Mirror/6.7)
 From   : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
Is this ok [y/N]: y
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Updating   : nspr-4.11.0-1.el6.x86_64                                              1/20
  Updating   : nss-util-3.21.3-1.el6_8.x86_64                                        2/20
  Installing : jpackage-utils-1.7.5-3.16.el6.noarch                                  3/20
  Updating   : nss-3.21.3-2.el6_8.x86_64                                             4/20
  Updating   : nss-sysinit-3.21.3-2.el6_8.x86_64                                     5/20
  Installing : tzdata-java-2016j-1.el6.noarch                                        6/20
  Installing : lksctp-tools-1.0.10-7.el6.x86_64                                      7/20
  Installing : giflib-4.1.6-3.1.el6.x86_64                                           8/20
  Installing : pcsc-lite-libs-1.5.2-15.el6.x86_64                                    9/20
  Installing : ttmkfdir-3.0.9-32.1.el6.x86_64                                       10/20
  Installing : xorg-x11-fonts-Type1-7.2-11.el6.noarch                               11/20
  Installing : 1:java-1.7.0-openjdk-devel-1.7.0.121-2.6.8.1.el6_8.x86_64            12/20
  Installing : 1:java-1.7.0-openjdk-1.7.0.121-2.6.8.1.el6_8.x86_64                  13/20
  Installing : sbt-0.13.13.1-1.noarch                                               14/20
  Updating   : nss-tools-3.21.3-2.el6_8.x86_64                                      15/20
  Cleanup    : nss-tools-3.18.0-5.3.el6_6.x86_64                                    16/20
  Cleanup    : nss-3.18.0-5.3.el6_6.x86_64                                          17/20
  Cleanup    : nss-sysinit-3.18.0-5.3.el6_6.x86_64                                  18/20
  Cleanup    : nss-util-3.18.0-1.el6_6.x86_64                                       19/20
  Cleanup    : nspr-4.10.8-1.el6_6.x86_64                                           20/20
  Verifying  : ttmkfdir-3.0.9-32.1.el6.x86_64                                        1/20
  Verifying  : 1:java-1.7.0-openjdk-devel-1.7.0.121-2.6.8.1.el6_8.x86_64             2/20
  Verifying  : pcsc-lite-libs-1.5.2-15.el6.x86_64                                    3/20
  Verifying  : nss-sysinit-3.21.3-2.el6_8.x86_64                                     4/20
  Verifying  : giflib-4.1.6-3.1.el6.x86_64                                           5/20
  Verifying  : jpackage-utils-1.7.5-3.16.el6.noarch                                  6/20
  Verifying  : lksctp-tools-1.0.10-7.el6.x86_64                                      7/20
  Verifying  : nss-3.21.3-2.el6_8.x86_64                                             8/20
  Verifying  : nss-tools-3.21.3-2.el6_8.x86_64                                       9/20
  Verifying  : nss-util-3.21.3-1.el6_8.x86_64                                       10/20
  Verifying  : xorg-x11-fonts-Type1-7.2-11.el6.noarch                               11/20
  Verifying  : sbt-0.13.13.1-1.noarch                                               12/20
  Verifying  : 1:java-1.7.0-openjdk-1.7.0.121-2.6.8.1.el6_8.x86_64                  13/20
  Verifying  : tzdata-java-2016j-1.el6.noarch                                       14/20
  Verifying  : nspr-4.11.0-1.el6.x86_64                                             15/20
  Verifying  : nss-3.18.0-5.3.el6_6.x86_64                                          16/20
  Verifying  : nss-sysinit-3.18.0-5.3.el6_6.x86_64                                  17/20
  Verifying  : nss-util-3.18.0-1.el6_6.x86_64                                       18/20
  Verifying  : nss-tools-3.18.0-5.3.el6_6.x86_64                                    19/20
  Verifying  : nspr-4.10.8-1.el6_6.x86_64                                           20/20

Installed:
  sbt.noarch 0:0.13.13.1-1

Dependency Installed:
  giflib.x86_64 0:4.1.6-3.1.el6
  java-1.7.0-openjdk.x86_64 1:1.7.0.121-2.6.8.1.el6_8
  java-1.7.0-openjdk-devel.x86_64 1:1.7.0.121-2.6.8.1.el6_8
  jpackage-utils.noarch 0:1.7.5-3.16.el6
  lksctp-tools.x86_64 0:1.0.10-7.el6
  pcsc-lite-libs.x86_64 0:1.5.2-15.el6
  ttmkfdir.x86_64 0:3.0.9-32.1.el6
  tzdata-java.noarch 0:2016j-1.el6
  xorg-x11-fonts-Type1.noarch 0:7.2-11.el6

Dependency Updated:
  nspr.x86_64 0:4.11.0-1.el6                   nss.x86_64 0:3.21.3-2.el6_8
  nss-sysinit.x86_64 0:3.21.3-2.el6_8          nss-tools.x86_64 0:3.21.3-2.el6_8
  nss-util.x86_64 0:3.21.3-1.el6_8

Complete!
[cloudera@quickstart spark]$ sbt
Getting org.scala-sbt sbt 0.13.7 ...
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.7/jars/                                sbt.jar ...
        [SUCCESSFUL ] org.scala-sbt#sbt;0.13.7!sbt.jar (5469ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.4/scala-libra                                ry-2.10.4.jar ...
        [SUCCESSFUL ] org.scala-lang#scala-library;2.10.4!scala-library.jar (11740ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/main/0.13.7/jars/main.jar ...
        [SUCCESSFUL ] org.scala-sbt#main;0.13.7!main.jar (11637ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compiler-interface/0.13.7/jars/compiler-interface-bin.jar ...
        [SUCCESSFUL ] org.scala-sbt#compiler-interface;0.13.7!compiler-interface-bin.jar (6160ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compiler-interface/0.13.7/jars/compiler-interface-src.jar ...
        [SUCCESSFUL ] org.scala-sbt#compiler-interface;0.13.7!compiler-interface-src.jar (5690ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/precompiled-2_8_2/0.13.7/jars/compiler-interface-bin.jar ...
        [SUCCESSFUL ] org.scala-sbt#precompiled-2_8_2;0.13.7!compiler-interface-bin.jar (6274ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/precompiled-2_9_2/0.13.7/jars/compiler-interface-bin.jar ...
        [SUCCESSFUL ] org.scala-sbt#precompiled-2_9_2;0.13.7!compiler-interface-bin.jar (6512ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/precompiled-2_9_3/0.13.7/jars/compiler-interface-bin.jar ...
        [SUCCESSFUL ] org.scala-sbt#precompiled-2_9_3;0.13.7!compiler-interface-bin.jar (6055ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/actions/0.13.7/jars/actions.jar ...
        [SUCCESSFUL ] org.scala-sbt#actions;0.13.7!actions.jar (6402ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/main-settings/0.13.7/jars/main-settings.jar ...
        [SUCCESSFUL ] org.scala-sbt#main-settings;0.13.7!main-settings.jar (6387ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/interface/0.13.7/jars/interface.jar ...
        [SUCCESSFUL ] org.scala-sbt#interface;0.13.7!interface.jar (5657ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/io/0.13.7/jars/io.jar ...
        [SUCCESSFUL ] org.scala-sbt#io;0.13.7!io.jar (7317ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/ivy/0.13.7/jars/ivy.jar ...
        [SUCCESSFUL ] org.scala-sbt#ivy;0.13.7!ivy.jar (7557ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/launcher-interface/0.13.7/jars/launcher-interface.jar ...
        [SUCCESSFUL ] org.scala-sbt#launcher-interface;0.13.7!launcher-interface.jar (5465ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/logging/0.13.7/jars/logging.jar ...
        [SUCCESSFUL ] org.scala-sbt#logging;0.13.7!logging.jar (6472ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/logic/0.13.7/jars/logic.jar ...
        [SUCCESSFUL ] org.scala-sbt#logic;0.13.7!logic.jar (6348ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/process/0.13.7/jars/process.jar ...
        [SUCCESSFUL ] org.scala-sbt#process;0.13.7!process.jar (6332ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/run/0.13.7/jars/run.jar ...
        [SUCCESSFUL ] org.scala-sbt#run;0.13.7!run.jar (6426ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/command/0.13.7/jars/command.jar ...
        [SUCCESSFUL ] org.scala-sbt#command;0.13.7!command.jar (6539ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/classpath/0.13.7/jars/classpath.jar ...
        [SUCCESSFUL ] org.scala-sbt#classpath;0.13.7!classpath.jar (5654ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/completion/0.13.7/jars/completion.jar ...
        [SUCCESSFUL ] org.scala-sbt#completion;0.13.7!completion.jar (6347ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/api/0.13.7/jars/api.jar ...
        [SUCCESSFUL ] org.scala-sbt#api;0.13.7!api.jar (6145ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compiler-integration/0.13.7/jars/compiler-integration.jar ...
        [SUCCESSFUL ] org.scala-sbt#compiler-integration;0.13.7!compiler-integration.jar (5868ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compiler-ivy-integration/0.13.7/jars/compiler-ivy-integration.jar ...
        [SUCCESSFUL ] org.scala-sbt#compiler-ivy-integration;0.13.7!compiler-ivy-integration.jar (5324ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/relation/0.13.7/jars/relation.jar ...
        [SUCCESSFUL ] org.scala-sbt#relation;0.13.7!relation.jar (5567ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/task-system/0.13.7/jars/task-system.jar ...
        [SUCCESSFUL ] org.scala-sbt#task-system;0.13.7!task-system.jar (6112ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/tasks/0.13.7/jars/tasks.jar ...
        [SUCCESSFUL ] org.scala-sbt#tasks;0.13.7!tasks.jar (5966ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/tracking/0.13.7/jars/tracking.jar ...
        [SUCCESSFUL ] org.scala-sbt#tracking;0.13.7!tracking.jar (5651ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/testing/0.13.7/jars/testing.jar ...
        [SUCCESSFUL ] org.scala-sbt#testing;0.13.7!testing.jar (6110ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar ...
        [SUCCESSFUL ] org.scala-lang#scala-compiler;2.10.4!scala-compiler.jar (16507ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar ...
        [SUCCESSFUL ] org.scala-lang#scala-reflect;2.10.4!scala-reflect.jar (3286ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/control/0.13.7/jars/control.jar ...
        [SUCCESSFUL ] org.scala-sbt#control;0.13.7!control.jar (6118ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/collections/0.13.7/jars/collections.jar ...
        [SUCCESSFUL ] org.scala-sbt#collections;0.13.7!collections.jar (6243ms)
downloading https://repo1.maven.org/maven2/jline/jline/2.11/jline-2.11.jar ...
        [SUCCESSFUL ] jline#jline;2.11!jline.jar (2310ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/incremental-compiler/0.13.7/jars/incremental-compiler.jar ...
        [SUCCESSFUL ] org.scala-sbt#incremental-compiler;0.13.7!incremental-compiler.jar (6228ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compile/0.13.7/jars/compile.jar ...
        [SUCCESSFUL ] org.scala-sbt#compile;0.13.7!compile.jar (5989ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/persist/0.13.7/jars/persist.jar ...
        [SUCCESSFUL ] org.scala-sbt#persist;0.13.7!persist.jar (6223ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/classfile/0.13.7/jars/classfile.jar ...
        [SUCCESSFUL ] org.scala-sbt#classfile;0.13.7!classfile.jar (5696ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-tools.sbinary/sbinary_2.10/0.4.2/jars/sbinary_2.10.jar ...
        [SUCCESSFUL ] org.scala-tools.sbinary#sbinary_2.10;0.4.2!sbinary_2.10.jar (6189ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/cross/0.13.7/jars/cross.jar ...
        [SUCCESSFUL ] org.scala-sbt#cross;0.13.7!cross.jar (5317ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt.ivy/ivy/2.3.0-sbt-fccfbd44c9f64523b61398a0155784dcbaeae28f/jars/ivy.jar ...
        [SUCCESSFUL ] org.scala-sbt.ivy#ivy;2.3.0-sbt-fccfbd44c9f64523b61398a0155784dcbaeae28f!ivy.jar (6917ms)
downloading https://repo1.maven.org/maven2/com/jcraft/jsch/0.1.46/jsch-0.1.46.jar ...
        [SUCCESSFUL ] com.jcraft#jsch;0.1.46!jsch.jar (2652ms)
downloading https://repo1.maven.org/maven2/org/json4s/json4s-native_2.10/3.2.10/json4s-native_2.10-3.2.10.jar ...
        [SUCCESSFUL ] org.json4s#json4s-native_2.10;3.2.10!json4s-native_2.10.jar (996ms)
downloading https://repo1.maven.org/maven2/org/spire-math/jawn-parser_2.10/0.6.0/jawn-parser_2.10-0.6.0.jar ...
        [SUCCESSFUL ] org.spire-math#jawn-parser_2.10;0.6.0!jawn-parser_2.10.jar (1066ms)
downloading https://repo1.maven.org/maven2/org/spire-math/json4s-support_2.10/0.6.0/json4s-support_2.10-0.6.0.jar ...
        [SUCCESSFUL ] org.spire-math#json4s-support_2.10;0.6.0!json4s-support_2.10.jar (804ms)
downloading https://repo1.maven.org/maven2/org/json4s/json4s-core_2.10/3.2.10/json4s-core_2.10-3.2.10.jar ...
        [SUCCESSFUL ] org.json4s#json4s-core_2.10;3.2.10!json4s-core_2.10.jar (2138ms)
downloading https://repo1.maven.org/maven2/org/json4s/json4s-ast_2.10/3.2.10/json4s-ast_2.10-3.2.10.jar ...
        [SUCCESSFUL ] org.json4s#json4s-ast_2.10;3.2.10!json4s-ast_2.10.jar (898ms)
downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar ...
        [SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.6!paranamer.jar (824ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scalap/2.10.0/scalap-2.10.0.jar ...
        [SUCCESSFUL ] org.scala-lang#scalap;2.10.0!scalap.jar (2235ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/cache/0.13.7/jars/cache.jar ...
        [SUCCESSFUL ] org.scala-sbt#cache;0.13.7!cache.jar (6741ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/test-agent/0.13.7/jars/test-agent.jar ...
        [SUCCESSFUL ] org.scala-sbt#test-agent;0.13.7!test-agent.jar (5478ms)
downloading https://repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar ...
        [SUCCESSFUL ] org.scala-sbt#test-interface;1.0!test-interface.jar (1488ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/apply-macro/0.13.7/jars/apply-macro.jar ...
        [SUCCESSFUL ] org.scala-sbt#apply-macro;0.13.7!apply-macro.jar (5741ms)

:: problems summary ::
:::: ERRORS
        Server access Error: Connection reset url=https://repo1.maven.org/maven2/org/scala-sbt/classpath/0.13.7/classpath-0.13.7.pom


:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
:: retrieving :: org.scala-sbt#boot-app
        confs: [default]
        51 artifacts copied, 0 already retrieved (15709kB/222ms)
Getting Scala 2.10.4 (for sbt)...
downloading https://repo1.maven.org/maven2/org/scala-lang/jline/2.10.4/jline-2.10.4.jar ...
        [SUCCESSFUL ] org.scala-lang#jline;2.10.4!jline.jar (1391ms)
downloading https://repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.4/jansi-1.4.jar ...
        [SUCCESSFUL ] org.fusesource.jansi#jansi;1.4!jansi.jar (842ms)
:: retrieving :: org.scala-sbt#boot-scala
        confs: [default]
        5 artifacts copied, 0 already retrieved (24459kB/294ms)
[info] Loading project definition from /home/cloudera/spark/project/project
[info] Updating {file:/home/cloudera/spark/project/project/}spark-build-build...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] Done updating.
[info] Compiling 1 Scala source to /home/cloudera/spark/project/project/target/scala-2.10/sbt-0.13/classes...
[warn] there were 1 deprecation warning(s); re-run with -deprecation for details
[warn] one warning found
Initialized empty Git repository in /home/cloudera/.sbt/0.13/staging/ad8e8574a5bcb2d22d23/sbt-pom-reader/.git/
Branch ignore_artifact_id set up to track remote branch ignore_artifact_id from origin.
[info] Loading project definition from /home/cloudera/.sbt/0.13/staging/ad8e8574a5bcb2d22d23/sbt-pom-reader/project
[warn] Multiple resolvers having different access mechanism configured with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).
[info] Updating {file:/home/cloudera/.sbt/0.13/staging/ad8e8574a5bcb2d22d23/sbt-pom-reader/project/}sbt-pom-reader-build...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] downloading https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/com.typesafe.sbt/sbt-ghpages/scala_2.10/sbt_0.13/0.5.2/jars/sbt-ghpages.jar ...
[info]  [SUCCESSFUL ] com.typesafe.sbt#sbt-ghpages;0.5.2!sbt-ghpages.jar (7356ms)
[info] downloading https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/com.typesafe.sbt/sbt-site/scala_2.10/sbt_0.13/0.7.1/jars/sbt-site.jar ...
[info]  [SUCCESSFUL ] com.typesafe.sbt#sbt-site;0.7.1!sbt-site.jar (9777ms)
[info] downloading https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/com.typesafe.sbt/sbt-git/scala_2.10/sbt_0.13/0.6.2/jars/sbt-git.jar ...
[info]  [SUCCESSFUL ] com.typesafe.sbt#sbt-git;0.6.2!sbt-git.jar (5855ms)
[info] downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/scripted-plugin/0.13.7/jars/scripted-plugin.jar ...
[info]  [SUCCESSFUL ] org.scala-sbt#scripted-plugin;0.13.7!scripted-plugin.jar (5798ms)
[info] downloading https://repo1.maven.org/maven2/net/databinder/unfiltered-jetty_2.10/0.6.8/unfiltered-jetty_2.10-0.6.8.jar ...
[info]  [SUCCESSFUL ] net.databinder#unfiltered-jetty_2.10;0.6.8!unfiltered-jetty_2.10.jar (1598ms)
[info] downloading https://repo1.maven.org/maven2/net/databinder/unfiltered-util_2.10/0.6.8/unfiltered-util_2.10-0.6.8.jar ...
[info]  [SUCCESSFUL ] net.databinder#unfiltered-util_2.10;0.6.8!unfiltered-util_2.10.jar (858ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-webapp/7.6.9.v20130131/jetty-webapp-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-webapp;7.6.9.v20130131!jetty-webapp.jar (1066ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-xml/7.6.9.v20130131/jetty-xml-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-xml;7.6.9.v20130131!jetty-xml.jar (826ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-servlet/7.6.9.v20130131/jetty-servlet-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-servlet;7.6.9.v20130131!jetty-servlet.jar (1032ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/7.6.9.v20130131/jetty-util-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-util;7.6.9.v20130131!jetty-util.jar (1487ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-security/7.6.9.v20130131/jetty-security-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-security;7.6.9.v20130131!jetty-security.jar (995ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/7.6.9.v20130131/jetty-server-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-server;7.6.9.v20130131!jetty-server.jar (1418ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/orbit/javax.servlet/2.5.0.v201103041518/javax.servlet-2.5.0.v201103041518.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty.orbit#javax.servlet;2.5.0.v201103041518!javax.servlet.jar(orbit) (1013ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-continuation/7.6.9.v20130131/jetty-continuation-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-continuation;7.6.9.v20130131!jetty-continuation.jar (781ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/7.6.9.v20130131/jetty-http-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-http;7.6.9.v20130131!jetty-http.jar (936ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/7.6.9.v20130131/jetty-io-7.6.9.v20130131.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jetty#jetty-io;7.6.9.v20130131!jetty-io.jar (896ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jgit/org.eclipse.jgit.pgm/2.2.0.201212191850-r/org.eclipse.jgit.pgm-2.2.0.201212191850-r.jar ...
[info]  [SUCCESSFUL ] org.eclipse.jgit#org.eclipse.jgit.pgm;2.2.0.201212191850-r!org.eclipse.jgit.pgm.jar (1068ms)
[info] downloading https://repo1.maven.org/maven2/args4j/args4j/2.0.12/args4j-2.0.12.jar ...
[info]  [SUCCESSFUL ] args4j#args4j;2.0.12!args4j.jar (799ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/commons/commons-co"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 20 Jan 2017 15:36:38 +0900","Re: GraphX-related ""open"" issues",Michael Allman <michael@videoamp.com>,"IMO SPARK-10335 should be tagged with ""Bug""? If so, I think we should not
close it and fix in future.

// maropu




-- 
---
Takeshi Yamamuro
"
Ryan Blue <rblue@netflix.com.INVALID>,"Fri, 20 Jan 2017 09:26:10 -0800","Re: Is it possible to get a job end kind of notification on the
 executor (slave)",keithgchapman@gmail.com,"up when an executor exits. That's not quite what you want (job finish) but
is probably a reasonable fix.

rb





-- 
Ryan Blue
Software Engineer
Netflix
"
Michael Allman <michael@videoamp.com>,"Fri, 20 Jan 2017 14:55:48 -0800","Re: GraphX-related ""open"" issues","Takeshi Yamamuro <linguin.m.s@gmail.com>
 <pony-6ff48b33cbb828151035af58a92f18a39f1d1036-424de1f36582e993d75c4826d9a55738432c6d61@dev.spark.apache.org>
 <CACVFzXBQynt97bH4ekegR-CtPUuU1T8oixkVqLf00Sd9CLV=eQ@mail.gmail.com>
 <CAMAsSdJ69kSu7c1rRzbogxcLhT3g8JzxZSk35z7oV825E+=s7g@mail.gmail.com>
 <CAJWPb4LDcbad0+3CQ6jw_NtSp8x=SWNxb-hPQ7GsyM8OXLFDDQ@mail.gmail.com>
 <CACVFzXAWXJSiT5zxRSrgjEe0g0RPZf6Pwc72PLaq3s+MNdNN_Q@mail.gmail.com>
 <7ADCF1A3-D282-41A0-9AA3-B7F6B23BCF9C@videoamp.com>
 <CAJWPb4+WpiLx-BF4uPVrSVymZ2Ku7fgBkq=BW_hWYxTNJtiFKA@mail.gmail.com>
 <65BD9741-9B4F-45FF-84C9-6163552F3D6F@videoamp.com>
 <CACVFzXCCZmNLFVatYarD2SeOjT6m=Z9OCi3Hnd1J2zFnbEoX3Q@mail.gmail.com>","Yes, SPARK-10335 is a bug that will be fixed when SPARK-5484 is fixed.

not close it and fix in future.
mention that we're closing them because these algorithms can be implemented using the existing API.
(see below) into 'won't fix'? After Implementing & uploading them as Spark Packages, commenting on those issues would be a reasonable solution. It would also be better for the potential users of those graph algorithms.
using Spark GraphX API <https://issues.apache.org/jira/browse/SPARK-15880>
<https://issues.apache.org/jira/browse/SPARK-7244>
<https://issues.apache.org/jira/browse/SPARK-7257>
Algorithm <https://issues.apache.org/jira/browse/SPARK-8497>
publishing algorithms which are implemented using the existing API as outside packages.
address the problem described in that ticket. I've reviewed that PR, but because it touches the ML codebase I'd like to get an ML committer to review that PR. It's a relatively simple change and fixes an significant barrier to scaling in GraphX.
<https://github.com/apache/spark/pull/15125>
implement these features as  a third-party library (and then upload them to the spark packages https://spark-packages.org/ <https://spark-packages.org/>)? ISTM graphx has already necessary and sufficient APIs for these third-party ones.
interest on SPARK-7244[^2] and SPARK-7257[^3]. In fact, SPARK-7244 and SPARK-7257 have some importance on graph analysis field.
analysis, I hope to take them.
distinction. I figure that if something times out and is closed, it's very unlikely to be looked at again. Therefore marking it as something to do 'later' seemed less accurate.
contribution guide (http://spark.apache.org/contributing.html <http://spark.apache.org/contributing.html>) and
make prs feel free to (re?-)open tickets.
""New Feature"" for now.
in the `Resolution` field for those issues.
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<http://www.facebook.com/dongjin.lee.kr>
<http://kr.linkedin.com/in/dongjinleekr>
<http://github.com/dongjinleekr>
<http://www.twitter.com/dongjinleekr>
<http://www.facebook.com/dongjin.lee.kr>
<http://kr.linkedin.com/in/dongjinleekr>
<http://github.com/dongjinleekr>
<http://www.twitter.com/dongjinleekr>

"
StanZhai <mail@zhaishidan.cn>,"Sun, 22 Jan 2017 01:57:52 -0700 (MST)","Executors exceed maximum memory defined with `--executor-memory` in
 Spark 2.1.0",dev@spark.apache.org,"Hi all,

We just upgraded our Spark from 1.6.2 to 2.1.0.

Our Spark application is started by spark-submit with config of
`--executor-memory 35G` in standalone model, but the actual use of memory up
to 65G after a full gc(jmap -histo:live $pid) as follow:

test@c6 ~ $ ps aux | grep CoarseGrainedExecutorBackend
test      181941  181 34.7 94665384 68836752 ?   Sl   09:25 711:21
/home/test/service/jdk/bin/java -cp
/home/test/service/hadoop/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/test/service/hadoop/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/test/service/spark/conf/:/home/test/service/spark/jars/*:/home/test/service/hadoop/etc/hadoop/
-Xmx35840M -Dspark.driver.port=47781 -XX:+PrintGCDetails
-XX:+PrintGCDateStamps -Xloggc:./gc.log -verbose:gc
org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url
spark://CoarseGrainedScheduler@xxx.xxx.xxx.xxx:47781 --executor-id 1
--hostname test-192 --cores 36 --app-id app-20170122092509-0017 --worker-url
spark://Worker@test-192:33890

Our Spark jobs are all sql.

The exceed memory looks like off-heap memory, but the default value of
`spark.memory.offHeap.enabled` is `false`.

We didn't find the problem in Spark 1.6.x, what causes this in Spark 2.1.0?

Any help is greatly appreicated!

Best,
Stan



--

---------------------------------------------------------------------


"
Shuai Lin <linshuai2012@gmail.com>,"Sun, 22 Jan 2017 20:51:47 +0800",A question about creating persistent table when in-memory catalog is used,dev@spark.apache.org,"Hi all,

Currently when the in-memory catalog is used, e.g. through `--conf
spark.sql.catalogImplementation=in-memory`, we can create a persistent
table, but inserting into this table would fail with error message ""Hive
support is required to insert into the following tables.."".

    sql(""create table t1 (id int, name string, dept string)"") // OK
    sql(""insert into t1 values (1, 'name1', 'dept1')"")  // ERROR


This doesn't make sense for me, because this table would always be empty if
we can't insert into it, thus would be of no use. But I wonder if there are
other good reasons for the current logic. If not, I would propose to raise
an error when creating the table in the first place.

Thanks!

Regards,
Shuai Lin (@lins05)
"
Asher Krim <akrim@hubspot.com>,"Sun, 22 Jan 2017 12:35:11 -0500",Spark 1.6.3 Driver OOM on createDataFrame,dev@spark.apache.org,"Hi All,

There seems to be a bug in Spark 1.6.3 which causes the driver to OOM when
creating a dataframe using a lot of data in memory on the driver. Examining
a heap dump, it looks like the driver is filled with multiple copies of the
data. The following java code reproduces the bug:

  public void run() {
    try (JavaSparkContext sc = getSparkContext()) {
      SQLContext sqlContext = new SQLContext(sc);

      DataFrame df = sqlContext.createDataFrame(generateData().stream()
              .map(floats -> RowFactory.create(floats))
              .collect(Collectors.toList()),
          DataTypes.createStructType(new StructField[] { VECTOR_FIELD }));

      LOG.info(""successfully parallelized {} rows"", df.count());
    }

  }

private List<List<Float>> generateData() { List<List<Float>> data = new
ArrayList<>(3_000_000); for (int i = 0; i < 3_000_000; i++) { List<Float>
row = new ArrayList<>(300); for (int j = 0; j < 300; j++) { row.add(random.
nextFloat()); } data.add(row); }


Increasing the driver memory to insane values (28g) doesn't help. I tested
in Spark 2 and the problem seems to have been solved, however I'm not sure
which issue is responsible for solving it. I assume it's one of these:
https://issues.apache.org/jira/browse/SPARK-12511?jql=project%20%3D%20SPARK%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%202.0.0%20AND%20text%20~%20%22OOM%22

The reason this is an issue is because some machine learning models are
represented as large-ish local data structures on the driver, so this bug
is encountered while attempting to save them. Unfortunately, using mllib
instead of ml is not an option since some mllib algorithms also rely on the
dataframe API for persisting the model (such as word2vec and LDA), even
though mllib is supposed to be based on RDDs. This makes these algorithms
unusable for anything larger than toy examples in < Spark 2.

If anyone is familiar with this bug, I would really appreciate it if they
could point me in the direction of the pr that fixed it.

Is a 1.6.4 release planned?
Would be possible to backport the dataframe bugfix?

Thanks,
Asher Krim
Senior Software Engineer
"
Reynold Xin <rxin@databricks.com>,"Sun, 22 Jan 2017 18:45:41 +0000","Re: Executors exceed maximum memory defined with `--executor-memory`
 in Spark 2.1.0","StanZhai <mail@zhaishidan.cn>, dev@spark.apache.org","Are you using G1 GC? G1 sometimes uses a lot more memory than the size
allocated.



"
Reynold Xin <rxin@databricks.com>,"Sun, 22 Jan 2017 19:14:30 +0000","Re: A question about creating persistent table when in-memory catalog
 is used","Shuai Lin <linshuai2012@gmail.com>, dev@spark.apache.org","I think this is something we are going to change to completely decouple the
Hive support and catalog.



"
Xiao Li <gatorsmile@gmail.com>,"Sun, 22 Jan 2017 11:18:49 -0800","Re: A question about creating persistent table when in-memory catalog
 is used",Reynold Xin <rxin@databricks.com>,"We have a pending PR to block users to create the Hive serde table when
using InMemroyCatalog. See: https://github.com/apache/spark/pull/16587 I
believe it answers your question.

BTW, we still can create the regular data source tables and insert the data
into the tables. The major difference is whether the metadata is
persistently stored or not.

Thanks,

Xiao Li

2017-01-22 11:14 GMT-08:00 Reynold Xin <rxin@databricks.com>:

"
Reynold Xin <rxin@databricks.com>,"Sun, 22 Jan 2017 19:20:48 +0000","Re: A question about creating persistent table when in-memory catalog
 is used",Xiao Li <gatorsmile@gmail.com>,"the catalog, and the other is the Hive serde and UDF support. We want to
get to a point that the choice of catalog does not impact the functionality
in Spark other than where the catalog is stored.



"
Xiao Li <gatorsmile@gmail.com>,"Sun, 22 Jan 2017 11:21:49 -0800","Re: A question about creating persistent table when in-memory catalog
 is used",Reynold Xin <rxin@databricks.com>,"Agree. : )

2017-01-22 11:20 GMT-08:00 Reynold Xin <rxin@databricks.com>:

"
Koert Kuipers <koert@tresata.com>,"Sun, 22 Jan 2017 14:53:37 -0500","Re: Executors exceed maximum memory defined with `--executor-memory`
 in Spark 2.1.0",Reynold Xin <rxin@databricks.com>,"could this be related to SPARK-18787?


"
StanZhai <mail@zhaishidan.cn>,"Mon, 23 Jan 2017 00:36:53 -0700 (MST)","Re: Executors exceed maximum memory defined with
 `--executor-memory` in Spark 2.1.0",dev@spark.apache.org,"I'm using Parallel GC.
rxin wrote











--

---------------------------------------------------------------------


"
Shuai Lin <linshuai2012@gmail.com>,"Mon, 23 Jan 2017 16:01:09 +0800","Re: A question about creating persistent table when in-memory catalog
 is used",dev@spark.apache.org,"Cool, thanks for the info.

I think this is something we are going to change to completely decouple the


Is there a ticket for this? I did a search in jira and only found
""SPARK-16275: Implement all the Hive fallback functions"", which seems to be
related to it.



"
Xiao Li <gatorsmile@gmail.com>,"Mon, 23 Jan 2017 00:12:57 -0800","Re: A question about creating persistent table when in-memory catalog
 is used",Shuai Lin <linshuai2012@gmail.com>,"Reynold mentioned the direction we are heading. You can see many PRs the
community submitted are for this target. To achieve this, a lot of works we
need to do.

For example, for some serde, Hive metastore will infer the schema when the
schema is not provided, but our InMemoryCatalog does not have such a
capability. Thus, we need to see how to resolve this.

Hopefully, it answers your question. BTW, the issue you mentioned at the
beginning has been resolved. Please fetch the latest master. You are unable
to create such a hive serde table without Hive support.

Thanks,

Xiao Li


2017-01-23 0:01 GMT-08:00 Shuai Lin <linshuai2012@gmail.com>:

"
Michael Allman <michael@videoamp.com>,"Mon, 23 Jan 2017 10:04:00 -0800","Re: Executors exceed maximum memory defined with `--executor-memory`
 in Spark 2.1.0",StanZhai <mail@zhaishidan.cn>,"Hi Stan,

What OS/version are you using?

Michael

size
memory
/home/test/service/hadoop/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/test/service/hadoop/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/test/service/spark/conf/:/home/test/service/spark/jars/*:/home/test/service/hadoop/etc/hadoop/
of
http://apache-spark-developers-list.1001551.n3.nabble.com/Executors-exceed-maximum-memory-defined-with-executor-memory-in-Spark-2-1-0-tp20697.html
---------------------------------------------------------------------
http://apache-spark-developers-list.1001551.n3.nabble.com/Executors-exceed-maximum-memory-defined-with-executor-memory-in-Spark-2-1-0-tp20697p20707.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Executors-exceed-maximum-memory-defined-with-executor-memory-in-Spark-2-1-0-tp20697p20707.html>
Nabble.com <http://nabble.com/>.
<mailto:dev-unsubscribe@spark.apache.org>
"
Julien Le Dem <julien@dremio.com>,"Mon, 23 Jan 2017 11:43:40 -0800",Re: [VOTE] Release Apache Parquet 1.8.2 RC1,"dev@parquet.apache.org, dev@spark.apache.org","Hi Spark dev,
Here is the voting thread for parquet 1.8.2 release.
Cheng or someone else we would appreciate you verify it as well and reply
to the thread.


id
eck the
checksums.
d
o



-- 
Julien
"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 23 Jan 2017 12:02:06 -0800",Re: [VOTE] Release Apache Parquet 1.8.2 RC1,"Julien Le Dem <julien@dremio.com>, dev@parquet.apache.org,
 dev@spark.apache.org","Sorry for being late, I'm building a Spark branch based on the most 
recent master to test out 1.8.2-rc1, will post my result here ASAP.

Cheng



"
Julien Le Dem <julien@dremio.com>,"Mon, 23 Jan 2017 12:33:37 -0800",Re: [VOTE] Release Apache Parquet 1.8.2 RC1,Cheng Lian <lian.cs.zju@gmail.com>,"Thank you Cheng!


t
:
e
heck the
a
nd
po


-- 
Julien
"
Joseph Bradley <joseph@databricks.com>,"Mon, 23 Jan 2017 17:03:36 -0800",Re: Feedback on MLlib roadmap process proposal,Felix Cheung <felixcheung_m@hotmail.com>,"Hi Seth,

The proposal is geared towards exactly the issue you're describing:
providing more visibility into the capacity and intentions of committers.
If there are things you'd add to it or change to improve further, it would
be great to hear ideas!  The past roadmap JIRA has some more background
discussion which is worth looking at too.

Let's break off the MLlib mission discussion into another thread.  I'll
start one now.

Thanks,
Joseph




-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Joseph Bradley <joseph@databricks.com>,"Mon, 23 Jan 2017 17:03:41 -0800",MLlib mission and goals,"""dev@spark.apache.org"" <dev@spark.apache.org>","This thread is split off from the ""Feedback on MLlib roadmap process
proposal"" thread for discussing the high-level mission and goals for
MLlib.  I hope this thread will collect feedback and ideas, not necessarily
lead to huge decisions.

Copying from the previous thread:

*Seth:*
""""""
I would love to hear some discussion on the higher level goal of Spark
MLlib (if this derails the original discussion, please let me know and we
can discuss in another thread). The roadmap does contain specific items
that help to convey some of this (ML parity with MLlib, model persistence,
etc...), but I'm interested in what the ""mission"" of Spark MLlib is. We
often see PRs for brand new algorithms which are sometimes rejected and
sometimes not. Do we aim to keep implementing more and more algorithms? Or
is our focus really, now that we have a reasonable library of algorithms,
to simply make the existing ones faster/better/more robust? Should we aim
to make interfaces that are easily extended for developers to easily
implement their own custom code (e.g. custom optimization libraries), or do
we want to restrict things to out-of-the box algorithms? Should we focus on
more flexible, general abstractions like distributed linear algebra?

I was not involved in the project in the early days of MLlib when this
discussion may have happened, but I think it would be useful to either
revisit it or restate it here for some of the newer developers.
""""""

*Mingjie:*
""""""
+1 general abstractions like distributed linear algebra.
""""""


I'll add my thoughts, starting with our past *t**rajectory*:
* Initially, MLlib was mainly trying to build a set of core algorithms.
* Two years ago, the big effort was adding Pipelines.
* In the last year, big efforts have been around completing Pipelines and
making the library more robust.

I agree with Seth that a few *immediate goals* are very clear:
* feature parity for DataFrame-based API
* completing and improving testing for model persistence
* Python, R parity

*In the future*, it's harder to say, but if I had to pick my top 2 items,
I'd list:

*(1) Making MLlib more extensible*
It will not be feasible to support a huge number of algorithms, so allowing
users to customize their ML on Spark workflows will be critical.  This is
IMO the most important thing we could do for MLlib.
Part of this could be building a healthy community of Spark Packages, and
we will need to make it easier for users to write their own algorithms and
packages to facilitate this.  Part of this could be allowing users to
customize existing algorithms with custom loss functions, etc.

*(2) Consistent improvements to core algorithms*
A less exciting but still very important item will be constantly improving
the core set of algorithms in MLlib. This could mean speed, scaling,
robustness, and usability for the few algorithms which cover 90% of use
cases.

There are plenty of other possibilities, and it will be great to hear the
community's thoughts!

Thanks,
Joseph

-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Stephen Boesch <javadba@gmail.com>,"Mon, 23 Jan 2017 17:07:10 -0800",Re: MLlib mission and goals,Joseph Bradley <joseph@databricks.com>,"Along the lines of #1:  the spark packages seemed to have had a good start
about two years ago: but now there are not more than a handful in general
use - e.g. databricks CSV.
When the available packages are browsed the majority are incomplete, empty,
unmaintained, or unclear.

Any ideas on how to resurrect spark packages in a way that there will be
sufficient adoption for it to be meaningful?

2017-01-23 17:03 GMT-08:00 Joseph Bradley <joseph@databricks.com>:

"
Sean Owen <sowen@cloudera.com>,"Tue, 24 Jan 2017 10:37:11 +0000",Re: MLlib mission and goals,"""dev@spark.apache.org"" <dev@spark.apache.org>","My $0.02, which shouldn't be weighted too much.

I believe the mission as of Spark ML has been to provide the framework, and
then implementation of 'the basics' only. It should have the tools that
cover ~80% of use cases, out of the box, in a pretty well-supported and
tested way.

It's not a goal to support an arbitrarily large collection of algorithms
because each one adds marginally less value, and IMHO, is proportionally
bigger baggage, because the contributors tend to skew academic, produce
worse code, and don't stick around to maintain it.

The project is already generally quite overloaded; I don't know if there's
bandwidth to even cover the current scope. While 'the basics' is a
subjective label, de facto, I think we'd have to define it as essentially
""what we already have in place"" for the foreseeable future.

That the bits on spark-packages.org aren't so hot is not a problem but a
symptom. Would these really be better in the core project?

And, or: I entirely agree with Joseph's take.


"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Tue, 24 Jan 2017 12:03:39 +0100",Re: MLlib mission and goals,Sean Owen <sowen@cloudera.com>,"I also agree with Joseph and Sean.
With respect to spark-packages. I think the issue is that you have to manually add it, although it basically fetches the package from Maven Central (or custom upload).

From an organizational perspective there are other issues. E.g. You have to download it from the internet instead of using an artifact repository within the enterprise. You do not want users to download arbitrarily packages from the Internet into a production cluster. You also want to make sure that they do not use outdated or snapshot versions, that you have control over dependencies, licenses etc.

Currently I do not see that big artifact repository managers will support spark packages anytime soon. I also do not see it from the big Hadoop distributions.


d then implementation of 'the basics' only. It should have the tools that cover ~80% of use cases, out of the box, in a pretty well-supported and tested way.
ecause each one adds marginally less value, and IMHO, is proportionally bigger baggage, because the contributors tend to skew academic, produce worse code, and don't stick around to maintain it. 
 bandwidth to even cover the current scope. While 'the basics' is a subjective label, de facto, I think we'd have to define it as essentially ""what we already have in place"" for the foreseeable future.
ymptom. Would these really be better in the core project?
ote:
osal"" thread for discussing the high-level mission and goals for MLlib.  I hope this thread will collect feedback and ideas, not necessarily lead to huge decisions.
lib (if this derails the original discussion, please let me know and we can discuss in another thread). The roadmap does contain specific items that help to convey some of this (ML parity with MLlib, model persistence, etc...), but I'm interested in what the ""mission"" of Spark MLlib is. We often see PRs for brand new algorithms which are sometimes rejected and sometimes not. Do we aim to keep implementing more and more algorithms? Or is our focus really, now that we have a reasonable library of algorithms, to simply make the existing ones faster/better/more robust? Should we aim to make interfaces that are easily extended for developers to easily implement their own custom code (e.g. custom optimization libraries), or do we want to restrict things to out-of-the box algorithms? Should we focus on more flexible, general abstractions like distributed linear algebra?
scussion may have happened, but I think it would be useful to either revisit it or restate it here for some of the newer developers.
 making the library more robust.
'd list:
ng users to customize their ML on Spark workflows will be critical.  This is IMO the most important thing we could do for MLlib.
 we will need to make it easier for users to write their own algorithms and packages to facilitate this.  Part of this could be allowing users to customize existing algorithms with custom loss functions, etc.
g the core set of algorithms in MLlib. This could mean speed, scaling, robustness, and usability for the few algorithms which cover 90% of use cases.
 community's thoughts!
"
Stephen Boesch <javadba@gmail.com>,"Tue, 24 Jan 2017 04:42:22 -0800",Re: MLlib mission and goals,Sean Owen <sowen@cloudera.com>,"re: spark-packages.org  and ""Would these really be better in the core
project?""   That was not at all the intent of my input: instead to ask ""how
and where to structure/place deployment quality code that yet were *not*
part of the distribution?""   The spark packages has no curation whatsoever
: no minimum standards of code quality and deployment structures, let alone
qualitative measures of usefulness.

While spark packages would never rival CRAN and friends there is not even
any mechanism in place to get started.  From the CRAN site:

   Even at the current growth rate of several packages a day, all
submissions are still rigorously quality-controlled using strong testing
features available in the R system .

Maybe give something that has a subset of these processes a try ?
Different folks than are already over-subscribed in MLlib ?

2017-01-24 2:37 GMT-08:00 Sean Owen <sowen@cloudera.com>:

"
Michael Heuer <heuermh@gmail.com>,"Tue, 24 Jan 2017 07:35:39 -0600",Re: [VOTE] Release Apache Parquet 1.8.2 RC1,dev <dev@spark.apache.org>,"Per comment
https://github.com/bigdatagenomics/adam/pull/1360#issuecomment-274681650

and Jenkins failure
https://amplab.cs.berkeley.edu/jenkins//job/ADAM-prb/1757/HADOOP_VERSION=2.6.0,SCALAVER=2.10,SPARK_VERSION=1.5.2,label=centos/

when bumping our build to 1.8.2-rc1 unit tests succeed but we encounter
NoSuchMethodError errors at runtime in spark-submit.  Our build is a mess
of transitive dependencies and exclusions, so I haven't yet been able to
identify why bumping the parquet version would cause this problem.

I wanted to mention this here in case anyone else runs into it.

   michael


---------- Forwarded message ----------
:
e
heck the
a
nd
po
"
Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Tue, 24 Jan 2017 16:30:29 +0100",[SPARK-16046] PR Review,dev@spark.apache.org,"Hi all,

there is a pull request that I would like to bring back to life. It is
related to the SQL programming guide and can be found here
<https://github.com/apache/spark/pull/16329>.

I believe the PR should be helpful. The initial review is done already.
Also, I updated it recently and checked that everything is still valid.

Locally built docs (without APIs) can be found here
<https://aokolnychyi.github.io/spark-docs/sql-programming-guide.html>.

Thanks,
Anton
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 24 Jan 2017 16:52:03 +0100",[YARN] $ and $$ in prepareCommand to resolve environment in ExecutorRunnable?,dev <dev@spark.apache.org>,"Hi,

Just noticed that [1] (and also [3]) is very cautious with $ and $$ to
expand environment variables.

    javaOpts += ""-Djava.io.tmpdir="" +
      new Path(
        YarnSparkHadoopUtil.expandEnvironment(Environment.PWD), // <-- here
        YarnConfiguration.DEFAULT_CONTAINER_TEMP_DIR
      )

Few lines below in the same method [2] the code doesn't seem to bother
to call $ directly :

Client.buildPath(Environment.PWD.$(), uri.getPath())

Why? Any particular reason for the difference in the two lines? #curious

[1] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L156

[2] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L203

[3] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L210

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Ilya Matiach <ilmat@microsoft.com>,"Tue, 24 Jan 2017 15:58:17 +0000",RE: Feedback on MLlib roadmap process proposal,"Joseph Bradley <joseph@databricks.com>, Felix Cheung
	<felixcheung_m@hotmail.com>","Just a few questions with regards to the MLLIB process:


  1.  Is there a list of committers who can/are shepherds and what code they own?  I’ve seen this page: http://spark.apache.org/committers.html but I’m not sure if it is up to date and it doesn’t mention what code the committers own.  It would be useful to know who owns ML or MLLIB.  From my limited personal experience this seems to be Joseph K. Bradley, Yanbo Liang and Sean Owen.
  2.  Based on both user votes and watchers, the top issue currently is “SPARK-5575: Artificial neural networks for MLlib deep learning”.  However, it looks like it has been opened for almost 2 years and not a lot of progress is being made.  There seem to be other top issues which aren’t getting addressed as well on these pages mentioned in the roadmap: MLlib, sorted by: Votes <https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(ML%2C%20MLlib)%20ORDER%20BY%20votes%20DESC> or Watchers <https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(ML%2C%20MLlib)%20ORDER%20BY%20Watchers%20DESC> .  Is my perception incorrect, or is there a very good reason for not addressing the top issues voted for by the community?  If there is a good reason, is there a way to filter such JIRAs out from the sorted lists, to know which JIRAs really should be taken/worked on?
  3.  Also, this might be a newbie question, but for new contributors to spark, is there a process to convince a committer to be assigned to a JIRA that we are working on. It would be useful if there was a clear threshold for whether a committer can reject to work on a JIRA ahead of time, so contributors won’t waste time working on issues that aren’t important to spark and focus on making progress on the issues that the spark committers would like us to fix.

Thank you, Ilya

From: Joseph Bradley [mailto:joseph@databricks.com]
Sent: Monday, January 23, 2017 8:04 PM
To: Felix Cheung <felixcheung_m@hotmail.com>
Cc: Mingjie Tang <tangrock@gmail.com>; Seth Hendrickson <seth.hendrickson16@gmail.com>; dev@spark.apache.org
Subject: Re: Feedback on MLlib roadmap process proposal

Hi Seth,

The proposal is geared towards exactly the issue you're describing: providing more visibility into the capacity and intentions of committers.  If there are things you'd add to it or change to improve further, it would be great to hear ideas!  The past roadmap JIRA has some more background discussion which is worth looking at too.

Let's break off the MLlib mission discussion into another thread.  I'll start one now.

Thanks,
Joseph

On Thu, Jan 19, 2017 at 1:51 PM, Felix Cheung <felixcheung_m@hotmail.com<mailto:felixcheung_m@hotmail.com>> wrote:
Hi Seth

Re: ""The most important thing we can do, given that MLlib currently has a very limited committer review bandwidth, is to make clear issues that, if worked on, will definitely get reviewed. ""

We are adopting a Shepherd model, as described in the JIRA Joseph has, in which, when assigned, the Shepherd will see it through with the contributor to make sure it lands with the target release.

I'm sure Joseph can explain it better than I do ;)

_____________________________
From: Mingjie Tang <tangrock@guary 19, 2017 10:30 AM
Subject: Re: Feedback on MLlib roadmap process proposal
To: Seth Hendrickson <seth.hendriCc: Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>>, <dev@spark.apache.org<mailto:dev@spark.apache.org>>


+1 general abstractions like distributed linear algebra.

On Thu, Jan 19, 2017 at 8:54 AM, Seth Hendrickson <seth.hendrickson16@gmail.com<mailto:seth.hendrickson16@gmail.com>> wrote:
I think the proposal laid out in SPARK-18813 is well done, and I do think it is going to improve the process going forward. I also really like the idea of getting the community to vote on JIRAs to give some of them priority - provided that we listen to those votes, of course. The biggest problem I see is that we do have several active contributors and those who want to help implement these changes, but PRs are reviewed rather sporadically and I imagine it is very difficult for contributors to understand why some get reviewed and some do not. The most important thing we can do, given that MLlib currently has a very limited committer review bandwidth, is to make clear issues that, if worked on, will definitely get reviewed. A hard thing to do in open source, no doubt, but even if we have to limit the scope of such issues to a very small subset, it's a gain for all I think.

On a related note, I would love to hear some discussion on the higher level goal of Spark MLlib (if this derails the original discussion, please let me know and we can discuss in another thread). The roadmap does contain specific items that help to convey some of this (ML parity with MLlib, model persistence, etc...), but I'm interested in what the ""mission"" of Spark MLlib is. We often see PRs for brand new algorithms which are sometimes rejected and sometimes not. Do we aim to keep implementing more and more algorithms? Or is our focus really, now that we have a reasonable library of algorithms, to simply make the existing ones faster/better/more robust? Should we aim to make interfaces that are easily extended for developers to easily implement their own custom code (e.g. custom optimization libraries), or do we want to restrict things to out-of-the box algorithms? Should we focus on more flexible, general abstractions like distributed linear algebra?

I was not involved in the project in the early days of MLlib when this discussion may have happened, but I think it would be useful to either revisit it or restate it here for some of the newer developers.

On Tue, Jan 17, 2017 at 3:38 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
Hi all,

This is a general call for thoughts about the process for the MLlib roadmap proposed in SPARK-18813.  See the section called ""Roadmap process.""

Summary:
* This process is about committers indicating intention to shepherd and review.
* The goal is to improve visibility and communication.
* This is fairly orthogonal to the SIP discussion since this proposal is more about setting release targets than about proposing future plans.

Thanks!
Joseph

--

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[http://databricks.com]<https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fdatabricks.com%2F&data=02%7C01%7Cilmat%40microsoft.com%7C4039ae5fd4ef4b3adf2f08d443f4d6c9%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208166242129223&sdata=yiBhrmwwGrCsV1YPvqfOnYXug9ZPVhgROO53xxCP0JE%3D&reserved=0>






--

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[http://databricks.com]<https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fdatabricks.com%2F&data=02%7C01%7Cilmat%40microsoft.com%7C4039ae5fd4ef4b3adf2f08d443f4d6c9%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208166242129223&sdata=yiBhrmwwGrCsV1YPvqfOnYXug9ZPVhgROO53xxCP0JE%3D&reserved=0>
"
Sean Owen <sowen@cloudera.com>,"Tue, 24 Jan 2017 16:23:08 +0000",Re: Feedback on MLlib roadmap process proposal,Ilya Matiach <ilmat@microsoft.com>,"
ters.html
ntion what code the
m my
Liang
access to subsets of the project. Tracking an informal notion would be
process mostly for its own sake, and probably just go out of date. We sort
of tried this with 'maintainers' and it didn't actually do anything.

I am not active much in ML, but will occasionally help commit simple
changes. What you see organically is pretty much what is, at any given
time. People you see responding are the active ones, and influencers,
commit bit or no.



g”.  However,
’t
ib,
AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(ML%2C%20MLlib)%20ORDER%20BY%20votes%20DESC>or
AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(ML%2C%20MLlib)%20ORDER%20BY%20Watchers%20DESC>.
ood
 to
product company where one group might give another group a list of top
priorities to work on. There's a general statement about this at
http://spark.apache.org/contributing.html under ""Code Review Criteria"". In
practice, it's a soft process of convincing other people that change X does
more good than harm, is worth taking the burden of supporting, matters to
users, etc. I ignore 80% of issues, that don't seem to fit these criteria,
and choose to help with the 20% that do, which are usually simple and/or
important bug fixes.

ANNs? that's a tangent but my snap reaction are:
It's something Everybody wants Somebody Else to create, which may explain
the votes vs activity?
There is one basic ANN implementation in Spark actually.
There are others outside Spark, so may be something people get elsewhere
like dl4j or BigDL, or strapping TF to Spark in various ways.
DL is also not an obviously-great fit for the data-parallel computation
model here.
It's not a goal to implement everything in Spark. It could be a good idea,
but, no need to tether it to the core project, to the exclusion of
""unblessed"" third-party packages.



JIRA
old
t important to
ters
No, there's no concept of being tasked to work on something by someone else
here. I can't imagine we could establish a clear objective threshold for
such a subjective thing.

It's not a satisfying answer but it is the most realistic one. All of these
OSS projects work on soft power, persuasion and cooperation. I think the
good news is that all the intuitive ways to gain soft power do work: give
time to others' problems if you want time on your own, help review, make
thoughtful careful changes, etc.

My general guidance is: don't bother doing significant feature work unless
you have some clear buy-in from someone who can commit.

I completely agree that issues should be closed more aggressively for the
overrun with issues but it's gotten a lot better culture-wise about
honestly rejecting lots of inbound stuff quickly.
"
Cody Koeninger <cody@koeninger.org>,"Tue, 24 Jan 2017 10:33:32 -0600",Re: Feedback on MLlib roadmap process proposal,Sean Owen <sowen@cloudera.com>,"Totally agree with most of what Sean said, just wanted to give an
alternate take on the ""maintainers"" thing


My perception of that situation is that the Apache process is actively
antagonistic towards factoring out responsibility for particular parts
of the code into a hierarchy.  I think if Spark was under a different
open source model, with otherwise exactly the same committers, that
attempt at identifying maintainers would have worked out differently.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 24 Jan 2017 10:13:16 -0800",welcoming Burak and Holden as committers,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Burak and Holden have recently been elected as Apache Spark committers.

Burak has been very active in a large number of areas in Spark, including
linear algebra, stats/maths functions in DataFrames, Python/R APIs for
DataFrames, dstream, and most recently Structured Streaming.

Holden has been a long time Spark contributor and evangelist. She has
written a few books on Spark, as well as frequent contributions to the
Python API to improve its usability and performance.

Please join me in welcoming the two!
"
Xiao Li <gatorsmile@gmail.com>,"Tue, 24 Jan 2017 10:14:54 -0800",Re: welcoming Burak and Holden as committers,Reynold Xin <rxin@databricks.com>,"Congratulations! Burak and Holden!

2017-01-24 10:13 GMT-08:00 Reynold Xin <rxin@databricks.com>:

"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 24 Jan 2017 10:23:59 -0800",Re: [VOTE] Release Apache Parquet 1.8.2 RC1,Michael Heuer <heuermh@gmail.com>,"Michael, the problem you're hitting is that Parquet's dependency moved to
Avro 1.8.0 from 1.7.7 and added a method that is missing when you use 1.7.7.

It looks like Spark is still using Avro 1.7.7. I think updating that to
1.8.x should fix the problem.

rb


=centos/
y
d
t
check the
e
 a
4


-- 
Ryan Blue
Software Engineer
Netflix
"
Dean Wampler <deanwampler@gmail.com>,"Tue, 24 Jan 2017 18:24:33 +0000",Re: welcoming Burak and Holden as committers,Xiao Li <gatorsmile@gmail.com>,"Congratulations to both of you!

dean

*Dean Wampler, Ph.D.*
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do>, Fast Data Architectures
for Streaming Applications
<http://www.oreilly.com/data/free/fast-data-architectures-for-streaming-applications.csp>,
Functional Programming for Java Developers
<http://shop.oreilly.com/product/0636920021667.do>, and Programming Hive
<http://shop.oreilly.com/product/0636920023555.do> (O'Reilly)
Lightbend <http://lightbend.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com
https://github.com/deanwampler


"
Russell Spitzer <russell.spitzer@gmail.com>,"Tue, 24 Jan 2017 18:26:14 +0000",Re: welcoming Burak and Holden as committers,"Dean Wampler <deanwampler@gmail.com>, Xiao Li <gatorsmile@gmail.com>","Great news! Congratulations!


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 24 Jan 2017 18:29:20 +0000",Re: welcoming Burak and Holden as committers,"Russell Spitzer <russell.spitzer@gmail.com>, Dean Wampler <deanwampler@gmail.com>, 
	Xiao Li <gatorsmile@gmail.com>","👏 👍

Congratulations, Burak and Holden.


pplications.csp>,
"
"""Dongjoon Hyun""<dongjoon@apache.org>","Tue, 24 Jan 2017 18:33:03 -0000",Re: welcoming Burak and Holden as committers,<dev@spark.apache.org>,"Great! Congratulations, Burak and Holden.

Bests,
Dongjoon.


---------------------------------------------------------------------


"
Shankar Venkataraman <shankarvenkataraman666@gmail.com>,"Tue, 24 Jan 2017 18:39:20 +0000",Re: welcoming Burak and Holden as committers,"Dongjoon Hyun <dongjoon@apache.org>, dev@spark.apache.org","Congrats Buraj and Holden!

plications.csp
:
s.
r
e
"
Joseph Bradley <joseph@databricks.com>,"Tue, 24 Jan 2017 10:39:42 -0800",Re: welcoming Burak and Holden as committers,Dongjoon Hyun <dongjoon@apache.org>,"Congratulations Burak & Holden!

:

:
s.
r
e


-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 24 Jan 2017 18:42:41 +0000",Re: welcoming Burak and Holden as committers,"Joseph Bradley <joseph@databricks.com>, Dongjoon Hyun <dongjoon@apache.org>","Awesome! Congrats Burak & Holden!!


plications.csp
:
s.
r
e
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 24 Jan 2017 10:46:22 -0800",Re: welcoming Burak and Holden as committers,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats Burak & Holden!


m>
or
s
he
"
Cody Koeninger <cody@koeninger.org>,"Tue, 24 Jan 2017 12:50:25 -0600",Re: welcoming Burak and Holden as committers,"""Shixiong (Ryan) Zhu"" <shixiong@databricks.com>","Congrats, glad to hear it


m
as
"
shane knapp <sknapp@berkeley.edu>,"Tue, 24 Jan 2017 10:50:55 -0800",Re: welcoming Burak and Holden as committers,Reynold Xin <rxin@databricks.com>,"congrats to the both of you!  :)


---------------------------------------------------------------------


"
Srabasti Banerjee <srabasti_b@ymail.com.INVALID>,"Tue, 24 Jan 2017 18:57:56 +0000 (UTC)",Re: welcoming Burak and Holden as committers,"shane knapp <sknapp@berkeley.edu>, Reynold Xin <rxin@databricks.com>","Congratulations Holden & Burak :-)
ThanksSrabasti
 

 

 congrats to the both of you!  :)


---------------------------------------------------------------------



   "
Suresh Thalamati <suresh.thalamati@gmail.com>,"Tue, 24 Jan 2017 11:20:48 -0800",Re: welcoming Burak and Holden as committers,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congratulations Burak and Holden!

-suresh

committers.
including linear algebra, stats/maths functions in DataFrames, Python/R APIs for DataFrames, dstream, and most recently Structured Streaming.
written a few books on Spark, as well as frequent contributions to the Python API to improve its usability and performance.


---------------------------------------------------------------------


"
Asher Krim <akrim@hubspot.com>,"Tue, 24 Jan 2017 15:17:43 -0500",Re: MLlib mission and goals,Miao Wang <wangmiao@us.ibm.com>,"scale testing. We've encountered issues with building large models that are
not apparent in small models, and these issues have made productizing
ML/MLLIB much more difficult than we first anticipated. Considering that
one of the biggest selling points for Spark is ease of scaling to large
datasets, I think fleshing out SPARK-15573 and testing large models should
be a priority






-- 
Asher Krim
Senior Software Engineer
"
Saikat Kanjilal <sxk1969@hotmail.com>,"Tue, 24 Jan 2017 20:58:57 +0000",Re: MLlib mission and goals,"Asher Krim <akrim@hubspot.com>, Miao Wang <wangmiao@us.ibm.com>","In reading through this and thinking about usability is there any interest in building a performance measurement framework around some (or maybe all) of the ML/Lib algorithms, I envision this as something that can get run for each release build for our end users, it may be useful for internal ml devs to see what impact each change to their code has on performance, please pardon me if this already exists, am new to the codebase and contributing to spark.


________________________________
From: Asher Krim <akrim@hubspot.com>
Sent: Tuesday, January 24, 2017 12:17 PM
To: Miao Wang
Cc: javadba@gmail.com; dev@spark.apache.org; Sean Owen
Subject: Re: MLlib mission and goals

le testing. We've encountered issues with building large models that are not apparent in small models, and these issues have made productizing ML/MLLIB much more difficult than we first anticipated. Considering that one of the biggest selling points for Spark is ease of scaling to large datasets, I think fleshing out SPARK-15573 and testing large models should be a priority

I started working on ML/MLLIB/R since last year. Here are some of my thoughts from a beginner's perspective:

Current ML/MLLIB core algorithms can serve as good implementation examples, which makes adding new algorithms easier. Even a beginner like me, can pick it up quickly and learn how to add new algorithms. So, adding new algorithms should not be a barrier for developers who really need specific algorithms and it should not be the first priority in ML/MLLIB long term goal. We should only add highly demanded algorithms. I hope there will be detailed JIRA/email discussions to decide whether we want to accept a new algorithm.

I strongly agree that we should improve ML/MLLIB usability, stability and performance in core algorithms and the foundations such as linear algebra library etc. This will keep Spark ML/MLLIB competitive in the area of machine learning framework. For example, Microsoft just open source a fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms. The performance and accuracy is much better than xboost. We need to follow up and improve Spark GBT alogrithms in near future.

Another related area is SparkR. API Parity between SparkR and ML/MLLIB is important. We should also pay attention to R users' habits and experiences when maintaining API parity.

Miao

----- Original message -----
From: Stephen Boesch <javadba@gmail.com<mailto:javadba@gmail.com>>
To: Sean Owen <sowen@cloudera.com<mailto:sowen@cloudera.com>>
Cc: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: MLlib mission and goals
Date: Tue, Jan 24, 2017 4:42 AM

re: spark-packages.org<http://spark-packages.org>  and ""Would these really be better in the core project?""   That was not at all the intent of my input: instead to ask ""how and where to structure/place deployment quality code that yet were *not* part of the distribution?""   The spark packages has no curation whatsoever : no minimum standards of code quality and deployment structures, let alone qualitative measures of usefulness.

While spark packages would never rival CRAN and friends there is not even any mechanism in place to get started.  From the CRAN site:

   Even at the current growth rate of several packages a day, all submissions are still rigorously quality-controlled using strong testing features available in the R system .

Maybe give something that has a subset of these processes a try ?  Different folks than are already over-subscribed in MLlib ?

2017-01-24 2:37 GMT-08:00 Sean Owen <sowen@cloudera.com<mailto:sowen@cloudera.com>>:
My $0.02, which shouldn't be weighted too much.

I believe the mission as of Spark ML has been to provide the framework, and then implementation of 'the basics' only. It should have the tools that cover ~80% of use cases, out of the box, in a pretty well-supported and tested way.

It's not a goal to support an arbitrarily large collection of algorithms because each one adds marginally less value, and IMHO, is proportionally bigger baggage, because the contributors tend to skew academic, produce worse code, and don't stick around to maintain it.

The project is already generally quite overloaded; I don't know if there's bandwidth to even cover the current scope. While 'the basics' is a subjective label, de facto, I think we'd have to define it as essentially ""what we already have in place"" for the foreseeable future.

That the bits on spark-packages.org<http://spark-packages.org> aren't so hot is not a problem but a symptom. Would these really be better in the core project?

And, or: I entirely agree with Joseph's take.

This thread is split off from the ""Feedback on MLlib roadmap process proposal"" thread for discussing the high-level mission and goals for MLlib.  I hope this thread will collect feedback and ideas, not necessarily lead to huge decisions.

Copying from the previous thread:

Seth:
""""""
I would love to hear some discussion on the higher level goal of Spark MLlib (if this derails the original discussion, please let me know and we can discuss in another thread). The roadmap does contain specific items that help to convey some of this (ML parity with MLlib, model persistence, etc...), but I'm interested in what the ""mission"" of Spark MLlib is. We often see PRs for brand new algorithms which are sometimes rejected and sometimes not. Do we aim to keep implementing more and more algorithms? Or is our focus really, now that we have a reasonable library of algorithms, to simply make the existing ones faster/better/more robust? Should we aim to make interfaces that are easily extended for developers to easily implement their own custom code (e.g. custom optimization libraries), or do we want to restrict things to out-of-the box algorithms? Should we focus on more flexible, general abstractions like distributed linear algebra?

I was not involved in the project in the early days of MLlib when this discussion may have happened, but I think it would be useful to either revisit it or restate it here for some of the newer developers.
""""""

Mingjie:
""""""
+1 general abstractions like distributed linear algebra.
""""""


I'll add my thoughts, starting with our past trajectory:
* Initially, MLlib was mainly trying to build a set of core algorithms.
* Two years ago, the big effort was adding Pipelines.
* In the last year, big efforts have been around completing Pipelines and making the library more robust.

I agree with Seth that a few immediate goals are very clear:
* feature parity for DataFrame-based API
* completing and improving testing for model persistence
* Python, R parity

In the future, it's harder to say, but if I had to pick my top 2 items, I'd list:

(1) Making MLlib more extensible
It will not be feasible to support a huge number of algorithms, so allowing users to customize their ML on Spark workflows will be critical.  This is IMO the most important thing we could do for MLlib.
Part of this could be building a healthy community of Spark Packages, and we will need to make it easier for users to write their own algorithms and packages to facilitate this.  Part of this could be allowing users to customize existing algorithms with custom loss functions, etc.

(2) Consistent improvements to core algorithms
A less exciting but still very important item will be constantly improving the core set of algorithms in MLlib. This could mean speed, scaling, robustness, and usability for the few algorithms which cover 90% of use cases.

There are plenty of other possibilities, and it will be great to hear the community's thoughts!

Thanks,
Joseph



--

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[http://databricks.com]<http://databricks.com/>



--------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>



--
Asher Krim
Senior Software Engineer
[http://cdn2.hubspot.net/hub/137828/file-223457316-png/HubSpot_User_Group_Images/HUG_lrg_HS.png?t=1477096082917]
"
Felix Cheung <felixcheung_m@hotmail.com>,"Tue, 24 Jan 2017 21:20:18 +0000",Re: welcoming Burak and Holden as committers,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Congrats and welcome!!


________________________________
From: Reynold Xin <rxin@databricks.com>
Sent: Tuesday, January 24, 2017 10:13:16 AM
To: dev@spark.apache.org
Cc: Burak Yavuz; Holden Karau
Subject: welcoming Burak and Holden as committers

Hi all,

Burak and Holden have recently been elected as Apache Spark committers.

Burak has been very active in a large number of areas in Spark, including linear algebra, stats/maths functions in DataFrames, Python/R APIs for DataFrames, dstream, and most recently Structured Streaming.

Holden has been a long time Spark contributor and evangelist. She has written a few books on Spark, as well as frequent contributions to the Python API to improve its usability and performance.

Please join me in welcoming the two!


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 24 Jan 2017 22:36:04 +0100",Re: welcoming Burak and Holden as committers,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats!




-- 


[image: Register today for Spark Summit East 2017!]
<https://spark-summit.org/east-2017/>

Herman van Hövell

Software Engineer

Databricks Inc.

hvanhovell@databricks.com

+31 6 420 590 27

databricks.com

[image: http://databricks.com] <http://databricks.com/>
"
zero323 <mszymkiewicz@gmail.com>,"Tue, 24 Jan 2017 14:43:23 -0700 (MST)",Re: welcoming Burak and Holden as committers,dev@spark.apache.org,"Kudos!



--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 24 Jan 2017 23:29:50 +0100",Re: welcoming Burak and Holden as committers,Reynold Xin <rxin@databricks.com>,"Wow! At long last. Congrats Burak and Holden!

p.s. I was a bit worried that the process of accepting new committers
is equally hard as passing Sean's sanity checks for PRs, but given
this it's so much easier it seems :D

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Burak Yavuz <brkyvz@gmail.com>,"Tue, 24 Jan 2017 15:24:45 -0800",Re: welcoming Burak and Holden as committers,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thank you very much everyone! Hoping to help out the community as much as I
can!

Best,
Burak


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 24 Jan 2017 15:38:34 -0800",Re: welcoming Burak and Holden as committers,Burak Yavuz <brkyvz@gmail.com>,"Also thanks everyone :) Looking forward to helping out (and if anyone wants
to get started contributing to PySpark please ping me :))




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Wed, 25 Jan 2017 09:20:49 +0900",Re: welcoming Burak and Holden as committers,dev@spark.apache.org,"Congrats, Burak and Holden!

- Kousuke



"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Wed, 25 Jan 2017 09:22:21 +0900",Re: welcoming Burak and Holden as committers,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats!

// maropu


g


-- 
---
Takeshi Yamamuro
"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 25 Jan 2017 09:50:24 +0900",Re: welcoming Burak and Holden as committers,dev <dev@spark.apache.org>,"Congratuation!!

2017-01-25 9:22 GMT+09:00 Takeshi Yamamuro <linguin.m.s@gmail.com>:

p
m
"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 24 Jan 2017 17:03:16 -0800",Re: welcoming Burak and Holden as committers,Hyukjin Kwon <gurwls223@gmail.com>,"Congratulations!


.
R
"
bradc <brad.carlile@oracle.com>,"Tue, 24 Jan 2017 19:02:42 -0700 (MST)",Re: MLlib mission and goals,dev@spark.apache.org,"I believe one of the higher level goals of Spark MLlib should be to improve
the efficiency of the ML algorithms that already exist.  Currently there ML
has a reasonable coverage of the important core algorithms.  The work to get
to feature parity for DataFrame-based API and model persistence are  also
important.
Apache Spark needs to use higher-level BLAS3 and LAPACK routines, instead of
BLAS1 & BLAS3.  For a long time we've used the concept of compute intensity
(compute_intensity = FP_operations/Word) to help look at the performance of
the underling compute kernels (see the papers referenced below).  It has
been proven in many implementations that performance, scalability, and huge
reduction in memory pressure can be achieved by using higher-level BLAS3 or
LAPACK routines in both single node as well as distributed computations.
I performed a survey of some of Apache Spark's ML algorithms.  Unfortunately
most of the ML algorithms are implemented with BLAS1 or BLAS2 routines which
have very low compute intensity.  BLAS2 and BLAS1 routines require a lot
more memory bandwidth and will not achieve peak performance on x86, GPUs, or
any other processor.  
Apache Spark 2.1.0 ML routines & BLAS Routines
ALS(Alternating Least Squares matrix factorization
BLAS2: _SPR, _TPSV
BLAS1: _AXPY, _DOT, _SCAL, _NRM2
Logistic regression classification
BLAS2: _GEMV
BLAS1:  _DOT, _SCAL
Generalized linear regression
BLAS1:  _DOT
Gradient-boosted tree regression
BLAS1: _DOT
GraphX SVD++
BLAS1: _AXPY, _DOT,_SCAL
Neural Net Multi-layer Perceptron
BLAS3: _GEMM
BLAS2: _GEMV
(DGEMM).  BTW the underscores are replaced by S, D, Z, C for (32-bit real,
64-bit double, 32-bit complex, 64-bit complex operations; respectably).
 Refactoring the algorithms to use BLAS3 routines or higher level LAPACK
routines will require coding changes to use sub-block algorithms but the
performance benefits can be great. 
More at: 
https://blogs.oracle.com/BestPerf/entry/improving_algorithms_in_spark_ml
<https://blogs.oracle.com/BestPerf/entry/improving_algorithms_in_spark_ml>  
Background:
Brad Carlile. Parallelism, compute intensity, and data vectorization.
SuperComputing'93, November 1993.
<https://blogs.oracle.com/BestPerf/resource/Carlile-app_compute-intensity-1993.pdf>  
John McCalpin.
213876927_Memory_Bandwidth_and_Machine_Balance_in_Current_High_Performance_Computers
1995
<https://www.researchgate.net/publication/213876927_Memory_Bandwidth_and_Machine_Balance_in_Current_High_Performance_Computers>  




--"
jiangxingbo <jiangxb1987@gmail.com>,"Wed, 25 Jan 2017 10:53:08 +0800",Re: welcoming Burak and Holden as committers,Reynold Xin <rxin@databricks.com>,"Congratulations Burak & Holden!

Xin <rxin@databricks.com> д
committers.
including linear algebra, stats/maths functions in DataFrames, Python/R APIs for DataFrames, dstream, and most recently Structured Streaming.
written a few books on Spark, as well as frequent contributions to the Python API to improve its usability and performance.


---------------------------------------------------------------------


"
Deepak Sharma <deepakmca05@gmail.com>,"Wed, 25 Jan 2017 08:30:26 +0530",Re: welcoming Burak and Holden as committers,jiangxingbo <jiangxb1987@gmail.com>,"Congratulations Holden & Burak


2:13，Reynold Xin <rxin@databricks.com> 写道：


-- 
Thanks
Deepak
www.bigdatabig.com
www.keosha.net
"
Joseph Bradley <joseph@databricks.com>,"Tue, 24 Jan 2017 19:30:50 -0800",Re: MLlib mission and goals,bradc <brad.carlile@oracle.com>,"*Re: performance measurement framework*
We (Databricks) used to use spark-perf
<https://github.com/databricks/spark-perf>, but that was mainly for the
RDD-based API.  We've now switched to spark-sql-perf
<https://github.com/databricks/spark-sql-perf>, which does include some ML
benchmarks despite the project name.  I'll see about updating the project
README to document how to run MLlib tests.





-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Chester Chen <chesterchen@gopro.com>,"Wed, 25 Jan 2017 03:32:21 +0000",Re: welcoming Burak and Holden as committers,"Felix Cheung <felixcheung_m@hotmail.com>, Reynold Xin
	<rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Congratulation to both.

Holden,  we need catch up.


Chester Chen
■ Senior Manager – Data Science & Engineering
3000 Clearview Way
San Mateo, CA 94402

[cid:image001.png@01D27678.9466E4D0]

From: Felix Cheung <felixcheung_m@hotmail.com>
Date: Tuesday, January 24, 2017 at 1:20 PM
To: Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>
Cc: Holden Karau <holden.karau@gmail.com>, Burak Yavuz <burak@databricks.com>
Subject: Re: welcoming Burak and Holden as committers

Congrats and welcome!!

________________________________
From: Reynold Xin <rxin@databricks.com>
Sent: Tuesday, January 24, 2017 10:13:16 AM
To: dev@spark.apache.org
Cc: Burak Yavuz; Holden Karau
Subject: welcoming Burak and Holden as committers

Hi all,

Burak and Holden have recently been elected as Apache Spark committers.

Burak has been very active in a large number of areas in Spark, including linear algebra, stats/maths functions in DataFrames, Python/R APIs for DataFrames, dstream, and most recently Structured Streaming.

Holden has been a long time Spark contributor and evangelist. She has written a few books on Spark, as well as frequent contributions to the Python API to improve its usability and performance.

Please join me in welcoming the two!


"
Yanbo Liang <ybliang8@gmail.com>,"Tue, 24 Jan 2017 19:53:38 -0800",Re: welcoming Burak and Holden as committers,Chester Chen <chesterchen@gopro.com>,"Congratulations, Burak and Holden.

:

"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 25 Jan 2017 05:39:46 +0000",Re: welcoming Burak and Holden as committers,"Yanbo Liang <ybliang8@gmail.com>, Chester Chen <chesterchen@gopro.com>","Congratulations Burak and Holden!

Yanbo Liang <ybliang8@gmail.com>于2017年1月25日周三 上午11:54写道：

"
Liang-Chi Hsieh <viirya@gmail.com>,"Tue, 24 Jan 2017 22:49:37 -0700 (MST)",Re: welcoming Burak and Holden as committers,dev@spark.apache.org,"
Congrats to Burak and Holden! Thanks for your work!


Jeff Zhang wrote


午11:54写道：


















g
20760/0/image001.png&gt;





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--
3.nabble.com/welcoming-Burak-and-Holden-as-committers-tp20726p20761.html
om.

---------------------------------------------------------------------


"
Ilya Matiach <ilmat@microsoft.com>,"Wed, 25 Jan 2017 06:01:37 +0000",RE: Feedback on MLlib roadmap process proposal,Sean Owen <sowen@cloudera.com>,"Thanks Sean, this is a really helpful overview, and contains good guidance for new contributors to ML/MLLIB.
My confusion was that the ML 2.2 roadmap critical features (https://issues.apache.org/jira/browse/SPARK-18813) did not line up with the top ML/MLLIB JIRAs by Votes <https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fissues.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520votes%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=%2FtFB0LY%2BIxLoEf%2FPr1i1%2FgvrjlpXPuYLSLbpnd89Tkg%3D&reserved=0> or Watchers<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fissues.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520Watchers%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=XkPfFiB2T%2FoVnJcdr3jf12dQjes7w%2BVJMrbhgx3ELRs%3D&reserved=0>.
Your explanation that they do not have to and there is a more complex process to choosing the changes that will make it into the next release makes sense to me.
My only humble recommendation would be to cleanup the top JIRAs by closing the ones which have spark packages for them (eg the NN one which already has several packages as you explained), noting or somehow marking on some that they will not be resolved, and changing the component on the ones not related to ML/MLLIB (eg https://issues.apache.org/jira/browse/SPARK-12965).
Also, I would love to do this if I had the permissions, but it would be great to change the JIRAs that are marked as “in progress” but where the corresponding pull request was closed/cancelled, for example https://issues.apache.org/jira/browse/SPARK-4638.  That JIRA is actually one of the top ones by number of watches (adding kernels like Radial Basis Function to SVM, and I can imagine why it’s one of the top ones), and seeing it marked as in progress with a pull request is somewhat confusing.  I’ve seen several other JIRAs similar to this one, where the pull request was closed but the JIRA status was not updated – and if the pull request was closed for a good reason, the corresponding JIRA should probably be closed as well.
Thank you, Ilya


From: Sean Owen [mailto:sowen@cloudera.com]
Sent: Tuesday, January 24, 2017 11:23 AM
To: Ilya Matiach <ilmat@microsoft.com>
Cc: dev@spark.apache.org
Subject: Re: Feedback on MLlib roadmap process proposal

On Tue, Jan 24, 2017 at 3:58 PM Ilya Matiach <ilmat@microsoft.com<mailto:ilmat@microsoft.com>> wrote:
Just a few questions with regards to the MLLIB process:


  1.  Is there a list of committers who can/are shepherds and what code they own?  I’ve seen this page: http://spark.apache.org/committers.html<https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fspark.apache.org%2Fcommitters.html&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=L6pZhfpFVoiAIUHXQjCP%2FhFZ3zINP4jhkYdiJPRQOj4%3D&reserved=0> but I’m not sure if it is up to date and it doesn’t mention what code the committers own.  It would be useful to know who owns ML or MLLIB.  From my limited personal experience this seems to be Joseph K. Bradley, Yanbo Liang and Sean Owen.
There is no such list because there's no formal notion of ownership or access to subsets of the project. Tracking an informal notion would be process mostly for its own sake, and probably just go out of date. We sort of tried this with 'maintainers' and it didn't actually do anything.

I am not active much in ML, but will occasionally help commit simple changes. What you see organically is pretty much what is, at any given time. People you see responding are the active ones, and influencers, commit bit or no.



  1.
  2.  Based on both user votes and watchers, the top issue currently is “SPARK-5575: Artificial neural networks for MLlib deep learning”.  However, it looks like it has been opened for almost 2 years and not a lot of progress is being made.  There seem to be other top issues which aren’t getting addressed as well on these pages mentioned in the roadmap: MLlib, sorted by: Votes <https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fissues.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520votes%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=%2FtFB0LY%2BIxLoEf%2FPr1i1%2FgvrjlpXPuYLSLbpnd89Tkg%3D&reserved=0> or Watchers <https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fissues.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520Watchers%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=XkPfFiB2T%2FoVnJcdr3jf12dQjes7w%2BVJMrbhgx3ELRs%3D&reserved=0> .  Is my perception incorrect, or is there a very good reason for not addressing the top issues voted for by the community?  If there is a good reason, is there a way to filter such JIRAs out from the sorted lists, to know which JIRAs really should be taken/worked on?
JIRA votes and watchers don't mean anything, formally. This isn't a product company where one group might give another group a list of top priorities to work on. There's a general statement about this at http://spark.apache.org/contributing.html<https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fspark.apache.org%2Fcontributing.html&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=0nK2BOC49dlx74fwkS0ihbJFotccBKG7TS2Z3Q4TNvs%3D&reserved=0> under ""Code Review Criteria"". In practice, it's a soft process of convincing other people that change X does more good than harm, is worth taking the burden of supporting, matters to users, etc. I ignore 80% of issues, that don't seem to fit these criteria, and choose to help with the 20% that do, which are usually simple and/or important bug fixes.

ANNs? that's a tangent but my snap reaction are:
It's something Everybody wants Somebody Else to create, which may explain the votes vs activity?
There is one basic ANN implementation in Spark actually.
There are others outside Spark, so may be something people get elsewhere like dl4j or BigDL, or strapping TF to Spark in various ways.
DL is also not an obviously-great fit for the data-parallel computation model here.
It's not a goal to implement everything in Spark. It could be a good idea, but, no need to tether it to the core project, to the exclusion of ""unblessed"" third-party packages.



  1.
  2.  Also, this might be a newbie question, but for new contributors to spark, is there a process to convince a committer to be assigned to a JIRA that we are working on. It would be useful if there was a clear threshold for whether a committer can reject to work on a JIRA ahead of time, so contributors won’t waste time working on issues that aren’t important to spark and focus on making progress on the issues that the spark committers would like us to fix.

No, there's no concept of being tasked to work on something by someone else here. I can't imagine we could establish a clear objective threshold for such a subjective thing.

It's not a satisfying answer but it is the most realistic one. All of these OSS projects work on soft power, persuasion and cooperation. I think the good news is that all the intuitive ways to gain soft power do work: give time to others' problems if you want time on your own, help review, make thoughtful careful changes, etc.

My general guidance is: don't bother doing significant feature work unless you have some clear buy-in from someone who can commit.

I completely agree that issues should be closed more aggressively for the reason you give. On the flip-side this often ruffles feathers. We are still overrun with issues but it's gotten a lot better culture-wise about honestly rejecting lots of inbound stuff quickly.

"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 25 Jan 2017 00:28:13 -0800",Re: welcoming Burak and Holden as committers,Reynold Xin <rxin@databricks.com>,"Congrats to both !!!




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 25 Jan 2017 02:40:57 -0800",Re: welcoming Burak and Holden as committers,Reynold Xin <rxin@databricks.com>,"Congratulations and welcome Holden and Burak !

Regards,
Mridul



---------------------------------------------------------------------


"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 25 Jan 2017 17:02:37 +0530",HBaseContext with Spark,"Spark Dev List <dev@spark.apache.org>, user <user@spark.apache.org>","Hello Spark Community Folks,

Currently I am using HBase 1.2.4 and Hive 1.2.1, I am looking for Bulk Load
from Hbase to Hive.

I have seen couple of good example at HBase Github Repo: https://github.com/
apache/hbase/tree/master/hbase-spark

If I would like to use HBaseContext with HBase 1.2.4, how it can be done ?
Or which version of HBase has more stability with HBaseContext ?

Thanks.
"
Sean Owen <sowen@cloudera.com>,"Wed, 25 Jan 2017 11:59:29 +0000",Re: Feedback on MLlib roadmap process proposal,Ilya Matiach <ilmat@microsoft.com>,"
s.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520votes%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=%2FtFB0LY%2BIxLoEf%2FPr1i1%2FgvrjlpXPuYLSLbpnd89Tkg%3D&reserved=0>or
s.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520Watchers%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=XkPfFiB2T%2FoVnJcdr3jf12dQjes7w%2BVJMrbhgx3ELRs%3D&reserved=0>

For Spark ML, Joseph is the de facto leader and does publish a tentative
roadmap. (We could also use JIRA mechanisms for this but any scheme is
better than none.) Yes, not based on Votes -- nothing here is. Votes are
noisy signal because it is usually measures: what would you like done if
you didn't have to do it and there were no downsides for you?



g
t

We do that. It occasionally generates protests, so, I find myself erring on
the side of ignoring. You can comment on any JIRA you think should be
closed. That's helpful.

That particular JIRA seems potentially legitimate. I wouldn't close it. It
also won't get fixed until someone proposes a resolution. I'd strongly
encourage people saying ""I have this problem too"" to try to fix it. I tend
to ignore these otherwise, myself, in favor of reviewing ones where someone
has gone to the trouble of proposing a working fix.



 but where the

Yes, flag these. I or others can close them if appropriate. Anyone who
consistently does this well, we could give JIRA permissions to.

Opening a PR automatically makes it ""In Progress"" but there's no
complementary process to un-mark it. You can ignore the Open / In Progress
distinction really.

This one is interesting because it does seem like a plausible feature to
add. The original PR was abandoned by the author and nobody else submitted
one -- despite the Votes. I hesitate to signal that no PRs would be
considered, but, doesn't seem like it's in demand enough for someone to
work on?


I think one of my messages is that, de facto, here, like in many Apache
projects, committers do not take requests. They pursue the work they
believe needs doing, and shepherd work initiated by others (a clear bug
report, a PR) to a resolution. Things get done by doing them, or by
building influence by doing other things the project needs doing. It isn't
a mechanical, objective process, and can't be. But it does work in a
recognizable way.

"
Ted Yu <yuzhihong@gmail.com>,"Wed, 25 Jan 2017 06:43:12 -0800",Re: HBaseContext with Spark,Chetan Khatri <chetan.opensource@gmail.com>,"Though no hbase release has the hbase-spark module, you can find the
backport patch on HBASE-14160 (for Spark 1.6)

You can build the hbase-spark module yourself.

Cheers


"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 25 Jan 2017 21:08:07 +0530",Re: HBaseContext with Spark,Ted Yu <yuzhihong@gmail.com>,"@Ted Yu, Correct but HBase-Spark module available at HBase repository seems
too old and written code is not optimized yet, I have been already
submitted PR for the same. I dont know if it is clearly mentioned that now
it is part of HBase itself then peopl"
Ted Yu <yuzhihong@gmail.com>,"Wed, 25 Jan 2017 07:56:05 -0800",Re: HBaseContext with Spark,Chetan Khatri <chetan.opensource@gmail.com>,"The references are vendor specific.

Suggest contacting vendor's mailing list for your PR.

My initial interpretation of HBase repository is that of Apache.

Cheers


"
Bryan Cutler <cutlerb@gmail.com>,"Wed, 25 Jan 2017 10:27:25 -0800",Re: welcoming Burak and Holden as committers,Reynold Xin <rxin@databricks.com>,"Congratulations Holden and Burak, well deserved!!!


"
Scott walent <scottwalent@gmail.com>,"Wed, 25 Jan 2017 19:06:04 +0000",=?UTF-8?Q?Spark_Summit_East_in_Boston_=E2=80=92_20=25_off_Code?=,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","*There’s less than two weeks to go until Spark Summit East 2017, happening
February 7-9 at the Hynes Convention Center in downtown Boston. It will be
the largest Spark Summit conference ever held on the East Coast, and we
hope to see you there. Sign up at https://spark-summit.org/east-2017
<https://spark-summit.org/east-2017> and use promo code ""SPARK17"" to save
20% on a two-day pass.The program will explore the future of Apache Spark
and the latest developments in data science, artificial intelligence,
machine learning and more with 110+ community talks in seven different
tracks. There will also be a dozen Keynote Presentations featuring leading
experts from the Broad Institute, Databricks, Forrester, IBM, Intel, UC
Berkeley’s new RISE Lab and other organizations.You’re also invited to
participated in the various networking activities associated with the
conference like the pre-conference Meetup with the Boston Apache Users
Group and the Women in Big Data luncheon.View the full schedule and
register to attend at https://spark-summit.org/east-2017
<https://spark-summit.org/east-2017>. We look forward to seeing you there.*
"
Amrit Jangid <amrit.jangid@goibibo.com>,"Wed, 25 Jan 2017 17:09:00 +0530",Re: HBaseContext with Spark,Chetan Khatri <chetan.opensource@gmail.com>,"Hi chetan,

If you just need HBase Data into Hive, You can use Hive EXTERNAL TABLE with

STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'.


Try this if you problem can be solved


https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration


Regards

Amrit


.


"
trsell@gmail.com,"Thu, 26 Jan 2017 00:36:43 +0000",Re: welcoming Burak and Holden as committers,"Bryan Cutler <cutlerb@gmail.com>, Reynold Xin <rxin@databricks.com>","Congratulations!


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 25 Jan 2017 17:05:21 -0800",Re: HBaseContext with Spark,Amrit Jangid <amrit.jangid@goibibo.com>,"Does the storage handler provide bulk load capability ?

Cheers

:
h 
ad from Hbase to Hive.
om/apache/hbase/tree/master/hbase-spark
 Or which version of HBase has more stability with HBaseContext ?
"
Shuai Lin <linshuai2012@gmail.com>,"Thu, 26 Jan 2017 18:47:00 +0800","Re: A question about creating persistent table when in-memory catalog
 is used",Xiao Li <gatorsmile@gmail.com>,"I see, thanks for the info!


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 26 Jan 2017 12:48:57 +0100",Why two makeOffers in CoarseGrainedSchedulerBackend? Duplication?,dev <dev@spark.apache.org>,"Hi,

Why are there two (almost) identical makeOffers in
CoarseGrainedSchedulerBackend [1] and [2]? I can't seem to figure out
why they are there and am leaning towards considering one a duplicate.

WDYT?

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L211

[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L229

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Thu, 26 Jan 2017 08:43:39 -0600",Re: Why two makeOffers in CoarseGrainedSchedulerBackend? Duplication?,Jacek Laskowski <jacek@japila.pl>,"one is used when exactly one task has finished -- that means you now have
free resources on just that one executor, so you only need to look for
something to schedule on that one.

the other one is used when you want to schedule everything you can across
the entire cluster.  For example, you have just submitted a new taskset, so
you want to try to use any idle resources across the entire cluster.  Or,
for delay scheduling, you periodically retry all idle resources, in case
they locality delay has expired.

you could eliminate the version which takes an executorId, and always make
offers across all idle hosts -- it would still be correct.  Its a small
efficiency improvement to avoid having to go through the list of all
resources.


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 26 Jan 2017 16:12:26 +0100",Re: Why two makeOffers in CoarseGrainedSchedulerBackend? Duplication?,Imran Rashid <irashid@cloudera.com>,"Hi Imran,

Thanks a lot for your detailed explanation, but IMHO the difference is
so small that I'm surprised it merits two versions -- both check
whether an executor is alive -- executorIsAlive(executorId) vs
executorDataMap.filterKeys(executorIsAlive) A bit fishy, isn't it?

But, on the other hand, since no one has considered it a small
duplication it could be perfectly fine (it did make the code a bit
less obvious to me).

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Thu, 26 Jan 2017 16:00:36 -0600",Re: Why two makeOffers in CoarseGrainedSchedulerBackend? Duplication?,Jacek Laskowski <jacek@japila.pl>,"it is a small difference but think about what this means with a cluster
where you have 10k tasks (perhaps 1k executors with 10 cores each).

When you have one task complete, you have to go through 1k more executors.

frequently, since each core in your cluster is finishing tasks
independently, and sending those updates back to the driver -- eg., you
expect to get 10k updates from one ""wave"" of tasks on your cluster.  So you
avoid going through a list of 1k executors 10k times in just one wave of
tasks.


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 26 Jan 2017 23:15:37 +0100",Re: Why two makeOffers in CoarseGrainedSchedulerBackend? Duplication?,Imran Rashid <irashid@cloudera.com>,"Hi Imran,

Ok, that makes sense for performance reasons. Thanks for bearing with
me and explaining that code with so much patience. Appreciated!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Thu, 26 Jan 2017 19:53:15 -0800",Re: Feedback on MLlib roadmap process proposal,Sean Owen <sowen@cloudera.com>,"Sean has given a great explanation.  A few more comments:

Roadmap: I have been creating roadmap JIRAs, but the goal really is to have
all committers working on MLlib help to set that roadmap, based on either
their knowledge of current maintenance/internal needs of the project or the
feedback given from the rest of the community.
@Committers - I see people actively shepherding PRs for MLlib, but I don't
see many major initiatives linked to the roadmap.  If there are ones large
enough to merit adding to the roadmap, please do.

In general, there are many process improvements we could make.  A few in my
mind are:
* Visibility: Let the community know what committers are focusing on.  This
was the primary purpose of the ""MLlib roadmap proposal.""
* Community initiatives: This is currently very organic.  Some of the
organic process could be improved, such as encouraging Votes/Watchers
(though I agree with Sean about these being one-sided metrics).  Cody's SIP
work is a great step towards adding more clarity and structure for major
initiatives.
* JIRA hygiene: Always a challenge, and always requires some manual
prodding.  But it's great to push for efforts on this.



es.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520votes%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=%2FtFB0LY%2BIxLoEf%2FPr1i1%2FgvrjlpXPuYLSLbpnd89Tkg%3D&reserved=0>or
es.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520Watchers%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=XkPfFiB2T%2FoVnJcdr3jf12dQjes7w%2BVJMrbhgx3ELRs%3D&reserved=0>
g
e
t
d
ne
 but where the
s
d
t


-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Richard Xin <richardxin168@yahoo.com.INVALID>,"Fri, 27 Jan 2017 20:15:19 +0000 (UTC)",Re: Issue creating row with java.util.Map type,"Ankur Srivastava <ankur.srivastava@gmail.com>, 
	""user@spark.apache.org"" <user@spark.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","try
Row newRow = RowFactory.create(row.getString(0), row.getString(1), row.getMap(2)); 

 

 + DEV Mailing List

Hi,
I am trying to map a Dataset with rows which have a map attribute. When I try to create a Row with the map attribute I get cast errors. I am able to reproduce the issue with the below sample code. The surprising thing is with same schema I am able to create a dataset from the List of rows.
I am on Spark 2.0 and scala 2.11public static void main(String[] args) {
    StructType schema = new StructType().add(""src"", DataTypes.StringType)
            .add(""dst"", DataTypes.StringType)
            .add(""freq"", DataTypes.createMapType( DataTypes.StringType, DataTypes.IntegerType));
    List<Row> inputData = new ArrayList<>();
    inputData.add(RowFactory.creat e(""1"", ""2"", new HashMap<>()));
    SparkSession sparkSession = SparkSession
            .builder()
            .appName(""IPCountFilterTest"")
            .master(""local"")
            .getOrCreate();

    Dataset<Row> out = sparkSession.createDataFrame( inputData, schema);
    out.show();

    Encoder<Row> rowEncoder = RowEncoder.apply(schema);
    out.map((MapFunction<Row, Row>) row -> {
        Row newRow = RowFactory.create(row. getString(0), row.getString(1), new HashMap<String, Integer>());       //Row newRow = RowFactory.create(row. getString(0), row.getString(1), row.getJavaMap(2));        return newRow;
    }, rowEncoder).show();
}
Below is the error:
17/01/26 17:05:30 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.RuntimeException: java.util.HashMap is not a valid external type for schema of map<string,int> at org.apache.spark.sql.catalyst. expressions.GeneratedClass$ GeneratedIterator.processNext( Unknown Source) at org.apache.spark.sql. execution.BufferedRowIterator. hasNext(BufferedRowIterator. java:43) at org.apache.spark.sql. execution. WholeStageCodegenExec$$ anonfun$8$$anon$1.hasNext( WholeStageCodegenExec.scala: 370) at org.apache.spark.sql. execution.SparkPlan$$anonfun$ 4.apply(SparkPlan.scala:246) at org.apache.spark.sql. execution.SparkPlan$$anonfun$ 4.apply(SparkPlan.scala:240) at org.apache.spark.rdd.RDD$$ anonfun$mapPartitionsInternal$ 1$$anonfun$apply$24.apply(RDD. scala:784) at org.apache.spark.rdd.RDD$$ anonfun$mapPartitionsInternal$ 1$$anonfun$apply$24.apply(RDD. scala:784) at org.apache.spark.rdd. MapPartitionsRDD.compute( MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD. computeOrReadCheckpoint(RDD. scala:319) at org.apache.spark.rdd.RDD. iterator(RDD.scala:283) at org.apache.spark.scheduler. ResultTask.runTask(ResultTask. scala:70) at org.apache.spark.scheduler. Task.run(Task.scala:85) at org.apache.spark.executor. Executor$TaskRunner.run( Executor.scala:274) at java.util.concurrent. ThreadPoolExecutor.runWorker( ThreadPoolExecutor.java:1142) at java.util.concurrent. ThreadPoolExecutor$Worker.run( ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread. java:745)17/01/26 17:05:30 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.RuntimeException: java.util.HashMap is not a valid external type for schema of map<string,int> at org.apache.spark.sql.catalyst. expressions.GeneratedClass$ GeneratedIterator.processNext( Unknown Source) at org.apache.spark.sql. execution.BufferedRowIterator. hasNext(BufferedRowIterator. java:43) at org.apache.spark.sql. execution. WholeStageCodegenExec$$ anonfun$8$$anon$1.hasNext( WholeStageCodegenExec.scala: 370) at org.apache.spark.sql. execution.SparkPlan$$anonfun$ 4.apply(SparkPlan.scala:246) at org.apache.spark.sql. execution.SparkPlan$$anonfun$ 4.apply(SparkPlan.scala:240) at org.apache.spark.rdd.RDD$$ anonfun$mapPartitionsInternal$ 1$$anonfun$apply$24.apply(RDD. scala:784) at org.apache.spark.rdd.RDD$$ anonfun$mapPartitionsInternal$ 1$$anonfun$apply$24.apply(RDD. scala:784) at org.apache.spark.rdd. MapPartitionsRDD.compute( MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD. computeOrReadCheckpoint(RDD. scala:319) at org.apache.spark.rdd.RDD. iterator(RDD.scala:283) at org.apache.spark.scheduler. ResultTask.runTask(ResultTask. scala:70) at org.apache.spark.scheduler. Task.run(Task.scala:85) at org.apache.spark.executor. Executor$TaskRunner.run( Executor.scala:274) at java.util.concurrent. ThreadPoolExecutor.runWorker( ThreadPoolExecutor.java:1142) at java.util.concurrent. ThreadPoolExecutor$Worker.run( ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread. java:745)

ThanksAnkur



   "
Scott walent <scottwalent@gmail.com>,"Sat, 28 Jan 2017 00:05:40 +0000",CFP for Spark Summit San Francisco closes on Feb. 6,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","In June, the 10th Spark Summit will take place in San Francisco at Moscone
West. We have expanded our CFP to include more topics and deep-dive
technical sessions.

Take center stage in front of your fellow Spark enthusiasts. Submit your
presentation and join us for the big ten. The CFP closes on February 6th!

Submit your abstracts at https://spark-summit.org/2017
"
Chetan Khatri <chetan.opensource@gmail.com>,"Sat, 28 Jan 2017 11:32:06 +0530",Re: HBaseContext with Spark,Ted Yu <yuzhihong@gmail.com>,"@Ted, I dont think so.


"
Chetan Khatri <chetan.opensource@gmail.com>,"Sat, 28 Jan 2017 13:21:16 +0530",Re: HBaseContext with Spark,Ted Yu <yuzhihong@gmail.com>,"storage handler bulk load:

SET hive.hbase.bulk=true;
INSERT OVERWRITE TABLE users SELECT … ;
But for now, you have to do some work and issue multiple Hive commands
Sample source data for range partitioning
Save sampling results to a file
Run CLUSTER BY query using HiveHFileOutputFormat and TotalOrderPartitioner
(sorts data, producing a large number of region files)
Import HFiles into HBase
HBase can merge files if necessary

m

e
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 28 Jan 2017 19:18:09 +0000","Typo on spark.apache.org? ""cyclic data flow""",Spark dev list <dev@spark.apache.org>,"The tagline on http://spark.apache.org/ says: ""Apache Spark has an advanced
DAG execution engine that supports cyclic data flow and in-memory
computing.""

Isn't that supposed to be ""acyclic"" rather than ""cyclic""?

What does it mean to support cyclic data flow anyway?

Nick
"
Sean Owen <sowen@cloudera.com>,"Sat, 28 Jan 2017 19:44:22 +0000","Re: Typo on spark.apache.org? ""cyclic data flow""","Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>","Certainly a typo -- feel free to make a PR for the spark-website repo.
(Might search for other instances of 'cyclic' too)


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 28 Jan 2017 19:49:44 +0000","Re: Typo on spark.apache.org? ""cyclic data flow""","Sean Owen <sowen@cloudera.com>, Spark dev list <dev@spark.apache.org>","Aye aye, cap'n. PRrrrrrrrr incoming.


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sat, 28 Jan 2017 22:04:56 +0000",Re: welcoming Burak and Holden as committers,trsell@gmail.com,"Congratulations !  





Congratulations!


Congratulations Holden and Burak, well deserved!!!

Hi all,
Burak and Holden have recently been elected as Apache Spark committers.
Burak has been very active in a large number of areas in Spark, including linear
algebra, stats/maths functions in DataFrames, Python/R APIs for DataFrames,
dstream, and most recently Structured Streaming.
Holden has been a long time Spark contributor and evangelist. She has written a
few books on Spark, as well as frequent contributions to the Python API to
improve its usability and performance.
Please join me in welcoming the two!



Olivier Girardot| Associé
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
Jacek Laskowski <jacek@japila.pl>,"Sun, 29 Jan 2017 08:45:27 +0100","Re: Typo on spark.apache.org? ""cyclic data flow""",Nicholas Chammas <nicholas.chammas@gmail.com>,"Hi Nicholas,

Interesting. Just on the past Monday I was introducing spark and ran into
it but thought it's my poor English skills :-) Thanks for spotting it!

(I also think that the entire welcome page begs for a face lifting - it's
from pre-2.0 days)

Jacek


"
aravasai <aravasai@gmail.com>,"Sun, 29 Jan 2017 16:44:29 -0700 (MST)",Maximum limit for akka.frame.size be greater than 500 MB ?,dev@spark.apache.org,"I have a spark job running on 2 terabytes of data which creates more than
30,000 partitions. As a result, the spark job fails with the error 
""Map output statuses were 170415722 bytes which exceeds spark.akka.frameSize
52428800 bytes"" (For 1 TB data)
However, when I increase the akka.frame.size to around 500 MB, the job hangs
with no further progress.

So, what is the ideal or maximum limit that i can assign akka.frame.size so
that I do not get the error of map output statuses exceeding limit for large
chunks of data ?

Is coalescing the data into smaller number of partitions the only solution
to this problem? Is there any better way than coalescing many intermediate
rdd's in program ?

My driver memory: 10G
Executor memory: 36G 
Executor memory overhead : 3G







--

---------------------------------------------------------------------


"
Chetan Khatri <chetan.opensource@gmail.com>,"Mon, 30 Jan 2017 11:22:30 +0530",Re: Error Saving Dataframe to Hive with Spark 2.0.0,Jacek Laskowski <jacek@japila.pl>,"Okey, you are saying that 2.0.0 don't have that patch fixed ? @dev cc--
I don't like everytime changing the service versions !

Thanks.


"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Mon, 30 Jan 2017 08:15:10 +0100",Re: Maximum limit for akka.frame.size be greater than 500 MB ?,aravasai <aravasai@gmail.com>,"Which Spark version are you using? What are you trying to do exactly and what is the input data? As far as I know, akka has been dropped in recent Spark versions.

ze
gs
o
ge


n3.nabble.com/Maximum-limit-for-akka-frame-size-be-greater-than-500-MB-tp20793.html
com.

---------------------------------------------------------------------


"
aravasai <aravasai@gmail.com>,"Mon, 30 Jan 2017 00:34:13 -0700 (MST)",Re: Maximum limit for akka.frame.size be greater than 500 MB ?,dev@spark.apache.org,"Currently, I am using 1.6.1 version. I continue to use it as my current
code is heavily reliant on RDD's and not dataframes. Also, because 1.6.1 is
stabler than newer versions.


The input data is user behavior data of 20 fields and 1 billion records (~
1.5 TB) . I am trying to group by user id and calculate some users
statistics. But, I guess the number of mapper tasks are too high resulting
in akka.frame.size error.

1) Does akka.frame.size has to be proportionately increased with size of
data which indirectly affects the number of partitions?
2) Or do the  huge number of mappers in the code (It may not be prevented)
result in the frame size error?

pers

e
ervlet.jtp?macro=unsubscribe_by_code&node=20793&code=YXJhdmFzYWlAZ21haWwuY29tfDIwNzkzfDEzNDU1NjkyNTk=>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Maximum-limit-for-akka-frame-size-be-greater-than-500-MB-tp20793p20797.html
om."
"""Vinayak Joshi5"" <vijoshi5@in.ibm.com>","Tue, 31 Jan 2017 15:55:36 +0530",Spark SQL Dataframe resulting from an except( ) is unusable,"""Spark dev list"" <dev@spark.apache.org>","With Spark 2.x, I construct a Dataframe from a sample libsvm file:

scala> val higgsDF = spark.read.format(""libsvm"").load(""higgs.libsvm"")
higgsDF: org.apache.spark.sql.DataFrame = [label: double, features: 
vector]


Then, build a new dataframe that involves an except( )

scala> val train_df = higgsDF.sample(false, 0.7, 42)
train_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: 
double, features: vector]

scala> val test_df = input_df.except(train_df)
test_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: 
double, features: vector]

Now, most operations on the test_df fail with this exception:

scala> test_df.show()
java.lang.RuntimeException: no default for type 
org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7
  at 
org.apache.spark.sql.catalyst.expressions.Literal$.default(literals.scala:179)
  at 
org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys$$anonfun$4.apply(patterns.scala:117)
  at 
org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys$$anonfun$4.apply(patterns.scala:110)
   .
   .

Debugging this, I see that this is the schema of this dataframe:

scala> test_df.schema
res4: org.apache.spark.sql.types.StructType = 
StructType(StructField(label,DoubleType,true), 
StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))

Looking a little deeper, the error occurs because the QueryPlanner ends up 
inside

  object ExtractEquiJoinKeys 
(/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala)

where it processes a LeftAnti Join. Then there is an attempt to generate a 
default Literal value for the org.apache.spark.ml.linalg.VectorUDT 
DataType which fails with the above exception. This is because there is no 
match for the VectorUDT in

def default(dataType: DataType): Literal = {..} 
(/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/literals.scala)


Any processing on this dataframe that causes Spark to build a query plan 
(i.e. almost all productive uses of this dataframe) fails due to this 
exception. 

Is it a miss in the Literal implementation that it does not handle 
UserDefinedTypes or is it left out intentionally? Is there a way to get 
around this problem? This problem seems to be present in all 2.x version.

Regards,
Vinayak Joshi


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 31 Jan 2017 16:06:25 +0100",[SQL][ML] Pipeline performance regression between 1.6 and 2.x,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

While experimenting with ML pipelines I experience a significant
performance regression when switching from 1.6.x to 2.x.

import org.apache.spark.ml.{Pipeline, PipelineStage}
VectorAssembler}

val df = (1 to 40).foldLeft(Seq((1, ""foo""), (2, ""bar""), (3,
""baz"")).toDF(""id"", ""x0""))((df, i) => df.withColumn(s""x$i"", $""x0""))
val indexers = df.columns.tail.map(c => new StringIndexer()
  .setInputCol(c)
  .setOutputCol(s""${c}_indexed"")
  .setHandleInvalid(""skip""))

  .setInputCol(indexer.getOutputCol)
  .setOutputCol(s""${indexer.getOutputCol}_encoded"")
  .setDropLast(true))

val assembler = new
VectorAssembler().setInputCols(encoders.map(_.getOutputCol))
val stages: Array[PipelineStage] = indexers ++ encoders :+ assembler

new Pipeline().setStages(stages).fit(df).transform(df).show

Task execution time is comparable and executors are most of the time
idle so it looks like it is a problem with the optimizer. Is it a known
issue? Are there any changes I've missed, that could lead to this behavior?

-- 
Best,
Maciej


---------------------------------------------------------------------


"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Tue, 31 Jan 2017 09:08:22 -0800",Unique Partition Id per partition,"User <user@spark.apache.org>, dev <dev@spark.apache.org>","Hi All

I have a rdd, which i partition based on some key, and then can sc.runJob
for each partition.
 Inside this function, i assign each partition a unique key using following:

""%s_%s"" % (id(part), int(round(time.time()))

This is to make sure that, each partition produces separate bookeeping stuff,

which can be aggregated by external system. However, I sometimes i
notice multiple

partition results pointing to same partition_id. Is this some issue due to the

way above code is serialized by Pyspark. What's the best way to define
a unique id

for each partition. I undestand that its same executor getting
multiple partitions to process,

but i would expect the above code to produce a unique id for each partition.



Regards
Sumit Chawla
"
Michael Allman <michael@videoamp.com>,"Tue, 31 Jan 2017 10:53:27 -0800",Re: Error Saving Dataframe to Hive with Spark 2.0.0,Chetan Khatri <chetan.opensource@gmail.com>,"That's understandable. Maybe I can help. :)

What happens if you set `HIVE_TABLE_NAME = ""default.employees""`?

Also, does that table exist before you call `filtered_output_timestamp.write.mode(""append"").saveAsTable(HIVE_TABLE_NAME)`?

Cheers,

Michael

cc-- 
ERROR since. 
still Exception raised.
SparkConf().setAppName(APP_NAME).setMaster(""local"")
org.apache.spark.sql.SQLContext(sparkContext)
SparkSession.builder().appName(APP_NAME).config(""hive.metastore.warehouse.dir"", HIVE_DATA_WAREHOUSE).config(""spark.sql.warehouse.dir"", HIVE_DATA_WAREHOUSE).enableHiveSupport().getOrCreate()
tuples from the table
classOf[TableInputFormat],
<http://hbase.io/>.ImmutableBytesWritable],
in HBase
Date(cell.getTimestamp.toLong)),
HH:mm:ss:SSS"").parse(datetimestamp_threshold).getTime()
datetimestampformat)
outPut.filter($""colDatetime"".between(1668493360,1668493365))
filtered_output_timestamp.write.mode(""append"").saveAsTable(HIVE_TABLE_NAME)
String, colDatetime: Long, colDatetimeStr: String, colValue: String, colType: String)
Database(name:default, description:default database, locationUri:hdfs://localhost:9000/usr/local/hive/warehouse, parameters:{})
ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:hdfs://localhost:9000/usr/local/hive/warehouse, parameters:{})	
AlreadyExistsException(message:Database default already exists)
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
org.apache.spark.sql.SparkSession.createDataset(SparkSession.scala:441)
org.apache.spark.sql.SQLContext.createDataset(SQLContext.scala:395)
org.apache.spark.sql.SQLImplicits.rddToDatasetHolder(SQLImplicits.scala:163)
com.chetan.poc.hbase.IncrementalJob$.main(IncrementalJob.scala:58)
com.chetan.poc.hbase.IncrementalJob.main(IncrementalJob.scala)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)

"
Michael Allman <michael@videoamp.com>,"Tue, 31 Jan 2017 10:57:37 -0800",Re: Unique Partition Id per partition,"""Chawla,Sumit"" <sumitkchawla@gmail.com>","Hi Sumit,

Can you use http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.mapPartitionsWithIndex <http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.mapPartitionsWithIndex> to solve your problem?

Michael

sc.runJob for each partition. 
following:
stuff, 
notice multiple 
due to the 
a unique id 
multiple partitions to process,
partition.

"
Alan Gates <alanfgates@gmail.com>,"Tue, 31 Jan 2017 11:28:02 -0800",Call for abstracts open for Dataworks & Hadoop Summit San Jose,dev@spark.apache.org,"The Dataworks & Hadoop summit will be in San Jose June 13-15, 2017.  The call for abstracts closes February 10.  You can submit an abstract at http://tinyurl.com/dwsj17CFA

There are tracks for Hadoop, data processing and warehousing, governance and security, IoT and streaming, cloud and operations, and Spark and data science.  As always the talks will be chosen by committees from the relevant communities.

Alan.
---------------------------------------------------------------------


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 31 Jan 2017 21:39:58 +0000",Structured Streaming Source error,dev@spark.apache.org,"Hi Folks


I am getting a weird error when trying to write a BigQuery Structured
Streaming source


Error:
java.lang.AbstractMethodError:
com.samelamin.spark.bigquery.streaming.BigQuerySource.commit(Lorg/apache/spark/sql/execution/streaming/Offset;)V
        at
org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$1$$anonfun$apply$mcV$sp$5.apply(StreamExecution.scala:359)
        at
org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$1$$anonfun$apply$mcV$sp$5.apply(StreamExecution.scala:358)


FYI if you are interested in Spark and BigQuery feel free to give my
connector a go! Still trying to get structured streaming to it as a source
hence this email. but you can use it as a sink!


Regards
Sam
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 31 Jan 2017 13:48:28 -0800",Re: Structured Streaming Source error,Sam Elamin <hussam.elamin@gmail.com>,"You used one Spark version to compile your codes but another newer version
to run. As the Source APIs are not stable, Spark doesn't guarantee that
they are binary compatibility.


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 31 Jan 2017 21:51:07 +0000",Re: Structured Streaming Source error,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Ha Ryan your everywhere,JIRA and maillist. I thought multitasking was a
myth!

Thanks for your help. It was using different versions!

Regards
Sam


"
