Umar Javed <umarj.javed@gmail.com>,"Sun, 1 Dec 2013 15:09:44 -0800",compiling a new RDD,dev@spark.incubator.apache.org,"Hi,

I want to write a new RDD. For testing, I just copied and pasted
HadoopRDD.scala into the file newRDD.scala with the appropriate
replacements of ""HadoopRDD'.
It compiles fine at this stage. Now I create newRDD() in SparkContext and
the compiler is giving me a type not found error:

.../incubator-spark/core/src/main/scala/org/apache/spark/SparkContext.scala:413:
not found: type newRDD

    new newRDD(
           ^

I must be missing something trivial here. Any ideas?

thanks!
"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 2 Dec 2013 19:09:59 +0200","PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",dev@spark.incubator.apache.org,"Hi Spark Devs

An idea developed recently out of a scikit-learn mailing list discussion (
http://sourceforge.net/mailarchive/forum.php?thread_name=CAFvE7K5HGKYH9Myp7imrJ-nU%3DpJgeGqcCn3JC0m4MmGWZi35Hw%40mail.gmail.com&forum_name=scikit-learn-general)
to have a coding sprint around Strata in Feb, focused on integration
between scikit-learn and PySpark for large-scale machine learning tasks.

Cloudera has kindly agreed to host the sprint, most likely in San
Francisco. Ideally it would be focused and capped at around 10 people. The
idea is not meant to be a teaching workshop for
newcomers but more as a prototyping session, so ideally it would be great
to have developers and users with deep knowledge of PySpark (Josh
especially :) and/or scikit-learn, attend.

Hopefully we can get some people from the Spark community involved, and
Olivier will drum up support from the scikit-learn community.

All the best and hope to see you there (though likely I will only be able
to join remotely).
Nick
"
Olivier Grisel <olivier.grisel@ensta.org>,"Mon, 2 Dec 2013 22:12:37 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",Nick Pentreath <nick.pentreath@gmail.com>,"Hi all,

Just a quick reply to say that I would be glad to meet some of you to
hack on some prototype scikit-learn / PySpark integration.

Cloudera just confirmed that we have a room for us at their San
Fransisco offices on Friday Feb 14 (right after Strata).

Hope to see you there or at Strata,

-- 
Olivier

"
Reynold Xin <rxin@apache.org>,"Mon, 2 Dec 2013 13:22:15 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Olivier,

Do you want us to create a Spark user meetup event for this hackathon?


"
Reynold Xin <rxin@apache.org>,"Mon, 2 Dec 2013 13:23:35 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",Reynold Xin <rxin@apache.org>,"Including the link to the meetup group: http://www.meetup.com/spark-users/



"
Olivier Grisel <olivier.grisel@ensta.org>,"Mon, 2 Dec 2013 22:43:21 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",dev@spark.incubator.apache.org,"2013/12/2 Reynold Xin <rxin@apache.org>:

I am not opposed to it but I am wondering if people will not confuse
it with a traditional meetup if we do so.

-- 
Olivier

"
Reynold Xin <rxin@apache.org>,"Mon, 2 Dec 2013 13:45:53 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",Olivier Grisel <olivier.grisel@ensta.org>,"Definitely some people will get confused. It's up to you. If we post it, we
can mark it in the title that this is a hackathon.



"
Horia <horia@alum.berkeley.edu>,"Tue, 3 Dec 2013 09:30:19 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",dev@spark.incubator.apache.org,"I am very interested in this and will most definitely participate!

Please share the event sign-up list and location details when all the
organizational hurdles have been resolved :-)




"
Olivier Grisel <olivier.grisel@ensta.org>,"Wed, 4 Dec 2013 15:00:13 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",horia@alum.berkeley.edu,"2013/12/3 Horia <horia@alum.berkeley.edu>:

Great! I just created a new entry for this sprint on the scikit-learn wiki:

  https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events#scikit-learn--pyspark-integration-sprint---friday-14-february-2014

Please feel free to register there.

-- 
Olivier
http://twitter.com/ogrisel - http://github.com/ogrisel

"
Olivier Grisel <olivier.grisel@ensta.org>,"Wed, 4 Dec 2013 15:01:33 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",horia <horia@alum.berkeley.edu>,"Oops. I got something wrong with the markup. Let me fix it first.

-- 
Olivier

"
Olivier Grisel <olivier.grisel@ensta.org>,"Wed, 4 Dec 2013 15:04:43 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",horia <horia@alum.berkeley.edu>,"That should be fixed but only if nobody clicks on the previous URL...
Use the following instead:

https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events

That's a weird github bug...

-- 
Olivier

"
Olivier Grisel <olivier.grisel@ensta.org>,"Wed, 4 Dec 2013 15:07:55 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",horia <horia@alum.berkeley.edu>,"2013/12/4 Olivier Grisel <olivier.grisel@ensta.org>:

I switched the rendering to markdown and the page is stable now. Sorry for that.

-- 
Olivier
http://twitter.com/ogrisel - http://github.com/ogrisel

"
Chris Mattmann <mattmann@apache.org>,"Wed, 04 Dec 2013 11:32:52 -0800",Sorry about business lately and general unavailability,<dev@spark.incubator.apache.org>,"Hey Guys,

Just wanted to apologize for the general lack of my availability
lately. I thought moving from Rancho Cucamonga, to Pasadena, CA
(over 50+ miles) wouldn't affect my productivity, and with that
and the holidays, and all the house work and moving stuff I've had
to do, coupled with $dayjob it's been tough.

I'm slowly catching up and coming out of the fog though so just
wanted to let you all know I'm going to be around and get back
to helping out as a mentor. Amazing thing though is that you guys
have really been kicking ass largely without me like you always do
and operating like a great ASF project.

I'd say you are headed for an early graduation and I will closely
monitor things like adding more PPMC members and committers (saw
you guys have been doing this), and also things like releases
(that too), and just keep doing what you're doing and you'll be
an ASF TLP shortly!

Cheers mates and rock on.

-Chris ""Champion in abenstia but back now"" Mattmann






"
Josh Rosen <rosenville@gmail.com>,"Wed, 4 Dec 2013 14:48:27 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014","""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","Thanks for organizing this!  I'll definitely be attending.

- Josh



"
Reynold Xin <rxin@apache.org>,"Wed, 4 Dec 2013 15:36:54 -0800",Re: Sorry about business lately and general unavailability,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks for the update Chris.

We do need to graduate soon. People have been asking me does ""incubating""
means the project is very immature. :(

That INFRA ticket hasn't moved much along. Can you help push that?




"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 4 Dec 2013 17:39:54 -0800",Re: Sorry about business lately and general unavailability,dev@spark.incubator.apache.org,"No worries Chris! Apart from the JIRA thing, we also plan another release or two soon.

Matei


""incubating""
JIRA.


"
Roman Shaposhnik <rvs@apache.org>,"Wed, 4 Dec 2013 19:46:30 -0800",Re: Sorry about business lately and general unavailability,dev@spark.incubator.apache.org,"
I think important thing to realize is that there's no rush
and the project should graduate whenever the community
has demonstrated its capacity for functioning under the
principles of the Apache Way.

Graduation has absolutely nothing to do with how polished
the code is. Remember ASF's motto: ""community over code"".


I think this is one of the misconceptions that we all need to actively
take part in clearing: the projects in the incubator can be extremely
mature as far as the software goes. You don't have to search for
example too hard: OpenOffice and CloudStack readily come to mind.
Both were extremely well established and respected pieces of software
when they entered the incubator.

than the quality of your community. There's plenty of excellent, beautifully
crafted software projects on GitHub.

Thanks,
Roman.

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 4 Dec 2013 20:32:20 -0800",Spark streaming quantile?,"dev@spark.incubator.apache.org, Uri Laserson <laserson@cloudera.com>","Hi All,

We're working on a Spark application that could make use of a computing
quantiles in a streaming fashion.  Something in the vein of what DataFu has
for Pig
http://linkedin.github.io/datafu/docs/current/datafu/pig/stats/StreamingQuantile.html
.

Does anything like this exist in the Spark ecosystem?  If not, would there
be a good place to contribute this if we write it?

thanks,
Sandy
"
Ryan Weald <ryan@weald.com>,"Wed, 4 Dec 2013 20:41:49 -0800",Re: Spark streaming quantile?,dev@spark.incubator.apache.org,"Hi Sandy,
You could take a look at using the Q-Tree data structure that is provided
by Twitter's Algebird<https://github.com/twitter/algebird/blob/develop/algebird-core/src/main/scala/com/twitter/algebird/QTree.scala>.
Due to the associative properties of Algebird's SemiGroup it is ideally
suited for streaming computations.

-Ryan



"
Olivier Grisel <olivier.grisel@ensta.org>,"Thu, 5 Dec 2013 08:54:34 +0100",Re: Spark streaming quantile?,dev <dev@spark.incubator.apache.org>,"You might also want to have a look at Ted Dunning's t-digest:

  https://github.com/tdunning/t-digest

There is a paper with some theory here:

  https://github.com/tdunning/t-digest/blob/master/docs/theory/t-digest-paper/histo.pdf?raw=true

t-digests of partitions can also be merged hence suitable for parallel
implementations.

-- 
Olivier

"
Olivier Grisel <olivier.grisel@ensta.org>,"Thu, 5 Dec 2013 09:48:34 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",dev <dev@spark.incubator.apache.org>,"2013/12/4 Josh Rosen <rosenville@gmail.com>:

Great. Looking forward to meet you to.

Uri, you might want to register as well on the wiki :)

-- 
Olivier
http://twitter.com/ogrisel - http://github.com/ogrisel

"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 5 Dec 2013 14:49:55 +0200",PySpark - Dill serialization,dev@spark.incubator.apache.org,"Hi devs

I came across Dill (
http://trac.mystic.cacr.caltech.edu/project/pathos/wiki/dill) for Python
serialization. Was wondering if it may be a replacement to the cloudpickle
stuff (and remove that piece of code that needs to be maintained within
PySpark)?

Josh have you looked into Dill? Any thoughts?

N
"
Sam Bessalah <samkiller@gmail.com>,"Thu, 5 Dec 2013 14:00:36 +0100",Re: Spark streaming quantile?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Just as stated before Algebird has many data structure to compute those like QTree, or Ted's tvdigest . Or you can look at stream-lib q digest https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/quantile/QDigest.java 
Or another one Frugal Streaming well described and with an implementation on the AK blog
http://blog.aggregateknowledge.com/2013/09/16/sketch-of-the-day-frugal-streaming/
There are some example in the Spark streaming sample on how to integrate algebird .
Sam Bessalah

ebird-core/src/main/scala/com/twitter/algebird/QTree.scala>.
e:
as
uantile.html
e
"
Aaron Davidson <ilikerps@gmail.com>,"Thu, 5 Dec 2013 19:00:42 -0800",IntelliJ Scala Import Organizer Plugin,dev@spark.incubator.apache.org,"Hi guys, just wanted to share a little plugin I wrote for IntelliJ to help
auto-organize Scala imports. Anyone who has submitted a patch to Spark has
probably felt the exhilaration of manually sorting and bucketing your
imports. Well, now you can let your IDE have some fun!

It's in the plugin repository, so you can download it from within IntelliJ.
Just go to *Settings -> Plugins -> Browse repositories...* and type ""Scala
Style -> Scala Imports Organizer *to configure the bucketing rules. Here
are some Matei-approvedTM rules for Spark:

import scala.language.*

import java.*

import scala.*

import *

import org.apache.spark.*


To actually organize imports, just press Ctrl+Shift+O by default (I think
that's Cmd+Shift+O on Macs) or *Tools -> Organize Scala Imports*.

If you find a bug or have a suggestion, please let me know. The IntelliJ
plugin page is here
<http://plugins.jetbrains.com/plugin/7350?pr=idea>(feel free to vote!)
and the Github project is
here <https://github.com/aarondav/scala-imports-organizer>.

Also be sure to have a great day!
"
Imran Rashid <imran@quantifind.com>,"Thu, 5 Dec 2013 21:23:37 -0800",Re: IntelliJ Scala Import Organizer Plugin,dev@spark.incubator.apache.org,"awesome, thanks.

I've been wanting this even for all my scala projects for a while



"
Josh Rosen <rosenville@gmail.com>,"Thu, 5 Dec 2013 20:02:49 -0800",Re: PySpark - Dill serialization,"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","Thanks for the link!  I wasn't aware of Dill, but it looks like a nice
library.  I like that it's being actively developed:
https://github.com/uqfoundation/dill

It also seems to work correctly for a few edge-cases that cloudpickle
didn't handle properly, such as serializing operator.itemgetter instances
(see https://spark-project.atlassian.net/browse/SPARK-791).

I'll put together a pull request to replace CloudPickle with Dill.  Dill
uses a 3-clause BSD license, so we should be able to package it into an
.egg in the python/lib/ folder like we did for Py4J.  It will be
interesting to see whether the change has any performance impact, although
the recent custom serializers pull request should help with that since it
would let us use Dill for serializing functions and a faster serializer for
serializing data.

- Josh





"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 5 Dec 2013 20:15:09 -0800",Re: PySpark - Dill serialization,dev@spark.incubator.apache.org,"Looks cool! Josh, if you replace CloudPickle with this, make sure to also update the LICENSE file, which is supposed to contain third-party licenses.

Matei


instances
Dill
an
although
it
serializer for
Python
cloudpickle
within


"
Josh Rosen <rosenville@gmail.com>,"Fri, 6 Dec 2013 11:28:36 -0800",Re: PySpark - Dill serialization,"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","I tried replacing cloudpickle with Dill (
https://github.com/JoshRosen/incubator-spark/commit/2ac8986f3009f0dc133b11d16887fc8ddb33c3d1)
but I ran into a few issues.

It looks like Dill pickles function closures differently for functions
defined in doctests versus in module code / the shell, which breaks
PySpark's test suite; I opened an issue for this in the Dill repo:
https://github.com/uqfoundation/dill/issues/18.

Its closure cleaning may work differently than cloudpickle's because I've
also encountered some examples that also fail in the shell (accumulators).

Many simple cases, like PySpark's wordcount.py example, work fine, so I'm
hoping we'll be able to make the switch to Dill if those doctest issues are
resolved.



"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 7 Dec 2013 14:15:28 +0200",Intellij IDEA build issues,dev@spark.incubator.apache.org,"Hi Spark Devs,

Hoping someone cane help me out. No matter what I do, I cannot get Intellij
to build Spark from source. I am using IDEA 13. I run sbt gen-idea and
everything seems to work fine.

When I try to build using IDEA, everything compiles but I get the error
below.

Have any of you come across the same?

======

Internal error: (java.lang.AssertionError)
java/nio/channels/FileChannel$MapMode already declared as
ch.epfl.lamp.fjbg.JInnerClassesAttribute$Entry@1b5b798b
java.lang.AssertionError: java/nio/channels/FileChannel$MapMode already
declared as ch.epfl.lamp.fjbg.JInnerClassesAttribute$Entry@1b5b798b
at
ch.epfl.lamp.fjbg.JInnerClassesAttribute.addEntry(JInnerClassesAttribute.java:74)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator$$anonfun$addInnerClasses$3.apply(GenJVM.scala:738)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator$$anonfun$addInnerClasses$3.apply(GenJVM.scala:733)
at
scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)
at scala.collection.immutable.List.foreach(List.scala:76)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator.addInnerClasses(GenJVM.scala:733)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator.emitClass(GenJVM.scala:200)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator.genClass(GenJVM.scala:355)
at
scala.tools.nsc.backend.jvm.GenJVM$JvmPhase$$anonfun$run$4.apply(GenJVM.scala:86)
at
scala.tools.nsc.backend.jvm.GenJVM$JvmPhase$$anonfun$run$4.apply(GenJVM.scala:86)
at
scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:104)
at
scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:104)
at scala.collection.Iterator$class.foreach(Iterator.scala:772)
at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:45)
at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:104)
at scala.tools.nsc.backend.jvm.GenJVM$JvmPhase.run(GenJVM.scala:86)
at scala.tools.nsc.Global$Run.compileSources(Global.scala:953)
at scala.tools.nsc.Global$Run.compile(Global.scala:1041)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
at xsbt.CachedCompiler0.liftedTree1$1(CompilerInterface.scala:99)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3$$anonfun$apply$1.apply$mcV$sp(AggressiveCompile.scala:106)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3$$anonfun$apply$1.apply(AggressiveCompile.scala:106)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3$$anonfun$apply$1.apply(AggressiveCompile.scala:106)
at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:173)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3.apply(AggressiveCompile.scala:105)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3.apply(AggressiveCompile.scala:102)
at scala.Option.foreach(Option.scala:236)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:102)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:102)
at scala.Option.foreach(Option.scala:236)
at
sbt.compiler.AggressiveCompile$$anonfun$6.compileScala$1(AggressiveCompile.scala:102)
at
sbt.compiler.AggressiveCompile$$anonfun$6.apply(AggressiveCompile.scala:151)
at
sbt.compiler.AggressiveCompile$$anonfun$6.apply(AggressiveCompile.scala:89)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
at sbt.inc.Incremental$.cycle(Incremental.scala:75)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:34)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:33)
at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:42)
at sbt.inc.Incremental$.compile(Incremental.scala:33)
at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:164)
at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:73)
at
org.jetbrains.jps.incremental.scala.local.CompilerImpl.compile(CompilerImpl.scala:61)
at
org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:26)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(ScalaBuilder.scala:118)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(ScalaBuilder.scala:100)
at scala.util.Either$RightProjection.map(Either.scala:536)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3.apply(ScalaBuilder.scala:100)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3.apply(ScalaBuilder.scala:99)
at scala.util.Either$RightProjection.flatMap(Either.scala:523)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5.apply(ScalaBuilder.scala:99)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5.apply(ScalaBuilder.scala:98)
at scala.util.Either$RightProjection.flatMap(Either.scala:523)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder.doBuild(ScalaBuilder.scala:98)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder.build(ScalaBuilder.scala:68)
at
org.jetbrains.jps.incremental.scala.ScalaBuilderService$ScalaBuilderDecorator.build(ScalaBuilderService.java:42)
at
org.jetbrains.jps.incremental.IncProjectBuilder.runModuleLevelBuilders(IncProjectBuilder.java:1086)
at
org.jetbrains.jps.incremental.IncProjectBuilder.runBuildersForChunk(IncProjectBuilder.java:797)
at
org.jetbrains.jps.incremental.IncProjectBuilder.buildTargetsChunk(IncProjectBuilder.java:845)
at
org.jetbrains.jps.incremental.IncProjectBuilder.buildChunkIfAffected(IncProjectBuilder.java:760)
at
org.jetbrains.jps.incremental.IncProjectBuilder.buildChunks(IncProjectBuilder.java:583)
at
org.jetbrains.jps.incremental.IncProjectBuilder.runBuild(IncProjectBuilder.java:344)
at
org.jetbrains.jps.incremental.IncProjectBuilder.build(IncProjectBuilder.java:184)
at org.jetbrains.jps.cmdline.BuildRunner.runBuild(BuildRunner.java:129)
at org.jetbrains.jps.cmdline.BuildSession.runBuild(BuildSession.java:224)
at org.jetbrains.jps.cmdline.BuildSession.run(BuildSession.java:113)
at
org.jetbrains.jps.cmdline.BuildMain$MyMessageHandler$1.run(BuildMain.java:133)
at
org.jetbrains.jps.service.impl.SharedThreadPoolImpl$1.run(SharedThreadPoolImpl.java:41)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 7 Dec 2013 15:24:57 -0800",[VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.1.

The tag to be voted on is v0.8.1-incubating (commit fba8738):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=720e75581ae5f0c4835513ee06bfa0cb71923c57

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc1/
- or -
https://dist.apache.org/repos/dist/dev/incubator/spark/spark-0.8.1-incubating-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-022/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc1-docs/

Please vote on releasing this package as Apache Spark 0.8.1-incubating!

The vote is open until Tuesday, December 9th at 21:30 UTC and passes if
a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.1-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/

"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 7 Dec 2013 17:41:18 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc1),dev@spark.incubator.apache.org,"Not sure.  I haven't been able to discern any pattern as to what new code
goes into both 0.9 and 0.8 vs. what goes only into 0.8, so I can't really
tell whether 0.8.1 is done or if something has been overlooked and not
cherry-picked from master.



"
Josh Rosen <rosenville@gmail.com>,"Sun, 8 Dec 2013 00:03:27 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","We can use git log to figure out which changes haven't made it into
branch-0.8.  Here's a quick attempt, which only lists pull requests that
were only merged into one of the branches.  For completeness, this could be
extended to find commits that weren't part of a merge and are only present
in one branch.

*Script:*

MASTER_BRANCH=origin/master
RELEASE_BRANCH=origin/branch-0.8

git log --oneline --grep ""Merge pull request"" $MASTER_BRANCH  | cut -f 2-
-d ' ' | sort > master-prs
git log --oneline --grep ""Merge pull request"" $RELEASE_BRANCH | cut -f 2-
-d ' ' | sort > release-prs

comm -23 master-prs release-prs > master-only
comm -23 release-prs master-prs > release-only


Merge pull request #1 from colorant/yarn-client-2.2
Merge pull request #105 from pwendell/doc-fix
Merge pull request #110 from pwendell/master
Merge pull request #146 from JoshRosen/pyspark-custom-serializers
Merge pull request #151 from russellcardullo/add-graphite-sink
Merge pull request #154 from soulmachine/ClusterScheduler
Merge pull request #156 from haoyuan/master
Merge pull request #159 from liancheng/dagscheduler-actor-refine
Merge pull request #16 from pwendell/master
Merge pull request #185 from mkolod/random-number-generator
Merge pull request #187 from aarondav/example-bcast-test
Merge pull request #190 from markhamstra/Stages4Jobs
Merge pull request #198 from ankurdave/zipPartitions-preservesPartitioning
Merge pull request #2 from colorant/yarn-client-2.2
Merge pull request #203 from witgo/master
Merge pull request #204 from rxin/hash
Merge pull request #205 from kayousterhout/logging
Merge pull request #206 from ash211/patch-2
Merge pull request #207 from henrydavidge/master
Merge pull request #209 from pwendell/better-docs
Merge pull request #210 from haitaoyao/http-timeout
Merge pull request #212 from markhamstra/SPARK-963
Merge pull request #216 from liancheng/fix-spark-966
Merge pull request #217 from aarondav/mesos-urls
Merge pull request #22 from GraceH/metrics-naming
Merge pull request #220 from rxin/zippart
Merge pull request #225 from ash211/patch-3
Merge pull request #226 from ash211/patch-4
Merge pull request #233 from hsaputra/changecontexttobackend
Merge pull request #239 from aarondav/nit
Merge pull request #242 from pwendell/master
Merge pull request #3 from aarondav/pv-test
Merge pull request #36 from pwendell/versions
Merge pull request #37 from pwendell/merge-0.8
Merge pull request #39 from pwendell/master
Merge pull request #45 from pwendell/metrics_units
Merge pull request #56 from jerryshao/kafka-0.8-dev
Merge pull request #64 from prabeesh/master
Merge pull request #66 from shivaram/sbt-assembly-deps
Merge pull request #670 from jey/ec2-ssh-improvements
Merge pull request #71 from aarondav/scdefaults
Merge pull request #78 from mosharaf/master
Merge pull request #8 from vchekan/checkpoint-ttl-restore
Merge pull request #80 from rxin/build
Merge pull request #82 from JoshRosen/map-output-tracker-refactoring
Merge pull request #86 from holdenk/master
Merge pull request #938 from ilikerps/master
Merge pull request #940 from ankurdave/clear-port-properties-after-tests
Merge pull request #98 from aarondav/docs
Merge pull request #99 from pwendell/master

Merge pull request #138 from marmbrus/branch-0.8
Merge pull request #140 from aarondav/merge-75
Merge pull request #231 from pwendell/branch-0.8
Merge pull request #241 from pwendell/branch-0.8
Merge pull request #241 from pwendell/master
Merge pull request #243 from pwendell/branch-0.8
Merge pull request #40 from pwendell/branch-0.8
Merge pull request #47 from xiliu82/branch-0.8
Merge pull request #79 from aarondav/scdefaults0.8
Merge pull request #801 from pwendell/print-launch-command
Merge pull request #918 from pwendell/branch-0.8
Revert ""Merge pull request #94 from aarondav/mesos-fix""





"
Henry Saputra <henry.saputra@gmail.com>,"Sun, 8 Dec 2013 11:37:25 -0800",[DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","HI Spark devs,

I have modified the Subject to avoid polluting the VOTE thread since
it related to more info how and which commits merge back to 0.8.*
branch.
Please respond to the previous question to this thread.

Technically the CHANGES.txt [1] file should describe the changes in a
particular release and it is the main requirement needed to cut an ASF
release.


- Henry

[1] https://github.com/apache/incubator-spark/blob/branch-0.8/CHANGES.txt


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 12:03:39 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Guys,

Matei found a few small doc fixes so I'm going to cut a new RC today.
I'll include the release credits and summary in that e-mail so people
know what they are voting in.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 12:04:37 -0800",Re: [DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Henry,

Are you suggesting we need to change something about or changes file?
Or are you just pointing people to the file?

- Patrick


"
Henry Saputra <henry.saputra@gmail.com>,"Sun, 8 Dec 2013 12:37:00 -0800",Re: [DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah, sorry for the confusion Patrick, like you said I was just trying to let
people aware about this file and the purpose of it.


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 12:41:23 -0800",[VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.1.

The tag to be voted on is v0.8.1-incubating (commit bf23794a):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=e6ba91b5a7527316202797fc3dce469ff86cf203

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-024/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc2-docs/

For information about the contents of this release see:
<attached> draft of release notes
<attached> draft of release credits
https://github.com/apache/incubator-spark/blob/branch-0.8/CHANGES.txt

Please vote on releasing this package as Apache Spark 0.8.1-incubating!

The vote is open until Wednesday, December 11th at 21:00 UTC and
passes if a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.1-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Michael Armbrust -- build fix

Pierre Borckmans -- typo fix in documentation

Evan Chan -- added `local://` scheme for dependency jars

Ewen Cheslack-Postava -- `add` method for python accumulators, support for setting config properties in python

Mosharaf Chowdhury -- optimized broadcast implementation

Frank Dai -- documentation fix

Aaron Davidson -- lead on shuffle file consolidation, lead on h/a mode for standalone scheduler, cleaned up representation of block id’s, several small improvements and bug fixes

Tathagata Das -- new streaming operators: `transformWith`, `leftInnerJoin`, and `rightOuterJoin`, fix for kafka concurrency bug

Ankur Dave -- support for pausing spot clusters on EC2

Harvey Feng -- optimization to JobConf broadcasts, minor fixes, lead on YARN 2.2 build

Ali Ghodsi -- scheduler support for SIMR, lead on YARN 2.2 build

Thomas Graves -- lead on Spark YARN integration including secure HDFS access over YARN

Li Guoqiang -- fix for maven build

Stephen Haberman -- bug fix

Haidar Hadi -- documentation fix

Nathan Howell -- bug fix relating to YARN

Holden Karau -- java version of `mapPartitionsWithIndex`

Du Li -- bug fix in make-distrubion.sh

Xi Lui -- bug fix and code clean-up

David McCauley -- bug fix in standalone mode JSON output

Michael (wannabeast) -- bug fix in memory store

Fabrizio Milo -- typos in documentation, minor clean-up in DAGScheduler, typo in scaladoc

Mridul Muralidharan -- fixes to meta-data cleaner and speculative scheduler

Sundeep Narravula -- build fix, bug fixes in scheduler and tests, minor code clean-up

Kay Ousterhout -- optimization to task result fetching, extensive code clean-up and refactoring (task schedulers, thread pools), result-fetching state in UI, showing task and attempt it in UI, several bug fixes in scheduler, UI, and unit tests

Nick Pentreath -- implicit feedback variant of ALS algorithm

Imran Rashid -- small improvement to executor launch

Ahir Reddy -- spark support for SIMR

Josh Rosen -- reduced memory overhead for BlockInfo objects, clean up of BlockManager code, fix to java API auditor, code clean-up in java API, and bug fixes in python API

Henry Saputra -- build fix

Jerry Shao -- refactoring of fair scheduler, support for running spark as a specific user, bug fix

Mingfei Shi -- documentation for JobLogger

Andre Schumacher -- sortByKey in pyspark and associated changes

Karthik Tunga -- bug fix in launch script

Patrick Wendell -- added `repartition` operator, logging improvements, instrumentation for shuffle write, documentation improvements, fix for streaming example, and release management

Neal Wiggins -- minor import clean-up, documentation typo

Andrew Xia -- bug fix in UI

Reynold Xin -- optimized hash set and hash tables for primitive types, task killing, support for setting job properties in repl, logging improvements, Kryo improvements, several bug fixes, and general clean-up

Matei Zaharia -- optimized hashmap for shuffle data, pyspark documentation, optimizations to kryo and chill serializers

Wu Zeming -- bug fix in executors UI
DRAFT OF RELEASE NOTES FOR SPARK 0.8.1

Apache Spark 0.8.1 is a maintenance release including several bug fixes and performance optimizations. It also includes a few new features. Contributions to 0.8.1 came from 40 developers.

== High availability mode for standalone scheduler ==
The standalone scheduler now has a High Availability (H/A) mode which can tolerate master failures. This is particularly useful for long-running applications such as streaming jobs and the shark server, where the scheduler master previous represented a single point of failure. Instructions for deploying H/A mode are included in the documentation. The current implementation uses Zookeeper for coordination.

== YARN 2.2 support ==
Support has been added for submitting Spark applications to YARN 2.2 and newer. Due to a dependency conflict, this did not work properly in Spark 0.8.0 and earlier. See the release documentation for specific instructions on how to build Spark for YARN 2.2+.

== Internal Optimizations ==
This release adds several performance optimizations:
  - Append only map for shuffle - an internal hashmap optimized for storing shuffle data
  - Efficient encoding for Job confs - improves latency for stages reading large numbers of blocks from HDFS, S3, and HBase
  - Shuffle file consolidation (off by default) - reduces the number of files created in large shuffles for better filesystem performance. We recommend users turn this on unless they are using ext3.
  - Torrent broadcast (off by default) - reduces network overhead and latency of broadcasting large objects.
  - Support for fetching large result sets - allows tasks to return large results without tuning akka buffer sizes.

== Python improvements == 
  - new `add` method for accumulators
  - it is now possible to set config properties directly from python
  - python now supports sorted RDD’s

== New operators and usability improvements == 
- local:// URI’s - allows users to specify already present on slaves as dependencies
- a new “result fetching” state has been added to the UI
- new spark streaming operators: transformWith, leftInnerJoin, rightOuterJoin
- new spark operators: repartition

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 12:47:37 -0800",[VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm cancelling this vote in favor of a vote on rc2. rc2 includes some
documentation clean-up and changes on top of rc1.

I also added (in that thread) more color on the general contents of
this release. 0.8.1 is a maintenance release, but because the
0.8-branch will be the last branch to support scala 2.9, we've elected
to add some larger features and optimizations that otherwise might not
have made the cut. The larger changes (H/A mode for the scheduler and
shuffle file consolidation) are not enabled by default to avoid major
changes for people upgrading from 0.8.0.

- Patrick

"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 13:30:45 -0800",Re: [DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),dev@spark.incubator.apache.org,"I'm aware of the changes file, but it really doesn't address the issue that
I am raising.  The changes file just tells me what has gone into the
release candidate.  In general, it doesn't tell me why those changes went
in or provide any rationale by which to judge whether that is the complete
set of changes that should go in.

I talked some with Matei about related versioning and release issues last
week, and I've raised them in other contexts previously, but I'm taking the
liberty to annoy people again because I really am not happy with our
current versioning and release process, and I really am of the opinion that
we've got to start doing much better before I can vote in favor of a 1.0
release.  I fully realize that this is not a 1.0 release, and that because
we are pre-1.0 we still have a lot of flexibility with releases that break
backward or forward compatibility and with version numbers that have
nothing like the semantic meaning that they will eventually need to have;
but it is not going to be easy to change our process and culture so that we
produce the kind of stability and reliability that Spark users need to be
able to depend upon and version numbers that clearly communicate what those
users expect them to mean.  I think that we should start making those
changes now.  Just because we have flexibility pre-1.0, that doesn't mean
that we shouldn't start training ourselves now to work within the
constraints of post-1.0 Spark.  If I'm to be happy voting for an eventual
1.0 release candidate, I'll need to have seen at least one full development
cycle that already adheres to the post-1.0 constraints, demonstrating the
maturity of our development process.

That demonstration cycle is clearly not this one -- and I understand that
there were some compelling reasons (particularly with regard too getting a
""full"" release of Spark based on Scala 2.9.3 before we make the jump to
2.10.  This ""patch-level"" release breaks binary compatibility and contains
a lot of code that isn't anywhere close to meeting the criterion for
inclusion in a real, post-1.0 patch-level release: essentially ""changes
that every, or nearly every, existing Spark user needs (not just wants),
and that work with all existing and future binaries built with the prior
patch-level version of Spark as a dependency.""  Like I said, we are clearly
nowhere close to that with the move from 0.8.0 to 0.8.1; but I also haven't
been able to recognize any alternative criterion by which to judge the
quality and completeness of this release candidate.

Maybe there just isn't one, and I'm just going to have to swallow my
concerns while watching 0.8.1 go out the door; but if we don't start doing
better on this kind of thing in the future, you are going to start hearing
more complaining from me. I just hope that it doesn't get to the point
where I feel compelled to actively oppose an eventual 1.0 release
candidate.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 14:12:55 -0800",Re: [DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark,

assess the quality and completeness of this release is to download the
release, run the tests, run the release in your dev environment, read
through the documentation, etc. This is one of the main points of
releasing an RC to the community... even if you disagree with some
patches that were merged in, this is still a way you can help validate
the release.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 14:45:29 -0800",Re: [DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),dev@spark.incubator.apache.org,"Yup, I'm already started on that process.

And it's not that I disagree with any particular change that was merged per
se -- I haven't seen anything merged that most users won't want.  It's more
that I object to the burden that our current development/versioning/release
process puts on Spark users responsible for production code.  For them,
adopting a new patch-level release should be a decision requiring almost no
thinking since the new release should be essentially just bug-fixes that
maintain full binary compatibility.  With our current process, those users
have to suck in a bunch of new, less-tested, less-mature code that may
comprise new features or functionality that the user doesn't want (at least
not right away in production), but that they can't cleanly separate from
the bug-fixes that they do want.  Our process simply has to change if we
place users' desires ahead of Spark developers' desires.



"
Taka Shinagawa <taka.epsilon@gmail.com>,"Sun, 8 Dec 2013 16:12:36 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"With Hadoop 2.2.0 (& Java 1.7.0_45) installed, I'm having trouble
completing the build process (sbt/sbt assembly) on Macbook. The sbt command
hangs at the last step.

...
...
[info] SHA-1: ce8275f5841002164c4305c912a2892ec7c1d395
[info] Packaging
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/tools/target/scala-2.9.3/spark-tools-assembly-0.8.1-incubating.jar
...
[info] SHA-1: 0657a347240266230247693f265a5797d40c326a
[info] Packaging
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/assembly/target/scala-2.9.3/spark-assembly-0.8.1-incubating-hadoop1.0.4.jar
...
(hangs here)
--------------------------


able to build it successfully.
..
..
[info] SHA-1: 77109cd085bd4f0d2b601b3451b35b961d357534
[info] Packaging
/Users/tshinagawa/Documents/Spark/RCs/spark-0.8.1-incubating/examples/target/scala-2.9.3/spark-examples-assembly-0.8.1-incubating.jar
...
[info] Done packaging.
[success] Total time: 266 s, completed Dec 8, 2013 3:03:10 PM
--------------------------




"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 16:24:59 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"Hey Take,

Could you start a separate thread to debug your build issue? In that
thread, could you paste the exact build command and entire output? The log
you posted here suggests the first build detected hadoop 1.0.4 not 2.2.0
based on the assembly file name it is logging.

---
sent from my phone

"
Taka Shinagawa <taka.epsilon@gmail.com>,"Sun, 8 Dec 2013 16:30:18 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"OK. I will post the entire output via separate email. I just upgraded
Hadoop to 2.2.0 recently. So there might be something I need to
remove/clean up.



"
Taka Shinagawa <taka.epsilon@gmail.com>,"Sun, 8 Dec 2013 16:46:39 -0800",Spark Build Issue ('sbt/sbt assembly' hangs),dev@spark.incubator.apache.org,"As I reported in the Spark 0.8-1 RC2 thread, the 'sbt/sbt assembly' hangs
at the last step. It happens on a Macbook with Hadoop 2.2.0 (& Java
1.7.0_45) installed. The build was successful on another system with Hadoop
1.1.1 installed.

Here's the build command and the entire log. Thanks for the help.

---------------------------------------------------------------
$ sbt/sbt assembly
[info] Loading project definition from
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/project/project
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/project/project/}default-c4ca6d...
[info] Resolving org.scala-sbt#precompiled-2_10_1;0.12.4 ...
[info] Done updating.
[info] Compiling 1 Scala source to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/project/project/target/scala-2.9.2/sbt-0.12/classes...
[info] Loading project definition from
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/project
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/project/}plugins...
[info] Resolving org.scala-sbt#precompiled-2_10_1;0.12.4 ...
[info] Done updating.
[info] Compiling 1 Scala source to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/project/target/scala-2.9.2/sbt-0.12/classes...
[info] Set current project to root (in build
file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/)
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}core...
[info] Resolving org.apache.derby#derby;10.4.2.0 ...
[info] Done updating.
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}streaming...
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}bagel...
[info] Resolving org.scala-lang#scala-library;2.9.3 ...
[info] Done updating.
[info] Resolving org.eclipse.jetty#jetty-util;7.6.8.v20121106 ...
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}mllib...
[info] Resolving org.mockito#mockito-all;1.8.5 ...
[info] Done updating.
[info] Resolving org.mockito#mockito-all;1.8.5 ...
[info] Done updating.
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}tools...
[info] Resolving org.mockito#mockito-all;1.8.5 ...
[info] Done updating.
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}examples...
[info] Resolving org.slf4j#slf4j-api;1.7.2 ...
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}repl...
[info] Resolving org.mockito#mockito-all;1.8.5 ...
[info] Done updating.
[info] Resolving org.apache.httpcomponents#httpcore;4.1 ...
[info] Compiling 285 Scala sources and 18 Java sources to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/core/target/scala-2.9.3/classes...
[info] Resolving org.mockito#mockito-all;1.8.5 ...
[info] Done updating.
[info] Updating
{file:/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/}assembly...
[info] Resolving org.mockito#mockito-all;1.8.5 ...
[info] Done updating.
[warn]
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:131:
method cleanupJob in class OutputCommitter is deprecated: see corresponding
Javadoc for more information.
[warn]     getOutputCommitter().cleanupJob(getJobContext())
[warn]                          ^
[warn]
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/core/src/main/scala/org/apache/spark/broadcast/BitTorrentBroadcast.scala:1059:
class BitTorrentBroadcast in package broadcast is deprecated: Use
TorrentBroadcast
[warn]   def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =
[warn]       ^
[warn]
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/core/src/main/scala/org/apache/spark/broadcast/BitTorrentBroadcast.scala:1060:
class BitTorrentBroadcast in package broadcast is deprecated: Use
TorrentBroadcast
[warn]     new BitTorrentBroadcast[T](value_, isLocal, id)
[warn]         ^
[warn]
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/core/src/main/scala/org/apache/spark/broadcast/TreeBroadcast.scala:600:
class TreeBroadcast in package broadcast is deprecated: Use TorrentBroadcast
[warn]   def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =
[warn]       ^
[warn]
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/core/src/main/scala/org/apache/spark/broadcast/TreeBroadcast.scala:601:
class TreeBroadcast in package broadcast is deprecated: Use TorrentBroadcast
[warn]     new TreeBroadcast[T](value_, isLocal, id)
[warn]         ^
[warn]
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:598:
method cleanupJob in class OutputCommitter is deprecated: see corresponding
Javadoc for more information.
[warn]     jobCommitter.cleanupJob(jobTaskContext)
[warn]                  ^
[warn] 6 warnings found
[warn] warning: [options] bootstrap class path not set in conjunction with
-source 1.5
[warn] Note: Some input files use unchecked or unsafe operations.
[warn] Note: Recompile with -Xlint:unchecked for details.
[warn] 1 warning
[info] Compiling 1 Scala source to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/bagel/target/scala-2.9.3/classes...
[info] Compiling 49 Scala sources to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/streaming/target/scala-2.9.3/classes...
[info] Compiling 25 Scala sources to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/mllib/target/scala-2.9.3/classes...
[info] Compiling 10 Scala sources to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/repl/target/scala-2.9.3/classes...
[warn]
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala:141:
method stop in class Thread is deprecated: see corresponding Javadoc for
more information.
[warn]         line.thread.stop()
[warn]                     ^
[warn] one warning found
[info] Compiling 39 Scala sources and 13 Java sources to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/examples/target/scala-2.9.3/classes...
[info] Compiling 1 Scala source to
/Users/taka/Documents/Spark/Releases/spark-0.8.1-incubating-rc2/tools/target/scala-2.9.3/classes...
[warn] warning: [options] bootstrap class path not set in conjunction with
-source 1.5
[warn] 1 warning
[info] Including: scala-compiler.jar
[info] Including: scala-compiler.jar
[info] Including: scala-library.jar
[info] Including: scala-library.jar
[info] Including: compress-lzf-0.8.4.jar
[info] Including: py4j-0.7.jar
[info] Including: compress-lzf-0.8.4.jar
[info] Including: config-0.3.1.jar
[info] Including: config-0.3.1.jar
[info] Including: guava-14.0.1.jar
[info] Including: guava-14.0.1.jar
[info] Including: kryo-2.21.jar
[info] Including: kryo-2.21.jar
[info] Including: log4j-1.2.17.jar
[info] Including: log4j-1.2.17.jar
[info] Including: metrics-core-3.0.0.jar
[info] Including: metrics-core-3.0.0.jar
[info] Including: metrics-ganglia-3.0.0.jar
[info] Including: metrics-ganglia-3.0.0.jar
[info] Including: metrics-json-3.0.0.jar
[info] Including: metrics-json-3.0.0.jar
[info] Including: metrics-jvm-3.0.0.jar
[info] Including: metrics-jvm-3.0.0.jar
[info] Including: netty-3.5.4.Final.jar
[info] Including: netty-3.5.4.Final.jar
[info] Including: snappy-java-1.0.5.jar
[info] Including: snappy-java-1.0.5.jar
[info] Including: akka-actor-2.0.5.jar
[info] Including: akka-actor-2.0.5.jar
[info] Including: akka-remote-2.0.5.jar
[info] Including: akka-remote-2.0.5.jar
[info] Including: akka-slf4j-2.0.5.jar
[info] Including: akka-slf4j-2.0.5.jar
[info] Including: akka-zeromq-2.0.5.jar
[info] Including: akka-zeromq-2.0.5.jar
[info] Including: asm-4.0.jar
[info] Including: asm-4.0.jar
[info] Including: asm-commons-4.0.jar
[info] Including: asm-commons-4.0.jar
[info] Including: asm-tree-4.0.jar
[info] Including: asm-tree-4.0.jar
[info] Including: avro-1.7.4.jar
[info] Including: avro-1.7.4.jar
[info] Including: avro-ipc-1.7.4.jar
[info] Including: avro-ipc-1.7.4.jar
[info] Including: chill-java-0.3.1.jar
[info] Including: chill-java-0.3.1.jar
[info] Including: chill_2.9.3-0.3.1.jar
[info] Including: chill_2.9.3-0.3.1.jar
[info] Including: colt-1.2.0.jar
[info] Including: colt-1.2.0.jar
[info] Including: commons-beanutils-1.7.0.jar
[info] Including: commons-beanutils-1.7.0.jar
[info] Including: commons-beanutils-core-1.8.0.jar
[info] Including: commons-beanutils-core-1.8.0.jar
[info] Including: commons-codec-1.4.jar
[info] Including: commons-codec-1.4.jar
[info] Including: commons-collections-3.2.1.jar
[info] Including: commons-collections-3.2.1.jar
[info] Including: commons-compress-1.4.1.jar
[info] Including: commons-compress-1.4.1.jar
[info] Including: commons-configuration-1.6.jar
[info] Including: commons-configuration-1.6.jar
[info] Including: commons-daemon-1.0.10.jar
[info] Including: commons-daemon-1.0.10.jar
[info] Including: commons-digester-1.8.jar
[info] Including: commons-digester-1.8.jar
[info] Including: commons-el-1.0.jar
[info] Including: commons-el-1.0.jar
[info] Including: commons-httpclient-3.1.jar
[info] Including: commons-httpclient-3.1.jar
[info] Including: commons-io-2.1.jar
[info] Including: commons-io-2.1.jar
[info] Including: commons-lang-2.4.jar
[info] Including: commons-lang-2.4.jar
[info] Including: commons-logging-1.1.1.jar
[info] Including: commons-logging-1.1.1.jar
[info] Including: commons-math-2.1.jar
[info] Including: commons-math-2.1.jar
[info] Including: commons-net-1.4.1.jar
[info] Including: commons-net-1.4.1.jar
[info] Including: concurrent-1.3.4.jar
[info] Including: concurrent-1.3.4.jar
[info] Including: dispatch-json_2.9.1-0.8.5.jar
[info] Including: dispatch-json_2.9.1-0.8.5.jar
[info] Including: fastutil-6.4.4.jar
[info] Including: fastutil-6.4.4.jar
[info] Including: flume-ng-sdk-1.2.0.jar
[info] Including: flume-ng-sdk-1.2.0.jar
[info] Including: gmetric4j-1.0.3.jar
[info] Including: gmetric4j-1.0.3.jar
[info] Including: h2-lzf-1.0.jar
[info] Including: h2-lzf-1.0.jar
[info] Including: hadoop-client-1.0.4.jar
[info] Including: hadoop-client-1.0.4.jar
[info] Including: hadoop-core-1.0.4.jar
[info] Including: hadoop-core-1.0.4.jar
[info] Including: hsqldb-1.8.0.10.jar
[info] Including: hsqldb-1.8.0.10.jar
[info] Including: httpclient-4.1.jar
[info] Including: httpclient-4.1.jar
[info] Including: httpcore-4.1.jar
[info] Including: httpcore-4.1.jar
[info] Including: jackson-annotations-2.2.2.jar
[info] Including: jackson-annotations-2.2.2.jar
[info] Including: jackson-core-2.2.2.jar
[info] Including: jackson-core-2.2.2.jar
[info] Including: jackson-core-asl-1.8.8.jar
[info] Including: jackson-core-asl-1.8.8.jar
[info] Including: jackson-databind-2.2.2.jar
[info] Including: jackson-databind-2.2.2.jar
[info] Including: jackson-mapper-asl-1.8.8.jar
[info] Including: jackson-mapper-asl-1.8.8.jar
[info] Including: jets3t-0.7.1.jar
[info] Including: jetty-6.1.26.jar
[info] Including: jblas-1.2.3.jar
[info] Including: jetty-continuation-7.6.8.v20121106.jar
[info] Including: jets3t-0.7.1.jar
[info] Including: jetty-http-7.6.8.v20121106.jar
[info] Including: jetty-io-7.6.8.v20121106.jar
[info] Including: jetty-6.1.26.jar
[info] Including: jetty-server-7.6.8.v20121106.jar
[info] Including: jetty-util-6.1.26.jar
[info] Including: jetty-continuation-7.6.8.v20121106.jar
[info] Including: jetty-http-7.6.8.v20121106.jar
[info] Including: jetty-util-7.6.8.v20121106.jar
[info] Including: jetty-io-7.6.8.v20121106.jar
[info] Including: jetty-server-7.6.8.v20121106.jar
[info] Including: jline-0.9.94.jar
[info] Including: jetty-util-6.1.26.jar
[info] Including: jetty-util-7.6.8.v20121106.jar
[info] Including: jna-3.0.9.jar
[info] Including: jline-0.9.94.jar
[info] Including: jna-3.0.9.jar
[info] Including: jnr-constants-0.8.2.jar
[info] Including: jnr-constants-0.8.2.jar
[info] Including: jsr305-1.3.9.jar
[info] Including: junit-3.8.1.jar
[info] Including: jsr305-1.3.9.jar
[info] Including: junit-3.8.1.jar
[info] Including: lift-json_2.9.2-2.5.jar
[info] Including: lift-json_2.9.2-2.5.jar
[info] Including: mesos-0.13.0.jar
[info] Including: mesos-0.13.0.jar
[info] Including: minlog-1.2.jar
[info] Including: minlog-1.2.jar
[info] Including: netty-all-4.0.0.Beta2.jar
[info] Including: netty-all-4.0.0.Beta2.jar
[info] Including: objenesis-1.2.jar
[info] Including: objenesis-1.2.jar
[info] Including: oncrpc-1.0.7.jar
[info] Including: oncrpc-1.0.7.jar
[info] Including: oro-2.0.8.jar
[info] Including: oro-2.0.8.jar
[info] Including: paranamer-2.4.1.jar
[info] Including: paranamer-2.4.1.jar
[info] Including: protobuf-java-2.4.1.jar
[info] Including: protobuf-java-2.4.1.jar
[info] Including: reflectasm-1.07-shaded.jar
[info] Including: reflectasm-1.07-shaded.jar
[info] Including: scalap-2.9.2.jar
[info] Including: scalap-2.9.2.jar
[info] Including: servlet-api-2.5-20110124.jar
[info] Including: servlet-api-2.5-20110124.jar
[info] Including: sjson_2.9.1-0.15.jar
[info] Including: sjson_2.9.1-0.15.jar
[info] Including: slf4j-api-1.7.5.jar
[info] Including: slf4j-api-1.7.5.jar
[info] Including: slf4j-log4j12-1.7.2.jar
[info] Including: slf4j-log4j12-1.7.2.jar
[info] Including: twitter4j-core-3.0.3.jar
[info] Including: twitter4j-core-3.0.3.jar
[info] Including: twitter4j-stream-3.0.3.jar
[info] Including: twitter4j-stream-3.0.3.jar
[info] Including: velocity-1.7.jar
[info] Including: velocity-1.7.jar
[info] Including: xmlenc-0.52.jar
[info] Including: xmlenc-0.52.jar
[info] Including: xz-1.0.jar
[info] Including: xz-1.0.jar
[info] Including: zeromq-scala-binding_2.9.1-0.0.6.jar
[info] Including: zeromq-scala-binding_2.9.1-0.0.6.jar
[info] Including: zkclient-0.1.jar
[info] Including: zkclient-0.1.jar
[info] Including: zookeeper-3.4.5.jar
[info] Including: zookeeper-3.4.5.jar
[info] Including: javax.servlet-2.5.0.v201103041518.jar
[info] Including: javax.servlet-2.5.0.v201103041518.jar
[info] Including: scala-jline.jar
[info] Including: kafka-0.7.2-spark.jar
[info] Including: kafka-0.7.2-spark.jar
[warn] Merging 'org/objenesis/instantiator/gcj/GCJInstantiator.class' with
strategy 'first'
[warn] Merging 'javax/servlet/SingleThreadModel.class' with strategy 'first'
[warn] Merging 'javax/servlet/RequestDispatcher.class' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/FloatArrayConverter.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/ResultSetIterator.class' with
strategy 'first'
[warn] Merging
'org/objenesis/instantiator/sun/SunReflectionFactorySerializationInstantiator.class'
with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/locale/LocaleBeanUtils.class'
with strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Label.class' with
strategy 'first'
[warn] Merging 'org/objenesis/ObjenesisSerializer.class' with strategy
'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/MethodVisitor.class'
with strategy 'first'
[warn] Merging 'org/apache/commons/collections/FastHashMap$EntrySet.class'
with strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassVisitor.class'
with strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/FieldVisitor.class'
with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/PropertyUtilsBean.class' with
strategy 'first'
[warn] Merging 'javax/servlet/ServletContextAttributeEvent.class' with
strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader.class'
with strategy 'first'
[warn] Merging 'com/esotericsoftware/minlog/Log$Logger.class' with strategy
'first'
[warn] Merging 'org/apache/commons/beanutils/ConvertingWrapDynaBean.class'
with strategy 'first'
[warn] Merging 'META-INF/ASL2.0' with strategy 'first'
[warn] Merging 'javax/servlet/http/NoBodyResponse.class' with strategy
'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/BigIntegerLocaleConverter.class'
with strategy 'first'
[warn] Merging
'org/objenesis/instantiator/basic/AccessibleInstantiator.class' with
strategy 'first'
[warn] Merging 'javax/servlet/resources/web-app_2_2.dtd' with strategy
'first'
[warn] Merging 'org/apache/commons/beanutils/locale/LocaleConverter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/resources/xml.xsd' with strategy 'first'
 [warn] Merging 'org/apache/commons/collections/Buffer.class' with strategy
'first'
[warn] Merging
'org/apache/commons/beanutils/converters/FloatConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/DoubleLocaleConverter.class'
with strategy 'first'
[warn] Merging
'org/objenesis/instantiator/basic/ObjectStreamClassInstantiator.class' with
strategy 'first'
[warn] Merging 'javax/servlet/resources/XMLSchema.dtd' with strategy 'first'
[warn] Merging 'org/objenesis/instantiator/ObjectInstantiator.class' with
strategy 'first'
 [warn] Merging 'javax/servlet/ServletRequestWrapper.class' with strategy
'first'
[warn] Merging
'org/apache/commons/beanutils/converters/IntegerConverter.class' with
strategy 'first'
[warn] Merging 'org/objenesis/Objenesis.class' with strategy 'first'
[warn] Merging 'javax/servlet/ServletRequestListener.class' with strategy
'first'
[warn] Merging 'javax/servlet/http/HttpSessionEvent.class' with strategy
'first'
[warn] Merging
'org/objenesis/instantiator/sun/SunReflectionFactoryInstantiator.class'
with strategy 'first'
[warn] Merging 'javax/servlet/ServletResponse.class' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/IntegerArrayConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/SqlDateLocaleConverter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/http/HttpSessionBindingListener.class' with
strategy 'first'
[warn] Merging 'org/objenesis/instantiator/NullInstantiator.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/DynaClass.class' with strategy
'first'
[warn] Merging 'META-INF/NOTICE.txt' with strategy 'first'
[warn] Merging 'com/esotericsoftware/minlog/Log.class' with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/MethodUtils.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/IntegerLocaleConverter.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/SqlTimeLocaleConverter.class'
with strategy 'first'
[warn] Merging 'org/objenesis/instantiator/gcj/GCJInstantiatorBase.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/ShortConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/LocaleBeanUtilsBean$Descriptor.class'
with strategy 'first'
[warn] Merging
'org/objenesis/strategy/SerializingInstantiatorStrategy.class' with
strategy 'first'
[warn] Merging 'org/objenesis/strategy/StdInstantiatorStrategy.class' with
strategy 'first'
[warn] Merging 'javax/servlet/ServletContext.class' with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/Converter.class' with strategy
'first'
[warn] Merging 'org/apache/commons/beanutils/DynaProperty.class' with
strategy 'first'
[warn] Merging
'org/objenesis/instantiator/sun/Sun13SerializationInstantiator.class' with
strategy 'first'
[warn] Merging 'javax/servlet/http/HttpSessionListener.class' with strategy
'first'
[warn] Merging
'org/objenesis/instantiator/basic/ObjectInputStreamInstantiator$MockStream.class'
with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/ConstructorUtils.class' with
strategy 'first'
[warn] Merging 'javax/servlet/http/HttpServletRequest.class' with strategy
'first'
[warn] Merging 'com/esotericsoftware/reflectasm/AccessClassLoader.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/ClassConverter.class' with
strategy 'first'
[warn] Merging 'javax/servlet/ServletContextListener.class' with strategy
'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Opcodes.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/BeanAccessLanguageException.class' with
strategy 'first'
[warn] Merging
'org/objenesis/instantiator/gcj/GCJInstantiatorBase$DummyStream.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/CharacterArrayConverter.class'
with strategy 'first'
[warn] Merging
'org/objenesis/instantiator/jrockit/JRockitLegacyInstantiator.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/LocaleConvertUtils.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/MappedPropertyDescriptor.class' with strategy
'first'
[warn] Merging 'javax/servlet/ServletResponseWrapper.class' with strategy
'first'
[warn] Merging
'org/objenesis/instantiator/gcj/GCJSerializationInstantiator.class' with
strategy 'first'
[warn] Merging 'javax/servlet/http/NoBodyOutputStream.class' with strategy
'first'
[warn] Merging 'javax/servlet/FilterConfig.class' with strategy 'first'
[warn] Merging 'javax/servlet/http/HttpSessionActivationListener.class'
with strategy 'first'
[warn] Merging
'org/objenesis/instantiator/perc/PercSerializationInstantiator.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/ConvertUtils.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/BeanUtils.class' with strategy
'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Frame.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/JDBCDynaClass.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/FileConverter.class' with strategy
'first'
[warn] Merging 'javax/servlet/UnavailableException.class' with strategy
'first'
[warn] Merging 'META-INF/NOTICE' with strategy 'first'
[warn] Merging
'org/objenesis/instantiator/basic/ObjectInputStreamInstantiator.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/DynaBean.class' with strategy
'first'
[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'
[warn] Merging 'javax/servlet/resources/j2ee_1_4.xsd' with strategy 'first'
[warn] Merging 'javax/servlet/resources/web-app_2_3.dtd' with strategy
'first'
[warn] Merging 'org/apache/commons/beanutils/LazyDynaMap.class' with
strategy 'first'
[warn] Merging 'javax/servlet/http/HttpSession.class' with strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/FieldWriter.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/BigDecimalLocaleConverter.class'
with strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Type.class' with
strategy 'first'
[warn] Merging 'javax/servlet/ServletContextEvent.class' with strategy
'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/StringLocaleConverter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/ServletInputStream.class' with strategy
'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Item.class' with
strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/AnnotationWriter.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/DateLocaleConverter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/http/HttpServletResponse.class' with strategy
'first'
[warn] Merging 'javax/servlet/ServletRequestEvent.class' with strategy
'first'
[warn] Merging 'org/apache/commons/beanutils/MutableDynaClass.class' with
strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Edge.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/BigDecimalConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/StringConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/ByteLocaleConverter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/http/Cookie.class' with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/RowSetDynaClass.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/SqlTimestampLocaleConverter.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/BigIntegerConverter.class' with
strategy 'first'
[warn] Merging 'javax/servlet/Servlet.class' with strategy 'first'
[warn] Merging 'org/apache/commons/collections/FastHashMap$KeySet.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/LocaleBeanUtilsBean$1.class' with
strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/MethodWriter.class'
with strategy 'first'
[warn] Merging 'org/objenesis/ObjenesisHelper.class' with strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Handler.class'
with strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Handle.class'
with strategy 'first'
[warn] Merging 'org/objenesis/instantiator/perc/PercInstantiator.class'
with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/WrapDynaClass.class' with
strategy 'first'
[warn] Merging 'javax/servlet/resources/web-app_2_5.xsd' with strategy
'first'
[warn] Merging 'org/objenesis/instantiator/sun/Sun13InstantiatorBase.class'
with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/NestedNullException.class'
with strategy 'first'
[warn] Merging 'javax/servlet/LocalStrings.properties' with strategy 'first'
[warn] Merging 'javax/servlet/http/LocalStrings.properties' with strategy
'first'
[warn] Merging
'org/apache/commons/collections/FastHashMap$CollectionView$CollectionViewIterator.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/SqlTimestampConverter.class' with
strategy 'first'
[warn] Merging 'com/esotericsoftware/reflectasm/FieldAccess.class' with
strategy 'first'
[warn] Merging 'META-INF/DEPENDENCIES' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/ByteArrayConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/StringArrayConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/DoubleConverter.class' with
strategy 'first'
[warn] Merging 'javax/servlet/http/HttpServletResponseWrapper.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/MethodUtils$MethodDescriptor.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/collections/FastHashMap.class' with
strategy 'first'
[warn] Merging 'javax/servlet/ServletOutputStream.class' with strategy
'first'
[warn] Merging
'org/apache/commons/beanutils/converters/AbstractArrayConverter.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/WrapDynaBean.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/DoubleArrayConverter.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/ShortLocaleConverter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/resources/jsp_2_0.xsd' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/ShortArrayConverter.class' with
strategy 'first'
[warn] Merging 'org/objenesis/ObjenesisBase.class' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/BooleanConverter.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/ContextClassLoaderLocal.class'
with strategy 'first'
[warn] Merging 'javax/servlet/ServletRequest.class' with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/ConversionException.class'
with strategy 'first'
[warn] Merging 'javax/servlet/resources/j2ee_web_services_1_1.xsd' with
strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/Attribute.class'
with strategy 'first'
[warn] Merging 'log4j.properties' with strategy 'discard'
[warn] Merging 'META-INF/ECLIPSEF.SF' with strategy 'discard'
[warn] Merging 'javax/servlet/Filter.class' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/LongConverter.class' with strategy
'first'
[warn] Merging
'org/apache/commons/beanutils/converters/CharacterConverter.class' with
strategy 'first'
[warn] Merging 'org/objenesis/ObjenesisStd.class' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/SqlDateConverter.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/BasicDynaClass.class' with
strategy 'first'
[warn] Merging 'javax/servlet/ServletRequestAttributeEvent.class' with
strategy 'first'
[warn] Merging 'javax/servlet/http/HttpUtils.class' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/LocaleBeanUtilsBean.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/ResultSetDynaClass.class' with
strategy 'first'
[warn] Merging 'com/esotericsoftware/reflectasm/ConstructorAccess.class'
with strategy 'first'
[warn] Merging 'org/objenesis/strategy/InstantiatorStrategy.class' with
strategy 'first'
[warn] Merging 'META-INF/INDEX.LIST' with strategy 'first'
[warn] Merging 'com/esotericsoftware/reflectasm/MethodAccess.class' with
strategy 'first'
[warn] Merging 'javax/servlet/resources/web-app_2_4.xsd' with strategy
'first'
[warn] Merging 'org/apache/commons/beanutils/BeanUtilsBean$1.class' with
strategy 'first'
[warn] Merging 'reference.conf' with strategy 'concat'
[warn] Merging 'org/objenesis/instantiator/sun/Sun13Instantiator.class'
with strategy 'first'
[warn] Merging
'org/objenesis/instantiator/SerializationInstantiatorHelper.class' with
strategy 'first'
[warn] Merging 'about.html' with strategy 'first'
[warn] Merging 'javax/servlet/ServletConfig.class' with strategy 'first'
[warn] Merging 'org/objenesis/strategy/BaseInstantiatorStrategy.class' with
strategy 'first'
[warn] Merging 'javax/servlet/ServletException.class' with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/PropertyUtils.class' with
strategy 'first'
[warn] Merging 'javax/servlet/http/HttpSessionContext.class' with strategy
'first'
[warn] Merging 'META-INF/LICENSE.txt' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/BooleanArrayConverter.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/BeanUtilsBean.class' with
strategy 'first'
[warn] Merging 'javax/servlet/resources/j2ee_web_services_client_1_1.xsd'
with strategy 'first'
[warn] Merging 'org/apache/commons/collections/FastHashMap$Values.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/SqlTimeConverter.class' with
strategy 'first'
[warn] Merging 'javax/servlet/GenericServlet.class' with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/LocaleBeanUtils$Descriptor.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/LazyDynaBean.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/ConvertUtilsBean.class' with
strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassWriter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/resources/datatypes.dtd' with strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/converters/URLConverter.class'
with strategy 'first'
[warn] Merging 'javax/servlet/http/HttpServletRequestWrapper.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/FloatLocaleConverter.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/LongArrayConverter.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/collections/ArrayStack.class' with
strategy 'first'
[warn] Merging
'com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ByteVector.class'
with strategy 'first'
[warn] Merging 'javax/servlet/http/HttpSessionAttributeListener.class' with
strategy 'first'
[warn] Merging 'org/apache/commons/beanutils/BasicDynaBean.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/collections/FastHashMap$CollectionView.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/LocaleConvertUtilsBean.class' with
strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/locale/converters/LongLocaleConverter.class'
with strategy 'first'
[warn] Merging
'org/apache/commons/beanutils/converters/ByteConverter.class' with strategy
'first'
[warn] Merging
'org/objenesis/instantiat"
,,,,
"
Matei Zaharia <matei.zaharia@gmail.com>,Sun"," 8 Dec 2013 16:52:49 -0800""",Re: [DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),dev@spark.incubator.apache.org,"I agree that minor releases should be binary-compatible for all public APIs, and I think thats a good goal for future ones. In fact our releases have always provided full compatibility for external APIs, just not for internal ones that you might use for defining a new RDD, new transformations, etc. However, it seems that more people want those directly, so thats a good goal to aim for.

In this case we pushed in more features than usual because this was the last branch on Scala 2.9, and there were some pretty key features (YARN 2.2 compatibility, standalone mode HA) that we thought 2.9 users would want.

Something else well probably do is mark more internal, yet useful-to-extend, APIs through an annotation. Im talking about things like writing a custom RDD or SparkListener. These may change in major versions, but at least youll be able to expect that maintenance releases in the original branch dont break them.

Matei


merged per
more
development/versioning/release
them,
almost no
that
users
least
from
we
the
validate
<mark@clearstorydata.com>
issue
went
last
taking
opinion
1.0
have;
that
to be
what
those
mean
eventual
demonstrating the
that
getting
to
""changes
wants),
prior
the
point
<henry.saputra@gmail.com
trying to
file?
since
in
an
https://github.com/apache/incubator-spark/blob/branch-0.8/CHANGES.txt
<rosenville@gmail.com>
into
requests
this
only
JoshRosen/pyspark-custom-serializers


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 16:58:51 -0800",Re: [DISCUSS] About the [VOTE] Release Apache Spark 0.8.1-incubating (rc1),dev@spark.incubator.apache.org,"Now that I can immediately give a +1.


e:

ses
 for
.2
like
,
t
t
m
e
e
ng
n
o
to
es
e
t
e?
ce
n
to
ts
s
y
"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 17:06:07 -0800",Re: Spark Build Issue ('sbt/sbt assembly' hangs),dev@spark.incubator.apache.org,"The assembly ""hang"" is something that I've also noticed over at least the
past few weeks.  If you are seeing what I am seeing, then the build is not
actually hung, but the building of assemblies takes a long time, a very
long time, a very very long time on Macs.  It's just the build of
assemblies via sbt on OSX that does this -- maven builds on Mac or any kind
do, I've seen the sbt assembly packaging take upwards of an hour.  Not good.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 17:28:37 -0800",Re: Spark Build Issue ('sbt/sbt assembly' hangs),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Taka,

Most likely this is just the assembly task taking a long time as mark
said. What happens if you run 'package' or 'compile'? It could be that
on Mac's there is something where this is slow... I'm not sure.

Also, unless you specify the hadoop version in the build it will build
Hadoop 1.0.4. You keep mentioning Hadoop 2.2.0, but you didn't specify
that when you built so Spark has no way of knowing what you want.

Checkout the README for documentation on how to do that.

Also - does this all work well in the 0.8.0 release? If this is not
something specific to the 0.8.1 release than it would be good to know.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 17:29:01 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","For my own part I'll give a +1 to this RC.


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 18:05:09 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"SPARK-962 should be resolved before release.  See also:
https://github.com/apache/incubator-spark/pull/195

With the references to the way I changed Debian packaging for ClearStory,
we should be at least 90% of the way toward doing it right for Apache.



"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 18:07:20 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"Probably not blockers, but there are still some non-deterministic test
failures -- e.g. streaming CheckpointSuite.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 19:30:19 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark - ya this would be good to get in.

Does merging that particular PR put this in sufficient shape for the
0.8.1 release or are there other open patches we need to look at?

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 19:46:37 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Looked into this a bit more - I think removing repl-bin is something
we should wait until 0.9 to do, because we've published it to maven in
0.8.0 and people might expect it to be there in 0.8.1.

Merging the directly referenced pull request (195) seems like a good
idea though since it fixes a bug in the script.

Is that what you are suggesting?

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 19:54:06 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"Whatever Debian package gets built has to work, so that's the first
requirement.  I don't know how to decide whether a change is acceptable in
0.8 or has to wait until 0.9, but the 0.9 packaging should definitely
leverage the assembly sub-project, making repl-bin unnecessary.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 20:02:41 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark,

What I'm asking is whether this patch is sufficient to have a working
debian build in 0.8.1, or are there other outstanding issues to make
it work? By working I mean, within the initial design that was
contributed (with repl-bin) it works according to that approach.

We can redesign this packaging in 0.9. That will require having a PR
against Apache Spark, discussing, etc. But it doesn't need to be on
the critical path for this release.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 20:07:17 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"Well, 195 is sufficient to give you something that runs, but it doesn't run
the same way as Spark built/distributed by other means -- e.g., after 195
the package still uses something equivalent to the old `run` script instead
of the current `spark-class` way.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 20:25:18 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark,

Okay if 195 gets this in working order in the branch 0.8 let's just
merge that to keep it consistent with our docs and the way this is
done in 0.8.0

We can do a broader refactoring in 0.9. Would be great if you could
kick off a JIRA discussion or submit a PR relating to that.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 20:41:23 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"Well, what I've already done for ClearStory is very close to how Debian
packaging should be done for Apache Spark.  That much can be put into a
pull request quickly.  The only real issues are exactly how the packages
should be named, checking that the metadata of the packages are exactly
correct for an Apache release, and deciding whether we should be producing
a spark-examples package, spark-tools package, separate the Java and Python
APIs into their own packages, create a source package, etc.  What we've
been doing up to now is essentially just the minimal packaging of a fat jar
that can be deployed by Chef or something similar.  It's never really been
put together in a way appropriate to go into a Debian or Ubuntu
distribution, for instance.



"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 8 Dec 2013 20:42:18 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),dev@spark.incubator.apache.org,"And I think 195 is sufficient to build something that works; but I haven't
personally tested it.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 21:13:12 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Okay Mark thanks for bringing this up, I'm going to cut a new RC with this fix.


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 22:30:23 -0800",[VOTE] Release Apache Spark 0.8.1-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.1.

The tag to be voted on is v0.8.1-incubating (commit c88a9916):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=ba05afd29c81e152a84461f95b0e61a783897d7a

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-025/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc3-docs/

For information about the contents of this release see:
<attached> draft of release notes
<attached> draft of release credits
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8

Please vote on releasing this package as Apache Spark 0.8.1-incubating!

The vote is open until Thursday, December 12th at 06:30 UTC and
passes if a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.1-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Michael Armbrust -- build fix

Pierre Borckmans -- typo fix in documentation

Evan Chan -- added `local://` scheme for dependency jars

Ewen Cheslack-Postava -- `add` method for python accumulators, support for setting config properties in python

Mosharaf Chowdhury -- optimized broadcast implementation

Frank Dai -- documentation fix

Aaron Davidson -- lead on shuffle file consolidation, lead on h/a mode for standalone scheduler, cleaned up representation of block id’s, several small improvements and bug fixes

Tathagata Das -- new streaming operators: `transformWith`, `leftInnerJoin`, and `rightOuterJoin`, fix for kafka concurrency bug

Ankur Dave -- support for pausing spot clusters on EC2

Harvey Feng -- optimization to JobConf broadcasts, minor fixes, lead on YARN 2.2 build

Ali Ghodsi -- scheduler support for SIMR, lead on YARN 2.2 build

Thomas Graves -- lead on Spark YARN integration including secure HDFS access over YARN

Li Guoqiang -- fix for maven build

Stephen Haberman -- bug fix

Haidar Hadi -- documentation fix

Nathan Howell -- bug fix relating to YARN

Holden Karau -- java version of `mapPartitionsWithIndex`

Du Li -- bug fix in make-distrubion.sh

Xi Lui -- bug fix and code clean-up

David McCauley -- bug fix in standalone mode JSON output

Michael (wannabeast) -- bug fix in memory store

Fabrizio Milo -- typos in documentation, minor clean-up in DAGScheduler, typo in scaladoc

Mridul Muralidharan -- fixes to meta-data cleaner and speculative scheduler

Sundeep Narravula -- build fix, bug fixes in scheduler and tests, minor code clean-up

Kay Ousterhout -- optimization to task result fetching, extensive code clean-up and refactoring (task schedulers, thread pools), result-fetching state in UI, showing task and attempt it in UI, several bug fixes in scheduler, UI, and unit tests

Nick Pentreath -- implicit feedback variant of ALS algorithm

Imran Rashid -- small improvement to executor launch

Ahir Reddy -- spark support for SIMR

Josh Rosen -- reduced memory overhead for BlockInfo objects, clean up of BlockManager code, fix to java API auditor, code clean-up in java API, and bug fixes in python API

Henry Saputra -- build fix

Jerry Shao -- refactoring of fair scheduler, support for running spark as a specific user, bug fix

Mingfei Shi -- documentation for JobLogger

Andre Schumacher -- sortByKey in pyspark and associated changes

Karthik Tunga -- bug fix in launch script

Patrick Wendell -- added `repartition` operator, logging improvements, instrumentation for shuffle write, documentation improvements, fix for streaming example, and release management

Neal Wiggins -- minor import clean-up, documentation typo

Andrew Xia -- bug fix in UI

Reynold Xin -- optimized hash set and hash tables for primitive types, task killing, support for setting job properties in repl, logging improvements, Kryo improvements, several bug fixes, and general clean-up

Matei Zaharia -- optimized hashmap for shuffle data, pyspark documentation, optimizations to kryo and chill serializers

Wu Zeming -- bug fix in executors UI
DRAFT OF RELEASE NOTES FOR SPARK 0.8.1

Apache Spark 0.8.1 is a maintenance release including several bug fixes and performance optimizations. It also includes a few new features. Contributions to 0.8.1 came from 40 developers.

== High availability mode for standalone scheduler ==
The standalone scheduler now has a High Availability (H/A) mode which can tolerate master failures. This is particularly useful for long-running applications such as streaming jobs and the shark server, where the scheduler master previous represented a single point of failure. Instructions for deploying H/A mode are included in the documentation. The current implementation uses Zookeeper for coordination.

== YARN 2.2 support ==
Support has been added for submitting Spark applications to YARN 2.2 and newer. Due to a dependency conflict, this did not work properly in Spark 0.8.0 and earlier. See the release documentation for specific instructions on how to build Spark for YARN 2.2+.

== Internal Optimizations ==
This release adds several performance optimizations:
  - Append only map for shuffle - an internal hashmap optimized for storing shuffle data
  - Efficient encoding for Job confs - improves latency for stages reading large numbers of blocks from HDFS, S3, and HBase
  - Shuffle file consolidation (off by default) - reduces the number of files created in large shuffles for better filesystem performance. We recommend users turn this on unless they are using ext3.
  - Torrent broadcast (off by default) - reduces network overhead and latency of broadcasting large objects.
  - Support for fetching large result sets - allows tasks to return large results without tuning akka buffer sizes.

== Python improvements == 
  - new `add` method for accumulators
  - it is now possible to set config properties directly from python
  - python now supports sorted RDD’s

== New operators and usability improvements == 
- local:// URI’s - allows users to specify already present on slaves as dependencies
- a new “result fetching” state has been added to the UI
- new spark streaming operators: transformWith, leftInnerJoin, rightOuterJoin
- new spark operators: repartition

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 8 Dec 2013 22:30:50 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm cancelling this vote in favor of RC3.


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 9 Dec 2013 00:05:43 -0800",Re: Spark streaming quantile?,dev@spark.incubator.apache.org,"Thanks all for the suggestions.  Exactly what I was looking for.

-Sandy



"
Taka Shinagawa <taka.epsilon@gmail.com>,"Mon, 9 Dec 2013 01:12:03 -0800",Re: Spark Build Issue ('sbt/sbt assembly' hangs),dev@spark.incubator.apache.org,"Seems like I'm having the same issue as Mark. It's not about Hadoop
versions-- sorry about the confusion. It turns out that the build process
is not hanging, although CPU usage goes below 1%.

After waiting for more than 2 hours, the sbt assembly command finished
successfully on the Macbook Pro (with 2.4GHz Intel Core 2 Duo & 8GB RAM).
with sbt always completes in about 4 minutes. So I never expected the sbt
assembly takes this much longer on the other Macbook. I also ran the same
sbt command on Ubuntu 12.04 LTS on VirtualBox, it took more than one hour
and a half. I'm seeing this problem on both Mac and Linux as well as with
0.8.0 release.

The solution seems to upgrade sbt-assembly to version 0.10.1 (and sbt to
0.13.0). But sbt 0.13 requires sbt-dependency-graph 0.7.4 (from 0.7.3),
which depends on Scala 2.10. So I've learned that we can't upgrade them for
this release.

Thanks for the help!







"
=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"Mon, 9 Dec 2013 10:43:46 +0100",Re: spark.task.maxFailures,dev@spark.incubator.apache.org,"Hi Reynold,

I submitted a pull request here -
https://github.com/apache/incubator-spark/pull/245
Do I need to do anything else (perhaps add a ticket in JIRA)?

Best,
Grega
--
[image: Inline image 1]
*Grega Kešpret*
Analytics engineer

Celtra — Rich Media Mobile Advertising
celtra.com <http://www.celtra.com/> |
@celtramobile<http://www.twitter.com/celtramobile>



ote:
be
1.
/org/apache/spark/scheduler/cluster/ClusterTaskSetManager.scala#L532
er
(
"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 9 Dec 2013 15:14:17 +0530",Re: Spark Build Issue ('sbt/sbt assembly' hangs),dev@spark.incubator.apache.org,"I have generally observed if I clean assembly before building it, it works
fine and also building assembly/assembly and example/assembly separately
helps.

sbt/sbt

or
shorter version
$ rm -r assembly/target
$ sbt/sbt assembly/assembly

and then if you want you can build example assembly separately with sbt/sbt
example/assembly .

HTH






-- 
s
"
=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"Mon, 9 Dec 2013 16:35:01 +0100",Re: spark.task.maxFailures,dev@spark.incubator.apache.org,"Hi!

I tried this (by setting spark.task.maxFailures to 1) and it still does not
fail-fast. I started a job and after some time, I killed all JVMs running
on one of the two workers. I was expecting Spark job to fail, however it
re-fetched tasks to one of the two workers that was still alive and the job
succeeded.

Grega
--
[image: Inline image 1]
*Grega Kešpret*
Analytics engineer

Celtra — Rich Media Mobile Advertising
celtra.com <http://www.celtra.com/> |
@celtramobile<http://www.twitter.com/celtramobile>


e:

m/celtramobile>
rote:
 1.
a/org/apache/spark/scheduler/cluster/ClusterTaskSetManager.scala#L532
 (
"
=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"Mon, 9 Dec 2013 17:18:27 +0100",Re: spark.task.maxFailures,dev@spark.incubator.apache.org,"I see that it is the DAGScheduler that orchestrates task resubmission. This
code<https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L625-L633>
is
responsible for calling submitStage for any failed stages. How does
spark.task.maxFailures affect this (if at all) ?

Log on driver:
https://gist.github.com/gregakespret/7874908#file-gistfile1-txt-L1045-L1062(lines
where I killed JVM worker are selected)


Grega
--
[image: Inline image 1]
*Grega Kešpret*
Analytics engineer

Celtra — Rich Media Mobile Advertising
celtra.com <http://www.celtra.com/> |
@celtramobile<http://www.twitter.com/celtramobile>


:

e
m/celtramobile>
ote:
om/celtramobile>
d
- 1.
la/org/apache/spark/scheduler/cluster/ClusterTaskSetManager.scala#L532
f
"
Deenar Toraskar <deenar.toraskar@db.com>,"Mon, 9 Dec 2013 14:46:13 +0100",Spark API - support for asynchronous calls - Reactive style [I],"""'dev@spark.incubator.apache.org'"" <dev@spark.incubator.apache.org>","Classification: For internal use only
Hi developers

Are there any plans to have Spark (and Shark) APIs that are asynchronous and non blocking? APIs that return Futures and Iteratee/Enumerators would be very useful to users building scalable apps using Spark, specially when combined with a fully asynchronous/non-blocking framework like Play!.

Something along the lines of ReactiveMongo
http://stephane.godbillon.com/2012/08/30/reactivemongo-for-scala-unleashing-mongodb-streaming-capabilities-for-realtime-web


Deenar

---
This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received this e-mail in error) please notify the sender immediately and delete this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden.

Please refer to http://www.db.com/en/content/eu_disclosures.htm for additional EU corporate and regulatory disclosures.
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 9 Dec 2013 09:31:33 -0800",Re: Spark API - support for asynchronous calls - Reactive style [I],dev@spark.incubator.apache.org,"Spark has already supported async jobs for awhile now --
https://github.com/apache/incubator-spark/pull/29, and they even work
correctly after https://github.com/apache/incubator-spark/pull/232

There are now implicit conversions from RDD to
AsyncRDDActions<https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala>,
where async actions like countAsync are defined.



"
Michael Malak <michaelmalak@yahoo.com>,"Mon, 9 Dec 2013 09:37:31 -0800 (PST)",Re: Kafka not shutting down cleanly; Actor serializtion?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I haven't seen a response to my September question and was wondering if anyone had any insights into the problem I was having cleanly shutting down Kafka.

Note: For the purposes of the Apache 2.0 license, regarding the contents of this and my earlier message: THIS IS NOT A CONTRIBUTION.


________________________________
 From: Michael Malak <michaelmalak@yahoo.com>
To: ""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org> 
Sent: Thursday, September 26, 2013 12:27 PM
Subject: Kafka not shutting down cleanly; Actor serializtion?
 


Tathagata:


I don't believe Kafka streams are being shut down cleanly, which implies that the most recent Kafka offsets are not being committed back to Zookeeper, which implies starting/restarting a Spark Streaming process would result in duplicate events.

The simple Spark Streaming code (running in local mode) pasted below at the end of this e-mail, which uses a hard-coded queueStream as its only input stream, exits cleanly when the presence of the sentinel file is detected. However, if the queueStream is replaced with a kafkaStream, the process never exits (unless I put a System.exit() as the very last line -- to forcibly kill all threads).

In attempting to understand the Kafka shutdown process, I traced through the Spark Streaming codebase with println()s. I noticed the following:

1. Although KafkaInputDStream.scala initializes the class member variableconsumerConnector in onStart(), I don't see a corresponding consumerConnector.shutdown() anywhere such as in the onStop(). It is my understanding that it is the consumer shutdown() that commits the offsets back to Zookeeper. See the Kafka example athttps://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example#ConsumerGroupExample-FullSourceCode

2. There is a similar apparent asymmetry with executorPool, where it is not released in the onStop(). (A further minor encumbrance is that it is a variable local to onStart() rather than being a class member variable)

3. Through my println() tracing and Akka debug-level logging, I'm not seeing NetworkReceiverActor ever receiving a StopReceiver message from ReceiverExecutor. From some poking around and testing, it seems possible to successfully send any type of message to NetworkReceiverActor only prior to that NetowrkReceiverActor being serialized into an RDD on line 146 of NetworkInputTracker.scala
https://github.com/mesos/spark/blob/branch-0.8/streaming/src/main/scala/org/apache/spark/streaming/NetworkInputTracker.scala#L146

Prior to the actor being put into the RDD, messages can be sent to the actor, but not after the actor is put into the RDD. Is it possible that Akka actors are intolerant of being serialized?

4. I noticed a lot of ""TODO"" comments sprinkled throughout the code relating to shutdown/termination/cleanup.

My biggest concern is #3 above, because if my suppositions are correct, then there might be some major re-architecting involved. The other issues I could probably fix on my own and commit back.


import spark.streaming._

object SimpleSparkStreaming {
 @volatile var receivedStop = false
 val sentinelFile = new java.io.File(""/home/mmalak/stop"")

 def main(args: Array[String]) {
  val Array(master, zkQuorum, broker, group, topics, numThreads) = args

  sentinelFile.delete

  val ssc = new StreamingContext(master, ""SimpleBeta"", Seconds(1), System.getenv(""SPARK_HOME""), Seq(""./target/scala-2.9.2/my.jar""))
  ssc.checkpoint(""/home/mmalak/checkpointing"")

  ssc.queueStream(new scala.collection.mutable.Queue[spark.RDD[Int]] += ssc.sparkContext.makeRDD(List(1))).foreach(rdd =>
   println(""receivedStop["" + receivedStop + ""]""))
  ssc.start()

  while (!sentinelFile.exists) {Thread.sleep(1000)}
  println(""Stop detected"")
  receivedStop = true
  ssc.stop()
  println(""Exiting main"")
 }
}"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Dec 2013 11:22:05 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll go ahead and kick this off with a +1.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Dec 2013 22:40:55 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm going to -1 this now because we had two issues reported today.
They were reported off the list so I'm summarizing here:

(1) Raymond Liu found an issue with the Maven build for YARN 2.2+.
Previously we had only tested the sbt build since this is what we
refer to in the docs, but we'd like to support this for Maven as well.

(2) I noticed we were missing some header files from recent patches.
This will result in a -1 downstream during an IPMC release, so we
should fix it.

- Patrick


"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 9 Dec 2013 22:44:25 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks for catching the problems, Patrick and Raymond.

- Henry


"
Justin Kestelyn <jkestelyn@cloudera.com>,"Tue, 10 Dec 2013 13:14:32 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",horia@alum.berkeley.edu,"Location:

Cloudera
433 California Street
San Francisco, CA





-- 

*Justin Kestelyn*
Developer Outreach
Cloudera | www.cloudera.com

Tel: 650.683.4688
Email: jkestelyn@cloudera.com
Twitter: @kestelyn, @ClouderaEng
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 10 Dec 2013 16:49:09 -0800",[VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.8.1.

The tag to be voted on is v0.8.1-incubating (commit b87d31d):
https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-040/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4-docs/

For information about the contents of this release see:
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8

Please vote on releasing this package as Apache Spark 0.8.1-incubating!

The vote is open until Saturday, December 14th at 01:00 UTC and
passes if a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.8.1-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/

"
Konstantin Boudnik <cos@apache.org>,"Tue, 10 Dec 2013 19:49:17 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc3),dev@spark.incubator.apache.org,"
Is there a ticket on the Maven issue? As I was the one who put it in place I
suppose it would be easier for me to fix it quickky.

Cos

te:
rote:
g;h=ba05afd29c81e152a84461f95b0e61a783897d7a
ob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
"
"""Liu, Raymond"" <raymond.liu@intel.com>","Wed, 11 Dec 2013 03:54:53 +0000",RE: [VOTE] Release Apache Spark 0.8.1-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Konstantin

 This one have been fixed. https://github.com/apache/incubator-spark/pull/248
 Though I am not sure whether there are better solution.

Best Regards,
Raymond Liu


-----Original Message-----
From: Konstantin Boudnik [mailto:cos@apache.org] 
Sent: Wednesday, December 11, 2013 11:49 AM
To: dev@spark.incubator.apache.org
Subject: Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc3)

On Mon, Dec 09, 2013 at 10:40PM, Patrick Wendell wrote:
> I'm going to -1 this now because we had two issues reported today.
> They were reported off the list so I'm summarizing here:
> 
> (1) Raymond Liu found an issue with the Maven build for YARN 2.2+.
> Previously we had only tested the sbt build since this is what we 
> refer to in the docs, but we'd like to support this for Maven as well.

Is there a ticket on the Maven issue? As I was the one who put it in place I suppose it would be easier for me to fix it quickky.

Cos

> (2) I noticed we were missing some header files from recent patches.
> This will result in a -1 downstream during an IPMC release, so we 
> should fix it.
> 
> - Patrick
> 
> On Mon, Dec 9, 2013 at 11:22 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> > I'll go ahead and kick this off with a +1.
> >
> > On Sun, Dec 8, 2013 at 10:30 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> >> Please vote on releasing the following candidate as Apache Spark
> >> (incubating) version 0.8.1.
> >>
> >> The tag to be voted on is v0.8.1-incubating (commit c88a9916):
> >> https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag
> >> ;h=ba05afd29c81e152a84461f95b0e61a783897d7a
> >>
> >> The release files, including signatures, digests, etc can be found at:
> >> http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc3/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-0
> >> 25/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc3-docs/
> >>
> >> For information about the contents of this release see:
> >> <attached> draft of release notes
> >> <attached> draft of release credits
> >> https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blo
> >> b;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branc
> >> h-0.8
> >>
> >> Please vote on releasing this package as Apache Spark 0.8.1-incubating!
> >>
> >> The vote is open until Thursday, December 12th at 06:30 UTC and 
> >> passes if a majority of at least 3 +1 PPMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 0.8.1-incubating [ ] -1 
> >> Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see 
> >> http://spark.incubator.apache.org/
"
Konstantin Boudnik <cos@apache.org>,"Tue, 10 Dec 2013 20:18:10 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc3),dev@spark.incubator.apache.org,"I think it's all good - thanks! Sorry for being later to the party.

Cos

/248
e I suppose it would be easier for me to fix it quickky.
rote:
tag
t:
blo
branc
ng!
 
"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 10 Dec 2013 20:24:12 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"+1

Built and tested it on Mac OS X.

Matei



https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CH"
Prashant Sharma <scrapcodes@gmail.com>,"Wed, 11 Dec 2013 13:28:35 +0530",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Hi Patrick and Matei,

Was trying out this and followed the quick start guide which says do
sbt/sbt assembly, like few others I was also stuck for few minutes on
faster.

Should we change the documentation to reflect this. It will not be great
for first time users to get stuck there.





-- 
s
"
Prashant Sharma <scrapcodes@gmail.com>,"Wed, 11 Dec 2013 13:32:05 +0530",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"forgot to mention, after running sbt/sbt assembly/assembly running sbt/sbt
examples/assembly takes just 37s. Not to mention my hardware is not really
great.






-- 
s
"
Taka Shinagawa <taka.epsilon@gmail.com>,"Wed, 11 Dec 2013 00:41:01 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"That's a good point. Although it's definitely not a blocker for this
release, it would be more user friendly to mention sbt/sbt
assembly/assembly as well as Maven build instructions in the README and
quick-start files (at least until the time-consuming sbt assembly process
on a regular hardware gets resolved).

I've been able to build RC4 (against Hadoop 2.2.0) with Maven on my slower
has completed in 4min.

Thanks for fixing the Maven build for Hadoop 2.2.0!



"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 11 Dec 2013 00:44:32 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"a long, long, looooong time to complete (a MBP, in my case), building three
separate assemblies (`./sbt/sbt assembly/assembly`, `./sbt/sbt
examples/assembly`, `./sbt/sbt tools/assembly`) takes much, much less time.




"
Prashant Sharma <scrapcodes@gmail.com>,"Wed, 11 Dec 2013 14:34:09 +0530",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"I hope this PR https://github.com/apache/incubator-spark/pull/252 can help.
Again this is not a blocker for the release from my side either.






-- 
s
"
Olivier Grisel <olivier.grisel@ensta.org>,"Wed, 11 Dec 2013 10:56:40 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",Justin Kestelyn <jkestelyn@cloudera.com>,"2013/12/10 Justin Kestelyn <jkestelyn@cloudera.com>:

Thanks I will add that on the wiki, shall we start at 9am? 9.30am?

-- 
Olivier

"
Horia <horia@alum.berkeley.edu>,"Wed, 11 Dec 2013 08:14:00 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",Olivier Grisel <olivier.grisel@ensta.org>,"If it's up for a vote, I'd say we start at 9:30 :)




"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 11 Dec 2013 18:56:11 +0200",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"   - Successfully built via sbt/sbt assembly/assembly on Mac OS X, as well
   as on a dev Ubuntu EC2 box
   - Successfully tested via sbt/sbt test locally
   - Successfully built and tested using mvn package locally
   - I've tested my own Spark jobs (built against 0.8.0-incubating) on this
   RC and all works fine, as well as tested with my job server (also built
   against 0.8.0-incubating)
   - Ran a few spark examples and the shell and PySpark shell
   - For my part, tested the MLlib implicit code I added, and checked docs


I'm +1



"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 11 Dec 2013 10:39:04 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Woah, weird, but definitely good to know.

If youre doing Spark development, theres also a more convenient option added by Shivaram in the master branch. You can do sbt assemble-deps to package *just* the dependencies of each project in a special assembly JAR, and then use sbt compile to update the code. This will use the classes directly out of the target/scala-2.9.3/classes directories. You have to redo assemble-deps only if your external dependencies change.

Matei


help.
takes
three
time.
<scrapcodes@gmail.com
<scrapcodes@gmail.com
do
on
much
https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
found
https://repository.apache.org/content/repositories/orgapachespark-040/
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4-docs/
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 11 Dec 2013 11:49:34 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll +1 myself also.

For anyone who has the slow build problem: does this issue happen when
building v0.8.0-incubating also? Trying to figure out whether it's
related to something we added in 0.8.1 or if it's a long standing
issue.

- Patrick

rote:
ion added by Shivaram in the master branch. You can do sbt assemble-deps to package *just* the dependencies of each project in a special assembly JAR, and then use sbt compile to update the code. This will use the classes directly out of the target/scala-2.9.3/classes directories. You have to redo assemble-deps only if your external dependencies change.
:
lp.
rote:
kes
hree
ime.
h
tor-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
b;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8

"
Taka Shinagawa <taka.epsilon@gmail.com>,"Wed, 11 Dec 2013 12:23:35 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"v0.8.0-incubating as well.

I'm suspecting the slow build problem is caused by sbt-assembly as reported
here.
https://github.com/sbt/sbt-assembly/issues/68

I tried to upgrade sbt-assembly to version 0.10.1 (and sbt to 0.13.0). But
because of the ""sbt-assembly 0.10.1 ==> sbt 0.13 ==> sbt-dependency-graph
0.7.4 ==> Scala 2.10"" dependency, I wasn't able to use sbt-assembly 0.10.1
for this release.

According to this thread ( https://github.com/sbt/sbt-assembly/issues/96 ),
with sbt-assembly 0.9.2, setting ""assemblyCacheOutput in assembly :=
false"" might
fix. I haven't verified, though.



:

ption
,
o
on
r-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
d
f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
"
Tom Graves <tgraves_cs@yahoo.com>,"Wed, 11 Dec 2013 12:49:08 -0800 (PST)",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey folks,

I'm trying to verify the signature on the rc4 but am getting a BAD signature, is it working for others?  Perhaps I messed up the import of the asc file.

$ gpg --verify spark-0.8.1-incubating.tgz.asc spark-0.8.1-incubating.tgz
gpg: Signature made Tue 10 Dec 2013 10:53:15 PM UTC using RSA key ID 9E4FE3AF
gpg: BAD signature from ""Patrick Wendell so.

For anyone who has the slow build problem: does this issue happen when
building v0.8.0-incubating also? Trying to figure out whether it's
related to something we added in 0.8.1
 or if it's a long standing
iknow.
>
> If you’re doing Spark development, there’s also a more convenient option added by Shivaram in the master branch. You can do sbt assemble-deps to package *just* the dependencies of each project in a special assembly JAR, and then use sbt compile to update the code. This will use the classes directly out of the target/scala-2.9.3/classes directories. You have to redo assemble-deps only if your external dependencies chae/incubator-spark/pull/252 can help.
>> Again this is not a blocker for tlong, looooong time to complete (a MBP, in
 my case), building three
>>ples/assembly`, `./sbt/sbt tools/assembly`) takes much, much less time.
>ng sbt/sbt assembly/assembly running
>>> sbt/sbt
>>>> examples/assembly takes just 37s. Not to mention my hardware is not
>>> really
>>>> greaays do
>>>>> sbt/sbt assembly, like few others I was also stuck for few mly it is much
>>>>> faster.
>>>>>
>>>>> Should we change the documentation to reflect this. It will not be
>>> great
>>>>> for first time
ote on releasing the following candidate as Apache Spark
>>>>>>> (incubating) version 0.8.1.
>>>>>>>
>>>>>>> The tag to be voted on is v0.8.1-incubating (commit b87d31d):
>>>>>>>
>>>>>>
>>>>
>>> https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
>>>>>>>
>>>>>>> The
 release files, including signatures, digests, etc can be found
>>> at:
>>>>>>> http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/
>>>>>>>
>>>>>>> Release artifacts are signed with the following key:
>>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>>>
>>>>>>> The staging repository for this release can be found at:
>>>>>-040/
>>>>>>>
>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>> http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4-docs/
>>>>>>>
>>>>>>> For information about the contents of this release see:
>>>>>>>
>>>>>>
>>>>
>>> https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
>>>>>>>
>>>>>>> Please vote on releasing this package as Apache Spark
>>>> 0.8.1-incubating!
>>>>>>>
>>>>>>> The vote is open until Saturday, December 14th at 01:00 UTC and
>>>>>>> passes if a majority of at least 3 +1 PPMC votes are cast.
>>>>>>>
>>>>>>> [ ] +1 Release this package as Apache Spark 0.8.1-incubating
>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>//spark.incubator.apache.org/
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> s
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> s
>>>>
>>>
>"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 11 Dec 2013 13:10:34 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"I don't know how to make sense of the numbers, but here's what I've got
from a very small sample size.

For both v0.8.0-incubating and v0.8.1-incubating, building separate
assemblies is faster than `./sbt/sbt assembly` and the times for building
separate assemblies for 0.8.0 and 0.8.1 are about the same.

For v0.8.0-incubating, `./sbt/sbt assembly` takes about 2.5x as long as the
sum of the separate assemblies.
For v0.8.1-incubating, `./sbt/sbt assembly` takes almost 8x as long as the
sum of the separate assemblies.

Weird.


:

ption
,
o
on
r-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
d
f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
"
Justin Kestelyn <jkestelyn@cloudera.com>,"Wed, 11 Dec 2013 13:52:08 -0800","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",Horia Airoh <horia@alum.berkeley.edu>,"Please also add to the venue info: The group will be on the 6th floor.





-- 

*Justin Kestelyn*
Developer Outreach & Relations
Cloudera | www.cloudera.com

Tel: 650.683.4688
Email: jkestelyn@cloudera.com
Twitter: @kestelyn, @ClouderaEng
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 11 Dec 2013 15:15:17 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Tom,

I re-verified the signatures and got someone else to do it. It seemed
fine. Here is what I did.

gpg --recv-key 9E4FE3AF
wget http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/spark-0.8.1-incubating.tgz.asc
wget http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/spark-0.8.1-incubating.tgz
gpg --verify spark-0.8.1-incubating.tgz.asc spark-0.8.1-incubating.tgz
gpg: Signature made Tue 10 Dec 2013 02:53:15 PM PST using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""

te:
he
e
te:
option
R,
m
g
s
do
 on
or-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
nd
;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 11 Dec 2013 16:27:16 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I also talked to a few people who got corrupted binaries when
downloading from the people.apache HTTP. In that case the checksum
failed but if they re-downloaded it worked. So maybe just re-download
and try again?

:
0.8.1-incubating.tgz.asc
0.8.1-incubating.tgz
E3AF
rote:
g
the
he
ote:
m>
 option
AR,
n
om
`
ng
ss
t
 do
s on
e
k
tor-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
und
:
b;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Dec 2013 00:21:00 -0800",Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Developers,

In the next few days we are planning to merge Scala 2.10 support into
Spark. For those that haven't been following this, Prashant Sharma has been
maintaining the scala-2.10 branch of Spark for several months. This branch
is current with master and has been reviewed for merging:

https://github.com/apache/incubator-spark/tree/scala-2.10

Scala 2.10 support is one of the most requested features for Spark - it
will be great to get this into Spark 0.9! Please note that *Scala 2.10 is
not binary compatible with Scala 2.9*. With that in mind, I wanted to give
a few heads-up/requests to developers:

If you are developing applications on top of Sparks master branch, those
will need to migrate to Scala 2.10. You may want to download and test the
current scala-2.10 branch in order to make sure you will be okay as Spark
developments move forward. Of course, you can always stick with the current
master commit and be fine (Ill cut a tag when we do the merge in order to
delineate where the version changes). Please open new threads on the dev
list to report and discuss any issues.

This merge will temporarily drop support for YARN 2.2 on the master branch.
This is because the workaround we used was only compiled for Scala 2.9. We
are going to come up with a more robust solution to YARN 2.2 support before
releasing 0.9.

Going forward, we will continue to make maintenance releases on branch-0.8
which will remain compatible with Scala 2.9.

For those interested, the primary code changes in this merge are upgrading
the akka version, changing the use of Scala 2.9s ClassManifest construct
to Scala 2.10s ClassTag, and updating the spark shell to work with Scala
2.10's repl.

- Patrick
"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 12 Dec 2013 13:56:23 +0530",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"+1, built and tested on linux.


:

-incubating.tgz.asc
-incubating.tgz
t
s
n
nt
to
s
to
an
ng
is
r-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
/
/
f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
nd



-- 
s
"
Olivier Grisel <olivier.grisel@ensta.org>,"Thu, 12 Dec 2013 09:38:39 +0100","Re: PySpark / scikit-learn integration sprint at Cloudera - Strata
 Conference Friday 14th Feb 2014",Justin Kestelyn <jkestelyn@cloudera.com>,"Done.

-- 
Olivier

"
"""Liu, Raymond"" <raymond.liu@intel.com>","Thu, 12 Dec 2013 08:42:24 +0000",RE: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick

	What does that means for drop YARN 2.2? seems codes are still there. You mean if build upon 2.2 it will break, and won't and work right? Since the home made akka build on scala 2.10 are not there. While, if for this case, can we just use akka 2.3-M1 which run on protobuf 2.5 for replacement?

Best Regards,
Raymond Liu



In the next few days we are planning to merge Scala 2.10 support into Spark. For those that haven't been following this, Prashant Sharma has been maintaining the scala-2.10 branch of Spark for several months. This branch is current with master and has been reviewed for merging:

https://github.com/apache/incubator-spark/tree/scala-2.10

Scala 2.10 support is one of the most requested features for Spark - it will be great to get this into Spark 0.9! Please note that *Scala 2.10 is not binary compatible with Scala 2.9*. With that in mind, I wanted to give a few heads-up/requests to developers:

If you are developing applications on top of Spark's master branch, those will need to migrate to Scala 2.10. You may want to download and test the current scala-2.10 branch in order to make sure you will be okay as Spark developments move forward. Of course, you can always stick with the current master commit and be fine (I'll cut a tag when we do the merge in order to delineate where the version changes). Please open new threads on the dev list to report and discuss any issues.

This merge will temporarily drop support for YARN 2.2 on the master branch.
This is because the workaround we used was only compiled for Scala 2.9. We are going to come up with a more robust solution to YARN 2.2 support before releasing 0.9.

Going forward, we will continue to make maintenance releases on branch-0.8 which will remain compatible with Scala 2.9.

For those interested, the primary code changes in this merge are upgrading the akka version, changing the use of Scala 2.9's ClassManifest construct to Scala 2.10's ClassTag, and updating the spark shell to work with Scala 2.10's repl.

- Patrick

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Dec 2013 01:12:21 -0800",Re: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Raymond,

This won't work because AFAIK akka 2.3-M1 is not binary compatible with
akka 2.2.3 (right?). For all of the non-yarn 2.2 versions we need to still
use the older protobuf library, so we'd need to support both.

I'd also be concerned about having a reference to a non-released version of
akka. Akka is the source of our hardest-to-find bugs and simultaneously
trying to support 2.2.3 and 2.3-M1 is a bit daunting. Of course, if you are
building off of master you can maintain a fork that uses this.

- Patrick



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Dec 2013 01:13:16 -0800",Re: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Also - the code is still there because of a recent merge that took in some
newer changes... we'll be removing it for the final merge.



"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 12 Dec 2013 16:36:02 +0530",Re: Scala 2.10 Merge,dev@spark.incubator.apache.org,"Hey Raymond,

I just gave what you said a try but have no intentions of maintaining it.
You might want it helpful
https://github.com/ScrapCodes/incubator-spark/tree/yarn-2.2 incase you are
willing to maintain it.

For the record, above branch is updated to master with scala 2.10 and uses
akka 2.3-M1 if one uses new-yarn and uses akka 2.2.3 otherwise.







-- 
s
"
Tom Graves <tgraves_cs@yahoo.com>,"Thu, 12 Dec 2013 05:29:34 -0800 (PST)",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>"," Wednesday, December 11, 2013 6:27 PM, Patrick Wendell <pwendell@gmail.com>
downloading from the people.apache HTTP. In that case the checksum
failed but if they re-downloaded it worked. So maybe just re-download
and trt someone else to do it. It seemed
> fine. Here is what I did.
>
> gpg --recv-key 9E4FE3AF
> wget http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/spark-0.8.1-incubating.tgz.asc
> wget http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/spark-0.8.1-incubating.tgz
> gpg --verify spark-0.8.1-incubating.tgz.asc spark-0.8.1-incubating.tgz
> gpg: Signature made Tue 10 Dec 2013 02:53:15 PM PST using RSA key ID 9E4FE3AF
> gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
>t I've got
>> from a very small sample size.
>>
>> For both v0.8.0-incubating and v0.8.1-incubating, building separate
>> assemblies is faster than `./sbt/sbt assembly` and the times for building
>> separate assemblies for 0.8.0 and 0.8.1 are about the same.
>>
>> For v0.8.0-incubating, `./sbt/sbt assembly` takes about 2.5x as long as the
>> sum of the separate assemblies.
>> For v0.8.1-incubating, `./sbt/sbt assembly` takes almost 8x as long as the
>> sum of the separate assemblies.
>>
>> Weird.o has the slow build problem: does this issue happen when
>>> building v0.8.0-incubating also? Trying to figure out whether it's
>>> related to something we added in 0.8.1 or if it's a long standing
>>> issue.
>>>
>od to know.
>>> >
>>> > If you’re doing Spark development, there’s also a more convenient option
>>> added by Shivaram in the master branch. You can do sbt assemble-deps to
>>> package *just* the dependencies of each project in a special assembly JAR,
>>> and then use sbt compile to update the code. This will use the classes
>>> directly out of the target/scala-2.9.3/classes directories. You have to
>>> redo assemble-deps only if your external dependencies change.
>>> >
>>> > Matei
>>/incubator-spark/pull/252 can
>>> help.
>>> >> Again this is not a bloc Dec 11, 2013 at 2:14 PM, Mark Hamstra <mark@clearstorydata.com
>>> >wrott/sbt assembly`
>>> takes
>>> >>> a long, long, looooong time to complete (a MBP, in my case), building
>>> three
>>> >>> separate assemblies (`./sbt/sbt assembly/assembly`, `./sbt/sbt
>>> >>> examples/assembly`, `./sbt/sbt tools/assembly`) takes much, much less
>>> time.
>>> >>>
>>>to mention, after running sbt/sbt assembly/assembly running
>>> >>> sbt/sbt
>>> >>>> examples/assembly takes just 37s. Not to mention my hardware is not
>>> >>> really
>>> >>>> great.
>>> >>>>
>>> >>>>
>>> >>>> h says do
>>> >>>>> sbt/sbt assembly, like few others I was also stuck fombly/assembly it is
>>> much
>>> >>>>> faster.
>>> >>>>>
>>> >>>>> Should we change the documentation to reflect this. It will not be
>>> >>1
>>> >>>>>>
>>> >>>>>> Built and tested it on Mac OS X.
>>> >>>>>>che Spark
>>> >>>>>>> (incubating) version 0.8.1.
>>> >>>>>>>
>>> >>>/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
>>> >>>>>>>
>>> >>>>>>> The release files, including signatures, digests, etc can be found
>>> >>> at:
>>ey:
>>> >>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>nd at:
>>> >>>>>>>
>>> >>>>
>>> https://repository.apache.org/content/repositories/orgapachespark-040/
>>> >>>>>>>
>>> >>>>>>> The documentation corresponding to this release can be found at:
>>> >>>>>>>
>>> http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4-docs/
>>> >>>>
>>> >>>>>>>
>>> >>>>>>
>>> >>>>
>>> >>>
>>> https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
>>> >>>>>>>
>>> 8.1-incubating!
>>> >>>>>>>
>>> >>>>>>> The vote is open until Saturday, December 14th at 01:00 UTC and
>>> >>>>>>> passes if a majority of at least 3 +1 PPMC votes are cast.
>>> >>>>>>>
>>> >>>>>>> [ ] +1 Release this package as Apache Spark 0.8.1-incubating
>>> >>>>>>> [ ] -1 Do not release this package because ...
>>> >>>>>>>
>>> >>>>>>> To learn more about Apache Spark, please see
>>> >>>>>>> http://spark.incubator.apache.org/
>>> >>>>>>
>>> >>>>>>
>>> >>>>>
>>> >>>>>
>>> >>>>> --
>>> >
>>> >
>>>"
Tom Graves <tgraves_cs@yahoo.com>,"Thu, 12 Dec 2013 07:02:03 -0800 (PST)",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1.

Built spark on yarn for both hadoop 0.23 and hadoop 2.2.0 on redhat linux using maven. Ran some tests on both a secure Hadoop 0.23 cluster and a secure Hadoop 2.2.0 cluster. Verified signatures and md5.
idate as Apache Spark
(incubating) version 0"
Evan Chan <ev@ooyala.com>,"Thu, 12 Dec 2013 08:56:05 -0800",Re: Spark API - support for asynchronous calls - Reactive style [I],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Mark,

Thanks.  The FutureAction API looks awesome.






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Evan Chan <ev@ooyala.com>,"Thu, 12 Dec 2013 08:59:00 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'd be personally fine with a standard workflow of assemble-deps +
packaging just the Spark files as separate packages, if it speeds up
everyone's development time.


e:

he
e
m
n
`
ng
ss
t
s
e
k
r-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
:
f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Evan Chan <ev@ooyala.com>,"Thu, 12 Dec 2013 09:03:50 -0800",Re: Intellij IDEA build issues,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Nick, have you tried using the latest Scala plug-in, which features native
SBT project imports?   ie you no longer need to run gen-idea.






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 12 Dec 2013 11:52:01 -0800",Re: Spark API - support for asynchronous calls - Reactive style [I],dev@spark.incubator.apache.org,"I'm having fun with it.  And it's almost all Reynold's work, so I can't
take credit for it.



"
"""Liu, Raymond"" <raymond.liu@intel.com>","Fri, 13 Dec 2013 01:56:59 +0000",RE: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick

	So what's the plan for support Yarn 2.2 in 0.9? As far as I can see, if you want to support both 2.2 and 2.0 , due to protobuf version incompatible issue. You need two version of akka anyway.

	Akka 2.3-M1 looks like have a little bit change in API, we probably could isolate the code like what we did on yarn part API. I remember that it is mentioned that to use reflection for different API is preferred. So the purpose to use reflection is to use one release bin jar to support both version of Hadoop/Yarn on runtime, instead of build different bin jar on compile time?

	 Then all code related to hadoop will also be built in separate modules for loading on demand? This sounds to me involve a lot of works. And you still need to have shim layer and separate code for different version API and depends on different version Akka etc. Sounds like and even strict demands versus our current approaching on master, and with dynamic class loader in addition, And the problem we are facing now are still there?

Best Regards,
Raymond Liu

newer changes... we'll be removing it for the final merge.


:

 uses this.
te:
t?
ging:
.

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Dec 2013 20:26:35 -0800",Re: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Reymond,

Let's move this discussion out of this thread and into the associated JIRA.
I'll write up our current approach over there.

https://spark-project.atlassian.net/browse/SPARK-995

- Patrick



"
Andy Konwinski <andykonwinski@gmail.com>,"Fri, 13 Dec 2013 11:26:26 -0800",Spark Summit 2013 videos available now,"user@spark.incubator.apache.org, 
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Spark user@ and dev@ list members,

We are happy to announce that videos and slides of all talks from the first
Spark Summit last week, Dec 2-3 in Downtown SF, are now available on the
Spark Summit 2013 webpage at http://spark-summit.org/summit-2013. There is
a link for each talk's slides and video next to the talk's title in the
summit agenda. There are also links to the YouTube playlists for each
Summit session (Keynotes<http://www.youtube.com/playlist?list=PL-x35fyliRwjXj33QvAXN0Vlx0gc6u0je>,
Track A<http://www.youtube.com/playlist?list=PL-x35fyliRwiNcKwIkDEQZBejiqxEJ79U>,
Track B<http://www.youtube.com/playlist?list=PL-x35fyliRwh6YpI80pu2t0JDVHXOtU7y>,
and Training Day<http://www.youtube.com/playlist?list=PL-x35fyliRwjR1Umntxz52zv3EcKpbzCp>
).

You can also now pre-register for Summit
2014<http://spark-summit.org/2014/pre-register>to get notified when
tickets go on sale. Finally please let us know if you
have feedback from the Summit.

Enjoy the videos!

Andy
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 13 Dec 2013 18:37:43 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The vote is now closed. This vote passes with 5 PPMC +1's and no 0 or -1
votes.

+1 (5 Total)
Matei Zaharia*
Nick Pentreath*
Patrick Wendell*
Prashant Sharma*
Tom Graves*

0 (0 Total)

-1 (0 Total)

* = Binding Vote

As per the incubator release guide [1] I'll be sending this to the
general incubator list for a final vote from IPMC members.

[1]
http://incubator.apache.org/guides/releasemanagement.html#best-practice-incubator-release-
vote



ng
n
nt
to
s
to
an
ng
ys
is
r-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
/
/
f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
nd
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 14 Dec 2013 00:59:26 -0800",Re: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Alright I just merged this in - so Spark is officially ""Scala 2.10""
from here forward.

For reference I cut a new branch called scala-2.9 with the commit
immediately prior to the merge:
https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=shortlog;h=refs/heads/scala-2.9

- Patrick


"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Sat, 14 Dec 2013 01:53:23 -0800 (PST)",Re: Scala 2.10 Merge,dev@spark.incubator.apache.org,"Whoohoo!

Great job everyone especially Prashant!

—
Sent from Mailbox for iPhone


tor-spark.git;a=shortlog;h=refs/heads/scala-2.9
JIRA.
can
is
jar to
different
separate
works. And
version
strict
class
there?
some
need
.
still
if
5
into
for
-
*Scala
will
always
when
.9."
Sam Bessalah <samkiller@gmail.com>,"Sat, 14 Dec 2013 11:03:58 +0100",Re: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes. Awesome.
Great job guys.

Sam Bessalah

-spark.git;a=shortlog;h=refs/heads/scala-2.9
te:
A.
ote:
s
r to
nt
 And
n
ct

me
.

"
"""=?utf-8?B?YW5keS5wZXRyZWxsYUBnbWFpbC5jb20=?="" <andy.petrella@gmail.com>","Sat, 14 Dec 2013 11:37:53 +0100",=?utf-8?B?UmUgOiBTY2FsYSAyLjEwIE1lcmdl?=,"dev@spark.incubator.apache.org,dev@spark.incubator.apache.org","That's a very good news!
Congrats

Envoyé depuis mon HTC

----- Reply message -----
De : ""Sam Bessalah"" <samkiller@gmail.com>
Pour : ""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>
Objet : Scala 2.10 Merge
Date : sam., déc. 14, 2013 11:03


Yes. Awesome.
Great job guys.

Sam Bessalah

> On Dec 14, 2013, at 9:59 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> 
> Alright I just merged this in - so Spark is officially ""Scala 2.10""
> from here forward.
> 
> For reference I cut a new branch called scala-2.9 with the commit
> immediately prior to the merge:
> https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=shortlog;h=refs/heads/scala-2.9
> 
> - Patrick
> 
>> On Thu, Dec 12, 2013 at 8:26 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Hey Reymond,
>> 
>> Let's move this discussion out of this thread and into the associated JIRA.
>> I'll write up our current approach over there.
>> 
>> https://spark-project.atlassian.net/browse/SPARK-995
>> 
>> - Patrick
>> 
>> 
>>> On Thu, Dec 12, 2013 at 5:56 PM, Liu, Raymond <raymond.liu@intel.com> wrote:
>>> 
>>> Hi Patrick
>>> 
>>>        So what's the plan for support Yarn 2.2 in 0.9? As far as I can
>>> see, if you want to support both 2.2 and 2.0 , due to protobuf version
>>> incompatible issue. You need two version of akka anyway.
>>> 
>>>        Akka 2.3-M1 looks like have a little bit change in API, we
>>> probably could isolate the code like what we did on yarn part API. I
>>> remember that it is mentioned that to use reflection for different API is
>>> preferred. So the purpose to use reflection is to use one release bin jar to
>>> support both version of Hadoop/Yarn on runtime, instead of build different
>>> bin jar on compile time?
>>> 
>>>         Then all code related to hadoop will also be built in separate
>>> modules for loading on demand? This sounds to me involve a lot of works. And
>>> you still need to have shim layer and separate code for different version
>>> API and depends on different version Akka etc. Sounds like and even strict
>>> demands versus our current approaching on master, and with dynamic class
>>> loader in addition, And the problem we are facing now are still there?
>>> 
>>> Best Regards,
>>> Raymond Liu
>>> 
>>> -----Original Message-----
>>> From: Patrick Wendell [mailto:pwendell@gmail.com]
>>> Sent: Thursday, December 12, 2013 5:13 PM
>>> To: dev@spark.incubator.apache.org
>>> Subject: Re: Scala 2.10 Merge
>>> 
>>> Also - the code is still there because of a recent merge that took in some
>>> newer changes... we'll be removing it for the final merge.
>>> 
>>> 
>>> On Thu, Dec 12, 2013 at 1:12 AM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>> 
>>>> Hey Raymond,
>>>> 
>>>> This won't work because AFAIK akka 2.3-M1 is not binary compatible
>>>> with akka 2.2.3 (right?). For all of the non-yarn 2.2 versions we need
>>>> to still use the older protobuf library, so we'd need to support both.
>>>> 
>>>> I'd also be concerned about having a reference to a non-released
>>>> version of akka. Akka is the source of our hardest-to-find bugs and
>>>> simultaneously trying to support 2.2.3 and 2.3-M1 is a bit daunting.
>>>> Of course, if you are building off of master you can maintain a fork
>>>> that uses this.
>>>> 
>>>> - Patrick
>>>> 
>>>> 
>>>> On Thu, Dec 12, 2013 at 12:42 AM, Liu, Raymond
>>>> <raymond.liu@intel.com>wrote:
>>>> 
>>>>> Hi Patrick
>>>>> 
>>>>>        What does that means for drop YARN 2.2? seems codes are still
>>>>> there. You mean if build upon 2.2 it will break, and won't and work
>>>>> right?
>>>>> Since the home made akka build on scala 2.10 are not there. While, if
>>>>> for this case, can we just use akka 2.3-M1 which run on protobuf 2.5
>>>>> for replacement?
>>>>> 
>>>>> Best Regards,
>>>>> Raymond Liu
>>>>> 
>>>>> 
>>>>> -----Original Message-----
>>>>> From: Patrick Wendell [mailto:pwendell@gmail.com]
>>>>> Sent: Thursday, December 12, 2013 4:21 PM
>>>>> To: dev@spark.incubator.apache.org
>>>>> Subject: Scala 2.10 Merge
>>>>> 
>>>>> Hi Developers,
>>>>> 
>>>>> In the next few days we are planning to merge Scala 2.10 support into
>>>>> Spark. For those that haven't been following this, Prashant Sharma
>>>>> has been maintaining the scala-2.10 branch of Spark for several
>>>>> months. This branch is current with master and has been reviewed for
>>>>> merging:
>>>>> 
>>>>> https://github.com/apache/incubator-spark/tree/scala-2.10
>>>>> 
>>>>> Scala 2.10 support is one of the most requested features for Spark -
>>>>> it will be great to get this into Spark 0.9! Please note that *Scala
>>>>> 2.10 is not binary compatible with Scala 2.9*. With that in mind, I
>>>>> wanted to give a few heads-up/requests to developers:
>>>>> 
>>>>> If you are developing applications on top of Spark's master branch,
>>>>> those will need to migrate to Scala 2.10. You may want to download
>>>>> and test the current scala-2.10 branch in order to make sure you will
>>>>> be okay as Spark developments move forward. Of course, you can always
>>>>> stick with the current master commit and be fine (I'll cut a tag when
>>>>> we do the merge in order to delineate where the version changes).
>>>>> Please open new threads on the dev list to report and discuss any
>>>>> issues.
>>>>> 
>>>>> This merge will temporarily drop support for YARN 2.2 on the master
>>>>> branch.
>>>>> This is because the workaround we used was only compiled for Scala 2.9.
>>>>> We are going to come up with a more robust solution to YARN 2.2
>>>>> support before releasing 0.9.
>>>>> 
>>>>> Going forward, we will continue to make maintenance releases on
>>>>> branch-0.8 which will remain compatible with Scala 2.9.
>>>>> 
>>>>> For those interested, the primary code changes in this merge are
>>>>> upgrading the akka version, changing the use of Scala 2.9's
>>>>> ClassManifest construct to Scala 2.10's ClassTag, and updating the
>>>>> spark shell to work with Scala 2.10's repl.
>>>>> 
>>>>> - Patrick
>> 
>> 
"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 14 Dec 2013 10:31:48 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick, as sebb has mentioned let's move the binaries from the
voting directory in your people.apache.org directory.
ASF release voting is for source code and not binaries, and
technically we provide binaries for convenience.

And add link to the KEYS location in the dist[1] to let verify signatures.

Sorry for the late response to the VOTE thread, guys.

- Henry

[1] https://dist.apache.org/repos/dist/release/incubator/spark/KEYS

:
ncubator-release-
t
ing
s
en
more convenient
 to
y
es
 to
can
ing
ays
 is
t
:
or-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
4/
0/
;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
and
<

"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 14 Dec 2013 10:40:07 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Actually we should be fine putting the binaries there as long as the
VOTE is for the source.

Let's verify with sebb in the general@ list about his concern.

- Henry

rote:
.
te:
incubator-release-
ot
ding
as
s
hen
 more convenient
s to
ly
ses
e to
m>
2can
h
ning
s
says
t is
ot
):
tor-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
e
c4/
40/
d
b;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
 and
g

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 14 Dec 2013 10:56:01 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Henry,

make sure they work. This is really valuable. If you'd like I could
add a caveat to the vote thread explaining that we are only voting on
the source.

- Patrick

rote:
s.
ote:
1
-incubator-release-
got
lding
 as
as
m
when
nient
ps to
bly
sses
ve to
om>
52can
.
ch
nning
is
 says
it is
not
d):
ator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
be
rc4/
040/
nd
/
ob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
C and
ng
a><

"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 14 Dec 2013 11:08:15 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick,

Yeap I agree, but technically ASF VOTE release on source only, there
even debate about it =), so putting it in the vote staging artifact
could confuse people because in our case we do package 3rd party
libraries in the binary jars.

I have sent email to sebb asking clarification about his concern in
general@ list.

- Henry

e:
es.
rote:
-1
e-incubator-release-
m
 got
ilding
g as
 as
om
 when
s
 a more convenient
eps to
mbly
asses
ave to
com>
252can
r.
t
uch
unning
 is
h says
 it is
 not
e
1d):
bator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
 be
-rc4/
-040/
und
s/
lob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
TC and
.
ing
la><

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 14 Dec 2013 11:12:25 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Henry - from that thread it looks like sebb's concern was something
different than this.

rote:
ote:
res.
 -1
ce-incubator-release-
om
e got
e
uilding
ng as
g as
com
n when
's
g
venient
deps to
embly
lasses
have to
.com>
/252can
er.
bt
much
running
e is
ch says
y it is
l not
he
31d):
ubator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e
n be
g-rc4/
k-040/
ound
cs/
blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8
UTC and
t.
ting
ala><

"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 14 Dec 2013 11:20:50 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yeah seems like it. He was ok with our prev release.
Let's wait for his reply


"
Azuryy Yu <azuryyyu@gmail.com>,"Sun, 15 Dec 2013 20:31:00 +0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Hi,
Spark-0.8.1 supports yarn 0.22 right? where to find the release note?
Thanks.



"
Azuryy Yu <azuryyyu@gmail.com>,"Sun, 15 Dec 2013 20:31:24 +0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"yarn 2.2, not yarn 0.22, I am so sorry.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 15 Dec 2013 10:43:12 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","You can checkout the docs mentioned in the vote thread. There is also
a pre-build binary for hadoop2 that is compiled for YARN 2.2

- Patrick


"
Azuryy Yu <azuryyyu@gmail.com>,"Mon, 16 Dec 2013 07:42:36 +0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Thanks Patrick.

"
Azuryy Yu <azuryyyu@gmail.com>,"Mon, 16 Dec 2013 10:30:05 +0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Hi here,
Do we have plan to upgrade protobuf from 2.4.1 to 2.5.0? PB has some
uncompatable API between these two versions.
Hadoop-2.x using protobuf-2.5.0


but if some guys want to run Spark on mesos, then mesos using
protobuf-2.4.1 currently. so we may discuss here for a better solution.




"
"""Liu, Raymond"" <raymond.liu@intel.com>","Mon, 16 Dec 2013 02:48:08 +0000",RE: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Azuryy

Please Check https://spark-project.atlassian.net/browse/SPARK-995 for this protobuf version issue

Best Regards,
Raymond Liu

Do we have plan to upgrade protobuf from 2.4.1 to 2.5.0? PB has some uncompatable API between these two versions.
Hadoop-2.x using protobuf-2.5.0


but if some guys want to run Spark on mesos, then mesos using
protobuf-2.4.1 currently. so we may discuss here for a better solution.





"
"""Liu, Raymond"" <raymond.liu@intel.com>","Mon, 16 Dec 2013 03:00:34 +0000",RE: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","That issue is for 0.9's solution.

And if you mean for 0.8.1, when you build against hadoop 2.2 Yarn, protobuf is already using 2.5.0 instead of 2.4.1. so it will works fine with hadoop 2.2
And regarding on 0.8.1 you build against hadoop 2.2 Yarn, while run upon mesos... strange combination, I am not sure, might have problem. If have problem, you might need to build mesos against 2.5.0, I don't test that, if you got time, mind take a test?

Best Regards,
Raymond Liu



Please Check https://spark-project.atlassian.net/browse/SPARK-995 for this protobuf version issue

Best Regards,
Raymond Liu

Do we have plan to upgrade protobuf from 2.4.1 to 2.5.0? PB has some uncompatable API between these two versions.
Hadoop-2.x using protobuf-2.5.0


but if some guys want to run Spark on mesos, then mesos using
protobuf-2.4.1 currently. so we may discuss here for a better solution.





"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 15 Dec 2013 22:02:05 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Mesos will almost certainly compile fine with protobuf 2.5. The protobuf compiler and binary format is forward-compatible across releases, its just the Java artifacts that arent. Youll need to ask Mesos to provide a version with protobuf 2.5, and use that with these versions of Hadoop.

Matei


protobuf is already using 2.5.0 instead of 2.4.1. so it will works fine with hadoop 2.2
upon mesos... strange combination, I am not sure, might have problem. If have problem, you might need to build mesos against 2.5.0, I don't test that, if you got time, mind take a test?
this protobuf version issue
uncompatable API between these two versions.
solution.
also 
note?
concern.

http://incubator.apache.org/guides/releasemanagement.html#best-practi


"
Azuryy Yu <azuryyyu@gmail.com>,"Mon, 16 Dec 2013 14:25:44 +0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Maybe I am not give a clear description. I am runing Spark on yarn. instead
of Mesos. I just want build Spark with protobuf2.5. I am not care about
Mesos.

I've changed Spark pom.xml to probobuf2.5 manually.




te:

just
n
if
:
e?
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 16 Dec 2013 00:22:28 -0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Are you using 0.8.1? It will build with protobuf 2.5 instead of 2.4 as long as you make it depend on Hadoop 2.2. But make sure you build it with SPARK_HADOOP_VERSION=2.2.0 or whatever.

Spark 0.8.0 doesnt support Hadoop 2.2 due to this issue.

Matei


instead
about
protobuf
its just
provide a
Hadoop.
fine
upon
have
that, if
for
solution.
also
note?
concern.
members.
http://incubator.apache.org/guides/releasemanagement.html#best-practi


"
Azuryy Yu <azuryyyu@gmail.com>,"Mon, 16 Dec 2013 16:38:44 +0800",Re: [VOTE] Release Apache Spark 0.8.1-incubating (rc4),dev@spark.incubator.apache.org,"Hi Matei,
Thanks for your response. I am using 0.8.1, and yes, It was using
protobuf-2.5. Sorry I made a mistake before this email.

I used -Phadoop2-yarn, so I don't find it using pb-2.5, actually, I should
use -Pnew-yarn.

Thank you Matei.




te:

uf
s
e a
e
ve
n.
:
so
"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 16 Dec 2013 11:36:55 +0200",Re: Intellij IDEA build issues,dev@spark.incubator.apache.org,"Thanks Evan, I tried it and the new SBT direct import seems to work well,
though I did run into issues with some yarn imports on Spark.

n



"
Evan Chan <ev@ooyala.com>,"Mon, 16 Dec 2013 09:58:50 -0800",Re: Re : Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Great job everyone!  A big step forward.



:
r-spark.git;a=shortlog;h=refs/heads/scala-2.9
n
n
I
e
?
ed
h.
l
if
5
to
r
-
a
ll
ys
en



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"Mon, 16 Dec 2013 23:12:39 +0100",Re: spark.task.maxFailures,dev@spark.incubator.apache.org,"Any news regarding this setting? Is this expected behaviour? Is there some
other way I can have Spark fail-fast?

Thanks!

:

e
"
Reynold Xin <rxin@apache.org>,"Mon, 16 Dec 2013 14:16:56 -0800",Re: spark.task.maxFailures,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I just merged your pull request
https://github.com/apache/incubator-spark/pull/245


e:

e
te:
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 16 Dec 2013 15:06:12 -0800",Re: spark.task.maxFailures,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This was a good fix - thanks for the contribution.

ote:
me
ote:
s

"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Mon, 16 Dec 2013 15:39:10 -0800",Re: spark.task.maxFailures,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","i guess it should really be ""maximum number of total task run attempts"".
 At least that's what it looks logically. in that sense, the rest of the
documentation is correct ( should be at least 1; 1 = task is allowed no
retries (1-1=0)).




e:

/org/apache/spark/scheduler/cluster/ClusterTaskSetManager.scala#L532
m/celtramobile>
"
Chris Mattmann <mattmann@apache.org>,"Tue, 17 Dec 2013 09:15:52 -0800",Re: [VOTE] Release Apache Spark 0.8.0-incubating (rc4),"""general@incubator.apache.org"" <general@incubator.apache.org>","Hi Guys,

+1 from me (binding):

SIGS pass, CHECKSUMS pass:

[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%
$HOME/bin/stage_apache_rc spark 0.8.1-incubating-bin-hadoop1
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100  131M  100  131M    0     0  1754k      0  0:01:16  0:01:16 --:--:--
1165k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   490  100   490    0     0   6965      0 --:--:-- --:--:-- --:--:--
13611
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   129  100   129    0     0   1839      0 --:--:-- --:--:-- --:--:--
3583
[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%
$HOME/bin/stage_apache_rc spark 0.8.1-incubating-bin-hadoop2
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100  215M  100  215M    0     0  1815k      0  0:02:01  0:02:01 --:--:--
1826k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   490  100   490    0     0   6831      0 --:--:-- --:--:-- --:--:--
13611
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   129  100   129    0     0   1819      0 --:--:-- --:--:-- --:--:--
3583
[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%
$HOME/bin/stage_apache_rc spark 0.8.1-incubating-bin-cdh
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/
[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%
$HOME/bin/stage_apache_rc spark 0.8.1-incubating-bin-cdh4
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100  136M  100  136M    0     0  1757k      0  0:01:19  0:01:19 --:--:--
1502k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   490  100   490    0     0   6892      0 --:--:-- --:--:-- --:--:--
13611
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   123  100   123    0     0   1702      0 --:--:-- --:--:-- --:--:--
3514
[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%
$HOME/bin/stage_apache_rc spark 0.8.1-incubating
http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100 4565k  100 4565k    0     0  1636k      0  0:00:02  0:00:02 --:--:--
1656k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100   490  100   490    0     0   6949      0 --:--:-- --:--:-- --:--:--
13611
  % Total    % Received % Xferd  Average Speed   Time    Time     Time
Current
                                 Dload  Upload   Total   Spent    Left
Speed
100    77  100    77    0     0   1109      0 --:--:-- --:--:-- --:--:--
2200
[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%
$HOME/bin/verify_gpg_sigs
Verifying Signature for file spark-0.8.1-incubating-bin-cdh4.tgz.asc
gpg: Signature made Tue Dec 10 15:03:24 2013 PST using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the
owner.
Primary key fingerprint: 5AA9 0E72 812F F246 7904  277D 548F 5FEE 9E4F E3AF
Verifying Signature for file spark-0.8.1-incubating-bin-hadoop1.tgz.asc
gpg: Signature made Tue Dec 10 14:58:15 2013 PST using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the
owner.
Primary key fingerprint: 5AA9 0E72 812F F246 7904  277D 548F 5FEE 9E4F E3AF
Verifying Signature for file spark-0.8.1-incubating-bin-hadoop2.tgz.asc
gpg: Signature made Tue Dec 10 15:09:16 2013 PST using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the
owner.
Primary key fingerprint: 5AA9 0E72 812F F246 7904  277D 548F 5FEE 9E4F E3AF
Verifying Signature for file spark-0.8.1-incubating.tgz.asc
gpg: Signature made Tue Dec 10 14:53:15 2013 PST using RSA key ID 9E4FE3AF
gpg: Good signature from ""Patrick Wendell <pwendell@gmail.com>""
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the
owner.
Primary key fingerprint: 5AA9 0E72 812F F246 7904  277D 548F 5FEE 9E4F E3AF
[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%
$HOME/bin/verify_md5_checksums
md5sum: stat '*.tar.gz': No such file or directory
md5sum: stat '*.bz2': No such file or directory
md5sum: stat '*.zip': No such file or directory
spark-0.8.1-incubating-bin-cdh4.tgz: OK
spark-0.8.1-incubating-bin-hadoop1.tgz: OK
spark-0.8.1-incubating-bin-hadoop2.tgz: OK
spark-0.8.1-incubating.tgz: OK
[chipotle:~/tmp/apache-spark-0.8.1-incubating-rc4] mattmann%



Thanks!

Cheers,
Chris





"
Roman Shaposhnik <rvs@apache.org>,"Tue, 17 Dec 2013 09:21:14 -0800",Re: [VOTE] Release Apache Spark 0.8.0-incubating (rc4),dev@spark.incubator.apache.org,"+1 (binding) from me as well.

That said, I'd expect the issues identified around
jar inclusion to be blocking for 0.9 (do we have
a blocker JIRA filed?). There's also a few issues
around the build but I need to spend time and file
JIRAs myself. Will do i"
Matthew Cheah <mccheah@uwaterloo.ca>,"Tue, 17 Dec 2013 10:43:17 -0800",Spark development for undergraduate project,dev@spark.incubator.apache.org,"Hi everyone,

During my most recent internship, I worked extensively with Apache Spark,
integrating it into a company's data analytics platform. I've now become
interested in contributing to Apache Spark.

I'm returning to undergraduate studies in January and there is an academic
course which is simply a standalone software engineering project. I was
thinking that some contribution to Apache Spark would satisfy my curiosity,
help continue support the company I interned at, and give me academic
credits required to graduate, all at the same time. It seems like too good
an opportunity to pass up.

With that in mind, I have the following questions:

   1. At this point, is there any self-contained project that I could work
   on within Spark? Ideally, I would work on it independently, in about a
   three month time frame. This time also needs to accommodate ramping up on
   the Spark codebase and adjusting to the Scala programming language and
   paradigms. The company I worked at primarily used the Java APIs. The output
   needs to be a technical report describing the project requirements, and the
   design process I took to engineer the solution for the requirements. In
   particular, it cannot just be a series of haphazard patches.
   2. How can I get started with contributing to Spark?
   3. Is there a high-level UML or some other design specification for the
   Spark architecture?

Thanks! I hope to be of some help =)

-Matt Cheah
"
Christopher Nguyen <ctn@adatao.com>,"Tue, 17 Dec 2013 11:25:51 -0800",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Matt, some suggestions.

If you're interested in the machine-learning layer, perhaps you could look
into helping to harmonize our (Adatao) dataframe representation with
MLlib's, and base RDDs for that matter. It requires someone to spend some
dedicated time looking into the trade-offs between generalizability vs
performance issues, etc. It's something our groups have talked about doing
but haven't been able to invest the resources to do.

Separately, neural nets/deep learning is an area of emerging interest to
look into with Spark. It may drive some alternate optimization patterns for
Spark, e.g., sub-cluster communication. If interested, I can connect you to
some deep learning folks at UoT (not too far from you) and Google. Matei
may also have some interest in this.

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
Matthew Cheah <matthew.c.cheah@gmail.com>,"Tue, 17 Dec 2013 11:30:45 -0800",Spark development for undergraduate project,dev@spark.incubator.apache.org,"Hi everyone,

During my most recent internship, I worked extensively with Apache Spark,
integrating it into a company's data analytics platform. I've now become
interested in contributing to Apache Spark.

I'm returning to undergraduate studies in January and there is an academic
course which is simply a standalone software engineering project. I was
thinking that some contribution to Apache Spark would satisfy my curiosity,
help continue support the company I interned at, and give me academic
credits required to graduate, all at the same time. It seems like too good
an opportunity to pass up.

With that in mind, I have the following questions:

   1. At this point, is there any self-contained project that I could work
   on within Spark? Ideally, I would work on it independently, in about a
   three month time frame. This time also needs to accommodate ramping up on
   the Spark codebase and adjusting to the Scala programming language and
   paradigms. The company I worked at primarily used the Java APIs. The output
   needs to be a technical report describing the project requirements, and the
   design process I took to engineer the solution for the requirements. In
   particular, it cannot just be a series of haphazard patches.
   2. How can I get started with contributing to Spark?
   3. Is there a high-level UML or some other design specification for the
   Spark architecture?

Thanks! I hope to be of some help =)

-Matt Cheah
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 17 Dec 2013 17:15:16 -0800",[RESULT] [VOTE] Release Apache Spark 0.8.1-incubating (rc4),general@incubator.apache.org,"The vote is now closed. This vote passes with 4 IPMC +1's and no 0 or -1 votes.

+1 (4 Total)
Marvin Humphrey
Henry Saputra
Chris Mattmann
Roman Shaposhnik

0 (0 Total)

-1 (0 Total)

* = Binding Vote

Thanks to everyone who helped vet this release.

- Patrick

"
Matthew Cheah <matthew.c.cheah@gmail.com>,"Tue, 17 Dec 2013 17:39:48 -0800",Fwd: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Hi everyone,

During my most recent internship, I worked extensively with Apache Spark,
integrating it into a company's data analytics platform. I've now become
interested in contributing to Apache Spark.

I'm returning to undergraduate studies in January and there is an academic
course which is simply a standalone software engineering project. I was
thinking that some contribution to Apache Spark would satisfy my curiosity,
help continue support the company I interned at, and give me academic
credits required to graduate, all at the same time. It seems like too good
an opportunity to pass up.

With that in mind, I have the following questions:

   1. At this point, is there any self-contained project that I could work
   on within Spark? Ideally, I would work on it independently, in about a
   three month time frame. This time also needs to accommodate ramping up on
   the Spark codebase and adjusting to the Scala programming language and
   paradigms. The company I worked at primarily used the Java APIs. The output
   needs to be a technical report describing the project requirements, and the
   design process I took to engineer the solution for the requirements. In
   particular, it cannot just be a series of haphazard patches.
   2. How can I get started with contributing to Spark?
   3. Is there a high-level UML or some other design specification for the
   Spark architecture?

Thanks! I hope to be of some help =)

-Matt Cheah
"
Azuryy Yu <azuryyyu@gmail.com>,"Wed, 18 Dec 2013 13:54:56 +0800",Contribute to Spark,dev@spark.incubator.apache.org,"Hi,

I make my first pull request here(just a minor fix):
https://github.com/apache/incubator-spark/pull/274

Another:
Spark use assemble plug in to generate a fat jar during build,
We should specify SPARK_JAR  in the command line, then Spark upload spark
jar and user jar to the HDFS,

but yarn has a configurable option:
  <property>
    <name>yarn.application.classpath</name>
  </property>

so we can do another way:
Exclude hadoop-* jar and hadoop related jars during build, which can
decreash the fat jar size. then put spark-fat-jar under HADOOP_LIB_DIR,
then Spark don't need to update spark jar to the HDFS, also we don't need
to specify SPARK_JAR in the command line.


If there is no problem, I want to do this change.
"
=?GB2312?B?tq3O97PJKGNsc2Vlcik=?= <clseer@gmail.com>,"Wed, 18 Dec 2013 14:03:43 +0800",spark developer from China,dev@spark.incubator.apache.org,"Hi, all,
  I'm a spark develpor in China, and want to join this Spark Mailing List.
-- 

Best regards,
Xicheng Dong

=======================================================
**: http://dongxicheng.org/
*鼮*: http://hadoop123.com/
*΢*: http://weibo.com/clseer
*ؼ*: YARN/Mesos,MapReduce,Tez,Storm
======================================================
"
Azuryy Yu <azuryyyu@gmail.com>,"Wed, 18 Dec 2013 14:17:37 +0800",Re: spark developer from China,dev@spark.incubator.apache.org,"Hi Clseer,

Please read here to subscribe the mailing list.
http://spark.incubator.apache.org/mailing-lists.html



On Wed, Dec 18, 2013 at 2:03 PM, (clseer) <clseer@gmail.com> wrote:

> Hi, all,
>   I'm a spark develpor in China, and want to join this Spark Mailing List.
> --
>
> Best regards,
> Xicheng Dong
>
> =======================================================
> **: http://dongxicheng.org/
> *鼮*: http://hadoop123.com/
> *΢*: http://weibo.com/clseer
> *ؼ*: YARN/Mesos,MapReduce,Tez,Storm
> ======================================================
>
"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 17 Dec 2013 23:05:14 -0800",Re: Scala 2.10 Merge,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","W00t :)


en
h
e
h, those
nt
 order to
h.
e
re
8
g
onstruct
th Scala
"
Marek Kolodziej <mkolod@gmail.com>,"Thu, 19 Dec 2013 12:27:24 -0500",View bound deprecation (Scala 2.11+),dev@spark.incubator.apache.org,"All,

Apparently view bounds will be deprecated going forward. Hopefully they'll
be around for a while after deprecation, but I wanted to raise this issue
for consideration. Here's the SIP:
https://issues.scala-lang.org/browse/SI-7629

Shall I file a Jira for that?

Thanks!

Marek
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 19 Dec 2013 10:29:56 -0800",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Hi Matt,

If you want to get started looking at Spark, I recommend the following resources:

- Our issue tracker at http://spark-project.atlassian.net contains some issues marked Starter that are good places to jump into. You might be able to take one of those and extend it into a bigger project.

- The contributing to Spark wiki page covers how to send patches and set up development: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark 

- This talk has an intro to Spark internals (video and slides are in the comments): http://www.meetup.com/spark-users/events/94101942/

For a longer project, here are some possible ones:

- Create a tool that automatically checks which Scala API methods are missing in Python. We had a similar one for Java that was very useful. Even better would be to automatically create wrappers for the Scala ones.

- Extend the Spark monitoring UI with profiling information (to sample the workers and say where theyre spending time, or what data structures consume the most memory).

- Pick and implement a new machine learning algorithm for MLlib.

Matei


Spark,
become
academic
was
curiosity,
good
work
a
up on
and
output
and the
In
the


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 19 Dec 2013 10:30:46 -0800",Re: View bound deprecation (Scala 2.11+),dev@spark.incubator.apache.org,"We can open a JIRA but lets wait to see what the Scala guys decide. Im sure theyll recommend some alternatives.

Matei


they'll
issue


"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 19 Dec 2013 10:38:26 -0800 (PST)",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Or if you're extremely ambitious work in implementing Spark Streaming in Python—
Sent from Mailbox for iPhone


resources:
issues marked “Starter” that are good places to jump into. You might be able to take one of those and extend it into a bigger project.
send patches and set up development: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark 
comments): http://www.meetup.com/spark-users/events/94101942/
missing in Python. We had a similar one for Java that was very useful. Even better would be to automatically create wrappers for the Scala ones.
the workers and say where they’re spending time, or what data structures consume the most memory).
,
become
academic
curiosity,
good
work
 a
 on
and
output
and the
In
the"
Andrew Ash <andrew@andrewash.com>,"Thu, 19 Dec 2013 13:56:49 -0500",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"I think there are also some improvements that could be made to
deployability in an enterprise setting.  From my experience:

1. Most places I deploy Spark in don't have internet access.  So I can't
build from source, compile against a different version of Hadoop, etc
without doing it locally and then getting that onto my servers manually.
 This is less a problem with Spark now that there are binary distributions,
but it's still a problem for using Mesos with Spark.
2. Configuration of Spark is confusing -- you can make configuration in
Java system properties, environment variables, command line parameters, and
for the standalone cluster deployment mode you need to worry about whether
these need to be set on the master, the worker, the executor, or the
application/driver program.  Also because spark-shell automatically
instantiates a SparkContext you have to set up any system properties in the
init scripts or on the command line with
JAVA_OPTS=""-Dspark.executor.memory=8g"" etc.  I'm not sure what needs to be
done, but it feels that there are gains to be made in configuration options
here.  Ideally, I would have one configuration file that can be used in all
4 places and that's the only place to make configuration changes.
3. Standalone cluster mode could use improved resiliency for starting,
stopping, and keeping alive a service -- there are custom init scripts that
call each other in a mess of ways: spark-shell, spark-daemon.sh,
spark-daemons.sh, spark-config.sh, spark-env.sh, compute-classpath.sh,
spark-executor, spark-class, run-example, and several others in the bin/
directory.  I would love it if Spark used the Tanuki Service Wrapper, which
is widely-used for Java service daemons, supports retries, installation as
init scripts that can be chkconfig'd, etc.  Let's not re-solve the ""how do
I keep a service running?"" problem when it's been done so well by Tanuki --
we use it at my day job for all our services, plus it's used by
Elasticsearch.  This would help solve the problem where a quick bounce of
the master causes all the workers to self-destruct.
4. Sensitivity to hostname vs FQDN vs IP address in spark URL -- this is
entirely an Akka bug based on previous mailing list discussion with Matei,
but it'd be awesome if you could use either the hostname or the FQDN or the
IP address in the Spark URL and not have Akka barf at you.

I've been telling myself I'd look into these at some point but just haven't
gotten around to them myself yet.  Some day!  I would prioritize these
requests from most- to least-important as 3, 2, 4, 1.

Andrew


rote:

. You might be
end patches and
e
en
uctures
me
s
 a
nd
"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 19 Dec 2013 10:57:58 -0800 (PST)",Re: [PySpark]: reading arbitrary Hadoop InputFormats,dev@spark.incubator.apache.org,"Hi


I managed to find the time to put together a PR on this: https://github.com/apache/incubator-spark/pull/263




Josh has had a look over it - if anyone else with an interest could give some feedback that would be great.




As mentioned in the PR it's more of an RFC and certainly still needs a bit of clean up work, and I need to add the concept of ""wrapper functions"" to deserialize classes that MsgPack can't handle out the box.




N
—
Sent from Mailbox for iPhone


soon
be
of
Python
uses
all
sequenceFileAsText()
them
used
8/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala#L255
to
7bc51ec0/python/pyspark/worker.py#L41
stage's
7bc51ec0/python/pyspark/rdd.py#L42
 if
additional
on
calls
toString
in. All
work
when
 like
the
written.
(serialization
from
conversions

like
Cassandra
make
com
from
in
we
have
com>
so
structured
arkInternals-customserializers
to
read
classpath
use
anyone's
cases
could
a/org/apache/spark/api/python/PythonRDD.scala#L224
of
when
for
Scala
quite
Reasons
it
 /
my
to
more
was
Writable
 of
must
from
such
yields
compile"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 19 Dec 2013 11:02:28 -0800 (PST)",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Some good things to look at though hopefully #2 will be largely addressed by: https://github.com/apache/incubator-spark/pull/230—
Sent from Mailbox for iPhone


distributions,
and
whether
the
needs to be
options
all
that
which
as
 do
Tanuki --
of
,
the
haven't
in
com>
following
some
into. You might be
send patches and
the
Even
sample
structures
become
was
academic
too
about a
ramping
and
The

.
for"
Andrew Ash <andrew@andrewash.com>,"Thu, 19 Dec 2013 14:19:17 -0500",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Wow yes, that PR#230 looks like exactly what I outlined in #2!  I'll leave
some comments on there.

Anything going on for service reliability (#3) since apparently someone is
reading my mind?


rote:

t
.
s to
/
i
of
s
in
m
ng
nto. You might be
o send patches and
k
e
le
structures
ic
oo
d
ng
e
s,
or
"
Christopher Nguyen <ctn@adatao.com>,"Thu, 19 Dec 2013 12:52:36 -0800",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"+1 to most of Andrew's suggestions here, and while we're in that
neighborhood, how about generalizing something like ""wtf-spark"" (from the
Bizo team (http://youtu.be/6Sn1xs5DN1Y?t=38m36s)? It may not be of high
academic interest, but it's something people"
Gill <conexiongill@gmail.com>,"Thu, 19 Dec 2013 12:58:31 -0800",How to contribute to the spark project,dev@spark.incubator.apache.org,"Hi,

I attended the spark summit and have been curios to know that how can I
contribute to the spark project. I'm working on query engine optimizations
so can help with spark query engine optimizations or with other query
engine features.

Thanks
Gurbir
510 410 5108
"
Azuryy Yu <azuryyyu@gmail.com>,"Fri, 20 Dec 2013 07:25:24 +0800",Re: How to contribute to the spark project,dev@spark.incubator.apache.org,"Hi Gill,
please read here:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 19 Dec 2013 17:15:38 -0800",Spark 0.8.1 Released,"user@spark.incubator.apache.org, 
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi everyone,

We've just posted Spark 0.8.1, a new maintenance release that contains
some bug fixes and improvements to the 0.8 branch. The full release
notes are available at [1]. Apart from various bug fixes, 0.8.1
includes support for YARN 2.2, a high availability mode for the
standalone scheduler, and optimizations to the shuffle. We recommend
that current users update to this release. You can grab the release at
[2].

[1] http://spark.incubator.apache.org/releases/spark-release-0-8-1.html
[2] http://spark.incubator.apache.org/downloads

Thanks to the following people who contributed to this release:

Michael Armbrust, Pierre Borckmans, Evan Chan, Ewen Cheslack, Mosharaf
Chowdhury, Frank Dai, Aaron Davidson, Tathagata Das, Ankur Dave,
Harvey Feng, Ali Ghodsi, Thomas Graves, Li Guoqiang, Stephen Haberman,
Haidar Hadi, Nathan Howell, Holden Karau, Du Li, Raymond Liu, Xi Liu,
David McCauley, Michael (wannabeast), Fabrizio Milo, Mridul
Muralidharan, Sundeep Narravula, Kay Ousterhout, Nick Pentreath, Imran
Rashid, Ahir Reddy, Josh Rosen, Henry Saputra, Jerry Shao, Mingfei
Shi, Andre Schumacher, Karthik Tunga, Patrick Wendell, Neal Wiggins,
Andrew Xia, Reynold Xin, Matei Zaharia, and Wu Zeming

- Patrick

"
Matthew Cheah <mccheah@uwaterloo.ca>,"Thu, 19 Dec 2013 17:27:03 -0800",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Thanks a lot everyone! I'm looking into adding an algorithm to MLib for the
project. Nice and self-contained.

-Matt Cheah


:

:
t
.
s to
/
i
of
s
ght be
hes
n
re
.
I
ld
ge
"
Andrew Ash <andrew@andrewash.com>,"Thu, 19 Dec 2013 20:33:45 -0500",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Sounds like a great choice.  It would be particularly impressive if you
could add the first online learning algorithm (all the current ones are
offline I believe) to pave the way for future contributions.


:

he
he
gh
in
s,
in
eds to
in
,
s
,
on
ow
e
or
e
ng
s
p into. You might
w to send patches
.
ta
he
w
n
.
e
s.
n
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 19 Dec 2013 17:46:09 -0800",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"+1 to that (assuming by 'online' Andrew meant MLLib algorithm from Spark
Streaming)

Something you can look into is implementing a streaming KMeans. Maybe you
can re-use a lot of the offline KMeans code in MLLib.

TD



tc
n
e
s
needs
d
r,
is
N
ns
u
re
ds"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 19 Dec 2013 18:08:38 -0800",Re: Spark 0.8.1 Released,dev@spark.incubator.apache.org,"Thanks Patrick for coordinating this release!

Matei




"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 19 Dec 2013 19:19:06 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,user@spark.incubator.apache.org,"Hey Mike,

As you probably noticed when you CC'd spark-developers@googlegroups.com,
that list has already be reconfigured so that it no longer allows posting
(and bounces emails sent to it).

We will be doing the same thing to the spark-users@googlegroups.com list
too (we'll announce a date for that soon).

That may sound very frustrating, and you are *not* alone feeling that way.
We've had a long conversation with our mentors about this, and I've felt
very similar to you, so I'd like to give you background.

As I'm coming to see it, part of becoming an Apache project is moving the
community *fully* over to Apache infrastructure, and more generally the
Apache way of organizing the community.

This applies in both the nuts-and-bolts sense of being on apache infra, but
possibly more importantly, it is also a guiding principle and way of
thinking.

In various ways, moving to apache Infra can be a painful process, and IMO
the loss of all the great mailing list functionality that comes with using
Google Groups is perhaps the most painful step. But basically, the de facto
mailing lists need to be the Apache ones, and not Google Groups. The
underlying reason is that Apache needs to take full accountability for
recording and publishing the mailing lists, it has to be able to
institutionally guarantee this. This is because discussion on mailing lists
is one of the core things that defines an Apache community. So at a minimum
this means Apache owning the master copy of the bits.

All that said, we are discussing the possibility of having a google group
that subscribes to each list that would provide an easier to use and
prettier archive for each list (so far we haven't gotten that to work).

I hope this was helpful. It has taken me a few years now, and a lot of
conversations with experienced (and patient!) Apache mentors, to
internalize some of the nuance about ""the Apache way"". That's why I wanted
to share.

Andy


"
Mike Potts <maspotts@gmail.com>,"Thu, 19 Dec 2013 19:58:09 -0800 (PST)","Re: IMPORTANT: Spark mailing lists moving to Apache by September
 1st",spark-users@googlegroups.com,"Thanks very much for the prompt and comprehensive reply!  I appreciate the 
overarching desire to integrate with apache: I'm very happy to hear that 
there's a move to use the existing groups as mirrors: that will overcome 
all of my objections: particularly if it's bidirectional! :)
"
Aaron Davidson <ilikerps@gmail.com>,"Thu, 19 Dec 2013 20:46:17 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,dev@spark.incubator.apache.org,"I'd be fine with one-way mirrors here (Apache threads being reflected in
Google groups) -- I have no idea how one is supposed to navigate the Apache
list to look for historic threads.



"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 20 Dec 2013 08:10:22 +0200",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","'s related projects is http://search-hadoop.com - managed by sematext. Perhaps we can plead with Otis to add Spark lists to search-spark.com, or the existing site?

Just throwing it out there as a potential solution to at least searching and navigating the Apache lists

Sent from my iPad

e
e

d

e

O
ng
cto
sts
mum
p
ed

al
a of
 to
eel

"
Ted Yu <yuzhihong@gmail.com>,"Thu, 19 Dec 2013 22:17:38 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","You may have noticed that the counter of searchable items for last 7 days on search-Hadoop is 0 and the counter for last 30 days is declining quickly. 

Cheers

e:

t's related projects is http://search-hadoop.com - managed by sematext. Perhaps we can plead with Otis to add Spark lists to search-spark.com, or the existing site?
nd navigating the Apache lists
he
he


t
nd
e
he


f
MO
ing
acto
ists
imum
up

ted

d
nal
ea of
g to
 feel


"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 20 Dec 2013 08:18:13 +0200",Re: Spark development for undergraduate project,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Another option would be:
1. Add another recommendation model based on mrec's sgd based model: https://github.com/mendeley/mrec
2. Look at the streaming K-means from Mahout and see if that might be integrated or adapted into MLlib
3. Work on adding to or refactoring the existing linear model framework, for example adaptive learning rate schedules, adaptive norm stuff from John Langford et al
4. Adding sparse vector/matrix support to MLlib?

Sent from my iPad

ote:

ds
nto. You
o send


"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 19 Dec 2013 22:49:01 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,user@spark.incubator.apache.org,"I've set up two new unofficial google groups to mirror the Apache Spark
user and dev lists:

https://groups.google.com/forum/#!forum/apache-spark-dev-mirror
https://groups.google.com/forum/#!forum/apache-spark-user-mirror

Basically these lists each subscribe to the corresponding Apache list.

They do not allow folks to subscribe directly to them. Getting emails from
the Google Group would offer no advantages that I can think of and we
really want to encourage folks to sign up for the official mailing list
instead.

The lists do allow the public to send email to them, which I think might be
necessary since the ""from:"" field for all emails that get distributed via
the Apache mailing list is set to the author of the email.

I think this might be a great compromise. At least we can try this out and
see how it goes.

Matei, can you confirm that Jan 1 is the date we want to turn off the
existing spark-users google group?

We could consider using the existing spark-developers and spark-users
google groups instead of the two new ones I just created but I think that
it is much more obvious to have the lists include the word mirror in their
names.

The dev list mirror seems to be working, because I see the last couple
emails from this thread in it already. I'll confirm and ensure that the
user list mirror is working too.

Thoughts?

Andy

P.S. Thanks to Patrick for suggesting this to me originally.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 19 Dec 2013 23:09:25 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,spark-users@googlegroups.com,"Yes, I agree that we should close down the existing Google group on Jan 1st. While its more convenient to use, its created confusion. I hope that we can get the ASF to support better search interfaces in the future too. I think we just have to drive this from within.

The Google Group should be a nice way to make the content searchable from the web. We should also see what it takes to make it mirrored on Nabble (http://www.nabble.com). Ive found a lot of information about other projects there, and other Apache projects do use it.

Matei


Spark user and dev lists:
from the Google Group would offer no advantages that I can think of and we really want to encourage folks to sign up for the official mailing list instead.
might be necessary since the ""from:"" field for all emails that get distributed via the Apache mailing list is set to the author of the email.
and see how it goes.
existing spark-users google group?
google groups instead of the two new ones I just created but I think that it is much more obvious to have the lists include the word mirror in their names.
emails from this thread in it already. I'll confirm and ensure that the user list mirror is working too.
in Google groups) -- I have no idea how one is supposed to navigate the Apache list to look for historic threads.
the overarching desire to integrate with apache: I'm very happy to hear that there's a move to use the existing groups as mirrors: that will overcome all of my objections: particularly if it's bidirectional! :)
that list has already be reconfigured so that it no longer allows posting (and bounces emails sent to it).
too (we'll announce a date for that soon).
way. We've had a long conversation with our mentors about this, and I've felt very similar to you, so I'd like to give you background.
the community *fully* over to Apache infrastructure, and more generally the Apache way of organizing the community.
infra, but possibly more importantly, it is also a guiding principle and way of thinking.
IMO the loss of all the great mailing list functionality that comes with using Google Groups is perhaps the most painful step. But basically, the de facto mailing lists need to be the Apache ones, and not Google Groups. The underlying reason is that Apache needs to take full accountability for recording and publishing the mailing lists, it has to be able to institutionally guarantee this. This is because discussion on mailing lists is one of the core things that defines an Apache community. So at a minimum this means Apache owning the master copy of the bits. 
group that subscribes to each list that would provide an easier to use and prettier archive for each list (so far we haven't gotten that to work).
conversations with experienced (and patient!) Apache mentors, to internalize some of the nuance about ""the Apache way"". That's why I wanted to share.
and also activity on the apache mailing list (which is a really horrible experience!).  Is it a firm policy on apache's front to disallow external groups?  I'm going to be ramping up on spark, and I really hate the idea of having to rely on the apache archives and my mail client.  Also: having to search for topics/keywords both in old threads (here) as well as new threads in apache's (clunky) archive, is going to be a pain!  I almost feel like I must be missing something because the current solution seems unfeasibly awkward!
Groups ""Spark Users"" group.
an email to spark-users...@googlegroups.com.
Groups ""Spark Users"" group.
an email to spark-users+unsubscribe@googlegroups.com.

"
Debasish Das <debasish.das83@gmail.com>,"Fri, 20 Dec 2013 00:15:33 -0800",Re: Spark development for undergraduate project,dev@spark.incubator.apache.org,"Decision trees, random forest, professor Hastie's gbdt R package are also
nice to have...

Gbdt 1 algorithm is in 0xdata and that too netflix guys were complaining it
does not scale beyond 1000 trees :)

hn
k
ou
u
e
or
m
tc
n
e
s
needs
d
r,
is
N
k
"
Mike Potts <maspotts@gmail.com>,"Fri, 20 Dec 2013 08:18:10 -0800 (PST)","Re: IMPORTANT: Spark mailing lists moving to Apache by September
 1st",spark-users@googlegroups.com,"This is great!  Could the new *-mirror groups start off with a complete 
copy of the (closed) original groups and the apache lists?  (So as to avoid 
having to search 3 different sources for historical information.)
"
Andy Konwinski <andykonwinski@gmail.com>,"Fri, 20 Dec 2013 08:29:40 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,spark-users@googlegroups.com,"That would be really awesome. I'm not familiar with any Google Groups
functionality that supports that but I'll look.

That's an argument for maybe just changing the names of the existing groups
to something with mirror in them instead of using newly created ones.

"
Mike Potts <maspotts@gmail.com>,"Fri, 20 Dec 2013 08:35:10 -0800 (PST)","Re: IMPORTANT: Spark mailing lists moving to Apache by September
 1st",spark-users@googlegroups.com,"I actually prefer that, but I didn't want my preference to get in the way 
of creating mirror groups, one way or the other :)  (My argument would be 
that since the old groups would be closing anyway, re-purposing them as 
mirrors is fair use: and less work/confusing than creating new *-mirror 
groups instead.)

"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 20 Dec 2013 08:37:23 -0800",Re: Spark 0.8.1 Released,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Awesome as usual Patrick.

Thanks for driving the release as RM =)

- Henry


"
Otis Gospodnetic <otis@sematext.com>,"Fri, 20 Dec 2013 13:08:20 -0500",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,Ted Yu <yuzhihong@gmail.com>,"Hi,

That's a Solr bug Ted, we'll fix in the coming days.

I assume Spark devs would like us to index Spark email, Wiki, JIRA, etc.?

Otis
--
Performance Monitoring * Log Analytics * Search Analytics
Solr & Elasticsearch Support * http://sematext.com/

tel: +1 347 480 1610   fax: +1 718 679 9190




"
Patrick Wendell <pwendell@gmail.com>,"Fri, 20 Dec 2013 11:41:10 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,"""spark-users@googlegroups.com"" <spark-users@googlegroups.com>","Andy and Mike,

I'd also prefer to just convert the old groups into mirrors. That way
people who are still subscribed to them will continue to get e-mails
(and most people on the list are read-only users).

Ideally we'd have the behavior that users who try to e-mail the google
group get a bounce back saying ""this is now a read only mirror"".

That said I have *no idea* of this is possible to set-up nicely within
google groups. I defer to Andy! Having the new mirror groups also
seems like a decent solution as well...

- Patrick


"
Reynold Xin <rxin@databricks.com>,"Fri, 20 Dec 2013 23:36:28 -0800","Akka problem when using scala command to launch Spark applications in
 the current 0.9.0-SNAPSHOT",dev@spark.incubator.apache.org,"It took me hours to debug a problem yesterday on the latest master branch
(0.9.0-SNAPSHOT), and I would like to share with the dev list in case
anybody runs into this Akka problem.

A little background for those of you who haven't followed closely the
development of Spark and YARN 2.2: YARN 2.2 uses protobuf 2.5, and Akka
uses an older version of protobuf that is not binary compatible. In order
to have a single build that is compatible for both YARN 2.2 and pre-2.2
YARN/Hadoop, we published a special version of Akka that builds with
protobuf shaded (i.e. using a different package name for the protobuf
stuff).

However, it turned out Scala 2.10 includes a version of Akka jar in its
default classpath (look at the lib folder in Scala 2.10 binary
distribution). If you use the scala command to launch any Spark application
on the current master branch, there is a pretty high chance that you
wouldn't be able to create the SparkContext (stack trace at the end of the
email). The problem is that the Akka packaged with Scala 2.10 takes
precedence in the classloader over the special Akka version Spark includes.

Before we have a good solution for this, the workaround is to use java to
launch the application instead of scala. All you need to do is to include
the right Scala jars (scala-library and scala-compiler) in the classpath.
Note that the scala command is really just a simple script that calls java
with the right classpath.


Stack trace:

java.lang.NoSuchMethodException:
akka.remote.RemoteActorRefProvider.<init>(java.lang.String,
akka.actor.ActorSystem$Settings, akka.event.EventStream,
akka.actor.Scheduler, akka.actor.DynamicAccess)
at java.lang.Class.getConstructor0(Class.java:2763)
at java.lang.Class.getDeclaredConstructor(Class.java:2021)
at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:77)
at scala.util.Try$.apply(Try.scala:161)
at
akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:74)
at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:85)
at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:85)
at scala.util.Success.flatMap(Try.scala:200)
at
akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:85)
at akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:546)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:79)
at org.apache.spark.SparkEnv$.createFromSystemProperties(SparkEnv.scala:120)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:106)
"
Andy Konwinski <andykonwinski@gmail.com>,"Sun, 22 Dec 2013 14:47:15 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,"""spark-users@googlegroups.com"" <spark-users@googlegroups.com>","Per Matei's suggestion, I've set up two nabble archive lists, one to
archive the apache dev list and one to archive the apache user list.

user list archive: http://apache-spark-user-list.1001560.n3.nabble.com
dev list archive: http://apache-spark-developers-list.1001551.n3.nabble.com

Between these and whatever solution we end up with for the google group
mirrors, we should have decent enough alternatives to reading via the
apache list archives going forward.


ote:

e that
 I
m
d
r
:
che
ing
ve
y the
,
of
th
he de
 The
lists
nimum
 and
.
nted
ble
rnal
dea of
ng to
t feel
d
"
Mike Potts <maspotts@gmail.com>,"Sun, 22 Dec 2013 15:51:29 -0800 (PST)","Re: IMPORTANT: Spark mailing lists moving to Apache by September
 1st",spark-users@googlegroups.com,"Thanks!!  Is is straightforward to pre-populate it with all the existing 
threads from apache?

"
Andy Konwinski <andykonwinski@gmail.com>,"Sun, 22 Dec 2013 16:23:20 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,"""spark-users@googlegroups.com"" <spark-users@googlegroups.com>","Unfortunately, I don't see that as an option for either these nabble lists
or the google groups.

For the google groups, we we could subscribe the old spark google groups to
the apache lists (instead of using the new mirror groups I created).

However the tradeoff is that as far as I can tell, we wouldn't be able to
have posting disabled or even subscriber only for those groups (since the
emails from the apache lists would then get bounced). In this case, I'm
concerned that if it remains possible to post to the google groups, then
people will continue posting to them, which would be confusing to everybody.

We could rename the old groups to make it clear they are the mirrors,
however then the people who are subscribed to them and have rules setup for
dealing with them would have to change their rules.

Finally, we could unsubscribe everybody from the old google groups and then
rename them to have the word mirror in them which would preserve history
and be no worse than if they just became silent and activity moved to the
new mirror groups i created (which as I mentioned, must allow posting by
anybody). So I guess this last option is what I propose we do.

Andy



"
Uri Laserson <laserson@cloudera.com>,"Sun, 22 Dec 2013 18:29:37 -0600",Installing PySpark on a local machine,dev@spark.incubator.apache.org,"Is there a documented/preferred method for installing PySpark on a local
machine?  I want to be able to run a Python interpreter on my local
machine, point it to my Spark cluster and go.  There doesn't appear to be a
setup.py file anywhere, nor is pyspark registered with PyPI.  I'm happy to
contribute these, but want to hear what the preferred method is first.

Uri

-- 
Uri Laserson, PhD
Data Scientist, Cloudera
Twitter/GitHub: @laserson
+1 617 910 0447
laserson@cloudera.com
"
Josh Rosen <rosenville@gmail.com>,"Sun, 22 Dec 2013 17:01:24 -0800",Re: Installing PySpark on a local machine,"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","I've thought about creating a setup.py file for PySpark; there are a couple
of subtleties involved:

   - PySpark's uses Py4J to create a regular Java Spark driver, so it's
   subject to the same limitations that Scala / Java Spark have when
   connecting from a local machine to a remote cluster; a number of ports need
   to be opened (this is discussed in more detail in other posts on this list;
   try searching for ""connect to remote cluster"" or something like that).
   - PySpark needs the Spark assembly JAR, so you'd still have to point the
   SPARK_HOME environment variable to local copy of the Spark assemblies.
   - We need to be careful about communication between incompatible
   versions of the Python and Java portions of the library.  We can probably
   fix this by embedding version numbers in the Python and Java libraries and
   comparing those numbers when launching the Java gateway.

If we decide to distribute a PySpark package on PyPI, we should integrate
its release with the regular Apache release process for Spark.

Does anyone know how other projects like Mesos distribute their Python
bindings?  Is there a good existing model that we should emulate?

- Josh



"
Mike Potts <maspotts@gmail.com>,"Sun, 22 Dec 2013 19:12:01 -0800 (PST)","Re: IMPORTANT: Spark mailing lists moving to Apache by September
 1st",spark-users@googlegroups.com,"Just a thought: if the mirroring was bidirectional (between apache lists 
and google groups) -- meaning that you could read/write via either channel 
and everything would be mirrored in both groups -- would that be a bad 
thing?  The apache lists would be comprehensive and available as a 
first-class communications channel; and so would the google groups: I would 
have thought the benefits of maintaining both channels (fully mirrored) 
would outweigh any drawbacks... but again I'm probably missing something :)

"
Andy Konwinski <andykonwinski@gmail.com>,"Sun, 22 Dec 2013 19:25:00 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,spark-users@googlegroups.com,"It would be a good thing, I'm just not sure how to achieve it, or if it's
possible. AFAIK, you cannot simply subscribe the lists to each other. Have
you heard of a setup like this being used before?

"
Mike Potts <maspotts@gmail.com>,"Sun, 22 Dec 2013 19:28:14 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,spark-users@googlegroups.com,"
Haha, no: Im afraid there my reach exceeds my grasp :(


it's possible. AFAIK, you cannot simply subscribe the lists to each other. Have you heard of a setup like this being used before?
lists and google groups) -- meaning that you could read/write via either channel and everything would be mirrored in both groups -- would that be a bad thing?  The apache lists would be comprehensive and available as a first-class communications channel; and so would the google groups: I would have thought the benefits of maintaining both channels (fully mirrored) would outweigh any drawbacks... but again I'm probably missing something :)
Groups ""Spark Users"" group.
an email to spark-users+unsubscribe@googlegroups.com.
Google Groups ""Spark Users"" group.
https://groups.google.com/d/topic/spark-users/vtg-5db8JWY/unsubscribe.
spark-users+unsubscribe@googlegroups.com.


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 24 Dec 2013 08:27:11 -0800",NullPointerException when running TaskResultGetterSuite,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,
In test output, I saw:

^[[32mTaskResultGetterSuite:^[[0m
^[[32m- handling results smaller than Akka frame size^[[0m
^[[32m- handling results larger than Akka frame size^[[0m
Exception in thread ""SparkListenerBus"" java.lang.NullPointerException
  at
org.apache.spark.ui.jobs.JobProgressListener.onTaskEnd(JobProgressListener.scala:149)
  at
org.apache.spark.scheduler.SparkListenerBus$$anon$2$$anonfun$run$7.apply(SparkListenerBus.scala:55)
  at
org.apache.spark.scheduler.SparkListenerBus$$anon$2$$anonfun$run$7.apply(SparkListenerBus.scala:55)
  at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at
org.apache.spark.scheduler.SparkListenerBus$$anon$2.run(SparkListenerBus.scala:55)

Has anyone else seen similar stack trace ?

Cheers
"
Dachuan Huang <huangda@cse.ohio-state.edu>,"Tue, 24 Dec 2013 12:05:26 -0500",Stackoverflow after a small change by me,dev@spark.incubator.apache.org,"Hello, developers,

Just out of curiosity, I have changed the ""mustCheckpoint"" in
StateDStream.scala to ""false"" by default. And run the
StatefulNetworkWordCount.scala example.

My input is a 3MB/s speed Serversocket.

It reports the following error after some time, the exception trace didn't
say anything about the spark code, so I don't know how to nail down the
root cause, can anybody help me with this? thanks.

Exception in thread ""DAGScheduler"" java.lang.StackOverflowError
at java.io.ObjectStreamClass.getPrimFieldValues(ObjectStreamClass.java:1233)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1532)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at scala.collection.immutable.$colon$colon.writeObject(List.scala:430)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java."
,,,,
"
Evan Chan <ev@ooyala.com>,Tue"," 24 Dec 2013 10:49:25 -0800""",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Andy, at first glance nabble seems great, it allows search plus
posting new topics, so it appears to be bidirectional.    Now just have to
register an account on there.


ote:

ope
om
 (
t
d
at
in
:
e
ar
:
t
t
g
y
d
or
g
of
w
ps
an
ps
an



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Evan Chan <ev@ooyala.com>,"Tue, 24 Dec 2013 10:50:42 -0800","Re: Akka problem when using scala command to launch Spark
 applications in the current 0.9.0-SNAPSHOT","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Reynold,

The default, documented methods of starting Spark all use the assembly jar,
and thus java, right?

-Evan







-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Reynold Xin <rxin@databricks.com>,"Tue, 24 Dec 2013 10:57:22 -0800","Re: Akka problem when using scala command to launch Spark
 applications in the current 0.9.0-SNAPSHOT",dev@spark.incubator.apache.org,"Yup - you are safe if you stick to the official documented method.

A lot of users also use scala for a variety of reasons (e.g. old script)
and that used to work also.



"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 24 Dec 2013 11:33:14 -0800",Re: Stackoverflow after a small change by me,dev@spark.incubator.apache.org,"Hello Dachuan,

RDDs generated by StateDStream are checkpointed because the tree of RDD
dependencies (i.e. the RDD lineage) can grow indefinitely as each state RDD
depends on the state RDD from the previous batch of data. Checkpointing
save an RDD to HDFS to cuts of all ties to its parent RDDs (i.e. truncates
the lineage). If you do not periodically checkpoint of the state RDDs,
these really large lineages can lead to all sorts of problems. The
""mustCheckpoint"" field ensures that state RDDs are automatically
checkpointed with some periodicity even if the user does not explicitly
specify one. Setting mustCheckpoint to false disables this automatic
checkpointing. I think that is leading to really large lineages, and
serializing the RDD with its lineage is causing the stack to overflow.

false? Maybe there is another way of achieving what you are trying to
achieve.

TD



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 24 Dec 2013 12:01:23 -0800",Re: IMPORTANT: Spark mailing lists moving to Apache by September 1st,"""spark-users@googlegroups.com"" <spark-users@googlegroups.com>","Hey Andy - these Nabble groups look great! Thanks for setting them up.



o
n
hope
e
k
t
ht
ed
e
te
)
e:
t
at
ng
nd
,
:
e
ew
s
edin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Dachuan Huang <huangda@cse.ohio-state.edu>,"Tue, 24 Dec 2013 15:34:35 -0500",Re: Stackoverflow after a small change by me,dev@spark.incubator.apache.org,"Yes, your explanation makes sense.

I try to disable the checkpoint and use that as a control group to measure
the overhead of checkpoint. Now I guess I can create a dummy CheckpointRDD
to do this task (a better way?).

Besides, I am curious about where exactly the StackOverflowError happens,
because serialization usually happens under the hood, it's hard to find it.
I have tried this line of code, but it seems this is not the root cause.

serializedTask = Task.serializeWithDependencies(task, sched.sc.addedFiles,
sched.sc.addedJars, ser)


"
Patrick Wendell <patrick@databricks.com>,"Tue, 24 Dec 2013 12:49:47 -0800","Re: Akka problem when using scala command to launch Spark
 applications in the current 0.9.0-SNAPSHOT",Evan Chan <ev@ooyala.com>,"Even,

This problem also exists for people who write their own applications that
depend on/include Spark. E.g. they bundle up their app and then launch the
driver with ""scala -cp my-budle.jar""... I've seen this cause an issue in
that setting.

- Patrick



"
=?koi8-r?B?5qPEz9Ig68/Sz9TLyco=?= <prime@yandex-team.ru>,"Wed, 25 Dec 2013 17:21:48 +0400",Spark graduate project ideas,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,

Currently I’m pursuing a masters degree in CS and I’m in search of my year project theme (in distributed systems field), and Spark seems very interesting to me. 

Can you suggest some problems or ideas to work on? 

By the way, what is the status of external sorting(https://spark-project.atlassian.net/browse/SPARK-983)?

"
Marvin <no-reply@apache.org>,"Wed, 25 Dec 2013 14:15:02 +0000 (UTC)",Incubator PMC/Board report for Jan 2014 ([ppmc]),dev@spark.incubator.apache.org,"

Dear podling,

This email was sent by an automated system on behalf of the Apache Incubator PMC.
It is an initial reminder to give you plenty of time to prepare your quarterly
board report.

The board meeting is scheduled for Wed, 15 January 2014, 10:30:30:00 PST. The report 
for your podling will form a part of the Incubator PMC report. The Incubator PMC 
requires your report to be submitted 2 weeks before the board meeting, to allow 
sufficient time for review and submission (Wed, Jan 1st).

Please submit your report with sufficient time to allow the incubator PMC, and 
subsequently board members to review and digest. Again, the very latest you 
should submit your report is 2 weeks prior to the board meeting.

Thanks,

The Apache Incubator PMC

Submitting your Report
----------------------

Your report should contain the following:

 * Your project name
 * A brief description of your project, which assumes no knowledge of the project
   or necessarily of its field
 * A list of the three most important issues to address in the move towards 
   graduation.
 * Any issues that the Incubator PMC or ASF Board might wish/need to be aware of
 * How has the community developed since the last report
 * How has the project developed since the last report.
 
This should be appended to the Incubator Wiki page at:

  http://wiki.apache.org/incubator/January2014

Note: This manually populated. You may need to wait a little before this page is
      created from a template.

Mentors
-------
Mentors should review reports for their project(s) and sign them off on the 
Incubator wiki page. Signing off reports shows that you are following the 
project - projects that are not signed may raise alarms for the Incubator PMC.

Incubator PMC


"
purav aggarwal <puravaggarwal123@gmail.com>,"Thu, 26 Dec 2013 09:02:56 +0530",Large DataStructure to Broadcast,dev@spark.incubator.apache.org,"Hi all,

I have a large file ( > 5 gigs) which I need to lookup. Since each slave
need to perform the search operation on the hashmap (built out of the file)
in parallel I need to broadcast the file. I was wondering if broadcasting
such a huge file is really a good idea. Do we have any benchmarks for the
broadcast variables. I am on a Standalone cluster and machine configuration
is not a problem at the moment.
Has anyone exploited broadcast to such an extent ?

Thanks,
Purav
"
Mosharaf Chowdhury <mosharafkabir@gmail.com>,"Wed, 25 Dec 2013 19:54:49 -0800",Re: Large DataStructure to Broadcast,dev@spark.incubator.apache.org,"You should try out TorrentBroadcast (NOT BitTorrentBroadcast) from the
0.8.1 branch.
In your config file, set spark.broadcast.factory=
org.apache.spark.broadcast.TorrentBroadcastFactory
It should perform significantly better than HttpBroadcast (some benchmarks
here: https://github.com/apache/incubator-spark/pull/68). I expect a 10X
improvement over the default.
Make sure you have enough memory in slaves.

--
Mosharaf Chowdhury
http://www.mosharaf.com/



"
Christopher Nguyen <ctn@adatao.com>,"Wed, 25 Dec 2013 21:11:15 -0800",Re: Large DataStructure to Broadcast,dev@spark.incubator.apache.org,"Purav, depending on the access pattern you should also consider the
trade-offs of setting up a lookup service (using, e.g., memcached, egad!)
which may end up being more efficient overall.

The general point is not to restrict yourself to only Spark APIs when
considering the overall architecture.
--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 26 Dec 2013 12:33:02 -0800",Option folding idiom,dev@spark.incubator.apache.org,"In code added to Spark over the past several months, I'm glad to see more
use of `foreach`, `for`, `map` and `flatMap` over `Option` instead of
pattern matching boilerplate.  There are opportunities to push `Option`
idioms even further now that we are using Scala 2.10 in master, but I want
to discuss the issue here a little bit before committing code whose form
may be a little unfamiliar to some Spark developers.

In particular, I really like the use of `fold` with `Option` to cleanly an
concisely express the ""do something if the Option is None; do something
else with the thing contained in the Option if it is Some"" code fragment.

An example:

Instead of...

val driver = drivers.find(_.id == driverId)
driver match {
  case Some(d) =>
    if (waitingDrivers.contains(d)) { waitingDrivers -= d }
    else {
      d.worker.foreach { w =>
        w.actor ! KillDriver(driverId)
      }
    }
    val msg = s""Kill request for $driverId submitted""
    logInfo(msg)
    sender ! KillDriverResponse(true, msg)
  case None =>
    val msg = s""Could not find running driver $driverId""
    logWarning(msg)
    sender ! KillDriverResponse(false, msg)
}

...using fold we end up with...

driver.fold
  {
    val msg = s""Could not find running driver $driverId""
    logWarning(msg)
    sender ! KillDriverResponse(false, msg)
  }
  { d =>
    if (waitingDrivers.contains(d)) { waitingDrivers -= d }
    else {
      d.worker.foreach { w =>
        w.actor ! KillDriver(driverId)
      }
    }
    val msg = s""Kill request for $driverId submitted""
    logInfo(msg)
    sender ! KillDriverResponse(true, msg)
  }


So the basic pattern (and my proposed formatting standard) for folding over
an `Option[A]` from which you need to produce a B (which may be Unit if
you're only interested in side effects) is:

anOption.fold
  {
    // something that evaluates to a B if anOption = None
  }
  { a =>
    // something that transforms `a` into a B if anOption = Some(a)
  }


Any thoughts?  Does anyone really, really hate this style of coding and
oppose its use in Spark?
"
"""Michael (Bach) Bui"" <freeman@adatao.com>","Thu, 26 Dec 2013 14:54:40 -0600",Re: Option folding idiom,dev@spark.incubator.apache.org,"+1.

It is a little bit harder to read for new comers but not really a big deal.






"
Christopher Nguyen <ctn@adatao.com>,"Thu, 26 Dec 2013 13:19:14 -0800",Re: Option folding idiom,dev@spark.incubator.apache.org,"+1 as you can't fight the future, but clear warning signs ahead would be
helpful :)

Just be careful that it's not an exact equivalent to *match*, else we can
get confused by behavior like this:


*scala> class parentdefined class parent*


*scala> class "
Evan Chan <ev@ooyala.com>,"Thu, 26 Dec 2013 17:51:07 -0800",Re: Option folding idiom,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for using more functional idioms in general.

That's a pretty clever use of `fold`, but putting the default condition
first there makes it not as intuitive.   What about the following, which
are more readable?

    option.map { a => someFuncMakesB() }
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 26 Dec 2013 18:23:48 -0800",Re: Option folding idiom,dev@spark.incubator.apache.org,"of the accumulator, and provides the expected result of folding over an
empty collection.

scala> val l: List[Int] = List()

l: List[Int] = List()


scala> l.fold(42)(_ + _)

res0: Int = 42


scala> val o: Option[Int] = None

o: Option[Int] = None


scala> o.fold(42)(_ + 1)

res1: Int = 42



"
Reynold Xin <rxin@databricks.com>,"Thu, 26 Dec 2013 17:58:24 -1000",Re: Option folding idiom,dev@spark.incubator.apache.org,"I'm not strongly against Option.fold, but I find the readability getting
worse for the use case you brought up.  For the use case of if/else, I find
Option.fold pretty confusing because it reverses the order of Some vs None.
Also, when code gets long, the lack of an obvious boundary (the only
boundary is ""} {"") with two closures is pretty confusing.



"
Holden Karau <holden@pigscanfly.ca>,"Thu, 26 Dec 2013 20:11:12 -0800",Re: Option folding idiom,dev@spark.incubator.apache.org,"I personally with Evan in that I prefer map with getOrElse over fold with
options (but that just my personal preference) :)






-- 
Cell : 425-233-8271
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 26 Dec 2013 21:12:29 -0800",Re: Option folding idiom,dev@spark.incubator.apache.org,"I agree with what Reynold said -- there's not a big benefit in terms of
lines of code (esp. compared to using getOrElse) and I think it hurts code
that it's very accessible for newcomers -- something that would be less
true with this use of ""fold"".



"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 27 Dec 2013 02:06:15 -0500",Re: Option folding idiom,dev@spark.incubator.apache.org,"I agree about using getOrElse instead. In choosing which code style and idioms to use, my goal has always been to maximize the ease of *other developers* understanding the code, and most developers today still dont know Scala. Its fine to use a maps or matches, because their meaning is obvious, but fold on Option is not obvious (even foreach is kind of weird for new people). In this case the benefit is so small that it doesnt seem worth it.

Note that if you use getOrElse, you can even throw exceptions in the else part if youd like. (This is because Nothing is a subtype of every type in Scala.) So for example you can do val stuff = option.getOrElse(throw new Exception(It wasnt set)). It looks a little weird, but note how the meaning is obvious even if you dont know anything about the type system.

Matei


of
code
is
less
with
getting
I
vs
<mark@clearstorydata.com
over an
instead
whose
Some(a)
coding


"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 26 Dec 2013 23:40:17 -0800 (PST)",Re: Option folding idiom,dev@spark.incubator.apache.org,"+1 for getOrElse


When I was new to Scala I tended to use match almost like if/else statements with Option. These days I try to use map/flatMap instead and use getOrElse extensively and I for one find it very intuitive.




I also agree that the fold syn"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 27 Dec 2013 00:29:48 -0800",Re: Option folding idiom,dev@spark.incubator.apache.org,"Well, you can throw exceptions from within a fold as well:

scala> val o: Option[Int] = None
o: Option[Int] = None

scala> o.fold { throw new Exception(""It wasn't set"") } { _.toString }
java.lang.Exception: It wasn't set
  at $anonfun$1.apply(<console>:9)
  at $anonfun$1.apply(<console>:9)
  at scala.Option.fold(Option.scala:157)
  ... 32 elided

But it looks to me like there's an emerging majority, if not a consensus,
who find clearly expressing the catamorphism not to be very useful or
helpful on its own, so I can certainly make do with other syntactic
constructs that others appreciate more.  That was worth clarifying for me,
so thank you all.



ote:

t
s
eem
 every
little
ing
is
I
m
e
a)
g
"
Christopher Nguyen <ctn@adatao.com>,"Fri, 27 Dec 2013 00:57:20 -0800",Re: Option folding idiom,dev@spark.incubator.apache.org,"I've learned and unlearned enough things to be careful when claiming
something is ""more intuitive"" than another, since it's subject to prior
knowledge. When I first encountered map().getOrElse() it wasn't any more
intuitive than this fold()() syntax. Maybe the ""OrElse"" helps a bit, but
the ""get"" in front of it confuses matters again (it sets one up to expect
two things following, not one). Meanwhile, people coming from
data-structure-folding background would argue that fold()() is more
""intuitive"".

If the choice is among three alternatives (match, map().getOrElse(), and
fold()()), and the goal is intuitively obvious syntax to the broadest
audience, then ""match"" wins by a reasonably good distance, with the latter
two about equal. This tie could be broken by the fact that more people by
now know about getOrElse than fold, crossed with the fact that it probably
isn't on the top of the Spark community's agenda to be avant garde on new
Scala syntax.



--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




se
y
t
s
eem
 every
little
ing
f
s
 I
s
r
d
se
(a)
ng
"
Imran Rashid <imran@quantifind.com>,"Fri, 27 Dec 2013 09:02:37 -0600",Re: Option folding idiom,dev@spark.incubator.apache.org,"I'm also against option.fold (though I wouldn't say I ""really, really hate
this style of coding""), for the readability reasons already mentioned.

I actually find myself pulling back from some scala-isms after having spent
some time with them, for readability / maintainability.





r
y
.
nd
t
 is
rd
of every
a little
s
se
d
ly
al
g,
o
"
"""=?utf-8?B?YW5keS5wZXRyZWxsYUBnbWFpbC5jb20=?="" <andy.petrella@gmail.com>","Fri, 27 Dec 2013 16:23:09 +0100",=?utf-8?B?UmUgOiBPcHRpb24gZm9sZGluZyBpZGlvbQ==?=,"""=?utf-8?B?ZGV2QHNwYXJrLmluY3ViYXRvci5hcGFjaGUub3Jn?="" <dev@spark.incubator.apache.org>,""=?utf-8?B?ZGV2QHNwYXJrLmluY3ViYXRvci5hcGFjaGUub3Jn?="" <dev@spark.incubator.apache.org>","What about cata()? 
* kidding*

Envoyé depuis mon HTC

----- Reply message -----
De : ""Imran Rashid"" <imran@quantifind.com>
Pour : <dev@spark.incubator.apache.org>
Objet : Option folding idiom
Date : ven., déc. 27, 2013 16:02


I'm also against option.fold (though I wouldn't say I ""really, really hate
this style of coding""), for the readability reasons already mentioned.

I actually find myself pulling back from some scala-isms after having spent
some time with them, for readability / maintainability.




On Fri, Dec 27, 2013 at 2:57 AM, Christopher Nguyen <ctn@adatao.com> wrote:

> I've learned and unlearned enough things to be careful when claiming
> something is ""more intuitive"" than another, since it's subject to prior
> knowledge. When I first encountered map().getOrElse() it wasn't any more
> intuitive than this fold()() syntax. Maybe the ""OrElse"" helps a bit, but
> the ""get"" in front of it confuses matters again (it sets one up to expect
> two things following, not one). Meanwhile, people coming from
> data-structure-folding background would argue that fold()() is more
> ""intuitive"".
>
> If the choice is among three alternatives (match, map().getOrElse(), and
> fold()()), and the goal is intuitively obvious syntax to the broadest
> audience, then ""match"" wins by a reasonably good distance, with the latter
> two about equal. This tie could be broken by the fact that more people by
> now know about getOrElse than fold, crossed with the fact that it probably
> isn't on the top of the Spark community's agenda to be avant garde on new
> Scala syntax.
>
>
>
> --
> Christopher T. Nguyen
> Co-founder & CEO, Adatao <http://adatao.com>
> linkedin.com/in/ctnguyen
>
>
>
> On Thu, Dec 26, 2013 at 11:40 PM, Nick Pentreath
> <nick.pentreath@gmail.com>wrote:
>
> > +1 for getOrElse
> >
> >
> > When I was new to Scala I tended to use match almost like if/else
> > statements with Option. These days I try to use map/flatMap instead and
> use
> > getOrElse extensively and I for one find it very intuitive.
> >
> >
> >
> >
> > I also agree that the fold syntax seems way less intuitive and I
> certainly
> > prefer readable Scala code to that which might be more ""idiomatic"" but
> > which I honestly tend to find very inscrutable and hard to grok quickly.
> > —
> > Sent from Mailbox for iPhone
> >
> > On Fri, Dec 27, 2013 at 9:06 AM, Matei Zaharia <matei.zaharia@gmail.com>
> > wrote:
> >
> > > I agree about using getOrElse instead. In choosing which code style and
> > idioms to use, my goal has always been to maximize the ease of *other
> > developers* understanding the code, and most developers today still don’t
> > know Scala. It’s fine to use a maps or matches, because their meaning is
> > obvious, but fold on Option is not obvious (even foreach is kind of weird
> > for new people). In this case the benefit is so small that it doesn’t
> seem
> > worth it.
> > > Note that if you use getOrElse, you can even throw exceptions in the
> > “else” part if you’d like. (This is because Nothing is a subtype of every
> > type in Scala.) So for example you can do val stuff =
> > option.getOrElse(throw new Exception(“It wasn’t set”)). It looks a little
> > weird, but note how the meaning is obvious even if you don’t know
> anything
> > about the type system.
> > > Matei
> > > On Dec 27, 2013, at 12:12 AM, Kay Ousterhout <keo@eecs.berkeley.edu>
> > wrote:
> > >> I agree with what Reynold said -- there's not a big benefit in terms
> of
> > >> lines of code (esp. compared to using getOrElse) and I think it hurts
> > code
> > >> readability.  One of the great things about the current Spark codebase
> > is
> > >> that it's very accessible for newcomers -- something that would be
> less
> > >> true with this use of ""fold"".
> > >>
> > >>
> > >> On Thu, Dec 26, 2013 at 8:11 PM, Holden Karau <holden@pigscanfly.ca>
> > wrote:
> > >>
> > >>> I personally with Evan in that I prefer map with getOrElse over fold
> > with
> > >>> options (but that just my personal preference) :)
> > >>>
> > >>>
> > >>> On Thu, Dec 26, 2013 at 7:58 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > >>>
> > >>>> I'm not strongly against Option.fold, but I find the readability
> > getting
> > >>>> worse for the use case you brought up.  For the use case of
> if/else, I
> > >>> find
> > >>>> Option.fold pretty confusing because it reverses the order of Some
> vs
> > >>> None.
> > >>>> Also, when code gets long, the lack of an obvious boundary (the only
> > >>>> boundary is ""} {"") with two closures is pretty confusing.
> > >>>>
> > >>>>
> > >>>> On Thu, Dec 26, 2013 at 4:23 PM, Mark Hamstra <
> > mark@clearstorydata.com
> > >>>>> wrote:
> > >>>>
> > >>>>> On the contrary, it is the completely natural place for the initial
> > >>> value
> > >>>>> of the accumulator, and provides the expected result of folding
> over
> > an
> > >>>>> empty collection.
> > >>>>>
> > >>>>> scala> val l: List[Int] = List()
> > >>>>>
> > >>>>> l: List[Int] = List()
> > >>>>>
> > >>>>>
> > >>>>> scala> l.fold(42)(_ + _)
> > >>>>>
> > >>>>> res0: Int = 42
> > >>>>>
> > >>>>>
> > >>>>> scala> val o: Option[Int] = None
> > >>>>>
> > >>>>> o: Option[Int] = None
> > >>>>>
> > >>>>>
> > >>>>> scala> o.fold(42)(_ + 1)
> > >>>>>
> > >>>>> res1: Int = 42
> > >>>>>
> > >>>>>
> > >>>>> On Thu, Dec 26, 2013 at 5:51 PM, Evan Chan <ev@ooyala.com> wrote:
> > >>>>>
> > >>>>>> +1 for using more functional idioms in general.
> > >>>>>>
> > >>>>>> That's a pretty clever use of `fold`, but putting the default
> > >>> condition
> > >>>>>> first there makes it not as intuitive.   What about the following,
> > >>>> which
> > >>>>>> are more readable?
> > >>>>>>
> > >>>>>>    option.map { a => someFuncMakesB() }
> > >>>>>>              .getOrElse(b)
> > >>>>>>
> > >>>>>>    option.map { a => someFuncMakesB() }
> > >>>>>>              .orElse { a => otherDefaultB() }.get
> > >>>>>>
> > >>>>>>
> > >>>>>> On Thu, Dec 26, 2013 at 12:33 PM, Mark Hamstra <
> > >>>> mark@clearstorydata.com
> > >>>>>>> wrote:
> > >>>>>>
> > >>>>>>> In code added to Spark over the past several months, I'm glad to
> > >>> see
> > >>>>> more
> > >>>>>>> use of `foreach`, `for`, `map` and `flatMap` over `Option`
> instead
> > >>> of
> > >>>>>>> pattern matching boilerplate.  There are opportunities to push
> > >>>> `Option`
> > >>>>>>> idioms even further now that we are using Scala 2.10 in master,
> > >>> but I
> > >>>>>> want
> > >>>>>>> to discuss the issue here a little bit before committing code
> whose
> > >>>>> form
> > >>>>>>> may be a little unfamiliar to some Spark developers.
> > >>>>>>>
> > >>>>>>> In particular, I really like the use of `fold` with `Option` to
> > >>>> cleanly
> > >>>>>> an
> > >>>>>>> concisely express the ""do something if the Option is None; do
> > >>>> something
> > >>>>>>> else with the thing contained in the Option if it is Some"" code
> > >>>>> fragment.
> > >>>>>>>
> > >>>>>>> An example:
> > >>>>>>>
> > >>>>>>> Instead of...
> > >>>>>>>
> > >>>>>>> val driver = drivers.find(_.id == driverId)
> > >>>>>>> driver match {
> > >>>>>>>  case Some(d) =>
> > >>>>>>>    if (waitingDrivers.contains(d)) { waitingDrivers -= d }
> > >>>>>>>    else {
> > >>>>>>>      d.worker.foreach { w =>
> > >>>>>>>        w.actor ! KillDriver(driverId)
> > >>>>>>>      }
> > >>>>>>>    }
> > >>>>>>>    val msg = s""Kill request for $driverId submitted""
> > >>>>>>>    logInfo(msg)
> > >>>>>>>    sender ! KillDriverResponse(true, msg)
> > >>>>>>>  case None =>
> > >>>>>>>    val msg = s""Could not find running driver $driverId""
> > >>>>>>>    logWarning(msg)
> > >>>>>>>    sender ! KillDriverResponse(false, msg)
> > >>>>>>> }
> > >>>>>>>
> > >>>>>>> ...using fold we end up with...
> > >>>>>>>
> > >>>>>>> driver.fold
> > >>>>>>>  {
> > >>>>>>>    val msg = s""Could not find running driver $driverId""
> > >>>>>>>    logWarning(msg)
> > >>>>>>>    sender ! KillDriverResponse(false, msg)
> > >>>>>>>  }
> > >>>>>>>  { d =>
> > >>>>>>>    if (waitingDrivers.contains(d)) { waitingDrivers -= d }
> > >>>>>>>    else {
> > >>>>>>>      d.worker.foreach { w =>
> > >>>>>>>        w.actor ! KillDriver(driverId)
> > >>>>>>>      }
> > >>>>>>>    }
> > >>>>>>>    val msg = s""Kill request for $driverId submitted""
> > >>>>>>>    logInfo(msg)
> > >>>>>>>    sender ! KillDriverResponse(true, msg)
> > >>>>>>>  }
> > >>>>>>>
> > >>>>>>>
> > >>>>>>> So the basic pattern (and my proposed formatting standard) for
> > >>>> folding
> > >>>>>> over
> > >>>>>>> an `Option[A]` from which you need to produce a B (which may be
> > >>> Unit
> > >>>> if
> > >>>>>>> you're only interested in side effects) is:
> > >>>>>>>
> > >>>>>>> anOption.fold
> > >>>>>>>  {
> > >>>>>>>    // something that evaluates to a B if anOption = None
> > >>>>>>>  }
> > >>>>>>>  { a =>
> > >>>>>>>    // something that transforms `a` into a B if anOption =
> Some(a)
> > >>>>>>>  }
> > >>>>>>>
> > >>>>>>>
> > >>>>>>> Any thoughts?  Does anyone really, really hate this style of
> coding
> > >>>> and
> > >>>>>>> oppose its use in Spark?
> > >>>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>> --
> > >>>>>> --
> > >>>>>> Evan Chan
> > >>>>>> Staff Engineer
> > >>>>>> ev@ooyala.com  |
> > >>>>>>
> > >>>>>> <http://www.ooyala.com/>
> > >>>>>> <http://www.facebook.com/ooyala><
> > >>>> http://www.linkedin.com/company/ooyala
> > >>>>>> <
> > >>>>>> http://www.twitter.com/ooyala>
> > >>>>>>
> > >>>>>
> > >>>>
> > >>>
> > >>>
> > >>>
> > >>> --
> > >>> Cell : 425-233-8271
> > >>>
> >
>
"
Evan Chan <ev@ooyala.com>,"Fri, 27 Dec 2013 23:11:17 -0800",scala-graph,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","http://www.scala-graph.org/

Have you guys seen the above site?  I wonder if this will ever be
merged into the Scala standard library, but might be interesting to
see if this fits into GraphX at all, or to add a Spark backend to it.

-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Ted Yu <yuzhihong@gmail.com>,"Sat, 28 Dec 2013 09:00:55 -0800",test suite results in OOME,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,
I used the following setting to run test suite:
export MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=812M
-XX:ReservedCodeCacheSize=512m""

I got:

[ERROR] [12/28/2013 08:34:03.747]
[sparkWorker1-akka.actor.default-dispatcher-14] [ActorSystem(sparkWorker1)]
Uncaught fatal error from thread
[sparkWorker1-akka.actor.default-dispatcher-14] shutting down ActorSystem
[sparkWorker1]
java.lang.OutOfMemoryError: PermGen space

How do I run test suite on Mac ?

Thanks
"
Reynold Xin <rxin@databricks.com>,"Sat, 28 Dec 2013 13:13:28 -1000",Re: test suite results in OOME,dev@spark.incubator.apache.org,"I usually use sbt. i.e. sbt/sbt test





"
Ted Yu <yuzhihong@gmail.com>,"Sat, 28 Dec 2013 16:04:08 -0800",Re: test suite results in OOME,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Build Tools slide from Matei's slides, transition toward maven only is
happening.

That was why I used mvn.

BTW I specified the following on the commandline:
-Dtest=TaskResultGetterSuite

Many other test suites were run.

How can I run one suite ?

Thanks



"
Hao Lin <hlin09pu@gmail.com>,"Sun, 29 Dec 2013 01:12:13 -0500",Systematically performance diagnose,dev@spark.incubator.apache.org,"Hi folks,

I am trying to test the performance on a couple of my Spark applications.
For benchmarking purpose, I am wondering if there is a good performance
analysis practice. The best way I can think of is to instrument log prints
and analyze the timestamps in logs on each node.

The major metrics I am interested in are computation ratios (computation
time, data transferring time, basically a timeline of detailed events),
memory usage, disk throughput. Could I have some suggestions on how Spark
is benchmarked.

Thanks,

Max
"
Reynold Xin <rxin@databricks.com>,"Mon, 30 Dec 2013 19:38:02 -1000",Re: test suite results in OOME,dev@spark.incubator.apache.org,"Again, I usually use sbt ...

sbt/sbt ""test-only *TaskResultGetterSuite*""



"
Reynold Xin <rxin@databricks.com>,"Mon, 30 Dec 2013 19:40:18 -1000",Re: Systematically performance diagnose,dev@spark.incubator.apache.org,"The application web ui is pretty useful. We have been adding more and more
information to the web ui for easier performance analysis.

Look at Patrick Wendell's two talks at the Spark Summit for more
information: http://spark-summit.org/summit-2013/



"
Reynold Xin <rxin@databricks.com>,"Tue, 31 Dec 2013 14:31:54 -0800",Re: Spark graduate project ideas,dev@spark.incubator.apache.org,"There is a recent discussion on academic projects on Spark.

Take a look at the replies to that email (unfortunately you have to dig
through the archive to find the replies):
http://mail-archives.apache.org/mod_mbox/spark-dev/201312.mbox/%3CCAHH8_ON-2y69fBfVtt6pngWtEPOZdsmvt4hZ=dOE-DZSK6k3sA@mail.gmail.com%3E




r
"
