Seth Hendrickson <seth.hendrickson16@gmail.com>,"Tue, 31 Jan 2017 16:15:54 -0800",Re: MLlib mission and goals,Joseph Bradley <joseph@databricks.com>,"I agree with what Sean said about not supporting arbitrarily many
algorithms. I think the goal of MLlib should be to support only core
algorithms for machine learning. Ideally Spark ML provides a relatively
small set of algorithms that are heavily optimized, and also provides a
framework that makes it easy for users to extend and build their own
packages and algos when they need to. Spark ML is already quite good for
this. We have of course been doing a lot of work migrating to this new API,
and now that we are approaching full parity, it would be good to shift the
focus to performance as others have noted. Supporting a few algorithms that
perform very well is significantly better than supporting many algorithms
with moderate performance, IMO.

I also think a more complete, optimized distributed linear algebra library
would be a great asset, but it may be a more long term goal. A performance
framework for regression testing would be great, but keeping it up to date
is difficult.

Thanks for kicking this thread off Joseph!


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 1 Feb 2017 08:20:34 -0700 (MST)",Re: Spark SQL Dataframe resulting from an except( ) is unusable,dev@spark.apache.org,"
Hi Vinayak,

Thanks for reporting this.

I don't think it is left out intentionally for UserDefinedType. If you
already know how the UDT is represented in internal format, you can
explicitly convert the UDT column to other SQL types, then you may get
around this problem. It is a bit hacky, anyway.

I submitted a PR to fix this, but not sure if it will get in the master
soon.


vijoshi wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Sam Elamin <hussam.elamin@gmail.com>,"Wed, 1 Feb 2017 23:25:56 +0000",Structured Streaming Schema Issue,dev <dev@spark.apache.org>,"Hi All

I am writing a bigquery connector here
<http://github.com/samelamin/spark-bigquery> and I am getting a strange
error with schemas being overwritten when a dataframe is passed over to the
Sink


for example the source returns this StructType
WARN streaming.BigQuerySource:
StructType(StructField(customerid,LongType,true),

and the sink is recieving this StructType
WARN streaming.BigQuerySink:
StructType(StructField(customerid,StringType,true)


Any idea why this might be happening?
I dont have infering schema on

spark.conf.set(""spark.sql.streaming.schemaInference"", ""false"")

I know its off by default but I set it just to be sure

So completely lost to what could be causing this

Regards
Sam
"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 1 Feb 2017 15:29:07 -0800",Re: Structured Streaming Schema Issue,Sam Elamin <hussam.elamin@gmail.com>,"You should make sure that schema of the streaming Dataset returned by
`readStream`, and the schema of the DataFrame returned by the sources
getBatch.


"
Sam Elamin <hussam.elamin@gmail.com>,"Wed, 1 Feb 2017 23:33:00 +0000",Re: Structured Streaming Schema Issue,Tathagata Das <tathagata.das1565@gmail.com>,"Thanks for the quick response TD!

Ive been trying to identify where exactly this transformation happens

The readStream returns a dataframe with the correct schema

The minute I call writeStream, by the time I get to the addBatch method,
the dataframe there has an incorrect Schema

So Im skeptical about the issue being prior to the readStream since the
output dataframe has the correct Schema


Am I missing something completely obvious?

Regards
Sam


"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 1 Feb 2017 15:40:47 -0800",Re: Structured Streaming Schema Issue,Sam Elamin <hussam.elamin@gmail.com>,"I am assuming that you have written your own BigQuerySource (i dont see
that code in the link you posted). In that source, you must have
implemented getBatch which uses offsets to return the Dataframe having the
data of a batch. Can you double check when this DataFrame returned by
getBatch, has the expected schema?


"
Sam Elamin <hussam.elamin@gmail.com>,"Wed, 1 Feb 2017 23:49:32 +0000",Re: Structured Streaming Schema Issue,Tathagata Das <tathagata.das1565@gmail.com>,"Yeah sorry Im still working on it, its on a branch you can find here
<https://github.com/samelamin/spark-bigquery/blob/Stream_reading/src/main/scala/com/samelamin/spark/bigquery/streaming/BigQuerySource.scala>,
ignore the logging messages I was trying to workout how the APIs work and
unfortunately because I have to shade the dependency I cant debug it in an
IDE (that I know of! )

So I can see the correct schema here
<https://github.com/samelamin/spark-bigquery/blob/Stream_reading/src/main/scala/com/samelamin/spark/bigquery/streaming/BigQuerySource.scala#L64>
and
also when the df is returned(After .load() )

But when that same df has writeStream applied to it, the addBatch dataframe
has a new schema. Its similar to the old schema but some ints have been
turned to strings.




"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 1 Feb 2017 16:01:30 -0800",Re: Structured Streaming Schema Issue,Sam Elamin <hussam.elamin@gmail.com>,"What is the query you are apply writeStream on? Essentially can you print
the whole query.

Also, you can do StreamingQuery.explain() to see in full details how the
logical plan changes to physical plan, for a batch of data. that might
help. try doing that with some other sink to make sure the source works
correctly, and then try using your sink.

If you want further debugging, then you will have to dig into the
StreamingExecution class in Spark, and debug stuff there.
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L523


"
Sam Elamin <hussam.elamin@gmail.com>,"Thu, 02 Feb 2017 00:04:11 +0000",Re: Structured Streaming Schema Issue,Tathagata Das <tathagata.das1565@gmail.com>,"There isn't a query per se.im writing the entire dataframe from the output
query aspect


I'll do a bit more digging. Thank you very much for your help. Structued
streaming is very exciting and I really am enjoying writing a connector for
it!

Regards
Sam

"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 1 Feb 2017 21:47:21 -0700 (MST)",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"
Hi Maciej,

Basically the fitting algorithm in Pipeline is an iterative operation.
Running iterative algorithm on Dataset would have RDD lineages and query
plans that grow fast. Without cache and checkpoint, it gets slower when the
iteration number increases.

I think it is why when you run a Pipeline with long stages, it gets much
longer time to finish. As I think it is not uncommon to have long stages in
a Pipeline, we should improve this. I will submit a PR for this.


zero323 wrote






-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 1 Feb 2017 23:22:35 -0700 (MST)",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"
Hi Maciej,

FYI, the PR is at https://github.com/apache/spark/pull/16775.


Liang-Chi Hsieh wrote






-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 1 Feb 2017 23:48:24 -0700 (MST)",Re: approx_percentile computation,dev@spark.apache.org,"
Hi,

You don't need to run approxPercentile against a list. Since it is an
aggregation function, you can simply run:

// Just for illustrate the idea.
val approxPercentile = new ApproximatePercentile(v1, Literal(percentage))
val agg_approx_percentile = Column(approxPercentile.toAggregateExpression())

df.groupBy (k1, k2, k3).agg(collect_list(v1), agg_approx_percentile)



Rishi wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 02 Feb 2017 07:38:20 +0000",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"Hi Maciej

If you're seeing a regression from 1.6 -> 2.0 *both using DataFrames *then
that seems to point to some other underlying issue as the root cause.

Even though adding checkpointing should help, we should understand why it's
different between 1.6 and 2.0?



"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 2 Feb 2017 01:32:01 -0700 (MST)",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"
Thanks Nick for pointing it out. I totally agreed.

In 1.6 codebase, actually Pipeline uses DataFrame instead of Dataset,
because they are not merged yet in 1.6.

this would deserialize the rows.

In 1.6, as they use DataFrame, there is no extra cost for deserialization.

I think this would cause some regression. As Maciej didn't show how much
performance regression observed, I can't judge if this is the root cause for
it. But this is the initial idea after I check 1.6 and current Pipeline.



Nick Pentreath wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 2 Feb 2017 14:16:48 +0100",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"Hi Liang-Chi,

Thank you for your answer and PR but what I think I wasn't specific
enough. In hindsight I should have illustrate this better. What really
troubles me here is a pattern of growing delays. Difference between
1.6.3 (roughly 20s runtime since the first job):


1.6 timeline

vs 2.1.0 (45 minutes or so in a bad case):

2.1.0 timeline

The code is just an example and it is intentionally dumb. You easily
mask this with caching, or using significantly larger data sets. So I
guess the question I am really interested in is - what changed between
1.6.3 and 2.x (this is more or less consistent across 2.0, 2.1 and
current master) to cause this and more important, is it a feature or is
it a bug? I admit, I choose a lazy path here, and didn't spend much time
(yet) trying to dig deeper.

I can see a bit higher memory usage, a bit more intensive GC activity,
but nothing I would really blame for this behavior, and duration of
individual jobs is comparable with some favor of 2.x. Neither
fitting in 1.6 and, as far as I can tell, they still do that in 2.x. And
the problem doesn't look that related to the data processing part in the
first place.



-- 
Maciej Szymkiewicz

"
Sam Elamin <hussam.elamin@gmail.com>,"Thu, 2 Feb 2017 15:30:38 +0000",Re: Structured Streaming Schema Issue,Tathagata Das <tathagata.das1565@gmail.com>,"Hi All

Ive done a bit more digging to where exactly this happens. It seems like
the schema is infered again after the data leaves the source and then comes
into the sink

Below is a stack trace, the schema at the BigQuerySource has a LongType for
customer id but then at the sink, the data received has an incorrect schema

where exactly is the data stored in between these steps? Shouldnt the sink
call the Schema method from Source?

If it helps I want to clarify that I am not passing in the schema when i
initialise the readStream, but I am overriding the sourceSchema in my class
that extends StreamSourceProvider. But even when that method is called the
schema is correct

p.s Ignore the ""production"" bucket, thats just a storage bucket I am using,
not actually using structured streaming in production just yet :)


17/02/02 14:57:22 WARN streaming.BigQuerySource:
StructType(StructField(customerid,LongType,true),
StructField(id,StringType,true), StructField(legacyorderid,LongType,true),
StructField(notifiycustomer,BooleanType,true), StructField(ordercontainer,
StructType(StructField(applicationinfo,StructType(
StructField(applicationname,StringType,true), StructField(
applicationversion,StringType,true), StructField(clientip,StringType,true),
StructField(jefeature,StringType,true),
StructField(useragent,StringType,true)),true),
StructField(basketinfo,StructType(StructField(basketid,StringType,true),
StructField(deliverycharge,FloatType,true),
StructField(discount,FloatType,true),
StructField(discounts,StringType,true), StructField(items,StructType(
StructField(combinedprice,FloatType,true),
StructField(description,StringType,true),
StructField(discounts,StringType,true), StructField(mealparts,StringType,true),
StructField(menucardnumber,StringType,true),
StructField(multibuydiscounts,StringType,true),
StructField(name,StringType,true),
StructField(optionalaccessories,StringType,true),
StructField(productid,LongType,true), StructField(producttypeid,LongType,true),
StructField(requiredaccessories,StructType(StructField(groupid,LongType,true),
StructField(name,StringType,true),
StructField(requiredaccessoryid,LongType,true),
StructField(unitprice,FloatType,true)),true),
StructField(synonym,StringType,true),
StructField(unitprice,FloatType,true)),true),
StructField(menuid,LongType,true),
StructField(multibuydiscount,FloatType,true),
StructField(subtotal,FloatType,true),
StructField(tospend,FloatType,true), StructField(total,FloatType,true)),true),
StructField(customerinfo,StructType(StructField(address,StringType,true),
StructField(city,StringType,true), StructField(email,StringType,true),
StructField(id,StringType,true), StructField(name,StringType,true),
StructField(phonenumber,StringType,true),
StructField(postcode,StringType,true),
StructField(previousjeordercount,LongType,true), StructField(
previousrestuarantordercount,LongType,true),
StructField(timezone,StringType,true)),true),
StructField(id,StringType,true), StructField(islocked,BooleanType,true),
StructField(legacyid,LongType,true), StructField(order,StructType(
StructField(duedate,StringType,true),
StructField(duedatewithutcoffset,StringType,true),
StructField(initialduedate,StringType,true), StructField(
initialduedatewithutcoffset,StringType,true),
StructField(notetorestaurant,StringType,true),
StructField(orderable,BooleanType,true),
StructField(placeddate,StringType,true),
StructField(promptasap,BooleanType,true),
StructField(servicetype,StringType,true)),true),
StructField(paymentinfo,StructType(StructField(
drivertipvalue,FloatType,true), StructField(orderid,StringType,true),
StructField(paiddate,StringType,true), StructField(paymentlines,
StructType(StructField(cardfee,FloatType,true),
StructField(cardtype,StringType,true),
StructField(paymenttransactionref,StringType,true),
StructField(pspname,StringType,true),
StructField(type,StringType,true), StructField(value,FloatType,true)),true),
StructField(total,FloatType,true),
StructField(totalcomplementary,FloatType,true)),true),
StructField(restaurantinfo,StructType(StructField(addresslines,StringType,true),
StructField(city,StringType,true), StructField(dispatchmethod,StringType,true),
StructField(id,StringType,true), StructField(latitude,FloatType,true),
StructField(longitude,FloatType,true), StructField(name,StringType,true),
StructField(offline,BooleanType,true),
StructField(phonenumber,StringType,true),
StructField(postcode,StringType,true), StructField(seoname,StringType,true),
StructField(tempoffline,BooleanType,true)),true)),true),
StructField(orderid,StringType,true),
StructField(orderresolutionstatus,StringType,true),
StructField(raisingcomponent,StringType,true),
StructField(restaurantid,LongType,true),
StructField(tenant,StringType,true), StructField(timestamp,StringType,true))
17/02/02 14:57:23 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0
on 127.0.0.1:51379 in memory (size: 4.8 KB, free: 366.3 MB)
17/02/02 14:57:23 INFO spark.ContextCleaner: Cleaned accumulator 66
17/02/02 14:57:23 INFO codegen.CodeGenerator: Code generated in 194.977984
ms
17/02/02 14:57:23 INFO codegen.CodeGenerator: Code generated in 10.736863 ms
17/02/02 14:57:23 INFO spark.SparkContext: Starting job: start at
EventStreamer.scala:61
17/02/02 14:57:23 INFO scheduler.DAGScheduler: Registering RDD 6 (start at
EventStreamer.scala:61)
17/02/02 14:57:23 INFO scheduler.DAGScheduler: Got job 1 (start at
EventStreamer.scala:61) with 1 output partitions
17/02/02 14:57:23 INFO scheduler.DAGScheduler: Final stage: ResultStage 2
(start at EventStreamer.scala:61)
17/02/02 14:57:23 INFO scheduler.DAGScheduler: Parents of final stage:
List(ShuffleMapStage 1)
17/02/02 14:57:23 INFO scheduler.DAGScheduler: Missing parents:
List(ShuffleMapStage 1)
17/02/02 14:57:23 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1
(MapPartitionsRDD[6] at start at EventStreamer.scala:61), which has no
missing parents
17/02/02 14:57:23 INFO memory.MemoryStore: Block broadcast_2 stored as
values in memory (estimated size 17.3 KB, free 366.0 MB)
17/02/02 14:57:23 INFO memory.MemoryStore: Block broadcast_2_piece0 stored
as bytes in memory (estimated size 9.6 KB, free 366.0 MB)
17/02/02 14:57:23 INFO storage.BlockManagerInfo: Added broadcast_2_piece0
in memory on 127.0.0.1:51379 (size: 9.6 KB, free: 366.3 MB)
17/02/02 14:57:23 INFO spark.SparkContext: Created broadcast 2 from
broadcast at DAGScheduler.scala:1012
17/02/02 14:57:23 INFO scheduler.DAGScheduler: Submitting 2 missing tasks
from ShuffleMapStage 1 (MapPartitionsRDD[6] at start at
EventStreamer.scala:61)
17/02/02 14:57:23 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
with 2 tasks
17/02/02 14:57:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage
1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 6444 bytes)
17/02/02 14:57:23 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
1.0 (TID 3, localhost, partition 1, PROCESS_LOCAL, 6444 bytes)
17/02/02 14:57:23 INFO executor.Executor: Running task 0.0 in stage 1.0
(TID 2)
17/02/02 14:57:23 INFO executor.Executor: Running task 1.0 in stage 1.0
(TID 3)
17/02/02 14:57:23 INFO rdd.NewHadoopRDD: Input split:
gs://je-bi-production/hadoop/tmp/bigquery/job_201702021456_0000/shard-1/data-*.json[3
estimated records]
17/02/02 14:57:23 INFO rdd.NewHadoopRDD: Input split:
gs://je-bi-production/hadoop/tmp/bigquery/job_201702021456_0000/shard-0/data-*.json[3
estimated records]
17/02/02 14:57:23 INFO bigquery.DynamicFileListRecordReader: Initializing
DynamicFileListRecordReader with split 'InputSplit:: length:3 locations: []
toString(): gs://je-bi-production/hadoop/tmp/bigquery/job_201702021456_0000/shard-1/data-*.json[3
estimated records]', task context 'TaskAttemptContext::
TaskAttemptID:attempt_201702021456_0000_m_000001_0 Status:'
17/02/02 14:57:23 INFO bigquery.DynamicFileListRecordReader: Initializing
DynamicFileListRecordReader with split 'InputSplit:: length:3 locations: []
toString(): gs://je-bi-production/hadoop/tmp/bigquery/job_201702021456_0000/shard-0/data-*.json[3
estimated records]', task context 'TaskAttemptContext::
TaskAttemptID:attempt_201702021456_0000_m_000000_0 Status:'
17/02/02 14:57:23 INFO codegen.CodeGenerator: Code generated in 92.93187 ms
17/02/02 14:57:23 INFO bigquery.DynamicFileListRecordReader: Adding new
file 'data-000000000000.json' of size 0 to knownFileSet.
17/02/02 14:57:23 INFO bigquery.DynamicFileListRecordReader: Moving to next
file 'gs://je-bi-production/hadoop/tmp/bigquery/job_201702021456_
0000/shard-1/data-000000000000.json' which has 0 bytes. Records read so
far: 0
17/02/02 14:57:23 INFO bigquery.DynamicFileListRecordReader: Adding new
file 'data-000000000000.json' of size 20424 to knownFileSet.
17/02/02 14:57:23 INFO bigquery.DynamicFileListRecordReader: Adding new
file 'data-000000000001.json' of size 0 to knownFileSet.
17/02/02 14:57:23 INFO bigquery.DynamicFileListRecordReader: Moving to next
file 'gs://je-bi-production/hadoop/tmp/bigquery/job_201702021456_
0000/shard-0/data-000000000000.json' which has 20424 bytes. Records read so
far: 0
17/02/02 14:57:24 INFO gcsio.GoogleCloudStorageReadChannel: Got 'range not
satisfiable' for reading gs://je-bi-production/hadoop/
tmp/bigquery/job_201702021456_0000/shard-1/data-000000000000.json at
position 0; assuming empty.
17/02/02 14:57:24 INFO bigquery.DynamicFileListRecordReader: Found
end-marker file 'data-000000000000.json' with index 0
17/02/02 14:57:24 INFO executor.Executor: Finished task 1.0 in stage 1.0
(TID 3). 1723 bytes result sent to driver
17/02/02 14:57:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage
1.0 (TID 3) in 844 ms on localhost (1/2)
17/02/02 14:57:24 INFO bigquery.DynamicFileListRecordReader: Moving to next
file 'gs://je-bi-production/hadoop/tmp/bigquery/job_201702021456_
0000/shard-0/data-000000000001.json' which has 0 bytes. Records read so
far: 6
17/02/02 14:57:24 INFO gcsio.GoogleCloudStorageReadChannel: Got 'range not
satisfiable' for reading gs://je-bi-production/hadoop/
tmp/bigquery/job_201702021456_0000/shard-0/data-000000000001.json at
position 0; assuming empty.
17/02/02 14:57:24 INFO bigquery.DynamicFileListRecordReader: Found
end-marker file 'data-000000000001.json' with index 1
17/02/02 14:57:24 INFO executor.Executor: Finished task 0.0 in stage 1.0
(TID 2). 1717 bytes result sent to driver
17/02/02 14:57:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
1.0 (TID 2) in 1344 ms on localhost (2/2)
17/02/02 14:57:24 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0,
whose tasks have all completed, from pool
17/02/02 14:57:24 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (start at
EventStreamer.scala:61) finished in 1.344 s
17/02/02 14:57:24 INFO scheduler.DAGScheduler: looking for newly runnable
stages
17/02/02 14:57:24 INFO scheduler.DAGScheduler: running: Set()
17/02/02 14:57:24 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
17/02/02 14:57:24 INFO scheduler.DAGScheduler: failed: Set()
17/02/02 14:57:24 INFO scheduler.DAGScheduler: Submitting ResultStage 2
(MapPartitionsRDD[9] at start at EventStreamer.scala:61), which has no
missing parents
17/02/02 14:57:24 INFO memory.MemoryStore: Block broadcast_3 stored as
values in memory (estimated size 7.0 KB, free 366.0 MB)
17/02/02 14:57:24 INFO memory.MemoryStore: Block broadcast_3_piece0 stored
as bytes in memory (estimated size 3.7 KB, free 366.0 MB)
17/02/02 14:57:24 INFO storage.BlockManagerInfo: Added broadcast_3_piece0
in memory on 127.0.0.1:51379 (size: 3.7 KB, free: 366.3 MB)
17/02/02 14:57:24 INFO spark.SparkContext: Created broadcast 3 from
broadcast at DAGScheduler.scala:1012
17/02/02 14:57:24 INFO scheduler.DAGScheduler: Submitting 1 missing tasks
from ResultStage 2 (MapPartitionsRDD[9] at start at EventStreamer.scala:61)
17/02/02 14:57:24 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0
with 1 tasks
17/02/02 14:57:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage
2.0 (TID 4, localhost, partition 0, ANY, 6233 bytes)
17/02/02 14:57:24 INFO executor.Executor: Running task 0.0 in stage 2.0
(TID 4)
17/02/02 14:57:24 INFO storage.ShuffleBlockFetcherIterator: Getting 2
non-empty blocks out of 2 blocks
17/02/02 14:57:24 INFO storage.ShuffleBlockFetcherIterator: Started 0
remote fetches in 5 ms
17/02/02 14:57:24 INFO executor.Executor: Finished task 0.0 in stage 2.0
(TID 4). 1873 bytes result sent to driver
17/02/02 14:57:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
2.0 (TID 4) in 37 ms on localhost (1/1)
17/02/02 14:57:24 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0,
whose tasks have all completed, from pool
17/02/02 14:57:24 INFO scheduler.DAGScheduler: ResultStage 2 (start at
EventStreamer.scala:61) finished in 0.037 s
17/02/02 14:57:24 INFO scheduler.DAGScheduler: Job 1 finished: start at
EventStreamer.scala:61, took 1.420357 s
17/02/02 14:57:24 INFO codegen.CodeGenerator: Code generated in 7.668964 ms
17/02/02 14:57:24 WARN streaming.BigQuerySink: ************ saving schema
is set to
17/02/02 14:57:24 WARN streaming.BigQuerySink:
StructType(StructField(customerid,StringType,true),
StructField(id,StringType,true), StructField(legacyorderid,StringType,true),
StructField(notifiycustomer,BooleanType,true), StructField(ordercontainer,
StructType(StructField(applicationinfo,StructType(
StructField(applicationname,StringType,true), StructField(
applicationversion,StringType,true), StructField(clientip,StringType,true),
StructField(jefeature,StringType,true),
StructField(useragent,StringType,true)),true),
StructField(basketinfo,StructType(StructField(basketid,StringType,true),
StructField(deliverycharge,LongType,true), StructField(discount,LongType,true),
StructField(discounts,ArrayType(StringType,true),true),
StructField(items,ArrayType(StructType(StructField(
combinedprice,DoubleType,true), StructField(description,StringType,true),
StructField(discounts,ArrayType(StringType,true),true),
StructField(mealparts,ArrayType(StringType,true),true),
StructField(menucardnumber,StringType,true), StructField(multibuydiscounts,
ArrayType(StringType,true),true), StructField(name,StringType,true),
StructField(optionalaccessories,ArrayType(StringType,true),true),
StructField(productid,StringType,true),
StructField(producttypeid,StringType,true),
StructField(requiredaccessories,ArrayType(StructType(StructField(groupid,StringType,true),
StructField(name,StringType,true),
StructField(requiredaccessoryid,StringType,true),
StructField(unitprice,LongType,true)),true),true),
StructField(synonym,StringType,true),
StructField(unitprice,DoubleType,true)),true),true),
StructField(menuid,StringType,true),
StructField(multibuydiscount,LongType,true),
StructField(subtotal,DoubleType,true), StructField(tospend,LongType,true),
StructField(total,DoubleType,true)),true), StructField(customerinfo,
StructType(StructField(address,StringType,true),
StructField(city,StringType,true), StructField(email,StringType,true),
StructField(id,StringType,true), StructField(name,StringType,true),
StructField(phonenumber,StringType,true),
StructField(postcode,StringType,true),
StructField(previousjeordercount,StringType,true), StructField(
previousrestuarantordercount,StringType,true),
StructField(timezone,StringType,true)),true),
StructField(id,StringType,true), StructField(islocked,BooleanType,true),
StructField(legacyid,StringType,true), StructField(order,StructType(
StructField(duedate,StringType,true),
StructField(duedatewithutcoffset,StringType,true),
StructField(initialduedate,StringType,true), StructField(
initialduedatewithutcoffset,StringType,true),
StructField(notetorestaurant,StringType,true),
StructField(orderable,BooleanType,true),
StructField(placeddate,StringType,true),
StructField(promptasap,BooleanType,true),
StructField(servicetype,StringType,true)),true),
StructField(paymentinfo,StructType(StructField(drivertipvalue,LongType,true),
StructField(orderid,StringType,true), StructField(paiddate,StringType,true),
StructField(paymentlines,ArrayType(StructType(StructField(cardfee,DoubleType,true),
StructField(cardtype,StringType,true),
StructField(paymenttransactionref,StringType,true),
StructField(pspname,StringType,true), StructField(type,StringType,true),
StructField(value,DoubleType,true)),true),true),
StructField(total,DoubleType,true),
StructField(totalcomplementary,LongType,true)),true),
StructField(restaurantinfo,StructType(StructField(addresslines,ArrayType(StringType,true),true),
StructField(city,StringType,true), StructField(dispatchmethod,StringType,true),
StructField(id,StringType,true), StructField(latitude,DoubleType,true),
StructField(longitude,DoubleType,true), StructField(name,StringType,true),
StructField(offline,BooleanType,true),
StructField(phonenumber,StringType,true),
StructField(postcode,StringType,true), StructField(seoname,StringType,true),
StructField(tempoffline,BooleanType,true)),true)),true),
StructField(orderid,StringType,true),
StructField(orderresolutionstatus,StringType,true),
StructField(raisingcomponent,StringType,true),
StructField(restaurantid,StringType,true),
StructField(tenant,StringType,true), StructField(timestamp,StringType,true))



"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 2 Feb 2017 08:52:14 -0700 (MST)",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"
Hi Maciej,

Thanks for the info you provided.

I tried to run the same example with 1.6 and current branch and record the
difference between the time cost on preparing the executed plan.

Current branch:

292 ms                                                                             
95 ms                             
57 ms
34 ms
128 ms
120 ms
63 ms
106 ms
179 ms
159 ms
235 ms
260 ms
334 ms
464 ms
547 ms                             
719 ms
942 ms
1130 ms
1928 ms
1751 ms
2159 ms                            
2767 ms
3333 ms
4175 ms
5106 ms
6269 ms
7683 ms
9210 ms
10931 ms
13237 ms
15651 ms
19222 ms
23841 ms
26135 ms
31299 ms
38437 ms
47392 ms
51420 ms
60285 ms
69840 ms
74294 ms

1.6:

3 ms
4 ms
10 ms
4 ms
17 ms
8 ms
12 ms
21 ms
15 ms
15 ms
19 ms
23 ms
28 ms
28 ms
58 ms
39 ms
43 ms
61 ms
56 ms
60 ms
81 ms
73 ms
100 ms
91 ms
96 ms
116 ms
111 ms
140 ms
127 ms
142 ms
148 ms
165 ms
171 ms
198 ms
200 ms
233 ms
237 ms
253 ms
256 ms
271 ms
292 ms
452 ms

Although they both take more time after each iteration due to the grown
query plan, it is obvious that current branch takes much more time than 1.6
branch. The optimizer and query planning in current branch is much more
complicated than 1.6.


zero323 wrote







-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Gabi Cristache <gabi.cristache@gmail.com>,"Thu, 2 Feb 2017 21:05:22 +0200",Apache Spark Contribution,dev@spark.apache.org,"Hello,

My name is Gabriel Cristache and I am a student in my final year of a
Computer Engineering/Science University. I want for my Bachelor Thesis to
add support for dynamic scaling to a spark streaming application.


*The goal of the project is to develop an algorithm that automatically
scales the cluster up and down based on the volume of data processed by the
application.*

*You will need to balance between quick reaction to traffic spikes (scale
up) and avoiding wasted resources (scale down) by implementing something
along the lines of a PID algorithm.*



 Do you think this is feasible? And if so are there any hints that you
could give me that would help my objective?


Thanks,

Gabriel Cristache
"
Scott walent <scottwalent@gmail.com>,"Thu, 02 Feb 2017 22:28:58 +0000",4 days left to submit your abstract to Spark Summit SF,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","We are just 4 days away from closing the CFP for Spark Summit 2017.

We have expanded the tracks in SF to include sessions that focus on AI,
Machine Learning and a 60 min deep dive track with technical demos.

Submit your presentation today and join us for the 10th Spark Summit!
Hurry, the CFP closes on February 6th!

https://spark-summit.org/2017/call-for-presentations/
"
StanZhai <mail@zhaishidan.cn>,"Thu, 2 Feb 2017 20:40:28 -0700 (MST)","Re: Executors exceed maximum memory defined with
 `--executor-memory` in Spark 2.1.0",dev@spark.apache.org,"CentOS 7.1,
Linux version 3.10.0-229.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc
version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) ) #1 SMP Fri Mar 6 11:36:42
UTC 2015


Michael Allman-2 wrote











--

---------------------------------------------------------------------


"
Shuai Lin <linshuai2012@gmail.com>,"Fri, 3 Feb 2017 11:43:59 +0800",Re: Apache Spark Contribution,Gabi Cristache <gabi.cristache@gmail.com>,"

By ""scale the cluster up and down"" do you mean:

1) adding/removing spark executors based on the load? How is that from the
dynamic resource allocation that is already supported by spark?

2) Or do you mean scaling the number of servers in a cluster (e.g. like AWS
autoscaling)? If so I'm afraid it's out of the scope of spark itself.

Regards,
Shuai


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 3 Feb 2017 13:18:47 +0900",Re: Oracle JDBC - Spark SQL - Key Not Found: Scale,ayan guha <guha.ayan@gmail.com>,"-user +dev
cc: xiao

Hi, ayan,

I made pr to fix the issue that your reported though, it seems all the
releases I checked (e.g., v1.6, v2.0, v2.1)
does not hit the issue. Could you described more about your environments
and conditions?

You first reported"
Liang-Chi Hsieh <viirya@gmail.com>,"Fri, 3 Feb 2017 00:07:25 -0700 (MST)",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"
Hi Maciej,

After looking into the details of the time spent on preparing the executed
plan, the cause of the significant difference between 1.6 and current
codebase when running the example, is the optimization process to generate
constraints.

There seems few operations in generating constraints which are not
optimized. Plus the fact the query plan grows continuously, the time spent
on generating constraints increases more and more.

I am trying to reduce the time cost. Although not as low as 1.6 because we
can't remove the process of generating constraints, it is significantly
lower than current codebase (74294 ms -> 2573 ms).

385 ms
107 ms
46 ms
58 ms
64 ms
105 ms
86 ms
122 ms
115 ms
114 ms
100 ms
109 ms
169 ms
196 ms
174 ms
212 ms
290 ms
254 ms
318 ms
405 ms
347 ms
443 ms
432 ms
500 ms
544 ms
619 ms
697 ms
683 ms
807 ms
802 ms
960 ms
1010 ms
1155 ms
1251 ms
1298 ms
1388 ms
1503 ms
1613 ms
2279 ms
2349 ms
2573 ms



Liang-Chi Hsieh wrote







-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Fri, 3 Feb 2017 00:34:20 -0700 (MST)",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"
Hi Maciej,

FYI, this fix is submitted at https://github.com/apache/spark/pull/16785.


Liang-Chi Hsieh wrote







-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Fri, 3 Feb 2017 10:24:23 +0100",Re: [SQL][ML] Pipeline performance regression between 1.6 and 2.x,dev@spark.apache.org,"Hi Liang-Chi,

Thank you for the updates. This looks promising.



---------------------------------------------------------------------


"
"""=?gb18030?B?wtyyt8u/s7S3uQ==?="" <1427357147@qq.com>","Fri, 3 Feb 2017 14:34:33 +0800",Re: No Reducer scenarios,"""=?gb18030?B?gTeoNCBSIE5haXIgKIEx0TaBMdIxgTHTMYEx0jKBMc0wgTHOOYEx0TYggTHQOIEx0zCBMdE1gTHRNik=?="" <ravishankar.nair@gmail.com>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>, ""=?gb18030?B?dXNlcg==?="" <user@hadoop.apache.org>, ""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>","HI  Nair,
have you know the class please? I tried to find but failed. I know NewDirectOutputCollector is used to write tmp files.


---Original---
From: ""74 R Nair (16111112101916 18101516)""<ravishankar.nair@gmail.com>
Date: 2017/1/30 13:32:04
To: ""dev""<dev@spark.apache.org>;""user""<user@hadoop.apache.org>;""user""<user@spark.apache.org>;
Subject: No Reducer scenarios


Dear all,



1) When we don't set the reducer class in driver program, IdentityReducer is invoked.


2) When we set setNumReduceTasks(0), no reducer, even IdentityReducer is invoked.


Now, in the second scenario, we observed that the output is part-m-xx format(instead of part-r-xx format) , which shows the map output. But we know that the output of Map is always written to intermediate local file system. So who/which class is responsible for taking these intermediate Map outputs from local file system and writes to HDFS ? Does this particular class performs this write operation only when setNumReduceTasks is set to zero?


Best, Ravion"
Sean Owen <sowen@cloudera.com>,"Fri, 03 Feb 2017 11:52:30 +0000",Remove support for Hadoop 2.5 and earlier?,dev <dev@spark.apache.org>,"Last year we discussed removing support for things like Hadoop 2.5 and
earlier. It was deprecated in Spark 2.1.0. I'd like to go ahead with this,
so am checking whether anyone has strong feelings about it.

The original rationale for separate Hadoop profile was bridging the
significant difference between Hadoop 1 and 2, and the moderate differences
between 2.0 alpha, 2.1 beta, and 2.2 final. 2.2 is really the ""stable""
Hadoop 2, and releases from there to current are comparatively very similar
from Spark's perspective. We nevertheless continued to make a separate
build profile for every minor release, which isn't serving much purpose.

The argument here is mostly that it will simplify code a little bit (less
reflection, fewer profiles), simplify the build -- we now have 6 profiles x
2 build systems x 4 major branches in Jenkins, whereas master could go down
to 2 profiles.

Realistically, I don't know how much we'd do to support Hadoop before 2.6
anyway. Any distro user is long since on 2.6+.

Would this cause anyone significant pain? if so, let's talk about when it
would be realistic to remove this, when does that change.
"
Praveen Mothkuri <Praveen.Mothkuri@kpit.com>,"Fri, 3 Feb 2017 11:46:25 +0000",RE: No Reducer scenarios,"=?utf-8?B?4pi8IFIgTmFpciAo4KSw4KS14KS/4KS24KSC4KSV4KSwIOCkqOCkvuCkrw==?=
 =?utf-8?B?4KSwKQ==?= <ravishankar.nair@gmail.com>, dev
	<dev@spark.apache.org>, user <user@hadoop.apache.org>, user
	<user@spark.apache.org>","In this case the output of the map-tasks directly go to distributed file-system, to the path set by FileOutputFormat.setOutputPath(JobConf, Path)<https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/FileOutputFormat.html#setOutputPath(org.apache.hadoop.mapred.JobConf,%20org.apache.hadoop.fs.Path)>. Also, the framework doesn't sort the map-outputs before writing it out to HDFS.
From: Praveen Mothkuri
Sent: Friday, February 03, 2017 5:14 PM
To: '涓楗';  R Nair (啶班さ啶苦ざ啶啶啶 啶ㄠぞ啶啶); dev; user; user
Subject: RE: No Reducer scenarios


In this case the output of the map-tasks directly go to distributed file-system, to the path set by FileOutputFormat.setOutputPath(JobConf, Path)<https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/FileOutputFormat.html#setOutputPath(org.apache.hadoop.mapred.JobConf,%20org.apache.hadoop.fs.Path)>. Also, the framework doesn't sort the map-outputs before writing it out to HDFS.


From: 涓楗 [mailto:1427357147@qq.com]
Sent: Friday, February 03, 2017 12:05 PM
To:  R Nair (啶班さ啶苦ざ啶啶啶 啶ㄠぞ啶啶); dev; user; user
Subject: Re: No Reducer scenarios

HI  Nair,
have you know the class please? I tried to find but failed. I know NewDirectOutputCollector is used to write tmp files.
---Original---
From: "" R Nair (啶班さ啶苦ざ啶啶啶 啶ㄠぞ啶啶)""<ravishankar.nair@gmail.com<mailto:ravishankar.nair@gmail.com>>
Date: 2017/1/30 13:32:04
To: ""dev""<dev@spark.apache.org<mailto:dev@spark.apache.org>>;""user""<user@hadoop.apache.org<mailto:user@hadoop.apacheche.org>>;
Subject: No Reducer scenarios

Dear all,


1) When we don't set the reducer class in driver program, IdentityReducer is invoked.

2) When we set setNumReduceTasks(0), no reducer, even IdentityReducer is invoked.

Now, in the second scenario, we observed that the output is part-m-xx format(instead of part-r-xx format) , which shows the map output. But we know that the output of Map is always written to intermediate local file system. So who/which class is responsible for taking these intermediate Map outputs from local file system and writes to HDFS ? Does this particular class performs this write operation only when setNumReduceTasks is set to zero?

Best, Ravion
This message contains information that may be privileged or confidential and is the property of the KPIT Technologies Ltd. It is intended only for the person to whom it is addressed. If you are not the intended recipient, you are not authorized to read, print, retain copy, disseminate, distribute, or use this message or any part thereof. If you receive this message in error, please notify the sender immediately and delete all copies of this message. KPIT Technologies Ltd. does not accept any liability for virus infected mails.
"
Sam Elamin <hussam.elamin@gmail.com>,"Fri, 03 Feb 2017 13:27:20 +0000",Re: Structured Streaming Schema Issue,Tathagata Das <tathagata.das1565@gmail.com>,"Hey td


I figured out what was happening

My source would return the correct schema but the schema on the returned df
was actually different. I'm loading json data from cloud storage and that
gets infered instead of set

So basically the schema I return on the source provider wasn't actually
being used

Thanks again for your help. Out of interest is there a way of debugging
this on an ide? What do you recommend because I've been adding far too many
debug statements just to understand what's happening!

Regards
Sam

"
Steve Loughran <stevel@hortonworks.com>,"Fri, 3 Feb 2017 13:40:21 +0000",Re: Remove support for Hadoop 2.5 and earlier?,Sean Owen <sowen@cloudera.com>,"
rlier. It was deprecated in Spark 2.1.0. I'd like to go ahead with this, so am checking whether anyone has strong feelings about it.
ficant difference between Hadoop 1 and 2, and the moderate differences between 2.0 alpha, 2.1 beta, and 2.2 final. 2.2 is really the ""stable"" Hadoop 2, and releases from there to current are comparatively very similar from Spark's perspective. We nevertheless continued to make a separate build profile for every minor release, which isn't serving much purpose.
 reflection, fewer profiles), simplify the build -- we now have 6 profiles x 2 build systems x 4 major branches in Jenkins, whereas master could go down to 2 profiles. 
 anyway. Any distro user is long since on 2.6+.

Hadoop 2.5 doesnt work properly on Java 7, so support for it is kind of implicitly false. indeed, Hadoop 2.6 only works on Java 7 if you disable kerberos, which isn't something I'd recommend in a shared physical cluster, though you may be able to get away with in an ephemeral one where you lock down all the ports.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 3 Feb 2017 21:42:02 +0100",Fwd: Google Summer of Code 2017 is coming,dev <dev@spark.apache.org>,"Hi,

Is this something Spark considering? Would be nice to mark issues as
GSoC in JIRA and solicit feedback. What do you think?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------- Forwarded message ----------
From: Ulrich St盲rk <uli@apache.org>
Date: Fri, Feb 3, 2017 at 8:50 PM
Subject: Google Summer of Code 2017 is coming
To: mentors@community.apache.org


Hello PMCs (incubator Mentors, please forward this email to your podlings),

Google Summer of Code [1] is a program sponsored by Google allowing
students to spend their summer
working on open source software. Students will receive stipends for
developing open source software
full-time for three months. Projects will provide mentoring and
project ideas, and in return have
the chance to get new code developed and - most importantly - to
identify and bring in new committers.

The ASF will apply as a participating organization meaning individual
projects don't have to apply
separately.

If you want to participate with your project we ask you to do the
following things as soon as
possible but by no later than 2017-02-09:

1. understand what it means to be a mentor [2].

2. record your project ideas.

Just create issues in JIRA, label them with gsoc2017, and they will
show up at [3]. Please be as
specific as possible when describing your idea. Include the
programming language, the tools and
skills required, but try not to scare potential students away. They
are supposed to learn what's
required before the program starts.

Use labels, e.g. for the programming language (java, c, c++, erlang,
python, brainfuck, ...) or
technology area (cloud, xml, web, foo, bar, ...) and record them at [5].

Please use the COMDEV JIRA project for recording your ideas if your
project doesn't use JIRA (e.g.
httpd, ooo). Contact dev@community.apache.org if you need assistance.

[4] contains some additional information (will be updated for 2017 shortly).

3. subscribe to mentors@community.apache.org; restricted to potential
mentors, meant to be used as a
private list - general discussions on the public
dev@community.apache.org list as much as possible
please). Use a recognized address when subscribing (@apache.org or one
of your alias addresses on
record).

Note that the ASF isn't accepted as a participating organization yet,
nevertheless you *have to*
start recording your ideas now or we will not get accepted.

Over the years we were able to complete hundreds of projects
successfully. Some of our prior
students are active contributors now! Let's make this year a success again!

Cheers,

Uli

P.S.: Except for the private parts (label spreadsheet mostly), this
email is free to be shared
publicly if you want to.

[1] https://summerofcode.withgoogle.com/
[2] http://community.apache.org/guide-to-being-a-mentor.html
[3] http://s.apache.org/gsoc2017ideas
[4] http://community.apache.org/gsoc.html
[5] http://s.apache.org/gsoclabels

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 3 Feb 2017 20:49:02 +0000",Re: Apache Spark Contribution,Gabi Cristache <gabi.cristache@gmail.com>,"You might want to look at Nephele: Efficient Parallel Data Processing in the Cloud, Warneke & Kao, 2009

http://stratosphere.eu/assets/papers/Nephele_09.pdf

This was some of the work done in the research project with gave birth to Flink, though this bit didn't surface as they chose to leave VM allocation to others.

essentially: the query planner could track allocations and lifespans of work, know that if a VM were to be released, pick the one closest to its our being up, let you choose between fast but expensive vs slow but (maybe) less expensive, etc, etc.

It's a complex problem, as to do it you need to think about more than just spot load, more ""how to efficiently divide work amongst a pool of machines with different lifespans""

what could be good to look at today would be rather than hard code the logic

-provide metrics information which higher level tools could use to make decisions/send hints down
-maybe schedule things to best support pre-emptible nodes in the cluster; the ones where you bid spot prices for from EC2, get 1 hour guaranteed, then after they can be killed without warning.

preemption-aware scheduling might imply making sure that any critical information is kept out the preemptible nodes, or at least replicated onto a long-lived one, and have stuff in the controller ready to react to unannounced pre-emption. FWIW when YARN preempts you do get notified, and maybe even some very early warning. I don't know if spark uses that.

There is some support in HDFS for declaring that some nodes have interdependent failures, ""failure domains"", so you could use that to have HDFS handle replication and only store 1 copy on preemptible VMs, leaving only the scheduling and recovery problem.

Finally, YARN container resizing: lets you ask for more resources when busy, release them when idle. This may be good for CPU load, though memory management isn't something programs can ever handle


Hello,

My name is Gabriel Cristache and I am a student in my final year of a Computer Engineering/Science University. I want for my Bachelor Thesis to add support for dynamic scaling to a spark streaming application.

The goal of the project is to develop an algorithm that automatically scales the cluster up and down based on the volume of data processed by the application.
You will need to balance between quick reaction to traffic spikes (scale up) and avoiding wasted resources (scale down) by implementing something along the lines of a PID algorithm.


 Do you think this is feasible? And if so are there any hints that you could give me that would help my objective?

Thanks,
Gabriel Cristache

"
Jacek Laskowski <jacek@japila.pl>,"Fri, 3 Feb 2017 22:28:09 +0100",Re: Remove support for Hadoop 2.5 and earlier?,Sean Owen <sowen@cloudera.com>,"Hi Sean,

Given that 3.0.0 is coming, removing the unused versions would be a
huge benefit from maintenance point of view. I'd support removing
support for 2.5 and earlier.

Speaking of Hadoop support, is anyone considering 3.0.0 support? Can't
find any JIRA for this.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 3 Feb 2017 22:37:07 +0100","Re: Executors exceed maximum memory defined with `--executor-memory`
 in Spark 2.1.0",StanZhai <mail@zhaishidan.cn>,"Hi,

Just to throw few zlotys to the conversation, I believe that Spark
Standalone does not enforce any memory checks to limit or even kill
executors beyond requested memory (like YARN). I also found that
memory does not have much of use while scheduling tasks and CPU
matters only.

My understanding of `spark.memory.offHeap.enabled` is `false` is that
it does not disable off heap memory used in Java NIO for buffers in
shuffling, RPC, etc. so the memory is always (?) more than you request
for mx using executor-memory.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Fri, 3 Feb 2017 13:56:54 -0800",Re: Google Summer of Code 2017 is coming,Jacek Laskowski <jacek@japila.pl>,"As someone who did GSoC back in University I think this could be a good
idea if there is enough interest from the PMC & I'd be willing the help
mentor if that is a bottleneck.


),
n!


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Sean Owen <sowen@cloudera.com>,"Fri, 03 Feb 2017 22:38:02 +0000",Re: Google Summer of Code 2017 is coming,"Holden Karau <holden@pigscanfly.ca>, Jacek Laskowski <jacek@japila.pl>","I have a contrarian opinion on GSoC from experience many years ago in
Mahout. Of 3 students I interacted with, 2 didn't come close to completing
the work they signed up for. I think it's mostly that students are hungry
for the resum茅 line item, and don't understand the amount of work they're
proposing, and ultimately have little incentive to complete their proposal.
The stipend is small.

I can appreciate the goal of GSoC but it makes more sense for projects that
don't get as much attention, and Spark gets plenty. I would not expect
students to be able to be net contributors to a project like Spark. The
time they consume in hand-holding will exceed the time it would take for
someone experienced to just do the work. I would caution anyone from
agreeing to this for Spark unless they are willing to devote 5-10 hours per
week for the summer to helping someone learn.

My net experience with GSoC is negative, mostly on account of the
participants.


),
n!
"
Jacek Laskowski <jacek@japila.pl>,"Sat, 4 Feb 2017 00:19:53 +0100",Re: Google Summer of Code 2017 is coming,Sean Owen <sowen@cloudera.com>,"Thanks Sean. You've again been very helpful to put the right tone to
the matters. I stand corrected and have no interest in GSoC anymore.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


g
ey're
l.
at
ime
ne
the
:
.

---------------------------------------------------------------------


"
StanZhai <mail@zhaishidan.cn>,"Sat, 4 Feb 2017 00:15:19 -0700 (MST)","[SQL]A confusing NullPointerException when creating table using
 Spark2.1.0",dev@spark.apache.org,"Hi all,

After upgrading our Spark from 1.6.2 to 2.1.0, I encounter a confusing
NullPointerException when creating table under Spark 2.1.0, but the problem
does not exists in Spark 1.6.1.

Environment: Hive 1.2.1, Hadoop 2.6.4

==================== Code ====================
// spark is an instance of HiveContext
// merge is a Hive UDF
val df = spark.sql(""SELECT merge(field_a, null) AS new_a, field_b AS new_b
FROM tb_1 group by field_a, field_b"")
df.createTempView(""tb_temp"")
spark.sql(""create table tb_result stored as parquet as "" +
  ""SELECT new_a"" +
  ""FROM tb_temp"" +
  ""LEFT JOIN `tb_2` ON "" +
  ""if(((`tb_temp`.`new_b`) = '' OR (`tb_temp`.`new_b`) IS NULL),
concat('GrLSRwZE_', cast((rand() * 200) AS int)), (`tb_temp`.`new_b`)) =
`tb_2`.`fka6862f17`"")

==================== Physical Plan ====================
*Project [new_a]
+- *BroadcastHashJoin [if (((new_b = ) || isnull(new_b))) concat(GrLSRwZE_,
cast(cast((_nondeterministic * 200.0) as int) as string)) else new_b],
[fka6862f17], LeftOuter, BuildRight
   :- HashAggregate(keys=[field_a, field_b], functions=[], output=[new_a,
new_b, _nondeterministic])
   :  +- Exchange(coordinator ) hashpartitioning(field_a, field_b, 180),
coordinator[target post-shuffle partition size: 1024880]
   :     +- *HashAggregate(keys=[field_a, field_b], functions=[],
output=[field_a, field_b])
   :        +- *FileScan parquet bdp.tb_1[field_a,field_b] Batched: true,
Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_1,
PartitionFilters: [], PushedFilters: [], ReadSchema: struct
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string,
true]))
      +- *Project [fka6862f17]
         +- *FileScan parquet bdp.tb_2[fka6862f17] Batched: true, Format:
Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_2,
PartitionFilters: [], PushedFilters: [], ReadSchema: struct

What does '*' mean before HashAggregate?

==================== Exception ====================
org.apache.spark.SparkException: Task failed while writing rows
...
java.lang.NullPointerException
	at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_2$(Unknown
Source)
	at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown
Source)
	at
org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$3.apply(AggregationIterator.scala:260)
	at
org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$3.apply(AggregationIterator.scala:259)
	at
org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.next(TungstenAggregationIterator.scala:392)
	at
org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.next(TungstenAggregationIterator.scala:79)
	at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown
Source)
	at
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at
org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:252)
	at
org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:199)
	at
org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:197)
	at
org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341)
	at
org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:202)
	at
org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$4.apply(FileFormatWriter.scala:138)
	at
org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$4.apply(FileFormatWriter.scala:137)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

I also found that when I changed my code as follow:

spark.sql(""create table tb_result stored as parquet as "" +
  ""SELECT new_b"" +
  ""FROM tb_temp"" +
  ""LEFT JOIN `tb_2` ON "" +
  ""if(((`tb_temp`.`new_b`) = '' OR (`tb_temp`.`new_b`) IS NULL),
concat('GrLSRwZE_', cast((rand() * 200) AS int)), (`tb_temp`.`new_b`)) =
`tb_2`.`fka6862f17`"")

or

spark.sql(""create table tb_result stored as parquet as "" +
  ""SELECT new_a"" +
  ""FROM tb_temp"" +
  ""LEFT JOIN `tb_2` ON "" +
  ""if(((`tb_temp`.`new_b`) = '' OR (`tb_temp`.`new_b`) IS NULL),
concat('GrLSRwZE_', cast((200) AS int)), (`tb_temp`.`new_b`)) =
`tb_2`.`fka6862f17`"")

will not have this problem.

== Physical Plan of select new_b ... ==
*Project [new_b]
+- *BroadcastHashJoin [if (((new_b = ) || isnull(new_b))) concat(GrLSRwZE_,
cast(cast((_nondeterministic * 200.0) as int) as string)) else new_b],
[fka6862f17], LeftOuter, BuildRight
   :- *HashAggregate(keys=[field_a, field_b], functions=[], output=[new_b,
_nondeterministic])
   :  +- Exchange(coordinator ) hashpartitioning(field_a, field_b, 180),
coordinator[target post-shuffle partition size: 1024880]
   :     +- *HashAggregate(keys=[field_a, field_b], functions=[],
output=[field_a, field_b])
   :        +- *FileScan parquet bdp.tb_1[field_a,field_b] Batched: true,
Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_1,
PartitionFilters: [], PushedFilters: [], ReadSchema: struct
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string,
true]))
      +- *Project [fka6862f17]
         +- *FileScan parquet bdp.tb_2[fka6862f17] Batched: true, Format:
Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_2,
PartitionFilters: [], PushedFilters: [], ReadSchema: struct

Difference is `HashAggregate(keys=[field_a, field_b], functions=[],
output=[new_b, _nondeterministic])` has a '*' char before it.

It looks like something wrong with WholeStageCodegen when combine HiveUDF +
rand().

How can I solve this problem?

Any help is greatly appreicated! 

Best, 
Stan



--

---------------------------------------------------------------------


"
leo9r <lezcano.leo@gmail.com>,"Sat, 4 Feb 2017 02:11:14 -0700 (MST)","How to checkpoint and RDD after a stage and before reaching an
 action?",dev@spark.apache.org,"Hi,

I have a 1-action job (saveAsObjectFile at the end), that includes several
like to checkpoint rdd1 right before the join to improve the stability of
the job. However, what I'm seeing is that the job gets executed all the way
to the end (saveAsObjectFile) without doing any checkpointing, and then
re-runing the computation to checkpoint rdd1 (when I see the files saved to
the checkpoint directory). I have no issue with recomputing, given that I'm
not caching rdd1, but the fact that the checkpointing of rdd1 happens after
the join brings no benefit because the whole DAG is executed in one piece
and the job fails. If that is actually what is happening, what would be the
best approach to solve this? 
What I'm currently doing is to manually save rdd1 to HDFS right after the
filter in line (4) and then load it back right before the join in line (11).
That prevents the job from failing by splitting it into 2 jobs (ie. 2
actions). My expectations was that rdd1.checkpoint in line (8) was going to
have the same effect but without the hassle of manually saving and loading
intermediate files.

///////////////////////////////////////////////

(1)   val rdd1 = loadData1
(2)     .map
(3)     .groupByKey
(4)     .filter
(5)
(6)   val rdd2 = loadData2
(7)
(8)   rdd1.checkpoint()
(9)
(10)  rdd1
(11)    .join(rdd2)
(12)    .saveAsObjectFile(...)

/////////////////////////////////////////////

Thanks in advance,
Leo



--

---------------------------------------------------------------------


"
Sam Elamin <hussam.elamin@gmail.com>,"Sat, 4 Feb 2017 13:46:00 +0000",specifing schema on dataframe,"dev <dev@spark.apache.org>, user@spark.apache.org","Hi All

I would like to specify a schema when reading from a json but when trying
to map a number to a Double it fails, I tried FloatType and IntType with no
joy!


When inferring the schema customer id is set to String, and I would like to
cast it as Double

so df1 is corrupted while df2 shows


Also FYI I need this to be generic as I would like to apply it to any json,
I specified the below schema as an example of the issue I am facing

import org.apache.spark.sql.types.{BinaryType, StringType,
StructField, DoubleType,FloatType, StructType, LongType,DecimalType}
val testSchema = StructType(Array(StructField(""customerid"",DoubleType)))
val df1 = spark.read.schema(testSchema).json(sc.parallelize(Array(""""""{""customerid"":""535137""}"""""")))
val df2 = spark.read.json(sc.parallelize(Array(""""""{""customerid"":""535137""}"""""")))
df1.show(1)
df2.show(1)


Any help would be appreciated, I am sure I am missing something obvious but
for the life of me I cant tell what it is!


Kind Regards
Sam
"
Steve Loughran <stevel@hortonworks.com>,"Sat, 4 Feb 2017 13:50:55 +0000",Re: Remove support for Hadoop 2.5 and earlier?,Apache Spark Dev <dev@spark.apache.org>,"

Hi Sean,

Given that 3.0.0 is coming, removing the unused versions would be a
huge benefit from maintenance point of view. I'd support removing
support for 2.5 and earlier.

Speaking of Hadoop support, is anyone considering 3.0.0 support? Can't
find any JIRA for this.



As it stands, hive 1.2.x rejects 3 as a supported Hadoop version, so dataframes won't work

https://issues.apache.org/jira/browse/SPARK-18673

There's a quick fix to get hadoop to lie about what version it is to keep hive quiet, building hadoop with -Ddeclared.hadoop.version=2.11 to force it into 2.11, but that's not production. It does at least verify that nobody has broken any of the APIs (at least excluding those called via reflection on codepaths not tested in unit testing)

the full Hive patch is very much a WiP and its aimed at Hive 2
https://issues.apache.org/jira/browse/HIVE-15016

...which means either backporting to the org,spark-project hive 1.2 fork or moving up to Hive 2, which is inevitably going to be a major change
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Sat, 4 Feb 2017 12:13:41 -0200",Re: specifing schema on dataframe,Sam Elamin <hussam.elamin@gmail.com>,"Hi Sam
Remove the "" from the number that it will work

Em 4 de fev de 2017 11:46 AM, ""Sam Elamin"" <hussam.elamin@gmail.com>
escreveu:

"
Sam Elamin <hussam.elamin@gmail.com>,"Sat, 4 Feb 2017 14:22:04 +0000",Re: specifing schema on dataframe,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Hi Direceu

Thanks your right! that did work


But now im facing an even bigger problem since i dont have access to change
the underlying data, I just want to apply a schema over something that was
written via the sparkContext.newAPIHadoopRDD

Basically I am reading in a RDD[JsonObject] and would like to convert it
into a dataframe which I pass the schema into

Whats the best way to do this?

I doubt removing all the quotes in the JSON is the best solution is it?

Regards
Sam


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sun, 05 Feb 2017 11:57:27 +0000",Re: Google Summer of Code 2017 is coming,dev <dev@spark.apache.org>,"I think Sean raises valid points - that the result is highly dependent on
the particular student, project and mentor involved, and that the actual
required time investment is very significant.

Having said that, it's not all bad certainly. Scikit-learn started as a
GSoC project 10 years ago!

Actually they have a pretty good model for accepting students - typically
the student must demonstrate significant prior knowledge and ability with
the project sufficient to complete the work.

The challenge I think Spark has is already folks are strapped for capacity
so finding mentors with time will be tricky. But if there are mentors and
the right project / student fit can be found, I think it's a good idea.



ry
they're
o
r
d
p
e
"
Asher Krim <akrim@hubspot.com>,"Sun, 5 Feb 2017 15:24:16 -0500",Re: ml word2vec finSynonyms return type,"""dev@spark.apache.org"" <dev@spark.apache.org>","It took me a while, but I finally got around this:
https://github.com/apache/spark/pull/16811/files


"
Evgenii Morozov <evgeny.a.morozov@gmail.com>,"Mon, 6 Feb 2017 00:33:23 +0300","FileNotFoundException, while file is actually available","Dev <dev@spark.apache.org>,
 user <user@spark.apache.org>","Hi, 

I see a lot of exceptions like the following during our machine learning pipeline calculation. Spark version 2.0.2.
Sometimes its just few executors that fails with this message, but the job is successful. 

Id appreciate any hint you might have.
Thank you.

2017-02-05 07:56:47.022 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 151558.0 (TID 993070, 10.61.12.43):
java.io.FileNotFoundException: File file:/path/to/file does not exist
        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
        at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
        at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
        at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Sun, 5 Feb 2017 14:00:22 -0800",Re: specifing schema on dataframe,Sam Elamin <hussam.elamin@gmail.com>,"-dev

You can use withColumn to change the type after the data has been loaded
<https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1023043053387187/1572067047091340/2840265927289860/latest.html>
.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 5 Feb 2017 19:45:12 -0700 (MST)","Re: How to checkpoint and RDD after a stage and before reaching an
 action?",dev@spark.apache.org,"
Hi Leo,

The checkpointing of a RDD will be performed after a job using this RDD has
completed. Since you have only one job, rdd1 will only be checkpointed after
it is finished.

To checkpoint rdd1, you can simply materialize (and maybe cache it to avoid
recomputation) rdd1 (e.g., rdd1.count) after calling rdd1.checkpoint().



leo9r wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Aseem Bansal <asmbansal2@gmail.com>,"Mon, 6 Feb 2017 13:16:54 +0530",Is there any plan to have a predict method for single instance on PipelineModel?,dev@spark.apache.org,"Hi

I looked up in the JIRA but could not find any JIRA to support predict
method for single instance on PipelineModel. Is there anything that I may
have missed?
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 06 Feb 2017 07:52:35 +0000",Re: Is there any plan to have a predict method for single instance on PipelineModel?,"Aseem Bansal <asmbansal2@gmail.com>, dev@spark.apache.org","I'm in mobile right now but there is a JIRA to add it to the models first
and on that JIRA people are discussing single element transform as a
possibility -
https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-10413

There might be others as well that just aren't as fresh in my memory.


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
censj <censj@lotuseed.com>,"Mon, 6 Feb 2017 15:58:45 +0800","Re: FileNotFoundException, while file is actually available",Evgenii Morozov <evgeny.a.morozov@gmail.com>,"If you deploy yarn model，you can used yarn logs -applicationId youApplicationId get yarn logs. You can get logs in details。
Then Looking error info,
===============================
Name: cen sujun
Mobile: 13067874572
Mail: censj@lotuseed.com

<evgeny.a.morozov@gmail.com> 写道：
learning pipeline calculation. Spark version 2.0.2.
but the job is successful. 
o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 151558.0 (TID 993070, 10.61.12.43):
org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)
org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

"
StanZhai <mail@zhaishidan.cn>,"Mon, 6 Feb 2017 05:32:55 -0700 (MST)","Re: [SQL]A confusing NullPointerException when creating table using
 Spark2.1.0",dev@spark.apache.org,"This issue has been fixed by  https://github.com/apache/spark/pull/16820
<https://github.com/apache/spark/pull/16820>  .



--

---------------------------------------------------------------------


"
StanZhai <mail@zhaishidan.cn>,"Mon, 6 Feb 2017 05:41:44 -0700 (MST)","[SQL]SQLParser fails to resolve nested CASE WHEN statement with
 parentheses in Spark 2.x",dev@spark.apache.org,"Hi all,

SQLParser fails to resolve nested CASE WHEN statement like this:

select case when
  (1) +
  case when 1>0 then 1 else 0 end = 2
then 1 else 0 end
from tb

==================== Exception ====================
Exception in thread ""main""
org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'then' expecting {'.', '[', 'OR', 'AND', 'IN', NOT,
'BETWEEN', 'LIKE', RLIKE, 'IS', 'WHEN', EQ, '<=>', '<>', '!=', '<', LTE,
'>', GTE, '+', '-', '*', '/', '%', 'DIV', '&', '|', '^'}(line 5, pos 0)

== SQL ==

select case when
  (1) +
  case when 1>0 then 1 else 0 end = 2
then 1 else 0 end
^^^
from tb

But锛remove parentheses will be fine锛

select case when
  1 +
  case when 1>0 then 1 else 0 end = 2
then 1 else 0 end
from tb

I've already filed a JIRA for this: 
https://issues.apache.org/jira/browse/SPARK-19472
<https://issues.apache.org/jira/browse/SPARK-19472>  

Any help is greatly appreciated!

Best,
Stan




--
3.nabble.com/SQL-SQLParser-fails-to-resolve-nested-CASE-WHEN-statement-with-parentheses-in-Spark-2-x-tp20867.html
om.

---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Mon, 6 Feb 2017 17:24:20 +0100","Re: [SQL]SQLParser fails to resolve nested CASE WHEN statement with
 parentheses in Spark 2.x",StanZhai <mail@zhaishidan.cn>,"Hi Stan,

I have opened https://github.com/apache/spark/pull/16821 to fix this.


====================
TE,
-


-- 


[image: Register today for Spark Summit East 2017!]
<https://spark-summit.org/east-2017/>

Herman van H枚vell

Software Engineer

Databricks Inc.

hvanhovell@databricks.com

+31 6 420 590 27

databricks.com

[image: http://databricks.com] <http://databricks.com/>
"
Adam Budde <budde.adam@gmail.com>,"Mon, 6 Feb 2017 14:44:08 -0800",[STREAMING] Looking for committer to review kinesis-asl PR,dev@spark.apache.org,"Hello all,

Apologies for spamming the entire dev list, but I'm having some difficulty
finding reviewers for a PR I have open to add additional authorization
options to KinesisUtils/KinesisReceiver:

https://github.com/apache/spark/pull/16744

The change passes all tests and merges cleanly. @srowen gave it an initial
look over but we are still looking for someone to review the substance of
the commit. Thanks in advance!

Adam
"
Adam Budde <budde.adam@gmail.com>,"Mon, 6 Feb 2017 14:33:17 -0800",[STREAMING] Looking for committer to review kinesis-asl PR,dev@spark.apache.org,"Hey all,

Apologies for spamming the entire dev list, but I'm having some difficulty
finding reviewers for a PR I have open to add additional authorization
options to KinesisUtils/KinesisReceiver:

https://github.com/apache/spark/pull/16744

The change passes all tests and merges cleanly. @srowen gave it an initial
look over but we are still looking for someone to review the substance of
the commit. Thanks in advance!

Adam
"
"""Budde, Adam"" <budde@amazon.com>","Mon, 6 Feb 2017 22:23:56 +0000",[SPARK-19405] Looking for committer to review kinesis-asl PR,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

Apologies for spamming this to the entire dev list, but I havent been able to find reviewers for a PR I have open to add a new authorization options to KinesisReciever/KinesisUtils:

    https://github.com/apache/spark/pull/16744

The PR merges cleanly and passes all tests. @srowen gave it an initial look over but were still looking for someone who can review the substance of the code. Thanks in advance!

Adam


"
Joseph Bradley <joseph@databricks.com>,"Mon, 6 Feb 2017 16:13:40 -0800",PSA: Java 8 unidoc build,"""dev@spark.apache.org"" <dev@spark.apache.org>","Public service announcement: Our doc build has worked with Java 8 for brief
time periods, but new changes keep breaking the Java 8 unidoc build.
Please be aware of this, and try to test doc changes with Java 8!  In
general, it is stricter than Java 7 for docs.

A shout out to @HyukjinKwon and others who have made many fixes for this!
See these sample PRs for some issues causing failures (especially around
links):
https://github.com/apache/spark/pull/16741
https://github.com/apache/spark/pull/16604

Thanks,
Joseph

-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 7 Feb 2017 09:21:59 +0900",Re: PSA: Java 8 unidoc build,Joseph Bradley <joseph@databricks.com>,"Oh, Joseph, thanks. It is nice to inform this in dev mailing list.

Let me please leave another PR to refer,

https://github.com/apache/spark/pull/16013

and the JIRA you kindly opened,

https://issues.apache.org/jira/browse/SPARK-18692



Public service announcement: Our doc build has worked with Java 8 for brief
time periods, but new changes keep breaking the Java 8 unidoc build.
Please be aware of this, and try to test doc changes with Java 8!  In
general, it is stricter than Java 7 for docs.

A shout out to @HyukjinKwon and others who have made many fixes for this!
See these sample PRs for some issues causing failures (especially around
links):
https://github.com/apache/spark/pull/16741
https://github.com/apache/spark/pull/16604

Thanks,
Joseph

-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 7 Feb 2017 09:35:37 +0900",Re: PSA: Java 8 unidoc build,Joseph Bradley <joseph@databricks.com>,"more cases that might easily be mistaken)


"
Pete Robbins <robbinspg@gmail.com>,"Tue, 07 Feb 2017 11:43:54 +0000",Java 9,Dev <dev@spark.apache.org>,"Is anyone working on support for running Spark on Java 9? Is this in a
roadmap anywhere?


Cheers,
"
Sean Owen <sowen@cloudera.com>,"Tue, 07 Feb 2017 11:46:21 +0000",Re: Java 9,"Pete Robbins <robbinspg@gmail.com>, Dev <dev@spark.apache.org>","I don't think anyone's tried it. I think we'd first have to agree to drop
Java 7 support before that could be seriously considered. The 8-9
difference is a bit more of a breaking change.


"
Pete Robbins <robbinspg@gmail.com>,"Tue, 07 Feb 2017 11:51:50 +0000",Re: Java 9,"Sean Owen <sowen@cloudera.com>, Dev <dev@spark.apache.org>","Yes, I agree but it may be worthwhile starting to look at this. I was just
trying a build and it trips over some of the now defunct/inaccessible
sun.misc classes.

I was just interested in hearing if anyone has already gone through this to
save me duplicating effort.

Cheers,


"
Timur Shenkao <tsh@timshenkao.su>,"Tue, 7 Feb 2017 13:03:26 +0100",Re: Java 9,Pete Robbins <robbinspg@gmail.com>,"If I'm not wrong, they got fid of   *sun.misc.Unsafe   *in Java 9.

This class is till used by several libraries & frameworks.

http://mishadoff.com/blog/java-magic-part-4-sun-dot-misc-dot-unsafe/


"
Sean Owen <sowen@cloudera.com>,"Tue, 07 Feb 2017 12:06:53 +0000",Re: PSA: Java 8 unidoc build,"Joseph Bradley <joseph@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>","I believe that if we ran the Jenkins builds with Java 8 we would catch
these? this doesn't require dropping Java 7 support or anything.

@joshrosen I know we are just now talking about modifying the Jenkins jobs
to remove old Hadoop configs. Is it possible to change the master jobs to
use Java 8? can't hurt really in any event.

Or maybe I'm mistaken and they already run Java 8 and it doesn't catch this
until Java 8 is the target.

Yeah this is going to keep breaking as javadoc 8 is pretty strict. Thanks
Hyukjin. It has forced us to clean up a lot of sloppy bits of doc though.


"
Reynold Xin <rxin@databricks.com>,"Tue, 7 Feb 2017 13:47:32 +0100",Re: PSA: Java 8 unidoc build,Sean Owen <sowen@cloudera.com>,"I don't know if this would help but I think we can also officially stop
supporting Java 7 ...



"
StanZhai <mail@zhaishidan.cn>,"Tue, 7 Feb 2017 07:00:57 -0700 (MST)","Re: Executors exceed maximum memory defined with
 `--executor-memory` in Spark 2.1.0",dev@spark.apache.org,"threads named  ""DataStreamer for file
/test/data/test_temp/_temporary/0/_temporary/attempt_20170207172435_80750_m_000069_1/part-00069-690407af-0900-46b1-9590-a6d6c696fe68.snappy.parquet""
in TIMED_WAITING state like this:
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n20881/QQ20170207-212340.png> 

The exceed off-heap memory may be caused by these abnormal threads. 

This problem occurs only when writing data to the Hadoop(tasks may be killed
by Executor during writing).

Could this be related to  https://issues.apache.org/jira/browse/HDFS-9812
<https://issues.apache.org/jira/browse/HDFS-9812>  ?

It's may be a bug of Spark when killing tasks during writing data. What's
the difference between Spark 1.6.x and 2.1.0 in killing tasks?

This is a critical issue, I've worked on this for days.

Any help?



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 7 Feb 2017 15:47:59 +0100",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Mark Hamstra <mark@clearstorydata.com>,"Bumping this.

Given we see the occassional build breaks with Java 8, we should reconsider
this and do it for 2.2 or 2.3. By the time 2.2 is released, it will almost
be an year since this thread started.



"
Reynold Xin <rxin@databricks.com>,"Tue, 7 Feb 2017 15:48:41 +0100",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Mark Hamstra <mark@clearstorydata.com>,"BTW I created a JIRA ticket for tracking:
https://issues.apache.org/jira/browse/SPARK-19493

We of course shouldn't do anything until we achieve consensus.



"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 7 Feb 2017 16:35:08 +0000",Structured Streaming. Dropping Duplicates,dev <dev@spark.apache.org>,"Hi All

When trying to read a stream off S3 and I try and drop duplicates I get the
following error:

Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Append
output mode not supported when there are streaming aggregations on
streaming DataFrames/DataSets;;


Whats strange if I use the batch ""spark.read.json"", it works

Can I assume you cant drop duplicates in structured streaming

Regards
Sam
"
Michael Armbrust <michael@databricks.com>,"Tue, 7 Feb 2017 08:49:55 -0800",Re: Structured Streaming. Dropping Duplicates,Sam Elamin <hussam.elamin@gmail.com>,"Here a JIRA: https://issues.apache.org/jira/browse/SPARK-19497

We should add this soon.


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 7 Feb 2017 16:58:12 +0000",Re: Structured Streaming. Dropping Duplicates,Michael Armbrust <michael@databricks.com>,"Thanks Micheal!




"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 7 Feb 2017 17:05:18 +0000",Re: Structured Streaming. Dropping Duplicates,Michael Armbrust <michael@databricks.com>,"
I noticed if I have  a stream running off s3 and I kill the process. The
next time the process starts running it dulplicates the last record
inserted. is that normal?




So say I have streaming enabled on one folder ""test"" which only has two
files ""update1"" and ""update 2"", then I kill the spark job using Ctrl+C.
When I rerun the stream it picks up ""update 2"" again

Is this normal? isnt ctrl+c a failure?

I would expect checkpointing to know that update 2 was already processed

Regards
Sam


"
Michael Armbrust <michael@databricks.com>,"Tue, 7 Feb 2017 09:20:42 -0800",Re: Structured Streaming. Dropping Duplicates,Sam Elamin <hussam.elamin@gmail.com>,"It is always possible that there will be extra jobs from failed batches.
However, for the file sink, only one set of files will make it into
_spark_metadata directory log.  This is how we get atomic commits even when
there are files in more than one directory.  When reading the files with
Spark, we'll detect this directory and use it instead of listStatus to find
the list of valid files.


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 7 Feb 2017 17:25:03 +0000",Re: Structured Streaming. Dropping Duplicates,Michael Armbrust <michael@databricks.com>,"Hmm ok I understand that but the job is running for a good few mins before
I kill it so there should not be any jobs left because I can see in the log
that its now polling for new changes, the latest offset is the right one

After I kill it and relaunch it picks up that same file?


Sorry if I misunderstood you


"
Felix Cheung <felixcheung_m@hotmail.com>,"Tue, 7 Feb 2017 17:27:08 +0000",Re: PSA: Java 8 unidoc build,"Reynold Xin <rxin@databricks.com>, Sean Owen <sowen@cloudera.com>","+1 for all the great work going in for this, HyukjinKwon, and +1 on what Sean says about ""Jenkins builds with Java 8"" and we should catch these nasty javadoc8 issue quickly.

I think that would be the great first step to move away from java 7


__________"
Michael Armbrust <michael@databricks.com>,"Tue, 7 Feb 2017 13:05:06 -0500",Re: Structured Streaming. Dropping Duplicates,Sam Elamin <hussam.elamin@gmail.com>,"Sorry, I think I was a little unclear.  There are two things at play here.

 - Exactly-once semantics with file output: spark writes out extra metadata
on which files are valid to ensure that failures don't cause us to ""double
count"" any of the input.  Spark 2.0+ detects this info automatically when
you use dataframe reader (spark.read...). There may be extra files, but
they will be ignored. If you are consuming the output with another system
you'll have to take this into account.
 - Retries: right now we always retry the last batch when restarting.  This
is safe/correct because of the above, but we could also optimize this away
by tracking more information about batch progress.


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 07 Feb 2017 18:16:56 +0000",Re: Structured Streaming. Dropping Duplicates,Michael Armbrust <michael@databricks.com>,"Ah I see ok so probably it's the retry that's causing it

So when you say I'll have to take this into account, how do I best do that?
My sink will have to know what was that extra file. And i was under the
impression spark would automagically know this because of the checkpoint
directory set when you created the writestream

If that's not the case then how would I go about ensuring no duplicates?


Thanks again for the awesome support!

Regards
Sam

"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 7 Feb 2017 11:17:15 -0800",Re: PSA: Java 8 unidoc build,Sean Owen <sowen@cloudera.com>,"@Sean, I'm using Java 8 but don't see these errors until I manually build
the API docs. Hence I think dropping Java 7 support may not help.

Right now we don't build docs in most of builds as building docs takes a
long time (e.g.,
https://amplab.cs.berkel"
Michael Armbrust <michael@databricks.com>,"Tue, 7 Feb 2017 14:24:17 -0500",Re: Structured Streaming. Dropping Duplicates,Sam Elamin <hussam.elamin@gmail.com>,"Read the JSON log of files that is in `/your/path/_spark_metadata` and only
read files that are present in that log (ignore anything else).


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 7 Feb 2017 20:29:21 +0000",Re: Structured Streaming. Dropping Duplicates,Michael Armbrust <michael@databricks.com>,"Hi Micheal

If thats the case for the below example, where should i be reading these
json log files first? im assuming sometime between df and query?


val df = spark
    .readStream
    .option(""tableReferenceSource"",tableName)
    .load()
setUpGoogle(spark.sqlContext)

val query = df
  .writeStream
  .option(""tableReferenceSink"",tableName2)
  .option(""checkpointLocation"",""checkpoint"")
  .start()



"
Michael Armbrust <michael@databricks.com>,"Tue, 7 Feb 2017 15:40:15 -0500",Re: Structured Streaming. Dropping Duplicates,Sam Elamin <hussam.elamin@gmail.com>,"The JSON log is only used by the file sink (which it doesn't seem like you
are using).  Though, I'm not sure exactly what is going on inside of
setupGoogle or how tableReferenceSource is used.

Typically you would run df.writeStream.option(""path"", ""/my/path"")... and
then the transaction log would go into /my/path/_spark_metadata.

There is not requirement that a sink uses this kind of a update log.  This
is just how we get better transactional semantics than HDFS is providing.
If your sink supports transactions natively you should just use those
instead.  We pass a unique ID to the sink method addBatch so that you can
make sure you don't commit the same transaction more than once.


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 7 Feb 2017 20:44:32 +0000",Re: Structured Streaming. Dropping Duplicates,Michael Armbrust <michael@databricks.com>,"Sorry those are methods I wrote so you can ignore them :)

so just adding a path parameter tells spark thats where the update log is?

Do I check for the unique id there and identify which batch was written and
which weren't

Are there any examples of this out there? there aren't much connectors in
the wild which I can reimplement is there
Should I look at how the file sink is set up and follow that pattern?


Regards
Sam


"
Sam Elamin <hussam.elamin@gmail.com>,"Tue, 7 Feb 2017 22:10:17 +0000",Re: Structured Streaming. Dropping Duplicates,Michael Armbrust <michael@databricks.com>,"Ignore me, a bit more digging and I was able to find the filesink source
<https://github.com/apache/spark/blob/1ae4652b7e1f77a984b8459c778cb06c814192c5/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala>

Following that pattern worked a treat!

Thanks again Micheal :)


"
kant kodali <kanth909@gmail.com>,"Tue, 7 Feb 2017 21:09:09 -0800",Re: Java 9,Timur Shenkao <tsh@timshenkao.su>,"Well and the module system!


"
Jakub Dubovsky <spark.dubovsky.jakub@gmail.com>,"Wed, 8 Feb 2017 12:54:46 +0100",Cannot find checkstyle.xml,dev@spark.apache.org,"Hello there,

I am trying to build spark locally so I can test something to help resolve this
ticket <https://issues.apache.org/jira/browse/SPARK-16599>.

git checkout v2.1.0
./build/mvn -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn
-Dhadoop.version=2.6.0-cdh5.7.1 -DskipTests -e clean install

This starts the build successfully. Then I changed one source file and a
version in pom.xml (exact diff). After this change when I run the same
build command as above I get failure:

Could not find resource 'dev/checkstyle.xml'

whole build log <<script
src=""https://gist.github.com/james64/85b3bf4613e7105bebd687502258a518.js""></script>>

How this one commit change can cause this error? checkstyle.xml is still
there. I run maven from project root in both cases. What should I change to
build this?

Thanks for your help

Jakub
"
Rich Bowen <rbowen@apache.org>,"Wed, 8 Feb 2017 09:09:58 -0500",FINAL REMINDER: CFP for ApacheCon closes February 11th,apachecon-discuss@apache.org,"Dear Apache Enthusiast,

This is your FINAL reminder that the Call for Papers (CFP) for ApacheCon
Miami is closing this weekend - February 11th. This is your final
opportunity to submit a talk for consideration at this event.

This year, we are running several mini conferences in conjunction with
the main event, so if you're submitting for one of those events, please
pay attention to the instructions below.

Apache: Big Data
* Event information:
http://events.linuxfoundation.org/events/apache-big-data-north-america
* CFP:
http://events.linuxfoundation.org/events/apache-big-data-north-america/program/cfp

Apache: IoT (Internet of Things)
* Event Information: http://us.apacheiot.org/
* CFP -
http://events.linuxfoundation.org/events/apachecon-north-america/program/cfp
(Indicate 'IoT' in the Target Audience field)

CloudStack Collaboration Conference
* Event information: http://us.cloudstackcollab.org/
* CFP -
http://events.linuxfoundation.org/events/apachecon-north-america/program/cfp
(Indicate 'CloudStack' in the Target Audience field)

FlexJS Summit
* Event information - http://us.apacheflexjs.org/
* CFP -
http://events.linuxfoundation.org/events/apachecon-north-america/program/cfp
(Indicate 'Flex' in the Target Audience field)

TomcatCon
* Event information - https://tomcat.apache.org/conference.html
* CFP -
http://events.linuxfoundation.org/events/apachecon-north-america/program/cfp
(Indicate 'Tomcat' in the Target Audience field)

All other topics and projects
* Event information -
http://events.linuxfoundation.org/events/apachecon-north-america/program/about
* CFP -
http://events.linuxfoundation.org/events/apachecon-north-america/program/cfp

Admission to any of these events also grants you access to all of the
others.

Thanks, and we look forward to seeing you in Miami!

-- 
Rich Bowen
VP Conferences, Apache Software Foundation
rbowen@apache.org
Twitter: @apachecon



(You are receiving this email because you are subscribed to a dev@ or
users@ list of some Apache Software Foundation project. If you do not
wish to receive email from these lists any more, you must follow that
list's unsubscription procedure. View the headers of this message for
unsubscription instructions.)

---------------------------------------------------------------------


"
Jakub Dubovsky <spark.dubovsky.jakub@gmail.com>,"Wed, 8 Feb 2017 17:19:45 +0100",Re: Cannot find checkstyle.xml,dev@spark.apache.org,"Sorry, correct links set in text below.

Hello there,
"
Jakub Dubovsky <spark.dubovsky.jakub@gmail.com>,"Wed, 8 Feb 2017 17:22:22 +0100",Re: Cannot find checkstyle.xml,dev@spark.apache.org,"Sorry, correct links set in text below.


Hello there,

I am trying to build spark locally so I can test something to help resolve this
ticket <https://issues.apache.org/jira/browse/SPARK-16599>.

git checkout v2.1.0
./build/mvn -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn
-Dhadoop.version=2.6.0-cdh5.7.1 -DskipTests -e clean install

This starts the build successfully. Then I changed one source file and a
version in pom.xml (exact diff
<https://gist.github.com/james64/cc158bdb81bc1828937c757fde94ce82>). After
this change when I run the same build command as above I get failure:

Could not find resource 'dev/checkstyle.xml'

whole build log
<https://gist.github.com/james64/85b3bf4613e7105bebd687502258a518>

How this one commit change can cause this error? checkstyle.xml is still
there. I run maven from project root in both cases. What should I change to
build this?

Thanks for your help

Jakub
"
Sam Elamin <hussam.elamin@gmail.com>,"Wed, 8 Feb 2017 23:54:47 +0000",Structured Streaming. S3 To Google BigQuery,"dev <dev@spark.apache.org>, user <user@spark.apache.org>","Hi All

Thank you all for the amazing support! I have written a BigQuery connector
for structured streaming that you can find here
<https://github.com/samelamin/spark-bigquery>

I just tweeted <https://twitter.com/samelamin/status/829477884024782852>
about it and would really appreciated it if you retweeted when you get a
chance

The more people know about it and use it the more feedback I can get to
make the connector better!

Ofcourse PRs and feedback are always welcome :)

Thanks again!

Regards
Sam
"
Koert Kuipers <koert@tresata.com>,"Thu, 9 Feb 2017 01:47:00 -0500",when is doGenCode called?,"""dev@spark.apache.org"" <dev@spark.apache.org>","hello all,
i am trying to add an Expression to catalyst.

my Expression compiles fine and has:

override def eval(input: InternalRow): Any = ...

override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = ...

it also seems to run fine. but i only ever see eval get called. how do i
tell spark to use doGenCode instead of eval?

thanks! koert
"
Jakub Dubovsky <spark.dubovsky.jakub@gmail.com>,"Thu, 9 Feb 2017 10:18:54 +0100",Re: Cannot find checkstyle.xml,"Ted Yu <yuzhihong@gmail.com>, dev@spark.apache.org","Thanks Ted for trying. (see below for Ted's reply). Con somebody confirm
that this is not an expected behaviour? Is there somebody else having same
issue?

Thanks!


"
Felix Yang <felix.yang@oracle.com>,"Thu, 9 Feb 2017 18:10:48 +0800",Re: Java 9,dev@spark.apache.org,"Excuse me,

    more detail of JDK 9 can be found here .

http://openjdk.java.net/projects/jdk9/

The changes mentioned below were tracked in http://openjdk.java.net/jeps/260

would be quite helpful.


There is a Quality Outreach program. Welcome to join and feedback.

Quality Outreach: 
https://wiki.openjdk.java.net/display/quality/Quality+Outreach

Thanks,
Felix

"
Reynold Xin <rxin@databricks.com>,"Thu, 9 Feb 2017 11:12:40 +0100",Re: Java 9,Felix Yang <felix.yang@oracle.com>,"tl;dr:

The critical internal APIs proposed to remain accessible in JDK 9 are:

sun.misc.{Signal,SignalHandler}

sun.misc.Unsafe (The functionality of many of the methods in this class is
now available via variable handles (JEP 193).)

sun.reflect.Reflection::getCallerClass(int) (The functionality of this
method may be provided in a standard form via JEP 259.)

sun.reflect.ReflectionFactory.newConstructorForSerialization



"
"""Rory O'Donnell"" <rory.odonnell@oracle.com>","Thu, 9 Feb 2017 12:58:16 +0000",Re: Java 9,"Felix Yang <felix.yang@oracle.com>, dev@spark.apache.org","Hi,

Let me fill you in on the Quality Outreach.

I send out email every 2-3 weeks depending on contents of the builds, 
example attached.
I try to highlight significant changes in the builds, allowing you to 
decide if you might want
to test with a particular build. We don't expect you to test every 
build, it's entirely up to you.

If you would like us to list your project(s) on the Quality Outreach 
wiki [1] , can you provide a
contact name , mailing list , the current status of you testing against 
JDK 8 &  JDK 9, and a
CI if possible  ?

Rgds, Rory

[1] https://wiki.openjdk.java.net/display/quality/Quality+Outreach

-- 
Rgds,Rory O'Donnell
Quality Engineering Manager
Oracle EMEA , Dublin, Ireland


---------------------------------------------------------------------"
Michael Allman <michael@videoamp.com>,"Thu, 9 Feb 2017 09:52:40 -0800",Simple bug fix PR looking for love,dev <dev@spark.apache.org>,"Hi Guys,

Can someone help move https://github.com/apache/spark/pull/16499 <https://github.com/apache/spark/pull/16499> along in the review process? This PR fixes replicated off-heap storage.

Thanks!

Michael"
Jacek Laskowski <jacek@japila.pl>,"Thu, 9 Feb 2017 22:15:59 +0100","Dynamic Allocation in Core vs YARN -- getDynamicAllocationInitialExecutors
 vs getInitialTargetExecutorNumber",dev <dev@spark.apache.org>,"Hi,

I'm wondering why YARN computes the initial number of executors (in
YarnSparkHadoopUtil.getInitialTargetExecutorNumber [1]) if Core's
Utils.getDynamicAllocationInitialExecutors [2] could do?

I'm to send a PR to remove the duplication as it's tricky enough to
keep right in one place given all the Spark properties and their
relationship:

* spark.dynamicAllocation.minExecutors
* spark.dynamicAllocation.initialExecutors
* spark.executor.instances

WDYT?

[1] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala#L270

[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L2516

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
"""John Fang"" <xiaojian.fxj@alibaba-inc.com>","Fri, 10 Feb 2017 12:35:31 +0800","=?UTF-8?B?RHJpdmVyIGh1bmcgYW5kIGhhcHBlbmQgb3V0IG9mIG1lbW9yeSB3aGlsZSB3cml0aW5nIHRv?=
  =?UTF-8?B?IGNvbnNvbGUgcHJvZ3Jlc3MgYmFy?=","""spark-dev"" <dev@spark.apache.org>,
  ""spark-user"" <user@spark.apache.org>","[Stage 172:==============================>                 (10328 + 93) / 16144]
[Stage 172:==============================>                 (10329 + 93) / 16144]
[Stage 172:==============================>                 (10330 + 93) / 16144]
[Stage 172:==============================>                 (10331 + 93) / 16144]
[Stage 172:==============================>                 (10333 + 92) / 16144]
[Stage 172:==============================>                 (10333 + 93) / 16144]
[Stage 172:==============================>                 (10333 + 94) / 16144]
[Stage 172:==============================>                 (10334 + 94) / 16144]
[Stage 172:==============================>                 (10338 + 93) / 16144]
[Stage 172:==============================>                 (10339 + 92) / 16144]
[Stage 172:==============================>                 (10340 + 93) / 16144]
[Stage 172:==============================>                 (10341 + 92) / 16144]
[Stage 172:==============================>                 (10341 + 93) / 16144]
[Stage 172:==============================>                 (10342 + 93) / 16144]
[Stage 172:==============================>                 (10343 + 93) / 16144]
[Stage 172:==============================>                 (10344 + 92) / 16144]
[Stage 172:==============================>                 (10345 + 92) / 16144]
[Stage 172:==============================>                 (10345 + 93) / 16144]
[Stage 172:==============================>                 (10346 + 93) / 16144]
[Stage 172:==============================>                 (10348 + 92) / 16144]
[Stage 172:==============================>                 (10348 + 93) / 16144]
[Stage 172:==============================>                 (10349 + 92) / 16144]
[Stage 172:==============================>                 (10349 + 93) / 16144]
[Stage 172:==============================>                 (10350 + 92) / 16144]
[Stage 172:==============================>                 (10352 + 92) / 16144]
[Stage 172:==============================>                 (10353 + 92) / 16144]
[Stage 172:==============================>                 (10354 + 92) / 16144]
[Stage 172:==============================>                 (10355 + 92) / 16144]
[Stage 172:==============================>                 (10356 + 92) / 16144]
[Stage 172:==============================>                 (10356 + 93) / 16144]
[Stage 172:==============================>                 (10357 + 92) / 16144]
[Stage 172:==============================>                 (10357 + 93) / 16144]
[Stage 172:==============================>                 (10358 + 92) / 16144]
[Stage 172:==============================>                 (10358 + 93) / 16144]
[Stage 172:==============================>                 (10359 + 92) / 16144]
[Stage 172:==============================>                 (10359 + 93) / 16144]
[Stage 172:==============================>                 (10359 + 94) / 16144]
[Stage 172:==============================>                 (10361 + 92) / 16144]
[Stage 172:==============================>                 (10361 + 93) / 16144]
[Stage 172:==============================>                 (10362 + 92) / 16144]
[Stage 172:==============================>                 (10362 + 93) / 16144]
[Stage 172:==============================>                 (10363 + 93) / 16144]
[Stage 172:==============================>                 (10364 + 92) / 16144]
[Stage 172:==============================>                 (10365 + 92) / 16144]
[Stage 172:==============================>                 (10365 + 93) / 16144]
[Stage 172:==============================>                 (10366 + 92) / 16144]
[Stage 172:==============================>                 (10366 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 92) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]
[Stage 172:==============================>                 (10367 + 93) / 16144]Exception in thread ""JobGenerator"" java.lang.OutOfMemoryError: Java heap space
	at com.fasterxml.jackson.core.util.BufferRecycler.calloc(BufferRecycler.java:156)
	at com.fasterxml.jackson.core.util.BufferRecycler.allocCharBuffer(BufferRecycler.java:124)
	at com.fasterxml.jackson.core.io.IOContext.allocTokenBuffer(IOContext.java:189)
	at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:879)

Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""JobGenerator""
Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOfRange(Arrays.java:3664)
	at java.lang.String.<init>(String.java:207)
	at java.lang.StringBuilder.toString(StringBuilder.java:407)
	at scala.collection.mutable.StringBuilder.toString(StringBuilder.scala:430)
	at org.apache.spark.ui.ConsoleProgressBar.show(ConsoleProgressBar.scala:101)
	at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:71)
	at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)"
"""John Fang"" <xiaojian.fxj@alibaba-inc.com>","Fri, 10 Feb 2017 12:41:57 +0800","=?UTF-8?B?5Zue5aSN77yaRHJpdmVyIGh1bmcgYW5kIGhhcHBlbmQgb3V0IG9mIG1lbW9yeSB3aGlsZSB3?=
  =?UTF-8?B?cml0aW5nIHRvIGNvbnNvbGUgcHJvZ3Jlc3MgYmFy?=","""spark-dev"" <dev@spark.apache.org>,
  ""spark-user"" <user@spark.apache.org>","the spark version is 2.1.0
------------------------------------------------------------------浠朵汉锛瑰(寮) <xiaojian.fxj@alibaba-inc.com>堕达2017骞210(浜) 12:35朵欢浜猴spark-dev <dev@spark.apache.org>; spark-user <user@spark.apache.org>涓汇棰锛Driver hung and happend out of memory while writing to console progress bar
[Stage 172:==============================>                 (10328 + 93) / 16144][Stage 172:==============================>                 (10329 + 93) / 16144][Stage 172:==============================>                 (10330 + 93) / 16144][Stage 172:==============================>                 (10331 + 93) / 16144][Stage 172:==============================>                 (10333 + 92) / 16144][Stage 172:==============================>                 (10333 + 93) / 16144][Stage 172:==============================>                 (10333 + 94) / 16144][Stage 172:==============================>                 (10334 + 94) / 16144][Stage 172:==============================>                 (10338 + 93) / 16144][Stage 172:==============================>                 (10339 + 92) / 16144][Stage 172:==============================>                 (10340 + 93) / 16144][Stage 172:==============================>                 (10341 + 92) / 16144][Stage 172:==============================>                 (10341 + 93) / 16144][Stage 172:==============================>                 (10342 + 93) / 16144][Stage 172:==============================>                 (10343 + 93) / 16144][Stage 172:==============================>                 (10344 + 92) / 16144][Stage 172:==============================>                 (10345 + 92) / 16144][Stage 172:==============================>                 (10345 + 93) / 16144][Stage 172:==============================>                 (10346 + 93) / 16144][Stage 172:==============================>                 (10348 + 92) / 16144][Stage 172:==============================>                 (10348 + 93) / 16144][Stage 172:==============================>                 (10349 + 92) / 16144][Stage 172:==============================>                 (10349 + 93) / 16144][Stage 172:==============================>                 (10350 + 92) / 16144][Stage 172:==============================>                 (10352 + 92) / 16144][Stage 172:==============================>                 (10353 + 92) / 16144][Stage 172:==============================>                 (10354 + 92) / 16144][Stage 172:==============================>                 (10355 + 92) / 16144][Stage 172:==============================>                 (10356 + 92) / 16144][Stage 172:==============================>                 (10356 + 93) / 16144][Stage 172:==============================>                 (10357 + 92) / 16144][Stage 172:==============================>                 (10357 + 93) / 16144][Stage 172:==============================>                 (10358 + 92) / 16144][Stage 172:==============================>                 (10358 + 93) / 16144][Stage 172:==============================>                 (10359 + 92) / 16144][Stage 172:==============================>                 (10359 + 93) / 16144][Stage 172:==============================>                 (10359 + 94) / 16144][Stage 172:==============================>                 (10361 + 92) / 16144][Stage 172:==============================>                 (10361 + 93) / 16144][Stage 172:==============================>                 (10362 + 92) / 16144][Stage 172:==============================>                 (10362 + 93) / 16144][Stage 172:==============================>                 (10363 + 93) / 16144][Stage 172:==============================>                 (10364 + 92) / 16144][Stage 172:==============================>                 (10365 + 92) / 16144][Stage 172:==============================>                 (10365 + 93) / 16144][Stage 172:==============================>                 (10366 + 92) / 16144][Stage 172:==============================>                 (10366 + 93) / 16144][Stage 172:==============================>                 (10367 + 92) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144]Exception in thread ""JobGenerator"" java.lang.OutOfMemoryError: Java heap space	at com.fasterxml.jackson.core.util.BufferRecycler.calloc(BufferRecycler.java:156)	at com.fasterxml.jackson.core.util.BufferRecycler.allocCharBuffer(BufferRecycler.java:124)	at com.fasterxml.jackson.core.io.IOContext.allocTokenBuffer(IOContext.java:189)	at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:879)Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""JobGenerator""Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: Java heap space	at java.util.Arrays.copyOfRange(Arrays.java:3664)	at java.lang.String.<init>(String.java:207)	at java.lang.StringBuilder.toString(StringBuilder.java:407)	at scala.collection.mutable.StringBuilder.toString(StringBuilder.scala:430)	at org.apache.spark.ui.ConsoleProgressBar.show(ConsoleProgressBar.scala:101)	at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:71)	at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55)	at java.util.TimerThread.mainLoop(Timer.java:555)	at java.util.TimerThread.run(Timer.java:505)"
Sean Owen <sowen@cloudera.com>,"Fri, 10 Feb 2017 16:47:06 +0000",Request for comments: Java 7 removal,dev <dev@spark.apache.org>,"As you have seen, there's a WIP PR to implement removal of Java 7 support:
https://github.com/apache/spark/pull/16871

I have heard several +1s at
https://issues.apache.org/jira/browse/SPARK-19493 but am asking for
concerns too, now that there's a concrete change to review.

If this goes in for 2.2 it can be followed by more extensive update of the
Java code to take advantage of Java 8; this is more or less the baseline
change.

We also just removed Hadoop 2.5 support. I know there was talk about
removing Python 2.6. I have no opinion on that myself, but, might be time
to revive that conversation too.
"
Ryan Blue <rblue@netflix.com.INVALID>,"Fri, 10 Feb 2017 09:34:54 -0800","Re: Driver hung and happend out of memory while writing to console
 progress bar",John Fang <xiaojian.fxj@alibaba-inc.com>,"This isn't related to the progress bar, it just happened while in that
section of code. Something else is taking memory in the driver, usually a
broadcast table or something else that requires a lot of memory and happens
on the driver.

You should check your driver memory settings and the query plan (if this
was SparkSQL) for this stage to investigate further.

rb

On Thu, Feb 9, 2017 at 8:41 PM, John Fang <xiaojian.fxj@alibaba-inc.com>
wrote:

> the spark version is 2.1.0
>
> ------------------------------------------------------------------
> 浠朵汉锛瑰(寮) <xiaojian.fxj@alibaba-inc.com>
> 堕达2017骞210(浜) 12:35
> 朵欢浜猴spark-dev <dev@spark.apache.org>; spark-user <user@spark.apache.org>
> 涓 棰锛Driver hung and happend out of memory while writing to console
> progress bar
>
> [Stage 172:==============================>                 (10328 + 93) / 16144][Stage 172:==============================>                 (10329 + 93) / 16144][Stage 172:==============================>                 (10330 + 93) / 16144][Stage 172:==============================>                 (10331 + 93) / 16144][Stage 172:==============================>                 (10333 + 92) / 16144][Stage 172:==============================>                 (10333 + 93) / 16144][Stage 172:==============================>                 (10333 + 94) / 16144][Stage 172:==============================>                 (10334 + 94) / 16144][Stage 172:==============================>                 (10338 + 93) / 16144][Stage 172:==============================>                 (10339 + 92) / 16144][Stage 172:==============================>                 (10340 + 93) / 16144][Stage 172:==============================>                 (10341 + 92) / 16144][Stage 172:==============================>                 (10341 + 93) / 16144][Stage 172:==============================>                 (10342 + 93) / 16144][Stage 172:==============================>                 (10343 + 93) / 16144][Stage 172:==============================>                 (10344 + 92) / 16144][Stage 172:==============================>                 (10345 + 92) / 16144][Stage 172:==============================>                 (10345 + 93) / 16144][Stage 172:==============================>                 (10346 + 93) / 16144][Stage 172:==============================>                 (10348 + 92) / 16144][Stage 172:==============================>                 (10348 + 93) / 16144][Stage 172:==============================>                 (10349 + 92) / 16144][Stage 172:==============================>                 (10349 + 93) / 16144][Stage 172:==============================>                 (10350 + 92) / 16144][Stage 172:==============================>                 (10352 + 92) / 16144][Stage 172:==============================>                 (10353 + 92) / 16144][Stage 172:==============================>                 (10354 + 92) / 16144][Stage 172:==============================>                 (10355 + 92) / 16144][Stage 172:==============================>                 (10356 + 92) / 16144][Stage 172:==============================>                 (10356 + 93) / 16144][Stage 172:==============================>                 (10357 + 92) / 16144][Stage 172:==============================>                 (10357 + 93) / 16144][Stage 172:==============================>                 (10358 + 92) / 16144][Stage 172:==============================>                 (10358 + 93) / 16144][Stage 172:==============================>                 (10359 + 92) / 16144][Stage 172:==============================>                 (10359 + 93) / 16144][Stage 172:==============================>                 (10359 + 94) / 16144][Stage 172:==============================>                 (10361 + 92) / 16144][Stage 172:==============================>                 (10361 + 93) / 16144][Stage 172:==============================>                 (10362 + 92) / 16144][Stage 172:==============================>                 (10362 + 93) / 16144][Stage 172:==============================>                 (10363 + 93) / 16144][Stage 172:==============================>                 (10364 + 92) / 16144][Stage 172:==============================>                 (10365 + 92) / 16144][Stage 172:==============================>                 (10365 + 93) / 16144][Stage 172:==============================>                 (10366 + 92) / 16144][Stage 172:==============================>                 (10366 + 93) / 16144][Stage 172:==============================>                 (10367 + 92) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144][Stage 172:==============================>                 (10367 + 93) / 16144]Exception in thread ""JobGenerator"" java.lang.OutOfMemoryError: Java heap space	at com.fasterxml.jackson.core.util.BufferRecycler.calloc(BufferRecycler.java:156)	at com.fasterxml.jackson.core.util.BufferRecycler.allocCharBuffer(BufferRecycler.java:124)	at com.fasterxml.jackson.core.io.IOContext.allocTokenBuffer(IOContext.java:189)	at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:879)Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""JobGenerator""Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: Java heap space	at java.util.Arrays.copyOfRange(Arrays.java:3664)	at java.lang.String.<init>(String.java:207)	at java.lang.StringBuilder.toString(StringBuilder.java:407)	at scala.collection.mutable.StringBuilder.toString(StringBuilder.scala:430)	at org.apache.spark.ui.ConsoleProgressBar.show(ConsoleProgressBar.scala:101)	at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:71)	at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55)	at java.util.TimerThread.mainLoop(Timer.java:555)	at java.util.TimerThread.run(Timer.java:505)
>
>


-- 
Ryan Blue
Software Engineer
Netflix
"
Denis Bolshakov <bolshakov.denis@gmail.com>,"Fri, 10 Feb 2017 22:03:24 +0300",Re: Request for comments: Java 7 removal,Sean Owen <sowen@cloudera.com>,"Hello Sean,

Thanks for asking.

release.
But as a lot of users still use java 7 could you please share your vision
about bug fix releases for 2.0 and 2.1?

About python 2.6
https://www.python.org/download/releases/2.6/
Python 2.6 (final) was released on October 1st, 2008.

If supporting python 2.6 has any costs I would definitely remove that.

Kind regards,
Denis





-- 
//with Best Regards
--Denis Bolshakov
e-mail: bolshakov.denis@gmail.com
"
Koert Kuipers <koert@tresata.com>,"Fri, 10 Feb 2017 14:22:54 -0500",benefits of code gen,"""dev@spark.apache.org"" <dev@spark.apache.org>","so i have been looking for a while now at all the catalyst expressions, and
all the relative complex codegen going on.

so first off i get the benefit of codegen to turn a bunch of chained
iterators transformations into a single codegen stage for spark. that makes
sense to me, because it avoids a bunch of overhead.

but what i am not so sure about is what the benefit is of converting the
actual stuff that happens inside the iterator transformations into codegen.

say if we have an expression that has 2 children and creates a struct for
them. why would this be faster in codegen by re-creating the code to do
this in a string (which is complex and error prone) compared to simply have
the codegen call the normal method for this in my class?

i see so much trivial code be re-created in codegen. stuff like this:

  private[this] def castToDateCode(
      from: DataType,
      ctx: CodegenContext): CastFunction = from match {
    case StringType =>
      val intOpt = ctx.freshName(""intOpt"")
      (c, evPrim, evNull) => s""""""
        scala.Option<Integer> $intOpt =
          org.apache.spark.sql.catalyst.util.DateTimeUtils.stringToDate($c);
        if ($intOpt.isDefined()) {
          $evPrim = ((Integer) $intOpt.get()).intValue();
        } else {
          $evNull = true;
        }
       """"""

is this really faster than simply calling an equivalent functions from the
codegen, and keeping the codegen logic restricted to the ""unrolling"" of
chained iterators?
"
Reynold Xin <rxin@databricks.com>,"Fri, 10 Feb 2017 20:24:39 +0100",Re: benefits of code gen,Koert Kuipers <koert@tresata.com>,"With complex types it doesn't work as well, but for primitive types the
biggest benefit of whole stage codegen is that we don't even need to put
the intermediate data into rows or columns anymore. They are just variables
(stored in CPU registers).


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Fri, 10 Feb 2017 19:36:57 +0000",spark sql versus interactive hive versus hive,"""dev@spark.apache.org"" <dev@spark.apache.org>","Folks,

I'm embarking on a project to build a POC around spark sql, I was wondering if anyone has experience in comparing spark sql with hive or interactive hive and data points around the types of queries suited for both, I am naively assuming that spark sql will beat hive in all queries given that computations are mostly done in memory but want to hear some more data  points around queries that maybe problematic in spark-sql, also are there debugging tools people ordinarily use with spark-sql to troubleshoot perf related issues.


I look forward to hearing from the community.

Regards
"
Sean Owen <sowen@cloudera.com>,"Fri, 10 Feb 2017 20:21:59 +0000",Re: Request for comments: Java 7 removal,Denis Bolshakov <bolshakov.denis@gmail.com>,"As usual I think maintenance release branches are created ad-hoc when there
seems to be some demand. I personally would guess there will be at least
one more 2.0.x and 2.1.x maintenance release. In that sense, yeah it's not
really even the end of actively supporting a Java 7-compatible release.


"
Koert Kuipers <koert@tresata.com>,"Fri, 10 Feb 2017 16:32:55 -0500",Re: benefits of code gen,Reynold Xin <rxin@databricks.com>,"based on that i take it that math functions would be primary beneficiaries
since they work on primitives.

so if i take UnaryMathExpression as an example, would i not get the same
benefit if i change it to this?

abstract class UnaryMathExpression(val f: Double => Double, name: String)
  extends UnaryExpression with Serializable with ImplicitCastInputTypes {

  override def inputTypes: Seq[AbstractDataType] = Seq(DoubleType)
  override def dataType: DataType = DoubleType
  override def nullable: Boolean = true
  override def toString: String = s""$name($child)""
  override def prettyName: String = name

  protected override def nullSafeEval(input: Any): Any = {
    f(input.asInstanceOf[Double])
  }

  // name of function in java.lang.Math
  def funcName: String = name.toLowerCase

  def function(d: Double): Double = f(d)

  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
    val self = ctx.addReferenceObj(name, this, getClass.getName)
    defineCodeGen(ctx, ev, c => s""$self.function($c)"")
  }
}

admittedly in this case the benefit in terms of removing complex codegen is
not there (the codegen was only one line), but if i can remove codegen here
i could also remove it in lots of other places where it does get very
unwieldy simply by replacing it with calls to methods.

Function1 is specialized, so i think (or hope) that my version does no
extra boxes/unboxing.


"
Sam Elamin <hussam.elamin@gmail.com>,"Fri, 10 Feb 2017 21:35:38 +0000",[Newbie] spark conf,dev <dev@spark.apache.org>,"Hi All,


really newbie question here folks, i have properties like my aws access and
secret keys in the core-site.xml in hadoop among other properties, but
thats the only reason I have hadoop installed which seems a bit of an
overkill.

Is there an equivalent of core-site.xml for spark so I dont have to
reference the HADOOP_CONF_DIR in my spark env.sh?

I know I can export env variables for the AWS credentials but other
properties that my application might want to use?

Regards
Sam
"
Reynold Xin <rxin@databricks.com>,"Fri, 10 Feb 2017 22:36:46 +0100",Re: [Newbie] spark conf,Sam Elamin <hussam.elamin@gmail.com>,"You can put them in spark's own conf/spark-defaults.conf file


"
Sam Elamin <hussam.elamin@gmail.com>,"Fri, 10 Feb 2017 21:39:20 +0000",Re: [Newbie] spark conf,Reynold Xin <rxin@databricks.com>,"yeah I thought of that but the file made it seem that its environment
specific rather than application specific configurations

Im more interested in the best practices, would you recommend using the
default conf file for this and uploading them to where the application will
be running (remote clusters etc) ?


Regards
Sam


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 10 Feb 2017 13:42:58 -0800",Re: [Newbie] spark conf,Sam Elamin <hussam.elamin@gmail.com>,"If you place core-site.xml in $SPARK_HOME/conf, I'm pretty sure Spark
will pick it up. (Sounds like you're not running YARN, which would
require HADOOP_CONF_DIR.)

Also this is more of a user@ question.




-- 
Marcelo

---------------------------------------------------------------------


"
Sam Elamin <hussam.elamin@gmail.com>,"Fri, 10 Feb 2017 21:48:53 +0000",Re: [Newbie] spark conf,Marcelo Vanzin <vanzin@cloudera.com>,"yup that worked

Thanks for the clarification!


"
Michael Armbrust <michael@databricks.com>,"Fri, 10 Feb 2017 13:56:25 -0800",Re: benefits of code gen,Koert Kuipers <koert@tresata.com>,"Function1 is specialized, but nullSafeEval is Any => Any, so that's still
going to box in the non-codegened execution path.


"
Koert Kuipers <koert@tresata.com>,"Fri, 10 Feb 2017 17:08:21 -0500",Re: benefits of code gen,Michael Armbrust <michael@databricks.com>,"yes agreed. however i believe nullSafeEval is not used for codegen?


"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Sat, 11 Feb 2017 09:22:15 +0100",Re: spark sql versus interactive hive versus hive,Saikat Kanjilal <sxk1969@hotmail.com>,"I think this is a rather simplistic view. All the tools to computation in-memory in the end. For certain type of computation and usage patterns it makes sense to keep them in memory. For example, most of the machine learning approaches require to include the same data in several iterative calculations. This is what Spark has been designed for. Most aggregations/precalculations are just done by using the data in-memory once. Here is where Hive+Tez and to a limited extend Spark can help. The third pattern, where users interactively query the data i.e. Many concurrent users query the same or similar data very frequently, is addressed by Hive on Tez + Llap, Hive Tez+ Ignite or Spark + ignite ( and there are other tools).

So it is important to understand what your users want to do.

Then, you have a lot of benchmark data on the web to compare. However I always recommend to generate or use data yourself that fits to the data the company is using. Keep also in mind that time is needed to convert this data in a efficient format.

g if anyone has experience in comparing spark sql with hive or interactive hive and data points around the types of queries suited for both, I am naively assuming that spark sql will beat hive in all queries given that computations are mostly done in memory but want to hear some more data  points around queries that maybe problematic in spark-sql, also are there debugging tools people ordinarily use with spark-sql to troubleshoot perf related issues.
"
Saikat Kanjilal <sxk1969@hotmail.com>,"Sat, 11 Feb 2017 15:54:14 +0000",Re: spark sql versus interactive hive versus hive,=?utf-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"Thanks Jorn for the input, our users want to run queries that perform large aggregations of data from different tables as well as simple ad hockey queries over 1 table.  The tables are all in orc format, they're currently using the hive plus tez architecture that you mention but experiencing perf issues, one of the things we're considering is to move them to spark sql where it makes sense which is why I wanted to know people's experience in using the various tools.

Sent from my iPhone

On Feb 11, 2017, at 12:22 AM, J枚rn Franke <jornfranke@gmail.com<mailto:jornfranke@gmail.com>> wrote:

I think this is a rather simplistic view. All the tools to computation in-memory in the end. For certain type of computation and usage patterns it makes sense to keep them in memory. For example, most of the machine learning approaches require to include the same data in several iterative calculations. This is what Spark has been designed for. Most aggregations/precalculations are just done by using the data in-memory once. Here is where Hive+Tez and to a limited extend Spark can help. The third pattern, where users interactively query the data i.e. Many concurrent users query the same or similar data very frequently, is addressed by Hive on Tez + Llap, Hive Tez+ Ignite or Spark + ignite ( and there are other tools).

So it is important to understand what your users want to do.

Then, you have a lot of benchmark data on the web to compare. However I always recommend to generate or use data yourself that fits to the data the company is using. Keep also in mind that time is needed to convert this data in a efficient format.

On 10 Feb 2017, at 20:36, Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>> wrote:


Folks,

I'm embarking on a project to build a POC around spark sql, I was wondering if anyone has experience in comparing spark sql with hive or interactive hive and data points around the types of queries suited for both, I am naively assuming that spark sql will beat hive in all queries given that computations are mostly done in memory but want to hear some more data  points around queries that maybe problematic in spark-sql, also are there debugging tools people ordinarily use with spark-sql to troubleshoot perf related issues.


I look forward to hearing from the community.

Regards
"
Cody Koeninger <cody@koeninger.org>,"Sat, 11 Feb 2017 10:57:48 -0500",Re: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"At the spark summit this week, everyone from PMC members to users I had
never met before were asking me about the Spark improvement proposals
idea.  It's clear that it's a real community need.

But it's been almost half a year, and nothing visible has been done.

Reynold, are you going to do this?

If so, when?

If not, why?

You already did the right thing by including long-deserved committers.
Please keep doing the right thing for the community.


w
P
t
 in
hor
ho
way.
e
nd
.
int.
icit
the
ast,
lse,
ng are:
e
sed on
e
Z7SUi4qMljg/edit#>
:
s
a
n
g>
t
t
s
t
-
o
e
ot
r
 a
s
e
.
t
nd
s
t
va
QL
n
y
re
a
r
e
ny
l
.
-
"
Xiao Li <gatorsmile@gmail.com>,"Sat, 11 Feb 2017 09:13:37 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"During the summit, I also had a lot of discussions over similar topics with
multiple Committers and active users. I heard many fantastic ideas. I
believe Spark improvement proposals are good channels to collect the
requirements/designs.


IMO, we also need to consider the priority when working on these items.
Even if the proposal is accepted, it does not mean it will be implemented
and merged immediately. It is not a FIFO queue.


Even if some PRs are merged, sometimes, we still have to revert them back,
if the design and implementation are not reviewed carefully. We have to
ensure our quality. Spark is not an application software. It is an
infrastructure software that is being used by many many companies. We have
to be very careful in the design and implementation, especially
adding/changing the external APIs.


When I developed the Mainframe infrastructure/middleware software in the
past 6 years, I were involved in the discussions with external/internal
customers. The to-do feature list was always above 100. Sometimes, the
customers are feeling frustrated when we are unable to deliver them on time
due to the resource limits and others. Even if they paid us billions, we
still need to do it phase by phase or sometimes they have to accept the
workarounds. That is the reality everyone has to face, I think.


Thanks,


Xiao Li

2017-02-11 7:57 GMT-08:00 Cody Koeninger <cody@koeninger.org>:

ew
u
IP
at
d in
thor
e
who
yway.
d
end
e
n.
d
oint.
licit
o
 the
east,
else,
s
ing are:
lored
nZ7SUi4qMljg/edit#>
g
g>
s
y
k
e
f
o
e
s
d
r
rs
?
d
e
o
e
on
s
s
o
s
r.
y
"
Shivam Sharma <28shivamsharma@gmail.com>,"Sun, 12 Feb 2017 22:40:35 +0530",Add hive-site.xml at runtime,dev@spark.apache.org,"Hi,

I have multiple hive configurations(hive-site.xml) and because of that only
I am not able to add any hive configuration in spark *conf* directory. I
want to add this configuration file at start of any *spark-submit* or
*spark-shell*. This conf file is huge so *--conf* is not a option for me.

Note :- I have tried user@spark.apache.org and my mail was bouncing each
time so Sean Owen suggested to mail dev.(
https://issues.apache.org/jira/browse/SPARK-19546). Please give solution to
above ticket also if possible.

Thanks

-- 
Shivam Sharma
"
StanZhai <mail@zhaishidan.cn>,"Mon, 13 Feb 2017 01:33:07 -0700 (MST)","Re: Executors exceed maximum memory defined with
 `--executor-memory` in Spark 2.1.0",dev@spark.apache.org,"I've filed a JIRA about this problem. 
https://issues.apache.org/jira/browse/SPARK-19532
<https://issues.apache.org/jira/browse/SPARK-19532>  

I've tried to set `spark.speculation` to `false`, but the off-heap also
exceed about 10G after triggering a FullGC to the Executor
process(--executor-memory 30G), as follow:

test      105371  106 21.5 67325492 42621992 ?   Sl   15:20  55:14
/home/test/service/jdk/bin/java -cp
/home/test/service/hadoop/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/test/service/hadoop/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:/home/test/service/spark/conf/:/home/test/service/spark/jars/*:/home/test/service/hadoop/etc/hadoop/
-Xmx30720M -Dspark.driver.port=9835 -Dtag=spark_2_1_test -XX:+PrintGCDetails
-XX:+PrintGCDateStamps -Xloggc:./gc.log -verbose:gc
org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url
spark://CoarseGrainedScheduler@172.16.34.235:9835 --executor-id 4 --hostname
test-192 --cores 36 --app-id app-20170213152037-0043 --worker-url
spark://Worker@test-192:33890

So, I think these are also other reasons for this problem.

We have been trying to upgrade our Spark from the releasing of Spark 2.1.0.

This version is unstable and not available for us because of the memory
problems, we should pay attention to this.


StanZhai wrote
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n20881/QQ20170207-212340.png> 





--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 13 Feb 2017 16:02:00 +0100",Re: Spark Improvement Proposals,Xiao Li <gatorsmile@gmail.com>,"Here's a new draft that incorporated most of the feedback:
https://docs.google.com/document/d/1-Zdi_W-wtuxS9hTK0P9qb2x-nRanvXmnZ7SUi4qMljg/edit#

I added a specific role for SPIP Author and another one for SPIP Shepherd.


I
,
e
me
:
.
few
?
PIP
d
hat
ed in
uthor
re
r
 who
nyway.
 do
ly in.
s
nd
point.
plicit
that
e
f the
least,
 else,
m
ilored
mnZ7SUi4qMljg/edit#>
t
s
e
m>
s
s
e?
t
l
r
d
t
s
e
s
o
e
s
k
"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 13 Feb 2017 09:31:20 -0800",Re: Add hive-site.xml at runtime,Shivam Sharma <28shivamsharma@gmail.com>,"Shivam,

We add hive-site.xml at runtime. We use --driver-class-path to add it to
the driver and --jars to add it for the executors.

rb





-- 
Ryan Blue
Software Engineer
Netflix
"
Koert Kuipers <koert@tresata.com>,"Mon, 13 Feb 2017 13:06:34 -0500",Re: benefits of code gen,Sumedh Wale <swale@snappydata.io>,"thanks for that detailed response!


"
Reynold Xin <rxin@databricks.com>,"Mon, 13 Feb 2017 20:16:32 +0100",welcoming Takuya Ueshin as a new Apache Spark committer,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Takuya-san has recently been elected an Apache Spark committer. He's been
active in the SQL area and writes very small, surgical patches that are
high quality. Please join me in congratulating Takuya-san!
"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 13 Feb 2017 11:24:28 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Reynold Xin <rxin@databricks.com>,"Congratulations, Takuya!

Bests,
Dongjoon.


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 13 Feb 2017 11:24:39 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Reynold Xin <rxin@databricks.com>,"Congratulations Takuya-san :D!




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Xiao Li <gatorsmile@gmail.com>,"Mon, 13 Feb 2017 11:25:37 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Holden Karau <holden@pigscanfly.ca>,"Congratulations, Takuya!

Xiao

2017-02-13 11:24 GMT-08:00 Holden Karau <holden@pigscanfly.ca>:

"
Xuefu Zhang <xuefu@uber.com>,"Mon, 13 Feb 2017 11:29:12 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Xiao Li <gatorsmile@gmail.com>,"Congratulations, Takuya!

--Xuefu


"
Felix Cheung <felixcheung_m@hotmail.com>,"Mon, 13 Feb 2017 19:33:33 +0000",Re: welcoming Takuya Ueshin as a new Apache Spark committer,"Xiao Li <gatorsmile@gmail.com>, Xuefu Zhang <xuefu@uber.com>","Congratulations!


________________________________
From: Xuefu Zhang <xuefu@uber.com>
Sent: Monday, February 13, 2017 11:29:12 AM
To: Xiao Li
Cc: Holden Karau; Reynold Xin; dev@spark.apache.org
Subject: Re: welcoming Takuya Ueshin as a new Apache Spark committer

Congratulations, Takuya!

--Xuefu

Congratulations, Takuya!

Xiao

2017-02-13 11:24 GMT-08:00 Holden Karau <holden@pigscanfly.ca<mailto:holden@pigscanfly.ca>>:
Congratulations Takuya-san :D!

Hi all,

Takuya-san has recently been elected an Apache Spark committer. He's been active in the SQL area and writes very small, surgical patches that are high quality. Please join me in congratulating Takuya-san!





--
Cell : 425-233-8271<tel:(425)%20233-8271>
Twitter: https://twitter.com/holdenkarau


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Feb 2017 20:03:17 +0000",Re: welcoming Takuya Ueshin as a new Apache Spark committer,"Felix Cheung <felixcheung_m@hotmail.com>, Xiao Li <gatorsmile@gmail.com>, 
	Xuefu Zhang <xuefu@uber.com>","Congratulations, Takuya! 


:
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Mon, 13 Feb 2017 22:02:46 +0100",Re: welcoming Takuya Ueshin as a new Apache Spark committer,dev@spark.apache.org,"Congratulations!




---------------------------------------------------------------------


"
Sam Elamin <hussam.elamin@gmail.com>,"Mon, 13 Feb 2017 21:05:52 +0000",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Congrats Takuya-san! Clearly well deserved! Well done :)


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 14 Feb 2017 07:13:49 +0900",Re: welcoming Takuya Ueshin as a new Apache Spark committer,dev <dev@spark.apache.org>,"congrats!





-- 
---
Takeshi Yamamuro
"
Burak Yavuz <brkyvz@gmail.com>,"Mon, 13 Feb 2017 14:19:21 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Dilip Biswal <dbiswal@us.ibm.com>,"Congrats Takuya!


"
Neelesh Salian <neeleshssalian@gmail.com>,"Mon, 13 Feb 2017 14:27:44 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Reynold Xin <rxin@databricks.com>,"Congratulations, Takuya!




-- 
Regards,
Neelesh S. Salian
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Mon, 13 Feb 2017 23:38:33 +0100",Re: welcoming Takuya Ueshin as a new Apache Spark committer,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats Takuya!


:
n


-- 

Herman van H枚vell

Software Engineer

Databricks Inc.

hvanhovell@databricks.com

+31 6 420 590 27

databricks.com

[image: http://databricks.com] <http://databricks.com/>
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 13 Feb 2017 15:01:39 -0800",[PYTHON][DISCUSS] Moving to cloudpickle and or Py4J as a dependencies?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi PySpark Developers,

Cloudpickle is a core part of PySpark, and is originally copied from (and
improved from) picloud. Since then other projects have found cloudpickle
useful and a fork of cloudpickle <https://github.com/cloudpipe/cloudpickle> was
created and is now maintained as its own library
<https://pypi.python.org/pypi/cloudpickle> (with better test coverage and
resulting bug fixes I understand). We've had a few PRs backporting fixes
from the cloudpickle project into Spark's local copy of cloudpickle - how
would people feel about moving to taking an explicit (pinned) dependency on
cloudpickle?

We could add cloudpickle to the setup.py and a requirements.txt file for
users who prefer not to do a system installation of PySpark.

Py4J is maybe even a simpler case, we currently have a zip of py4j in our
repo but could instead have a pinned version required. While we do depend
on a lot of py4j internal APIs, version pinning should be sufficient to
ensure functionality (and simplify the update process).

Cheers,

Holden :)

-- 
Twitter: https://twitter.com/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Tue, 14 Feb 2017 00:03:28 +0100",Re: [PYTHON][DISCUSS] Moving to cloudpickle and or Py4J as a dependencies?,Holden Karau <holden@pigscanfly.ca>,"With any dependency update (or refactoring of existing code), I always ask
this question: what's the benefit? In this case it looks like the benefit
is to reduce efforts in backports. Do you know how often we needed to do
those?



"
Holden Karau <holden@pigscanfly.ca>,"Mon, 13 Feb 2017 15:22:59 -0800",Re: [PYTHON][DISCUSS] Moving to cloudpickle and or Py4J as a dependencies?,Reynold Xin <rxin@databricks.com>,"It's a good question. Py4J seems to have been updated 5 times in 2016 and
is a bit involved (from a review point of view verifying the zip file
contents is somewhat tedious).

cloudpickle is a bit difficult to tell since we can have changes to
cloudpickle which aren't correctly tagged as backporting changes from the
fork (and this can take awhile to review since we don't always catch them
right away as being backports).

Another difficulty with looking at backports is that since our review
process for PySpark has historically been on the slow side, changes
benefiting systems like dask or IPython parallel were not backported to
Spark unless they caused serious errors.

I think the key benefits are better test coverage of the forked version of
cloudpickle, using a more standardized packaging of dependencies, simpler
updates of dependencies reduces friction to gaining benefits from other
related projects work - Python serialization really isn't our secret sauce.

If I'm missing any substantial benefits or costs I'd love to know :)




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Tue, 14 Feb 2017 08:24:19 +0900",Re: welcoming Takuya Ueshin as a new Apache Spark committer,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congratulations,  Takuya!

- Kousuke


"
Asher Krim <akrim@hubspot.com>,"Mon, 13 Feb 2017 18:29:30 -0500",Re: welcoming Takuya Ueshin as a new Apache Spark committer,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congrats!

Asher Krim
Senior Software Engineer


m
t
"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Tue, 14 Feb 2017 08:29:25 +0900",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Reynold Xin <rxin@databricks.com>,"Congrats!

Kazuaki Ishizaki



From:   Reynold Xin <rxin@databricks.com>
To:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   2017/02/14 04:18
Subject:        welcoming Takuya Ueshin as a new Apache Spark committer



Hi all,

Takuya-san has recently been elected an Apache Spark committer. He's been 
active in the SQL area and writes very small, surgical patches that are 
high quality. Please join me in congratulating Takuya-san!




"
Yanbo Liang <ybliang8@gmail.com>,"Mon, 13 Feb 2017 16:47:48 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>,"Congratulations!


"
Charles Allen <charles.allen@metamarkets.com>,"Tue, 14 Feb 2017 01:14:56 +0000",Re: Request for comments: Java 7 removal,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","I think the biggest concern is enterprise users/operators who do not have
the authority or access to upgrade hadoop/yarn clusters to java8. As a
reference point, apparently CDH 5.3
<https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_rn_new_in_cdh_53.html#concept_c1n_bln_tj>
 shipped with java 8 in December 2014. I would be surprised if such users
were active consumers of the dev mailing list, though. Unfortunately
there's a bit of a selection bias in this list.

The other concern is if there is guaranteed compatibility between scala and
java8 for all versions you want to use (which is somewhat touched upon in
the PR). Are you thinking about supporting scala 2.10 against java 8 byte
code?

See https://groups.google.com/d/msg/druid-user/aTGQlnF1KLk/NvBPfmigAAAJ for
the similar discussion that went forward in the Druid community.



"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 14 Feb 2017 10:41:42 +0900",Re: Strange behavior with 'not' and filter pushdown,Xiao Li <gatorsmile@gmail.com>,"cc: xiao

IIUC a xiao's commit below fixed this issue in master.
https://github.com/apache/spark/commit/2eb093decb5e87a1ea71bbaa28092876a8c84996

Is this fix worth backporting to the v2.0 branch?
I checked I could reproduce there:

---

scala> Seq((1, ""a""), (2, ""b""), (3, null)).toDF(""c0"",
""c1"").write.mode(""overwrite"").parquet(""/Users/maropu/Desktop/data"")
scala>
spark.read.parquet(""/Users/maropu/Desktop/data"").createOrReplaceTempView(""t"")
scala> val df = sql(""SELECT c0 FROM t WHERE NOT(c1 IS NOT NULL)"")
scala> df.explain(true)
== Parsed Logical Plan ==
'Project ['c0]
+- 'Filter NOT isnotnull('c1)
   +- 'UnresolvedRelation `t`

== Analyzed Logical Plan ==
c0: int
Project [c0#16]
+- Filter NOT isnotnull(c1#17)
   +- SubqueryAlias t
      +- Relation[c0#16,c1#17] parquet

== Optimized Logical Plan ==
Project [c0#16]
+- Filter (isnotnull(c1#17) && NOT isnotnull(c1#17))
           ^^^^^^^^^^^^^^^^
   +- Relation[c0#16,c1#17] parquet

== Physical Plan ==
*Project [c0#16]
+- *Filter (isnotnull(c1#17) && NOT isnotnull(c1#17))
   +- *BatchedScan parquet [c0#16,c1#17] Format: ParquetFormat, InputPaths:
file:/Users/maropu/Desktop/data, PartitionFilters: [], PushedFilters:
[IsNotNull(c1), Not(IsNotNull(c1))], ReadSchema: struct<c0:int,c1:string>

scala> df.show
+---+
| c0|
+---+
+---+




// maropu


d

ll(username),
ct=false)],
Distinct=false)],
t
th simple
ny
blem
e
not(is not null),
ll*
t
m
y of NOT


-- 
---
Takeshi Yamamuro
"
Takuya UESHIN <ueshin@happy-camper.st>,"Tue, 14 Feb 2017 11:54:41 +0900",Re: welcoming Takuya Ueshin as a new Apache Spark committer,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thank you very much everyone!
I really look forward to working with you!





-- 
Takuya UESHIN
Tokyo, Japan

http://twitter.com/ueshin
"
Xiao Li <gatorsmile@gmail.com>,"Mon, 13 Feb 2017 19:32:33 -0800",Re: Strange behavior with 'not' and filter pushdown,Takeshi Yamamuro <linguin.m.s@gmail.com>,"https://github.com/apache/spark/pull/16894

Already backported to Spark 2.0

Thanks!

Xiao

2017-02-13 17:41 GMT-08:00 Takeshi Yamamuro <linguin.m.s@gmail.com>:

ull(username),
nct=false)],
sDistinct=false)],
ith
ws
et
oblem
he
not(is not null),
ot
am
ty of NOT
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Tue, 14 Feb 2017 12:33:49 +0900",Re: Strange behavior with 'not' and filter pushdown,Xiao Li <gatorsmile@gmail.com>,"Oh, Thanks for checking!


Null(username),
inct=false)],
isDistinct=false)],
:
with
ows
get
roblem
the
not(is not null),
ity of


-- 
---
Takeshi Yamamuro
"
Jakub Dubovsky <spark.dubovsky.jakub@gmail.com>,"Tue, 14 Feb 2017 10:35:45 +0100",Re: Cannot find checkstyle.xml,dev@spark.apache.org,"Somebody is able to help with this? I am stuck on this in my attempt to
help solve issues:

SPARK-16599 <https://issues.apache.org/jira/browse/SPARK-16599>
sparkNB-807 <https://github.com/andypetrella/spark-notebook/issues/807>

Thanks


"
Anis Nasir <aadi.anis@gmail.com>,"Tue, 14 Feb 2017 13:57:13 +0000",Fwd: Handling Skewness and Heterogeneity,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear all,

Can you please comment on the below mentioned use case.

Thanking you in advance

Regards,
Anis


---------- Forwarded message ---------
From: Anis Nasir <aadi.anis@gmail.com>
Date: Tue, 14 Feb 2017 at 17:01
Subject: Handling Skewness and Heterogeneity
To: <user@spark.apache.org>


Dear All,

I have few use cases for spark streaming where spark cluster consist of
heterogenous machines.

Additionally, there is skew present in both the input distribution (e.g.,
each tuple is drawn from a zipf distribution) and the service time (e.g.,
service time required for each tuple comes from a zipf distribution).

I want to know who spark will handle such use cases.

Any help will be highly appreciated!


Regards,
Anis
"
Aseem Bansal <asmbansal2@gmail.com>,"Tue, 14 Feb 2017 20:02:56 +0530","Fwd: tylerchapman@yahoo-inc.com is no longer with Yahoo! (was:
 Dealing with missing columns in SPARK SQL in JSON)",dev@spark.apache.org,"Can someone please remove tylerchapman@yahoo-inc.com from the mailing list?
I was told in a spark JIRA that dev mailing list is the right place to ask
for this.

---------- Forwarded message ----------
From: Yahoo! No Reply <postmaster@yahoo-inc.com>
Date: Tue, Feb 14, 2017 at 8:00 PM
Subject: tylerchapman@yahoo-inc.com is no longer with Yahoo! (was: Dealing
with missing columns in SPARK SQL in JSON)
To: asmbansal2@gmail.com



This is an automatically generated message.

tylerchapman@yahoo-inc.com is no longer with Yahoo! Inc.

Your message will not be forwarded.

If you have a sales inquiry, please email yahoosales@yahoo-inc.com and
someone will follow up with you shortly.

If you require assistance with a legal matter, please send a message to
legal-notices@yahoo-inc.com

Thank you!
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 14 Feb 2017 16:19:15 +0100","Re: [PYTHON][DISCUSS] Moving to cloudpickle and or Py4J as a
 dependencies?",dev@spark.apache.org,"I don't have any strong views, so just to highlight possible issues:

  * Based on different issues I've seen there is a substantial amount of
    users which depend on system wide Python installations. As far as I
    am aware neither Py4j nor cloudpickle are present in the standard
    system repositories in Debian or Red Hat derivatives.
  * Assuming that Spark is committed to supporting Python 2 beyond its
    end of life we have to be sure that any external dependency has the
    same policy.
  * Py4j is missing from default Anaconda channel. Not a big issue, just
    a small annoyance.
  * External dependencies with pinned versions add some overhead to the
    development across versions (effectively we may need a separate env
    for each major Spark release). I've seen small inconsistencies in
    PySpark behavior with different Py4j versions so it is not
    completely hypothetical.
  * Adding possible version conflicts. It is probably not a big risk but
    something to consider (for example in combination Blaze + Dask +
    PySpark).
  * Adding another party user has to trust.



-- 
Maciej Szymkiewicz

"
Sean Owen <sowen@cloudera.com>,"Tue, 14 Feb 2017 19:21:33 +0000",Re: Request for comments: Java 7 removal,"Charles Allen <charles.allen@metamarkets.com>, dev <dev@spark.apache.org>","Yes, that's a key concern about the Java dependency, that its update is a
function of the OS packages and those who control them, which is often not
the end user. I think that's why this has been delayed a while. My general
position is that, of course, someone in that boat can use Spark 2.1.x. It's
likely going to see maintenance releases through the end of the year, even.
while. It wouldn't surprise me if some people are yet still stuck on Java
7; it would surprise me if they expect to use the latest of any package at
this stage. Taking your CDH example, yes it's been a couple years since
people have been able to deploy it on Java 8. Spark 2 isn't supported
before 5.7 anyway. The default is Java 8.

Scala 2.10 is a good point that we are dealing with now. It's not really a
question of whether it will run -- it's all libraries and bytecode to the
JVM and it will happily deal with a mix of 7 and 8 bytecode. It's a
question of whether the build for 2.10 will succeed. I believe it's 'yes'
but am following up on some tests there.


"
Koert Kuipers <koert@tresata.com>,"Tue, 14 Feb 2017 17:46:56 -0500",Re: Request for comments: Java 7 removal,Sean Owen <sowen@cloudera.com>,"what about the conversation about dropping scala 2.10?


"
Yuming Wang <wgyumg@gmail.com>,"Wed, 15 Feb 2017 09:37:41 +0800",Re: Request for comments: Java 7 removal,Koert Kuipers <koert@tresata.com>,"There is a way only Spark use Java 8, Hadoop still use Java 7:
spark-conf.jpg
(58K)
<https://mail.google.com/mail/u/0/?ui=2&ik=a9af0e7eb1&view=att&th=15a3f68a367de778&attid=0.1&disp=safe&realattid=f_iz6aduk80&zw>



By the way, I have a way to install any spark version on CM5.4 - CM5.7 by
custom CSD <https://github.com/wangyum/cm_csds/tree/master/SPARK> and
custom Spark parcel <https://github.com/wangyum/spark-parcel>.



---------------------------------------------------------------------"
Cody Koeninger <cody@koeninger.org>,"Tue, 14 Feb 2017 19:53:24 -0600",Re: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"Thanks for doing that.

Given that there are at least 4 different Apache voting processes, ""typical
Apache vote process"" isn't meaningful to me.

I think the intention is that in order to pass, it needs at least 3 +1
votes from PMC members *and no -1 votes from PMC members*.  But the
document doesn't explicitly say that second part.

There's also no mention of the duration a vote should remain open.  There's
a mention of a month for finding a shepherd, but that's different.

Other than that, LGTM.


.
 I
d
ve
ve
ime
e
t
e.
 few
k?
SPIP
ld
that
ied in
author
ere
e
or
e who
anyway.
e do
tly in.
ck and
 point.
xplicit
 that
se
f
of the
s
g
 least,
g else,
ailored
XmnZ7SUi4qMljg/edit#>
s
h
1
s
re
e
s
l
e
y
,
t
s
d
t
,
a
k
d
e
s
o
t
a
d
""
g
e
w
n
t
"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 15 Feb 2017 12:14:24 +0530",Update Public Documentation - SparkSession instead of SparkContext,Spark Dev List <dev@spark.apache.org>,"Hello Spark Dev Team,

I was working with my team having most of the confusion that why your
public documentation is not updated with SparkSession if SparkSession is
the ongoing extension and best practice instead of creating sparkcontext.

Thanks.
"
Reynold Xin <rxin@databricks.com>,"Wed, 15 Feb 2017 10:56:31 +0100",Re: Update Public Documentation - SparkSession instead of SparkContext,Chetan Khatri <chetan.opensource@gmail.com>,"There is an existing pull request to update it:
https://github.com/apache/spark/pull/16856

But it is a little bit tricky.




"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 15 Feb 2017 15:51:09 +0530",Re: Update Public Documentation - SparkSession instead of SparkContext,"Sean Owen <sowen@cloudera.com>, Spark Dev List <dev@spark.apache.org>","Sorry, The context i am referring is for below URL
http://spark.apache.org/docs/2.0.1/programming-guide.html




"
Sam Elamin <hussam.elamin@gmail.com>,"Wed, 15 Feb 2017 19:34:26 +0000",Structured Streaming Spark Summit Demo - Databricks people,dev <dev@spark.apache.org>,"Hey folks

This one is mainly aimed at the databricks folks, I have been trying to
replicate the cloudtrail demo <https://www.youtube.com/watch?v=IJmFTXvUZgY>
Micheal did at Spark Summit. The code for it can be found here
<https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/2070341989008532/3601578643761083/latest.html>

My question is how did you get the results to be displayed and updated
continusly in real time

I am also using databricks to duplicate it but I noticed the code link
mentions

 ""If you count the number of rows in the table, you should find the value
increasing over time. Run the following every few minutes.""
This leads me to believe that the version of Databricks that Micheal was
using for the demo is still not released, or at-least the functionality to
display those changes in real time aren't

Is this the case? or am I completely wrong?

Can I display the results of a structured streaming query in realtime using
the databricks ""display"" function?


Regards
Sam
"
Kay Ousterhout <kayousterhout@gmail.com>,"Wed, 15 Feb 2017 12:10:27 -0800",File JIRAs for all flaky test failures,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more
common than not now that the tests need to be re-run at least once on PRs
before they pass.  This is both annoying and problematic because it makes
it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins
fails on a PR (for a reason unrelated to the PR).  Just provide a quick
description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or
""Tests failed because 250m timeout expired"", a link to the failed build,
and include the ""Tests"" component.  If there's already a JIRA for the
issue, just comment with a link to the latest failure.  I know folks don't
always have time to track down why a test failed, but this it at least
helpful to someone else who, later on, is trying to diagnose when the issue
started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways
to make the tests less flaky!

-Kay
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 15 Feb 2017 20:13:48 +0000",Re: Structured Streaming Spark Summit Demo - Databricks people,"Sam Elamin <hussam.elamin@gmail.com>, dev <dev@spark.apache.org>","I don't think this is the right place for questions about Databricks. I'm
pretty sure they have their own website with a forum for questions about
their product.

Maybe this? https://forums.databricks.com/


"
Sam Elamin <hussam.elamin@gmail.com>,"Wed, 15 Feb 2017 20:21:20 +0000",Re: Structured Streaming Spark Summit Demo - Databricks people,"Nicholas Chammas <nicholas.chammas@gmail.com>, dev <dev@spark.apache.org>","Fair enough your absolutely right

Thanks for pointing me in the right direction

"
Chris Fregly <chris@fregly.com>,"Wed, 15 Feb 2017 12:21:16 -0800",Re: Structured Streaming Spark Summit Demo - Databricks people,"Sam Elamin <hussam.elamin@gmail.com>, dev <dev@spark.apache.org>, 
 Nicholas Chammas <nicholas.chammas@gmail.com>","Just be warned: the last time I asked a question about a non-working Databricks Keynote Demo from Spark Summit on the forum mentioned here, they deleted my question! And im a major contributor to those forums!!

Often times, those on-stage demos dont actually work until many months after theyre presented on stage - especially the proprietary demos involving dbutils() and display().

Chris Fregly
Research Scientist @ PipelineIO
Founder @ Advanced Spark and TensorFlow Meetup
San Francisco - Chicago - Washington DC - London

'm pretty sure they have their own website with a forum for questions about their product.
g to replicate the cloudtrail demo Micheal did at Spark Summit. The code for it can be found here
ted continusly in real time
ink mentions
ind the value increasing over time. Run the following every few minutes.""
l was using for the demo is still not released, or at-least the functionality to display those changes in real time aren't
me using the databricks ""display"" function?
"
Saikat Kanjilal <sxk1969@hotmail.com>,"Wed, 15 Feb 2017 20:24:19 +0000",Re: File JIRAs for all flaky test failures,"Kay Ousterhout <kayousterhout@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I was working on something to address this a while ago https://issues.apache.org/jira/browse/SPARK-9487 but the difficulty in testing locally made things a lot more complicated to fix for each of the unit tests, should we resurface this JIRA again, I would whole heartedly agree with the flakiness assessment of the unit tests.

[SPARK-9487] Use the same num. worker threads in Scala ...<https://issues.apache.org/jira/browse/SPARK-9487>
issues.apache.org
In Python we use `local[4]` for unit tests, while in Scala/Java we use `local[2]` and `local` for some unit tests in SQL, MLLib, and other components. If the ...




________________________________
From: Kay Ousterhout <kayousterhout@gmail.com>
Sent: Wednesday, February 15, 2017 12:10 PM
To: dev@spark.apache.org
Subject: File JIRAs for all flaky test failures

Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more common than not now that the tests need to be re-run at least once on PRs before they pass.  This is both annoying and problematic because it makes it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins fails on a PR (for a reason unrelated to the PR).  Just provide a quick description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or ""Tests failed because 250m timeout expired"", a link to the failed build, and include the ""Tests"" component.  If there's already a JIRA for the issue, just comment with a link to the latest failure.  I know folks don't always have time to track down why a test failed, but this it at least helpful to someone else who, later on, is trying to diagnose when the issue started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways to make the tests less flaky!

-Kay
"
Armin Braun <me@obrown.io>,"Wed, 15 Feb 2017 21:48:22 +0100",Re: File JIRAs for all flaky test failures,Saikat Kanjilal <sxk1969@hotmail.com>,"I think one thing that is contributing to this a lot too is the general
issue of the tests taking up a lot of file descriptors (10k+ if I run them
on a standard Debian machine).
There are a few suits that contribute to this in particular like
`org.apache.spark.ExecutorAllocationManagerSuite` which, like a few others,
appears to consume a lot of fds.

Wouldn't it make sense to open JIRAs about those and actively try to reduce
the resource consumption of these tests?
Seems to me these can cause a lot of unpredictable behavior (making the
reason for flaky tests hard to identify especially when there's timeouts
etc. involved) + they make it prohibitively expensive for many to test
locally imo.


"
shane knapp <sknapp@berkeley.edu>,"Wed, 15 Feb 2017 12:50:53 -0800",Re: File JIRAs for all flaky test failures,Armin Braun <me@obrown.io>,"it's not an open-file limit -- i have the jenkins workers set up w/a soft
file limit of 100k, and a hard limit of 200k.


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Wed, 15 Feb 2017 20:51:39 +0000",Re: File JIRAs for all flaky test failures,Armin Braun <me@obrown.io>,"I would recommend we just open JIRA's for unit tests based on module (core/ml/sql etc) and we fix this one module at a time, this at least keeps the number of unit tests needing fixing down to a manageable number.


________________________________
From: Armin Braun <me@obrown.io>
Sent: Wednesday, February 15, 2017 12:48 PM
To: Saikat Kanjilal
Cc: Kay Ousterhout; dev@spark.apache.org
Subject: Re: File JIRAs for all flaky test failures

I think one thing that is contributing to this a lot too is the general issue of the tests taking up a lot of file descriptors (10k+ if I run them on a standard Debian machine).
There are a few suits that contribute to this in particular like `org.apache.spark.ExecutorAllocationManagerSuite` which, like a few others, appears to consume a lot of fds.

Wouldn't it make sense to open JIRAs about those and actively try to reduce the resource consumption of these tests?
Seems to me these can cause a lot of unpredictable behavior (making the reason for flaky tests hard to identify especially when there's timeouts etc. involved) + they make it prohibitively expensive for many to test locally imo.


I was working on something to address this a while ago https://issues.apache.org/jira/browse/SPARK-9487 but the difficulty in testing locally made things a lot more complicated to fix for each of the unit tests, should we resurface this JIRA again, I would whole heartedly agree with the flakiness assessment of the unit tests.

[SPARK-9487] Use the same num. worker threads in Scala ...<https://issues.apache.org/jira/browse/SPARK-9487>
issues.apache.org<http://issues.apache.org>
In Python we use `local[4]` for unit tests, while in Scala/Java we use `local[2]` and `local` for some unit tests in SQL, MLLib, and other components. If the ...




________________________________
From: Kay Ousterhout <kayousterhout@gmail.com<mailto:kayousterhout@gmail.com>>
Sent: Wednesday, February 15, 2017 12:10 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: File JIRAs for all flaky test failures

Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more common than not now that the tests need to be re-run at least once on PRs before they pass.  This is both annoying and problematic because it makes it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins fails on a PR (for a reason unrelated to the PR).  Just provide a quick description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or ""Tests failed because 250m timeout expired"", a link to the failed build, and include the ""Tests"" component.  If there's already a JIRA for the issue, just comment with a link to the latest failure.  I know folks don't always have time to track down why a test failed, but this it at least helpful to someone else who, later on, is trying to diagnose when the issue started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways to make the tests less flaky!

-Kay

"
Sean Owen <sowen@cloudera.com>,"Wed, 15 Feb 2017 21:13:22 +0000",Does anyone here run the TheApacheSpark Youtube channel?,dev <dev@spark.apache.org>,"I just saw https://www.youtube.com/user/TheApacheSpark and wondered who
'owns' it? if it's a quasi-official channel, can we list it on
http://spark.apache.org/community.html but then, how does one add videos?

If it's the Spark Summit video account, as it seems to be at the moment, it
shouldn't be called ""Apache Spark"" right?
"
naresh gundla <nareshgundla@gmail.com>,"Wed, 15 Feb 2017 13:58:00 -0800","Need Help: getting java.lang.OutOfMemory Error : GC overhead limit
 exceeded (TransportChannelHandler)",dev@spark.apache.org,"Hi ,

I am running a spark application and getting out of memory errors in yarn
nodemanager logs and container get killed. Please find below for the errors
details.
Has anyone faced with this issue?

*Enabled spark dynamic allocation and yarn shuffle*

 2017-02-15 14:50:48,047 WARN io.netty.util.concurrent.DefaultPromise: An
exception was thrown by
org.apache.spark.network.server.TransportRequestHandler$2.operationComplete()
java.lang.OutOfMemoryError: GC overhead limit exceeded
2017-02-15 15:21:09,506 ERROR
org.apache.spark.network.server.TransportRequestHandler: Error opening
block StreamChunkId{streamId=1374579274227, chunkIndex=241} for request
from /10.154.16.83:50042
java.lang.IllegalStateException: Received out-of-order chunk index 241
(expected 114)
        at
        at
org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:121)
        at
org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:100)
        at
org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
        at
org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
        at
io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at
io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
        at
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at
io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
        at
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at
org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
        at
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at
io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)
        at
io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)
        at
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at java.lang.Thread.run(Thread.java:745)

2017-02-15 14:50:14,692 WARN
org.apache.spark.network.server.TransportChannelHandler: Exception in
connection from /10.154.16.74:58547
java.lang.OutOfMemoryError: GC overhead limit exceeded

Thanks
Naresh
"
Ryan Blue <rblue@netflix.com.INVALID>,"Wed, 15 Feb 2017 14:33:48 -0800","Re: Need Help: getting java.lang.OutOfMemory Error : GC overhead
 limit exceeded (TransportChannelHandler)",naresh gundla <nareshgundla@gmail.com>,"Naresh,

We've configured our Spark JVMs to shut down if there is an
OutOfMemoryError. Otherwise, the error will bring down a random thread an
cause trouble like the IllegalStateException you hit. It is best to let
Spark recover by replacing the executor or failing the job.

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
Josh Rosen <joshrosen@databricks.com>,"Thu, 16 Feb 2017 01:39:52 +0000",Re: File JIRAs for all flaky test failures,"Saikat Kanjilal <sxk1969@hotmail.com>, Armin Braun <me@obrown.io>","A useful tool for investigating test flakiness is my Jenkins Test Explorer
service, running at https://spark-tests.appspot.com/

This has some useful timeline views for debugging flaky builds. For
instance, at
https://spark-tests.appspot.com/jobs/spark-master-test-maven-hadoop-2.6 (may
be slow to load) you can see this chart: https://i.imgur.com/j8LV3pX.png.
Here, each column represents a test run and each row represents a test
which failed at least once over the displayed time period.

In that linked example screenshot you'll notice that a few columns have
grey squares indicating that tests were skipped but lack any red squares to
indicate test failures. This usually indicates that the build failed due to
a problem other than an individual test failure. For example, I clicked
into one of those builds and found that one test suite failed in test setup
because the previous suite had not properly cleaned up its SparkContext
(I'll file a JIRA for this).

You can click through the interface to drill down to reports on individual
builds, tests, suites, etc. As an example of an individual test's detail
page,
https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.rdd.LocalCheckpointSuite&test_name=missing+checkpoint+block+fails+with+informative+message
shows
the patterns of flakiness in a streaming checkpoint test.

Finally, there's an experimental ""interesting new test failures"" report
which tries to surface tests which have started failing very recently:
https://spark-tests.appspot.com/failed-tests/new. Specifically, entries in
this feed are test failures which a) occurred in the last week, b) were not
part of a build which had 20 or more failed tests, and c) were not observed
to fail in during the previous week (i.e. no failures from [2 weeks ago, 1
week ago)), and d) which represent the first time that the test failed this
week (i.e. a test case will appear at most once in the results list). I've
also exposed this as an RSS feed at
https://spark-tests.appspot.com/rss/failed-tests/new.



I would recommend we just open JIRA's for unit tests based on module
(core/ml/sql etc) and we fix this one module at a time, this at least keeps
the number of unit tests needing fixing down to a manageable number.


------------------------------
*From:* Armin Braun <me@obrown.io>
*Sent:* Wednesday, February 15, 2017 12:48 PM
*To:* Saikat Kanjilal
*Cc:* Kay Ousterhout; dev@spark.apache.org
*Subject:* Re: File JIRAs for all flaky test failures

I think one thing that is contributing to this a lot too is the general
issue of the tests taking up a lot of file descriptors (10k+ if I run them
on a standard Debian machine).
There are a few suits that contribute to this in particular like
`org.apache.spark.ExecutorAllocationManagerSuite` which, like a few others,
appears to consume a lot of fds.

Wouldn't it make sense to open JIRAs about those and actively try to reduce
the resource consumption of these tests?
Seems to me these can cause a lot of unpredictable behavior (making the
reason for flaky tests hard to identify especially when there's timeouts
etc. involved) + they make it prohibitively expensive for many to test
locally imo.


I was working on something to address this a while ago
https://issues.apache.org/jira/browse/SPARK-9487 but the difficulty in
testing locally made things a lot more complicated to fix for each of the
unit tests, should we resurface this JIRA again, I would whole heartedly
agree with the flakiness assessment of the unit tests.
[SPARK-9487] Use the same num. worker threads in Scala ...
<https://issues.apache.org/jira/browse/SPARK-9487>
issues.apache.org
In Python we use `local[4]` for unit tests, while in Scala/Java we use
`local[2]` and `local` for some unit tests in SQL, MLLib, and other
components. If the ...



------------------------------
*From:* Kay Ousterhout <kayousterhout@gmail.com>
*Sent:* Wednesday, February 15, 2017 12:10 PM
*To:* dev@spark.apache.org
*Subject:* File JIRAs for all flaky test failures

Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more
common than not now that the tests need to be re-run at least once on PRs
before they pass.  This is both annoying and problematic because it makes
it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins
fails on a PR (for a reason unrelated to the PR).  Just provide a quick
description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or
""Tests failed because 250m timeout expired"", a link to the failed build,
and include the ""Tests"" component.  If there's already a JIRA for the
issue, just comment with a link to the latest failure.  I know folks don't
always have time to track down why a test failed, but this it at least
helpful to someone else who, later on, is trying to diagnose when the issue
started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways
to make the tests less flaky!

-Kay
"
Saikat Kanjilal <sxk1969@hotmail.com>,"Thu, 16 Feb 2017 02:12:29 +0000",Re: File JIRAs for all flaky test failures,Josh Rosen <joshrosen@databricks.com>,"The issue was not with a lack of tooling, I used the url you are describing below to drill down to the exact test failure/stack trace, the problem was that my builds would work like a charm locally but fail with these errors on Jenkins, this was the whole challenge in fixing the unit tests, it was rare (if ever) where I would be able to replicate test failures locally.

Sent from my iPhone

On Feb 15, 2017, at 5:40 PM, Josh Rosen <joshrosen@databricks.com<mailto:joshrosen@databricks.com>> wrote:

A useful tool for investigating test flakiness is my Jenkins Test Explorer service, running at https://spark-tests.appspot.com/

This has some useful timeline views for debugging flaky builds. For instance, at https://spark-tests.appspot.com/jobs/spark-master-test-maven-hadoop-2.6 (may be slow to load) you can see this chart: https://i.imgur.com/j8LV3pX.png. Here, each column represents a test run and each row represents a test which failed at least once over the displayed time period.

In that linked example screenshot you'll notice that a few columns have grey squares indicating that tests were skipped but lack any red squares to indicate test failures. This usually indicates that the build failed due to a problem other than an individual test failure. For example, I clicked into one of those builds and found that one test suite failed in test setup because the previous suite had not properly cleaned up its SparkContext (I'll file a JIRA for this).

You can click through the interface to drill down to reports on individual builds, tests, suites, etc. As an example of an individual test's detail page, https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.rdd.LocalCheckpointSuite&test_name=missing+checkpoint+block+fails+with+informative+message shows the patterns of flakiness in a streaming checkpoint test.

Finally, there's an experimental ""interesting new test failures"" report which tries to surface tests which have started failing very recently: https://spark-tests.appspot.com/failed-tests/new. Specifically, entries in this feed are test failures which a) occurred in the last week, b) were not part of a build which had 20 or more failed tests, and c) were not observed to fail in during the previous week (i.e. no failures from [2 weeks ago, 1 week ago)), and d) which represent the first time that the test failed this week (i.e. a test case will appear at most once in the results list). I've also exposed this as an RSS feed at https://spark-tests.appspot.com/rss/failed-tests/new.


On Wed, Feb 15, 2017 at 12:51 PM Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>> wrote:

I would recommend we just open JIRA's for unit tests based on module (core/ml/sql etc) and we fix this one module at a time, this at least keeps the number of unit tests needing fixing down to a manageable number.


________________________________
From: Armin Braun <me@obrown.io<mailto:me@obrown.io>>
Sent: Wednesday, February 15, 2017 12:48 PM
To: Saikat Kanjilal
Cc: Kay Ousterhout; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: File JIRAs for all flaky test failures

I think one thing that is contributing to this a lot too is the general issue of the tests taking up a lot of file descriptors (10k+ if I run them on a standard Debian machine).
There are a few suits that contribute to this in particular like `org.apache.spark.ExecutorAllocationManagerSuite` which, like a few others, appears to consume a lot of fds.

Wouldn't it make sense to open JIRAs about those and actively try to reduce the resource consumption of these tests?
Seems to me these can cause a lot of unpredictable behavior (making the reason for flaky tests hard to identify especially when there's timeouts etc. involved) + they make it prohibitively expensive for many to test locally imo.

On Wed, Feb 15, 2017 at 9:24 PM, Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>> wrote:

I was working on something to address this a while ago https://issues.apache.org/jira/browse/SPARK-9487 but the difficulty in testing locally made things a lot more complicated to fix for each of the unit tests, should we resurface this JIRA again, I would whole heartedly agree with the flakiness assessment of the unit tests.

[SPARK-9487] Use the same num. worker threads in Scala ...<https://issues.apache.org/jira/browse/SPARK-9487>
issues.apache.org<http://issues.apache.org>
In Python we use `local[4]` for unit tests, while in Scala/Java we use `local[2]` and `local` for some unit tests in SQL, MLLib, and other components. If the ...




________________________________
From: Kay Ousterhout <kayousterhout@gmail.com<mailto:kayousterhout@gmail.com>>
Sent: Wednesday, February 15, 2017 12:10 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: File JIRAs for all flaky test failures

Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more common than not now that the tests need to be re-run at least once on PRs before they pass.  This is both annoying and problematic because it makes it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins fails on a PR (for a reason unrelated to the PR).  Just provide a quick description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or ""Tests failed because 250m timeout expired"", a link to the failed build, and include the ""Tests"" component.  If there's already a JIRA for the issue, just comment with a link to the latest failure.  I know folks don't always have time to track down why a test failed, but this it at least helpful to someone else who, later on, is trying to diagnose when the issue started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways to make the tests less flaky!

-Kay

"
Joseph Bradley <joseph@databricks.com>,"Wed, 15 Feb 2017 18:30:58 -0800",Re: welcoming Takuya Ueshin as a new Apache Spark committer,Takuya UESHIN <ueshin@happy-camper.st>,"Congrats and welcome!





-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Chetan Khatri <chetan.opensource@gmail.com>,"Thu, 16 Feb 2017 10:45:12 +0530",Spark Job Performance monitoring approaches,"Spark Dev List <dev@spark.apache.org>, user <user@spark.apache.org>","Hello All,

What would be the best approches to monitor Spark Performance, is there any
tools for Spark Job Performance monitoring ?

Thanks.
"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 15 Feb 2017 22:43:51 -0700 (MST)",Re: welcoming Takuya Ueshin as a new Apache Spark committer,dev@spark.apache.org,"
Congratulations!


Takuya UESHIN wrote















-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Georg Heiler <georg.kf.heiler@gmail.com>,"Thu, 16 Feb 2017 07:00:43 +0000",Re: Spark Job Performance monitoring approaches,"Chetan Khatri <chetan.opensource@gmail.com>, Spark Dev List <dev@spark.apache.org>, 
	user <user@spark.apache.org>","I know of the following tools
https://sites.google.com/site/sparkbigdebug/home
https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark


Chetan Khatri <chetan.opensource@gmail.com> schrieb am Do., 16. Feb. 2017
um 06:15 Uhr:

"
Chetan Khatri <chetan.opensource@gmail.com>,"Thu, 16 Feb 2017 12:54:19 +0530",Re: Spark Job Performance monitoring approaches,Georg Heiler <georg.kf.heiler@gmail.com>,"Thank you Georg


"
Reynold Xin <rxin@databricks.com>,"Thu, 16 Feb 2017 17:22:12 +0100",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Updated. Any feedback from other community members?



.
d.
. I
ed
ave
ave
e
time
e
d
et it
ne.
o
a few
rk?
 SPIP
uld
 that
fied in
 author
here
 is
l
 or
ne who
 anyway.
n
we do
d
ntly in.
ick and
s point.
s
explicit
e
t
h that
ese
of
 of the
 is
I'd
s
tailored
w
vXmnZ7SUi4qMljg/edit#>
Ps
g
is
s
e
e
,
y
r
e
a
y
I
t
d
s
g
e
da
t
o
,
k
e
f
f
r
e
g
"
Saikat Kanjilal <sxk1969@hotmail.com>,"Thu, 16 Feb 2017 16:26:04 +0000",Re: File JIRAs for all flaky test failures,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'd just like to follow up again on this thread, should we devote some energy to fixing unit tests based on module, there wasn't much interest in this last time but given the nature of this thread I'd be willing to deep dive into this again with some help.

________________________________
From: Saikat Kanjilal <sxk1969@hotmail.com>
Sent: Wednesday, February 15, 2017 6:12 PM
To: Josh Rosen
Cc: Armin Braun; Kay Ousterhout; dev@spark.apache.org
Subject: Re: File JIRAs for all flaky test failures

The issue was not with a lack of tooling, I used the url you are describing below to drill down to the exact test failure/stack trace, the problem was that my builds would work like a charm locally but fail with these errors on Jenkins, this was the whole challenge in fixing the unit tests, it was rare (if ever) where I would be able to replicate test failures locally.

Sent from my iPhone


A useful tool for investigating test flakiness is my Jenkins Test Explorer service, running at https://spark-tests.appspot.com/

This has some useful timeline views for debugging flaky builds. For instance, at https://spark-tests.appspot.com/jobs/spark-master-test-maven-hadoop-2.6 (may be slow to load) you can see this chart: https://i.imgur.com/j8LV3pX.png. Here, each column represents a test run and each row represents a test which failed at least once over the displayed time period.

In that linked example screenshot you'll notice that a few columns have grey squares indicating that tests were skipped but lack any red squares to indicate test failures. This usually indicates that the build failed due to a problem other than an individual test failure. For example, I clicked into one of those builds and found that one test suite failed in test setup because the previous suite had not properly cleaned up its SparkContext (I'll file a JIRA for this).

You can click through the interface to drill down to reports on individual builds, tests, suites, etc. As an example of an individual test's detail page, https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.rdd.LocalCheckpointSuite&test_name=missing+checkpoint+block+fails+with+informative+message shows the patterns of flakiness in a streaming checkpoint test.

Finally, there's an experimental ""interesting new test failures"" report which tries to surface tests which have started failing very recently: https://spark-tests.appspot.com/failed-tests/new. Specifically, entries in this feed are test failures which a) occurred in the last week, b) were not part of a build which had 20 or more failed tests, and c) were not observed to fail in during the previous week (i.e. no failures from [2 weeks ago, 1 week ago)), and d) which represent the first time that the test failed this week (i.e. a test case will appear at most once in the results list). I've also exposed this as an RSS feed at https://spark-tests.appspot.com/rss/failed-tests/new.



I would recommend we just open JIRA's for unit tests based on module (core/ml/sql etc) and we fix this one module at a time, this at least keeps the number of unit tests needing fixing down to a manageable number.


________________________________
From: Armin Braun <me@obrown.io<mailto:me@obrown.io>>
Sent: Wednesday, February 15, 2017 12:48 PM
To: Saikat Kanjilal
Cc: Kay Ousterhout; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: File JIRAs for all flaky test failures

I think one thing that is contributing to this a lot too is the general issue of the tests taking up a lot of file descriptors (10k+ if I run them on a standard Debian machine).
There are a few suits that contribute to this in particular like `org.apache.spark.ExecutorAllocationManagerSuite` which, like a few others, appears to consume a lot of fds.

Wouldn't it make sense to open JIRAs about those and actively try to reduce the resource consumption of these tests?
Seems to me these can cause a lot of unpredictable behavior (making the reason for flaky tests hard to identify especially when there's timeouts etc. involved) + they make it prohibitively expensive for many to test locally imo.


I was working on something to address this a while ago https://issues.apache.org/jira/browse/SPARK-9487 but the difficulty in testing locally made things a lot more complicated to fix for each of the unit tests, should we resurface this JIRA again, I would whole heartedly agree with the flakiness assessment of the unit tests.

[SPARK-9487] Use the same num. worker threads in Scala ...<https://issues.apache.org/jira/browse/SPARK-9487>
issues.apache.org<http://issues.apache.org>
In Python we use `local[4]` for unit tests, while in Scala/Java we use `local[2]` and `local` for some unit tests in SQL, MLLib, and other components. If the ...




________________________________
From: Kay Ousterhout <kayousterhout@gmail.com<mailto:kayousterhout@gmail.com>>
Sent: Wednesday, February 15, 2017 12:10 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: File JIRAs for all flaky test failures

Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more common than not now that the tests need to be re-run at least once on PRs before they pass.  This is both annoying and problematic because it makes it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins fails on a PR (for a reason unrelated to the PR).  Just provide a quick description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or ""Tests failed because 250m timeout expired"", a link to the failed build, and include the ""Tests"" component.  If there's already a JIRA for the issue, just comment with a link to the latest failure.  I know folks don't always have time to track down why a test failed, but this it at least helpful to someone else who, later on, is trying to diagnose when the issue started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways to make the tests less flaky!

-Kay

"
Reynold Xin <rxin@databricks.com>,"Thu, 16 Feb 2017 17:27:41 +0100",Re: File JIRAs for all flaky test failures,Saikat Kanjilal <sxk1969@hotmail.com>,"What exactly is the issue? I've been working on Spark dev for a long time
and very rarely do I actually run into an issue that only manifest on
Jenkins but not locally. I don't have some magic local setup either.

We should definitely cut down test flakiness.



"
Saikat Kanjilal <sxk1969@hotmail.com>,"Thu, 16 Feb 2017 16:32:41 +0000",Re: Spark Job Performance monitoring approaches,"Chetan Khatri <chetan.opensource@gmail.com>, Georg Heiler
	<georg.kf.heiler@gmail.com>","There's also this:

https://github.com/databricks/spark-perf

[https://avatars2.githubusercontent.com/u/4998052?v=3&s=400]<https://github.com/databricks/spark-perf>

GitHub - databricks/spark-perf: Performance tests for Spark<https://github.com/databricks/spark-perf>
github.com
Sweeps sets of parameters to test against multiple Spark and test configurations. Automatically downloads and builds Spark: Maintains a cache of successful builds to ...


Not sure if it exactly fits what you are looking for but hoping it helps


________________________________
From: Chetan Khatri <chetan.opensource@gmail.com>
Sent: Wednesday, February 15, 2017 11:24 PM
To: Georg Heiler
Cc: Spark Dev List; user
Subject: Re: Spark Job Performance monitoring approaches

Thank you Georg

I know of the following tools
https://sites.google.com/site/sparkbigdebug/home https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hon/sparklint

Chetan Khatri <chetan.opensource@gmail.com<mailto:chetan.opensource@gmail.com>> schrieb am Do., 16. Feb. 2017 um 06:15 Uhr:
Hello All,

What would be the best approches to monitor Spark Performance, is there any tools for Spark Job Performance monitoring ?

Thanks.

"
Saikat Kanjilal <sxk1969@hotmail.com>,"Thu, 16 Feb 2017 16:34:08 +0000",Re: File JIRAs for all flaky test failures,Reynold Xin <rxin@databricks.com>,"Reynold,

Its not one issue , I encountered multiple issues (stack traces/exceptions etc) where the issue only occured on Jenkins but not on my local environments, I would have to dig up all those old unit tests to list them all []  and I'm not willing to do that unless we deem this to be an actual problem that we want to spend time and energy to fix.


Thanks


________________________________
From: Reynold Xin <rxin@databricks.com>
Sent: Thursday, February 16, 2017 8:27 AM
To: Saikat Kanjilal
Cc: dev@spark.apache.org
Subject: Re: File JIRAs for all flaky test failures

What exactly is the issue? I've been working on Spark dev for a long time and very rarely do I actually run into an issue that only manifest on Jenkins but not locally. I don't have some magic local setup either.

We should definitely cut down test flakiness.


On Thu, Feb 16, 2017 at 5:26 PM, Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>> wrote:

I'd just like to follow up again on this thread, should we devote some energy to fixing unit tests based on module, there wasn't much interest in this last time but given the nature of this thread I'd be willing to deep dive into this again with some help.

________________________________
From: Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>>
Sent: Wednesday, February 15, 2017 6:12 PM
To: Josh Rosen
Cc: Armin Braun; Kay Ousterhout; dev@spark.apache.org<mailto:dev@spark.apache.org>

Subject: Re: File JIRAs for all flaky test failures

The issue was not with a lack of tooling, I used the url you are describing below to drill down to the exact test failure/stack trace, the problem was that my builds would work like a charm locally but fail with these errors on Jenkins, this was the whole challenge in fixing the unit tests, it was rare (if ever) where I would be able to replicate test failures locally.

Sent from my iPhone

On Feb 15, 2017, at 5:40 PM, Josh Rosen <joshrosen@databricks.com<mailto:joshrosen@databricks.com>> wrote:

A useful tool for investigating test flakiness is my Jenkins Test Explorer service, running at https://spark-tests.appspot.com/

This has some useful timeline views for debugging flaky builds. For instance, at https://spark-tests.appspot.com/jobs/spark-master-test-maven-hadoop-2.6 (may be slow to load) you can see this chart: https://i.imgur.com/j8LV3pX.png. Here, each column represents a test run and each row represents a test which failed at least once over the displayed time period.

In that linked example screenshot you'll notice that a few columns have grey squares indicating that tests were skipped but lack any red squares to indicate test failures. This usually indicates that the build failed due to a problem other than an individual test failure. For example, I clicked into one of those builds and found that one test suite failed in test setup because the previous suite had not properly cleaned up its SparkContext (I'll file a JIRA for this).

You can click through the interface to drill down to reports on individual builds, tests, suites, etc. As an example of an individual test's detail page, https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.rdd.LocalCheckpointSuite&test_name=missing+checkpoint+block+fails+with+informative+message shows the patterns of flakiness in a streaming checkpoint test.

Finally, there's an experimental ""interesting new test failures"" report which tries to surface tests which have started failing very recently: https://spark-tests.appspot.com/failed-tests/new. Specifically, entries in this feed are test failures which a) occurred in the last week, b) were not part of a build which had 20 or more failed tests, and c) were not observed to fail in during the previous week (i.e. no failures from [2 weeks ago, 1 week ago)), and d) which represent the first time that the test failed this week (i.e. a test case will appear at most once in the results list). I've also exposed this as an RSS feed at https://spark-tests.appspot.com/rss/failed-tests/new.


On Wed, Feb 15, 2017 at 12:51 PM Saikat Kanjilal <sxk1969@hotmaimmend we just open JIRA's for unit tests based on module (core/ml/sql etc) and we fix this one module at a time, this at least keeps the number of unit tests needing fixing down to a manageable number.


________________________________
From: Armin Braun <me@obrown.io<mailto:me@obrown.io>>
Sent: Wednesday, February 15, 2017 12:48 PM
To: Saikat Kanjilal
Cc: Kay Ousterhout; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: File JIRAs for all flaky test failures

I think one thing that is contributing to this a lot too is the general issue of the tests taking up a lot of file descriptors (10k+ if I run them on a standard Debian machine).
There are a few suits that contribute to this in particular like `org.apache.spark.ExecutorAllocationManagerSuite` which, like a few others, appears to consume a lot of fds.

Wouldn't it make sense to open JIRAs about those and actively try to reduce the resource consumption of these tests?
Seems to me these can cause a lot of unpredictable behavior (making the reason for flaky tests hard to identify especially when there's timeouts etc. involved) + they make it prohibitively expensive for many to test locally imo.

On Wed, Feb 15, 2017 at 9:24 PM, Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>> wrote:

I was working on something to address this a while ago https://issues.apache.org/jira/browse/SPARK-9487 but the difficulty in testing locally made things a lot more complicated to fix for each of the unit tests, should we resurface this JIRA again, I would whole heartedly agree with the flakiness assessment of the unit tests.

[SPARK-9487] Use the same num. worker threads in Scala ...<https://issues.apache.org/jira/browse/SPARK-9487>
issues.apache.org<http://issues.apache.org>
In Python we use `local[4]` for unit tests, while in Scala/Java we use `local[2]` and `local` for some unit tests in SQL, MLLib, and other components. If the ...




________________________________
From: Kay Ousterhout <kayousterhout@gmail.com<mailto:kayousterhout@gmail.com>>
Sent: Wednesday, February 15, 2017 12:10 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: File JIRAs for all flaky test failures

Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more common than not now that the tests need to be re-run at least once on PRs before they pass.  This is both annoying and problematic because it makes it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins fails on a PR (for a reason unrelated to the PR).  Just provide a quick description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or ""Tests failed because 250m timeout expired"", a link to the failed build, and include the ""Tests"" component.  If there's already a JIRA for the issue, just comment with a link to the latest failure.  I know folks don't always have time to track down why a test failed, but this it at least helpful to someone else who, later on, is trying to diagnose when the issue started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways to make the tests less flaky!

-Kay


"
Ryan Blue <rblue@netflix.com.INVALID>,"Thu, 16 Feb 2017 08:42:35 -0800",Re: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"The current proposal seems process-heavy to me. That's not necessarily bad,
but there are a couple areas I haven't seen discussed.

Why is there a shepherd? If the person proposing a change has a good idea,
I don't see why one is either a good idea or necessary. The result of this
requirement is that each SPIP must attract the attention of a PMC member,
and that PMC member has then taken on extra responsibility. Why can't the
SPIP author simply call a vote when an idea has been sufficiently
discussed? I think *this* proposal would have moved faster if Cody had felt
empowered to bring it to a vote. More steps out of the author's control
will cause fewer ideas to move forward, regardless of quality, so we should
make sure this is balanced by a real benefit.

Why are only PMC members allowed a binding vote? I don't have a strong
inclination one way or another, but until recently this was an open
question. I'd like to hear the argument for restricting voting to PMC
members, or I think we should change it to allow all commiters. If this
decision is left to default, let's be more inclusive.

I would be fine with the proposal overall if there are good reasons behind
these choices.

rb


t.
:
s. I
.
ted
have
have
ernal
 time
we
e
sals
get it
ine.
are a
n
ark?
. SPIP
ould
e
g that
ified in
t author
There
P is
al
P or
one who
d anyway.
 we do
is
h
pick and
is point.
 an
 enough
 If these
 of
f of the
d is
 I'd
gs
 tailored
nvXmnZ7SUi4qMljg/edit#>
g
r
n
-
e
f
r
n
r
o
da
n
g
s
id
r
d
I
da
l
e
s
t
n
.


-- 
Ryan Blue
Software Engineer
Netflix
"
Sean Owen <sowen@cloudera.com>,"Thu, 16 Feb 2017 16:43:52 +0000",Re: Spark Improvement Proposals,"Reynold Xin <rxin@databricks.com>, Cody Koeninger <cody@koeninger.org>","The text seems fine to me. Really, this is not describing a fundamentally
new process, which is good. We've always had JIRAs, we've always been able
to call a VOTE for a big question. This just writes down a sensible set of
guidelines for putting those two together when a major change is proposed.
I look forward to turning some big JIRAs into a request for a SPIP.

My only hesitation is that this seems to be perceived by some as a new or
different thing, that is supposed to solve some problems that aren't
otherwise solvable. I see mentioned problems like: clear process for
managing work, public communication, more committers, some sort of binding
outcome and deadline.

If SPIP is supposed to be a way to make people design in public and a way
to force attention to a particular change, then, this doesn't do that by
itself. Therefore I don't want to let a detailed discussion of SPIP detract
from the discussion about doing what SPIP implies. It's just a process
document.

Still, a fine step IMHO.


"
Sean Owen <sowen@cloudera.com>,"Thu, 16 Feb 2017 16:45:53 +0000",Re: File JIRAs for all flaky test failures,"Saikat Kanjilal <sxk1969@hotmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I'm not sure what you're specifically suggesting. Of course flaky tests are
bad and they should be fixed, and people do. Yes, some are pretty hard to
fix because they are rarely reproducible if at all. If you want to fix,
fix; there's nothing more to it.

I don't perceive flaky tests to be a significant problem. It has gone from
bad to occasional over the past year in my anecdotal experience.


"
Cody Koeninger <cody@koeninger.org>,"Thu, 16 Feb 2017 11:17:06 -0600",Re: Spark Improvement Proposals,Sean Owen <sowen@cloudera.com>,"Reynold, thanks, LGTM.

Sean, great concerns.  I agree that behavior is largely cultural and
writing down a process won't necessarily solve any problems one way or
the other.  But one outwardly visible change I'm hoping for out of
this a way for people who have a stake in Spark, but can't follow
jiras closely, to go to the Spark website, see the list of proposed
major changes, contribute discussion on issues that are relevant to
their needs, and see a clear direction once a vote has passed.  We
don't have that now.

Ryan, realistically speaking any PMC member can and will stop any
changes they don't like anyway, so might as well be up front about the
reality of the situation.


---------------------------------------------------------------------


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Thu, 16 Feb 2017 17:19:39 +0000",Re: File JIRAs for all flaky test failures,"Sean Owen <sowen@cloudera.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I am specifically suggesting documenting a list of the the flaky tests and fixing them, that's all.  To organize the effort I suggested tackling this by module.  Your second sentence is what I was trying to gauge from the community before putting anymore effort into this.


________________________________
From: Sean Owen <sowen@cloudera.com>
Sent: Thursday, February 16, 2017 8:45 AM
To: Saikat Kanjilal; dev@spark.apache.org
Subject: Re: File JIRAs for all flaky test failures

I'm not sure what you're specifically suggesting. Of course flaky tests are bad and they should be fixed, and people do. Yes, some are pretty hard to fix because they are rarely reproducible if at all. If you want to fix, fix; there's nothing more to it.

I don't perceive flaky tests to be a significant problem. It has gone from bad to occasional over the past year in my anecdotal experience.


I'd just like to follow up again on this thread, should we devote some energy to fixing unit tests based on module, there wasn't much interest in this last time but given the nature of this thread I'd be willing to deep dive into this again with some help.

________________________________
From: Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>>
Sent: Wednesday, February 15, 2017 6:12 PM
To: Josh Rosen
Cc: Armin Braun; Kay Ousterhout; dev@spark.apache.org<mailto:dev@spark.apache.org>

Subject: Re: File JIRAs for all flaky test failures
The issue was not with a lack of tooling, I used the url you are describing below to drill down to the exact test failure/stack trace, the problem was that my builds would work like a charm locally but fail with these errors on Jenkins, this was the whole challenge in fixing the unit tests, it was rare (if ever) where I would be able to replicate test failures locally.

Sent from my iPhone


A useful tool for investigating test flakiness is my Jenkins Test Explorer service, running at https://spark-tests.appspot.com/

This has some useful timeline views for debugging flaky builds. For instance, at https://spark-tests.appspot.com/jobs/spark-master-test-maven-hadoop-2.6 (may be slow to load) you can see this chart: https://i.imgur.com/j8LV3pX.png. Here, each column represents a test run and each row represents a test which failed at least once over the displayed time period.

In that linked example screenshot you'll notice that a few columns have grey squares indicating that tests were skipped but lack any red squares to indicate test failures. This usually indicates that the build failed due to a problem other than an individual test failure. For example, I clicked into one of those builds and found that one test suite failed in test setup because the previous suite had not properly cleaned up its SparkContext (I'll file a JIRA for this).

You can click through the interface to drill down to reports on individual builds, tests, suites, etc. As an example of an individual test's detail page, https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.rdd.LocalCheckpointSuite&test_name=missing+checkpoint+block+fails+with+informative+message shows the patterns of flakiness in a streaming checkpoint test.

Finally, there's an experimental ""interesting new test failures"" report which tries to surface tests which have started failing very recently: https://spark-tests.appspot.com/failed-tests/new. Specifically, entries in this feed are test failures which a) occurred in the last week, b) were not part of a build which had 20 or more failed tests, and c) were not observed to fail in during the previous week (i.e. no failures from [2 weeks ago, 1 week ago)), and d) which represent the first time that the test failed this week (i.e. a test case will appear at most once in the results list). I've also exposed this as an RSS feed at https://spark-tests.appspot.com/rss/failed-tests/new.



I would recommend we just open JIRA's for unit tests based on module (core/ml/sql etc) and we fix this one module at a time, this at least keeps the number of unit tests needing fixing down to a manageable number.


________________________________
From: Armin Braun <me@obrown.io<mailto:me@obrown.io>>
Sent: Wednesday, February 15, 2017 12:48 PM
To: Saikat Kanjilal
Cc: Kay Ousterhout; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: File JIRAs for all flaky test failures

I think one thing that is contributing to this a lot too is the general issue of the tests taking up a lot of file descriptors (10k+ if I run them on a standard Debian machine).
There are a few suits that contribute to this in particular like `org.apache.spark.ExecutorAllocationManagerSuite` which, like a few others, appears to consume a lot of fds.

Wouldn't it make sense to open JIRAs about those and actively try to reduce the resource consumption of these tests?
Seems to me these can cause a lot of unpredictable behavior (making the reason for flaky tests hard to identify especially when there's timeouts etc. involved) + they make it prohibitively expensive for many to test locally imo.


I was working on something to address this a while ago https://issues.apache.org/jira/browse/SPARK-9487 but the difficulty in testing locally made things a lot more complicated to fix for each of the unit tests, should we resurface this JIRA again, I would whole heartedly agree with the flakiness assessment of the unit tests.

[SPARK-9487] Use the same num. worker threads in Scala ...<https://issues.apache.org/jira/browse/SPARK-9487>
issues.apache.org<http://issues.apache.org>
In Python we use `local[4]` for unit tests, while in Scala/Java we use `local[2]` and `local` for some unit tests in SQL, MLLib, and other components. If the ...




________________________________
From: Kay Ousterhout <kayousterhout@gmail.com<mailto:kayousterhout@gmail.com>>
Sent: Wednesday, February 15, 2017 12:10 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: File JIRAs for all flaky test failures

Hi all,

I've noticed the Spark tests getting increasingly flaky -- it seems more common than not now that the tests need to be re-run at least once on PRs before they pass.  This is both annoying and problematic because it makes it harder to tell when a PR is introducing new flakiness.

To try to clean this up, I'd propose filing a JIRA *every time* Jenkins fails on a PR (for a reason unrelated to the PR).  Just provide a quick description of the failure -- e.g., ""Flaky test: DagSchedulerSuite"" or ""Tests failed because 250m timeout expired"", a link to the failed build, and include the ""Tests"" component.  If there's already a JIRA for the issue, just comment with a link to the latest failure.  I know folks don't always have time to track down why a test failed, but this it at least helpful to someone else who, later on, is trying to diagnose when the issue started to find the problematic code / test.

If this seems like too high overhead, feel free to suggest alternative ways to make the tests less flaky!

-Kay

"
Reynold Xin <rxin@databricks.com>,"Thu, 16 Feb 2017 18:22:00 +0100",Re: File JIRAs for all flaky test failures,Saikat Kanjilal <sxk1969@hotmail.com>,"Josh's tool should give enough signal there already. I don't think we need
some manual process to document them. If you want to work on those that'd
be great. I bet you will get a lot of love because all developers hate
flaky tests.



"
shane knapp <sknapp@berkeley.edu>,"Thu, 16 Feb 2017 10:22:36 -0800",[build system] jenkins restart in ~1 hour,"dev <dev@spark.apache.org>, rise-research@eecs.berkeley.edu, 
	Frank A Nothaft <fnothaft@berkeley.edu>","we don't have many builds running right now, and i need to restart the
daemon quickly to enable a new plugin.

i'll wait until the pull request builder jobs are finished and then
(gently) kick jenkins.

updates as they come,

shane (who's always nervous about touching this house of cards)

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 16 Feb 2017 11:40:07 -0800",Re: [build system] jenkins restart in ~1 hour,"dev <dev@spark.apache.org>, rise-research@eecs.berkeley.edu, 
	Frank A Nothaft <fnothaft@berkeley.edu>","and we're back!  :)


---------------------------------------------------------------------


"
Tim Hunter <timhunter@databricks.com>,"Thu, 16 Feb 2017 12:23:15 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"The doc looks good to me.

Ryan, the role of the shepherd is to make sure that someone
knowledgeable with Spark processes is involved: this person can advise
on technical and procedural considerations for people outside the
community. Also, if no one is willing to be a shepherd, the proposed
idea is probably not going to receive much traction in the first
place.

Tim


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 16 Feb 2017 12:30:06 -0800",Re: Structured Streaming Spark Summit Demo - Databricks people,Sam Elamin <hussam.elamin@gmail.com>,"Thanks for your interest in Apache Spark Structured Streaming!

There is nothing secret in that demo, though I did make some configuration
changes in order to get the timing right (gotta have some dramatic effect
:) ).  Also I think the visualizations based on metrics output by the
StreamingQueryListener
<https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/streaming/StreamingQueryListener.html>
are
still being rolled out, but should be available everywhere soon.

First, I set two options to make sure that files were read one at a time,
thus allowing us to see incremental results.

spark.readStream
  .option(""maxFilesPerTrigger"", ""1"")
  .option(""latestFirst"", ""true"")
...

There is more detail on how these options work in this post
<https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html>
.

Regarding continually updating result of a streaming query using display(df)for
streaming DataFrames (i.e. one created with spark.readStream), that has
worked in Databrick's since Spark 2.1.  The longer form example we
published requires you to rerun the count to see it change at the end of
the notebook because that is not a streaming query. Instead it is a batch
query over data that has been written out by another stream.  I'd like to
add the ability to run a streaming query from data that has been written
out by the FileSink (tracked here SPARK-19633
<https://issues.apache.org/jira/browse/SPARK-19633>).

In the demo, I started two different streaming queries:
 - one that reads from json / kafka => writes to parquet
 - one that reads from json / kafka => writes to memory sink
<http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks>
/ pushes latest answer to the js running in a browser using the
StreamingQueryListener
<https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/streaming/StreamingQueryListener.html>.
This is packaged up nicely in display(), but there is nothing stopping you
from building something similar with vanilla Apache Spark.

Michael



"
Sam Elamin <hussam.elamin@gmail.com>,"Thu, 16 Feb 2017 20:43:43 +0000",Re: Spark Improvement Proposals,Tim Hunter <timhunter@databricks.com>,"Hi Folks

I thought id chime in as someone new to the process so feel free to
disregard it if it doesn't make sense.

I definitely agree that we need a new forum to identify or discuss changes
as JIRA isnt exactly the best place to do that, its a Bug tracker first and
foremost.

For example I was cycling in through the JIRAs to see if theres anything I
can contribute to and there isnt one overall story or goal. There are bugs
or wish lists but I am talking overall high level goals or wishes.

If the point of the SPIP is to focus the discussions on the problems but
also encourage non PMC to be more active in the project the requirement
that every single idea must have a shepherd might do more harm than good.

As Cody mentioned, any PMC member can veto a change, thats fine for
stopping potential detrimental changes, but if this process is hoping to
open the flood gates to allow literally anyone to come up with an idea or
at least facilitate conversations on positive changes then I foresee that
the PMCs will be far too stretched to govern it, since the number of ideas
or discussions will be a number of magnitudes greater than the number of
PMCs who can manage it. We will end up needing to get ""project managers""
and ""scrum masters"" and nobody wants that!

Hence it would deter anyone from raising ideas in the first place unless
they are willing to find a shepherd for it. It reminds me of the need to
raise a BOC (Business Operation Case) when I worked in big corporations.
Overall it reduces morale since by default this isn't necessarily the
average engineers strong point. There is also the issue that the PMCs might
just be too busy to shepherd all the ideas regardless of merit so potential
great additions might die off purely because there just arent enough PMCs
around to take time out of their already busy schedules and give all the
SPIPs the attention they deserve

My point is this, allow anyone to raise ideas, facilitate discussions
proposals in a safe environment

At the end of the day  PMC members will have the guide/veto anything since
they have the experience.

I hope I was able to articulate what I meant, I really am loving working on
Spark and I think the future looks very promising for it and very much look
forward to being involved in the evolution of it

Kind Regards
Sam





"
Sam Elamin <hussam.elamin@gmail.com>,"Thu, 16 Feb 2017 20:47:26 +0000",Re: Structured Streaming Spark Summit Demo - Databricks people,Michael Armbrust <michael@databricks.com>,"Thanks Micheal it really was a great demo

I figured I needed to add a trigger to display the results. But Buraz from
Databricks mentioned here
<https://forums.databricks.com/questions/10925/structured-streaming-in-real-time.html#comment-10929>
that the display on this functionality wont be available till potentially
the next release of databricks 2.1-db3

Ill take your points into account and try and duplicate it

Apologies if this isn't the forum for the question, im happy to take the
question offline but I genuinely believe the mailing list users might find
it very interesting

Happy to take the discussion offline though :)




"
Tim Hunter <timhunter@databricks.com>,"Thu, 16 Feb 2017 13:00:04 -0800",Design document - MLlib's statistical package for DataFrames,dev@spark.apache.org,"Hello all,

I have been looking at some of the missing items for complete feature
parity between spark.ml and spark.mllib. Here is a proposal for
porting mllib.stats, the descriptive statistics package:

https://docs.google.com/document/d/1ELVpGV3EBjc2KQPLN9_9_Ge9gWchPZ6SGtDW5tTm_50/edit?usp=sharing

The umbrella ticket for this task is:
https://issues.apache.org/jira/browse/SPARK-4591

Please comment on the document. Also, if you want to work on one of
the algorithms, the design doc and the umbrella ticket have subtasks
that you can assign yourself to.

The cutoff deadline for Spark 2.2 is rapidly approaching, and it would
be great if we could claim parity for this release!

Cheers

Tim

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Thu, 16 Feb 2017 13:26:36 -0800",Re: Spark Improvement Proposals,Tim Hunter <timhunter@databricks.com>,"people outside the community

The sentiment is good, but this doesn't justify requiring a shepherd for a
proposal. There are plenty of people that wouldn't need this, would get
feedback during discussion, or would ask a committer or PMC member if it
weren't a formal requirement.

going to receive much traction in the first place.

This also doesn't sound like a reason for needing a shepherd. Saying that a
shepherd probably won't hurt the process doesn't give me an idea of why a
shepherd should be required in the first place.

What was the motivation for adding a shepherd originally? It may not be bad
and it could be helpful, but neither of those makes me think that they
should be required or else the proposal fails.

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
bradc <brad.carlile@oracle.com>,"Thu, 16 Feb 2017 15:21:31 -0700 (MST)",Re: Design document - MLlib's statistical package for DataFrames,dev@spark.apache.org,"Hi,

While it is also missing in spark.mllib, I'd suggest adding cardinality as
part of the Simple descriptive statistics for both spark.ml and spark.mlib? 
This is useful even for data in double precision FP to understand the
""uniqueness"" of the feature data.

Cheers,
Brad




--

---------------------------------------------------------------------


"
Tim Hunter <timhunter@databricks.com>,"Fri, 17 Feb 2017 10:48:59 -0800",Re: Design document - MLlib's statistical package for DataFrames,bradc <brad.carlile@oracle.com>,"Hi Brad,

this task is focusing on moving the existing algorithms, so that we
are held up by parity issues.

Do you have some paper suggestions for cardinality? I do not think
there is a feature request on JIRA either.

Tim


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 18 Feb 2017 03:15:33 +0000",Will .count() always trigger an evaluation of each row?,Spark dev list <dev@spark.apache.org>,"Especially during development, people often use .count() or
.persist().count() to force evaluation of all rows  exposing any problems,
e.g. due to bad data  and to load data into cache to speed up subsequent
operations.

But as the optimizer gets smarter, Im guessing it will eventually learn
that it doesnt have to do all that work to give the correct count. (This
blog post
<https://databricks.com/blog/2017/02/16/processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast.html>
suggests that something like this is already happening.) This will change
Sparks practical behavior while technically preserving semantics.

What will people need to do then to force evaluation or caching?

Nick

"
vaquar khan <vaquar.khan@gmail.com>,"Fri, 17 Feb 2017 22:41:48 -0600",Re: Spark Improvement Proposals,Ryan Blue <rblue@netflix.com.invalid>,"I like document and happy to see SPIP draft version however i feel shepherd
role is again hurdle in process improvement ,It's like everything depends
only on shepherd .

Also want to add point that SPIP  should be time bound with define SLA else
will defeats purpose.


Regards,
Vaquar khan





-- 
Regards,
Vaquar Khan
+1 -224-436-0783

IT Architect / Lead Consultant
Greater Chicago
"
Sean Owen <sowen@cloudera.com>,"Sat, 18 Feb 2017 10:16:23 +0000",Re: Will .count() always trigger an evaluation of each row?,"Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>","I think the right answer is ""don't do that"" but if you really had to you
could trigger a Dataset operation that does nothing per partition. I
presume that would be more reliable because the whole partition has to be
computed to make it available in practice. Or, go so far as to loop over
every element.

m>

peed up
y learn
t. (This
nd-single-machine-can-nested-loop-joins-fast.html>
.
"
"""Pritish Nawlakhe"" <pritish@nirvana-international.com>","Sat, 18 Feb 2017 14:10:38 -0500",RE: Design document - MLlib's statistical package for DataFrames,"""'Tim Hunter'"" <timhunter@databricks.com>,
	""'bradc'"" <brad.carlile@oracle.com>","Hi 

Would anyone know how to unsubscribe to this list?



Thank you!!

Regards
Pritish
Nirvana International Inc.

Big Data, Hadoop, Oracle EBS and IT Solutions
VA - SWaM, MD - MBE Certified Company
pritish@nirvana-international.com 
http://www.nirvana-international.com 
Twitter: @nirvanainternat 


Hi Brad,

this task is focusing on moving the existing algorithms, so that we are held up by parity issues.

Do you have some paper suggestions for cardinality? I do not think there is a feature request on JIRA either.

Tim

spark.ml and spark.mlib?
Nabble.com.

---------------------------------------------------------------------



---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Sat, 18 Feb 2017 19:10:19 +0000",Re: Design document - MLlib's statistical package for DataFrames,"Pritish Nawlakhe <pritish@nirvana-international.com>, 
	Tim Hunter <timhunter@databricks.com>, bradc <brad.carlile@oracle.com>","It's at the bottom of every message (although some mail clients hide it for
some reason), send an email to dev-unsubscribe@spark.apache.org


Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 18 Feb 2017 23:29:31 -0800",Re: Will .count() always trigger an evaluation of each row?,Sean Owen <sowen@cloudera.com>,"always evaluates everything, but on DataFrame/Dataset, it turns into the equivalent of ""select count(*) from ..."" in SQL, which can be done other hand though, caching a DataFrame / Dataset does require everything to be cached.

Matei

you could trigger a Dataset operation that does nothing per partition. I presume that would be more reliable because the whole partition has to be computed to make it available in practice. Or, go so far as to loop over every element.
.persist().count() to force evaluation of all rows  exposing any problems, e.g. due to bad data  and to load data into cache to speed up subsequent operations.
eventually learn that it doesnt have to do all that work to give the correct count. (This blog post <https://databricks.com/blog/2017/02/16/processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast.html> suggests that something like this is already happening.) This will change Sparks practical behavior while technically preserving semantics.

"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sun, 19 Feb 2017 02:13:05 -0700 (MST)",RE: Will .count() always trigger an evaluation of each row?,dev@spark.apache.org,"Actually, when I did a simple test on parquet (spark.read.parquet(somefile).cache().count()) the UI showed me that the entire file is cached. Is this just a fluke?

In any case I believe the question is still valid, how to make sure a dataframe is cached.
Consider for example a case where we read from a remote host (which is costly) and we want to make sure the original read is done at a specific time (when the network is less crowded).
I for one used .count() till now but if this is not guaranteed to cache, then how would I do that? Of course I could always save the dataframe to disk but that would cost a lot more in performance than I would like

As for doing a map partitions for the dataset, wouldnt that cause the row to be converted to the case class for each row? That could also be heavy.
Maybe cache should have a lazy parameter which would be false by default but we could call .cache(true) to make it materialize (similar to what we have with checkpoint).
Assaf.

From: Matei Zaharia [via Apache Spark Developers List] [mailto:ml-node+s1001551n21024h33@n3.nabble.com]
Sent: Sunday, February 19, 2017 9:30 AM
To: Mendelson, Assaf
Subject: Re: Will .count() always trigger an evaluation of each row?

 evaluates everything, but on DataFrame/Dataset, it turns into the equivalent of ""select count(*) from ..."" in SQL, which can be done without scanningaching a DataFrame / Dataset does require everything to be cached.

Matei


I think the right answer is ""don't do that"" but if you really had to you could trigger a Dataset operation that does nothing per partition. I presume that would be more reliable because the whole partition has to be computed to make it available in practice. Or, go so far as to loop over every element.


Especially during development, people often use .count() or .persist().count() to force evaluation of all rows  exposing any problems, e.g. due to bad data  and to load data into cache to speed up subsequent operations.

But as the optimizer gets smarter, Im guessing it will eventually learn that it doesnt have to do all that work to give the correct count. (This blog post<https://databricks.com/blog/2017/02/16/processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast.html> suggests that something like this is already happening.) This will change Sparks practical behavior while technically preserving semantics.

What will people need to do then to force evaluation or caching?

Nick



________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Will-count-always-trigger-an-evaluation-of-each-row-tp21018p21024.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Will-count-always-trigger-an-evaluation-of-each-row-tp21018p21025.html
om."
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Sun, 19 Feb 2017 11:13:07 +0100",Re: Will .count() always trigger an evaluation of each row?,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","I think your example relates to scheduling, e.g. it makes sense to use oozie or similar to fetch the data at specific point in times.

I am also not a big fan of caching everything. In a Multi-user cluster with a lot of Applications you waste a lot of resources making everybody less efficient. 


somefile).cache().count()) the UI showed me that the entire file is cached. Is this just a fluke?
frame is cached.
tly) and we want to make sure the original read is done at a specific time (when the network is less crowded).
hen how would I do that? Of course I could always save the dataframe to disk but that would cost a lot more in performance than I would like
 the row to be converted to the case class for each row? That could also be heavy.
ut we could call .cache(true) to make it materialize (similar to what we have with checkpoint).
dden email]] 
s evaluates everything, but on DataFrame/Dataset, it turns into the equivalent of ""select count(*) from ..."" in SQL, which can be done without scanning ting a DataFrame / Dataset does require everything to be cached.
ould trigger a Dataset operation that does nothing per partition. I presume that would be more reliable because the whole partition has to be computed to make it available in practice. Or, go so far as to loop over every element.
nt() to force evaluation of all rows  exposing any problems, e.g. due to bad data  and to load data into cache to speed up subsequent operations.
 learn that it doesnt have to do all that work to give the correct count. (This blog post suggests that something like this is already happening.) This will change Sparks practical behavior while technically preserving semantics.
elow:
s-trigger-an-evaluation-of-each-row-tp21018p21024.html
il] 
on of each row?
com.
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sun, 19 Feb 2017 04:14:24 -0700 (MST)",RE: Will .count() always trigger an evaluation of each row?,dev@spark.apache.org,"I am not saying you should cache everything, just that it is a valid use case.


From: J枚rn Franke [via Apache Spark Developers List] [mailto:ml-node+s1001551n21026h95@n3.nabble.com]
Sent: Sunday, February 19, 2017 12:13 PM
To: Mendelson, Assaf
Subject: Re: Will .count() always trigger an evaluation of each row?

I think your example relates to scheduling, e.g. it makes sense to use oozie or similar to fetch the data at specific point in times.

I am also not a big fan of caching everything. In a Multi-user cluster with a lot of Applications you waste a lot of resources making everybody less efficient.

Actually, when I did a simple test on parquet (spark.read.parquet(somefile).cache().count()) the UI showed me that the entire file is cached. Is this just a fluke?

In any case I believe the question is still valid, how to make sure a dataframe is cached.
Consider for example a case where we read from a remote host (which is costly) and we want to make sure the original read is done at a specific time (when the network is less crowded).
I for one used .count() till now but if this is not guaranteed to cache, then how would I do that? Of course I could always save the dataframe to disk but that would cost a lot more in performance than I would like

As for doing a map partitions for the dataset, wouldnt that cause the row to be converted to the case class for each row? That could also be heavy.
Maybe cache should have a lazy parameter which would be false by default but we could call .cache(true) to make it materialize (similar to what we have with checkpoint).
Assaf.

From: Matei Zaharia [via Apache Spark Developers List] [mailto:ml-node+[hidden email]</user/SendEmail.jtp?type=node&node=21025&i=0>]
Sent: Sunday, February 19, 2017 9:30 AM
To: Mendelson, Assaf
Subject: Re: Will .count() always trigger an evaluation of each row?

 evaluates everything, but on DataFrame/Dataset, it turns into the equivalent of ""select count(*) from ..."" in SQL, which can be done without scanningaching a DataFrame / Dataset does require everything to be cached.

Matei


I think the right answer is ""don't do that"" but if you really had to you could trigger a Dataset operation that does nothing per partition. I presume that would be more reliable because the whole partition has to be computed to make it available in practice. Or, go so far as to loop over every element.


Especially during development, people often use .count() or .persist().count() to force evaluation of all rows  exposing any problems, e.g. due to bad data  and to load data into cache to speed up subsequent operations.

But as the optimizer gets smarter, Im guessing it will eventually learn that it doesnt have to do all that work to give the correct count. (This blog post<https://databricks.com/blog/2017/02/16/processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast.html> suggests that something like this is already happening.) This will change Sparks practical behavior while technically preserving semantics.

What will people need to do then to force evaluation or caching?

Nick



________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Will-count-always-trigger-an-evaluation-of-each-row-tp21018p21024.html
To start a new topic under Apache Spark Developers List, email [hidden email]</user/SendEmail.jtp?type=node&node=21025&i=1>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: RE: Will .count() always trigger an evaluation of each row?<http://apache-spark-developers-list.1001551.n3.nabble.com/Will-count-always-trigger-an-evaluation-of-each-row-tp21018p21025.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com<http://Nabble.com>.

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Will-count-always-trigger-an-evaluation-of-each-row-tp21018p21026.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Will-count-always-trigger-an-evaluation-of-each-row-tp21018p21027.html
om."
"""=?gb18030?B?wtyyt8u/s7S3uQ==?="" <1427357147@qq.com>","Mon, 20 Feb 2017 20:31:40 +0800",compile about the code,"""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","hi all,

when i compile  spark2.0.2,  i meet an error about the  antlr4.   

i paste the info  in the attach file, wpuld you like  help me pls?
---------------------------------------------------------------------"
StanZhai <mail@zhaishidan.cn>,"Mon, 20 Feb 2017 05:40:24 -0700 (MST)",Re:compile about the code,dev@spark.apache.org,"Your antlr4-maven-plugin looks like incomplete, you can try to delete ~/.m2 in your home directory, then re-compile spark.




------------------ Original ------------------
From:  "" 涓楗 [via Apache Spark Developers List]"";<ml-node+s1001551n21030h90@n3.nabble.com>;
Date:  Feb 20, 2017
To:  ""Stan Zhai""<mail@zhaishidan.cn>; 

Subject:  compile about the code



 	    hi all,

when i compile  spark2.0.2,  i meet an error about the  antlr4.   

i paste the info  in the attach file, wpuld you like  help me pls?

--------------------------------------------------------------------- 
 0220_2.png (76K) Download Attachment
 	 	 	 	
 	
 	
 	 		If you reply to this email, your message will be added to the discussion below:
 		http://apache-spark-developers-list.1001551.n3.nabble.com/compile-about-the-code-tp21030.html 	
 	 		To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h91@n3.nabble.com 
 		NAML



--
3.nabble.com/compile-about-the-code-tp21030p21031.html
om."
"""=?gb18030?B?wtyyt8u/s7S3uQ==?="" <1427357147@qq.com>","Mon, 20 Feb 2017 21:36:01 +0800",Re: Re:compile about the code,"""=?gb18030?B?U3RhblpoYWk=?="" <mail@zhaishidan.cn>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","hi zhai,

l redid it  by your  idea,  it is ok now.  thanks.


---Original---
From: ""StanZhai""<mail@zhaishidan.cn>
Date: 2017/2/20 20:40:24
To: ""dev""<dev@spark.apache.org>;
Subject: Re:compile about the code


Your antlr4-maven-plugin looks like incomplete, you can try to delete ~/.m2 in your home directory, then re-compile spark.




------------------ Original ------------------
From:  "" 萝卜丝炒饭 [via Apache Spark Developers List]"";<[hidden email]>;
Date:  Feb 20, 2017
To:  ""Stan Zhai""<[hidden email]>; 

Subject:  compile about the code



     hi all,

when i compile  spark2.0.2,  i meet an error about the  antlr4.   

i paste the info  in the attach file, wpuld you like  help me pls?

--------------------------------------------------------------------- 
To unsubscribe e-mail: [hidden email]
 0220_2.png (76K) Download Attachment
 
 
 
  If you reply to this email, your message will be added to the discussion below:
 http://apache-spark-developers-list.1001551.n3.nabble.com/compile-about-the-code-tp21030.html 
  To start a new topic under Apache Spark Developers List, email [hidden email] 
 To unsubscribe from Apache Spark Developers List, click here.
 NAML 

 

 View this message in context: Re:compile about the code
 Sent from the Apache Spark Developers List mailing list archive at Nabble.com."
Matthew Schauer <matthew.schauer@ibm.com>,"Mon, 20 Feb 2017 11:14:06 -0700 (MST)",Output Committers for S3,dev@spark.apache.org,"I'm using Spark 1.5.2 and trying to append a data frame to partitioned
Parquet directory in S3.  It is known that the default
`ParquetOutputCommitter` performs poorly in S3 because move is implemented
as copy/delete, but the `DirectParquetOutputCommitter` is not safe to use
for append operations in case of failure.  I'm not very familiar with the
intricacies of job/task committing/aborting, but I've written a rough
replacement output committer that seems to work.  It writes the results
directly to their final locations and uses the write UUID to determine which
files to remove in the case of a job/task abort.  It seems to be a workable
concept in the simple tests that I've tried.  However, I can't make Spark
use this alternate output committer because the changes in SPARK-8578
categorically prohibit any custom output committer from being used, even if
it's safe for appending.  I have two questions: 1) Does anyone more familiar
with output committing have any feedback on my proposed ""safe"" append
strategy, and 2) is there any way to circumvent the restriction on append
committers without editing and recompiling Spark?  Discussion of solutions
in Spark 2.1 is also welcome. 



--

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 20 Feb 2017 17:00:05 -0800",Re: Output Committers for S3,Matthew Schauer <matthew.schauer@ibm.com>,"We just wrote a couple new committers for S3 that we're beginning to roll
out to our Spark users. I've uploaded a repo with it if you'd like to take
a look:

  https://github.com/rdblue/s3committer

The main problem with the UUID approach is that data is live as soon as the
S3 upload completes. That means that readers can get partial results while
a job is running that may not be eventually committed (since you will
remove the UUID later). You may also have a problem with partitioned task
outputs. You'd have to encode the task ID in the output file name to
identify files to roll back in the event you need to revert a task, but if
you have partitioned output, you have to do a lot of directory listing to
find all the files that need to be removed. That, or you could risk
duplicate data by not rolling back tasks.

The approach we took is to use the multi-part upload API to stage data from
tasks without issuing the final call to complete the upload and make the
data live in S3. That way, we get distributed uploads without any visible
data until the job committer runs. The job committer reads all of the
pending uploads and commits them. If the job has failed, then it can roll
back the known uploads by aborting them instead, with the data never
visible to readers.

The flaw in this approach is that you can still get partial writes if the
driver fails while running the job committer, but it covers the other cases.

We're working on getting users moved over to the new committers, so now
seems like a good time to get a copy out to the community. Please let me
know what you think.

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 20 Feb 2017 17:11:54 -0800",Re: Will .count() always trigger an evaluation of each row?,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","I think it is a great idea to have a way to force execution to build a
cached dataset.

The use case for this that we see the most is to build broadcast tables.
Right now, there's a 5-minute timeout to build a broadcast table. That's
plenty of time if the data is sitting in a table, but we see a lot of users
that have a dataframe with a complicated query plan that they know is small
enough to broadcast. If that query plan is several stages, it can cause the
job to fail because of the timeout. I usually recommend caching/persisting
the content and then running the broadcast join to avoid the timeout.

I realize that the right solution is to get rid of the timeout when
building a broadcast table, but forcing materialization is useful for
things like this. I'd like to see a legitimate way to do it, since people
currently rely on count.

rb


de+[hidden
howed me that

e the row
[hidden
t
d.
peed up
y learn
t. (This
nd-single-machine-can-nested-loop-joins-fast.html>
.
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ays-trigger-an-evaluation-of-each-row-tp21018p21025.html>
m
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ays-trigger-an-evaluation-of-each-row-tp21018p21027.html>



-- 
Ryan Blue
Software Engineer
Netflix
"
ron8hu <ron.hu@huawei.com>,"Mon, 20 Feb 2017 19:51:47 -0700 (MST)","In Intellij, maven failed to build Catalyst project",dev@spark.apache.org,"I am using Intellij IDEA 15.0.6.   I used to use Maven to compile Spark
project Catalyst inside Intellij without problem.  

A couple of days ago, I fetched latest Spark code from its master
repository.  There was a change in CreateJacksonParser.scala.  So I used
Maven to compile Catalyst project again.  Then I got this error:
...../spark/sql/catalyst/json/CreateJacksonParser.scala:33: value
getByteBuffer is not a member of org.apache.spark.unsafe.types.UTF8String

Note that I was able to compile the source code using sbt.  This compile
error happens inside Intellij/Maven.  Below is the detailed message.   Any
advice will be appreciated.  
*******  ****************  *******
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @
spark-catalyst_2.11 ---
[INFO] Using zinc server for incremental compilation

Compiling 203 Scala sources and 26 Java sources to
/home/rhu/spark-20/sql/catalyst/target/scala-2.11/classes....
/home/rhu/spark-20/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/CreateJacksonParser.scala:33:
value getByteBuffer is not a member of
org.apache.spark.unsafe.types.UTF8String
    val bb = record.getByteBuffer
                    ^ not found

[INFO] Compile failed at Feb 20, 2017
[INFO]
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 01:53 min
[INFO] Finished at: 2017-02-20T17:37:55-08:00
[INFO] Final Memory: 38M/795M
[INFO]
------------------------------------------------------------------------
[WARNING] The requested profile ""hive"" could not be activated because it
does not exist.
[ERROR] Failed to execute goal
net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on
project spark-catalyst_2.11: Execution scala-compile-first of goal
net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed. CompileFailed ->
[Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please
read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException

Process finished with exit code 1




--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 21 Feb 2017 05:35:31 +0000","Re: In Intellij, maven failed to build Catalyst project","ron8hu <ron.hu@huawei.com>, dev@spark.apache.org","I saw this too yesterday but not today. It may have been fixed by some
recent commits.


I am using Intellij IDEA 15.0.6.   I used to use Maven to compile Spark
project Catalyst inside Intellij without problem.

A couple of days ago, I fetched latest Spark code from its master
repository.  There was a change in CreateJacksonParser.scala.  So I used
Maven to compile Catalyst project again.  Then I got this error:
...../spark/sql/catalyst/json/CreateJacksonParser.scala:33: value
getByteBuffer is not a member of org.apache.spark.unsafe.types.UTF8String

Note that I was able to compile the source code using sbt.  This compile
error happens inside Intellij/Maven.  Below is the detailed message.   Any
advice will be appreciated.
*******  ****************  *******
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @
spark-catalyst_2.11 ---
[INFO] Using zinc server for incremental compilation

Compiling 203 Scala sources and 26 Java sources to
/home/rhu/spark-20/sql/catalyst/target/scala-2.11/classes....
/home/rhu/spark-20/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/CreateJacksonParser.scala:33:
value getByteBuffer is not a member of
org.apache.spark.unsafe.types.UTF8String
    val bb = record.getByteBuffer
                    ^ not found

[INFO] Compile failed at Feb 20, 2017
[INFO]
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 01:53 min
[INFO] Finished at: 2017-02-20T17:37:55-08:00
[INFO] Final Memory: 38M/795M
[INFO]
------------------------------------------------------------------------
[WARNING] The requested profile ""hive"" could not be activated because it
does not exist.
[ERROR] Failed to execute goal
net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on
project spark-catalyst_2.11: Execution scala-compile-first of goal
net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed. CompileFailed ->
[Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please
read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException

Process finished with exit code 1




--
View this message in context:
http://apache-spark-developers-list.1001551.n3.nabble.com/In-Intellij-maven-failed-to-build-Catalyst-project-tp21036.html
Nabble.com.

---------------------------------------------------------------------
"
Armin Braun <me@obrown.io>,"Tue, 21 Feb 2017 07:40:31 +0100","Re: In Intellij, maven failed to build Catalyst project",Sean Owen <sowen@cloudera.com>,"I think the reason you're seeing this (and it then disappearing in Sean's
case) is likely that there was a change in another that required a
recompile of a module dependency.
Maven doesn't do this automatically by default. So it eventually goes away
when you do a full build either with Maven or SBT.

You could add `--also-make-dependents`  (-amd) as a build flag to fix this
behavior and rebuild dependencies too.


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 21 Feb 2017 13:52:01 +0000",Re: Output Committers for S3,Matthew Schauer <matthew.schauer@ibm.com>,"

I'm using Spark 1.5.2 and trying to append a data frame to partitioned
Parquet directory in S3.  It is known that the default
`ParquetOutputCommitter` performs poorly in S3 because move is implemented
as copy/delete, but the `DirectParquetOutputCommitter` is not safe to use
for append operations in case of failure.  I'm not very familiar with the
intricacies of job/task committing/aborting, but I've written a rough
replacement output committer that seems to work.  It writes the results
directly to their final locations and uses the write UUID to determine which
files to remove in the case of a job/task abort.  It seems to be a workable
concept in the simple tests that I've tried.  However, I can't make Spark
use this alternate output committer because the changes in SPARK-8578
categorically prohibit any custom output committer from being used, even if
it's safe for appending.  I have two questions: 1) Does anyone more familiar
with output committing have any feedback on my proposed ""safe"" append
strategy, and 2) is there any way to circumvent the restriction on append
committers without editing and recompiling Spark?  Discussion of solutions
in Spark 2.1 is also welcome.




Matthew, as part of the S3guard committer I'm doing in the Hadoop codebase (which requires a consistent object store implemented natively or via a dynamo db database), I'm modifying FileOutputFormat to take alternate committers underneath.

Algorithm
https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3a_committer.md

Code:

https://github.com/steveloughran/hadoop/tree/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit


Modified FOF: https://github.com/steveloughran/hadoop/tree/s3guard/HADOOP-13786-committer/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output

Current status: getting the low level tests at the MR layer working. Spark committer exists to the point of compiling, but not yet tested. If you do want to get involved; the JIRA is: https://issues.apache.org/jira/browse/HADOOP-13786



--
3.nabble.com/Output-Committers-for-S3-tp21033.html
om<http://Nabble.com>.

---------------------------------------------------------------------
ibe@spark.apache.org>



"
Steve Loughran <stevel@hortonworks.com>,"Tue, 21 Feb 2017 14:15:48 +0000",Re: Output Committers for S3,Ryan Blue <rblue@netflix.com.INVALID>,"

We just wrote a couple new committers for S3 that we're beginning to roll out to our Spark users. I've uploaded a repo with it if you'd like to take a look:

  https://github.com/rdblue/s3committer

The main problem with the UUID approach is that data is live as soon as the S3 upload completes. That means that readers can get partial results while a job is running that may not be eventually committed (since you will remove the UUID later). You may also have a problem with partitioned task outputs.


You'd have to encode the task ID in the output file name to identify files to roll back in the event you need to revert a task, but if you have partitioned output, you have to do a lot of directory listing to find all the files that need to be removed. That, or you could risk duplicate data by not rolling back tasks.


Bear in mind that recursive directory listing isn't so expensive once you have the O(1)-ish listFiles(files, recursive) operation of HADOOP-13208.



The approach we took is to use the multi-part upload API to stage data from tasks without issuing the final call to complete the upload and make the data live in S3. That way, we get distributed uploads without any visible data until the job committer runs. The job committer reads all of the pending uploads and commits them. If the job has failed, then it can roll back the known uploads by aborting them instead, with the data never visible to readers.


Yes, that's what I've been doing too. I'm choreographing the task and commi to roll back all completely written files, without the need for any task-job communications. I'm still thinking about having an optional+async scan for pending commits to the dest path, to identify problems and keep bills down.



The flaw in this approach is that you can still get partial writes if the driver fails while running the job committer, but it covers the other cases.

There's a bit of that in both the FileOutputFormat and indeed, in HadoopMapReduceCommitProtocol. It's just a small window, especially if you do those final PUTs in parallel


We're working on getting users moved over to the new committers, so now seems like a good time to get a copy out to the community. Please let me know what you think.

I'll have a look at your code, see how it compares to mine. I'm able to take advantage of the fact that we can tune the S3A FS, for example, by modifying the block output stream to *not* commit its work in the final close()

https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java

This means that provided the output writer doesn't attempt to read the file it's just written, we can do a write straight to the final destination


What your patch has made me realise is that I could also do a delayed-commit copy by reading in a file, doing a multipart put to its final destination, and again, postponing the final commit. this is something which tasks could do in their commit rather than a normal COPY+DELETE  rename, passing the final pending commit information to the job committer. This'd make the rename() slower as it will read and write the data again, rather than the 6-10 MB/s of in-S3 copies, but as these happen in-task-commit, rather than in-job-commit, they slow down the overall job less. That could be used for the absolute path commit phase.


-Steve

rb

I'm using Spark 1.5.2 and trying to append a data frame to partitioned
Parquet directory in S3.  It is known that the default
`ParquetOutputCommitter` performs poorly in S3 because move is implemented
as copy/delete, but the `DirectParquetOutputCommitter` is not safe to use
for append operations in case of failure.  I'm not very familiar with the
intricacies of job/task committing/aborting, but I've written a rough
replacement output committer that seems to work.  It writes the results
directly to their final locations and uses the write UUID to determine which
files to remove in the case of a job/task abort.  It seems to be a workable
concept in the simple tests that I've tried.  However, I can't make Spark
use this alternate output committer because the changes in SPARK-8578
categorically prohibit any custom output committer from being used, even if
it's safe for appending.  I have two questions: 1) Does anyone more familiar
with output committing have any feedback on my proposed ""safe"" append
strategy, and 2) is there any way to circumvent the restriction on append
committers without editing and recompiling Spark?  Discussion of solutions
in Spark 2.1 is also welcome.



--
3.nabble.com/Output-Committers-for-S3-tp21033.html
om<http://Nabble.com>.

---------------------------------------------------------------------
ibe@spark.apache.org>




--
Ryan Blue
Software Engineer
Netflix

"
Steve Loughran <stevel@hortonworks.com>,"Tue, 21 Feb 2017 14:32:54 +0000",Re: Output Committers for S3,Ryan Blue <rblue@netflix.com.INVALID>,"

What your patch has made me realise is that I could also do a delayed-commit copy by reading in a file, doing a multipart put to its final destination, and again, postponing the final commit. this is something which tasks could do in their commit rather than a normal COPY+DELETE  rename, passing the final pending commit information to the job committer. This'd make the rename() slower as it will read and write the data again, rather than the 6-10 MB/s of in-S3 copies, but as these happen in-task-commit, rather than in-job-commit, they slow down the overall job less. That could be used for the absolute path commit phase.


though as you can do specify a copy-range in a multipart put, you could do a parallelized copies of parts of a file in the s3 filestore itself and leave the result pending, reducing copy time in seconds to ~ filesize / (parts * 6e6), the same as you get from a parallel copy in s3 today. That is: same time as a rename, merely not visible until the final job chooses to materialize the object
"
Matthew Schauer <matthew.schauer@ibm.com>,"Tue, 21 Feb 2017 13:39:52 -0700 (MST)",Re: Output Committers for S3,dev@spark.apache.org,"Thanks for the repo, Ryan!  I had heard that Netflix had a committer that
used the local filesystem as a temporary store, but I wasn't able to find
that anywhere until now.  I implemented something similar that writes to
HDFS and then copies to S3, but it doesn't use the multipart upload API, so
I'm sure yours will be faster.  I think this is the best thing until S3Guard
comes out.

As far as my UUID-tracking approach goes, I was under the impression that a
given task would write the same set of files on each attempt.  Thus, if the
task fails, either the whole job is aborted and the files are removed, or
see how having partially-written data visible to readers immediately could
cause problems, and that is a good reason to avoid my approach.

Steve -- that design document was a very enlightening read.  I will be
interested in following and possibly contributing to S3Guard in the future.



--

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 21 Feb 2017 13:20:46 -0800",Re: Output Committers for S3,Steve Loughran <stevel@hortonworks.com>,"ote:
les to roll back in the event you need to revert a task, but if you have partitioned output, you have to do a lot of directory listing to find all the files that need to be removed. That, or you could risk duplicate data by not rolling back tasks.
 have the O(1)-ish listFiles(files, recursive) operation of HADOOP-13208.

This doesn't work so well for us because we have an additional layer
to provide atomic commits by swapping locations in the metastore. That
causes areas in the prefix listing where we have old data mixed with
new. You make a good point that it won't be quite so expensive to list
an entire repository in the future, but I'd still rather avoid it.

ritten files, without the need for any task-job communications. I'm still thinking about having an optional+async scan for pending commits to the dest path, to identify problems and keep bills down.

What do you mean by avoiding task-job communications? Don't you still
need to communicate to the job commit what files the tasks produced?
It sounds like you're using S3 for that instead of HDFS with the
FileOutputCommitter like this does. Long-term, I'd probably agree that
we shouldn't rely on HDFS, but it seems like a good way to get
something working right now.

he driver fails while running the job committer, but it covers the other cases.
apReduceCommitProtocol. It's just a small window, especially if you do those final PUTs in parallel

Yeah, it is unavoidable right now with the current table format.

 seems like a good time to get a copy out to the community. Please let me know what you think.
ake advantage of the fact that we can tune the S3A FS, for example, by modifying the block output stream to *not* commit its work in the final close()
er/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java
le it's just written, we can do a write straight to the final destination

We're also considering an implementation that does exactly what we use
the local FS for today, but keeps data in memory. When you close the
stream, you'd get the same PendingUpload object we build from the file
system. This is something we put off to get something out more
quickly.

Thanks for taking a look!

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 21 Feb 2017 13:24:38 -0800",Re: Output Committers for S3,Matthew Schauer <matthew.schauer@ibm.com>,"Does S3Guard help with this? I thought it was like S3mper and could
help detect eventual consistency problems, but wouldn't help with the
committer problem.

rb




-- 
Ryan Blue
Software Engineer
Netflix

---------------------------------------------------------------------


"
gen tang <gen.tang86@gmail.com>,"Wed, 22 Feb 2017 13:57:16 +0800",A DataFrame cache bug,dev@spark.apache.org,"Hi All,

I found a strange bug which is related with reading data from a updated
path and cache operation.
Please consider the following code:

import org.apache.spark.sql.DataFrame

def f(data: DataFrame): DataFrame = {
  val df = data.filter(""id>10"")
  df.cache
  df.count
  df
}

f(spark.range(100).asInstanceOf[DataFrame]).count // output 89 which is
correct
f(spark.range(1000).asInstanceOf[DataFrame]).count // output 989 which is
correct

val dir = ""/tmp/test""
spark.range(100).write.mode(""overwrite"").parquet(dir)
val df = spark.read.parquet(dir)
df.count // output 100 which is correct
f(df).count // output 89 which is correct

spark.range(1000).write.mode(""overwrite"").parquet(dir)
val df1 = spark.read.parquet(dir)
df1.count // output 1000 which is correct, in fact other operation expect
df1.filter(""id>10"") return correct result.
f(df1).count // output 89 which is incorrect

In fact when we use df1.filter(""id>10""), spark will however use old cached
dataFrame

Any idea? Thanks a lot

Cheers
Gen
"
gen tang <gen.tang86@gmail.com>,"Wed, 22 Feb 2017 14:01:18 +0800",Re: A DataFrame cache bug,dev@spark.apache.org,"Hi All,

I might find a related issue on jira:

https://issues.apache.org/jira/browse/SPARK-15678

This issue is closed, may be we should reopen it.

Thanks

Cheers
Gen



"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Wed, 22 Feb 2017 15:22:40 +0900",Re: A DataFrame cache bug,gen tang <gen.tang86@gmail.com>,"Hi,
Thank you for pointing out the JIRA.
I think that this JIRA suggests you to insert 
""spark.catalog.refreshByPath(dir)"".

val dir = ""/tmp/test""
spark.range(100).write.mode(""overwrite"").parquet(dir)
val df = spark.read.parquet(dir)
df.count // output 100 which is correct
f(df).count // output 89 which is correct

spark.range(1000).write.mode(""overwrite"").parquet(dir)
spark.catalog.refreshByPath(dir)  // insert a NEW statement
val df1 = spark.read.parquet(dir)
df1.count // output 1000 which is correct, in fact other operation expect 
df1.filter(""id>10"") return correct result.
f(df1).count // output 89 which is incorrect

Regards,
Kazuaki Ishizaki



From:   gen tang <gen.tang86@gmail.com>
To:     dev@spark.apache.org
Date:   2017/02/22 15:02
Subject:        Re: A DataFrame cache bug



Hi All,

I might find a related issue on jira:

https://issues.apache.org/jira/browse/SPARK-15678

This issue is closed, may be we should reopen it.

Thanks 

Cheers
Gen


Hi All,

I found a strange bug which is related with reading data from a updated 
path and cache operation.
Please consider the following code:

import org.apache.spark.sql.DataFrame

def f(data: DataFrame): DataFrame = {
  val df = data.filter(""id>10"")
  df.cache
  df.count
  df
}

f(spark.range(100).asInstanceOf[DataFrame]).count // output 89 which is 
correct
f(spark.range(1000).asInstanceOf[DataFrame]).count // output 989 which is 
correct

val dir = ""/tmp/test""
spark.range(100).write.mode(""overwrite"").parquet(dir)
val df = spark.read.parquet(dir)
df.count // output 100 which is correct
f(df).count // output 89 which is correct

spark.range(1000).write.mode(""overwrite"").parquet(dir)
val df1 = spark.read.parquet(dir)
df1.count // output 1000 which is correct, in fact other operation expect 
df1.filter(""id>10"") return correct result.
f(df1).count // output 89 which is incorrect

In fact when we use df1.filter(""id>10""), spark will however use old cached 
dataFrame

Any idea? Thanks a lot

Cheers
Gen



"
gen tang <gen.tang86@gmail.com>,"Wed, 22 Feb 2017 15:47:29 +0800",Re: A DataFrame cache bug,Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>,"Hi Kazuaki Ishizaki

Thanks a lot for your help. It works. However, a more strange bug appears
as follows:

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.SparkSession

def f(path: String, spark: SparkSession): DataFrame = {
  val data = spark.read.option(""mergeSchema"", ""true"").parquet(path)
  println(data.count)
  val df = data.filter(""id>10"")
  df.cache
  println(df.count)
  val df1 = df.filter(""id>11"")
  df1.cache
  println(df1.count)
  df1
}

val dir = ""/tmp/test""
spark.range(100).write.mode(""overwrite"").parquet(dir)
spark.catalog.refreshByPath(dir)
f(dir, spark).count // output 88 which is correct

spark.range(1000).write.mode(""overwrite"").parquet(dir)
spark.catalog.refreshByPath(dir)
f(dir, spark).count // output 88 which is incorrect

If we move refreshByPath into f(), just before spark.read. The whole code
works fine.

Any idea? Thanks a lot

Cheers
Gen



"
gen tang <gen.tang86@gmail.com>,"Wed, 22 Feb 2017 16:02:50 +0800",Re: A DataFrame cache bug,Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>,"Hi, The example that I provided is not very clear. And I add a more clear
example in jira.

Thanks

Cheers
Gen


"
Matthew Schauer <matthew.schauer@ibm.com>,"Wed, 22 Feb 2017 08:20:49 -0700 (MST)",Re: Output Committers for S3,dev@spark.apache.org,"Well, the issue I'm trying to solve is slow writing due to S3's
implementation of move as copy/delete.  It seems like your S3 committers and
S3Guard both ameliorate that somewhat by parallelizing the copy.  I assume
there's no better way to solve this issue without sacrificing safety.  Even
if there were, I couldn't use it, because I'm stuck on Spark 1.5 and there
doesn't seem to be a way to force the use of a given output committer.



--

---------------------------------------------------------------------


"
StanZhai <mail@zhaishidan.cn>,"Wed, 22 Feb 2017 10:30:00 -0700 (MST)",The driver hangs at DataFrame.rdd in Spark 2.1.0,dev@spark.apache.org,"Hi all,


The driver hangs at DataFrame.rdd in Spark 2.1.0 when the DataFrame(SQL) is complex, Following thread dump of my driver:


org.apache.spark.sql.catalyst.expressions.AttributeReference.equals(namedExpressions.scala:230) org.apache.spark.sql.catalyst.expressions.IsNotNull.equals(nullExpressions.scala:312) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:315) scala.collection.mutable.FlatHashTable$class.addEntry(FlatHashTable.scala:151) scala.collection.mutable.HashSet.addEntry(HashSet.scala:40) scala.collection.mutable.FlatHashTable$class.addElem(FlatHashTable.scala:139) scala.collection.mutable.HashSet.addElem(HashSet.scala:40) scala.collection.mutable.HashSet.$plus$eq(HashSet.scala:59) scala.collection.mutable.HashSet.$plus$eq(HashSet.scala:40) scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59) scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59) scala.collection.mutable.HashSet.foreach(HashSet.scala:78) scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) scala.collection.mutable.AbstractSet.$plus$plus$eq(Set.scala:46) scala.collection.mutable.HashSet.clone(HashSet.scala:83) scala.collection.mutable.HashSet.clone(HashSet.scala:40) org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus(ExpressionSet.scala:65) org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus(ExpressionSet.scala:50) scala.collection.SetLike$$anonfun$$plus$plus$1.apply(SetLike.scala:141) scala.collection.SetLike$$anonfun$$plus$plus$1.apply(SetLike.scala:14h(HashSet.scala:316) scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972) scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972) scala.collection.immutable.HashSet$HashTrieSet.foreach(Hce.scala:151) scala.collection.AbstractTraversable.$div$colon(Traversable.scala:104) scala.collection.SetLike$class.$plus$plus(SetLike.scala:141) org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus$plus(ExpressionSet.scala:50) org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$anonfun$getAliasedConstraints$1.apply(LogicalPlan.scala:300) org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$anonfun$getAliasedConstraints$1.apply(LogicalPlan.scala:297) scala.collection.immutable.List.foreach(List.scala:381) org.apache.spark.sql.catalyst.plans.logical.UnaryNode.getAliasedConstraints(LogicalPlan.scala:297) org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:58) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187) => holding Monitor(org.apache.spark.sql.catalyst.plans.logical.Join@1365611745}) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187) org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:58) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187) => holding Monitor(org.apache.spark.sql.catalyst.plans.logical.Join@1365611745}) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187) org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:58) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187) => holding Monitor(org.apache.spark.sql.catalyst.plans.logical.Join@1365611745}) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187) org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:58) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187) => holding Monitor(org.apache.spark.sql.catalyst.plans.logical.Join@1365611745}) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187) org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:58) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187) => holding Monitor(org.apache.spark.sql.catalyst.plans.logical.Join@1365611745}) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187) org.apache.spark.sql.catalyst.plans.logical.Join.validConstraints(basicLogicalOperators.scala:302) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187) => holding Monitor(org.apache.spark.sql.catalyst.plans.logical.Join@1365611745}) org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$$anonfun$apply$13.applyOrElse(Optimizer.scala:618) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$$anonfun$apply$13.applyOrElse(Optimizer.scala:605) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:332) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:332) org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70) org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:331) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1.apply(TreeNode.scala:202) org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188) org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:200) org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1.apply(TreeNode.scala:202) org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188) org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:200) org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1.apply(TreeNode.scala:202) org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188) org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:200) org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1.apply(TreeNode.scala:202) org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188) org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:200) org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:337) org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:321) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$.apply(Optimizer.scala:605) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$.apply(Optimizer.scala:604) org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85) org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82) scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35) org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82) org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74) scala.collection.immutable.List.foreach(List.scala:381) org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74) org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73) => holding Monitor(org.apache.spark.sql.execution.QueryExecution@184471747}) org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73) org.apache.spark.sql.execution.QueryExecution$$anonfun$toString$2.apply(QueryExecution.scala:230) org.apache.spark.sql.execution.QueryExecution$$anonfun$toString$2.apply(QueryExecution.scala:230) org.apache.spark.sql.execution.QueryExecution.stringOrError(QueryExecution.scala:107) org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:230) org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2544) org.apache.spark.sql.Dataset.rdd(Dataset.scala:2544)...


The CPU usage of the driver remains 100% like this:



I didn't find this issue in Spark 1.6.2, what causes this in Spark 2.1.0? 


Any help is greatly appreciated!


Best,
Stan

25FEF70B@52873242.80CAAD58 (27K) <http://apache-spark-developers-list.1001551.n3.nabble.com/attachment/21050/0/25FEF70B%4052873242.80CAAD58>




--
3.nabble.com/The-driver-hangs-at-DataFrame-rdd-in-Spark-2-1-0-tp21050.html
om."
Shouheng Yi <shouyi@microsoft.com.INVALID>,"Wed, 22 Feb 2017 20:51:19 +0000",[Spark Namespace]: Expanding Spark ML under Different Namespace?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark developers,

Currently my team at Microsoft is extending Spark's machine learning functionalities to include new learners and transformers. We would like users to use these within spark pipelines so that they can mix and match with existing Spark learners/transformers, and overall have a native spark experience. We cannot accomplish this using a non-""org.apache"" namespace with the current implementation, and we don't want to release code inside the apache namespace because it's confusing and there could be naming rights issues.

We need to extend several classes from spark which happen to have ""private[spark]."" For example, one of our class extends VectorUDT[0] which has private[spark] class VectorUDT as its access modifier. This unfortunately put us in a strange scenario that forces us to work under the namespace org.apache.spark.

To be specific, currently the private classes/traits we need to use to create new Spark learners & Transformers are HasInputCol, VectorUDT and Logging. We will expand this list as we develop more.

Is there a way to avoid this namespace issue? What do other people/companies do in this scenario? Thank you for your help!

[0]: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/linalg/VectorUDT.scala

Best,
Shouheng

"
StanZhai <mail@zhaishidan.cn>,"Wed, 22 Feb 2017 18:16:40 -0700 (MST)",Re:  The driver hangs at DataFrame.rdd in Spark 2.1.0,dev@spark.apache.org,"Thanks for lian's reply.


Here is the QueryPlan generated by Spark 1.6.2(I can't get it in Spark 2.1.0):


== Parsed Logical Plan == 'GroupingSets [0], ['col0], [unresolvedalias('col0 AS col0#3605374),unresolvedalias('sum('col1) AS col1#3605375),unresolvedalias('sum('col2) AS col2#3605376),unresolvedalias('avg('col3) AS col3#3605377),unresolvedalias('sum('col4) AS col4#3605378),unresolvedalias('sum('col5) AS col5#3605379),unresolvedalias(('sum('col22) / 'sum('col23)) AS col6#3605380),unresolvedalias('sum('col7) AS col7#3605381),unresolvedalias('sum('col8) AS col8#3605382),unresolvedalias('sum('col9) AS col9#3605383),unresolvedalias('sum('col10) AS col10#3605384),unresolvedalias('sum('col11) AS col11#3605385),unresolvedalias(('sum('col24) / 'sum('col25)) AS col12#3605386),unresolvedalias(('sum('col24) / 'sum('col27)) AS col13#3605387),unresolvedalias(('sum('col24) / 'sum('col29)) AS col14#3605388),unresolvedalias(('sum('col24) / 'sum('col31)) AS col15#3605389),unresolvedalias(('sum('col25) / 'sum('col23)) AS col16#3605390),unresolvedalias(('sum('col27) / 'sum('col23)) AS col17#3605391),unresolvedalias(('sum('col29) / 'sum('col27)) AS col18#3605392),unresolvedalias(('sum('col31) / 'sum('col29)) AS col19#3605393),unresolvedalias(('sum('col31) / 'sum('col27)) AS col20#3605394),unresolvedalias('sum('col21) AS col21#3605395),unresolvedalias('GROUPING__ID AS GROUPING__ID#3605396)] +- 'UnresolvedRelation `tmp_join_tb_5c0f9ebc18fc8b07_3`, None == Analyzed Logical Plan == col0: string, col1: double, col2: double, col3: double, col4: double, col5: double, col6: double, col7: double, col8: double, col9: double, col10: double, col11: double, col12: double, col13: double, col14: double, col15: double, col16: double, col17: double, col18: double, col19: double, col20: double, col21: double, GROUPING__ID: int Aggregate [col0#3605399,grouping__id#3605398], [col0#3605399 AS col0#3605374,(sum(col1#3605333),mode=Complete,isDistinct=false) AS col1#3605375,(sum(col2#3605334),mode=Complete,isDistinct=false) AS col2#3605376,(avg(col3#3605335),mode=Complete,isDistinct=false) AS col3#3605377,(sum(col4#3605336),mode=Complete,isDistinct=false) AS col4#3605378,(sum(col5#3605337),mode=Complete,isDistinct=false) AS col5#3605379,((sum(col22#3605354),mode=Complete,isDistinct=false) / (sum(col23#3605355),mode=Complete,isDistinct=false)) AS col6#3605380,(sum(col7#3605339),mode=Complete,isDistinct=false) AS col7#3605381,(sum(col8#3605340),mode=Complete,isDistinct=false) AS col8#3605382,(sum(col9#3605341),mode=Complete,isDistinct=false) AS col9#3605383,(sum(col10#3605342),mode=Complete,isDistinct=false) AS col10#3605384,(sum(col11#3605343),mode=Complete,isDistinct=false) AS col11#3605385,((sum(col24#3605356),mode=Complete,isDistinct=false) / (sum(col25#3605357),mode=Complete,isDistinct=false)) AS col12#3605386,((sum(col24#3605356),mode=Complete,isDistinct=false) / (sum(col27#3605359),mode=Complete,isDistinct=false)) AS col13#3605387,((sum(col24#3605356),mode=Complete,isDistinct=false) / (sum(col29#3605361),mode=Complete,isDistinct=false)) AS col14#3605388,((sum(col24#3605356),mode=Complete,isDistinct=false) / (sum(col31#3605363),mode=Complete,isDistinct=false)) AS col15#3605389,((sum(col25#3605357),mode=Complete,isDistinct=false) / (sum(col23#3605355),mode=Complete,isDistinct=false)) AS col16#3605390,((sum(col27#3605359),mode=Complete,isDistinct=false) / (sum(col23#3605355),mode=Complete,isDistinct=false)) AS col17#3605391,((sum(col29#3605361),mode=Complete,isDistinct=false) / (sum(col27#3605359),mode=Complete,isDistinct=false)) AS col18#3605392,((sum(col31#3605363),mode=Complete,isDistinct=false) / (sum(col29#3605361),mode=Complete,isDistinct=false)) AS col19#3605393,((sum(col31#3605363),mode=Complete,isDistinct=false) / (sum(col27#3605359),mode=Complete,isDistinct=false)) AS col20#3605394,(sum(col21#3605353),mode=Complete,isDistinct=false) AS col21#3605395,GROUPING__ID#3605398 AS GROUPING__ID#3605396] +- Expand [ArrayBuffer(col0#3605332, col1#3605333, col2#3605334, col3#3605335, col4#3605336, col5#3605337, col6#3605338, col7#3605339, col8#3605340, col9#3605341, col10#3605342, col11#3605343, col12#3605344, col13#3605345, col14#3605346, col15#3605347, col16#3605348, col17#3605349, col18#3605350, col19#3605351, col20#3605352, col21#3605353, col22#3605354, col23#3605355, col24#3605356, col25#3605357, col26#3605358, col27#3605359, col28#3605360, col29#3605361, col30#3605362, col31#3605363, col32#3605364, col33#3605365, col34#3605366, col35#3605367, col36#3605368, col37#3605369, col38#3605370, col39#3605371, col40#3605372, col41#3605373, null, 0)], [col0#3605332,col1#3605333,col2#3605334,col3#3605335,col4#3605336,col5#3605337,col6#3605338,col7#3605339,col8#3605340,col9#3605341,col10#3605342,col11#3605343,col12#3605344,col13#3605345,col14#3605346,col15#3605347,col16#3605348,col17#3605349,col18#3605350,col19#3605351,col20#3605352,col21#3605353,col22#3605354,col23#3605355,col24#3605356,col25#3605357,col26#3605358,col27#3605359,col28#3605360,col29#3605361,col30#3605362,col31#3605363,col32#3605364,col33#3605365,col34#3605366,col35#3605367,col36#3605368,col37#3605369,col38#3605370,col39#3605371,col40#3605372,col41#3605373,col0#3605399,grouping__id#3605398]    +- Project [col0#3605332,col1#3605333,col2#3605334,col3#3605335,col4#3605336,col5#3605337,col6#3605338,col7#3605339,col8#3605340,col9#3605341,col10#3605342,col11#3605343,col12#3605344,col13#3605345,col14#3605346,col15#3605347,col16#3605348,col17#3605349,col18#3605350,col19#3605351,col20#3605352,col21#3605353,col22#3605354,col23#3605355,col24#3605356,col25#3605357,col26#3605358,col27#3605359,col28#3605360,col29#3605361,col30#3605362,col31#3605363,col32#3605364,col33#3605365,col34#3605366,col35#3605367,col36#3605368,col37#3605369,col38#3605370,col39#3605371,col40#3605372,col41#3605373,col0#3605332 AS col0#3605399]       +- Subquery tmp_join_tb_5c0f9ebc18fc8b07_3          +- Project [col0#3604986 AS col0#3605332,col1#3604987 AS col1#3605333,col2#3605081 AS col2#3605334,col3#3604989 AS col3#3605335,col4#3604990 AS col4#3605336,col5#3604991 AS col5#3605337,col6#3604992 AS col6#3605338,col7#3604993 AS col7#3605339,col8#3604994 AS col8#3605340,col9#3605088 AS col9#3605341,col10#3604996 AS col10#3605342,col11#3604997 AS col11#3605343,col12#3604998 AS col12#3605344,col13#3604999 AS col13#3605345,col14#3605000 AS col14#3605346,col15#3605001 AS col15#3605347,col16#3605002 AS col16#3605348,col17#3605003 AS col17#3605349,col18#3605004 AS col18#3605350,col19#3605005 AS col19#3605351,col20#3605006 AS col20#3605352,col21#3605100 AS col21#3605353,col22#3605008 AS col22#3605354,col23#3605009 AS col23#3605355,col24#3605010 AS col24#3605356,col25#3605011 AS col25#3605357,col26#3605012 AS col26#3605358,col27#3605013 AS col27#3605359,col28#3605014 AS col28#3605360,col29#3605015 AS col29#3605361,col30#3605016 AS col30#3605362,col31#3605017 AS col31#3605363,col32#3605018 AS col32#3605364,col33#3605019 AS col33#3605365,col34#3605020 AS col34#3605366,col35#3605021 AS col35#3605367,col36#3605022 AS col36#3605368,col37#3605023 AS col37#3605369,col38#3605024 AS col38#3605370,col39#3605025 AS col39#3605371,col40#3605026 AS col40#3605372,col41#3605027 AS col41#3605373]             +- Join LeftOuter, Some((if (isnull(col0#3604986))  else col0#3604986 = if (isnull(col0#3605079))  else col0#3605079))                :- Subquery tmp_tb_e2e59562f7f4d928                :  +- Project [col0#3604944 AS col0#3604986,col1#3604945 AS col1#3604987,col2#3604946 AS col2#3604988,col3#3604947 AS col3#3604989,col4#3604948 AS col4#3604990,col5#3604949 AS col5#3604991,col6#3604950 AS col6#3604992,col7#3604951 AS col7#3604993,col8#3604952 AS col8#3604994,col9#3604953 AS col9#3604995,col10#3604954 AS col10#3604996,col11#3604955 AS col11#3604997,col12#3604956 AS col12#3604998,col13#3604957 AS col13#3604999,col14#3604958 AS col14#3605000,col15#3604959 AS col15#3605001,col16#3604960 AS col16#3605002,col17#3604961 AS col17#3605003,col18#3604962 AS col18#3605004,col19#3604963 AS col19#3605005,col20#3604964 AS col20#3605006,col21#3604965 AS col21#3605007,col22#3604966 AS col22#3605008,col23#3604967 AS col23#3605009,col24#3604968 AS col24#3605010,col25#3604969 AS col25#3605011,col26#3604970 AS col26#3605012,col27#3604971 AS col27#3605013,col28#3604972 AS col28#3605014,col29#3604973 AS col29#3605015,col30#3604974 AS col30#3605016,col31#3604975 AS col31#3605017,col32#3604976 AS col32#3605018,col33#3604977 AS col33#3605019,col34#3604978 AS col34#3605020,col35#3604979 AS col35#3605021,col36#3604980 AS col36#3605022,col37#3604981 AS col37#3605023,col38#3604982 AS col38#3605024,col39#3604983 AS col39#3605025,col40#3604984 AS col40#3605026,col41#3604985 AS col41#3605027]                :     +- Subquery tmp_tb_e1f587a505324947                :        +- Project [col0#3604902 AS col0#3604944,col1#3604903 AS col1#3604945,col2#3604904 AS col2#3604946,col3#3604905 AS col3#3604947,col4#3604906 AS col4#3604948,col5#3604907 AS col5#3604949,col6#3604908 AS col6#3604950,col7#3604909 AS col7#3604951,col8#3604910 AS col8#3604952,col9#3604911 AS col9#3604953,col10#3604912 AS col10#3604954,col11#3604913 AS col11#3604955,col12#3604914 AS col12#3604956,col13#3604915 AS col13#3604957,col14#3604916 AS col14#3604958,col15#3604917 AS col15#3604959,col16#3604918 AS col16#3604960,col17#3604919 AS col17#3604961,col18#3604920 AS col18#3604962,col19#3604921 AS col19#3604963,col20#3604922 AS col20#3604964,col21#3604923 AS col21#3604965,col22#3604924 AS col22#3604966,col23#3604925 AS col23#3604967,col24#3604926 AS col24#3604968,col25#3604927 AS col25#3604969,col26#3604928 AS col26#3604970,col27#3604929 AS col27#3604971,col28#3604930 AS col28#3604972,col29#3604931 AS col29#3604973,col30#3604932 AS col30#3604974,col31#3604933 AS col31#3604975,col32#3604934 AS col32#3604976,col33#3604935 AS col33#3604977,col34#3604936 AS col34#3604978,col35#3604937 AS col35#3604979,col36#3604938 AS col36#3604980,col37#3604939 AS col37#3604981,col38#3604940 AS col38#3604982,col39#3604941 AS col39#3604983,col40#3604942 AS col40#3604984,col41#3604943 AS col41#3604985]                :           +- Subquery tmp_tb_af5dfe0207076186                :              +- Project [col0#3604860 AS col0#3604902,col1#3604861 AS col1#3604903,col2#3604862 AS col2#3604904,col3#3604863 AS col3#3604905,col4#3604864 AS col4#3604906,col5#3604865 AS col5#3604907,col6#3604866 AS col6#3604908,col7#3604867 AS col7#3604909,col8#3604868 AS col8#3604910,col9#3604869 AS col9#3604911,col10#3604870 AS col10#3604912,col11#3604871 AS col11#3604913,col12#3604872 AS col12#3604914,col13#3604873 AS col13#3604915,col14#3604874 AS col14#3604916,col15#3604875 AS col15#3604917,col16#3604876 AS col16#3604918,col17#3604877 AS col17#3604919,col18#3604878 AS col18#3604920,col19#3604879 AS col19#3604921,col20#3604880 AS col20#3604922,col21#3604881 AS col21#3604923,col22#3604882 AS col22#3604924,col23#3604883 AS col23#3604925,col24#3604884 AS col24#3604926,col25#3604885 AS col25#3604927,col26#3604886 AS col26#3604928,col27#3604887 AS col27#3604929,col28#3604888 AS col28#3604930,col29#3604889 AS col29#3604931,col30#3604890 AS col30#3604932,col31#3604891 AS col31#3604933,col32#3604892 AS col32#3604934,col33#3604893 AS col33#3604935,col34#3604894 AS col34#3604936,col35#3604895 AS col35#3604937,col36#3604896 AS col36#3604938,col37#3604897 AS col37#3604939,col38#3604898 AS col38#3604940,col39#3604899 AS col39#3604941,col40#3604900 AS col40#3604942,col41#3604901 AS col41#3604943]                :                 +- Subquery tmp_tb_d6388581be8bd02b                :                    +- Project [col0#3604818 AS col0#3604860,col1#3604819 AS col1#3604861,col2#3604820 AS col2#3604862,col3#3604821 AS col3#3604863,col4#3604822 AS col4#3604864,col5#3604823 AS col5#3604865,col6#3604824 AS col6#3604866,col7#3604825 AS col7#3604867,col8#3604826 AS col8#3604868,col9#3604827 AS col9#3604869,col10#3604828 AS col10#3604870,col11#3604829 AS col11#3604871,col12#3604830 AS col12#3604872,col13#3604831 AS col13#3604873,col14#3604832 AS col14#3604874,col15#3604833 AS col15#3604875,col16#3604834 AS col16#3604876,col17#3604835 AS col17#3604877,col18#3604836 AS col18#3604878,col19#3604837 AS col19#3604879,col20#3604838 AS col20#3604880,col21#3604839 AS col21#3604881,col22#3604840 AS col22#3604882,col23#3604841 AS col23#3604883,col24#3604842 AS col24#3604884,col25#3604843 AS col25#3604885,col26#3604844 AS col26#3604886,col27#3604845 AS col27#3604887,col28#3604846 AS col28#3604888,col29#3604847 AS col29#3604889,col30#3604848 AS col30#3604890,col31#3604849 AS col31#3604891,col32#3604850 AS col32#3604892,col33#3604851 AS col33#3604893,col34#3604852 AS col34#3604894,col35#3604853 AS col35#3604895,col36#3604854 AS col36#3604896,col37#3604855 AS col37#3604897,col38#3604856 AS col38#3604898,col39#3604857 AS col39#3604899,col40#3604858 AS col40#3604900,col41#3604859 AS col41#3604901]                :                       +- Filter ((((((((((((((((((((isnotnull(col1#3604819) || isnotnull(col2#3604820)) || isnotnull(col3#3604821)) || isnotnull(col4#3604822)) || isnotnull(col5#3604823)) || isnotnull(col6#3604824)) || isnotnull(col7#3604825)) || isnotnull(col8#3604826)) || isnotnull(col9#3604827)) || isnotnull(col10#3604828)) || isnotnull(col11#3604829)) || isnotnull(col12#3604830)) || isnotnull(col13#3604831)) || isnotnull(col14#3604832)) || isnotnull(col15#3604833)) || isnotnull(col16#3604834)) || isnotnull(col17#3604835)) || isnotnull(col18#3604836)) || isnotnull(col19#3604837)) || isnotnull(col20#3604838)) || isnotnull(col21#3604839))                :                          +- Subquery tmp_join_tb_00d22afcd5fd25d4                :                             +- Project [col0#3604561 AS col0#3604818,col1#3604562 AS col1#3604819,col2#3604563 AS col2#3604820,col4#3604779 AS col3#3604821,col4#3604565 AS col4#3604822,col5#3604566 AS col5#3604823,col6#3604567 AS col6#3604824,col7#3604568 AS col7#3604825,col8#3604569 AS col8#3604826,col9#3604570 AS col9#3604827,col10#3604571 AS col10#3604828,col11#3604572 AS col11#3604829,col12#3604573 AS col12#3604830,col13#3604574 AS col13#3604831,col14#3604575 AS col14#3604832,col15#3604576 AS col15#3604833,col16#3604577 AS col16#3604834,col17#3604578 AS col17#3604835,col18#3604579 AS col18#3604836,col19#3604580 AS col19#3604837,col20#3604581 AS col20#3604838,col21#3604582 AS col21#3604839,col22#3604583 AS col22#3604840,col23#3604584 AS col23#3604841,col24#3604585 AS col24#3604842,col25#3604586 AS col25#3604843,col26#3604587 AS col26#3604844,col27#3604588 AS col27#3604845,col28#3604589 AS col28#3604846,col29#3604590 AS col29#3604847,col30#3604591 AS col30#3604848,col31#3604592 AS col31#3604849,col32#3604593 AS col32#3604850,col33#3604594 AS col33#3604851,col34#3604595 AS col34#3604852,col35#3604596 AS col35#3604853,col36#3604597 AS col36#3604854,col37#3604598 AS col37#3604855,col38#3604599 AS col38#3604856,col39#3604600 AS col39#3604857,col40#3604601 AS col40#3604858,col41#3604602 AS col41#3604859]                :                                +- Join LeftOuter, Some(((if (isnull(col0#3604561))  else col0#3604561 = if (isnull(col1#3604776))  else col1#3604776) && (if (isnull(1))  else cast(1 as string) = if (isnull(1))  else cast(1 as string))))                :                                   :- Subquery tmp_tb_0b50b8442300cd3b_1                :                                   :  +- Project [col0#3604519 AS col0#3604561,col1#3604520 AS col1#3604562,col2#3604521 AS col2#3604563,col3#3604522 AS col3#3604564,col4#3604523 AS col4#3604565,col5#3604524 AS col5#3604566,col6#3604525 AS col6#3604567,col7#3604526 AS col7#3604568,col8#3604527 AS col8#3604569,col9#3604528 AS col9#3604570,col10#3604529 AS col10#3604571,col11#3604530 AS col11#3604572,col12#3604531 AS col12#3604573,col13#3604532 AS col13#3604574,col14#3604533 AS col14#3604575,col15#3604534 AS col15#3604576,col16#3604535 AS col16#3604577,col17#3604536 AS col17#3604578,col18#3604537 AS col18#3604579,col19#3604538 AS col19#3604580,col20#3604539 AS col20#3604581,col21#3604540 AS col21#3604582,col22#3604541 AS col22#3604583,col23#3604542 AS col23#3604584,col24#3604543 AS col24#3604585,col25#3604544 AS col25#3604586,col26#3604545 AS col26#3604587,col27#3604546 AS col27#3604588,col28#3604547 AS col28#3604589,col29#3604548 AS col29#3604590,col30#3604549 AS col30#3604591,col31#3604550 AS col31#3604592,col32#3604551 AS col32#3604593,col33#3604552 AS col33#3604594,col34#3604553 AS col34#3604595,col35#3604554 AS col35#3604596,col36#3604555 AS col36#3604597,col37#3604556 AS col37#3604598,col38#3604557 AS col38#3604599,col39#3604558 AS col39#3604600,col40#3604559 AS col40#3604601,col41#3604560 AS col41#3604602]                :                                   :     +- Subquery 726cbefe72d042efacfe27eefed5a4ef_0c82a43bbcb57a2e_0                :                                   :        +- Aggregate [fkc3802436#3604322], [fkc3802436#3604322 AS col0#3604519,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col1#3604520,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col2#3604521,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col3#3604522,(sum(cast(fk6938b0f1#3604330 as double)),mode=Complete,isDistinct=false) AS col4#3604523,(sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false) AS col5#3604524,((sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false)) AS col6#3604525,(sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false) AS col7#3604526,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col8#3604527,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col9#3604528,(sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false) AS col10#3604529,(sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) AS col11#3604530,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false)) AS col12#3604531,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false)) AS col13#3604532,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false)) AS col14#3604533,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false)) AS col15#3604534,((sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false)) AS col16#3604535,((sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false)) AS col17#3604536,((sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false)) AS col18#3604537,((sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false)) AS col19#3604538,((sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false)) AS col20#3604539,(sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) AS col21#3604540,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col22#3604541,(sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false) AS col23#3604542,(sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) AS col24#3604543,(sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false) AS col25#3604544,(sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) AS col26#3604545,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col27#3604546,(sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) AS col28#3604547,(sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false) AS col29#3604548,(sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) AS col30#3604549,(sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) AS col31#3604550,(sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false) AS col32#3604551,(sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false) AS col33#3604552,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col34#3604553,(sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false) AS col35#3604554,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col36#3604555,(sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false) AS col37#3604556,(sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false) AS col38#3604557,(sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) AS col39#3604558,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col40#3604559,(sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) AS col41#3604560]                :                                   :           +- Filter ((fke1ef2a3d#3604321 = 涓) && ((2017-02-01 00:00:00 <= fkb804c35c#3604315) && (fkb804c35c#3604315 <= 2017-02-21 23:59:59)))                :                                   :              +- Subquery mock_tb_id_86a163a55407d3b9_aa610e9a5676044c133c92922491a0a9                :                                   :                 +- Project [fkb804c35c#3604315,fke1ef2a3d#3604321,fkc3802436#3604322,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357]                :                                   :                    +- Subquery 726cbefe72d042efacfe27eefed5a4ef_expendvfield_aa610e9a5676044c133c92922491a0a9                :                                   :                       +- Project [fkb804c35c#3604315,fke1ef2a3d#3604321,fkc3802436#3604322,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357]                :                                   :                          +- Subquery 726cbefe72d042efacfe27eefed5a4ef                :                                   :                             +- Relation[fkb804c35c#3604315,fk14b81600#3604316,fkc5848e45#3604317,fkc5fd3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] ParquetRelation: stan.726cbefe72d042efacfe27eefed5a4ef                :                                   +- Subquery tmp_tb_599dc55b4f21d9d6                :                                      +- Project [col0#3604732 AS col0#3604775,col1#3604733 AS col1#3604776,col2#3604734 AS col2#3604777,col3#3604735 AS col3#3604778,col4#3604736 AS col4#3604779,col5#3604737 AS col5#3604780,col6#3604738 AS col6#3604781,col7#3604739 AS col7#3604782,col8#3604740 AS col8#3604783,col9#3604741 AS col9#3604784,col10#3604742 AS col10#3604785,col11#3604743 AS col11#3604786,col12#3604744 AS col12#3604787,col13#3604745 AS col13#3604788,col14#3604746 AS col14#3604789,col15#3604747 AS col15#3604790,col16#3604748 AS col16#3604791,col17#3604749 AS col17#3604792,col18#3604750 AS col18#3604793,col19#3604751 AS col19#3604794,col20#3604752 AS col20#3604795,col21#3604753 AS col21#3604796,col22#3604754 AS col22#3604797,col23#3604755 AS col23#3604798,col24#3604756 AS col24#3604799,col25#3604757 AS col25#3604800,col26#3604758 AS col26#3604801,col27#3604759 AS col27#3604802,col28#3604760 AS col28#3604803,col29#3604761 AS col29#3604804,col30#3604762 AS col30#3604805,col31#3604763 AS col31#3604806,col32#3604764 AS col32#3604807,col33#3604765 AS col33#3604808,col34#3604766 AS col34#3604809,col35#3604767 AS col35#3604810,col36#3604768 AS col36#3604811,col37#3604769 AS col37#3604812,col38#3604770 AS col38#3604813,col39#3604771 AS col39#3604814,col40#3604772 AS col40#3604815,col41#3604773 AS col41#3604816,col42#3604774 AS col42#3604817]                :                                         +- Filter (col0#3604732 = 2017-02-21 00:00:00)                :                                            +- Subquery tmp_join_tb_88eeb94b7e70026a                :                                               +- Project [col0#3604646 AS col0#3604732,col1#3604647 AS col1#3604733,col2#3604648 AS col2#3604734,col3#3604649 AS col3#3604735,HiveSimpleUDF#site.stanzhai.udf.UDFNumCmp(col4#3604650,col4#3604693,1) AS col4#3604736,col5#3604651 AS col5#3604737,col6#3604652 AS col6#3604738,col7#3604653 AS col7#3604739,col8#3604654 AS col8#3604740,col9#3604655 AS col9#3604741,col10#3604656 AS col10#3604742,col11#3604657 AS col11#3604743,col12#3604658 AS col12#3604744,col13#3604659 AS col13#3604745,col14#3604660 AS col14#3604746,col15#3604661 AS col15#3604747,col16#3604662 AS col16#3604748,col17#3604663 AS col17#3604749,col18#3604664 AS col18#3604750,col19#3604665 AS col19#3604751,col20#3604666 AS col20#3604752,col21#3604667 AS col21#3604753,col22#3604668 AS col22#3604754,col23#3604669 AS col23#3604755,col24#3604670 AS col24#3604756,col25#3604671 AS col25#3604757,col26#3604672 AS col26#3604758,col27#3604673 AS col27#3604759,col28#3604674 AS col28#3604760,col29#3604675 AS col29#3604761,col30#3604676 AS col30#3604762,col31#3604677 AS col31#3604763,col32#3604678 AS col32#3604764,col33#3604679 AS col33#3604765,col34#3604680 AS col34#3604766,col35#3604681 AS col35#3604767,col36#3604682 AS col36#3604768,col37#3604683 AS col37#3604769,col38#3604684 AS col38#3604770,col39#3604685 AS col39#3604771,col40#3604686 AS col40#3604772,col41#3604687 AS col41#3604773,col42#3604688 AS col42#3604774]                :                                                  +- Join LeftOuter, Some((((if (isnull(col1#3604647))  else col1#3604647 = if (isnull(col1#3604690))  else col1#3604690) && (col0#3604646 = HiveSimpleUDF#site.stanzhai.udf.UDFGetNext(col0#3604689,day,day,1))) && (col0#3604646 = 2017-02-21 00:00:00)))                :                                                     :- Subquery tmp_tb_51d812b1654a0a98                :                                                     :  +- Project [col0#3604603 AS col0#3604646,col1#3604604 AS col1#3604647,col2#3604605 AS col2#3604648,col3#3604606 AS col3#3604649,col4#3604607 AS col4#3604650,col5#3604608 AS col5#3604651,col6#3604609 AS col6#3604652,col7#3604610 AS col7#3604653,col8#3604611 AS col8#3604654,col9#3604612 AS col9#3604655,col10#3604613 AS col10#3604656,col11#3604614 AS col11#3604657,col12#3604615 AS col12#3604658,col13#3604616 AS col13#3604659,col14#3604617 AS col14#3604660,col15#3604618 AS col15#3604661,col16#3604619 AS col16#3604662,col17#3604620 AS col17#3604663,col18#3604621 AS col18#3604664,col19#3604622 AS col19#3604665,col20#3604623 AS col20#3604666,col21#3604624 AS col21#3604667,col22#3604625 AS col22#3604668,col23#3604626 AS col23#3604669,col24#3604627 AS col24#3604670,col25#3604628 AS col25#3604671,col26#3604629 AS col26#3604672,col27#3604630 AS col27#3604673,col28#3604631 AS col28#3604674,col29#3604632 AS col29#3604675,col30#3604633 AS col30#3604676,col31#3604634 AS col31#3604677,col32#3604635 AS col32#3604678,col33#3604636 AS col33#3604679,col34#3604637 AS col34#3604680,col35#3604638 AS col35#3604681,col36#3604639 AS col36#3604682,col37#3604640 AS col37#3604683,col38#3604641 AS col38#3604684,col39#3604642 AS col39#3604685,col40#3604643 AS col40#3604686,col41#3604644 AS col41#3604687,col42#3604645 AS col42#3604688]                :                                                     :     +- Subquery 726cbefe72d042efacfe27eefed5a4ef_f54b458f5cbb803c_1                :                                                     :        +- Aggregate [fkc3802436#3604322,year(cast(fkb804c35c#3604315 as date)),month(cast(fkb804c35c#3604315 as date)),dayofmonth(cast(fkb804c35c#3604315 as date))], [if (((isnull(year(cast(fkb804c35c#3604315 as date))) || isnull(month(cast(fkb804c35c#3604315 as date)))) || isnull(dayofmonth(cast(fkb804c35c#3604315 as date))))) null else UDF(year(cast(fkb804c35c#3604315 as date)),month(cast(fkb804c35c#3604315 as date)),dayofmonth(cast(fkb804c35c#3604315 as date))) AS col0#3604603,fkc3802436#3604322 AS col1#3604604,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col2#3604605,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col3#3604606,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col4#3604607,(sum(cast(fk6938b0f1#3604330 as double)),mode=Complete,isDistinct=false) AS col5#3604608,(sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false) AS col6#3604609,((sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false)) AS col7#3604610,(sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false) AS col8#3604611,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col9#3604612,(sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) AS col10#3604613,(sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false) AS col11#3604614,(sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) AS col12#3604615,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false)) AS col13#3604616,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false)) AS col14#3604617,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false)) AS col15#3604618,((sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) / (sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false)) AS col16#3604619,((sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false)) AS col17#3604620,((sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false)) AS col18#3604621,((sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false)) AS col19#3604622,((sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk983801f0#3604356 as double)),mode=Complete,isDistinct=false)) AS col20#3604623,((sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) / (sum(cast(fk2afad0bc#3604349 as double)),mode=Complete,isDistinct=false)) AS col21#3604624,(sum(cast(fke9adbd11#3604357 as double)),mode=Complete,isDistinct=false) AS col22#3604625,(sum(cast(fka9b7b4ee#3604332 as double)),mode=Complete,isDistinct=false) AS col23#3604626,(sum(cast(fk9768a0c6#3604331 as double)),mode=Complete,isDistinct=false) AS col24#3604627,(sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) AS col25#3604628,(sum(cast(fk5b784c3d#3604350 as double)),mode=Complete,isDistinct=false) AS col26#3604629,(sum(cast(fk644dd14e#3604333 as double)),mode=Complete,isDistinct=false) AS col27#3604630,(sum(c"
Cheng Lian <lian@databricks.com>,"Wed, 22 Feb 2017 17:41:58 -0800",Re: The driver hangs at DataFrame.rdd in Spark 2.1.0,Stan Zhai <mail@zhaishidan.cn>,"Just from the thread dump you provided, it seems that this particular 
query plan jams our optimizer. However, it's also possible that the 
driver just happened to be running optimizer rules at that particular 
time point.

Since query planning doesn't touch any actual data, could you please try 
to minimize this query by replacing the actual relations with temporary 
views derived from Scala local collections? In this way, it would be 
much easier for others to reproduce issue.

Cheng











 



 
S 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
S 
3605337,col6#3605338,col7#3605339,col8#3605340,col9#3605341,col10#3605342,col11#3605343,col12#3605344,col13#3605345,col14#3605346,col15#3605347,col16#3605348,col17#3605349,col18#3605350,col19#3605351,col20#3605352,col21#3605353,col22#3605354,col23#3605355,col24#3605356,col25#3605357,col26#3605358,col27#3605359,col28#3605360,col29#3605361,col30#3605362,col31#3605363,col32#3605364,col33#3605365,col34#3605366,col35#3605367,col36#3605368,col37#3605369,col38#3605370,col39#3605371,col40#3605372,col41#3605373,col0#3605399,grouping__id#3605398] 
3605337,col6#3605338,col7#3605339,col8#3605340,col9#3605341,col10#3605342,col11#3605343,col12#3605344,col13#3605345,col14#3605346,col15#3605347,col16#3605348,col17#3605349,col18#3605350,col19#3605351,col20#3605352,col21#3605353,col22#3605354,col23#3605355,col24#3605356,col25#3605357,col26#3605358,col27#3605359,col28#3605360,col29#3605361,col30#3605362,col31#3605363,col32#3605364,col33#3605365,col34#3605366,col35#3605367,col36#3605368,col37#3605369,col38#3605370,col39#3605371,col40#3605372,col41#3605373,col0#3605332 


<= 
: 
04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
491a0a9 
04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 

4#3604693,1) 
04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
491a0a9 
04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 
ter 
04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
491a0a9 
04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 
3605033,col6#3605034,col7#3605035,col8#3605036,col9#3605037,col10#3605038,col11#3605039,col12#3605040,col13#3605041,col14#3605042,col15#3605043,col16#3605044,col17#3605045,col18#3605046,col19#3605047,col20#3605048,col21#3605049,col22#3605050,col23#3605051,col24#3605052,col25#3605053,col26#3605054,col27#3605055,col28#3605056,col29#3605057,col30#3605058,col31#3605059,col32#3605060,col33#3605061,col34#3605062,col35#3605063,col36#3605064,col37#3605065,col38#3605066,col39#3605067,col40#3605068,col41#3605069] 
3605034,col7#3605035,col8#3605036,col10#3605038,col11#3605039,col12#3605040,col13#3605041,col14#3605042,col15#3605043,col16#3605044,col17#3605045,col18#3605046,col19#3605047,col20#3605048,col22#3605050,col23#3605051,col24#3605052,col25#3605053,col26#3605054,col27#3605055,col28#3605056,col29#3605057,col30#3605058,col31#3605059,col32#3605060,col33#3605061,col34#3605062,col35#3605063,col36#3605064,col37#3605065,col38#3605066,col39#3605067,col40#3605068,col41#3605069,_w0#3605070,_w1#3605071,_w2#3605072,_w3#3605073,_w4#3605074,_w5#3605075,_we0#3605076,_we1#3605077,_we2#3605078,(_w0#3605070 
3605034,col7#3605035,col8#3605036,col10#3605038,col11#3605039,col12#3605040,col13#3605041,col14#3605042,col15#3605043,col16#3605044,col17#3605045,col18#3605046,col19#3605047,col20#3605048,col22#3605050,col23#3605051,col24#3605052,col25#3605053,col26#3605054,col27#3605055,col28#3605056,col29#3605057,col30#3605058,col31#3605059,col32#3605060,col33#3605061,col34#3605062,col35#3605063,col36#3605064,col37#3605065,col38#3605066,col39#3605067,col40#3605068,col41#3605069,_w0#3605070,_w1#3605071,_w2#3605072,_w3#3605073,_w4#3605074,_w5#3605075], 
m(_w1#3605071) 
enericUDAFSum(_w3#3605073) 
enericUDAFSum(_w5#3605075) 


04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
491a0a9 
04330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk2afad0bc#3604349,fk5b784c3d#3604350,fk983801f0#3604356,fke9adbd11#3604357] 
d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 
 





 



 
S 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
S 

10#3605342,col29#3605361,col23#3605355,col21#3605353,col8#3605340,col24#3605356,grouping__id#3605398,col3#3605335,col4#3605336,col27#3605359,col1#3605333,col5#3605337,col22#3605354,col2#3605334,col11#3605343] 
3605337,col6#3605338,col7#3605339,col8#3605340,col9#3605341,col10#3605342,col11#3605343,col12#3605344,col13#3605345,col14#3605346,col15#3605347,col16#3605348,col17#3605349,col18#3605350,col19#3605351,col20#3605352,col21#3605353,col22#3605354,col23#3605355,col24#3605356,col25#3605357,col26#3605358,col27#3605359,col28#3605360,col29#3605361,col30#3605362,col31#3605363,col32#3605364,col33#3605365,col34#3605366,col35#3605367,col36#3605368,col37#3605369,col38#3605370,col39#3605371,col40#3605372,col41#3605373,col0#3605399,grouping__id#3605398] 


04330,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
:00:00 <= 
: 
d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 
4#3604693,1) 
 
04330,fkb804c35c#3604315,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 
ect 

d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 
3605034,col7#3605035,col8#3605036,col10#3605038,col11#3605039,col12#3605040,col13#3605041,col14#3605042,col15#3605043,col16#3605044,col17#3605045,col18#3605046,col19#3605047,col20#3605048,col22#3605050,col23#3605051,col24#3605052,col25#3605053,col26#3605054,col27#3605055,col28#3605056,col29#3605057,col30#3605058,col31#3605059,col32#3605060,col33#3605061,col34#3605062,col35#3605063,col36#3605064,col37#3605065,col38#3605066,col39#3605067,col40#3605068,col41#3605069,_w0#3605070,_w1#3605071,_w2#3605072,_w3#3605073,_w4#3605074,_w5#3605075], 
m(_w1#3605071) 
enericUDAFSum(_w3#3605073) 
enericUDAFSum(_w5#3605075) 
 
04330,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
00 <= 

d3d59#3604318,fkb3b66e06#3604319,fk5345154e#3604320,fke1ef2a3d#3604321,fkc3802436#3604322,fkc1835f5e#3604323,fk3c97e290#3604324,fk0d9f524c#3604325,fk94739939#3604326,fk6f06b7e6#3604327,fk2f4bc742#3604328,fka894a817#3604329,fk6938b0f1#3604330,fk9768a0c6#3604331,fka9b7b4ee#3604332,fk644dd14e#3604333,fk14156ab8#3604334,fk0cff3ee7#3604335,fkbeb2eaff#3604336,fk65d2e57c#3604337,fk2057de12#3604338,fk6b48d715#3604339,fkfc93a68e#3604340,fk0444374a#3604341,fk87c5db7c#3604342,fkae319449#3604343,fk571f3eb5#3604344,fk4ca40a2b#3604345,fkc3ae8b08#3604346,fk51d422ab#3604347,fka82a7d62#3604348,fk2afad0bc#3604349,fk5b784c3d#3604350,fkca3a89a4#3604351,fk720a720c#3604352,fk8582dbe5#3604353,fk786d6e3c#3604354,fk3c675be7#3604355,fk983801f0#3604356,fke9adbd11#3604357,fk19d5a76b#3604358,fk0d0f580e#3604359,fk3a8ccb91#3604360,fk8fa0d5e8#3604361,fkd31e4724#3604362,fk874ca4e5#3604363,fk809384be#3604364,fkb9df99f7#3604365,fk2b120bee#3604366,fkcc1e115b#3604367,fk355fc528#3604368,fk2d0e2f85#3604369,fkc8054eb7#3604370,fked1cbfa7#3604371,fk65026b16#3604372,fk1874f932#3604373,fkd382f64d#3604374,fk0e328bae#3604375,fk44e818fd#3604376,fk554714b9#3604377,fka2c91a0e#3604378,fk2b120c52#3604379,fk32c3890a#3604380,fka54b4e53#3604381,fk5bd5ea3c#3604382] 

, 
ol2#3605334),mode=Final,isDistinct=false),(avg(col3#3605335),mode=Final,isDistinct=false),(sum(col4#3605336),mode=Final,isDistinct=false),(sum(col5#3605337),mode=Final,isDistinct=false),(sum(col22#3605354),mode=Final,isDistinct=false),(sum(col23#3605355),mode=Final,isDistinct=false),(sum(col7#3605339),mode=Final,isDistinct=false),(sum(col8#3605340),mode=Final,isDistinct=false),(sum(col9#3605341),mode=Final,isDistinct=false),(sum(col10#3605342),mode=Final,isDistinct=false),(sum(col11#3605343),mode=Final,isDistinct=false),(sum(col24#3605356),mode=Final,isDistinct=false),(sum(col25#3605357),mode=Final,isDistinct=false),(sum(col27#3605359),mode=Final,isDistinct=false),(sum(col29#3605361),mode=Final,isDistinct=false),(sum(col31#3605363),mode=Final,isDistinct=false),(sum(col21#3605353),mode=Final,isDistinct=false)], 
378,col5#3605379,col6#3605380,col7#3605381,col8#3605382,col9#3605383,col10#3605384,col11#3605385,col12#3605386,col13#3605387,col14#3605388,col15#3605389,col16#3605390,col17#3605391,col18#3605392,col19#3605393,col20#3605394,col21#3605395,GROUPING__ID#3605396]) 
(col2#3605334),mode=Partial,isDistinct=false),(avg(col3#3605335),mode=Partial,isDistinct=false),(sum(col4#3605336),mode=Partial,isDistinct=false),(sum(col5#3605337),mode=Partial,isDistinct=false),(sum(col22#3605354),mode=Partial,isDistinct=false),(sum(col23#3605355),mode=Partial,isDistinct=false),(sum(col7#3605339),mode=Partial,isDistinct=false),(sum(col8#3605340),mode=Partial,isDistinct=false),(sum(col9#3605341),mode=Partial,isDistinct=false),(sum(col10#3605342),mode=Partial,isDistinct=false),(sum(col11#3605343),mode=Partial,isDistinct=false),(sum(col24#3605356),mode=Partial,isDistinct=false),(sum(col25#3605357),mode=Partial,isDistinct=false),(sum(col27#3605359),mode=Partial,isDistinct=false),(sum(col29#3605361),mode=Partial,isDistinct=false),(sum(col31#3605363),mode=Partial,isDistinct=false),(sum(col21#3605353),mode=Partial,isDistinct=false)], 
#3605470,count#3605471L,sum#3605472,sum#3605473,sum#3605474,sum#3605475,sum#3605476,sum#3605477,sum#3605478,sum#3605479,sum#3605480,sum#3605481,sum#3605482,sum#3605483,sum#3605484,sum#3605485,sum#3605486]) 
10#3605342,col29#3605361,col23#3605355,col21#3605353,col8#3605340,col24#3605356,grouping__id#3605398,col3#3605335,col4#3605336,col27#3605359,col1#3605333,col5#3605337,col22#3605354,col2#3605334,col11#3605343] 
3605337,col6#3605338,col7#3605339,col8#3605340,col9#3605341,col10#3605342,col11#3605343,col12#3605344,col13#3605345,col14#3605346,col15#3605347,col16#3605348,col17#3605349,col18#3605350,col19#3605351,col20#3605352,col21#3605353,col22#3605354,col23#3605355,col24#3605356,col25#3605357,col26#3605358,col27#3605359,col28#3605360,col29#3605361,col30#3605362,col31#3605363,col32#3605364,col33#3605365,col34#3605366,col35#3605367,col36#3605368,col37#3605369,col38#3605370,col39#3605371,col40#3605372,col41#3605373,col0#3605399,grouping__id#3605398] 

as 
as 
as 
as 
as 
as 
as 
523,col5#3604524,col6#3604525,col7#3604526,col8#3604527,col9#3604528,col10#3604529,col11#3604530,col12#3604531,col13#3604532,col14#3604533,col15#3604534,col16#3604535,col17#3604536,col18#3604537,col19#3604538,col20#3604539,col21#3604540,col22#3604541,col23#3604542,col24#3604543,col25#3604544,col26#3604545,col27#3604546,col28#3604547,col29#3604548,col30#3604549,col31#3604550,col32#3604551,col33#3604552,col34#3604553,col35#3604554,col36#3604555,col37#3604556,col38#3604557,col39#3604558,col40#3604559,col41#3604560]) 

0 
1 
0 
9 
6 
7 
3 
05270,sum#3605271,sum#3605272,sum#3605273,sum#3605274]) 
04330,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
:00:00 <= 
: 
4356,fke9adbd11#3604357,fk6938b0f1#3604330,fkb804c35c#3604315,fke1ef2a3d#3604321,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
d5a4ef, 
4#3604693,1) 
y,day,1)], 
5 
as 
as 
as 
as 
as 
as 
as 
607,col5#3604608,col6#3604609,col7#3604610,col8#3604611,col9#3604612,col10#3604613,col11#3604614,col12#3604615,col13#3604616,col14#3604617,col15#3604618,col16#3604619,col17#3604620,col18#3604621,col19#3604622,col20#3604623,col21#3604624,col22#3604625,col23#3604626,col24#3604627,col25#3604628,col26#3604629,col27#3604630,col28#3604631,col29#3604632,col30#3604633,col31#3604634,col32#3604635,col33#3604636,col34#3604637,col35#3604638,col36#3604639,col37#3604640,col38#3604641,col39#3604642,col40#3604643,col41#3604644,col42#3604645]) 
5 
0 
1 
0 
9 
6 
7 
3 
298,sum#3605299,sum#3605300,sum#3605301]) 
04330,fkb804c35c#3604315,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
Relation: 
4356,fke9adbd11#3604357,fk6938b0f1#3604330,fkb804c35c#3604315,fke1ef2a3d#3604321,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
d5a4ef, 
 00:00:00 

y,day,1),114), 
5 
e 
5 

4315,fka9b7b4ee#3604332,fke1ef2a3d#3604321] 
d5a4ef, 
3605034,col7#3605035,col8#3605036,col10#3605038,col11#3605039,col12#3605040,col13#3605041,col14#3605042,col15#3605043,col16#3605044,col17#3605045,col18#3605046,col19#3605047,col20#3605048,col22#3605050,col23#3605051,col24#3605052,col25#3605053,col26#3605054,col27#3605055,col28#3605056,col29#3605057,col30#3605058,col31#3605059,col32#3605060,col33#3605061,col34#3605062,col35#3605063,col36#3605064,col37#3605065,col38#3605066,col39#3605067,col40#3605068,col41#3605069,_w0#3605070,_w1#3605071,_w2#3605072,_w3#3605073,_w4#3605074,_w5#3605075], 
m(_w1#3605071) 
enericUDAFSum(_w3#3605073) 
enericUDAFSum(_w5#3605075) 
as 
as 
as 
as 
as 
as 
as 
033,col6#3605034,col7#3605035,col8#3605036,col10#3605038,col11#3605039,col12#3605040,col13#3605041,col14#3605042,col15#3605043,col16#3605044,col17#3605045,col18#3605046,col19#3605047,col20#3605048,col22#3605050,col23#3605051,col24#3605052,col25#3605053,col26#3605054,col27#3605055,col28#3605056,col29#3605057,col30#3605058,col31#3605059,col32#3605060,col33#3605061,col34#3605062,col35#3605063,col36#3605064,col37#3605065,col38#3605066,col39#3605067,col40#3605068,col41#3605069,_w0#3605070,_w1#3605071,_w2#3605072,_w3#3605073,_w4#3605074,_w5#3605075]) 
0 
1 
0 
9 
6 
7 
3 
05327,sum#3605328,sum#3605329,sum#3605330,sum#3605331]) 
04330,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
00 <= 

4356,fke9adbd11#3604357,fk6938b0f1#3604330,fkb804c35c#3604315,fke1ef2a3d#3604321,fk9768a0c6#3604331,fk2afad0bc#3604349,fkc3802436#3604322,fk644dd14e#3604333,fka9b7b4ee#3604332] 
d5a4ef, 


"
StanZhai <mail@zhaishidan.cn>,"Thu, 23 Feb 2017 00:11:04 -0700 (MST)",Re:  The driver hangs at DataFrame.rdd in Spark 2.1.0,dev@spark.apache.org,"Could this be related to https://issues.apache.org/jira/browse/SPARK-17733 ?




------------------ Original ------------------
From:  ""Cheng Lian-3 [via Apache Spark Developers List]"";<ml-node+s1001551n21053h3@n3.nabble.com>;
Send time: Thursday, Feb 23, 2017 9:43 AM
To: ""Stan Zhai""<mail@zhaishidan.cn>; 

Subject:  Re: The driver hangs at DataFrame.rdd in Spark 2.1.0



 	                   
Just from the thread dump you provided, it seems that this       particular query plan jams our optimizer. However, it's also       possible that the driver just happened to be running optimizer       rules at that particular time point.
     
     
Since query planning doesn't touch any actual data, could you       please try to minimize this query by replacing the actual       relations with temporary views derived from Scala local       collections? In this way, it would be much easier for others to       reproduce issue.
     
Cheng
     
     
     
            Thanks for lian's reply.
       
       
       Here is the QueryPlan generated by Spark 1.6.2(I can't get it         in Spark 2.1.0):
                ...       
        
                
         
         ------------------ Original ------------------
                    Subject:  Re: The driver hangs at DataFrame.rdd             in Spark 2.1.0
         
         
         
         
What is the query plan? We had once observed query plans that           grow exponentially in iterative ML workloads and the query           planner hangs forever. For example, each iteration combines 4           plan trees of the last iteration and forms a larger plan tree.           The size of the plan tree can easily reach billions of nodes           after 15 iterations.
         
         
         
                    Hi all,
           
           
           The driver hangs at DataFrame.rdd in Spark 2.1.0 when the             DataFrame(SQL) is complex, Following thread dump of my             driver:
           ...
                  
       
          
    	 	 	 	
 	
 	
 	 		If you reply to this email, your message will be added to the discussion below:
 		http://apache-spark-developers-list.1001551.n3.nabble.com/Re-The-driver-hangs-at-DataFrame-rdd-in-Spark-2-1-0-tp21052p21053.html 	
 	 		To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h91@n3.nabble.com 
 		NAML



--"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 23 Feb 2017 08:56:41 +0000",Re: [Spark Namespace]: Expanding Spark ML under Different Namespace?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Currently your only option is to write (or copy) your own implementations.

Logging is definitely intended to be internal use only, and it's best to
use your own logging lib - Typesafe scalalogging is a common option that
I've used.

As for the VectorUDT, for now that is private. There are no plans to open
it up as yet. It should not be too difficult to have your own UDT
implementation. What type of extensions are you trying to do with the UDT?

Likewise the shared params are for now private. It is a bit annoying to
have to re-create them, but most of them are pretty simple so it's not a
huge overhead.

Perhaps you can add your thoughts & comments to
https://issues.apache.org/jira/browse/SPARK-19498 in terms of extending
Spark ML. Ultimately I support making it easier to extend. But we do have
to balance that with exposing new public APIs and classes that impose
backward compat guarantees.

Perhaps now is a good time to think about some of the common shared params
for example.

Thanks
Nick



Hi Spark developers,



Currently my team at Microsoft is extending Sparks machine learning
functionalities to include new learners and transformers. We would like
users to use these within spark pipelines so that they can mix and match
with existing Spark learners/transformers, and overall have a native spark
experience. We cannot accomplish this using a non-org.apache namespace
with the current implementation, and we dont want to release code inside
the apache namespace because its confusing and there could be naming
rights issues.



We need to extend several classes from spark which happen to have
private[spark]. For example, one of our class extends VectorUDT[0] which
has private[spark] class VectorUDT as its access modifier. This
unfortunately put us in a strange scenario that forces us to work under the
namespace org.apache.spark.



To be specific, currently the private classes/traits we need to use to
create new Spark learners & Transformers are HasInputCol, VectorUDT and
Logging. We will expand this list as we develop more.



Is there a way to avoid this namespace issue? What do other
people/companies do in this scenario? Thank you for your help!



[0]:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/linalg/VectorUDT.scala



Best,

Shouheng
"
"""=?gb18030?B?wtyyt8u/s7S3uQ==?="" <1427357147@qq.com>","Thu, 23 Feb 2017 17:07:33 +0800",Filestream can not recognize the copied file,"""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","Hi all,
I tested filestream today, my code looks like:

val  fs = ssc.textFileStream(*)
erroelines = fs.filter( _.contains(""erroe""))
erroelines.print
ssc.start()


when I edit a file and save it to the dir, it works well.
If i copy a file to the dir, it does work.

my issues are:
1, is it OK please? I means whether the code was designed to work as this.
2,which class  monitor the dir pls?"
Donam Kim <sstmih@gmail.com>,"Thu, 23 Feb 2017 20:26:41 +1100",unsubscribe,dev@spark.incubator.apache.org,"unsubscribe
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 23 Feb 2017 10:36:06 +0000","Re: [Spark Namespace]: Expanding Spark ML under Different
 Namespace?",Shouheng Yi <shouyi@microsoft.com.INVALID>,"
On 22 Feb 2017, at 20:51, Shouheng Yi <shouyi@microsoft.com.INVALID<mailto:shouyi@microsoft.com.INVALID>> wrote:

Hi Spark developers,

Currently my team at Microsoft is extending Sparks machine learning functionalities to include new learners and transformers. We would like users to use these within spark pipelines so that they can mix and match with existing Spark learners/transformers, and overall have a native spark experience. We cannot accomplish this using a non-org.apache namespace with the current implementation, and we dont want to release code inside the apache namespace because its confusing and there could be naming rights issues.

This isn't actually the ASF has a strong stance against, more left to projects themselves. After all: the source is licensed by the ASF, and the license doesn't say you can't.

Indeed, there's a bit of org.apache.hive in the Spark codebase where the hive team kept stuff package private. Though that's really a sign that things could be improved there.

Where is problematic is that stack traces end up blaming the wrong group; nobody likes getting a bug report which doesn't actually exist in your codebase., not least because you have to waste time to even work it out.

You also have to expect absolutely no stability guarantees, so you'd better set your nightly build to work against trunk

Apache Bahir does put some stuff into org.apache.spark.stream, but they've sort of inherited that right.when they picked up the code from spark. new stuff is going into org.apache.bahir


We need to extend several classes from spark which happen to have private[spark]. For example, one of our class extends VectorUDT[0] which has private[spark] class VectorUDT as its access modifier. This unfortunately put us in a strange scenario that forces us to work under the namespace org.apache.spark.

To be specific, currently the private classes/traits we need to use to create new Spark learners & Transformers are HasInputCol, VectorUDT and Logging. We will expand this list as we develop more.

I do think tis a shame that logging went from public to private.

One thing that could be done there is to copy the logging into Bahir, under an org.apache.bahir package, for yourself and others to use. That's be beneficial to me too.

For the ML stuff, that might be place to work too, if you are going to open source the code.



Is there a way to avoid this namespace issue? What do other people/companies do in this scenario? Thank you for your help!

I've hit this problem in the past.  Scala code tends to force your hand here precisely because of that (very nice) private feature. While it offers the ability of a project to guarantee that implementation details aren't picked up where they weren't intended to be, in OSS dev, all that implementation is visible and for lower level integration,

What I tend to do is keep my own code in its package and try to do as think a bridge over to it from the [private] scope. It's also important to name things obviously, say,  org.apache.spark.microsoft , so stack traces in bug reports can be dealt with more easily


[0]: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/linalg/VectorUDT.scala

Best,
Shouheng

"
"""=?gb18030?B?wtyyt8u/s7S3uQ==?="" <1427357147@qq.com>","Thu, 23 Feb 2017 20:02:03 +0800",Re: Filestream can not recognize the copied file,"""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","hi all,

I checked the code just now.
Spark scan the dir and filter files by timestamp of file.
I copied files to the dir but not changed the timestamp.
I know the reason now.

thanks 

---Original---
From: ""萝卜丝炒饭""<1427357147@qq.com>
Date: 2017/2/23 17:07:33
To: ""dev""<dev@spark.apache.org>;
Subject: Filestream can not recognize the copied file


Hi all,
I tested filestream today, my code looks like:

val  fs = ssc.textFileStream(*)
erroelines = fs.filter( _.contains(""erroe""))
erroelines.print
ssc.start()


when I edit a file and save it to the dir, it works well.
If i copy a file to the dir, it does work.

my issues are:
1, is it OK please? I means whether the code was designed to work as this.
2,which class  monitor the dir pls?"
n1kt0 <nikita.balyschew@googlemail.com>,"Thu, 23 Feb 2017 05:23:01 -0700 (MST)",Re: Implementation of RNN/LSTM in Spark,dev@spark.apache.org,"Hi,
can anyone tell me what the current status about RNNs in Spark is?



--

---------------------------------------------------------------------


"
Sam Elamin <hussam.elamin@gmail.com>,"Thu, 23 Feb 2017 12:35:15 +0000",Re: Support for decimal separator (comma or period) in spark 2.1,"Arkadiusz Bicz <arkadiusz.bicz@gmail.com>, dev@spark.apache.org","Hi Arkadiuz

Not sure if there is a localisation ability but I'm sure other will correct
me if I'm wrong


What you could do is write a udf function that replaces the commas with a .

Assuming you know the column in question


Regards
Sam

"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 23 Feb 2017 12:39:33 +0000",Re: Implementation of RNN/LSTM in Spark,dev@spark.apache.org,"The short answer is there is none and highly unlikely to be inside of Spark
MLlib any time in the near future.

The best bets are to look at other DL libraries - for JVM there is
Deeplearning4J and BigDL (there are others but these seem to be the most
comprehensive I have come across) - that run on Spark. Also there are
various flavours of TensorFlow / Caffe on Spark. And of course the libs
such as Torch, Keras, Tensorflow, MXNet, Caffe etc. Some of them have Java
or Scala APIs and some form of Spark integration out there in the community
(in varying states of development).

Integrations with Spark are a bit patchy currently but include the
others).


"
Joeri Hermans <joeri.raymond.e.hermans@cern.ch>,"Thu, 23 Feb 2017 12:51:23 +0000",RE: Implementation of RNN/LSTM in Spark,"Nick Pentreath <nick.pentreath@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Nikita,

We are actively working on this: https://github.com/cerndb/dist-keras This will allow you to run Keras on Spark (with distributed optimization algorithms) through pyspark. I recommend you to check the examples https://github.com/cerndb/dist-keras/tree/master/examples. However, you need to be aware that distributed optimization is a research topic, and has several approaches and caveats you need to be aware of. I wrote a blog post on this if you like to have some additional information on this topic https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras

However, if you don't want to use a distributed optimization algorithm, we also support a ""sequential trainer"" which allows you to train a model on Spark dataframes.

Kind regards,

Joeri
________________________________________.
From: Nick Pentreath [nick.pentreath@gmail.com]
Sent: 23 February 2017 13:39
To: dev@spark.apache.org
Subject: Re: Implementation of RNN/LSTM in Spark

The short answer is there is none and highly unlikely to be inside of Spark MLlib any time in the near future.

The best bets are to look at other DL libraries - for JVM there is Deeplearning4J and BigDL (there are others but these seem to be the most comprehensive I have come across) - that run on Spark. Also there are various flavours of TensorFlow / Caffe on Spark. And of course the libs such as Torch, Keras, Tensorflow, MXNet, Caffe etc. Some of them have Java or Scala APIs and some form of Spark integration out there in the community (in varying states of development).

k"" flavours mentioned above and TensorFrames (again, there may be others).

Hi,
can anyone tell me what the current status about RNNs in Spark is?



--
3.nabble.com/Implementation-of-RNN-LSTM-in-Spark-tp14866p21060.html
om.

---------------------------------------------------------------------
ibe@spark.apache.org>


---------------------------------------------------------------------


"
Arkadiusz Bicz <arkadiusz.bicz@gmail.com>,"Thu, 23 Feb 2017 13:53:43 +0100",Re: Support for decimal separator (comma or period) in spark 2.1,Sam Elamin <hussam.elamin@gmail.com>,"Thank you Sam for answer, I have solved problem by loading all decimals
columns as string and replacing all commas with dots but this solution is
lacking of automatic infer schema which is quite nice functionality.

I can work on adding new option to DataFrameReader  for localization like:

spark.read.option(""NumberLocale"", ""German"").csv(""filefromeurope.csv"")

Just wonder if it will be accepted?

Best Regards,

Arkadiusz Bicz


"
=?UTF-8?B?4KSo4KS/4KSk4KWH4KS2?= <nskentc@gmail.com>,"Thu, 23 Feb 2017 18:22:39 +0530",unsubscribe,dev@spark.incubator.apache.org,"unsubscribe
"
Hyukjin Kwon <gurwls223@gmail.com>,"Thu, 23 Feb 2017 21:56:04 +0900",Re: Support for decimal separator (comma or period) in spark 2.1,Arkadiusz Bicz <arkadiusz.bicz@gmail.com>,"Please take a look at https://issues.apache.org/jira/browse/SPARK-18359.

2017-02-23 21:53 GMT+09:00 Arkadiusz Bicz <arkadiusz.bicz@gmail.com>:

"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 23 Feb 2017 13:32:02 +0000",Re: Feedback on MLlib roadmap process proposal,"""dev@spark.apache.org"" <dev@spark.apache.org>","Sorry for being late to the discussion. I think Joseph, Sean and others
have covered the issues well.

Overall I like the proposed cleaned up roadmap & process (thanks Joseph!).
As for the actual critical roadmap items mentioned on SPARK-18813, I think
it makes sense and will comment a bit further on that JIRA.

I would like to encourage votes & watching for issues to give a sense of
what the community wants (I guess Vote is more explicit yet passive, while
actually Watching an issue is more informative as it may indicate a real
use case dependent on the issue?!).

I think if used well this is valuable information for contributors. Of
course not everything on that list can get done. But if I look through the
top votes or watch list, while not all of those are likely to go in, a
great many of the issues are fairly non-contentious in terms of being good
additions to the project.

Things like these are good examples IMO (I just sample a few of them, not
exhaustive):
- sample weights for RF / DT
- multi-model and/or parallel model selection
- make sharedParams public?
- multi-column support for various transformers
- incremental model training
- tree algorithm enhancements

Now, whether these can be prioritised in terms of bandwidth available to
reviewers and committers is a totally different thing. But as Sean mentions
there is some process there for trying to find the balance of the issue
being a ""good thing to add"", a shepherd with bandwidth & interest in the
issue to review, and the maintenance burden imposed.

Let's take Deep Learning / NN for example. Here's a good example of
something that has a lot of votes/watchers and as Sean mentions it is
something that ""everyone wants someone else to implement"". In this case,
much of the interest may in fact be ""stale"" - 2 years ago it would have
been very interesting to have a strong DL impl in Spark. Now, because there
are a plethora of very good DL libraries out there, how many of those Votes
would be ""deleted""? Granted few are well integrated with Spark but that can
and is changing (DL4J, BigDL, the ""XonSpark"" flavours etc).

So this is something that I dare say will not be in Spark any time in the
foreseeable future or perhaps ever given the current status. Perhaps it's
worth seriously thinking about just closing these kind of issues?




ct
t
e
IP
s.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520votes%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=%2FtFB0LY%2BIxLoEf%2FPr1i1%2FgvrjlpXPuYLSLbpnd89Tkg%3D&reserved=0>or
s.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520Watchers%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=XkPfFiB2T%2FoVnJcdr3jf12dQjes7w%2BVJMrbhgx3ELRs%3D&reserved=0>
g
t
t
d
ne
 but where the
s
d
t
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 23 Feb 2017 10:28:15 -0800",Re: The driver hangs at DataFrame.rdd in Spark 2.1.0,"StanZhai <mail@zhaishidan.cn>, dev@spark.apache.org","This one seems to be relevant, but it's already fixed in 2.1.0.

analyzer/optimizer behaves.



"
Tim Hunter <timhunter@databricks.com>,"Thu, 23 Feb 2017 11:38:50 -0800",Re: Feedback on MLlib roadmap process proposal,Nick Pentreath <nick.pentreath@gmail.com>,"As Sean wrote very nicely above, the changes made to Spark are decided in
an organic fashion based on the interests and motivations of the committers
and contributors. The case of deep learning is a good example. There is a
lot of interest, and the core algorithms could be implemented without too
much problem in a few thousands of lines of scala code. However, the
performance of such a simple implementation would be one to two order of
magnitude slower than what would get from the popular frameworks out there.

At this point, there are probably more man-hours invested in TensorFlow (as
an example) than in MLlib, so I think we need to be realistic about what we
can expect to achieve inside Spark. Unlike BLAS for linear algebra, there
flavors explores a slightly different design. It will be interesting to see
what works well in practice. In the meantime, though, there are plenty of
things that we could do to help developers of other libraries to have a
great experience with Spark. Matei alluded to that in his Spark Summit
keynote when he mentioned better integration with low-level libraries.

Tim



.
k
e
e
d
ns
re
es
an
:
ect
es
SIP
:
es.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520votes%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=%2FtFB0LY%2BIxLoEf%2FPr1i1%2FgvrjlpXPuYLSLbpnd89Tkg%3D&reserved=0>or
es.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520Watchers%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=XkPfFiB2T%2FoVnJcdr3jf12dQjes7w%2BVJMrbhgx3ELRs%3D&reserved=0>
g
e
y
nd
one
 but where the
ss
ed
't
"
Donam Kim <sstmih@gmail.com>,"Fri, 24 Feb 2017 08:54:07 +1100",unsubscribe,dev@spark.apache.org,"unsubscribe
"
Joseph Bradley <joseph@databricks.com>,"Thu, 23 Feb 2017 15:42:35 -0800",Re: [Spark Namespace]: Expanding Spark ML under Different Namespace?,Steve Loughran <stevel@hortonworks.com>,"+1 for Nick's comment about discussing APIs which need to be made public in
https://issues.apache.org/jira/browse/SPARK-19498 !


ing
k
 namespace
e inside
ming
e
e
ectorUDT[0] which
he
rs
o
/


-- 

Joseph Bradley

Software Engineer - Machine Learning
"
StanZhai <mail@zhaishidan.cn>,"Thu, 23 Feb 2017 20:41:15 -0700 (MST)",Re:  The driver hangs at DataFrame.rdd in Spark 2.1.0,dev@spark.apache.org,"Thanks for Cheng's help.


It must be something wrong with InferFiltersFromConstraints, I just removed InferFiltersFromConstraints from org/apache/spark/sql/catalyst/optimizer/Optimizer.scala to avoid this issue. I will analysis this issue with the method your provided.




------------------ Original ------------------
From:  ""Cheng Lian [via Apache Spark Developers List]"";<ml-node+s1001551n21069h99@n3.nabble.com>;
Send time: Friday, Feb 24, 2017 2:28 AM
To: ""Stan Zhai""<mail@zhaishidan.cn>; 

Subject:  Re: The driver hangs at DataFrame.rdd in Spark 2.1.0



 	                   
This one seems to be relevant, but it's already fixed in 2.1.0.
     
     
     
     
            Could this be related to https://issues.apache.org/jira/browse/SPARK-17733 ?
                
         
         
         
         ------------------ Original ------------------
                    From:  ""Cheng Lian-3 [via Apache Spark Developers             List]"";<[hidden               email]>;
           Send time: Thursday, Feb 23, 2017 9:43 AM
           To: ""Stan Zhai""<[hidden               email]>; 
           Subject:  Re: The driver hangs at DataFrame.rdd             in Spark 2.1.0
         
         
         
         
Just from the thread dump you provided, it seems that this           particular query plan jams our optimizer. However, it's also           possible that the driver just happened to be running optimizer           rules at that particular time point.
         
         
Since query planning doesn't touch any actual data, could you           please try to minimize this query by replacing the actual           relations with temporary views derived from Scala local           collections? In this way, it would be much easier for others           to reproduce issue.
         
Cheng
         
         
         
                    Thanks for lian's reply.
           
           
           Here is the QueryPlan generated by Spark 1.6.2(I can't             get it in Spark 2.1.0):
                        ...           
                        
                        
             
             ------------------ Original ------------------
                            Subject:  Re: The driver hangs at                 DataFrame.rdd in Spark 2.1.0
             
             
             
             
What is the query plan? We had once observed query plans               that grow exponentially in iterative ML workloads and the               query planner hangs forever. For example, each iteration               combines 4 plan trees of the last iteration and forms a               larger plan tree. The size of the plan tree can easily               reach billions of nodes after 15 iterations.
             
             
             
                            Hi all,
               
               
               The driver hangs at DataFrame.rdd in Spark 2.1.0 when                 the DataFrame(SQL) is complex, Following thread dump of                 my driver:
               ...
                          
           
                  
         
         
         
                    If you reply to this email, your             message will be added to the discussion below:
           http://apache-spark-developers-list.1001551.n3.nabble.com/Re-The-driver-hangs-at-DataFrame-rdd-in-Spark-2-1-0-tp21052p21053.html         
                    To start a new topic under Apache Spark Developers List, email           [hidden email]           
           NAML 
       
       
       
       View this message in context: Re:         The driver hangs at DataFrame.rdd in Spark 2.1.0
       Sent from the Apache         Spark Developers List mailing list archive at Nabble.com.
          
    	 	 	 	
 	
 	
 	 		If you reply to this email, your message will be added to the discussion below:
 		http://apache-spark-developers-list.1001551.n3.nabble.com/Re-The-driver-hangs-at-DataFrame-rdd-in-Spark-2-1-0-tp21052p21069.html 	
 	 		To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h91@n3.nabble.com 
 		NAML



--"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 24 Feb 2017 08:28:16 +0000",Re: Feedback on MLlib roadmap process proposal,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI I've started going through a few of the top Watched JIRAs and tried to
identify those that are obviously stale and can probably be closed, to try
to clean things up a bit.


rs
e.
a,
rk
ee
.
k
e
e
d
ns
re
es
an
:
ct
t
e
IP
s.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520votes%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=%2FtFB0LY%2BIxLoEf%2FPr1i1%2FgvrjlpXPuYLSLbpnd89Tkg%3D&reserved=0>or
s.apache.org%2Fjira%2Fissues%2F%3Fjql%3Dproject%2520%253D%2520SPARK%2520AND%2520status%2520in%2520(Open%252C%2520%2522In%2520Progress%2522%252C%2520Reopened)%2520AND%2520component%2520in%2520(ML%252C%2520MLlib)%2520ORDER%2520BY%2520Watchers%2520DESC&data=02%7C01%7Cilmat%40microsoft.com%7C180d196083534d9eee6b08d444754fae%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636208718015178106&sdata=XkPfFiB2T%2FoVnJcdr3jf12dQjes7w%2BVJMrbhgx3ELRs%3D&reserved=0>
g
t
t
d
ne
 but where the
s
d
t
"
Liwei Lin <lwlin7@gmail.com>,"Fri, 24 Feb 2017 20:37:45 +0800",Nightly builds for master branch have been failing,dev <dev@spark.apache.org>,"Nightly builds for master branch have been failing:
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-maven-snapshots/buildTimeTrend

It'd be great if we can get this fixed. Thanks.



Cheers,
Liwei
"
Cody Koeninger <cody@koeninger.org>,"Fri, 24 Feb 2017 07:35:06 -0600",Re: Spark Improvement Proposals,vaquar khan <vaquar.khan@gmail.com>,"It's been a week since any further discussion.

Do PMC members think the current draft is OK to vote on?


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Feb 2017 15:26:26 +0000",Re: Nightly builds for master branch have been failing,"Liwei Lin <lwlin7@gmail.com>, dev <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>","This job is using Java 7 still. Josh are you the person to ask? I think it
needs to set JAVA_HOME to Java 8.


"
Tim Hunter <timhunter@databricks.com>,"Fri, 24 Feb 2017 09:08:27 -0800",Re: [Spark Namespace]: Expanding Spark ML under Different Namespace?,Joseph Bradley <joseph@databricks.com>,"Regarding logging, Graphframes makes a simple wrapper this way:

https://github.com/graphframes/graphframes/blob/master/src/main/scala/org/
graphframes/Logging.scala

Regarding the UDTs, they have been hidden to be reworked for Datasets, the
reasons being detailed here [1]. Can you describe your use case in more
details? You may be better off copy/pasting the UDT code outside of Spark,
depending on your use case.

[1] https://issues.apache.org/jira/browse/SPARK-14155


ning
rk
 namespace
de inside
aming
he
;
VectorUDT[0] which
the
s
ers
to
s
"
Josh Rosen <joshrosen@databricks.com>,"Fri, 24 Feb 2017 19:18:57 +0000",Re: Nightly builds for master branch have been failing,"Sean Owen <sowen@cloudera.com>, Liwei Lin <lwlin7@gmail.com>, dev <dev@spark.apache.org>","I spotted the problem and it appears to be a misconfiguration / missing
entry in the template which generates the packaging jobs. I've corrected
the problem but now the jobs appear to be hanging / flaking on the Git
clone. Hopefully this is just a transient issue, so let's retry tonight and
see whether things are fixed.


"
Joseph Bradley <joseph@databricks.com>,"Fri, 24 Feb 2017 20:28:56 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"The current draft LGTM.  I agree some of the various concerns may need to
be addressed in the future, depending on how SPIPs progress in practice.
If others agree, let's put it to a vote and revisit the proposal in a few
months.
Joseph




-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Sean Owen <sowen@cloudera.com>,"Sat, 25 Feb 2017 19:10:20 +0000",Re: Straw poll: dropping support for things like Scala 2.10,dev <dev@spark.apache.org>,"I want to bring up the issue of Scala 2.10 support again, to see how people
feel about it. Key opinions from the previous responses, I think:

Cody: only drop 2.10 support when 2.12 support is added
Koert: we need all dependencies to support 2.12; Scala updates are pretty
transparent to IT/ops
Ofir: make sure to deprecate 2.10 in Spark 2.1
Reynold: lets maybe remove support for Scala 2.10 and Java 7 in Spark 2.2
Matei: lets not remove things unless theyre burdensome for the project;
some people are still on old environments that their IT cant easily update

Scala 2.10 support was deprecated in 2.1, and we did remove Java 7 support
for 2.2. https://issues.apache.org/jira/browse/SPARK-14220 tracks the work
to support 2.12, and there is progress, especially in dependencies
supporting 2.12.

It looks like 2.12 support may even entail a breaking change as documented
in https://issues.apache.org/jira/browse/SPARK-14643 and will mean dropping
Kafka 0.8, for example. In any event its going to take some surgery and a
few hacks to make one code base work across 2.11 and 2.12. I dont see this
happening for Spark 2.2.0 because there are just a few weeks left.

Supporting three versions at once is probably infeasible, so dropping 2.10
should precede 2.12 support. Right now, I would like to make progress
towards changes that 2.12 will require but that 2.11/2.10 can support. For
example, we have to update scalatest, breeze, chill, etc. and can do that
before 2.12 is enabled. However Im finding making those changes tricky or
maybe impossible in one case while 2.10 is still supported.

For 2.2.0, Im wondering if it makes sense to go ahead and drop 2.10
support, and even get in additional prep work for 2.12, into the 2.2.0
release. The move to support 2.12 in 2.3.0 would then be a smaller change.
It isnt strictly necessary. We could delay all of that until after 2.2.0
and get it all done between 2.2.0 and 2.3.0. But I wonder if 2.10 is legacy
enough at this stage to drop for Spark 2.2.0?

I dont feel strongly about it but there are some reasonable arguments for
dropping it:

- 2.10 doesnt technically support Java 8, though we do have it working
still even after requiring Java 8
- Safe to say virtually all common _2.10 libraries has a _2.11 counterpart
at this point?
- 2.10.x was EOL in September 2015 with the final 2.10.6 release
- For a vendor viewpoint: CDH only supports Scala 2.11 with Spark 2.x

Before I open a JIRA, just soliciting opinions.



t
.
as
ot
"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 26 Feb 2017 05:25:51 -0700 (MST)",Re: A DataFrame cache bug,dev@spark.apache.org,"

Hi Gen,

I submitted a PR to fix the issue of refreshByPath:
https://github.com/apache/spark/pull/17064

Thanks.



tgbaggio wrote

















-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 26 Feb 2017 05:39:44 -0700 (MST)",Re: The driver hangs at DataFrame.rdd in Spark 2.1.0,dev@spark.apache.org,"Hi Stan,

Looks like it is the same issue we are working to solve. Related PRs are:

https://github.com/apache/spark/pull/16998
https://github.com/apache/spark/pull/16785

You can take a look of those PRs and help review too. Thanks.



StanZhai wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 26 Feb 2017 05:55:53 -0700 (MST)",Re:  The driver hangs at DataFrame.rdd in Spark 2.1.0,dev@spark.apache.org,"Hi Stan,

Looks like it is the same issue we are working to solve. Related PRs are:

https://github.com/apache/spark/pull/16998
https://github.com/apache/spark/pull/16785

You can take a look of those PRs and help review too. Thanks. 


StanZhai wrote





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
kant kodali <kanth909@gmail.com>,"Sun, 26 Feb 2017 12:00:17 -0800",Are we still dependent on Guava jar in Spark 2.1.0 as well?,"""user @spark"" <user@spark.apache.org>, dev <dev@spark.apache.org>","Are we still dependent on Guava jar in Spark 2.1.0 as well (Given Guava jar
incompatibility issues)?
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Mon, 27 Feb 2017 13:24:14 +0900",Re: Oracle JDBC - Spark SQL - Key Not Found: Scale,ayan guha <guha.ayan@gmail.com>,"Hi,

Thank for your report!
Sean had already told us why this happened here:
https://issues.apache.org/jira/browse/SPARK-19392

// maropu





-- 
---
Takeshi Yamamuro
"
vdukic <vdukic@inf.ethz.ch>,"Mon, 27 Feb 2017 01:24:41 -0700 (MST)",[Spark Core] How do Spark workers exchange data in standalone mode?,dev@spark.apache.org,"Hello All,

I want to know more about data exchange between Spark workers in
standalone mode. Every time a task wants to read result of another task,
I want to log that event.

Information I need:
    source task / stage
    destination task / stage
    size of the data transfer

So far I've managed to do something similar by changing two methods in
Spark Core:

In order to get which task produced which partition / block, I added
SortShuffleWriter.sala#SortShuffleWriter#write:

logError(s""""""PRODUCED SORT:
        |BlockId: ${blockId.shuffleId} ${blockId.mapId}
        |PartitionId: ${context.partitionId()}
        |TaskAttemptId: ${context.taskAttemptId()}
        |StageId: ${context.stageId()}
       """""".stripMargin)

To get which task consumed which partition / block, I added to
ShuffleBlockFetcherIterator.scala#ShuffleBlockFetcherIterator#sendRequest

blockIds.foreach{ blockId =>
      logError(
        s""""""CONSUMED:
           |BlockId: ${blockId},
           |PartitionId: ${context.partitionId()},
           |TaskAttemptId: ${context.taskAttemptId()}
           |StageId: ${context.stageId()},
           |Address: ${address}
           |Size: ${sizeMap(blockId)}
                 """""".stripMargin)
    }

Using these two changes, I managed to partially reconstruct the
communication graph, but there are a couple of problems:
1. I cannot map all PRODUCED/CONSUMED logs
2. The amount of data (filed ""size"") does not match real traffic numbers
Shuffle Read/Write on Spark History Server.

I've found an article that explains data exchange in Apache Flink to a
certain extent. Is there something similar for Spark?
https://cwiki.apache.org/confluence/display/FLINK/Data+exchange+between+tasks

Thanks.




--

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 27 Feb 2017 13:08:39 -0800",Re: Spark Improvement Proposals,Joseph Bradley <joseph@databricks.com>,"I'd like to see more discussion on the issues I raised. I don't think there
was a response for why voting is limited to PMC members.

Tim was kind enough to reply with his rationale for a shepherd, but I don't
think that it justifies failing proposals. I think it boiled down to
""shepherds can be helpful"", which isn't a good reason to require them in my
opinion. Sam also had some good comments on this and I think that there's
more to talk about.

That said, I'd rather not have this proposal fail because we're tired of
talking about it. If most people are okay with it as it stands and want a
vote, I'm fine testing this out and fixing it later.

rb





-- 
Ryan Blue
Software Engineer
Netflix
"
Sean Owen <sowen@cloudera.com>,"Mon, 27 Feb 2017 21:57:35 +0000",Re: Spark Improvement Proposals,Ryan Blue <rblue@netflix.com>,"To me, no new process is being invented here, on purpose, and so we should
just rely on whatever governs any large JIRA or vote, because SPIPs are
really just guidance for making a big JIRA.

http://apache.org/foundation/voting.html suggests that PMC members have the
binding votes in general, and for code-modification votes in particular,
which is what this is. Absent a strong reason to diverge from that, I'd go
with that.

are blessed just by majority vote. Oh well, not that it has mattered.)

I also don't see a need to require a shepherd, because JIRAs don't have
such a process, though I also can't see a situation where nobody with a
vote cares to endorse the SPIP ever, but three people vote for it and
nobody objects?

Perhaps downgrade this to ""strongly suggested, so that you don't waste your
time.""

Or, implicitly, that proposing a SPIP calls a vote that lasts for, dunno, a
month. If fewer than 3 PMC vote for it, it doesn't pass anyway. If at least
1 does, OK, they're the shepherd(s). No new process.


"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Feb 2017 13:58:52 -0800","[build system] emergency jenkins master reboot, got some wedged processes","rise-research@eecs.berkeley.edu, dev <dev@spark.apache.org>","the jenkins master is wedged and i'm going to reboot it to increase
it's happiness.

more updates as they come.

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Feb 2017 14:07:40 -0800","Re: [build system] emergency jenkins master reboot, got some wedged processes","rise-research@eecs.berkeley.edu, dev <dev@spark.apache.org>","we're back and things are much snappier!  sorry for the downtime.


---------------------------------------------------------------------


"
Yuhao Yang <hhbyyh@gmail.com>,"Mon, 27 Feb 2017 23:11:55 -0800",Re: Implementation of RNN/LSTM in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, dishu.905@gmail.com","Welcome to try and contribute to our BigDL:
https://github.com/intel-analytics/BigDL

It's native on Spark and fast by leveraging Intel MKL.

2017-02-23 4:51 GMT-08:00 Joeri Hermans <joeri.raymond.e.hermans@cern.ch>:

"
Michael Allman <michael@videoamp.com>,"Tue, 28 Feb 2017 09:11:23 -0800",Re: Implementation of RNN/LSTM in Spark,Yuhao Yang <hhbyyh@gmail.com>,"Hi Yuhao,

BigDL looks very promising and it's a framework we're considering using. It seems the general approach to high performance DL is via GPUs. Your project mentions performance on a Xeon comparable to that of a GPU, but where does this claim come from? Can you provide benchmarks?

Thanks,

Michael

https://github.com/intel-analytics/BigDL <https://github.com/intel-analytics/BigDL> 
<joeri.raymond.e.hermans@cern.ch <mailto:joeri.raymond.e.hermans@cern.ch>>:
<https://github.com/cerndb/dist-keras> This will allow you to run Keras on Spark (with distributed optimization algorithms) through pyspark. I recommend you to check the examples https://github.com/cerndb/dist-keras/tree/master/examples <https://github.com/cerndb/dist-keras/tree/master/examples>. However, you need to be aware that distributed optimization is a research topic, and has several approaches and caveats you need to be aware of. I wrote a blog post on this if you like to have some additional information on this topic https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras <https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras>
algorithm, we also support a ""sequential trainer"" which allows you to train a model on Spark dataframes.
<mailto:nick.pentreath@gmail.com>]
Spark MLlib any time in the near future.
Deeplearning4J and BigDL (there are others but these seem to be the most comprehensive I have come across) - that run on Spark. Also there are various flavours of TensorFlow / Caffe on Spark. And of course the libs such as Torch, Keras, Tensorflow, MXNet, Caffe etc. Some of them have Java or Scala APIs and some form of Spark integration out there in the community (in varying states of development).
be others).
<mailto:nikita.balyschew@googlemail.com><mailto:nikita.balyschew@googlemaihttp://apache-spark-developers-list.1001551.n3.nabble.com/Implementation-of-RNN-LSTM-in-Spark-tp14866p21060.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Implementation-of-RNN-LSTM-in-Spark-tp14866p21060.html>
Nabble.com.
<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org <mailto:dev-unsubscribe@spark.apache.org>>
<mailto:dev-unsubscribe@spark.apache.org>

"
