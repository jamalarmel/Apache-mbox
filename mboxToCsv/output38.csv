nirandap <niranda.perera@gmail.com>,"Thu, 30 Jun 2016 19:39:28 -0700 (MST)",Re: Debugging Spark itself in standalone cluster mode,dev@spark.apache.org,"Guys,

Aren't TaskScheduler and DAGScheduler residing in the spark context? So,
the debug configs need to be set in the JVM where the spark context is
running? [1]

But yes, I agree, if you really need to check the execution, you need to
set those configs in the executors [2]

[1]
https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sparkcontext.html
[2]
http://spark.apache.org/docs/latest/configuration.html#runtime-environment



t.
d
y
ng
e
he
k-itself-in-standalone-cluster-mode-tp18139.html
-itself-in-standalone-cluster-mode-tp18139p18141.html
ervlet.jtp?macro=unsubscribe_by_code&node=1&code=bmlyYW5kYS5wZXJlcmFAZ21haWwuY29tfDF8NjAxMDUyMzU5>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>



-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/




--
3.nabble.com/Debugging-Spark-itself-in-standalone-cluster-mode-tp18139p18145.html
om."
cbruegg <mail@cbruegg.com>,"Fri, 1 Jul 2016 02:11:44 -0700 (MST)",Re: Debugging Spark itself in standalone cluster mode,dev@spark.apache.org,"Thanks for the guidance! Setting the --driver-java-options in spark-shell
instead of SPARK_MASTER_OPTS made the debugger connect to the right JVM. My
breakpoints get hit now.

nirandap [via Apache Spark Developers List] <
ml-node+s1001551n18145h83@n3.nabble.com> schrieb am Fr., 1. Juli 2016 um
04:39 Uhr:

parkcontext.html
t
<[hidden
:
r
nd
by
f
he
rk-itself-in-standalone-cluster-mode-tp18139.html
k-itself-in-standalone-cluster-mode-tp18139p18141.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
-itself-in-standalone-cluster-mode-tp18139p18145.html
ick
ervlet.jtp?macro=unsubscribe_by_code&node=18139&code=bWFpbEBjYnJ1ZWdnLmNvbXwxODEzOXwtMjAxMTcyNDY4OQ==>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Debugging-Spark-itself-in-standalone-cluster-mode-tp18139p18146.html
om."
Yanbo Liang <ybliang8@gmail.com>,"Fri, 1 Jul 2016 04:00:16 -0700",Re: MinMaxScaler With features include category variables,=?UTF-8?B?5q6155+z55+z?= <burness1990@gmail.com>,"You can combine the columns which are need to be normalized into a vector
by VectorAssembler and do normalization on it.
Do another assembling for columns should not be normalized. At last, you
can assemble the two vector into one vector as the feature column and feed
it into model training.

Thanks
Yanbo

2016-06-25 21:16 GMT-07:00 段石石 <burness1990@gmail.com>:

i
er
e
n.
"
Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Fri, 1 Jul 2016 13:11:16 +0200",Code Style Formatting,dev@spark.apache.org,"Hi, all.

I've read the Spark code style guide.
I am wondering if there is an easy way to configure the code formatting in
IntelliJ IDEA to match the existing code base style.
IntelliJ IDEA highlights all failed checks from scalastyle-config.xml.
However, I did not find any predefined configurations that I can import to
IntelliJ IDEA to adjust hot it does the formatting.
Is it possible to avoid the manual configuration?

Best regards,
Anton Okolnychyi
"
Rishabh Bhardwaj <rbnext29@gmail.com>,"Fri, 1 Jul 2016 17:24:06 +0530",Deploying ML Pipeline Model,user <user@spark.apache.org>,"Hi All,

I am looking for ways to deploy a ML Pipeline model in production .
Spark has already proved to be a one of the best framework for model
training and creation, but once the ml pipeline model is ready how can I
deploy it outside spark context ?
MLlib model has toPMML method but today Pipeline model can not be saved to
PMML. There are some frameworks like MLeap which are trying to abstract
Pipeline Model and provide ML Pipeline Model deployment outside spark
context,but currently they don't have most of the ml transformers and
estimators.
I am looking for related work going on this area.
Any pointers will be helpful.

Thanks,
Rishabh.
"
Cody Koeninger <cody@koeninger.org>,"Fri, 1 Jul 2016 08:02:34 -0500",Jenkins networking / port contention,dev@spark.apache.org,"Can someone familiar with amplab's jenkins setup clarify whether all tests
running at a given time are competing for network ports, or whether there's
some sort of containerization being done?

former.
"
Stephen Hellberg <hellbes@uk.ibm.com>,"Fri, 1 Jul 2016 09:03:20 -0700 (MST)",Jetty 9.3 CVE to be avoided...,dev@spark.apache.org,"To anyone contemplating an upgrade of the Jetty component in use with Apache
Spark, please be aware of  CVE-2016-4800
<http://www.ocert.org/advisories/ocert-2016-001.html>  , and ensure that you
are attempting to only integrate a version of the Jetty 9.3 stream that is
*9.3.9* /or later/.

Hopefully forewarned is forearmed; no need to expose vulnerabilities
unnecessarily!  ;-)



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 1 Jul 2016 09:18:48 -0700",Re: Jenkins networking / port contention,Cody Koeninger <cody@koeninger.org>,"i assume you're talking about zinc ports?

the tests are designed to run one at a time on randomized ports -- no
containerization.  we're on bare metal.

the test launch code executes this for each build:
# Generate random point for Zinc
export ZINC_PORT
ZINC_PORT=$(python -S -c ""import random; print random.randrange(3030,4030)"")


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 1 Jul 2016 11:38:25 -0500",Re: Jenkins networking / port contention,shane knapp <sknapp@berkeley.edu>,"Thanks for the response.  I'm talking about test code that starts up
embedded network services for integration testing.

KafkaTestUtils in particular always attempts to start a kafka broker
on the standard port, 9092.  Util.startServiceInPort is intended to
pick a higher port if the starting one has a bind collision... but in
my local testing multiple KafkaTestUtils instances running at the same
time on the same machine don't actually behave correctly.

I already updated the kafka 0.10 consumer tests to use a random port,
and can do the same for the 0.8 consumer tests, but wanted to make
sure I understood what was happening in the Jenkins environment.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 1 Jul 2016 09:43:29 -0700",Re: Jenkins networking / port contention,"Cody Koeninger <cody@koeninger.org>, Josh Rosen <joshrosen@databricks.com>","gotcha...  adding @joshrosen directly who might be of more assistance...  :)


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 1 Jul 2016 09:45:07 -0700",Re: Jenkins networking / port contention,Cody Koeninger <cody@koeninger.org>,"Multiple instances of test runs are usually running in parallel, so they
would need to bind to different ports.


"
Cody Koeninger <cody@koeninger.org>,"Fri, 1 Jul 2016 11:49:49 -0500",Re: Jenkins networking / port contention,Reynold Xin <rxin@databricks.com>,"Makes sense.  I'll submit a fix for kafka 0.8 and do a scan through of
other tests to see if I can find similar issues.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 1 Jul 2016 10:10:14 -0700",[build system] quick jenkins restart,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Dan Crankshaw <crankshaw@eecs.berkeley.edu>","i put jenkins in quiet mode as i noticed we have almost no builds
queued.  one of our students needed rust installed on the workers, and
i need to update the PATH on all of the workers.

we should be back up and building within 30 minutes.

thanks!

shane

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 1 Jul 2016 10:28:07 -0700",Re: [build system] quick jenkins restart,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Dan Crankshaw <crankshaw@eecs.berkeley.edu>","aaaaaand we're back.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 1 Jul 2016 17:04:56 -0700",Re: Code Style Formatting,Anton Okolnychyi <anton.okolnychyi@gmail.com>,"There isn't one pre-made, but the default works out OK. The main thing
you'd need to update are spacing changes for function argument indentation
and import ordering.


"
Koert Kuipers <koert@tresata.com>,"Sat, 2 Jul 2016 00:41:01 -0400",branch-2.0 is now 2.0.1-SNAPSHOT?,"""dev@spark.apache.org"" <dev@spark.apache.org>","is that correct?
where do i get the latest 2.0.0-SNAPSHOT?
thanks,
koert
"
Sean Owen <sowen@cloudera.com>,"Sat, 2 Jul 2016 08:35:22 +0100",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Koert Kuipers <koert@tresata.com>,"Yeah, interesting question about whether it should be 2.0.1-SNAPSHOT
at this stage because 2.0.0 is not yet released. But I'm not sure we
publish snapshots anyway?


---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Sat, 2 Jul 2016 12:25:56 -0400",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Sean Owen <sowen@cloudera.com>,"You do, snapshots for spark 2.0.0-SNAPSHOT are updated daily on the apache
snapshot repo. I use them in our own unit tests to find regressions etc. in
spark and report them back

"
Sean Owen <sowen@cloudera.com>,"Sat, 2 Jul 2016 18:37:21 +0100",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Koert Kuipers <koert@tresata.com>,"So, on the one hand I think branch-2.0 should really still be on
2.0.0-SNAPSHOT but is on 2.0.1-SNAPSHOT, and while master should
technically be on 2.1.0-SNAPSHOT but we can't quite because of MiMa
right now, I do see that both snapshots are being produced still:

https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.11/

2.0.0-SNAPSHOT is actually from master, kinda confusingly. Not sure if
that helps.


---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Sat, 2 Jul 2016 11:10:50 -0700",Can't build scala unidoc since Kafka 0.10 support was added,dev@spark.apache.org,"Hello,

I'm no longer able to successfully run `sbt unidoc` in branch-2.0, and the problem seems to stem from the addition of Kafka 0.10 support. If I remove either the Kafka 0.8 or 0.10 projects from the build then unidoc works. If I keep both in I get two dozen inexplicable compilation errors as part of the unidoc task execution. Here's the first few:

[error] /Users/msa/workspace/spark-2.0/external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/CachedKafkaConsumer.scala:50: value assign is not a member of org.apache.kafka.clients.consumer.KafkaConsumer[K,V]
[error]     c.assign(tps)
[error]       ^
[error] /Users/msa/workspace/spark-2.0/external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/CachedKafkaConsumer.scala:95: too many arguments for method seek: (x$1: java.util.Map[org.apache.kafka.common.TopicPartition,Long])Unit
[error]     consumer.seek(topicPartition, offset)
[error]                  ^
[error] /Users/msa/workspace/spark-2.0/external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/CachedKafkaConsumer.scala:100: value records is not a member of java.util.Map[String,org.apache.kafka.clients.consumer.ConsumerRecords[K,V]]
[error]     val r = p.records(topicPartition)

Running `sbt compile` completes without error.

Has anyone else seen this behavior? Any ideas? This seems to be an issue around dependency management, but I'm otherwise stumped.

Cheers,

Michael
---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Sat, 2 Jul 2016 16:07:40 -0400",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Sean Owen <sowen@cloudera.com>,"that helps, now i know i simply need to look at master


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 2 Jul 2016 23:11:25 +0200","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them","""Sean Owen (JIRA)"" <jira@apache.org>","Hi Sean, devs,

How is this possible that Fix Version/s is 2.0.1 given 2.0.0 was not
released yet? Why is that that master is not what's going to be
released so eventually becomes 2.0.0? I don't get it. Appreciate any
guidance. Thanks.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


sian.jira.plugin.system.issuetabpanels:all-tabpanel ]
ead of hard code them
---------------------
document pages are using the include_example Jekyll plugin to extract snippets from actual source files under the examples sub-project. In this way, we can guarantee that Java and Scala code are compilable, and it would be much easier to verify these example snippets since they are part of complete Spark applications.

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 2 Jul 2016 22:19:18 +0100","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Jacek Laskowski <jacek@japila.pl>,"Because a 2.0.0 release candidate is out. If for some reason the
release candidate becomes the 2.0.0 release, then anything merged to
branch-2.0 after it is necessarily fixed in 2.0.1 at best. At this
stage we know the RC1 will not be 2.0.0, so really that vote should be
formally cancelled. Then we just mark anything fixed for 2.0.1 as
fixed for 2.0.0 and make another RC.

master is not what will be released as 2.0.0. branch-2.0 is what will
contain that release.

ssian.jira.plugin.system.issuetabpanels:all-tabpanel ]
tead of hard code them
----------------------
 document pages are using the include_example Jekyll plugin to extract snippets from actual source files under the examples sub-project. In this way, we can guarantee that Java and Scala code are compilable, and it would be much easier to verify these example snippets since they are part of complete Spark applications.

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Sat, 2 Jul 2016 14:20:31 -0700","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Jacek Laskowski <jacek@japila.pl>,"2.0.1 just means that the fix will be included in 2.0.1 (eg its not in the
current 2.0.0 RC).



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Koert Kuipers <koert@tresata.com>,"Sat, 2 Jul 2016 17:24:54 -0400",Dataset and Aggregator API pain points,"""dev@spark.apache.org"" <dev@spark.apache.org>","after working with the Dataset and Aggregator apis for a few weeks porting
some fairly complex RDD algos (an overall pleasant experience) i wanted to
summarize the pain points and some suggestions for improvement given my
experience. all of these are already mentioned on mailing list or jira, but
i figured its good to put them in one place.
see below.
best,
koert

*) a lot of practical aggregation functions do not have a zero. this can be
dealt with correctly using null or None as the zero for Aggregator. in
algebird for example this is expressed as converting an algebird.Aggregator
(which does not have a zero) into a algebird.MonoidAggregator (which does
have a zero, so similar to spark Aggregator) by lifting it. see:
https://github.com/twitter/algebird/blob/develop/algebird-core/src/main/scala/com/twitter/algebird/Aggregator.scala#L420
something similar should be possible in spark. however currently Aggregator
does not like its zero to be null or an Option, making this approach
difficult. see:
https://www.mail-archive.com/user@spark.apache.org/msg53106.html
https://issues.apache.org/jira/browse/SPARK-15810

*) KeyValueGroupedDataset.reduceGroups needs to be efficient, probably
using an Aggregator (with null or None as the zero) under the hood. the
current implementation does a flatMapGroups which is suboptimal.

*) KeyValueGroupedDataset needs mapValues. without this porting many algos
from RDD to Dataset is difficult and clumsy. see:
https://issues.apache.org/jira/browse/SPARK-15780

*) Aggregators need to also work within DataFrames (so
RelationalGroupedDataset) without having to fall back on using Row objects
as input. otherwise all code ends up being written twice, once for
Aggregator and once for UserDefinedAggregateFunction/UDAF. this doesn't
make sense to me. my attempt at addressing this:
https://issues.apache.org/jira/browse/SPARK-15769
https://github.com/apache/spark/pull/13512

best,
koert
"
Jacek Laskowski <jacek@japila.pl>,"Sat, 2 Jul 2016 23:27:44 +0200","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Sean Owen <sowen@cloudera.com>,"Hi,

Thanks Sean! It makes sense.

I'm not fully convinced that's how it should be, so I apologize if I
ever ask about the version management in Spark again :)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


:
assian.jira.plugin.system.issuetabpanels:all-tabpanel ]
stead of hard code them
-----------------------
L document pages are using the include_example Jekyll plugin to extract snippets from actual source files under the examples sub-project. In this way, we can guarantee that Java and Scala code are compilable, and it would be much easier to verify these example snippets since they are part of complete Spark applications.

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 02 Jul 2016 21:36:22 +0000","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Jacek Laskowski <jacek@japila.pl>,"I am not sure any other process makes sense. What are you suggesting should
happen?


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 3 Jul 2016 00:29:54 +0200","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Sean Owen <sowen@cloudera.com>,"Hi,

Always release from master. What could be the gotchas?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 2 Jul 2016 17:59:03 -0700","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Jacek Laskowski <jacek@japila.pl>,"Because in that case you cannot merge anything meant for 2.1 until 2.0 is
released.


"
Reynold Xin <rxin@databricks.com>,"Sat, 2 Jul 2016 18:35:37 -0700",Re: Dataset and Aggregator API pain points,Koert Kuipers <koert@tresata.com>,"Thanks, Koert, for the great email. They are all great points.

We should probably create an umbrella JIRA for easier tracking.


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 3 Jul 2016 14:24:40 +0200","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Reynold Xin <rxin@databricks.com>,"Hi,

Why would I need to start 2.1? If it's ready for master, why could it be
not part of 2.0? ""Release early and often"" is what would benefit Spark a
lot. The time to ship 2.0 is far too long I think. And I know companies
that won't use 2.0 because...it's ""0"" version :-(

Jacek

"
Sean Owen <sowen@cloudera.com>,"Sun, 3 Jul 2016 13:48:18 +0100","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Jacek Laskowski <jacek@japila.pl>,"It's not that you're starting 2.1 per se, but, that you're committing
things that are not in 2.0. Releases are never made from master in
moderately complex projects. It has nothing to do with pace of
release.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 3 Jul 2016 15:42:59 +0200","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Sean Owen <sowen@cloudera.com>,"Hi Sean,

What's wrong with the following release procedure?

1. Use master to create RC (versions - master: 2.0.0-SNAPSHOT branch: 2.0.0-RC1)
2. Add new features to master (versions - master: 2.0.0-SNAPSHOT
branch: 2.0.0-RC1)

3. RC passes a vote => ship it (versions - master: 2.1.0-SNAPSHOT
branch: 2.0.0-RC1 + branch: 2.0.0) <-- master changes to another
SNAPSHOT + copy of 2.0.0-RC1 to 2.0.0

4. RC doesn't pass a vote => cut another RC (versions - master:
2.0.0-SNAPSHOT branch: 2.0.0-RC2)

Repeat 2 + 3 + 4 until RC passes a vote. master is PRed as usual.

What am I missing? I must be missing something, but can't see it.

You're right, it has nothing to do with pace of release but the
project needs frequent releases say quarterly.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 3 Jul 2016 14:49:30 +0100","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Jacek Laskowski <jacek@japila.pl>,"
Either:
a) you prohibit anyone from committing anything to master that can't
go into 2.0.0 at this point until it's released, holding up
development, or
b) you allow it and then have a problem below:


Here, there is no way to create a second release candidate that does
not also have commits in master that weren't intended for 2.0.0

---------------------------------------------------------------------


"
"""Gil Vernik"" <GILV@il.ibm.com>","Sun, 3 Jul 2016 17:29:45 +0300",Spark-13979: issues with hadoopConf,"""Spark dev list"" <dev@spark.apache.org>","Hello,

Any ideas about this one https://issues.apache.org/jira/browse/SPARK-13979
?
Does others see the same issues?

Thanks
Gil.



"
Jacek Laskowski <jacek@japila.pl>,"Sun, 3 Jul 2016 20:03:30 +0200","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Sean Owen <sowen@cloudera.com>,"
Your ""Either"" is my ""And"" :) If a change is on master, it's worth to
be released, isn't it? So, when a RC is rejected, master becomes
another RC with all the changes in-between. What's wrong with the
approach? I can only see benefits.

Jacek

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 3 Jul 2016 19:23:47 +0100","Re: [jira] [Resolved] (SPARK-16345) Extract graphx programming guide
 example snippets from source files instead of hard code them",Jacek Laskowski <jacek@japila.pl>,"
You seem to be accurately describing management of branch-2.0, but we
are talking about master. You're implicitly assuming option (a) where
nobody is merging changes that are suitable for a future release but
not branch-2.0 -- not ""and"" (b) (they are after all mutually
exclusive).

I don't think anyone agrees that's acceptable, when we have such an
easy solution: branching.

---------------------------------------------------------------------


"
WangTaoTheTonic <barneystinson@aliyun.com>,"Sun, 3 Jul 2016 20:54:48 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),dev@spark.apache.org,"Do we have a feature list or release notes for 2.0 like before?



--

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 04 Jul 2016 13:41:08 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey everyone,

Is there an idea for updated timeline for cutting a next RC? Do we have a
clear picture of outstanding issues? I see 21 issues marked Blocker or
Critical targeted at 2.0.0.

The only blockers I see on JIRA are related to MLlib doc updates etc (I
will go through a few of these to clean them up and see where they stand).
If there are other blockers then we should mark them as such to help
tracking progress?



 /
==============================================
ts.py"",
text.py"",
-src.zip/py4j/java_gateway.py"",
-src.zip/py4j/protocol.py"",
scala:183)
:214)
:122)
7)
cessorImpl.java:62)
ructorAccessorImpl.java:45)
a:80)
==============================================
ts.py"",
y"",
.py"",
.py"",
o
for this
XhVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
,
le>
XhVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
XhVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
,
le>
XhVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
org_jira_browse_SPARK-2D16121&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBgK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=9200NP4SpeJSUNrSrlWWEC7vFvjWSyCHnx5LD7Sj9u4&e=>.
ts
.org_jira_browse_SPARK-2D16078&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UX2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=SuVdXUNGdAhYgtA2fMLe5vZ2PFrPOaeO3i3cbhYU4tc&e=>)
t
.
ng
6.
UnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=goarAptcJYfLg44f7BAwhbipqJlRFKz9Y6Z36HItiKg&e=>
pache_spark_pull_10990&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6jXY8_yzfAosPOqqaV4wtMg6ZPwM&s=dFymYD9NRVHIJ5MKpmzPcH_NYwLjOWcZd7FUuQBpTUU&e=>
27
st
97
$1))
_spark-2D2.0.0-2Drc1-2Djira&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=ZD_PezvsJ1GyDhv7MhaeUrVba_uhED5mPkqKpfenKEE&e=>
d
n/
.org_-7Epwendell_spark-2Dreleases_spark-2D2.0.0-2Drc1-2Dbin_&d=DQMFaQ&cbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wSbzZ2LyuDcNKaCijEPdt9rokQ0R9w66tn2jMfjKN2I&e=>
e.org_keys_committer_pwendell.asc&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3VnJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=i1Uxw1NyUf2iuA3CXbyiEODD1RR24rAXUvkc42ut8Ao&e=>
87/
pache.org_content_repositories_orgapachespark-2D1187_&d=DQMFaQ&c=izlc9mDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=QjsvnxXe6JBQqXwKw6r-fIIHI9E0ugeeICAqjRXRNwc&e=>
cs/
.org_-7Epwendell_spark-2Dreleases_spark-2D2.0.0-2Drc1-2Ddocs_&d=DQMFaQ&cbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=_6IZExLgc8WoxW0kft_weR7AvELgbFXnHZdezQ_IYGk&e=>
=================
=================
n
==========================
==========================
-
"
nihed mbarek <nihedmm@gmail.com>,"Tue, 5 Jul 2016 11:24:20 +0200",SparkSession replace SQLContext,dev <dev@spark.apache.org>,"Hi,

I just discover that that SparkSession will replace SQLContext for spark
2.0
JavaDoc is clear
https://spark.apache.org/docs/2.0.0-preview/api/java/org/apache/spark/sql/SparkSession.html
but there is no mention in sql programming guide
https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html#starting-point-sqlcontext

Is it possible to update documentation before the release ?


Thank you

-- 

MBAREK Med Nihed,
Fedora Ambassador, TUNISIA, Northern Africa
http://www.nihed.com

<http://tn.linkedin.com/in/nihed>
"
Romi Kuntsman <romi@totango.com>,"Tue, 5 Jul 2016 13:49:10 +0300",Re: SparkSession replace SQLContext,nihed mbarek <nihedmm@gmail.com>,"You can also claim that there's a whole section of ""Migrating from 1.6 to
2.0"" missing there:
https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html#migration-guide

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Jul 2016 15:22:56 +0200",Why's ds.foreachPartition(println) not possible?,dev <dev@spark.apache.org>,"Hi,

It's with the master built today. Why can't I call
ds.foreachPartition(println)? Is using type annotation the only way to
go forward? I'd be so sad if that's the case.

scala> ds.foreachPartition(println)
<console>:28: error: overloaded method value foreachPartition with alternatives:
  (func: org.apache.spark.api.java.function.ForeachPartitionFunction[Record])Unit
<and>
  (f: Iterator[Record] => Unit)Unit
 cannot be applied to (Unit)
       ds.foreachPartition(println)
          ^

scala> sc.version
res9: String = 2.0.0-SNAPSHOT

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 5 Jul 2016 14:32:25 +0100",Re: Why's ds.foreachPartition(println) not possible?,Jacek Laskowski <jacek@japila.pl>,"Do you not mean ds.foreachPartition(_.foreach(println)) or similar?


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Jul 2016 15:47:36 +0200",Re: Why's ds.foreachPartition(println) not possible?,Sean Owen <sowen@cloudera.com>,"Sort of. Your example works, but could you do a mere
ds.foreachPartition(println)? Why not? What should I even see the Java
version?

scala> val ds = spark.range(10)
ds: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> ds.foreachPartition(println)
<console>:26: error: overloaded method value foreachPartition with alternatives:
  (func: org.apache.spark.api.java.function.ForeachPartitionFunction[Long])Unit
<and>
  (f: Iterator[Long] => Unit)Unit
 cannot be applied to (Unit)
       ds.foreachPartition(println)
          ^

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 5 Jul 2016 14:53:25 +0100",Re: Why's ds.foreachPartition(println) not possible?,Jacek Laskowski <jacek@japila.pl>,"A DStream is a sequence of RDDs, not of elements. I don't think I'd
expect to express an operation on a DStream as if it were elements.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Jul 2016 16:18:31 +0200",Re: Why's ds.foreachPartition(println) not possible?,Sean Owen <sowen@cloudera.com>,"ds is Dataset and the problem is that println (or any other
one-element function) would not work here (and perhaps other methods
with two variants - Java's and Scala's).

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 5 Jul 2016 15:21:02 +0100",Re: Why's ds.foreachPartition(println) not possible?,Jacek Laskowski <jacek@japila.pl>,"Right, should have noticed that in your second mail. But foreach
already does what you want, right? it would be identical here.

How these two methods do conceptually different things on different
arguments. I don't think I'd expect them to accept the same functions.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Jul 2016 16:47:12 +0200",Re: Why's ds.foreachPartition(println) not possible?,Sean Owen <sowen@cloudera.com>,"Well, there is foreach for Java and another foreach for Scala. That's
what I can understand. But while supporting two language-specific APIs
-- Scala and Java -- Dataset API lost support for such simple calls
without type annotations so you have to be explicit about the variant
(since I'm using Scala I want to use Scala API right). It appears that
any single-argument-function operators in Datasets are affected :(

My question was to know whether there are works to fix it (if possible
-- I don't know if it is).

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Tue, 5 Jul 2016 08:57:37 -0700",Re: SparkSession replace SQLContext,Romi Kuntsman <romi@totango.com>,"These topics have been included in the documentation for recent builds of Spark 2.0.

Michael

to 2.0"" missing there:
https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html#migration-guide <https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html#migration-guide>
spark 2.0
https://spark.apache.org/docs/2.0.0-preview/api/java/org/apache/spark/sql/SparkSession.html <https://spark.apache.org/docs/2.0.0-preview/api/java/org/apache/spark/sql/SparkSession.html>
https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html#starting-point-sqlcontext <https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html#starting-point-sqlcontext>

"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Jul 2016 09:19:25 -0700",Re: Why's ds.foreachPartition(println) not possible?,Jacek Laskowski <jacek@japila.pl>,"This seems like a Scala compiler bug.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 5 Jul 2016 10:00:19 -0700",Re: Call to new JObject sometimes returns an empty R environment,pvillacorta@stratio.com,"-sparkr-dev@googlegroups +dev@spark.apache.org

[Please send SparkR development questions to the Spark user / dev
mailing lists. Replies inline]


The reason this is different in Spark 1.6 is that we added support for
automatically deserializing Maps retu"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Jul 2016 11:41:06 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please consider this vote canceled and I will work on another RC soon.


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Jul 2016 21:53:54 +0200",Re: Why's ds.foreachPartition(println) not possible?,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Is this already reported and tracked somewhere. I'm quite sure that
people will be asking about the reasons Spark does this. Where are
such issues reported usually?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 5 Jul 2016 22:07:53 +0200","Re: spark git commit: [SPARK-15204][SQL] improve nullability
 inference for Aggregator","wenchen@apache.org, dev <dev@spark.apache.org>","...

Why do we assert predicates? If it's true, it's true already (no need
to compare whether it's true or not). I'd vote to ""fix"" it.

Jacek

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Jul 2016 13:10:39 -0700","Re: spark git commit: [SPARK-15204][SQL] improve nullability
 inference for Aggregator",Jacek Laskowski <jacek@japila.pl>,"Jacek,

This is definitely not necessary, but I wouldn't waste cycles ""fixing""
things like this when they have virtually zero impact. Perhaps next time we
update this code we can ""fix"" it.

Also can you comment on the pull request directly?



"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Jul 2016 16:11:12 -0400","Re: spark git commit: [SPARK-15204][SQL] improve nullability
 inference for Aggregator",Jacek Laskowski <jacek@japila.pl>,"oh you mean instead of:
assert(ds3.select(NameAgg.toColumn).schema.head.nullable === true)
just do:
assert(ds3.select(NameAgg.toColumn).schema.head.nullable)

i did mostly === true because i also had === false, and i liked the
symmetry, but sure this can be fixed if its not the norm


"
Cody Koeninger <cody@koeninger.org>,"Tue, 5 Jul 2016 15:27:09 -0500",Re: Why's ds.foreachPartition(println) not possible?,Jacek Laskowski <jacek@japila.pl>,"I don't think that's a scala compiler bug.

println is a valid expression that returns unit.

Unit is not a single-argument function, and does not match any of the
overloads of foreachPartition

You may be used to a conversion taking place when println is passed to
method expecting a function, but that's not a safe thing to do
silently for multiple overloads.

tldr;

just use

ds.foreachPartition(x => println(x))

you don't need any type annotations



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Jul 2016 13:31:51 -0700",Re: Why's ds.foreachPartition(println) not possible?,Cody Koeninger <cody@koeninger.org>,"You can file it here: https://issues.scala-lang.org/secure/Dashboard.jspa

Perhaps ""bug"" is not the right word, but ""limitation"". println accepts a
single argument of type Any and returns Unit, and it appears that Scala
fails to infer the correct overloaded method in this case.

  def println() = Console.println()
  def println(x: Any) = Console.println(x)




"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 5 Jul 2016 17:46:38 -0700",Re: Why's ds.foreachPartition(println) not possible?,Jacek Laskowski <jacek@japila.pl>,"I asked this question in Scala user group two years ago:
https://groups.google.com/forum/#!topic/scala-user/W4f0d8xK1nk

Take a look if you are interested in.


"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Jul 2016 22:35:50 -0700",[VOTE] Release Apache Spark 2.0.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, July 8, 2016 at 23:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc2
(4a55b2326c8cf50f772907a8b73fd5e7b3d1aa06).

This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1189/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc2-docs/


=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.

==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Jul 2016 23:22:26 -0700",Re: Dataset and Aggregator API pain points,Koert Kuipers <koert@tresata.com>,"See https://issues.apache.org/jira/browse/SPARK-16390


"
Yuhao Yang <hhbyyh@gmail.com>,"Wed, 6 Jul 2016 00:23:19 -0700",Re: MinMaxScaler With features include category variables,Yanbo Liang <ybliang8@gmail.com>,"You may also find VectorSlicer and SQLTransformer useful in your case. Just
out of curiosity, how would you typically handles categorical features,

Regards,
Yuhao

2016-07-01 4:00 GMT-07:00 Yanbo Liang <ybliang8@gmail.com>:

d
.com>:
i
"
Priya Ch <learnings.chitturi@gmail.com>,"Wed, 6 Jul 2016 13:31:22 +0530",Re: Spark Task failure with File segment length as negative,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Is anyone resolved this ?


Thanks,
Padma CH


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 6 Jul 2016 11:53:52 +0200",Re: Why's ds.foreachPartition(println) not possible?,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Thanks Cody, Reynold, and Ryan! Learnt a lot and feel ""corrected"".

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 6 Jul 2016 13:28:24 +0200",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),Reynold Xin <rxin@databricks.com>,"-1
https://issues.apache.org/jira/browse/SPARK-16379
https://issues.apache.org/jira/browse/SPARK-16371

2016-07-06 7:35 GMT+02:00 Reynold Xin <rxin@databricks.com>:
s
=========
=========
==================
==================



-- 
Maciek Bryński

------"
Sean Owen <sowen@cloudera.com>,"Wed, 6 Jul 2016 14:25:47 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),Reynold Xin <rxin@databricks.com>,"Yeah we still have some blockers; I agree SPARK-16379 is a blocker
which came up yesterday. We also have 5 existing blockers, all doc
related:

SPARK-14808 Spark MLlib, GraphX, SparkR 2.0 QA umbrella
SPARK-14812 ML, Graph 2.0 QA: API: Experimental, DeveloperApi, final,
sealed audit
SPARK-14816 Update MLlib, GraphX, SparkR websites for 2.0
SPARK-14817 ML, Graph, R 2.0 QA: Programming guide update and migration guide
SPARK-15124 R 2.0 QA: New R APIs and API docs

While we'll almost surely need another RC, this one is well worth
testing. It's much closer than even the last one.

The sigs/hashes check out, and I successfully built with Ubuntu 16 /
Java 8 with -Pyarn -Phadoop-2.7 -Phive. Tests pass except for:

DirectKafkaStreamSuite:
- offset recovery *** FAILED ***
  The code passed to eventually never returned normally. Attempted 196
times over 10.028979855 seconds. Last failure message:
strings.forall({
    ((x$1: Any) => DirectKafkaStreamSuite.collectedData.contains(x$1))
  }) was false. (DirectKafkaStreamSuite.scala:250)
- Direct Kafka stream report input information

I know we've seen this before and tried to fix it but it may need another look.


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Wed, 6 Jul 2016 09:30:09 -0500",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),Sean Owen <sowen@cloudera.com>,"I know some usages of the 0.10 kafka connector will be broken until
https://github.com/apache/spark/pull/14026  is merged, but the 0.10
connector is a new feature, so not blocking.

Sean I'm assuming the DirectKafkaStreamSuite failure you saw was for
0.8?  I'll take another look at it.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 6 Jul 2016 08:19:48 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),Sean Owen <sowen@cloudera.com>,"Running the following command:
build/mvn clean -Phive -Phive-thriftserver -Pyarn -Phadoop-2.6 -Psparkr
-Dhadoop.version=2.7.0 package

The build stopped with this test failure:

^[[31m- SPARK-9757 Persist Parquet relation with decimal column *** FAILED
***^[[0m



"
Holden Karau <holden@pigscanfly.ca>,"Wed, 6 Jul 2016 12:41:04 -0700",[PySPARK] - Py4J binary transfer survey,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi PySpark Devs,

The Py4j developer has a survey up for Py4J users -
https://github.com/bartdag/py4j/issues/237 it might be worth our time to
provide some input on how we are using and would like to be using Py4J if
binary transfer was improved. I'm happy to fill it out with my thoughts -
but if other people are interested too maybe we could work on a response
together?

Cheers,

Holden :)

-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Mr rty ff <yasha32@yahoo.com.INVALID>,"Wed, 6 Jul 2016 19:49:38 +0000 (UTC)",Stopping Spark executors,"""dev@spark.apache.org"" <dev@spark.apache.org>","HiI like to recreate this bug https://issues.apache.org/jira/browse/SPARK-13979They talking about stopping Spark executors.Its not clear exactly how do I stop the executorsThanks
 "
Yash Sharma <yash360@gmail.com>,"Thu, 7 Jul 2016 12:09:27 +1000","Spark deletes all existing partitions in SaveMode.Overwrite -
 Expected behavior ?","""user@spark.apache.org"" <dev@spark.apache.org>","Hi All,
While writing a partitioned data frame as partitioned text files I see that
Spark deletes all available partitions while writing few new partitions.

dataDF.write.partitionBy(“year”, “month”,
test2/events/”)


Is this an expected behavior ?

I have a past correction job which would overwrite couple of past
partitions based on new arriving data. I would only want to remove those
partitions.

Is there a neater way to do that other than:
- Find the partitions
- Delete using Hadoop API's
- Write DF in Append Mode


Cheers
Yash
"
nirandap <niranda.perera@gmail.com>,"Wed, 6 Jul 2016 21:00:49 -0700 (MST)","Re: Spark deletes all existing partitions in SaveMode.Overwrite -
 Expected behavior ?",dev@spark.apache.org,"Hi Yash,

Yes, AFAIK, that is the expected behavior of the Overwrite mode.

I think you can use the following approaches if you want to perform a job
on each partitions
[1] for each partition in DF :
https://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala#L1444
[2] run job in SC:
https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/SparkContext.scala#L1818

Best


/test2/events/”)
ll-existing-partitions-in-SaveMode-Overwrite-Expected-behavior-tp18219.html
ervlet.jtp?macro=unsubscribe_by_code&node=1&code=bmlyYW5kYS5wZXJlcmFAZ21haWwuY29tfDF8NjAxMDUyMzU5>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>



-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/




--
3.nabble.com/Spark-deletes-all-existing-partitions-in-SaveMode-Overwrite-Expected-behavior-tp18219p18220.html
om."
Niranda Perera <niranda.perera@gmail.com>,"Thu, 7 Jul 2016 11:32:06 +0530",Latest spark release in the 1.4 branch,"""dev@spark.apache.org"" <dev@spark.apache.org>, Patrick Wendell <pwendell@gmail.com>, 
	Reynold Xin <rxin@databricks.com>","Hi guys,

May I know if you have halted development in the Spark 1.4 branch? I see
that there is a release tag for 1.4.2 but it was never released.

Can we expect a 1.4.x bug fixing release anytime soon?

Best
-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Reynold Xin <rxin@databricks.com>,"Wed, 6 Jul 2016 23:03:43 -0700",Re: Latest spark release in the 1.4 branch,Niranda Perera <niranda.perera@gmail.com>,"I think last time I tried I had some trouble releasing it because the
release scripts no longer work with branch-1.4. You can build from the
branch yourself, but it might be better to upgrade to the later versions.


"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 7 Jul 2016 11:38:04 +0530",Re: Latest spark release in the 1.4 branch,Reynold Xin <rxin@databricks.com>,"Thanks Reynold for the prompt response. Do you think we could use a
1.4-branch latest build in a production environment?






-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Reynold Xin <rxin@databricks.com>,"Wed, 6 Jul 2016 23:10:36 -0700",Re: Latest spark release in the 1.4 branch,Niranda Perera <niranda.perera@gmail.com>,"Yes definitely.



"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 7 Jul 2016 11:42:41 +0530",Re: Latest spark release in the 1.4 branch,Reynold Xin <rxin@databricks.com>,"Thanks Reynold




-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
linxi zeng <linxizeng0615@gmail.com>,"Thu, 7 Jul 2016 14:18:51 +0800","SparkSQL Added file get Exception: is a directory and recursive is
 not turned on","user@spark.apache.org, dev@spark.apache.org","Hi, all:
   As recorded in https://issues.apache.org/jira/browse/SPARK-16408, when
using Spark-sql to execute sql like:
   add file hdfs://xxx/user/test;
   If the HDFS path( hdfs://xxx/user/test) is a directory, then we will get
an exception like:

org.apache.spark.SparkException: Added file hdfs://xxx/user/test is a
directory and recursive is not turned on.
       at org.apache.spark.SparkContext.addFile(SparkContext.scala:1372)
       at org.apache.spark.SparkContext.addFile(SparkContext.scala:1340)
       at
org.apache.spark.sql.hive.execution.AddFile.run(commands.scala:117)
       at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)
       at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)
       at
org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)


   I think we should add an parameter (spark.input.dir.recursive) to
control the value of recursive, and make this parameter works by modify
some code, like:

diff --git
a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/commands.scala
b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/commands.scala
index 6b16d59..3be8553 100644
---
a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/commands.scala
+++
b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/commands.scala
@@ -113,8 +113,9 @@ case class AddFile(path: String) extends
RunnableCommand {

   override def run(sqlContext: SQLContext): Seq[Row] = {
     val hiveContext = sqlContext.asInstanceOf[HiveContext]
+    val recursive =
sqlContext.sparkContext.getConf.getBoolean(""spark.input.dir.recursive"",
false)
     hiveContext.runSqlHive(s""ADD FILE $path"")
-    hiveContext.sparkContext.addFile(path)
+    hiveContext.sparkContext.addFile(path, recursive)
     Seq.empty[Row]
   }
 }
"
Sean Owen <sowen@cloudera.com>,"Thu, 7 Jul 2016 08:42:53 +0100",Re: SPARK-8813 - combining small files in spark sql,Ajay Srivastava <a_k_srivastava@yahoo.com>,"-user

Reynold made the comment that he thinks this was resolved by another
change; maybe he can comment.


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 7 Jul 2016 00:42:48 -0700",Re: Latest spark release in the 1.4 branch,Niranda Perera <niranda.perera@gmail.com>,"You've got to satisfy my curiosity, though.  Why would you want to run such
a badly out-of-date version in production?  I mean, 2.0.0 is just about
ready for release, and lagging three full releases behind, with one of them
being a major version release, is a long way from where Spark is now.


"
Amit Rana <amitranavsr94@gmail.com>,"Thu, 7 Jul 2016 13:19:19 +0530",Understanding pyspark data flow on worker nodes,dev@spark.apache.org,"Hi all,

I am trying  to trace the data flow in pyspark. I am using intellij IDEA in
windows 7.
I had submitted  a python  job as follows:
--master local[4] <path to pyspark  job> <arguments to the job>

I have made the following  insights after running the above command in
debug mode:
->Locally when a pyspark's interpreter starts, it also starts a JVM with
which it communicates through socket.
->py4j is used to handle this communication
->Now this JVM acts as actual spark driver, and loads a JavaSparkContext
which communicates with the spark executors in cluster.

In cluster I have read that the data flow between spark executors and
python interpreter happens using pipes. But I am not able to trace that
data flow.

Please correct me if my understanding is wrong. It would be very helpful
if, someone can help me understand tge code-flow for data transfer between
JVM and python workers.

Thanks,
Amit Rana
"
Sun Rui <sunrise_win@163.com>,"Thu, 7 Jul 2016 16:14:42 +0800",Re: Understanding pyspark data flow on worker nodes,Amit Rana <amitranavsr94@gmail.com>,"You can read https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals <https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals>
For pySpark data flow on worker nodes, you can read the source code of PythonRDD.scala. Python worker processes communicate with Spark executors via sockets instead of pipes.

IDEA in windows 7.
debug mode:
with which it communicates through socket.
JavaSparkContext which communicates with the spark executors in cluster.
python interpreter happens using pipes. But I am not able to trace that data flow.
helpful if, someone can help me understand tge code-flow for data transfer between JVM and python workers.

"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 7 Jul 2016 14:26:31 +0530",Re: Latest spark release in the 1.4 branch,Mark Hamstra <mark@clearstorydata.com>,"Hi Mark,

I agree. :-) We already have a product released with Spark 1.4.1 with some
custom extensions and now we are doing a patch release. We will update
Spark to the latest 2.x version in the next release.

Best




-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Amit Rana <amitranavsr94@gmail.com>,"Thu, 7 Jul 2016 14:28:22 +0530",Re: Understanding pyspark data flow on worker nodes,Sun Rui <sunrise_win@163.com>,"As mentioned in the documentation:
PythonRDD objects launch Python subprocesses and communicate with them
using pipes, sending the user's code and the data to be processed.

I am trying to understand  the implementation of how this data transfer is
happening  using pipes.
Can anyone please guide me along that line??

Thanks,
Amit Rana

"
=?UTF-8?B?5qWK6ZaU5a+M?= <tilumi0@gmail.com>,"Thu, 7 Jul 2016 19:18:07 +0800",Why the org.apache.spark.sql.catalyst.expressions.SortArray is with CodegenFallback?,dev@spark.apache.org,"I found CollapseCodengenStages.supportCodegen(e: Expression) will determine
SortArray expression not CodegenSupported since SortArray is with
CodegenFallback. Can I ask why the SortArray is not CodeGenSupoort??
"
Zhan Zhang <zhazhan@gmail.com>,"Thu, 7 Jul 2016 11:37:46 -0700 (MST)",Anyone knows the hive repo for spark-2.0?,dev@spark.apache.org,"I saw the pom file having hive version as    
<hive.version>1.2.1.spark2</hive.version>. But I cannot find the branch in 
https://github.com/pwendell/

Does anyone know where the repo is?

Thanks.

Zhan Zhang




--

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 7 Jul 2016 11:38:53 -0700",Re: Anyone knows the hive repo for spark-2.0?,Zhan Zhang <zhazhan@gmail.com>,"My guess would be https://github.com/pwendell/hive/tree/release-1.2.1-spark




-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 7 Jul 2016 11:40:01 -0700",Re: Anyone knows the hive repo for spark-2.0?,Zhan Zhang <zhazhan@gmail.com>,"(Actually that's ""spark"" and not ""spark2"", so yeah, that doesn't
really answer the question.)




-- 
Marcelo

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 7 Jul 2016 18:47:32 +0000 (UTC)","Re: [DISCUSS] Minimize use of MINOR, BUILD, and HOTFIX w/ no JIRA","Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Popping this back up to the dev list again.  I see a bunch of checkins with minor or hotfix.  
It seems to me we shouldn't be doing this, but I would like to hear thoughts from others.  I see no reason we can't have a jira for each of those issues, it only takes a few seconds to file one and it makes things much easier to track.
For instance, I tend to watch the jiras on the mailing list and if I hit an issue I search jira to see if there is existing one for it, but if there isn't jira then I don't see and can't find what someone perhaps already fixed with a [MINOR] checkin.
Tom 

 

 Hey All,

Just a request here - it would be great if people could create JIRA's
for any and all merged pull requests. The reason is that when patches
get reverted due to build breaks or other issues, it is very difficult
to keep track of what is going on if there is no JIRA. Here is a list
of 5 patches we had to revert recently that didn't include a JIRA:

    Revert ""[MINOR] [BUILD] Use custom temp directory during build.""
    Revert ""[SQL] [TEST] [MINOR] Uses a temporary log4j.properties in
HiveThriftServer2Test to ensure expected logging behavior""
    Revert ""[BUILD] Always run SQL tests in master build.""
    Revert ""[MINOR] [CORE] Warn users who try to cache RDDs with
dynamic allocation on.""
    Revert ""[HOT FIX] [YARN] Check whether `/lib` exists before
listing its files""

The cost overhead of creating a JIRA relative to other aspects of
development is very small. If it's *really* a documentation change or
something small, that's okay.

But anything affecting the build, packaging, etc. These all need to
have a JIRA to ensure that follow-up can be well communicated to all
Spark developers.

Hopefully this is something everyone can get behind, but opened a
discussion here in case others feel differently.

- Patrick

---------------------------------------------------------------------



  "
Reynold Xin <rxin@databricks.com>,"Thu, 7 Jul 2016 11:20:30 -0700",Re: SPARK-8813 - combining small files in spark sql,Ajay Srivastava <a_k_srivastava@yahoo.com>,"When using native data sources (e.g. Parquet, ORC, JSON, ...), partitions
are automatically merged so they would add up to a specific size,
configurable by spark.sql.files.maxPartitionBytes.

spark.sql.files.openCostInBytes is used to specify the cost of each ""file"".
That is, an empty file will be considered to have at
least spark.sql.files.openCostInBytes bytes.


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 7 Jul 2016 20:58:00 +0200",Re: Stopping Spark executors,Mr rty ff <yasha32@yahoo.com>,"Hi,

Use jps -lm and see the processes on the machine(s) to kill.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 Jul 2016 19:07:59 +0000",Expanded docs for the various storage levels,Spark dev list <dev@spark.apache.org>,"I’m looking at the docs here:

http://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.StorageLevel
<http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel>

A newcomer to Spark won’t understand the meaning of _2, or the meaning of
_SER (or its value), and won’t understand how exactly memory and disk play
together when something like MEMORY_AND_DISK is selected.

Is there a place in the docs that expands on the storage levels a bit? If
not, shall we create a JIRA and expand this documentation? I don’t mind
taking on this task, though frankly I’m interested in this because I don’t
fully understand the differences myself. :)

Nick
​
"
Jonathan Kelly <jonathakamzn@gmail.com>,"Thu, 07 Jul 2016 19:13:51 +0000",Re: Anyone knows the hive repo for spark-2.0?,"Marcelo Vanzin <vanzin@cloudera.com>, Zhan Zhang <zhazhan@gmail.com>","I'm not sure, but I think it's
https://github.com/JoshRosen/hive/tree/release-1.2.1-spark2.

It would be really nice though to have this whole process better documented
and more ""official"" than just building from somebody's personal fork of
Hive.

Or is there some way that the Spark community could contribute back these
changes to Hive in such a way that they would accept them into trunk? Then
Spark could depend upon an official version of Hive rather than this fork.

~ Jonathan


"
Reynold Xin <rxin@databricks.com>,"Thu, 7 Jul 2016 12:17:53 -0700",Re: Expanded docs for the various storage levels,Nicholas Chammas <nicholas.chammas@gmail.com>,"Please create a patch. Thanks!



eLevel
ageLevel>
aning of
disk
t mind
e I don’t
"
Michael Allman <michael@videoamp.com>,"Thu, 7 Jul 2016 12:53:25 -0700",Re: Anyone knows the hive repo for spark-2.0?,Jonathan Kelly <jonathakamzn@gmail.com>,"FYI if you just want to look at the source code, there are source jars for those binary versions in maven central. I was just looking at the metastore source code last night.

Michael

https://github.com/JoshRosen/hive/tree/release-1.2.1-spark2 <https://github.com/JoshRosen/hive/tree/release-1.2.1-spark2>.
documented and more ""official"" than just building from somebody's personal fork of Hive.
these changes to Hive in such a way that they would accept them into trunk? Then Spark could depend upon an official version of Hive rather than this fork.
https://github.com/pwendell/hive/tree/release-1.2.1-spark <https://github.com/pwendell/hive/tree/release-1.2.1-spark>
branch in
http://apache-spark-developers-list.1001551.n3.nabble.com/Anyone-knows-the-hive-repo-for-spark-2-0-tp18234.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Anyone-knows-the-hive-repo-for-spark-2-0-tp18234.html>
Nabble.com.
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-unsubscribe@spark.apache.org>

"
Sean Owen <sowen@cloudera.com>,"Thu, 7 Jul 2016 20:56:31 +0100","Re: [DISCUSS] Minimize use of MINOR, BUILD, and HOTFIX w/ no JIRA",Tom Graves <tgraves_cs@yahoo.com>,"I don't agree that every change needs a JIRA, myself. Really, we
didn't choose to have this system split across JIRA and Github PRs.
It's necessitated by how the ASF works (and with some good reasons).
But while we have this dual system, I figure, let's try to make some
sense of it.

I think it makes sense to make a JIRA for any non-trivial change.
What's non-trivial? where the ""how"" is different from the ""what"". That
is, if the JIRA is not just a repeat of the pull request, they should
probably be separate. But, if the change is so simple that describing
it amounts to dictating how it's implemented -- well, seems like a
JIRA is just overhead.

ONe problem that I think happened above was: pretty non-trivial things
were being merged without a JIRA. The evidence? they were reverted.
That means their effect was not quite obvious. They probably deserved
more discussion. Anything that needs some discussion probably deserves
a JIRA.

Also: we have some hot-fixes here that aren't connected to JIRAs.
Either they belong with an existing JIRA and aren't tagged correctly,
or, again, are patching changes that weren't really trivial enough to
skip a JIRA to begin with.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 Jul 2016 20:26:17 +0000",Re: Expanded docs for the various storage levels,Reynold Xin <rxin@databricks.com>,"JIRA is here: https://issues.apache.org/jira/browse/SPARK-16427


geLevel
rageLevel>
eaning
and disk
f
t mind
se I don’t
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 Jul 2016 20:27:16 +0000",Bad JIRA components,Spark dev list <dev@spark.apache.org>,"https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:components-panel

There are several bad components in there, like docs, MLilb, and sq;. I’ve
updated the issues that were assigned to them, but I don’t know if there is
a way to delete these components from the drop down so they don’t get
mistakenly selected again.

Can a JIRA admin take a look?

Nick
​
"
Mr rty ff <yasha32@yahoo.com.INVALID>,"Thu, 7 Jul 2016 20:27:46 +0000 (UTC)",Re: Stopping Spark executors,Jacek Laskowski <jacek@japila.pl>,"This what I get when I run the command946 sun.tools.jps.Jps -lm7443 org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shellI don't think that shululd kill SparkSubmit  process
 

ote:
 

 Hi,

Use jps -lm and see the processes on the machine(s) to kill.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


:
 I

---------------------------------------------------------------------



  "
Jacek Laskowski <jacek@japila.pl>,"Thu, 7 Jul 2016 22:31:53 +0200",Re: Stopping Spark executors,Mr rty ff <yasha32@yahoo.com>,"Hi,

It appears you're running local mode (local[*] assumed) so killing
spark-shell *will* kill the one and only executor -- the driver :)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Mr rty ff <yasha32@yahoo.com.INVALID>,"Thu, 7 Jul 2016 20:35:56 +0000 (UTC)",Re: Stopping Spark executors,Jacek Laskowski <jacek@japila.pl>,"I don't think Its the proper way to recreate the bug becouse I should continue to send commands to the shellThey talking about killing the CoarseGrainedExecutorBackend

rote:
 

 Hi,

It appears you're running local mode (local[*] assumed) so killing
spark-shell *will* kill the one and only executor -- the driver :)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


in
e:
te:
o

---------------------------------------------------------------------



  "
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 7 Jul 2016 20:40:26 +0000 (UTC)","Re: [DISCUSS] Minimize use of MINOR, BUILD, and HOTFIX w/ no JIRA",Sean Owen <sowen@cloudera.com>,"I think the problems comes in with your definition as well as peoples interpretation of that.  I don't agree with your statement of ""where the ""how"" is different from the ""what"""".  
This could apply to a lot of things.  I could easily file a jira that says remove synchronization on routine x, then change a lock.  No discussion needed and the how is the same as the what.  But it could have huge impact on the code and definitely should have a jira.  It may be a contrived example but there is a lot of leeway in that.  Which is why I think Patrick originally sent this email and you said it yourself above that some of the things reverted  weren't trivial enough to begin with.  That just proves people can't make that judgement call themselves.  So why not just file a jira for everything?  Another example of this could be doc changes.  you may think they are trivial but if someone changes the docs and remove configs or change the wording such that users don't understand, then to me it should have had a jira and possibly discussion before changing.  
So based on that it seems like spending the 5 to 30 seconds to file a jira would only help in tracking things and isn't much overhead.  
We also base our release notes and other things on jira. 
Also for hotfixes I think they should have the original jira or a separate jira (with brokenby linked to original), again for tracking purposes. If we check something into master and then later want to cherry-pick it back, I might just pick the original commit and totally miss this ""HOTFIX"" that was required if they aren't properly linked.
Tom 

:
 

 I don't agree that every change needs a JIRA, myself. Really, we
didn't choose to have this system split across JIRA and Github PRs.
It's necessitated by how the ASF works (and with some good reasons).
But while we have this dual system, I figure, let's try to make some
sense of it.

I think it makes sense to make a JIRA for any non-trivial change.
What's non-trivial? where the ""how"" is different from the ""what"". That
is, if the JIRA is not just a repeat of the pull request, they should
probably be separate. But, if the change is so simple that describing
it amounts to dictating how it's implemented -- well, seems like a
JIRA is just overhead.

ONe problem that I think happened above was: pretty non-trivial things
were being merged without a JIRA. The evidence? they were reverted.
That means their effect was not quite obvious. They probably deserved
more discussion. Anything that needs some discussion probably deserves
a JIRA.

Also: we have some hot-fixes here that aren't connected to JIRAs.
Either they belong with an existing JIRA and aren't tagged correctly,
or, again, are patching changes that weren't really trivial enough to
skip a JIRA to begin with.

rote:
ns with
hts
 issues,
o
an
ld.""
es in


  "
Reynold Xin <rxin@databricks.com>,"Thu, 7 Jul 2016 14:03:13 -0700",Re: Bad JIRA components,Nicholas Chammas <nicholas.chammas@gmail.com>,"I deleted those.


m

jira.jira-projects-plugin:components-panel
t know if
’t
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 7 Jul 2016 23:05:10 +0200",Re: Stopping Spark executors,Mr rty ff <yasha32@yahoo.com>,"Hi,

Then use --master with spark standalone, yarn, or mesos.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 07 Jul 2016 21:06:29 +0000",Re: Bad JIRA components,Reynold Xin <rxin@databricks.com>,"Thanks Reynold.


.jira.jira-projects-plugin:components-panel
t know if
’t
"
Mr rty ff <yasha32@yahoo.com.INVALID>,"Thu, 7 Jul 2016 21:26:53 +0000 (UTC)",Re: Stopping Spark executors,Jacek Laskowski <jacek@japila.pl>,"HiI am sorry but its still not clearDo you mean ./bin/spark-shell --master localAnd what I do after that killing the org.apache.spark.deploy.SparkSubmit --master local --class org.apache.spark.repl.Main --name Spark shell spark-shell
will kill the shell so I couldn't send the commands .Thanks 

te:
 

 Hi,

Then use --master with spark standalone, yarn, or mesos.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


te:
do

---------------------------------------------------------------------



  "
kevin <kiss.kevin119@gmail.com>,"Fri, 8 Jul 2016 09:25:23 +0800",spark1.6.2 ClassNotFoundException: org.apache.parquet.hadoop.ParquetOutputCommitter,"""dev.spark"" <dev@spark.apache.org>","hi,all:
I build spark 1.6.2 frpm source with :
./make-distribution.sh --name ""hadoop2.7.1"" --tgz
""-Pyarn,hadoop-2.6,parquet-provided,hive,hive-thriftserver"" -DskipTests
-Dhadoop.version=2.7.1

when I try to run :
./bin/run-example sql.RDDRelation
or
./spark-shell

I met the error with :(but I can run example
 about org.apache.spark.examples.SparkPi )

java.lang.NoClassDefFoundError:
org/apache/parquet/hadoop/ParquetOutputCommitter
at org.apache.spark.sql.SQLConf$.<init>(SQLConf.scala:319)
at org.apache.spark.sql.SQLConf$.<clinit>(SQLConf.scala)
at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:85)
at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)
at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.apache.spark.repl.SparkILoop.createSQLContext(SparkILoop.scala:1028)
at $iwC$$iwC.<init>(<console>:15)
at $iwC.<init>(<console>:24)
at <init>(<console>:26)
at .<init>(<console>:30)
at .<clinit>(<console>)
at .<init>(<console>:7)
at .<clinit>(<console>)
at $print(<console>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
at
org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:132)
at
org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:124)
at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
at
org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:124)
at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:64)
at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
at
org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:159)
at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
at
org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:108)
at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:64)
at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
at
scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
at org.apache.spark.repl.SparkILoop.org
$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
at org.apache.spark.repl.Main$.main(Main.scala:31)
at org.apache.spark.repl.Main.main(Main.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException:
org.apache.parquet.hadoop.ParquetOutputCommitter
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 57 more

<console>:16: error: not found: value sqlContext
         import sqlContext.implicits._
                ^
<console>:16: error: not found: value sqlContext
         import sqlContext.sql


*what should  I do ?*
"
Sun Rui <sunrise_win@163.com>,"Fri, 8 Jul 2016 10:08:32 +0800",Re: spark1.6.2 ClassNotFoundException: org.apache.parquet.hadoop.ParquetOutputCommitter,kevin <kiss.kevin119@gmail.com>,"maybe related to ""parquet-provided”? 
remove ""parquet-provided” profile when making the distribution or adding the parquet jar into class path when running Spark

"
Amit Rana <amitranavsr94@gmail.com>,"Fri, 8 Jul 2016 11:31:48 +0530",Re: Understanding pyspark data flow on worker nodes,dev@spark.apache.org,"Hi all,

Did anyone get a chance to look into it??
Any sort of guidance will be much appreciated.

Thanks,
Amit Rana

"
Reynold Xin <rxin@databricks.com>,"Thu, 7 Jul 2016 23:03:04 -0700",Re: Understanding pyspark data flow on worker nodes,Amit Rana <amitranavsr94@gmail.com>,"You can look into its source code:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala



"
Jacek Laskowski <jacek@japila.pl>,"Fri, 8 Jul 2016 12:05:27 +0200",Re: Stopping Spark executors,Mr rty ff <yasha32@yahoo.com>,"Hi,

Read the doc http://spark.apache.org/docs/latest/spark-standalone.html
which seems to be the cluster manager the OP uses.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 8 Jul 2016 11:09:47 +0100",Re: Understanding pyspark data flow on worker nodes,Amit Rana <amitranavsr94@gmail.com>,"Hi, sharing what I discovered with PySpark too, corroborates with what 
Amit notices and also interested in the pipe question:
h
ttps://mail-archives.apache.org/mod_mbox/spark-dev/201603.mbox/%3C201603291521.u2TFLBfO024212@d06av05.portsmouth.uk.ibm.com%3E


// Start a thread to feed the process input from our parent's iterator 
  val writerThread = new WriterThread(env, worker, inputIterator, 
partitionIndex, context)

...

// Return an iterator that read lines from the process's stdout 
  val stream = new DataInputStream(new 
BufferedInputStream(worker.getInputStream, bufferSize))

The above code and what follows look to be the important parts.



Note that Josh Rosen replied to my comment with more information:

that Python UDFs and RDD API code can be executed. Some slightly-outdated 
but mostly-correct reference material for this can be found at 
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals. 

See also: search the Spark codebase for PythonRDD and look at 
python/pyspark/worker.py""




From:   Reynold Xin <rxin@databricks.com>
To:     Amit Rana <amitranavsr94@gmail.com>
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   08/07/2016 07:03
Subject:        Re: Understanding pyspark data flow on worker nodes



You can look into its source code: 
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala


Hi all,
Did anyone get a chance to look into it??
Any sort of guidance will be much appreciated.
Thanks,
Amit Rana
As mentioned in the documentation:
PythonRDD objects launch Python subprocesses and communicate with them 
using pipes, sending the user's code and the data to be processed.
I am trying to understand  the implementation of how this data transfer is 
happening  using pipes.
Can anyone please guide me along that line??
Thanks, 
Amit Rana
You can read 
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals
For pySpark data flow on worker nodes, you can read the source code of 
PythonRDD.scala. Python worker processes communicate with Spark executors 
via sockets instead of pipes.


Hi all,
I am trying  to trace the data flow in pyspark. I am using intellij IDEA 
in windows 7.
I had submitted  a python  job as follows:
--master local[4] <path to pyspark  job> <arguments to the job>
I have made the following  insights after running the above command in 
debug mode:
->Locally when a pyspark's interpreter starts, it also starts a JVM with 
which it communicates through socket.
->py4j is used to handle this communication 
->Now this JVM acts as actual spark driver, and loads a JavaSparkContext 
which communicates with the spark executors in cluster.
In cluster I have read that the data flow between spark executors and 
python interpreter happens using pipes. But I am not able to trace that 
data flow.
Please correct me if my understanding is wrong. It would be very helpful 
if, someone can help me understand tge code-flow for data transfer between 
JVM and python workers.
Thanks,
Amit Rana



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 8 Jul 2016 11:17:09 +0100",Spark 2.0.0 performance; potential large Spark core regression,dev <dev@spark.apache.org>,"Hi, we've been testing the performance of Spark 2.0 compared to previous 
releases, unfortunately there are no Spark 2.0 compatible versions of 
HiBench and SparkPerf apart from those I'm working on (see 
https://github.com/databricks/spark-perf/issues/108)

With the Spark 2.0 version of SparkPerf we've noticed a 30% geomean 
regression with a very small scale factor and so we've generated a couple 
of profiles comparing 1.5.2 vs 2.0.0. Same JDK version and same platform. 
We will gather a 1.6.2 comparison and increase the scale factor.

Has anybody noticed a similar problem? My changes for SparkPerf and Spark 
2.0 are very limited and AFAIK don't interfere with Spark core 
functionality, so any feedback on the changes would be much appreciated 
and welcome, I'd much prefer it if my changes are the problem.

A summary for your convenience follows (this matches what I've mentioned 
on the SparkPerf issue above)

1. spark-perf/config/config.py : SCALE_FACTOR=0.05
No. Of Workers: 1
Executor per Worker : 1
Executor Memory: 18G
Driver Memory : 8G
Serializer: kryo

2. $SPARK_HOME/conf/spark-defaults.conf: executor Java Options: 
-Xdisableexplicitgc -Xcompressedrefs

Main changes I made for the benchmark itself
Use Scala 2.11.8 and Spark 2.0.0 RC2 on our local filesystem
MLAlgorithmTests use Vectors.fromML
For streaming-tests in HdfsRecoveryTest we use wordStream.foreachRDD not 
wordStream.foreach
KVDataTest uses awaitTerminationOrTimeout in a SparkStreamingContext 
instead of awaitTermination
Trivial: we use compact not compact.render for outputting json

In Spark 2.0 the top five methods where we spend our time is as follows, 
the percentage is how much of the overall processing time was spent in 
this particular method:
2.      SortShuffleWriter.write 19%
3.      SizeTracker.estimateSize 7.5%
4.      SizeEstimator.estimate 5.36%
5.      Range.foreach 3.6%

and in 1.5.2 the top five methods are:
2.      ExternalSorter.insertAll 33%
3.      Range.foreach 4%
4.      SizeEstimator.estimate 2%
5.      SizeEstimator.visitSingleObject 2%

I see the following scores, on the left I have the test name followed by 
the 1.5.2 time and then the 2.0.0 time
scheduling throughput: 5.2s vs 7.08s
agg by key; 0.72s vs 1.01s
agg by key int: 0.93s vs 1.19s
agg by key naive: 1.88s vs 2.02
sort by key: 0.64s vs 0.8s
sort by key int: 0.59s vs 0.64s
scala count: 0.09s vs 0.08s
scala count w fltr: 0.31s vs 0.47s

This is only running the Spark core tests (scheduling throughput through 
scala-count-w-filtr, including all inbetween).

Cheers,


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Michael Allman <michael@videoamp.com>,"Fri, 8 Jul 2016 08:44:19 -0700",Re: Spark 2.0.0 performance; potential large Spark core regression,Adam Roberts <aroberts@uk.ibm.com>,"Hi Adam,

Do you have your spark confs and your spark-env.sh somewhere where we can see them? If not, can you make them available?

Cheers,

Michael

previous releases, unfortunately there are no Spark 2.0 compatible versions of HiBench and SparkPerf apart from those I'm working on (see https://github.com/databricks/spark-perf/issues/108 <https://github.com/databricks/spark-perf/issues/108>) 
regression with a very small scale factor and so we've generated a couple of profiles comparing 1.5.2 vs 2.0.0. Same JDK version and same platform. We will gather a 1.6.2 comparison and increase the scale factor. 
Spark 2.0 are very limited and AFAIK don't interfere with Spark core functionality, so any feedback on the changes would be much appreciated and welcome, I'd much prefer it if my changes are the problem. 
mentioned on the SparkPerf issue above) 
-Xdisableexplicitgc -Xcompressedrefs 
not wordStream.foreach
instead of awaitTermination
follows, the percentage is how much of the overall processing time was spent in this particular method: 
by the 1.5.2 time and then the 2.0.0 time
through scala-count-w-filtr, including all inbetween). 
number 741598. 
3AU

"
Michael Allman <michael@videoamp.com>,"Fri, 8 Jul 2016 08:58:11 -0700",Spark performance regression test suite,dev <dev@spark.apache.org>,"Hello,

I've seen a few messages on the mailing list regarding Spark performance concerns, especially regressions from previous versions. It got me thinking that perhaps an automated performance regression suite would be a worthwhile contribution? Is anyone working on this? Do we have a Jira issue for it?

I cannot commit to taking charge of such a project. I just thought it would be a great contribution for someone who does have the time and the chops to build it.

Cheers,

Michael
---------------------------------------------------------------------


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 8 Jul 2016 16:58:08 +0100",Re: Spark 2.0.0 performance; potential large Spark core regression,Michael Allman <michael@videoamp.com>,"Hi Michael, the two Spark configuration files aren't very exciting

spark-env.sh
Same as the template apart from a JAVA_HOME setting

spark-defaults.conf
spark.io.compression.codec lzf

config.py has the Spark home set, is running Spark standalone mode, we run 
and prep Spark tests only, driver 8g, executor memory 16g, Kryo, 0.66 
memory fraction, 100 trials

We can post the 1.6.2 comparison early next week, running lots of 
iterations over the weekend once we get the dedicated time again

Cheers,





From:   Michael Allman <michael@videoamp.com>
To:     Adam Roberts/UK/IBM@IBMGB
Cc:     dev <dev@spark.apache.org>
Date:   08/07/2016 16:44
Subject:        Re: Spark 2.0.0 performance; potential large Spark core 
regression



Hi Adam,

Do you have your spark confs and your spark-env.sh somewhere where we can 
see them? If not, can you make them available?

Cheers,

Michael


Hi, we've been testing the performance of Spark 2.0 compared to previous 
releases, unfortunately there are no Spark 2.0 compatible versions of 
HiBench and SparkPerf apart from those I'm working on (see 
https://github.com/databricks/spark-perf/issues/108) 

With the Spark 2.0 version of SparkPerf we've noticed a 30% geomean 
regression with a very small scale factor and so we've generated a couple 
of profiles comparing 1.5.2 vs 2.0.0. Same JDK version and same platform. 
We will gather a 1.6.2 comparison and increase the scale factor. 

Has anybody noticed a similar problem? My changes for SparkPerf and Spark 
2.0 are very limited and AFAIK don't interfere with Spark core 
functionality, so any feedback on the changes would be much appreciated 
and welcome, I'd much prefer it if my changes are the problem. 

A summary for your convenience follows (this matches what I've mentioned 
on the SparkPerf issue above) 

1. spark-perf/config/config.py : SCALE_FACTOR=0.05
No. Of Workers: 1
Executor per Worker : 1
Executor Memory: 18G
Driver Memory : 8G
Serializer: kryo 

2. $SPARK_HOME/conf/spark-defaults.conf: executor Java Options: 
-Xdisableexplicitgc -Xcompressedrefs 

Main changes I made for the benchmark itself 
Use Scala 2.11.8 and Spark 2.0.0 RC2 on our local filesystem 
MLAlgorithmTests use Vectors.fromML 
For streaming-tests in HdfsRecoveryTest we use wordStream.foreachRDD not 
wordStream.foreach 
KVDataTest uses awaitTerminationOrTimeout in a SparkStreamingContext 
instead of awaitTermination 
Trivial: we use compact not compact.render for outputting json

In Spark 2.0 the top five methods where we spend our time is as follows, 
the percentage is how much of the overall processing time was spent in 
this particular method: 
2.        SortShuffleWriter.write 19% 
3.        SizeTracker.estimateSize 7.5% 
4.        SizeEstimator.estimate 5.36% 
5.        Range.foreach 3.6% 

and in 1.5.2 the top five methods are: 
2.        ExternalSorter.insertAll 33% 
3.        Range.foreach 4% 
4.        SizeEstimator.estimate 2% 
5.        SizeEstimator.visitSingleObject 2% 

I see the following scores, on the left I have the test name followed by 
the 1.5.2 time and then the 2.0.0 time
scheduling throughput: 5.2s vs 7.08s
agg by key; 0.72s vs 1.01s
agg by key int: 0.93s vs 1.19s
agg by key naive: 1.88s vs 2.02
sort by key: 0.64s vs 0.8s
sort by key int: 0.59s vs 0.64s
scala count: 0.09s vs 0.08s
scala count w fltr: 0.31s vs 0.47s 

This is only running the Spark core tests (scheduling throughput through 
scala-count-w-filtr, including all inbetween). 

Cheers, 


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Michael Allman <michael@videoamp.com>,"Fri, 8 Jul 2016 09:01:25 -0700",Re: Spark 2.0.0 performance; potential large Spark core regression,Adam Roberts <aroberts@uk.ibm.com>,"Hi Adam,

From our experience we've found the default Spark 2.0 configuration to be highly suboptimal. I don't know if this affects your benchmarks, but I would consider running some tests with tuned and alternate configurations.

Michael


run and prep Spark tests only, driver 8g, executor memory 16g, Kryo, 0.66 memory fraction, 100 trials 
iterations over the weekend once we get the dedicated time again 
core regression 
can see them? If not, can you make them available? 
previous releases, unfortunately there are no Spark 2.0 compatible versions of HiBench and SparkPerf apart from those I'm working on (see https://github.com/databricks/spark-perf/issues/108 <https://github.com/databricks/spark-perf/issues/108>) 
regression with a very small scale factor and so we've generated a couple of profiles comparing 1.5.2 vs 2.0.0. Same JDK version and same platform. We will gather a 1.6.2 comparison and increase the scale factor. 
Spark 2.0 are very limited and AFAIK don't interfere with Spark core functionality, so any feedback on the changes would be much appreciated and welcome, I'd much prefer it if my changes are the problem. 
mentioned on the SparkPerf issue above) 
-Xdisableexplicitgc -Xcompressedrefs 
not wordStream.foreach
instead of awaitTermination
follows, the percentage is how much of the overall processing time was spent in this particular method: 
by the 1.5.2 time and then the 2.0.0 time
through scala-count-w-filtr, including all inbetween). 
number 741598. 
3AU 
number 741598. 
3AU

"
Michael Allman <michael@videoamp.com>,"Fri, 8 Jul 2016 09:05:45 -0700",Re: Spark 2.0.0 performance; potential large Spark core regression,Adam Roberts <aroberts@uk.ibm.com>,"Here are some settings we use for some very large GraphX jobs. These are based on using EC2 c3.8xl workers:

    .set(""spark.sql.shuffle.partitions"", ""1024"")
    .set(""spark.sql.tungsten.enabled"", ""true"")
    .set(""spark.executor.memory"", ""24g"")
    .set(""spark.kryoserializer.buffer.max"",""1g"")
    .set(""spark.sql.codegen.wholeStage"", ""true"")
    .set(""spark.memory.offHeap.enabled"", ""true"")
    .set(""spark.memory.offHeap.size"", ""25769803776"") // 24 GB

Some of these are in fact default configurations. Some are not.

Michael


to be highly suboptimal. I don't know if this affects your benchmarks, but I would consider running some tests with tuned and alternate configurations.
we run and prep Spark tests only, driver 8g, executor memory 16g, Kryo, 0.66 memory fraction, 100 trials 
iterations over the weekend once we get the dedicated time again 
<mailto:michael@videoamp.com>> 

core regression 
can see them? If not, can you make them available? 
previous releases, unfortunately there are no Spark 2.0 compatible versions of HiBench and SparkPerf apart from those I'm working on (see https://github.com/databricks/spark-perf/issues/108 <https://github.com/databricks/spark-perf/issues/108>) 
regression with a very small scale factor and so we've generated a couple of profiles comparing 1.5.2 vs 2.0.0. Same JDK version and same platform. We will gather a 1.6.2 comparison and increase the scale factor. 
Spark 2.0 are very limited and AFAIK don't interfere with Spark core functionality, so any feedback on the changes would be much appreciated and welcome, I'd much prefer it if my changes are the problem. 
mentioned on the SparkPerf issue above) 
-Xdisableexplicitgc -Xcompressedrefs 
not wordStream.foreach
instead of awaitTermination
follows, the percentage is how much of the overall processing time was spent in this particular method: 
by the 1.5.2 time and then the 2.0.0 time
through scala-count-w-filtr, including all inbetween). 
number 741598. 
PO6 3AU 
number 741598. 
PO6 3AU

"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 8 Jul 2016 17:22:05 +0100",Re: Spark 2.0.0 performance; potential large Spark core regression,Michael Allman <michael@videoamp.com>,"Thanks Michael, we can give your options a try and aim for a 2.0.0 tuned 
vs 2.0.0 default vs 1.6.2 default comparison, for future reference the 
defaults in Spark 2 RC2 look to be:

sql.shuffle.partitions: 200
Tungsten enabled: true
Executor memory: 1 GB (we set to 18 GB)
kryo buffer max: 64mb
WholeStageCodegen: on I think, we turned it off when fixing a bug
offHeap.enabled: false
offHeap.size: 0

Cheers,




From:   Michael Allman <michael@videoamp.com>
To:     Adam Roberts/UK/IBM@IBMGB
Cc:     dev <dev@spark.apache.org>
Date:   08/07/2016 17:05
Subject:        Re: Spark 2.0.0 performance; potential large Spark core 
regression



Here are some settings we use for some very large GraphX jobs. These are 
based on using EC2 c3.8xl workers:

    .set(""spark.sql.shuffle.partitions"", ""1024"")
    .set(""spark.sql.tungsten.enabled"", ""true"")
    .set(""spark.executor.memory"", ""24g"")
    .set(""spark.kryoserializer.buffer.max"",""1g"")
    .set(""spark.sql.codegen.wholeStage"", ""true"")
    .set(""spark.memory.offHeap.enabled"", ""true"")
    .set(""spark.memory.offHeap.size"", ""25769803776"") // 24 GB

Some of these are in fact default configurations. Some are not.

Michael



Hi Adam,

highly suboptimal. I don't know if this affects your benchmarks, but I 
would consider running some tests with tuned and alternate configurations.

Michael



Hi Michael, the two Spark configuration files aren't very exciting 

spark-env.sh 
Same as the template apart from a JAVA_HOME setting 

spark-defaults.conf 
spark.io.compression.codec lzf 

config.py has the Spark home set, is running Spark standalone mode, we run 
and prep Spark tests only, driver 8g, executor memory 16g, Kryo, 0.66 
memory fraction, 100 trials 

We can post the 1.6.2 comparison early next week, running lots of 
iterations over the weekend once we get the dedicated time again 

Cheers, 





From:        Michael Allman <michael@videoamp.com> 
To:        Adam Roberts/UK/IBM@IBMGB 
Cc:        dev <dev@spark.apache.org> 
Date:        08/07/2016 16:44 
Subject:        Re: Spark 2.0.0 performance; potential large Spark core 
regression 



Hi Adam, 

Do you have your spark confs and your spark-env.sh somewhere where we can 
see them? If not, can you make them available? 

Cheers, 

Michael 


Hi, we've been testing the performance of Spark 2.0 compared to previous 
releases, unfortunately there are no Spark 2.0 compatible versions of 
HiBench and SparkPerf apart from those I'm working on (see 
https://github.com/databricks/spark-perf/issues/108) 

With the Spark 2.0 version of SparkPerf we've noticed a 30% geomean 
regression with a very small scale factor and so we've generated a couple 
of profiles comparing 1.5.2 vs 2.0.0. Same JDK version and same platform. 
We will gather a 1.6.2 comparison and increase the scale factor. 

Has anybody noticed a similar problem? My changes for SparkPerf and Spark 
2.0 are very limited and AFAIK don't interfere with Spark core 
functionality, so any feedback on the changes would be much appreciated 
and welcome, I'd much prefer it if my changes are the problem. 

A summary for your convenience follows (this matches what I've mentioned 
on the SparkPerf issue above) 

1. spark-perf/config/config.py : SCALE_FACTOR=0.05
No. Of Workers: 1
Executor per Worker : 1
Executor Memory: 18G
Driver Memory : 8G
Serializer: kryo 

2. $SPARK_HOME/conf/spark-defaults.conf: executor Java Options: 
-Xdisableexplicitgc -Xcompressedrefs 

Main changes I made for the benchmark itself 
Use Scala 2.11.8 and Spark 2.0.0 RC2 on our local filesystem 
MLAlgorithmTests use Vectors.fromML 
For streaming-tests in HdfsRecoveryTest we use wordStream.foreachRDD not 
wordStream.foreach 
KVDataTest uses awaitTerminationOrTimeout in a SparkStreamingContext 
instead of awaitTermination 
Trivial: we use compact not compact.render for outputting json

In Spark 2.0 the top five methods where we spend our time is as follows, 
the percentage is how much of the overall processing time was spent in 
this particular method: 
2.        SortShuffleWriter.write 19% 
3.        SizeTracker.estimateSize 7.5% 
4.        SizeEstimator.estimate 5.36% 
5.        Range.foreach 3.6% 

and in 1.5.2 the top five methods are: 
2.        ExternalSorter.insertAll 33% 
3.        Range.foreach 4% 
4.        SizeEstimator.estimate 2% 
5.        SizeEstimator.visitSingleObject 2% 

I see the following scores, on the left I have the test name followed by 
the 1.5.2 time and then the 2.0.0 time
scheduling throughput: 5.2s vs 7.08s
agg by key; 0.72s vs 1.01s
agg by key int: 0.93s vs 1.19s
agg by key naive: 1.88s vs 2.02
sort by key: 0.64s vs 0.8s
sort by key int: 0.59s vs 0.64s
scala count: 0.09s vs 0.08s
scala count w fltr: 0.31s vs 0.47s 

This is only running the Spark core tests (scheduling throughput through 
scala-count-w-filtr, including all inbetween). 

Cheers, 


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU 



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 8 Jul 2016 09:26:08 -0700",Re: Spark 2.0.0 performance; potential large Spark core regression,Adam Roberts <AROBERTS@uk.ibm.com>,"bq. we turned it off when fixing a bug

Adam:
Can you refer to the bug JIRA ?

Thanks


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 8 Jul 2016 09:35:37 -0700",Re: Spark performance regression test suite,Michael Allman <michael@videoamp.com>,"Found a few issues:

[SPARK-6810] Performance benchmarks for SparkR
[SPARK-2833] performance tests for linear regression
[SPARK-15447] Performance test for ALS in Spark 2.0

Haven't found one for Spark core.


"
Holden Karau <holden@pigscanfly.ca>,"Fri, 8 Jul 2016 09:50:47 -0700",Re: Spark performance regression test suite,Ted Yu <yuzhihong@gmail.com>,"There are also the spark-perf and spark-sql-perf projects in the Databricks
github (although I see an open issue for Spark 2.0 support in one of them).



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Eric Liang <ekl@databricks.com>,"Fri, 08 Jul 2016 19:28:43 +0000",Re: Spark performance regression test suite,"Holden Karau <holden@pigscanfly.ca>, Ted Yu <yuzhihong@gmail.com>","Something like speed.pypy.org
<http://speed.pypy.org/timeline/#/?exe=3,6,1,5&base=2+472&ben=grid&env=1&revs=200&equid=off>
or
the Chrome performance dashboards <https://chromeperf.appspot.com/> would
be very useful.


"
Ahmed Mahran <ahmahran@gmail.com>,"Sun, 10 Jul 2016 02:27:51 +0200",Reactive Spark: generalizing streaming API from micro-batches to event-based,dev@spark.apache.org,"Hi,

I'd like to present an idea about generalizing the legacy streaming API a
bit. The streaming API assumes an equi-frequent micro-batches model such
that streaming data are allocated and jobs are submitted into a batch every
fixed amount of time (aka batchDuration). This model could be extended a
bit; instead of generating a batch every batchDuration, batch generation
could be event based; such that Spark listens to event sources and
generates batches upon events. The equi-frequent micro-batches becomes
equivalent to a timer event source that fires a timer event every
batchDuration.

This allows a fine grain scheduling of Spark jobs. The same code could run
as either streaming or batching. With this model, a batch job could easily
be configured to run periodically. There would be no need to deploy or
configure an external scheduler (like Apache Oozie or linux crons). This
model easily allows jobs with dependencies that span across time, like
daily logs transformations concluded by weekly aggregations.

Please find more details at
https://github.com/mashin-io/rich-spark/blob/eventum-master/docs/reactive-spark-doc.md

I'd like to discuss maybe how worth this idea could be (especially given
the structured streaming API).

Looking forward to having your precious feedback.

Regards,
Ahmed Mahran
"
Fei Hu <hufei68@gmail.com>,"Sun, 10 Jul 2016 04:03:05 +0000",Spark application Runtime Measurement,dev@spark.apache.org,"Dear all,

I have a question about how to measure the runtime for a Spak application.
Here is an example:


   as following

[image: Screen Shot 2016-07-09 at 11.45.44 PM.png]

   - However, when I check the jobs launched by the application, the time
   is 13s + 0.8s + 4s = 17.8 seconds, which is much less than 120 seconds. I
   am not sure which time I should choose to measure the performance of the
   Spark application.

[image: Screen Shot 2016-07-09 at 11.48.26 PM.png]

   - I also check the event timeline as following. There is a big gap
   between the second job and the third job. I do not know what happened
   during that gap.

[image: Screen Shot 2016-07-09 at 11.53.29 PM.png]

Is there anyone who can help explain which time is the exact time to
measure the performance of a Spark application.

Thanks in advance,
Fei
"
jinhong lu <lujinhong2@gmail.com>,"Mon, 11 Jul 2016 11:35:51 +0800",mllib based on dataset or dataframe,"spark users <user@spark.apache.org>,
 dev@spark.apache.org","Hi,
    Since the DataSet will be the major API in spark2.0,  why mllib will DataFrame-based, and 'future development will focus on the DataFrame-based API.

   Any plan will change mllib form DataFrame-based to DataSet-based?


=============
Thanks,
lujinhong


---------------------------------------------------------------------


"
Yanbo Liang <ybliang8@gmail.com>,"Sun, 10 Jul 2016 22:14:22 -0700",Re: mllib based on dataset or dataframe,jinhong lu <lujinhong2@gmail.com>,"DataFrame is a kind of special case of Dataset, so they mean the same thing.
Actually the ML pipeline API will accept Dataset[_] instead of DataFrame in
Spark 2.0.
We can say that MLlib will focus on the Dataset-based API for futher
development more accurately.

Thanks
Yanbo

2016-07-10 20:35 GMT-07:00 jinhong lu <lujinhong2@gmail.com>:

d
"
Dmitry Zhukov <dzhukov@transferwise.com>,"Mon, 11 Jul 2016 12:07:30 +0300",SPARK-15465 - AnalysisException: cannot cast StructType to VectorUDT,dev <dev@spark.apache.org>,"Hi!

I want to bring this issue of Spark 2.0 here
https://issues.apache.org/jira/browse/SPARK-15465.
It looks quite major (I would even say critical) to me. Should it be fixed
within RC?

I would also like to contribute myself but struggle to find a place where
to start...

Thanks!

--
Dmitry Zhukov
TransferWise
"
Dmitry Zhukov <dzhukov@transferwise.com>,"Mon, 11 Jul 2016 12:15:57 +0300",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Koert Kuipers <koert@tresata.com>,"So, as I understand the correct git branch to maven version mapping should
be the following:

branch-2.0 -> 2.0.0-SNAPSHOT
master -> 2.1.0-SNAPSHOT

but the current is

branch-2.0 -> 2.0.1-SNAPSHOT
master -> 2.0.0-SNAPTHOT


We are starting to play with Spark 2.0 in TransferWise and find the
versioning of the development branches very confusing. Any plans to fix it?

Thanks!


"
Sean Owen <sowen@cloudera.com>,"Mon, 11 Jul 2016 10:19:40 +0100",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Dmitry Zhukov <dzhukov@transferwise.com>,"You are right, this is messed up a bit now.

branch-2.0 should really still be 2.0.0-SNAPSHOT, technically. I think
that was accidentally updated in the RC release. It won't matter a
whole lot except for people who consume snapshots, but, you can always
roll your own. After 2.0.0 it should be 2.0.1-SNAPSHOT anyway.

Master isn't done yet because of a hiccup in the API checking
component, MiMa. It should really be on 2.1.0-SNAPSHOT. At the latest
it will be so after 2.0.0 is released but it sorta looks like Reynold
maybe has an answer as of a few hours ago?

Sean


---------------------------------------------------------------------


"
Dmitry Zhukov <dzhukov@transferwise.com>,"Mon, 11 Jul 2016 12:32:15 +0300",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),Ted Yu <yuzhihong@gmail.com>,"Sorry for bringing this topic up. Any updates here?

Really looking forward to the upcoming RC.

Thanks!


"
Pete Robbins <robbinspg@gmail.com>,"Mon, 11 Jul 2016 10:08:30 +0000",Stability of branch-2.0,Dev <dev@spark.apache.org>,"It looks like the vote on 2.0-rc2 will not pass so there will be a new RC
from the 2.0 branch. With a project management hat on I would expect to see
only fixes to the remaining blocker issues or high priority bug fixes going
into the 2.0 branch as defect burn down. However, I see several new
functional PRs which were originally targeted at 2.1 being merged into
branch-2.0 (eg children of https://issues.apache.org/jira/browse/SPARK-16275
) and these will now be in the upcoming 2.0-RC3.

I assume these are zero risk changes that will not further delay a 2.0
release.

Cheers,
"
Sun Rui <sunrise_win@163.com>,"Mon, 11 Jul 2016 18:12:02 +0800",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"-1
https://issues.apache.org/jira/browse/SPARK-16379 <https://issues.apache.org/jira/browse/SPARK-16379>

<https://issues.apache.org/jira/browse/SPARK-16379>
"
Sean Owen <sowen@cloudera.com>,"Mon, 11 Jul 2016 12:33:18 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),Sun Rui <sunrise_win@163.com>,"Yeah there were already other blockers when the RC was released. This
one was already noted in this thread. There will another RC soon I'm
sure. I guess it would be ideal if the remaining blockers were
resolved one way or the other before that, to make it possible that
RC3 could be the final release:

SPARK-14808 Spark MLlib, GraphX, SparkR 2.0 QA umbrella
SPARK-14812 ML, Graph 2.0 QA: API: Experimental, DeveloperApi, final,
sealed audit
SPARK-14813 ML 2.0 QA: API: Python API coverage
SPARK-14816 Update MLlib, GraphX, SparkR websites for 2.0
SPARK-14817 ML, Graph, R 2.0 QA: Programming guide update and migration guide
SPARK-15124 R 2.0 QA: New R APIs and API docs
SPARK-15623 2.0 python coverage ml.feature
SPARK-15630 2.0 python coverage ml root module

These are possibly all or mostly resolved already and have been
knocking around a while.

In any event, even a DoA RC3 might be useful if it kept up the testing.

Sean


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 11 Jul 2016 12:37:59 +0100",Re: Stability of branch-2.0,Pete Robbins <robbinspg@gmail.com>,"I agree -- Wenchen/Reynold do you know what's the theory there?

TBH I think that there has not been a 'real' release candidate yet.
It's not that big a deal if these first two have been speculative RCs
to get more feedback earlier for a major release, and that in fact
people want to let this bake somewhat longer that the RC would imply.
As long as it's converging towards fewer, more critical changes.

Excepting these merges I think that had been generally happening. It's
*mostly* critical stuff now. But yeah this won't actually get released
until blockers are resolved and merges slow down to what belongs in a
maintenance branch.



---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 11 Jul 2016 08:59:46 -0700",Re: Spark performance regression test suite,Eric Liang <ekl@databricks.com>,"I second any effort to update, automate, and communicate the results of
spark-perf (https://github.com/databricks/spark-perf)




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Adam Roberts <AROBERTS@uk.ibm.com>,"Mon, 11 Jul 2016 17:26:35 +0100",Re: Spark performance regression test suite,Michael Gummelt <mgummelt@mesosphere.io>,"Agreed, this is something that we do regularly when producing our own 
Spark distributions in IBM and so it will be beneficial to share updates 
with the wider community, so far it looks like Spark 1.6.2 is the best out 
of the box on spark-perf and HiBench (of course this may vary for real 
workloads, individual applications and tuning efforts) but we have more 
2.0 tests to be performed and we're not aware of any regressions between 
previous versions except for perhaps with the Spark 2.0.0 post I made.

I'm looking for testing and feedback from any Spark gurus with my 2.0 
changes for spark-perf (have a look at the open issue Holden's mentioned: 
https://github.com/databricks/spark-perf/issues/108) and the same goes for 
HiBench (FWIW we see the same regression on HiBench too: 
https://github.com/intel-hadoop/HiBench/issues/221).

of the existing contribution process, an ideal solution IMO would involve 
an additional parameter for the Jenkins job that when ticked will result 
in a performance run being done with and without the change. As we don't 
have direct access to the Jenkins build button in the community, when 
contributing a change users could mark their change with something like 
@performance or ""jenkins performance test this please"". 

Alternatively the influential Spark folk could notice a change with a 
potential performance impact and have it tested accordingly. While 
microbenchmarks are useful it will be important to test the whole of 
Spark. Then there's also the use of tags in the JIRA - lots for us to work 
with if we wanted this.

This probably means the addition and therefore maintenance of dedicated 
machines in the build farm although this would highlight any regressions 
FAST as opposed to later on in the development cycle.

If there is indeed a regression we may have the fun task of binary 
chopping commits between 1.6.2 and now...again TBC but a real possibility, 
so interested to see if anybody else is doing regression testing and if 
they see a similar problem.

If we don't go down the ""benchmark as you contribute"" route, having such a 
suite will be perfect - it would clone the latest versions of each 
benchmark, build them for the current version of Spark (can identify the 
release from the pom), run the benchmarks we care about (let's say in 
Spark standalone mode with a couple of executors) and produce a geomean 
score - highlighting any significant deviations.

I'm happy to help with designing/reviewing this

Cheers,







From:   Michael Gummelt <mgummelt@mesosphere.io>
To:     Eric Liang <ekl@databricks.com>
Cc:     Holden Karau <holden@pigscanfly.ca>, Ted Yu <yuzhihong@gmail.com>, 
Michael Allman <michael@videoamp.com>, dev <dev@spark.apache.org>
Date:   11/07/2016 17:00
Subject:        Re: Spark performance regression test suite



I second any effort to update, automate, and communicate the results of 
spark-perf (https://github.com/databricks/spark-perf)

Something like speed.pypy.org or the Chrome performance dashboards would 
be very useful.

There are also the spark-perf and spark-sql-perf projects in the 
Databricks github (although I see an open issue for Spark 2.0 support in 
one of them).

Found a few issues:

[SPARK-6810] Performance benchmarks for SparkR
[SPARK-2833] performance tests for linear regression
[SPARK-15447] Performance test for ALS in Spark 2.0

Haven't found one for Spark core.

Hello,

I've seen a few messages on the mailing list regarding Spark performance 
concerns, especially regressions from previous versions. It got me 
thinking that perhaps an automated performance regression suite would be a 
worthwhile contribution? Is anyone working on this? Do we have a Jira 
issue for it?

I cannot commit to taking charge of such a project. I just thought it 
would be a great contribution for someone who does have the time and the 
chops to build it.

Cheers,

Michael
---------------------------------------------------------------------




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau




-- 
Michael Gummelt
Software Engineer
Mesosphere

Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Adam Roberts <AROBERTS@uk.ibm.com>,"Mon, 11 Jul 2016 17:42:30 +0100",Re: Spark 2.0.0 performance; potential large Spark core regression,Ted Yu <yuzhihong@gmail.com>,"Ted,

That bug was https://issues.apache.org/jira/browse/SPARK-15822 and only 
found as part of running an sql-flights application (not with the unit 
tests), I don't know if this has anything to do with the regression we're 
seeing

with HiBench (large profile, 25g executor memory, 4g driver), again we 
will be carefully checking how these benchmarks are being run and what 
difference the options and configurations can make

Cheers,




From:   Ted Yu <yuzhihong@gmail.com>
To:     Adam Roberts/UK/IBM@IBMGB
Cc:     Michael Allman <michael@videoamp.com>, dev <dev@spark.apache.org>
Date:   08/07/2016 17:26
Subject:        Re: Spark 2.0.0 performance; potential large Spark core 
regression



bq. we turned it off when fixing a bug

Adam:
Can you refer to the bug JIRA ?

Thanks

Thanks Michael, we can give your options a try and aim for a 2.0.0 tuned 
vs 2.0.0 default vs 1.6.2 default comparison, for future reference the 
defaults in Spark 2 RC2 look to be: 

sql.shuffle.partitions: 200 
Tungsten enabled: true 
Executor memory: 1 GB (we set to 18 GB) 
kryo buffer max: 64mb 
WholeStageCodegen: on I think, we turned it off when fixing a bug 
offHeap.enabled: false 
offHeap.size: 0 

Cheers, 




From:        Michael Allman <michael@videoamp.com> 
To:        Adam Roberts/UK/IBM@IBMGB 
Cc:        dev <dev@spark.apache.org> 
Date:        08/07/2016 17:05 
Subject:        Re: Spark 2.0.0 performance; potential large Spark core 
regression 



Here are some settings we use for some very large GraphX jobs. These are 
based on using EC2 c3.8xl workers: 

    .set(""spark.sql.shuffle.partitions"", ""1024"")
   .set(""spark.sql.tungsten.enabled"", ""true"")
   .set(""spark.executor.memory"", ""24g"")
   .set(""spark.kryoserializer.buffer.max"",""1g"")
   .set(""spark.sql.codegen.wholeStage"", ""true"")
   .set(""spark.memory.offHeap.enabled"", ""true"")
   .set(""spark.memory.offHeap.size"", ""25769803776"") // 24 GB

Some of these are in fact default configurations. Some are not. 

Michael



Hi Adam, 

highly suboptimal. I don't know if this affects your benchmarks, but I 
would consider running some tests with tuned and alternate configurations. 


Michael 



Hi Michael, the two Spark configuration files aren't very exciting 

spark-env.sh 
Same as the template apart from a JAVA_HOME setting 

spark-defaults.conf 
spark.io.compression.codec lzf 

config.py has the Spark home set, is running Spark standalone mode, we run 
and prep Spark tests only, driver 8g, executor memory 16g, Kryo, 0.66 
memory fraction, 100 trials 

We can post the 1.6.2 comparison early next week, running lots of 
iterations over the weekend once we get the dedicated time again 

Cheers, 





From:        Michael Allman <michael@videoamp.com> 
To:        Adam Roberts/UK/IBM@IBMGB 
Cc:        dev <dev@spark.apache.org> 
Date:        08/07/2016 16:44 
Subject:        Re: Spark 2.0.0 performance; potential large Spark core 
regression 



Hi Adam, 

Do you have your spark confs and your spark-env.sh somewhere where we can 
see them? If not, can you make them available? 

Cheers, 

Michael 


Hi, we've been testing the performance of Spark 2.0 compared to previous 
releases, unfortunately there are no Spark 2.0 compatible versions of 
HiBench and SparkPerf apart from those I'm working on (see 
https://github.com/databricks/spark-perf/issues/108) 

With the Spark 2.0 version of SparkPerf we've noticed a 30% geomean 
regression with a very small scale factor and so we've generated a couple 
of profiles comparing 1.5.2 vs 2.0.0. Same JDK version and same platform. 
We will gather a 1.6.2 comparison and increase the scale factor. 

Has anybody noticed a similar problem? My changes for SparkPerf and Spark 
2.0 are very limited and AFAIK don't interfere with Spark core 
functionality, so any feedback on the changes would be much appreciated 
and welcome, I'd much prefer it if my changes are the problem. 

A summary for your convenience follows (this matches what I've mentioned 
on the SparkPerf issue above) 

1. spark-perf/config/config.py : SCALE_FACTOR=0.05
No. Of Workers: 1
Executor per Worker : 1
Executor Memory: 18G
Driver Memory : 8G
Serializer: kryo 

2. $SPARK_HOME/conf/spark-defaults.conf: executor Java Options: 
-Xdisableexplicitgc -Xcompressedrefs 

Main changes I made for the benchmark itself 
Use Scala 2.11.8 and Spark 2.0.0 RC2 on our local filesystem 
MLAlgorithmTests use Vectors.fromML 
For streaming-tests in HdfsRecoveryTest we use wordStream.foreachRDD not 
wordStream.foreach 
KVDataTest uses awaitTerminationOrTimeout in a SparkStreamingContext 
instead of awaitTermination 
Trivial: we use compact not compact.render for outputting json

In Spark 2.0 the top five methods where we spend our time is as follows, 
the percentage is how much of the overall processing time was spent in 
this particular method: 
2.        SortShuffleWriter.write 19% 
3.        SizeTracker.estimateSize 7.5% 
4.        SizeEstimator.estimate 5.36% 
5.        Range.foreach 3.6% 

and in 1.5.2 the top five methods are: 
2.        ExternalSorter.insertAll 33% 
3.        Range.foreach 4% 
4.        SizeEstimator.estimate 2% 
5.        SizeEstimator.visitSingleObject 2% 

I see the following scores, on the left I have the test name followed by 
the 1.5.2 time and then the 2.0.0 time
scheduling throughput: 5.2s vs 7.08s
agg by key; 0.72s vs 1.01s
agg by key int: 0.93s vs 1.19s
agg by key naive: 1.88s vs 2.02
sort by key: 0.64s vs 0.8s
sort by key int: 0.59s vs 0.64s
scala count: 0.09s vs 0.08s
scala count w fltr: 0.31s vs 0.47s 

This is only running the Spark core tests (scheduling throughput through 
scala-count-w-filtr, including all inbetween). 

Cheers, 


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU 



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU 




Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 11 Jul 2016 09:44:52 -0700",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Sean Owen <sowen@cloudera.com>,"
Actually it's correct, even if at first sight it might appear wrong.

The tag that is being voted on (rc2) has the ""2.0.0"" release. So,
technically, anything after that tag is after 2.0.0, so that in case
the vote passes, there's no need to push a commit that ""fixes"" the
version number to be 2.0.1-SNAPSHOT.

If the event of an rc3, as part of preparing the rc, the version will
be reverted to ""2.0.0"" and a new commit added after the rc3 tag to
forward the version to 2.0.1-SNAPSHOT.

See commits:
4a55b2326c8cf50f772907a8b73fd5e7b3d1aa06
6e8fa86ebf30a9b850f4a66810d5d38d1f188b33

-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 11 Jul 2016 09:46:00 -0700",Re: branch-2.0 is now 2.0.1-SNAPSHOT?,Sean Owen <sowen@cloudera.com>,"I just bumped master branch version to 2.1.0-SNAPSHOT
https://github.com/apache/spark/commit/ffcb6e055a28f36208ed058a42df09c154555332

We used to have a problem with binary compatibility check not having the
2.0.0 base version in Maven (because 2.0.0 hasn't been released yet) but I
figured out a way yesterday to work around it.


"
Sean Owen <sowen@cloudera.com>,"Mon, 11 Jul 2016 20:33:23 +0100",Re: KEYS file?,dev <dev@spark.apache.org>,"Eh, to anyone else who's ever pushed to the SVN-hosted
spark.apache.org site: are you able to commit anything right now? This
error is brand-new and has stumped me:

svn: E195023: Changing file
'/Users/srowen/Documents/asf-spark-site/downloads.md' is forbidden by
the server
svn: E175013: Access to
'/repos/asf/!svn/txr/1752209-12gpm/spark/downloads.md' forbidden

Maybe my perms got messed up, so, first checking to see if it affects
anyone else. FWIW this is all I'm trying to change; anyone is welcome
to commit this:


Index: downloads.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- downloads.md (revision 1752185)
+++ downloads.md (revision )
@@ -31,7 +31,7 @@

 4. Download Spark: <span id=""spanDownloadLink""></span>

-5. Verify this release using the <span id=""sparkDownloadVerify""></span>.
+5. Verify this release using the <span
id=""sparkDownloadVerify""></span> and [project release
KEYS](https://www.apache.org/dist/spark/KEYS).

 _Note: Scala 2.11 users should download the Spark source package and build
 [with Scala 2.11
support](http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211)._
Index: site/downloads.html
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- site/downloads.html (revision 1752185)
+++ site/downloads.html (revision )
@@ -213,7 +213,7 @@
     <p>Download Spark: <span id=""spanDownloadLink""></span></p>
   </li>
   <li>
-    <p>Verify this release using the <span
id=""sparkDownloadVerify""></span>.</p>
+    <p>Verify this release using the <span
id=""sparkDownloadVerify""></span> and <a
href=""https://www.apache.org/dist/spark/KEYS"">project release
KEYS</a>.</p>
   </li>
 </ol>





---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 11 Jul 2016 12:54:33 -0700",Re: KEYS file?,Sean Owen <sowen@cloudera.com>,"It's related to this apparently:
https://issues.apache.org/jira/servicedesk/customer/portal/1/INFRA-12055


"
Sean Owen <sowen@cloudera.com>,"Mon, 11 Jul 2016 21:52:20 +0100",Re: KEYS file?,Reynold Xin <rxin@databricks.com>,"Aha, that's landed. OK I'll figure it out tomorrow and push my update
to verify it all works.


---------------------------------------------------------------------


"
Priya Ch <learnings.chitturi@gmail.com>,"Tue, 12 Jul 2016 18:55:00 +0530",Send real-time alert using Spark,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi All,

 I am building Real-time Anomaly detection system where I am using k-means
to detect anomaly. Now in-order to send alert to mobile or an email alert
how do i send it using Spark itself ?

Thanks,
Padma CH
"
Marcin Tustin <mtustin@handybook.com>,"Tue, 12 Jul 2016 10:00:45 -0400",Re: Send real-time alert using Spark,Priya Ch <learnings.chitturi@gmail.com>,"Priya,

You wouldn't necessarily ""use spark"" to send the alert. Spark is in an
important sense one library among many. You can have your application use
any other library available for your language to send the alert.

Marcin



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Sean Owen <sowen@cloudera.com>,"Tue, 12 Jul 2016 15:01:28 +0100",Re: KEYS file?,Reynold Xin <rxin@databricks.com>,"PS I've already opened a test PR for the new apache/spark-website repo:

https://github.com/apache/spark-website/pull/1

I guess we'll follow the same process for reviewing there. Next: see
if the main merge script works for this repo!


---------------------------------------------------------------------


"
Priya Ch <learnings.chitturi@gmail.com>,"Tue, 12 Jul 2016 20:05:04 +0530",Re: Send real-time alert using Spark,Marcin Tustin <mtustin@handybook.com>,"I mean model training on incoming data is taken care by Spark. For detected
anomalies, need to send alert. Could we do this with Spark or any other
framework like Akka/REST API would do it ?

Thanks,
Padma CH


"
"""aka.fe2s"" <aka.fe2s@gmail.com>","Tue, 12 Jul 2016 19:54:15 +0300",Re: ml and mllib persistence,"dev@spark.apache.org, user@spark.apache.org","Okay, I think I found an answer on my question. Some models (for instance
org.apache.spark.mllib.recommendation.MatrixFactorizationModel) hold RDDs,
so just serializing these objects will not work.

--
Oleksiy Dyagilev


"
Reynold Xin <rxin@databricks.com>,"Tue, 12 Jul 2016 09:57:07 -0700",Re: ml and mllib persistence,"""aka.fe2s"" <aka.fe2s@gmail.com>","Also Java serialization isn't great for cross platform compatibility.


"
Salih Gedik <me@salih.xyz>,"Tue, 12 Jul 2016 20:01:40 +0300",Re: ml and mllib persistence,dev@spark.apache.org,"Hi Reynold,

I was wondering if you meant cross language or cross platform? Thanks

-- 
Salih Gedik

"
Reynold Xin <rxin@databricks.com>,"Tue, 12 Jul 2016 10:04:12 -0700",Re: ml and mllib persistence,Salih Gedik <me@salih.xyz>,"Platform as a general word, eg language platforms, OS, different JVM
versions, different JVM vendors, different Spark versions...


"
Salih Gedik <me@salih.xyz>,"Tue, 12 Jul 2016 20:16:59 +0300",Re: ml and mllib persistence,Reynold Xin <rxin@databricks.com>,"I see. I could not find any explanation about this. Could you tell me 
what sort of portability issue is this? Isn't JVM supposed to give the 
abstraction of that?

Thanks!



-- 
Salih Gedik

"
Niranda Perera <niranda.perera@gmail.com>,"Wed, 13 Jul 2016 09:45:41 +0530",Why isnt spark-yarn module is excluded from the spark parent pom?,"""dev@spark.apache.org"" <dev@spark.apache.org>, Reynold Xin <rxin@databricks.com>","Hi guys,

I could not find the spark-yarn module in the spark parent pom? Is there
any particular reason why this has been excluded?

Best
-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Sean Owen <sowen@cloudera.com>,"Wed, 13 Jul 2016 08:22:32 +0100",Re: Why isnt spark-yarn module is excluded from the spark parent pom?,Niranda Perera <niranda.perera@gmail.com>,"It's activated by a profile called 'yarn', like several other modules.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 13 Jul 2016 15:01:02 +0200",Re: Why isnt spark-yarn module is excluded from the spark parent pom?,Sean Owen <sowen@cloudera.com>,"And the reason is that not all Spark installations are for YARN as the
cluster manager.

Jacek

"
Noorul Islam Kamal Malmiyoda <noorul@noorul.com>,"Wed, 13 Jul 2016 22:26:25 +0530","Re: When worker is killed driver continues to run causing issues in
 supervise mode","user@spark.apache.org, dev@spark.apache.org","Adding dev list

"
Dongjoon Hyun <dongjoon@apache.org>,"Wed, 13 Jul 2016 12:05:51 -0700",Spark Homepage,dev <dev@spark.apache.org>,"Hi, All.

Currently, Spark Homepage (http://spark.apache.org/) shows file listing
(containing md files)
Is there any maintenance operation on that? :)

Warmly,
Dongjoon.
"
Holden Karau <holden@pigscanfly.ca>,"Wed, 13 Jul 2016 12:07:21 -0700",Re: Spark Homepage,Dongjoon Hyun <dongjoon@apache.org>,"This has also been reported on the user@ by a few people - other apache
projects (arrow & hadoop) don't seem to be affected so maybe it was a just
bad update for the Spark website?





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Jul 2016 12:09:35 -0700",Re: Spark Homepage,Holden Karau <holden@pigscanfly.ca>,"It's related to
https://issues.apache.org/jira/servicedesk/agent/INFRA/issue/INFRA-12055




"
Dongjoon Hyun <dongjoon@apache.org>,"Wed, 13 Jul 2016 12:10:21 -0700",Re: Spark Homepage,Holden Karau <holden@pigscanfly.ca>,"Oh, thank you, Holden.
Next time, I'll search user@ first before asking here. :)

Dongjoon


"
Dongjoon Hyun <dongjoon@apache.org>,"Wed, 13 Jul 2016 12:12:21 -0700",Re: Spark Homepage,Reynold Xin <rxin@databricks.com>,"Thank you for the pointer, Reynold!
I see the situation now.


"
Matthias Niehoff <matthias.niehoff@codecentric.de>,"Wed, 13 Jul 2016 22:35:13 +0200",Structured Streaming and Microbatches,"user <user@spark.apache.org>, dev@spark.apache.org","Hi everybody,

as far as I understand with new the structured Streaming API the output
will not get processed every x seconds anymore. Instead the data will be
processed as soon as is arrived. But there might be a delay due to
processing time for the data.

A small example:
Data comes in and the processing takes 1 second (quite long)
In this 1 second a lot of new data come in which will be processed after
the processing of the first data finished.

My questions are:
Is the data for each processing, i.e all the data collected in the 1 second
still processed as a microbatch (included reprocessing in case of failure
on another worker, etc.)? Or is the bulk of data processed one by one?

With regards to the processing time: is the behavior the same for high
processing times as in spark 1.x? Meaning we get a scheduling delay, data
is stored by a receiver,.. (is there even a concept of receiver in Spark 2?
Is a source in streaming basically a receiver?)

Hope those questions aren’t to confusing :-)

Thank you!
"
Jacek Laskowski <jacek@japila.pl>,"Wed, 13 Jul 2016 23:12:05 +0200",Re: Structured Streaming and Microbatches,Matthias Niehoff <matthias.niehoff@codecentric.de>,"Hi,

It's still microbatching architecture with triggers as batchIntervals. It's
just faster by default and the API is more pleasant, i.e. Dataset-driven.

Jacek

Hi everybody,

as far as I understand with new the structured Streaming API the output
will not get processed every x seconds anymore. Instead the data will be
processed as soon as is arrived. But there might be a delay due to
processing time for the data.

A small example:
Data comes in and the processing takes 1 second (quite long)
In this 1 second a lot of new data come in which will be processed after
the processing of the first data finished.

My questions are:
Is the data for each processing, i.e all the data collected in the 1 second
still processed as a microbatch (included reprocessing in case of failure
on another worker, etc.)? Or is the bulk of data processed one by one?

With regards to the processing time: is the behavior the same for high
processing times as in spark 1.x? Meaning we get a scheduling delay, data
is stored by a receiver,.. (is there even a concept of receiver in Spark 2?
Is a source in streaming basically a receiver?)

Hope those questions aren’t to confusing :-)

Thank you!
"
mateo7 <mateusz.boryn@gmail.com>,"Thu, 14 Jul 2016 06:37:20 -0700 (MST)",DataFrameWriter.insertInto() to table with Avro schema by URL,dev@spark.apache.org,"There is a Hive table with Avro schema specified by URL pointing to file in
HDFS.

CREATE EXTERNAL TABLE IF NOT EXISTS my_table
      ROW FORMAT SERDE
            'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
          STORED AS INPUTFORMAT
            'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
          OUTPUTFORMAT
            'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
          LOCATION 'hdfs:///tmp/my_table'
          TBLPROPERTIES (
            'avro.schema.url'='hdfs:///tmp/my_table.avsc'
-- contents of hdfs:///tmp/my_table.avsc is the same as below
--            'avro.schema.literal'='{   ""type"": ""record"",   ""name"":
""my_table"",   ""fields"": [     {""type"": ""string"", ""name"": ""KEY""},    
{""type"": ""string"", ""name"": ""VALUE""}   ] }'
          )
    ;

DESCRIBE EXTENDED my_table;


I'm trying to save DataFrame to that table with this line:

    import org.apache.spark.sql.SaveMode
    val location = ...
    var df = sqlContext.read.format(""csv"").option(""delimiter"",
""\t"").load(location)
    df = df.withColumnRenamed(""C0"", ""KEY"").withColumnRenamed(""C1"", ""VALUE"")
   
df.write.mode(SaveMode.Append).format(""com.databricks.spark.avro"").insertInto(""my_table"")

However, there is NullPointerException thrown from spark/hive internals. I
found that
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.newSerializer(tableDesc:
TableDesc) has call to

    serializer.initialize(null, tableDesc.getProperties)

So Avro Serde is initialized with null configuration. It seems configuration
is sometimes required, like for accessing schema present in HDFS.
Is null passed there on purpose? For comparison there is
org.apache.spark.sql.hive.orc.OrcOutputWriter class which passes non-null
configuration object.
Is there any workaround for this problem? Literal Avro schema is not an
option for me.


Hive 1.2.1, Spark 1.6.2

16/07/14 13:27:04 WARN AvroSerDe: Encountered exception determining schema.
Returning signal schema to indicate problem
java.lang.NullPointerException
        at
org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:182)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:363)
        at
org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFromFS(AvroSerdeUtils.java:131)
        at
org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:112)
        at
org.apache.hadoop.hive.serde2.avro.AvroSerDe.determineSchemaOrReturnErrorSchema(AvroSerDe.java:167)
        at
org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:103)
        at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.newSerializer(InsertIntoHiveTable.scala:59)
        at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.outputClass$lzycompute(InsertIntoHiveTable.scala:53)
        at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.outputClass(InsertIntoHiveTable.scala:53)
        at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult$lzycompute(InsertIntoHiveTable.scala:201)
        at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult(InsertIntoHiveTable.scala:127)
        at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.doExecute(InsertIntoHiveTable.scala:276)
        at
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)




--

---------------------------------------------------------------------


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Thu, 14 Jul 2016 16:46:10 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),"Sean Owen <sowen@cloudera.com>, Sun Rui <sunrise_win@163.com>","I see that all blockers targeted for 2.0.0 have either been resolved or
downgraded. Do you have an ETA for the next RC?

Thanks,
Jonathan


:
"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Jul 2016 11:59:50 -0700",[VOTE] Release Apache Spark 2.0.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Sunday, July 17, 2016 at 12:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc4
(e5f8c1117e0c48499f54d62b556bc693435afae0).

This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
*https://repository.apache.org/content/repositories/orgapachespark-1192/
<https://repository.apache.org/content/repositories/orgapachespark-1192/>*

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs/


=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.

==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.


Note: There was a mistake made during ""rc3"" preparation, and as a result
there is no ""rc3"", but only ""rc4"".
"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Jul 2016 12:01:48 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of rc4.



"
Michael Gummelt <mgummelt@mesosphere.io>,"Thu, 14 Jul 2016 12:02:54 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Reynold Xin <rxin@databricks.com>,"-1

Pending resolution of https://issues.apache.org/jira/browse/SPARK-16522
(diagnosing now)




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 14 Jul 2016 20:20:33 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Just curious: Did we have an RC3? I don't remember seeing one.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 14 Jul 2016 20:21:04 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Oh nevermind, just noticed your note. Apologies.


"
Matthias Niehoff <matthias.niehoff@codecentric.de>,"Thu, 14 Jul 2016 22:56:53 +0200",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Nicholas Chammas <nicholas.chammas@gmail.com>,"Some of the programming guides in the docs only give me blank page (Spark
programming guide works. Streaming, DataFrame/SQL and Structured Streaming
do not).

2016-07-14 22:21 GMT+02:00 Nicholas Chammas <nicholas.chammas@gmail.com>:

n
sses
/
/>*
==========
==========
===================
===================
y
he
n
t


-- 
Matthias Niehoff | IT-Consultant | Agile Software Factory  | Consulting
codecentric AG | Zeppelinstr 2 | 76185 Karlsruhe | Deutschland
tel: +49 (0) 721.9595-681 | fax: +49 (0) 721.9595-666 | mobil: +49 (0)
172.1702676
www.codecentric.de | blog.codecentric.de | www.meettheexperts.de |
www.more4fi.de

Sitz der Gesellschaft: Solingen | HRB 25917| Amtsgericht Wuppertal
Vorstand: Michael Hochgürtel . Mirko Novakovic . Rainer Vehns
Aufsichtsrat: Patric Fedlmeier (Vorsitzender) . Klaus Jäger . Jürgen Schütz

Diese E-Mail einschließlich evtl. beigefügter Dateien enthält vertrauliche
und/oder rechtlich geschützte Informationen. Wenn Sie nicht der richtige
Adressat sind oder diese E-Mail irrtümlich erhalten haben, informieren Sie
bitte sofort den Absender und löschen Sie diese E-Mail und evtl.
beigefügter Dateien umgehend. Das unerlaubte Kopieren, Nutzen oder Öffnen
evtl. beigefügter Dateien sowie die unbefugte Weitergabe dieser E-Mail ist
nicht gestattet
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 14 Jul 2016 14:28:48 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Matthias Niehoff <matthias.niehoff@codecentric.de>,"I think the docs build was broken because of
https://issues.apache.org/jira/browse/SPARK-16553 - A fix has been
merged and we are testing it now

Shivaram

g
:
on
asses
:
/
/
==========
==========
n
===================
===================
ly
the
on
lt
rgen Schütz
lt vertrauliche
tige
en Sie
gefügter
 nicht

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 14 Jul 2016 21:47:40 +0000",RE: [VOTE] Release Apache Spark 2.0.0 (RC4),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","-1, due to unresolved https://issues.apache.org/jira/browse/SPARK-15899

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Thursday, July 14, 2016 12:00 PM
To: dev@spark.apache.org
Subject: [VOTE] Release Apache Spark 2.0.0 (RC4)

Please vote on"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Jul 2016 15:05:54 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Michael Gummelt <mgummelt@mesosphere.io>,"So the mesos issue (SPARK-16522) Michael is investigating. Unless it is
actually failing Mesos, I wouldn't call it a blocker.

The file path thing Alex brought up (SPARK-15899) only impacts test cases
on Windows. I think it's important to fix, but definitely not a blocker
either.

For the doc changes (SPARK-16553) I will republish the docs, but I won't
build another rc unless this vote does not go through, since docs are
published separately from release artifacts, as outlined in the original
email.




"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Jul 2016 16:25:51 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Updated documentation at
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs-updated/




"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 15 Jul 2016 01:25:31 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","-1

There is a typo here
<http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs-updated/sql-programming-guide.html#nan-semantics>
:

There is specially handling for not-a-number (NaN)…

Just kidding, of course (about the vote). :)

I vo"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 15 Jul 2016 02:56:51 +0000",RE: [VOTE] Release Apache Spark 2.0.0 (RC4),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","-1

Breaks the existing applications while using the Script Transformation in Spark SQL, as the default Record/Column delimiter class changed since we don’t get the default conf value from HiveConf any more, see SPARK-16515;

This is a regression.
"
=?UTF-8?B?SXNtYcOrbCBNZWrDrWE=?= <iemejia@gmail.com>,"Fri, 15 Jul 2016 14:29:18 +0200",spark-packages with maven,dev@spark.apache.org,"Hello, I would like to know if there is an easy way to package a new
spark-package
with maven, I just found this repo, but I am not an sbt user.

https://github.com/databricks/sbt-spark-package

do
you need to include in a spark-package (any special file, manifest, etc) ? I
have not found any doc in the website.

Thanks,
Ismael
"
Sean Owen <sowen@cloudera.com>,"Fri, 15 Jul 2016 14:59:25 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Reynold Xin <rxin@databricks.com>,"Signatures and hashes are OK. I built and ran tests successfully on
Ubuntu 16 + Java 8 with ""-Phive -Phadoop-2.7 -Pyarn"". Although I
encountered a few tests failures, none were repeatable.

Regarding other issues brought up so far:

SPARK-16522
Does not seem quite enough to be a blocker if it's just an error at
shutdown that does not affect the result. If there's another RC, worth
fixing.
SPARK-15899
tests. Not a regression.
SPARK-16515
Not sure but Cheng please mark it a Blocker if you're pretty confident
it must be fixed.

Davies marked SPARK-16011 a Blocker, though should confirm that it's
for 2.0.0. That's the only one officially open now.

So I suppose that's provisionally a -1 from me as it's not clear there
aren't blocking issues. It's close, and this should be tested by
everyone.


Remaining Critical issues are below. I'm still uncomfortable with
documentation issues for 2.0 not being done before 2.0. If anyone's
intent is to release and then finish the docs a few days later, I'd
vote against that. There's just no rush that makes that make sense.

However it's entirely possible that the remaining work is not
essential for 2.0; I don't know. These should be retitled then. But to
make this make sense, one or the other needs to happen. ""Audit"" JIRAs
are similar, especially before a major release.


SPARK-13393 Column mismatch issue in left_outer join using Spark DataFrame
SPARK-13753 Column nullable is derived incorrectly
SPARK-13959 Audit MiMa excludes added in SPARK-13948 to make sure none
are unintended incompatibilities
SPARK-14808 Spark MLlib, GraphX, SparkR 2.0 QA umbrella
SPARK-14816 Update MLlib, GraphX, SparkR websites for 2.0
SPARK-14817 ML, Graph, R 2.0 QA: Programming guide update and migration guide
SPARK-14823 Fix all references to HiveContext in comments and docs
SPARK-15340 Limit the size of the map used to cache JobConfs to void OOM
SPARK-15393 Writing empty Dataframes doesn't save any _metadata files
SPARK-15703 Spark UI doesn't show all tasks as completed when it should
SPARK-15944 Make spark.ml package backward compatible with spark.mllib vectors
SPARK-16032 Audit semantics of various insertion operations related to
partitioned tables
SPARK-16090 Improve method grouping in SparkR generated docs
SPARK-16301 Analyzer rule for resolving using joins should respect
case sensitivity setting


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 15 Jul 2016 10:43:41 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Sean Owen <sowen@cloudera.com>,"Hashes, sigs match. I built and ran tests with Hadoop 2.3 (""-Pyarn
-Phadoop-2.3 -Phive -Pkinesis-asl -Phive-thriftserver""). I couldn't
get the following tests to pass but I think it might be something
specific to my setup as Jenkins on branch-2.0 seems quite stable.

[error] Failed tests:
[error] org.apache.spark.sql.hive.client.VersionsSuite
[error] org.apache.spark.sql.hive.HiveSparkSubmitSuite
[error] Error during tests:
[error] org.apache.spark.sql.hive.HiveExternalCatalogSuite

Regarding the open issues, I agree with Sean that most of them seem
minor to me and not worth blocking a release for. It would be good to
get more details on SPARK-16011 though

As for the docs, ideally we should have them in place before the RC
but given that this is a recurring issue I'm wondering if having a
separate updatable link (like the 2.0.0-rc4-updated that Reynold
posted yesterday) can be used. The semantics we could then have are
that the docs should be ready when the vote succeeds rather than being
ready when the vote starts.

Thanks
Shivaram


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 15 Jul 2016 19:48:44 +0200",Re: spark-packages with maven,=?UTF-8?B?SXNtYcOrbCBNZWrDrWE=?= <iemejia@gmail.com>,"+1000

Thanks Ismael for bringing this up! I meant to have send it earlier too
since I've been struggling with a sbt-based Scala project for a Spark
package myself this week and haven't yet found out how to do local
publishing.

If such a guide existed fo"
Reynold Xin <rxin@databricks.com>,"Fri, 15 Jul 2016 11:08:18 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"I just retargeted SPARK-16011 to 2.1.



"
ram kumar <ramkumarrock3@gmail.com>,"Sat, 16 Jul 2016 00:01:30 +0530",Error starting HiveServer2: could not start ThriftBinaryCLIService,dev@spark.apache.org,"Hi all,

I started Hive Thrift Server with command,

/sbin/start-thriftserver.sh --master yarn -hiveconf
hive.server2.thrift.port 10003

The Thrift server started at the particular node without any error.


When doing the same, except pointing to different node to start the server,

./sbin/start-thriftserver.sh --master yarn --hiveconf
hive.server2.thrift.bind.host
DIFFERENT_NODE_IP --hiveconf hive.server2.thrift.port 10003

I am getting following error,

16/07/15 13:04:35 INFO service.AbstractService:
Service:ThriftBinaryCLIService is started.
16/07/15 13:04:35 INFO service.AbstractService: Service:HiveServer2 is
started.
16/07/15 13:04:35 INFO thriftserver.HiveThriftServer2: HiveThriftServer2
started
16/07/15 13:04:36 ERROR thrift.ThriftCLIService: Error:
org.apache.thrift.transport.TTransportException: Could not create
ServerSocket on address DIFFERENT_NODE_IP:10003.
    at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
    at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
    at
org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactory.java:236)
    at
org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:69)
    at java.lang.Thread.run(Thread.java:745)

Can anyone help me with this?

Thanks,
Ram.
"
Burak Yavuz <brkyvz@gmail.com>,"Fri, 15 Jul 2016 11:32:05 -0700",Re: spark-packages with maven,Jacek Laskowski <jacek@japila.pl>,"Hi Ismael and Jacek,

If you use Maven for building your applications, you may use the
spark-package command line tool (
https://github.com/databricks/spark-package-cmd-tool) to perform packaging.
It requires you to build your jar using maven first, and then does all the
extra magic that Spark Package requires.

Please contact me directly if you have any issues.

Best,
Burak


r
ote:
?
"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 15 Jul 2016 13:12:50 -0700",Re: spark-packages with maven,Jacek Laskowski <jacek@japila.pl>,"
r
ote:
?

I was under the impression that spark-packages was more like a place for
one to list/advertise their extensions,  but when you do spark submit with
--packages, it will use maven to resolve your package
and as long as it succeeds, it will use it (e.g. you can do mvn clean
install for your local packages, and use --packages with a spark server
running on that same machine).

something like

publishTo := Some(""Local Maven Repository"" at
""file://""+Path.userHome.absolutePath+""/.m2/repository"")



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
=?UTF-8?B?SXNtYcOrbCBNZWrDrWE=?= <iemejia@gmail.com>,"Fri, 15 Jul 2016 22:42:02 +0200",Re: spark-packages with maven,dev@spark.apache.org,"Thanks for the info Burak, I will check the repo you mention, do you know
concretely what is the 'magic' that spark-packages need or if is there any
document with info about it ?


:
rote:
h
bsolutePath+""/.m2/repository"")
"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 15 Jul 2016 14:50:02 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Reynold Xin <rxin@databricks.com>,"Can't find the ""spark-assembly-2.0.0-hadoop2.7.0.jar"" after compilation.
Usually it is in the assembly/target/scala-2.11
Has the packaging changed for 2.0.0 ?
Cheers
<k/>


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 15 Jul 2016 15:02:49 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Krishna Sankar <ksankar42@gmail.com>,"Yes.  https://github.com/apache/spark/pull/11796


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 15 Jul 2016 15:16:49 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Reynold Xin <rxin@databricks.com>,"+1 (non-binding, of course)

1. Compiled OS X 10.11.5 (El Capitan) OK Total time: 26:27 min
     mvn clean package -Pyarn -Phadoop-2.7 -DskipTests
2. Tested pyspark, mllib (iPython 4.0)
2.0 Spark version is 2.0.0
2.1. statistics (min,max,mean,Pearson,Spea"
james <yiazhou@gmail.com>,"Fri, 15 Jul 2016 22:08:38 -0700 (MST)",RE: [VOTE] Release Apache Spark 2.0.0 (RC4),dev@spark.apache.org,"-1

This bug SPARK-16515 in Spark 2.0 breaks our cases which can run on 1.6.



--

---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 18 Jul 2016 14:00:57 -0700",Re: Build changes after SPARK-13579,Reynold Xin <rxin@databricks.com>,"I just flailed on this a bit before finding this email.  Can someone please
update
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IDESetup




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Mon, 18 Jul 2016 15:26:25 -0700",Re: transtition SQLContext to SparkSession,Michael Armbrust <michael@databricks.com>,"Good idea.

https://github.com/apache/spark/pull/14252




"
Melissa Warnkin <missywarnkin@yahoo.com.INVALID>,"Mon, 18 Jul 2016 23:14:28 +0000 (UTC)",ApacheCon: Getting the word out internally,"""dev@spark.apache.org"" <dev@spark.apache.org>","ApacheCon: Getting the word out internally
Dear Apache Enthusiast,

As you are no doubt already aware, we will be holding ApacheCon in
Seville, Spain, the week of November 14th, 2016. The call for papers
(CFP) for this event is now open, and will remain open until
September 9th.

The event is divided into two parts, each with its own CFP. The first
part of the event, called Apache Big Data, focuses on Big Data
projects and related technologies.

Website: http://events.linuxfoundation.org/events/apache-big-data-europe
CFP:
http://events.linuxfoundation.org/events/apache-big-data-europe/program/cfp

The second part, called ApacheCon Europe, focuses on the Apache
Software Foundation as a whole, covering all projects, community
issues, governance, and so on.

Website: http://events.linuxfoundation.org/events/apachecon-europe
CFP: http://events.linuxfoundation.org/events/apachecon-europe/program/cfp

ApacheCon is the official conference of the Apache Software
Foundation, and is the best place to meet members of your project and
other ASF projects, and strengthen your project's community.

If your organization is interested in sponsoring ApacheCon, contact Rich Bowen
at evp@apache.org  ApacheCon is a great place to find the brightest
developers in the world, and experts on a huge range of technologies.

I hope to see you in Seville!
==================================

Melissaon behalf of the ApacheCon Team
"
Sean Owen <sowen@cloudera.com>,"Tue, 19 Jul 2016 09:07:42 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Reynold Xin <rxin@databricks.com>,"I think unfortunately at least this one is gonna block:
https://issues.apache.org/jira/browse/SPARK-16620

Good news is that just about anything else that's at all a blocker has
been resolved and there are only about 6 issues of any kind at all
targeted for 2.0. It seems very close.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 19 Jul 2016 09:18:34 +0100",Re: Build changes after SPARK-13579,Michael Gummelt <mgummelt@mesosphere.io>,"If the change is just to replace ""sbt assembly/assembly"" with ""sbt
package"", done. LMK if there are more edits.


---------------------------------------------------------------------


"
EarthsonLu <Earthson.Lu@gmail.com>,"Tue, 19 Jul 2016 01:36:28 -0700 (MST)",[SparkSQL][UDAF] CatalystTypeConverters for each update?,dev@spark.apache.org,"I just find that MutableAggregationBuffer.update will convert data for every
update, which is terrible when I use something like Map, Array.

It is hard to implement a collect_set udaf, which will be O(n^2) in this
convert semantic.

Any advice?



--

---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Tue, 19 Jul 2016 11:13:12 +0200",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Sean Owen <sowen@cloudera.com>,"@Sean Owen,
As we're not planning to implement DataSets in Python do you plan to revert
this Jira ?
https://issues.apache.org/jira/browse/SPARK-13594

2016-07-19 10:07 GMT+02:00 Sean Owen <sowen@cloudera.com>:

n
==========
==========
===================
"
Pete Robbins <robbinspg@gmail.com>,"Tue, 19 Jul 2016 11:03:12 +0000",Re: Spark 2.0.0 preview docs uploaded,Sean Owen <sowen@cloudera.com>,"Are there any 'work in progress' release notes for 2.0.0 yet? I don't see
anything in the rc docs like ""what's new"" or ""migration guide""?


"
nseggert <nicholas.eggert@target.com>,"Tue, 19 Jul 2016 07:20:42 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),dev@spark.apache.org,"Can I point out this guy? https://issues.apache.org/jira/browse/SPARK-15705

I managed to find a workaround, but this is still IMO a pretty significant
bug.



--

---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Tue, 19 Jul 2016 09:47:03 -0700",Re: spark-packages with maven,Luciano Resende <luckbr1975@gmail.com>,"Luciano,
afaik the spark-package-tool also makes it easy to upload packages to
spark-packages website. You are of course free to include any maven
coordinate in the --packages parameter

--jakob

y
e:
)
th

---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Tue, 19 Jul 2016 18:47:36 +0200",Re: transtition SQLContext to SparkSession,Reynold Xin <rxin@databricks.com>,"@Reynold Xin,
How this will work with Hive Support ?
SparkSession.sqlContext return HiveContext ?

2016-07-19 0:26 GMT+02:00 Reynold Xin <rxin@databricks.com>:
m>
:
le as
sy
both



-- 
Maciek Bryński

----------------------------------------------------"
Reynold Xin <rxin@databricks.com>,"Tue, 19 Jul 2016 10:01:26 -0700",Re: transtition SQLContext to SparkSession,=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Yes. But in order to access methods available only in HiveContext a user
cast is required.


be
so
se
e,
"
Michael Allman <michael@videoamp.com>,"Tue, 19 Jul 2016 10:58:19 -0700",Re: transtition SQLContext to SparkSession,Reynold Xin <rxin@databricks.com>,"Sorry Reynold, I want to triple check this with you. I'm looking at the `SparkSession.sqlContext` field in the latest 2.0 branch, and it appears that that val is set specifically to an instance of the `SQLContext` class. A cast to `HiveContext` will fail. Maybe there's a misunderstanding here. This is what I'm looking at:

https://github.com/apache/spark/blob/24ea875198ffcef4a4c3ba28aba128d6d7d9a395/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L122

Michael


user cast is required. 
<javascript:;>>:
<https://github.com/apache/spark/pull/14252>
<michael@databricks.com <javascript:;>>
should be
SparkSession, so
available as
be easy
to use
codebase,
independendly (both

"
Reynold Xin <rxin@databricks.com>,"Tue, 19 Jul 2016 11:02:21 -0700",Re: transtition SQLContext to SparkSession,Michael Allman <michael@videoamp.com>,"dropping user list

Yup I just took a look -- you are right.

What's the reason you'd need a HiveContext? The only method that
HiveContext has and SQLContext does not have is refreshTable. Given this is
meant for helping code transition, it might be easier to just use
SQLContext and change the places that use refreshTable?

In order for SparkSession.sqlContext to return an actual HiveContext, we'd
need to use reflection to create a HiveContext, which is pretty hacky.




s.
a395/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L122
y
"
Michael Gummelt <mgummelt@mesosphere.io>,"Tue, 19 Jul 2016 11:18:21 -0700",Re: Build changes after SPARK-13579,Sean Owen <sowen@cloudera.com>,"This line: ""build/sbt clean assembly""

should also be changed, right?





-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Michael Allman <michael@videoamp.com>,"Tue, 19 Jul 2016 11:30:32 -0700",Re: transtition SQLContext to SparkSession,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

So far we've been able to transition everything to `SparkSession`. I was just following up on behalf of Maciej.

Michael

HiveContext has and SQLContext does not have is refreshTable. Given this is meant for helping code transition, it might be easier to just use SQLContext and change the places that use refreshTable?
we'd need to use reflection to create a HiveContext, which is pretty hacky.
the `SparkSession.sqlContext` field in the latest 2.0 branch, and it appears that that val is set specifically to an instance of the `SQLContext` class. A cast to `HiveContext` will fail. Maybe there's a misunderstanding here. This is what I'm looking at:
https://github.com/apache/spark/blob/24ea875198ffcef4a4c3ba28aba128d6d7d9a395/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L122 <https://github.com/apache/spark/blob/24ea875198ffcef4a4c3ba28aba128d6d7d9a395/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L122>
user cast is required. 
<https://github.com/apache/spark/pull/14252>
<michael@databricks.com <>>
should be
SparkSession, so
available as
be easy
to use
codebase,
independendly (both

"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 19 Jul 2016 15:13:26 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Reynold Xin <rxin@databricks.com>,"+0

Our internal test suites seem mostly happy, except for SPARK-16632.
Since there's a somewhat easy workaround, I don't think it's a blocker
for 2.0.0.




-- 
Marcelo

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 19 Jul 2016 15:19:29 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Reynold Xin <rxin@databricks.com>,"-1 : The docs don't seem to be fully built (e.g.
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs/streaming-programming-guide.html
is a zero byte file currently) - although if this is a transient apache
issue no worries.




-- 
Cell"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Tue, 19 Jul 2016 22:23:20 +0000",Missing Exector Logs From Yarn After Spark Failure,"""'user@spark.apache.org'"" <user@spark.apache.org>,
	""'dev@spark.apache.org'"" <dev@spark.apache.org>","I am trying to find the root cause of recent Spark application failure in production. When the Spark application is running I can check NodeManager's yarn.nodemanager.log-dir property to get the Spark executor container logs.

The container has logs for both the running Spark applications

Here is the view of the container logs: drwx--x--- 3 yarn yarn 51 Jul 19 09:04 application_1467068598418_0209 drwx--x--- 5 yarn yarn 141 Jul 19 09:04 application_1467068598418_0210

But when the application is killed both the application logs are automatically deleted. I have set all the log retention setting etc in Yarn to a very large number. But still these logs are deleted as soon as the Spark applications are crashed.

Question: How can we retain these Spark application logs in Yarn for debugging when the Spark application is crashed for some reason.
"
Jonathan Kelly <jonathakamzn@gmail.com>,"Tue, 19 Jul 2016 22:26:48 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),"Holden Karau <holden@pigscanfly.ca>, Reynold Xin <rxin@databricks.com>","The docs link from Reynold's initial email is apparently no longer valid.
He posted an updated link a little later in this same thread.

http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs-updated/


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 19 Jul 2016 16:55:36 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),Jonathan Kelly <jonathakamzn@gmail.com>,"Ah in that case: 0





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Tue, 19 Jul 2016 18:23:58 -0700","HiveContext , difficulties in accessing tables in hive
 schema's/database's other than default database.","user@spark.apache.org, dev@spark.apache.org","Hi All,

I have been trying to access tables from other schema's , apart from
default , to pull data into dataframe.

i was successful in doing it using the default schema in hive database.
But when i try any other schema/database in hive, i am getting below
error.(Have also not seen any examples related to accessing tables in other
schema/Database apart from default).

16/07/19 18:16:06 INFO hive.metastore: Connected to metastore.
16/07/19 18:16:08 INFO storage.MemoryStore: Block broadcast_0 stored as
values in memory (estimated size 472.3 KB, free 472.3 KB)
16/07/19 18:16:08 INFO storage.MemoryStore: Block broadcast_0_piece0 stored
as bytes in memory (estimated size 39.6 KB, free 511.9 KB)
16/07/19 18:16:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0
in memory on localhost:41434 (size: 39.6 KB, free: 2.4 GB)
16/07/19 18:16:08 INFO spark.SparkContext: Created broadcast 0 from show at
sparkHive.scala:70
Exception in thread ""main"" java.lang.NoSuchMethodError:
org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Lorg/apache/hadoop/hive/ql/plan/TableDesc;Lorg/apache/hadoop/mapred/JobConf;)V
at
org.apache.spark.sql.hive.HadoopTableReader$.initializeLocalJobConfFunc(TableReader.scala:324)
at
org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:276)
at
org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:276)
at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)
at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)
at scala.Option.map(Option.scala:145)
at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:176)
at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:195)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
at
org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
at
org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
at
org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:190)
at
org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)
at
org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)
at
org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)
at
org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)
at
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)
at org.apache.spark.sql.DataFrame.org
$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1537)
at org.apache.spark.sql.DataFrame.org
$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1544)
at
org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1414)
at
org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1413)
at org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2138)
at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1413)
at org.apache.spark.sql.DataFrame.take(DataFrame.scala:1495)
at org.apache.spark.sql.DataFrame.showString(DataFrame.scala:171)
at org.apache.spark.sql.DataFrame.show(DataFrame.scala:394)
at org.apache.spark.sql.DataFrame.show(DataFrame.scala:355)
at org.apache.spark.sql.DataFrame.show(DataFrame.scala:363)
at sparkHive$delayedInit$body.apply(sparkHive.scala:70)
at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
at scala.App$$anonfun$main$1.apply(App.scala:71)
at scala.App$$anonfun$main$1.apply(App.scala:71)
at scala.collection.immutable.List.foreach(List.scala:318)
at
scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)
at scala.App$class.main(App.scala:71)
at sparkHive$.main(sparkHive.scala:7)

and PFB the pom.xml i am using through intelllij.

   <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.10</artifactId>
            <version>1.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.10</artifactId>
            <version>1.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.10</artifactId>
            <version>1.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-auth</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-yarn-api</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-core</artifactId>
            <version>1.2.1</version>
        </dependency>
        <dependency>
            <groupId>com.sksamuel.elastic4s</groupId>
            <artifactId>elastic4s-json4s_2.10</artifactId>
            <version>1.7.5</version>
        </dependency>

        <!--
http://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.10 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.10</artifactId>
            <version>1.6.0</version>
        </dependency>

        <!--
https://mvnrepository.com/artifact/org.apache.spark/spark-hive-thriftserver_2.10
-->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive-thriftserver_2.10</artifactId>
            <version>1.6.0</version>
        </dependency>

        <!-- http://mvnrepository.com/artifact/org.apache.hive/hive-jdbc -->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-jdbc</artifactId>
            <version>1.1.0</version>
        </dependency>
        <!--
https://mvnrepository.com/artifact/org.apache.hive/hive-metastore -->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-metastore</artifactId>
            <version>1.2.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-shims
-->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-shims</artifactId>
            <version>1.2.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-service
-->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-service</artifactId>
            <version>1.2.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-serde
-->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-serde</artifactId>
            <version>1.2.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-cli -->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-cli</artifactId>
            <version>1.2.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec
-->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>1.2.0</version>
        </dependency>


        <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-contrib
-->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-contrib</artifactId>
            <version>1.2.1</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-common
-->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-common</artifactId>
            <version>1.2.1</version>
        </dependency>


        <!-- required at run time by hive -->
        <dependency>
            <groupId>org.datanucleus</groupId>
            <artifactId>datanucleus-core</artifactId>
            <version>3.2.10</version>
        </dependency>
        <dependency>
            <groupId>org.datanucleus</groupId>
            <artifactId>datanucleus-rdbms</artifactId>
            <version>3.2.10</version>
        </dependency>


    </dependencies>

Any help or inputs would be appriciated.

Regards.
"
Reynold Xin <rxin@databricks.com>,"Tue, 19 Jul 2016 19:35:19 -0700",[VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc5
(13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).

This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1195/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/


=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.

==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
"
Reynold Xin <rxin@databricks.com>,"Tue, 19 Jul 2016 19:35:52 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of rc5.



"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Wed, 20 Jul 2016 17:01:43 +0300","Snappy initialization issue, spark assembly jar missing snappy classes?","user <user@spark.apache.org>, dev@spark.apache.org","Greetings!

We're reading input files with newApiHadoopFile that is configured with
multiline split. Everything's fine, besides
https://issues.apache.org/jira/browse/MAPREDUCE-6549. It looks like the
issue is fixed, but within hadoop 2.7.2. Which means we have to download
spark without hadoop and provide custom version of it. Now we use
spark-1.6.1.

It mostly fine, there is doc how to configure, spark started, but when I
use it it gives me nasty exception about snappy cannot be initialized. I
tried few things - update snappy version inside hadoop, package snappy into
my own application jar, but it works only when I literally copy
snappy-java.jar classes into spark-assembly-1.6.1-hadoop2.2.0.jar. It seems
working for now, but I dislike this approach, because I simply cannot know
what else won't work tomorrow.
It looks like I can just turn off snappy, but I want it, I believe it makes
sense to compress data shuffled and stored around.

Could you suggest any way besides copying these classes inside assembled
spark jar file?


The snappy exception
Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most
recent failure: Lost task 1.3 in stage 1.0 (TID 69,
icomputer.petersburg.epam.com): java.io.IOException:
java.lang.reflect.InvocationTargetException
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1222)
at
org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
at
org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
at
org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
at
org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
at
org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:72)
at
org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:65)
at org.apache.spark.broadcast.TorrentBroadcast.org
$apache$spark$broadcast$TorrentBroadcast$$setConf(TorrentBroadcast.scala:73)
at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:167)
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1219)
... 11 more
Caused by: java.lang.IllegalArgumentException:
java.lang.NoClassDefFoundError: Could not initialize class
org.xerial.snappy.Snappy
at
org.apache.spark.io.SnappyCompressionCodec$.liftedTree1$1(CompressionCodec.scala:171)
at
org.apache.spark.io.SnappyCompressionCodec$.org$apache$spark$io$SnappyCompressionCodec$$version$lzycompute(CompressionCodec.scala:168)
at
org.apache.spark.io.SnappyCompressionCodec$.org$apache$spark$io$SnappyCompressionCodec$$version(CompressionCodec.scala:168)
at
org.apache.spark.io.SnappyCompressionCodec.<init>(CompressionCodec.scala:152)
... 19 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class
org.xerial.snappy.Snappy
at
org.apache.spark.io.SnappyCompressionCodec$.liftedTree1$1(CompressionCodec.scala:169)
... 22 more
--
Be well!
Jean Morozov
"
Sean Owen <sowen@cloudera.com>,"Wed, 20 Jul 2016 16:50:58 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"+1 at last. Sigs and hashes check out, and compiles and passes tests
with ""-Pyarn -Phadoop-2.7 -Phive"" on Ubuntu 16 + Java 8.


There are actually only 2 issues still targeted for 2.0.0, which is great:
SPARK-16633 lag/lead does not return the default val"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 20 Jul 2016 09:06:12 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"+1

SHA and MD5 sums match for all binaries. Docs look fine this time
around. Built and ran `dev/run-tests` with Java 7 on a linux machine.

No blocker bugs on JIRA and the only critical bug with target as 2.0.0
is SPARK-16633, which doesn't look like a r"
Marcin Tustin <mtustin@handybook.com>,"Wed, 20 Jul 2016 12:10:29 -0400",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"Whatever happened with the query regarding benchmarks? Is that resolved?



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Michael Allman <michael@videoamp.com>,"Wed, 20 Jul 2016 09:18:30 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Marcin Tustin <mtustin@handybook.com>,"Marcin,

I'm not sure what you're referring to. Can you be more specific?

Cheers,

Michael

resolved?
version 2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes if a majority of at least 3 +1 PMC votes are cast.
(13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).
https://s.apache.org/spark-2.0.0-jira <https://s.apache.org/spark-2.0.0-jira>
at:
<http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/>
<https://people.apache.org/keys/committer/pwendell.asc>
https://repository.apache.org/content/repositories/orgapachespark-1195/ <https://repository.apache.org/content/repositories/orgapachespark-1195/>
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/ <http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/>
=========
=========
an existing Spark workload and running on this release candidate, then reporting any regressions from 1.x.
==================
==================
features will not necessarily block this release. Note that historically Spark documentation has been published on the website separately from the main release so we do not need to block the release due to documentation errors either.
<http://www.handy.com/careers>
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led by Fidelity

"
Marcin Tustin <mtustin@handybook.com>,"Wed, 20 Jul 2016 12:23:48 -0400",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Michael Allman <michael@videoamp.com>,"I refer to Maciej Bryński's (maciek@brynski.pl) emails of 29 and 30 June
2016 to this list. He said that his benchmarking suggested that Spark 2.0
was slower than 1.6.

I'm wondering if that was ever investigated, and if so if the speed is back
up, or not.


:
ses
=========
=========
==================
==================
e
m-in-round-led-by-fidelity/> led

-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Michael Allman <michael@videoamp.com>,"Wed, 20 Jul 2016 09:51:54 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,
 Marcin Tustin <mtustin@handybook.com>","In reference to https://issues.apache.org/jira/browse/SPARK-16320, the code path for reading data from parquet files has been refactored extensively. The fact that Maciej was testing performance on a table with 400 partitions makes me wonder if my PR for https://issues.apache.org/jira/browse/SPARK-15968 will make a difference for repeated queries on partitioned tables. That PR was merged into master and backported to 2.0. The commit short hash is d5d2457.

Maciej, can you rerun your test on your original dataset with a version of Spark 2.0 with that commit in it? And run it more than once? And ensure that when you compare your query performance for the first query, ensure that you're starting with a fresh spark-shell or spark-sql for each so caching is not a factor.

As for the issue with initial query performance on a partitioned table or query performance on an unpartitioned table being inferior, I can do a quick test to see if I can reproduce that issue on our end. Assuming there is a perf regression, I may be able to spend some time debugging today. I've spent a substantial amount of time debugging and optimizing parquet table query perf in Spark, and we've been using 2.0 for at least a month now. Not sure if I'll have time to dig that deep, though.

Michael


<mailto:maciek@brynski.pl>) emails of 29 and 30 June 2016 to this list. He said that his benchmarking suggested that Spark 2.0 was slower than 1.6.
back up, or not.
resolved?
version 2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes if a majority of at least 3 +1 PMC votes are cast.
(13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).
https://s.apache.org/spark-2.0.0-jira <https://s.apache.org/spark-2.0.0-jira>
at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/ <http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/>
<https://people.apache.org/keys/committer/pwendell.asc>
https://repository.apache.org/content/repositories/orgapachespark-1195/ <https://repository.apache.org/content/repositories/orgapachespark-1195/>
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/ <http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/>
=========
=========
an existing Spark workload and running on this release candidate, then reporting any regressions from 1.x.
==================
==================
features will not necessarily block this release. Note that historically Spark documentation has been published on the website separately from the main release so we do not need to block the release due to documentation errors either.
<http://www.handy.com/careers>
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led by Fidelity
<http://www.handy.com/careers>
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led by Fidelity

"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 20 Jul 2016 23:16:02 +0200",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Michael Allman <michael@videoamp.com>,"@Michael,
I answered in Jira and could repeat here.
I think that my problem is unrelated to Hive, because I'm using
read.parquet method.
I also attached some VisualVM snapshots to SPARK-16321 (I think I should
merge both issues)
And code profiling suggest"
Michael Allman <michael@videoamp.com>,"Wed, 20 Jul 2016 14:48:22 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"I've run some tests with some real and some synthetic parquet data with nested columns with and without the hive metastore on our Spark 1.5, 1.6 and 2.0 versions. I haven't seen any unexpected performance surprises, except that Spark 2.0 now does schema inference across all files in a partitioned parquet metastore table. Granted, you aren't using a metastore table, but maybe Spark does that for partitioned non-metastore tables as well.

Michael

read.parquet method.
should merge both issues)
performance.


---------------------------------------------------------------------


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Wed, 20 Jul 2016 21:52:41 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"Michael Allman <michael@videoamp.com>, =?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>","+1 (non-binding)

:

e
ote:
d
"
Joseph Gonzalez <joseph.e.gonzalez@gmail.com>,"Wed, 20 Jul 2016 15:18:17 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),dev@spark.apache.org,"+1

Sent from my iPad

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 20 Jul 2016 17:23:00 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"+1 (non-binding, of course)

1. Compiled OS X 10.11.5 (El Capitan) OK Total time: 24:07 min
     mvn clean package -Pyarn -Phadoop-2.7 -DskipTests
2. Tested pyspark, mllib (iPython 4.0)
2.0 Spark version is 2.0.0
2.1. statistics (min,max,mean,Pearson,Spea"
Dongjoon Hyun <dongjoon@apache.org>,"Wed, 20 Jul 2016 17:27:57 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)

- MD5/SHA/GPG matched.
- Test passed on Ubuntu (16.04) +  Oracle JDK (1.7.0_80) + R(3.2.3)
  * build/mvn -Phive -Phadoop-2.7 -Pyarn clean package
  * python python/run-tests.py
  * R/install-dev.sh & R/run-tests.sh

Cheers!

Dongjoon.


"
Reynold Xin <rxin@databricks.com>,"Wed, 20 Jul 2016 21:24:23 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Krishna Sankar <ksankar42@gmail.com>,1
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Thu, 21 Jul 2016 17:41:40 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non binding)

Tested PySpark Core, DataFrame/SQL, MLlib and Streaming on a standalone
cluster


"
Andrew Duffy <root@aduffy.org>,"Thu, 21 Jul 2016 19:10:04 +0100",master snapshots not publishing?,dev@spark.apache.org,"I’m trying to use a Snapshot build off of master, and after looking through
Jenkins it appears that the last commit where the snapshot was built is
back on 757dc2c09d23400dacac22e51f52062bbe471136, 22 days ago:
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-master-maven-snapshots/



Looking at the Jenkins page it says that the master-maven build is
disabled, is this purposeful?



-Andrew
"
Josh Rosen <joshrosen@databricks.com>,"Thu, 21 Jul 2016 19:52:38 +0000",Re: master snapshots not publishing?,"Andrew Duffy <root@aduffy.org>, dev@spark.apache.org","Yeah, it's on purpose: we had to disable it back when both the master and
branch-2.0 branches had the same versions in their POMs because that was
causing the master snapshots to overwrite the 2.0.0-SNAPSHOTS which are
generated off of branch-2.0.

I can go ahead and re-enable it later today.


ng
aster-maven-snapshots/
"
Andrew Duffy <root@aduffy.org>,"Thu, 21 Jul 2016 23:36:12 +0100",Re: master snapshots not publishing?,Josh Rosen <joshrosen@databricks.com>,"Gotcha, that'd be great!


ing
master-maven-snapshots/


-- 
Andrew Duffy
aduffy.org
"
Jacek Laskowski <jacek@japila.pl>,"Fri, 22 Jul 2016 07:31:02 +0200",BUILD broken - one hotfix ready for merging,dev <dev@spark.apache.org>,"Hi,

It seems that the current master is broken twice. I've just sent a PR
for the first one. Please review and merge.

https://github.com/apache/spark/pull/14315

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
=?UTF-8?Q?Mikael_St=C3=A5ldal?= <mikael.staldal@magine.com>,"Fri, 22 Jul 2016 13:55:57 +0200",Build error,dev@spark.apache.org,"I get this error when trying to build from Git master branch:

[ERROR] Failed to execute goal
net.alchim31.maven:scala-maven-plugin:3.2.2:doc-jar (attach-scaladocs) on
project spark-catalyst_2.11: MavenReportException: Error while creating
archive: wrap: Process exited with an error: 1 (Exit value: 1) -> [Help 1]

-- 
[image: MagineTV]

*Mikael Ståldal*
Senior software developer

*Magine TV*
mikael.staldal@magine.com
Grev Turegatan 3  | 114 46 Stockholm, Sweden  |   www.magine.com

Privileged and/or Confidential Information may be contained in this
message. If you are not the addressee indicated in this message
(or responsible for delivery of the message to such a person), you may not
copy or deliver this message to anyone. In such case,
you should destroy this message and kindly notify the sender by reply
email.
"
Jacek Laskowski <jacek@japila.pl>,"Fri, 22 Jul 2016 14:17:23 +0200",Re: Build error,=?UTF-8?Q?Mikael_St=C3=A5ldal?= <mikael.staldal@magine.com>,"Hi,

Fixed now. git pull and start over.

https://github.com/apache/spark/commit/e1bd70f44b11141b000821e9754efeabc14f24a5


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

.com>

]
t
"
=?UTF-8?Q?Mikael_St=C3=A5ldal?= <mikael.staldal@magine.com>,"Fri, 22 Jul 2016 16:54:59 +0200",Build speed,dev@spark.apache.org,"Is there any way to speed up an incremental build of Spark?

For me it takes 8 minutes to build the project with just a few code changes.

-- 
[image: MagineTV]

*Mikael Ståldal*
Senior software developer

*Magine TV*
mikael.staldal@magine.com
Grev Turegatan 3  | 114 46 Stockholm, Sweden  |   www.magine.com

Privileged and/or Confidential Information may be contained in this
message. If you are not the addressee indicated in this message
(or responsible for delivery of the message to such a person), you may not
copy or deliver this message to anyone. In such case,
you should destroy this message and kindly notify the sender by reply
email.
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 22 Jul 2016 07:58:32 -0700",Re: Build speed,=?UTF-8?Q?Mikael_St=C3=A5ldal?= <mikael.staldal@magine.com>,"I assume you have enabled Zinc.

Cheers

.com>

t
"
Michael Allman <michael@videoamp.com>,"Fri, 22 Jul 2016 08:49:57 -0700",Re: Build speed,=?utf-8?Q?Mikael_St=C3=A5ldal?= <mikael.staldal@magine.com>,"I use sbt. Rebuilds are super fast.

Michael

changes.
<http://www.magine.com/>
message. If you are not the addressee indicated in this message
not copy or deliver this message to anyone. In such case, 
email.   

"
VG <vlinked@gmail.com>,"Fri, 22 Jul 2016 21:48:36 +0530",Re: ml ALS.fit(..) issue,dev@spark.apache.org,"Dev team,

Can someone please help me here.

-VG


"
VG <vlinked@gmail.com>,"Fri, 22 Jul 2016 21:49:34 +0530","Re: Dataset , RDD zipWithIndex -- How to use as a map .","=?UTF-8?Q?Sergio_Fern=C3=A1ndez?= <wikier@apache.org>, 
	dev@spark.apache.org, User <user@spark.apache.org>","Hi All,

Any suggestions for this

Regards,
VG


"
dhruve ashar <dhruveashar@gmail.com>,"Fri, 22 Jul 2016 11:47:03 -0500",Re: Build speed,Michael Allman <michael@videoamp.com>,"If you are making changes to a specific project, you can just specify it
using -pl in mvn.

Small changes take ~2 mins to build.


om>
t


-- 
-Dhruve Ashar
"
VG <vlinked@gmail.com>,"Fri, 22 Jul 2016 23:01:20 +0530",Error in running JavaALSExample example from spark examples,"User <user@spark.apache.org>, dev@spark.apache.org","I am getting the following error

Exception in thread ""main"" java.lang.NoSuchMethodError:
scala.reflect.api.JavaUniverse.runtimeMirror(Ljava/lang/ClassLoader;)Lscala/reflect/api/JavaMirrors$JavaMirror;
at org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:452)

Any suggestions to resolve this

VG
"
VG <vlinked@gmail.com>,"Fri, 22 Jul 2016 23:13:40 +0530",Re: Error in running JavaALSExample example from spark examples,Aaron Ilovici <ailovici@wayfair.com>,"Using 2.0.0-preview using maven
So all dependencies should be correct I guess

<dependency><!-- Spark -->
<groupId>org.apache.spark</groupId>
<artifactId>spark-core_2.11</artifactId>
<version>2.0.0-preview</version>
<scope>provided</scope>
</dependency>

I see in maven dependencies that this brings in
scala-reflect-2.11.4
scala-compiler-2.11.0

and so on




"
Joseph Bradley <joseph@databricks.com>,"Fri, 22 Jul 2016 11:18:03 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

Mainly tested ML/Graph/R.  Perf tests from Tim Hunter showed minor speedups
from 1.6 for common ML algorithms.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 22 Jul 2016 11:38:20 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Joseph Bradley <joseph@databricks.com>,"+1

Tested on Mac.

Matei

speedups from 1.6 for common ML algorithms.
<ricardo.almeida@actnowib.com <mailto:ricardo.almeida@actnowib.com>> standalone cluster
count)
itertools OK
registerTempTable, sql OK
OrderDetails.OrderID,ShipCountry,UnitPrice,Qty,Dis"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 22 Jul 2016 12:45:33 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"+ 1 (non-binding)

Found a minor issue when trying to run some of the docker tests, but
nothing blocking the release. Will create a JIRA for that.




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Ramon Rosa da Silva <ramon.silva@neogrid.com>,"Fri, 22 Jul 2016 21:05:31 +0000",Spark jdbc update SaveMode,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Folks,

What do you think about allow update SaveMode from DataFrame.write.mode(""update"")?
Now Spark just has jdbc insert.


This e-mail message, including any attachments, is for the sole use of the person to whom it has been sent and may contain information that is confidential or legally protected. If you are not the intended recipient or have received this message in error, you are not authorized to copy, distribute, or otherwise use it or its attachments. Please notify the sender immediately by return email and permanently delete this message and any attachments. NeoGrid makes no warranty that this email is error or virus free. NeoGrid Europe Limited is a company registered in the United Kingdom with the registration number 7717968. The registered office is 8-10 Upper Marlborough Road, St Albans AL1 3UR, Hertfordshire, UK. NeoGrid Netherlands B.V. is a company registered in the Netherlands with the registration number 3416.6499 and registered office at Science Park 400, 1098 XH Amsterdam, NL. NeoGrid North America Limited is a company registered in the United States with the registration number 52-2242825. The registered office is 55 West Monroe Street, Suite 3590-60603, Chicago, IL, USA. NeoGrid Japan is located at New Otani Garden Court 7F, 4-1 Kioi-cho, Chiyoda-ku, Tokyo 102-0094, Japan. NeoGrid Software SA is a company registered in Brazil, with the registration number CNPJ: 03.553.145/0001-08 and located at Av. Santos Dumont, 935, 89.218-105, Joinville - SC - Brazil.

Esta mensagem pode conter informa??o confidencial ou privilegiada, sendo seu sigilo protegido por lei. Se voc? n?o for o destinat?rio ou a pessoa autorizada a receber esta mensagem, n?o pode usar, copiar ou divulgar as informa??es nela contidas ou tomar qualquer a??o baseada nessas informa??es. Se voc? recebeu esta mensagem por engano, por favor, avise imediatamente ao remetente, respondendo o e-mail e em seguida apague-a. Agradecemos sua coopera??o.
"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Fri, 22 Jul 2016 15:06:09 -0600",Re: ml ALS.fit(..) issue,VG <vlinked@gmail.com>,"The dev list is meant for working on development of Spark, not as a way of
escalating an issue just fyi.

If someone hasn't replied on the user list either you haven't given it
enough time or no one has a fix for you. I've definitely gotten replies
from committers multiple times to many questions so its definitely *not*
the case that they don't care




-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 909-353-4423
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Fri, 22 Jul 2016 23:17:54 +0200",Re: ml ALS.fit(..) issue,Pedro Rodriguez <ski.rodriguez@gmail.com>,"Seems like there is an incompatibility regarding scala versions between
your program and the scala version Spark was compiled against.
Either you're using scala 2.11 and your spark installation was built using
2.10 or the other way around.




-- 
Ben Fradet.
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Fri, 22 Jul 2016 23:37:28 +0200",Re: Spark jdbc update SaveMode,Ramon Rosa da Silva <ramon.silva@neogrid.com>,"2016-07-22 23:05 GMT+02:00 Ramon Rosa da Silva <ramon.silva@neogrid.com>:

I'm working on patch that creates new mode - 'upsert'.
In Mysql it will use 'REPLACE INTO' command.

M.

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Fri, 22 Jul 2016 14:42:35 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Luciano Resende <luckbr1975@gmail.com>,"+1 (non-binding)

Built locally on Ubuntu 14.04, basic pyspark sanity checking & tested with
a simple structured streaming project (spark-structured-streaming-ml) &
spark-testing-base & high-performance-spark-examples (minor changes
required from preview "
Michael Armbrust <michael@databricks.com>,"Fri, 22 Jul 2016 15:17:34 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>",1
Felix Cheung <felixcheung_m@hotmail.com>,"Sat, 23 Jul 2016 00:30:37 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

Tested on Ubuntu, ran a bunch of SparkR tests, found a broken link in doc but not a blocker.


_____________________________
From: Michael Armbrust <michael@databricks.com<mailto:michael@databricks.com>>
Sent: Friday, July 22, 2016 3:18 PM
Subject: Re"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Fri, 22 Jul 2016 18:25:22 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)

Tested data source api , and jdbc data sources. 


version 2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes if a majority of at least 3 +1 PMC votes are cast.
(13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).
https:/"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Sat, 23 Jul 2016 11:32:19 +0900",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Tested on my cluster with three slave nodes.


"
Xiao Li <gatorsmile@gmail.com>,"Fri, 22 Jul 2016 19:45:27 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),sarutak@oss.nttdata.co.jp,"+1

2016-07-22 19:32 GMT-07:00 Kousuke Saruta <sarutak@oss.nttdata.co.jp>:

"
Ewan Leith <ewan.leith@realitymine.com>,"Sat, 23 Jul 2016 05:24:09 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","I think this new issue in JIRA blocks the release unfortunately?

https://issues.apache.org/jira/browse/SPARK-16664 - Persist call on data frames with more than 200 columns is wiping out the data

Otherwise there'll need to be 2.0.1 pretty much right after?

Thanks,
Ewan

On 23 Jul 2016 03:46, Xiao Li <gatorsmile@gmail.com> wrote:
+1

2016-07-22 19:32 GMT-07:00 Kousuke Saruta <sarutak@oss.nttdata.co.jp<mailto:sarutak@oss.nttdata.co.jp>>:

+1 (non-binding)

Tested on my cluster with three slave nodes.


On 2016/07/23 10:25, Suresh Thalamati wrote:
+1 (non-binding)

Tested data source api , and jdbc data sources.


On Jul 19, 2016, at 7:35 PM, Reynold Xe:

Please vote on releasing the following candidate as Apache Spark version 2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc5 (13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).

This release candidate resolves ~2500 issues: https://s.apache.org/spark-2.0.0-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/<http://people.apache.org/%7Epwendell/spark-releases/spark-2.0.0-rc5-bin/>

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1195/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/<http://people.apache.org/%7Epwendell/spark-releases/spark-2.0.0-rc5-docs/>


=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions from 1.x.

==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new features will not necessarily block this release. Note that historically Spark documentation has been published on the website separately from the main release so we do not need to block the release due to documentation errors either.





"
Reynold Xin <rxin@databricks.com>,"Sat, 23 Jul 2016 02:05:47 -0400",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","The vote has passed with the following +1 votes and no -1 votes. I will
work on packaging the new release next week.


+1

Reynold Xin*
Sean Owen*
Shivaram Venkataraman*
Jonathan Kelly
Joseph E. Gonzalez*
Krishna Sankar
Dongjoon Hyun
Ricardo Almeida
Joseph Bradley*
Matei Zaharia*
Luciano Resende
Holden Karau
Michael Armbrust*
Felix Cheung
Suresh Thalamati
Kousuke Saruta
Xiao Li


* binding votes



Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc5
(13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).

This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1195/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/


=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.

==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
"
Reynold Xin <rxin@databricks.com>,"Sat, 23 Jul 2016 02:06:52 -0400",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),"Ewan Leith <ewan.leith@realitymine.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Ewan not sure if you wanted to explicitly -1 so I didn’t include you in
that.

I will document this as a known issue in the release notes. We have other
bugs that we have fixed since RC5, and we can fix those together in 2.0.1.


I think this new issue in JIRA blocks the release unfortunately?

https://issues.apache.org/jira/browse/SPARK-16664 - Persist call on data
frames with more than 200 columns is wiping out the data

Otherwise there'll need to be 2.0.1 pretty much right after?

Thanks,
Ewan


+1

2016-07-22 19:32 GMT-07:00 Kousuke Saruta <sarutak@oss.nttdata.co.jp>:

+1 (non-binding)

Tested on my cluster with three slave nodes.


+1 (non-binding)

Tested data source api , and jdbc data sources.



Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc5
(13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).

This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1195/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/


=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.

==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
"
Ewan Leith <ewan.leith@realitymine.com>,"Sat, 23 Jul 2016 07:42:25 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"Ok cool, i didn't vote as I've done no real testing myself and i think the window had already closed anyway.

I'm happy to wait for 2.0.1 for our systems.

Thanks,
Ewan

On 23 Jul 2016 07:07, Reynold Xin <rxin@databricks.com> wrote:
Ewan not sure if you wanted to explicitly -1 so I didn’t include you in that.

I will document this as a known issue in the release notes. We have other bugs that we have fixed since RC5, and we can fix those together in 2.0.1.


On July 22, 2016 at 10:24:32 PM, Ewan Leith (ewan.leith@realitymine.com<mailto:ewan.leith@realitymine.com>) wrote:

I think this new issue in JIRA blocks the release unfortunately?

https://issues.apache.org/jira/browse/SPARK-16664 - Persist call on data frames with more than 200 columns is wiping out the data

Otherwise there'll need to be 2.0.1 pretty much right after?

Thanks,
Ewan

On 23 Jul 2016 03:46, Xiao Li <gatorsmile@gmail.com<mailto:gatorsmile@gmail.com>> wrote:
+1

2016-07-22 19:32 GMT-07:00 Kousuke Saruta <sarutak@oss.nttdata.co.jp<mailto:sarutak@oss.nttdata.co.jp>>:

+1 (non-binding)

Tested on my cluster with three slave nodes.


On 2016/07/23 10:25, Suresh Thalamati wrote:
+1 (non-binding)

Tested data source api , and jdbc data sources.


On Jul 19, 2016, at 7:35 PM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:

Please vote on releasing the following candidate as Apache Spark version 2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc5 (13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).

This release candidate resolves ~2500 issues: https://s.apache.org/spark-2.0.0-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/<http://people.apache.org/%7Epwendell/spark-releases/spark-2.0.0-rc5-bin/>

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1195/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/<http://people.apache.org/%7Epwendell/spark-releases/spark-2.0.0-rc5-docs/>


=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions from 1.x.

==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new features will not necessarily block this release. Note that historically Spark documentation has been published on the website separately from the main release so we do not need to block the release due to documentation errors either.






"
Koert Kuipers <koert@tresata.com>,"Sat, 23 Jul 2016 17:54:09 -0400",drop java 7 support for spark 2.1.x or spark 2.2.x,dev@spark.apache.org,"dropping java 7 support was considered for spark 2.0.x but we decided
against it.

ideally dropping support for a java version should be communicated far in
advance to facilitate the transition.

is this the right time to make that decision and start communicating it
(mailing list, jira, etc.)? perhaps for spark 2.1.x or spark 2.2.x?

my general sense is that most cluster admins have plans to migrate to java
8 before end of year. so that could line up nicely with spark 2.2
"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 23 Jul 2016 15:10:28 -0700",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Koert Kuipers <koert@tresata.com>,"Why the push to remove Java 7 support as soon as possible (which is how I
read your ""cluster admins plan to migrate by date X, so Spark should end
Java 7 support then, too"")?  First, I don't think we should be removing
Java 7 support until some time after all or nearly all relevant clusters
are actually no longer running on Java 6, and that targeting removal of
support at our best guess about when admins are just *planning* to migrate
isn't a very good idea.  Second, I don't see the significant difficulty or
harm in continuing to support Java 7 for a while longer.


"
Koert Kuipers <koert@tresata.com>,"Sat, 23 Jul 2016 18:50:28 -0400",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Mark Hamstra <mark@clearstorydata.com>,"i care about signalling it in advance mostly. and given the performance
differences we do have some interest in pushing towards java 8


Why the push to remove Java 7 support as soon as possible (which is how I
read your ""cluster admins plan to migrate by date X, so Spark should end
Java 7 support then, too"")?  First, I don't think we should be removing
Java 7 support until some time after all or nearly all relevant clusters
are actually no longer running on Java 6, and that targeting removal of
support at our best guess about when admins are just *planning* to migrate
isn't a very good idea.  Second, I don't see the significant difficulty or
harm in continuing to support Java 7 for a while longer.


"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 23 Jul 2016 15:59:54 -0700",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Koert Kuipers <koert@tresata.com>,"Sure, signalling well ahead of time is good, as is getting better
performance from Java 8; but do either of those interests really require
dropping Java 7 support sooner rather than later?

Now, to retroactively copy edit myself, when I previously wrote ""after all
or nearly all relevant clusters are actually no longer running on Java 6"",
I meant ""...no longer running on Java 7"".  We should be at a point now
where there aren't many Java 6 clusters left, but my sense is that there
are still quite a number of Java 7 clusters around, and that there will be
for a good while still.


"
Koert Kuipers <koert@tresata.com>,"Sat, 23 Jul 2016 19:35:28 -0400",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Mark Hamstra <mark@clearstorydata.com>,"They don't require dropping it sooner rather than later. But signalling in
some way that java 8 is (strongly) recommend would be good.


"
Dongjoon Hyun <dongjoon@apache.org>,"Sat, 23 Jul 2016 16:54:15 -0700",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Koert Kuipers <koert@tresata.com>,"Hi, All.

What about providing a official benchmark result between `Apache Spark on
JDK7` and `Apache Spark on JDK8`?
I think that is enough for this issue since we cannot drive users.
We had better let users choose one of JDK7/JDK8 for their own benefits.

Bests,
Dongjoon.




"
Sean Owen <sowen@cloudera.com>,"Sun, 24 Jul 2016 08:04:02 +0100",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Koert Kuipers <koert@tresata.com>,"I had favored this for 2.0 even, so favor it sooner than later.
The general shape of the argument is:

- supporting Java 7 is starting to pinch a little because of extra
builds and the inevitable gap between what the PR builder (7) tests
and what the later Java 8 tests runs show
- requiring Java 8 would allow a few minor cleanups in the code
- Java 7 is already EOL (no, I don't count paying Oracle for support)

but mostly importantly,
- anyone can keep using 2.0 if they want to use Java 7. This does
*not* require people to use Java 8. It requires them to use Java 8 to
use the very latest version.


---------------------------------------------------------------------


"
Ofir Manor <ofir.manor@equalum.io>,"Sun, 24 Jul 2016 10:34:43 +0300",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,Koert Kuipers <koert@tresata.com>,"BTW - ""signalling ahead of time"" is called deprecating, not dropping
support...
(personally I only use JDK 8 / Scala 2.11 so I'm for it)


Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
Steve Loughran <stevel@hortonworks.com>,"Sun, 24 Jul 2016 14:40:24 +0000",Re: drop java 7 support for spark 2.1.x or spark 2.2.x,,"

Sure, signalling well ahead of time is good, as is getting better performance from Java 8; but do either of those interests really require dropping Java 7 support sooner rather than later?

Now, to retroactively copy edit myself, when I previously wrote ""after all or nearly all relevant clusters are actually no longer running on Java 6"", I meant ""...no longer running on Java 7"".  We should be at a point now where there aren't many Java 6 clusters left, but my sense is that there are still quite a number of Java 7 clusters around, and that there will be for a good while still.




  1.  There are a lot of Hadoop clusters on Java 7; the migration to Java 8 there is pretty slow due to inertia of existing apps.
  2.  But...you can run Java 8 code in a Hadoop cluster, if you set up the JVM properly
  3.  And orgs playing with spark tend not to be the ones running older Hadoop versions

Ignoring runtime issues, mixing versions of: Java & Scala complicate the dev and test matrix; getting a Java 7 JDK to build and test on is getting harder and harder, so most developers are using Java 8. Even if using java 7 language, there's the differences in the JRE libraries. Scala code has it easier language wise, but there are enough diffs from 2.10 and 2.11 that again, life is complex. There's invariably a conflict between developers ""I want to play with the latest toys!"" and ops ""It worked in 2012, let's keep using it""

Meanwhile, Java 9 is coming, which is going to break a lot of things, log4j 1.x included.

This is going to be fairly traumatic all round, as log4j 2, while a better design, doesn't parse the 1.x properties files (https://blogs.apache.org/logging/entry/moving_on_to_log4j_2).

While Java 9 is something nobody will use in production this year, it's coming, and it's something which has a lot of consequences up the stack. Being able to stop worrying about java 7 backwards compatibility would save time which can now be directed to worrying about java 9. Indeed, the ASF builds team are being offered Java 9 EARs, along with Java 8u112""

---------------
From: Rory O'Donnell <rory.odonnell@oracle.com<mailto:rory.odonnell@oracle.com>>
Subject: Early Access builds of JDK 8u112 b03, JDK 9 b128 are available on java.net<http://java.net>
Date: 22 July 2016 at 10:30:19 BST
To: <struberg@yahoo.de<mailto:struberg@yahoo.de>>
Cc: <rory.odonnell@oracle.com<mailto:rory.odonnell@oracle.com>>, Dalibor Topic <dalibor.topic@oracle.com<mailto:dalibor.topic@oracle.com>>, Balchandra Vaidya <balchandra.vaidya@oracle.com<mailto:balchandra.vaidya@oracle.com>>, Muneer Kolarkunnu <abdul.kolarkunnu@oracle.com<mailto:abdul.kolarkunnu@oracle.com>>, <gavin@16degrees.com.au<mailto:gavin@16degrees.com.au>>, <builds@apache.org<mailto:builds@apache.org>>
Reply-To: <builds@apache.org<mailto:builds@apache.org>>


Hi Mark & Gavin,

Early Access b128 <https://jdk9.java.net/download/> for JDK 9 is available on java.net<http://java.net>, summary of  changes are listed here <http://www.java.net/download/java/jdk9/changes/jdk-9+128.html>.

Early Access b127 <https://jdk9.java.net/jigsaw/> (#5304) for JDK 9 with Project Jigsaw is available on java.net<http://java.net>, summary of changes are listed here <http://download.java.net/java/jigsaw/archive/127/binaries/jdk-9+127.html>

Early Access b03 <https://jdk8.java.net/download.html> for JDK 8u112 is available on java.net<http://java.net>, summary of  changes are listed here <http://www.java.net/download/java/jdk8u112/changes/jdk8u112-b03.html>

Alan Bateman posted new EA builds contain initial implementation of current proposals , more info [0]

  The jigsaw/jake forest has been updated with an initial
  implementation of the proposals that Mark brought to the
  jpms-spec-experts mailing list last week. For those that don't build
  from source then the EA build/downloads [1] has also been refreshed.


Rgds,Rory

[0] http://mail.openjdk.java.net/pipermail/jigsaw-dev/2016-July/008467.html
[1] https://jdk9.java.net/jigsaw/

--
Rgds,Rory O'Donnell
Quality Engineering Manager
Oracle EMEA , Dublin, Ireland

"
Josh Rosen <joshrosen@databricks.com>,"Mon, 25 Jul 2016 00:44:16 +0000",Re: master snapshots not publishing?,Andrew Duffy <root@aduffy.org>,"Should be back and building now:
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-master-maven-snapshots/1595/console

I see a 2.1.0-SNAPSHOT in
https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.11/,
so it looks like everything should be working.


d
king
-master-maven-snapshots/
"
kevin <kiss.kevin119@gmail.com>,"Mon, 25 Jul 2016 12:05:04 +0800",where I can find spark-streaming-kafka for spark2.0,"""user.spark"" <user@spark.apache.org>, ""dev.spark"" <dev@spark.apache.org>","hi,all :
I try to run example org.apache.spark.examples.streaming.KafkaWordCount , I
got error :
Exception in thread ""main"" java.lang.NoClassDefFoundError:
org/apache/spark/streaming/kafka/KafkaUtils$
at
org.apache.spark.examples.streaming.KafkaWordCount$.main(KafkaWordCount.scala:57)
at
org.apache.spark.examples.streaming.KafkaWordCount.main(KafkaWordCount.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:724)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException:
org.apache.spark.streaming.kafka.KafkaUtils$
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 11 more

so where I can find spark-streaming-kafka for spark2.0
"
kevin <kiss.kevin119@gmail.com>,"Mon, 25 Jul 2016 17:20:54 +0800",Re: where I can find spark-streaming-kafka for spark2.0,"""user.spark"" <user@spark.apache.org>, ""dev.spark"" <dev@spark.apache.org>","I have compile it from source code

2016-07-25 12:05 GMT+08:00 kevin <kiss.kevin119@gmail.com>:

"
kevin <kiss.kevin119@gmail.com>,"Mon, 25 Jul 2016 17:33:31 +0800",spark2.0 can't run SqlNetworkWordCount,"""user.spark"" <user@spark.apache.org>, ""dev.spark"" <dev@spark.apache.org>","hi,all:
I download spark2.0 per-build. I can run SqlNetworkWordCount test use :
bin/run-example org.apache.spark.examples.streaming.SqlNetworkWordCount
master1 9999

but when I use spark2.0 example source code SqlNetworkWordCount.scala and
build it to a jar bao with dependencies ( JDK 1.8 AND SCALA2.10)
when I use spark-submit to run it I got error:

16/07/25 17:28:30 INFO scheduler.JobScheduler: Starting job streaming job
1469438910000 ms.0 from job set of time 1469438910000 ms
Exception in thread ""streaming-job-executor-2"" java.lang.NoSuchMethodError:
scala.reflect.api.JavaUniverse.runtimeMirror(Ljava/lang/ClassLoader;)Lscala/reflect/api/JavaMirrors$JavaMirror;
at
main.SqlNetworkWordCount$$anonfun$main$1.apply(SqlNetworkWordCount.scala:67)
at
main.SqlNetworkWordCount$$anonfun$main$1.apply(SqlNetworkWordCount.scala:61)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
at
org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
at scala.util.Try$.apply(Try.scala:192)
at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:247)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at
org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:246)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
"
=?iso-8859-2?Q?Tomasz_Gaw=EAda?= <tomasz.gaweda@outlook.com>,"Mon, 25 Jul 2016 09:40:28 +0000",Odp.: spark2.0 can't run SqlNetworkWordCount,"kevin <kiss.kevin119@gmail.com>, user.spark <user@spark.apache.org>,
	dev.spark <dev@spark.apache.org>","Hi,

Please change Scala version to 2.11.  As far as I know, Spark packages are now build with Scala 2.11 and I've got other - 2.10 - version


________________________________
Od: kevin <kiss.kevin119@gmail.com>
Wysane: 25 lipca 2016 11:33
Do: user.spark; dev.spark
Temat: spark2.0 can't run SqlNetworkWordCount

hi,all:
I download spark2.0 per-build. I can run SqlNetworkWordCount test use : bin/run-example org.apache.spark.examples.streaming.SqlNetworkWordCount master1 9999

but when I use spark2.0 example source code SqlNetworkWordCount.scala and build it to a jar bao with dependencies ( JDK 1.8 AND SCALA2.10)
when I use spark-submit to run it I got error:

16/07/25 17:28:30 INFO scheduler.JobScheduler: Starting job streaming job 1469438910000 ms.0 from job set of time 1469438910000 ms
Exception in thread ""streaming-job-executor-2"" java.lang.NoSuchMethodError: scala.reflect.api.JavaUniverse.runtimeMirror(Ljava/lang/ClassLoader;)Lscala/reflect/api/JavaMirrors$JavaMirror;
at main.SqlNetworkWordCount$$anonfun$main$1.apply(SqlNetworkWordCount.scala:67)
at main.SqlNetworkWordCount$$anonfun$main$1.apply(SqlNetworkWordCount.scala:61)
at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
at scala.util.Try$.apply(Try.scala:192)
at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:247)
at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)
at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:246)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)




"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 25 Jul 2016 12:01:09 +0200",orc/parquet sql conf,dev <dev@spark.apache.org>,"Hi,

Assuming I have some data in both ORC/Parquet formats, and some complex workflow that eventually combine results of some queries on these datasets, I would like to get the best execution and looking at the default configs I noticed:

1) Vectorized query execution possible with Parquet only, can you confirm this is possible with the ORC format?

parameter spark.sql.parquet.enableVectorizedReader
[1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala <https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala>
Hive is assuming ORC, parameter hive.vectorized.execution.enabled
[2] https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution <https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution>

2) Enabling filter pushdown is by default true for Parquet only, why not also for ORC?
spark.sql.parquet.filterPushdown=true
spark.sql.orc.filterPushdown=false

3) Should I even try to process ORC format with Spark at it seems there is Parquet native support?


Thank you!

Best,
Ovidiu"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 25 Jul 2016 19:20:59 +0900",Re: orc/parquet sql conf,Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"For the question 1., It is possible but not supported yet. Please refer
https://github.com/apache/spark/pull/13775

Thanks!

2016-07-25 19:01 GMT+09:00 Ovidiu-Cristian MARCU <
ovidiu-cristian.marcu@inria.fr>:

"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 25 Jul 2016 13:46:39 +0200",Re: orc/parquet sql conf,Hyukjin Kwon <gurwls223@gmail.com>,"Thank you! Any chance for this work being reviewed and integrated with next Spark release?

Best,
Ovidiu
refer https://github.com/apache/spark/pull/13775 <https://github.com/apache/spark/pull/13775>
<ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>>:
complex workflow that eventually combine results of some queries on these datasets, I would like to get the best execution and looking at the default configs I noticed:
confirm this is possible with the ORC format?
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala <https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala>
https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution <https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution>
not also for ORC?
there is Parquet native support?

"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 25 Jul 2016 17:24:36 +0200",Spark RC5 - OutOfMemoryError: Requested array size exceeds VM limit,dev <dev@spark.apache.org>,"Hi,

I am running some tpcds queries (data is Parquet stored in hdfs) with spark 2.0 rc5 and for some queries I get this OOM:

java.lang.OutOfMemoryError: Requested array size exceeds VM limit
	at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:73)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Can you please provide some hints on how to avoid/fix it?

Thank you!

Best,
Ovidiu"
Yang Cao <cybeater@gmail.com>,"Mon, 25 Jul 2016 23:59:30 +0800",get hdfs file path in spark,"dev@spark.apache.org,
 user@spark.apache.org","Hi,
To be new here, I hope to get assistant from you guys. I wonder whether I have some elegant way to get some directory under some path. For example, I have a path like on hfs /a/b/c/d/e/f, and I am given a/b/c, is there any straight forward way to get the path /a/b/c/d/e . I think I can do it with the help of regex. But I still hope to find whether there is easier way that make my code cleaner. My evn: spark 1.6, language: Scala


Thx
---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 25 Jul 2016 13:29:24 -0400",Re: where I can find spark-streaming-kafka for spark2.0,"Andy Davidson <andy@santacruzintegration.com>, Marco Mistroni <mmistroni@gmail.com>, 
	kevin <kiss.kevin119@gmail.com>","The presentation at Spark Summit SF was probably referring to Structured
Streaming. The existing Spark Streaming (dstream) in Spark 2.0 has the same
production stability level as Spark 1.6. There is also Kafka 0.10 support
in dstream.


Hi Kevin

Just a heads up at the recent spark summit in S.F. There was a presentation
about streaming in 2.0. They said that streaming was not going to
production ready in 2.0.

I am not sure if the older 1.6.x version will be supported. My project will
not be able to upgrade with streaming support. We also use kafka

Andy

From: Marco Mistroni <mmistroni@gmail.com>
Date: Monday, July 25, 2016 at 2:33 AM
To: kevin <kiss.kevin119@gmail.com>
Cc: ""user @spark"" <user@spark.apache.org>, ""dev.spark"" <dev@spark.apache.org
Subject: Re: where I can find spark-streaming-kafka for spark2.0

Hi Kevin
  you should not need to rebuild everything.
Instead, i believe you should launch spark-submit by specifying the kafka
jar file in your --packages... i had to follow same when integrating spark
streaming with flume

  have you checked this link ?
https://spark.apache.org/docs/latest/streaming-kafka-integration.html


hth




"
Jonathan Gray <jonny.gray@gmail.com>,"Mon, 25 Jul 2016 19:07:18 +0100",Re: Nested/Chained case statements generate codegen over 64k exception,Reynold Xin <rxin@databricks.com>,"I came back to this to try and investigate further using the latest version
of the project.  However, I don't have enough experience with the code base
to understand fully what is now happening, could someone take a look at the
testcase attached to this JIRA and run on the latest version of the code
base?

It currently appears as one branch of the code receives the code
compilation exception and so applies the fallback.  However, subsequent a
similar exception is thrown for different branches of the code (does the
non-compilable code get put into a cache somewhere?)  So, where it should
now be falling back to non-codegen it doesn't appear to completely.


"
Cody Koeninger <cody@koeninger.org>,"Mon, 25 Jul 2016 13:12:32 -0500",Re: where I can find spark-streaming-kafka for spark2.0,Reynold Xin <rxin@databricks.com>,"For 2.0, the kafka dstream support is in two separate subprojects
depending on which version of Kafka you are using

spark-streaming-kafka-0-10
or
spark-streaming-kafka-0-8

corresponding to brokers that are version 0.10+ or 0.8+


---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 25 Jul 2016 19:36:36 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Reynold Xin <rxin@databricks.com>,"When are we planning to push the release maven artifacts ? We are waiting
for this in order to push an official Apache Bahir release supporting Spark
2.0.




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
shane knapp <sknapp@berkeley.edu>,"Mon, 25 Jul 2016 14:29:40 -0700","[build system] jenkins downtime friday afternoon, july 29th 2016","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","around 1pm  friday, july 29th, we will be taking jenkins down for a
rack move and celebrating national systems administrator day.

the outage should only last a couple of hours at most, and will be
concluded with champagne toasts.

yes, the outage and holiday are real, but the champagne in the colo is
not...  ;)

shane

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 25 Jul 2016 22:43:05 +0000",Cartesian join between DataFrames,Spark dev list <dev@spark.apache.org>,"It appears that RDDs can do a cartesian join, but not DataFrames. Is there
a fundamental reason why not, or is this just waiting for someone to
implement?

I know you can get the RDDs underlying the DataFrames and do the cartesian
join that way, but you lose the schema of course.

Nick
"
Reynold Xin <rxin@databricks.com>,"Mon, 25 Jul 2016 18:45:31 -0400",Re: Cartesian join between DataFrames,"Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>","DataFrame can do cartesian joins.



It appears that RDDs can do a cartesian join, but not DataFrames. Is there
a fundamental reason why not, or is this just waiting for someone to
implement?

I know you can get the RDDs underlying the DataFrames and do the cartesian
join that way, but you lose the schema of course.

Nick
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 25 Jul 2016 23:01:41 +0000",Re: Cartesian join between DataFrames,"Reynold Xin <rxin@databricks.com>, Spark dev list <dev@spark.apache.org>","Oh, sorry you’re right. I looked at the doc for join()
<http://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join>
and didn’t realize you could do a cartesian join. But it turns out that
df1.join(df2) does the job and matches the SQL equivalent too.
​


e
n
"
Cody Koeninger <cody@koeninger.org>,"Mon, 25 Jul 2016 20:02:38 -0500","Re: Potential Change in Kafka's Partition Assignment Semantics when
 Subscription Changes",Vahid S Hashemian <vahidhashemian@us.ibm.com>,"This seems really low risk to me.  In order to be impacted, it'd have
to be someone who was using the kafka integration in spark 2.0, which
isn't even officially released yet.

s
ion+Assignment+Semantics+on+New+Consumer%27s+Subscription+Change
tion+Assignment+Semantics+on+New+Consumer%27s+Subscription+Change#KIP-70:RevisePartitionAssignmentSemanticsonNewConsumer'sSubscriptionChange-Compatibility,Deprecation,andMigrationPlan)

---------------------------------------------------------------------


"
Don Drake <dondrake@gmail.com>,"Mon, 25 Jul 2016 20:12:09 -0500",Fwd: Outer Explode needed,dev@spark.apache.org,"No response on the Users list, I thought I would repost here.

See below.

-Don
---------- Forwarded message ----------
From: Don Drake <dondrake@gmail.com>
Date: Sun, Jul 24, 2016 at 2:18 PM
Subject: Outer Explode needed
To: user <user@spark.apache.org>


I have a nested data structure (array of structures) that I'm using the DSL
df.explode() API to flatten the data.  However, when the array is empty,
I'm not getting the rest of the row in my output as it is skipped.

This is the intended behavior, and Hive supports a SQL ""OUTER explode()"" to
generate the row when the explode would not yield any output.

https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView

Can we get this same outer explode in the DSL?  I have to jump through some
outer join hoops to get the rows where the array is empty.

Thanks.

-Don

-- 
Donald Drake
Drake Consulting
http://www.drakeconsulting.com/
https://twitter.com/dondrake <http://www.MailLaunder.com/>
800-733-2143



-- 
Donald Drake
Drake Consulting
http://www.drakeconsulting.com/
https://twitter.com/dondrake <http://www.MailLaunder.com/>
800-733-2143
"
kevin <kiss.kevin119@gmail.com>,"Tue, 26 Jul 2016 09:13:23 +0800",Re: where I can find spark-streaming-kafka for spark2.0,Cody Koeninger <cody@koeninger.org>,"Thank you,I can't find spark-streaming-kafka_2.10 jar for spark2 from maven
center. so I try the version 1.6.2,it not work ,it need class
org.apache.spark.Logging, which can't find in spark2. so I build
spark-streaming-kafka_2.10
jar for spark2 from the source code. it's work now.

2016-07-26 2:12 GMT+08:00 Cody Koeninger <cody@koeninger.org>:

"
Michael Armbrust <michael@databricks.com>,"Mon, 25 Jul 2016 18:15:29 -0700",Re: Outer Explode needed,Don Drake <dondrake@gmail.com>,"I don't think this would be hard to implement.  The physical explode
operator supports it (for our HiveQL compatibility).

Perhaps comment on this JIRA?
https://issues.apache.org/jira/browse/SPARK-13721

It could probably just be another argument to explode()

Michael


"
kevin <kiss.kevin119@gmail.com>,"Tue, 26 Jul 2016 09:59:59 +0800",Re: Odp.: spark2.0 can't run SqlNetworkWordCount,=?UTF-8?Q?Tomasz_Gaw=C4=99da?= <tomasz.gaweda@outlook.com>,"thanks a lot .after change to scala 2.11 , it works.

2016-07-25 17:40 GMT+08:00 Tomasz Gawęda <tomasz.gaweda@outlook.com>:

e
la/reflect/api/JavaMirrors$JavaMirror;
67)
61)
ly$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
ly$mcV$sp$1.apply(ForEachDStream.scala:51)
ly$mcV$sp$1.apply(ForEachDStream.scala:51)
Stream.scala:415)
(ForEachDStream.scala:50)
hDStream.scala:50)
hDStream.scala:50)
$1.apply$mcV$sp(JobScheduler.scala:247)
$1.apply(JobScheduler.scala:247)
$1.apply(JobScheduler.scala:247)
uler.scala:246)
:1142)
a:617)
"
kevin <kiss.kevin119@gmail.com>,"Tue, 26 Jul 2016 11:25:12 +0800",spark2.0 how to use sparksession and StreamingContext same time,"""user.spark"" <user@spark.apache.org>, ""dev.spark"" <dev@spark.apache.org>","hi,all:
I want to read data from kafka and regist as a table then join a jdbc table.
My sample like this :

val spark = SparkSession
      .builder
      .config(sparkConf)
      .getOrCreate()

    val jdbcDF = spark.read.format(""jdbc"").options(Map(""url"" ->
""jdbc:mysql://master1:3306/demo"", ""driver"" -> ""com.mysql.jdbc.Driver"",
""dbtable"" -> ""i_user"", ""user"" -> ""root"", ""password"" -> ""passok"")).load()
    jdbcDF.cache().createOrReplaceTempView(""black_book"")
      val df = spark.sql(""select * from black_book"")
      df.show()

    val ssc = new StreamingContext(sparkConf, Seconds(2))
    ssc.checkpoint(""checkpoint"")

    val topicMap = topics.split("","").map((_, numThreads.toInt)).toMap
    val lines = KafkaUtils.createStream(ssc, zkQuorum, group,
topicMap).map(_._2)
    val words = lines.flatMap(_.split("" ""))

*I got error :*

16/07/26 11:18:07 WARN AbstractHandler: No Server set for
org.spark_project.jetty.server.handler.ErrorHandler@6f0ca692
+--------------------+--------+--------+
|                  id|username|password|
+--------------------+--------+--------+
|e6faca36-8766-4dc...|       a|       a|
|699285a3-a108-457...|   admin|     123|
|e734752d-ac98-483...|    test|    test|
|c0245226-128d-487...|   test2|   test2|
|4f1bbdb2-89d1-4cc...|     119|     911|
|16a9a360-13ee-4b5...|    1215|    1215|
|bf7d6a0d-2949-4c3...|   demo3|   demo3|
|de30747c-c466-404...|     why|     why|
|644741c9-8fd7-4a5...|   scala|       p|
|cda1e44d-af4b-461...|     123|     231|
|6e409ed9-c09b-4e7...|     798|      23|
+--------------------+--------+--------+

SparkContext may be running in this JVM (see SPARK-2243). To ignore this
error, set spark.driver.allowMultipleContexts = true. The currently running
SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:749)
main.POC$.main(POC.scala:43)
main.POC.main(POC.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:724)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
at
org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2211)
at
org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2207)
at scala.Option.foreach(Option.scala:257)
at
org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2207)
at
org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2277)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:91)
at
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
at
org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
at main.POC$.main(POC.scala:50)
at main.POC.main(POC.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:724)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
"
kevin <kiss.kevin119@gmail.com>,"Tue, 26 Jul 2016 12:12:28 +0800",Re: spark2.0 how to use sparksession and StreamingContext same time,Terry Hoo <hujie.eagle@gmail.com>,"thanks a lot Terry

2016-07-26 12:03 GMT+08:00 Terry Hoo <hujie.eagle@gmail.com>:

"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Tue, 26 Jul 2016 12:51:24 +0200",Remove / update version in spark-packages.org,dev@spark.apache.org,"Hi all,

Maybe I am missing something, but... Is there a way to update a package uploaded to spark-packages.org under the same version?

Given a release called my_package 1.1.2, I would like to re-upload it due to build failure; but I want to do it also as version 1.1.2...

Thank you.
---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 26 Jul 2016 14:48:17 +0200",Renaming spark.driver.appUIAddress to spark.yarn.driver.appUIAddress?,dev <dev@spark.apache.org>,"Hi,

Since spark.driver.appUIAddress is only used in Spark on YARN to
""announce"" the web UI's address, I think the setting should rather be
called spark.yarn.driver.appUIAddress (for consistency with the other
YARN-specific settings).

What do you think? I'd like to hear your thoughts before filling an JIRA issue.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Yong Zhang <java8964@hotmail.com>,"Tue, 26 Jul 2016 13:25:51 +0000",Re: Outer Explode needed,"Don Drake <dondrake@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","The reason of no response is that this feature is not available yet.


You can vote and following this JIRA https://issues.apache.org/jira/browse/SPARK-13721, if you really need this feature.


Yong


________________________________
From: Don Drake <dondrake@gmail.com>
Sent: Monday, July 25, 2016 9:12 PM
To: dev@spark.apache.org
Subject: Fwd: Outer Explode needed

No response on the Users list, I thought I would repost here.

See below.

-Don
---------- Forwarded message ----------
From: Don Drake <dondrake@gmail.com<mailto:dondrake@gmail.com>>
Date: Sun, Jul 24, 2016 at 2:18 PM
Subject: Outer Explode needed
To: user <user@spark.apache.org<mailto:user@spark.apache.org>>


I have a nested data structure (array of structures) that I'm using the DSL df.explode() API to flatten the data.  However, when the array is empty, I'm not getting the rest of the row in my output as it is skipped.

This is the intended behavior, and Hive supports a SQL ""OUTER explode()"" to generate the row when the explode would not yield any output.

https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView

Can we get this same outer explode in the DSL?  I have to jump through some outer join hoops to get the rows where the array is empty.

Thanks.

-Don

--
Donald Drake
Drake Consulting
http://www.drakeconsulting.com/
https://twitter.com/dondrake<http://www.MailLaunder.com/>
800-733-2143<tel:800-733-2143>



--
Donald Drake
Drake Consulting
http://www.drakeconsulting.com/
https://twitter.com/dondrake<http://www.MailLaunder.com/>
800-733-2143
"
thibaut <thibaut.gensollen@gmail.com>,"Tue, 26 Jul 2016 09:45:27 -0400",,dev@spark.apache.org,"unsuscribe

---------------------------------------------------------------------


"
Stephen Hellberg <hellbes@uk.ibm.com>,"Tue, 26 Jul 2016 07:37:12 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),dev@spark.apache.org," -1   Sorry, I've just noted that the RC5 proposal includes shipping Derby @
10.11.1.1 which is vulnerable to CVE: 2015-1832.
It would be ideal if we could instead ship 10.12.1.1 real soon.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 26 Jul 2016 16:03:35 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),Stephen Hellberg <hellbes@uk.ibm.com>,"The release vote has already closed and passed. Derby is only used in
tests AFAIK, so I don't think this is even critical let alone a
blocker. Updating is fine though, open a PR.


---------------------------------------------------------------------


"
Stephen Hellberg <hellbes@uk.ibm.com>,"Tue, 26 Jul 2016 09:43:38 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.0 (RC5),dev@spark.apache.org,"Yeah, I thought the vote was closed... but I couldn't think of a better
thread to remark upon!
That's a useful comment on Derby's role - thanks.  Certainly, we'd just
attempted a build-and-test execution with revising the Derby level to the
current 10.12.1.1, and hadn't observed any issues... a PR will be
forthcoming.



--

---------------------------------------------------------------------


"
Burak Yavuz <brkyvz@gmail.com>,"Tue, 26 Jul 2016 15:19:04 -0700",Re: Remove / update version in spark-packages.org,Julio Antonio Soto de Vicente <julio@esbet.es>,"Hi,

It's bad practice to change jars for the same version and is prohibited in
Spark Packages. Please bump your version number and make a new release.

Best regards,
Burak


"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Wed, 27 Jul 2016 01:52:22 +0200",Re: Remove / update version in spark-packages.org,Burak Yavuz <brkyvz@gmail.com>,"Hi Burak,

Yes, you're right.

Thanks.

 Spark Packages. Please bump your version number and make a new release.
ploaded to spark-packages.org under the same version?
 to build failure; but I want to do it also as version 1.1.2...
"
Reynold Xin <rxin@databricks.com>,"Tue, 26 Jul 2016 23:00:22 -0700",[ANNOUNCE] Announcing Apache Spark 2.0.0,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi all,

Apache Spark 2.0.0 is the first release of Spark 2.x line. It includes
2500+ patches from 300+ contributors.

To download Spark 2.0, head over to the download page:
http://spark.apache.org/downloads.html

To view the release notes:
http://spark.apache.org/releases/spark-release-2-0-0.html


(note: it can take a few hours for everything to be propagated, so you
might get 404 on some download links.  If you see any issues with the
release notes or webpage *please contact me directly, off-list*)
"
Ofir Manor <ofir.manor@equalum.io>,"Wed, 27 Jul 2016 10:18:41 +0300",Re: [ANNOUNCE] Announcing Apache Spark 2.0.0,Reynold Xin <rxin@databricks.com>,"Hold the release! There is a minor documentation issue :)
But seriously, congrats all on this massive achievement!

Anyway, I think it would be very helpful to add a link to the Structured
Streaming Developer Guide (Alpha) to both the documentation home page and
from the beginning of the ""old"" Spark Streaming Programming Guide, as I
think many users will look for them. I had a ""deep link"" to that page so I
haven't noticed that it is very hard to find until now. I'm referring to
this page:

http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html



Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


"
kevin <kiss.kevin119@gmail.com>,"Wed, 27 Jul 2016 19:06:02 +0800",tpcds for spark2.0,"""user.spark"" <user@spark.apache.org>, ""dev.spark"" <dev@spark.apache.org>","hi,all:
I want to have a test about tpcds99 sql run on spark2.0.
I user https://github.com/databricks/spark-sql-perf

about the master version ,when I run :val tpcds = new TPCDS (sqlContext =
sqlContext) I got error:

scala> val tpcds = new TPCDS (sqlContext = sqlContext)
error: missing or invalid dependency detected while loading class file
'Benchmarkable.class'.
Could not access term typesafe in package com,
because it (or its dependencies) are missing. Check your build definition
for
missing or conflicting dependencies. (Re-run with -Ylog-classpath to see
the problematic classpath.)
A full rebuild may help if 'Benchmarkable.class' was compiled against an
incompatible version of com.
error: missing or invalid dependency detected while loading class file
'Benchmarkable.class'.
Could not access term scalalogging in value com.typesafe,
because it (or its dependencies) are missing. Check your build definition
for
missing or conflicting dependencies. (Re-run with -Ylog-classpath to see
the problematic classpath.)
A full rebuild may help if 'Benchmarkable.class' was compiled against an
incompatible version of com.typesafe.

about spark-sql-perf-0.4.3 when I run
:tables.genData(""hdfs://master1:9000/tpctest"", ""parquet"", true, false,
false, false, false) I got error:

Generating table catalog_sales in database to
hdfs://master1:9000/tpctest/catalog_sales with save mode Overwrite.
16/07/27 18:59:59 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0,
slave1): java.lang.ClassCastException: cannot assign instance of
scala.collection.immutable.List$SerializationProxy to field
org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type
scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD
"
Holden Karau <holden@pigscanfly.ca>,"Wed, 27 Jul 2016 12:11:56 -0700",Internal Deprecation warnings - worth fixing?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Now that the 2.0 release is out the door and I've got some cycles to do
some cleanups -  I'd like to know what other people think of the internal
deprecation warnings we've introduced in a lot of a places in our code.
use the deprecated code to expose the deprecated API wouldn't gum up the
build logs - but is there interest in doing that or are we more interested
in not paying attention to the deprecation warnings for internal Spark
components (e.g.
https://twitter.com/thepracticaldev/status/725769766603001856 )?


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Jacek Laskowski <jacek@japila.pl>,"Wed, 27 Jul 2016 21:35:29 +0200",Re: Internal Deprecation warnings - worth fixing?,Holden Karau <holden@pigscanfly.ca>,"Kill 'em all -- one by one slowly yet gradually! :)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 27 Jul 2016 19:53:14 +0000",Re: Internal Deprecation warnings - worth fixing?,"Holden Karau <holden@pigscanfly.ca>, Jacek Laskowski <jacek@japila.pl>","+1 I don't believe there's any reason for the warnings to still be there
except for available dev time & focus :)


"
Dongjoon Hyun <dongjoon@apache.org>,"Wed, 27 Jul 2016 13:00:38 -0700",Re: Internal Deprecation warnings - worth fixing?,Nick Pentreath <nick.pentreath@gmail.com>,"+1 for fixing :)

Dongjoon.


"
Sean Owen <sowen@cloudera.com>,"Wed, 27 Jul 2016 16:11:19 -0700",Re: Internal Deprecation warnings - worth fixing?,Holden Karau <holden@pigscanfly.ca>,"Yeah I tried to zap lots of those before the release, but there are
still many of them, mostly from the accumulator change (really, that
should have been fixed as part of the original change? not so nice to
merge a change that adds 200 build warnings). Some deprecated code
must be called from tests to still test the deprecated code but it
ought to be possible to make the non-test code avoid it entirely.


---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Wed, 27 Jul 2016 16:20:00 -0700",Re: Internal Deprecation warnings - worth fixing?,Sean Owen <sowen@cloudera.com>,"Great it sounds like is a general consensus that these are worth fixing so
I'll start tackling some of these so we can hopefully get down to the point
that we will notice new warnings and be able to consider them.




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
shane knapp <sknapp@berkeley.edu>,"Wed, 27 Jul 2016 17:39:44 -0700","Re: [build system] jenkins downtime friday afternoon, july 29th 2016","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","reminder -- this is happening friday afternoon.

i will pause the build queue late friday morning.


---------------------------------------------------------------------


"
yuming wang <wgyumg@gmail.com>,"Thu, 28 Jul 2016 10:50:56 +0800",Apply patch to 1.2.1.spark2,dev@spark.apache.org,"Hi,

Is there a way to apply this patch
<https://issues.apache.org/jira/browse/HIVE-10790> to ?
"
Neil Chang <iambbq@gmail.com>,"Wed, 27 Jul 2016 23:32:54 -0400",How do a new developer create or assign a jira ticket?,dev@spark.apache.org,"Seems no way to get assigned a ticket or create a new one.
What's the best place to start contribution to SparkR?

Thanks,
Neil
"
Holden Karau <holden@pigscanfly.ca>,"Wed, 27 Jul 2016 20:48:26 -0700",Re: How do a new developer create or assign a jira ticket?,Neil Chang <iambbq@gmail.com>,"Hi Neil,

Thanks for your interest in participating in Apache Spark! You can create
JIRAs - but first you will need to signup for an Apache JIRA account.
Generally we can't assign JIRAs to ourselves - but you can leave a comment
saying your interested in working. I think for R a good place to get
started is possibly plumbing through existing functionality from Spark -
but thats based on my work in PySpark so this might not actually be the
best place for Spark R. If you havesn't already looked at
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark its
a good place to start.

Cheers,

Holden :)





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Neil Chang <iambbq@gmail.com>,"Thu, 28 Jul 2016 00:04:27 -0400",Re: How do a new developer create or assign a jira ticket?,Holden Karau <holden@pigscanfly.ca>,"Thanks Holden,
    Will check out!

Neil


"
censj <censj@lotuseed.com>,"Thu, 28 Jul 2016 17:31:54 +0800",,dev@spark.apache.org,"16/07/28 17:07:34 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig
  at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:45)
  at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:163)
  at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
  at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:150)
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)
  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:149)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:500)
  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2256)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:831)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$8.apply(SparkSession.scala:823)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823)
  at org.apache.spark.repl.Main$.createSparkSession(Main.scala:95)
  ... 47 elided
Caused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig
  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 60 more
<console>:14: error: not found: value spark
       import spark.implicits._
              ^
<console>:14: error: not found: value spark
       import spark.sql
              ^
Welcome to




hi:
	I use spark 2.0,but when I run  ""/etc/spark-2.0.0-bin-hadoop2.6/bin/spark-shell --master yarn , appear this Error.

/etc/spark-2.0.0-bin-hadoop2.6/bin/spark-submit
export YARN_CONF_DIR=/etc/hadoop/conf
export HADOOP_CONF_DIR=/etc/hadoop/conf
export SPARK_HOME=/etc/spark-2.0.0-bin-hadoop2.6


how I to update?





===============================
Name: cen sujun
Mobile: 13067874572
Mail: censj@lotuseed.com

"
Miki Shingo <m-shingo@bq.jp.nec.com>,"Thu, 28 Jul 2016 09:34:25 +0000",ERROR: java.net.UnknownHostException ,"""dev@spark.apache.org"" <dev@spark.apache.org>","To whom who has knowledge?

I have faced the following error try to use HA configuration.
(java.net.UnknownHostException)

below is the error for reference. 

16/07/27 22:42:56 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, dphmuyarn1107.hadoop.local): java.lang.IllegalArgumentException: java.net.UnknownHostException: hdpha
        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:411)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:311)
        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:150)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:170)
        at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:656)
        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:438)
        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:411)
        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(SparkContext.scala:1038)
        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(SparkContext.scala:1038)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:178)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:178)
        at scala.Option.map(Option.scala:145)
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:178)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:216)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:275)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hdpha
        ... 36 more

Thanks & Regards

  Miki

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 28 Jul 2016 18:46:45 -0400","renaming ""minor release"" to ""feature release""","""dev@spark.apache.org"" <dev@spark.apache.org>","*tl;dr*

I would like to propose renaming “minor release” to “feature release” in
Apache Spark.


*details*

Apache Spark’s official versioning policy follows roughly semantic
versioning. Each Spark release is versioned as
[major].[minor].[maintenance]. That is to say, 1.0.0 and 2.0.0 are both
“major releases”, whereas “1.1.0” and “1.3.0” would be minor releases.

I have gotten a lot of feedback from users that the word “minor” is
confusing and does not accurately describes those releases. When users hear
the word “minor”, they think it is a small update that introduces couple
minor features and some bug fixes. But if you look at the history of Spark
1.x, here are just a subset of large features added:

Spark 1.1: sort-based shuffle, JDBC/ODBC server, new stats library, 2-5X
perf improvement for machine learning.

Spark 1.2: HA for streaming, new network module, Python API for streaming,
ML pipelines, data source API.

Spark 1.3: DataFrame API, Spark SQL graduate out of alpha, tons of new
algorithms in machine learning.

Spark 1.4: SparkR, Python 3 support, DAG viz, robust joins in SQL, math
functions, window functions, SQL analytic functions, Python API for
pipelines.

Spark 1.5: code generation, Project Tungsten

Spark 1.6: automatic memory management, Dataset API, ML pipeline persistence


So while “minor” is an accurate depiction of the releases from an API
compatibiility point of view, we are miscommunicating and doing Spark a
disservice by calling these releases “minor”. I would actually call these
releases “major”, but then it would be a larger deviation from semantic
versioning. I think calling these “feature releases” would be a smaller
change and a more accurate depiction of what they are.

That said, I’m not attached to the name “feature” and am open to
suggestions, as long as they don’t convey the notion of “minor”.
"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Jul 2016 16:06:06 -0700","Re: [build system] jenkins downtime friday afternoon, july 29th 2016","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","reminder -- this is happening TOMORROW.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 28 Jul 2016 16:10:46 -0700","Re: renaming ""minor release"" to ""feature release""",Reynold Xin <rxin@databricks.com>,"Although 'minor' is the standard term, the important thing is making
the nature of the release understood. 'feature release' seems OK to me
as an additional description.

Is it worth agreeing on or stating a little more about the theory?

patch release: backwards/forwards compatible within a minor release,
generally fixes only
minor/feature release: backwards compatible within a major release,
not forward; generally also includes new features
major release: not backwards compatible and may remove or change
existing features

feature release” in
c
1.3.0” would be minor releases.
” is
ar
troduces couple
k
,
nce
 from an API
ually call these
 from semantic
d be a smaller
 and am open to
minor”.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 28 Jul 2016 16:15:09 -0700","Re: renaming ""minor release"" to ""feature release""",Sean Owen <sowen@cloudera.com>,"Yea definitely. Those are consistent with what is defined here:
https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy

The only change I'm proposing is replacing ""minor"" with ""feature"".



feature release” in
tic
“1.3.0” would be minor releases.
” is
introduces couple
X
es from an API
ctually call these
on from semantic
uld be a smaller
 and am open to
minor”.
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Fri, 29 Jul 2016 08:15:58 +0900",Re: ERROR: java.net.UnknownHostException,"Miki Shingo <m-shingo@bq.jp.nec.com>,
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Miki,

What version of Spark are you using?
If the version is > 1.4,  you might hit SPARK-11227.

- Kousuke




---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 28 Jul 2016 16:20:03 -0700","Re: renaming ""minor release"" to ""feature release""",Reynold Xin <rxin@databricks.com>,"I also agree with this given the way we develop stuff. We don't really want to move to possibly-API-breaking major releases super often, but we do have lots of large features that come out all the time, and our current name doesn't convey that.

Matei

https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy <https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy“feature release” in
semantic
both
and “1.3.0” would be minor releases.
r” is
users hear
that introduces couple
Spark
2-5X
streaming,
new
math
persistence
releases from an API
Spark a
would actually call these
deviation from semantic
would be a smaller
 and am open to
“minor”.

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 29 Jul 2016 00:33:39 +0000","Re: renaming ""minor release"" to ""feature release""","Matei Zaharia <matei.zaharia@gmail.com>, Reynold Xin <rxin@databricks.com>","+1

The semantics conveyed by ""feature release"" are compatible with the meaning
of ""minor release"" under strict SemVer, but as argued are clearer from a
user-communication point of view.

http://semver.org

Nick
2016년 7월 28일 (목) 오후 7:20, Matei"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 29 Jul 2016 02:06:57 +0000",PySpark UDFs with a return type of FloatType can't handle int return values,Spark dev list <dev@spark.apache.org>,"If I define a UDF in PySpark that has a return type of FloatType, but the
underlying function actually returns an int, the UDF throws the int away
and returns None.

It seems that some machinery inside pyspark.sql.types is perhaps unaware
that it can always cast ints to floats.

Is this functionality that we would want to add in, or is it beyond the
scope of what UDFs should be expected to do?

Nick
​
"
Miki Shingo <m-shingo@bq.jp.nec.com>,"Thu, 28 Jul 2016 09:44:55 +0000",RE: ERROR: java.net.UnknownHostException ,"""dev@spark.apache.org"" <dev@spark.apache.org>","All,

I have resolved the issue.
Sorry for your interruption.

Regards,

  Miki


I have faced the following error try to use HA configuration.
(java.net.UnknownHostException)

below is the error for reference. 

16/07/27 22:42:56 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, dphmuyarn1107.hadoop.local): java.lang.IllegalArgumentException: java.net.UnknownHostException: hdpha
        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:411)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:311)
        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:150)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:170)
        at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:656)
        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:438)
        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:411)
        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(SparkContext.scala:1038)
        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(SparkContext.scala:1038)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:178)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:178)
        at scala.Option.map(Option.scala:145)
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:178)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:216)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:275)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hdpha
        ... 36 more

Thanks & Regards

  Miki

---------------------------------------------------------------------


"
vaquar khan <vaquar.khan@gmail.com>,"Thu, 28 Jul 2016 22:20:48 -0500","Re: renaming ""minor release"" to ""feature release""",Matei Zaharia <matei.zaharia@gmail.com>,"+1
Though following is commonly use standard for
release(http://semver.org/) ,feature
also looks good as Minor release indicate significant features have been
added

   1. MAJOR version when you make incompatible API changes,
   2. MINOR version when you "
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 29 Jul 2016 13:17:48 +0000",Re: tpcds for spark2.0,kevin <kiss.kevin119@gmail.com>,"I have the same kind of issue (not using spark-sql-perf), just trying to deploy
2.0.0 on mesos. I'll keep you posted as I investigate





hi,all: I want to have a test about tpcds99 sql run on spark2.0. I user https://github.com/databricks/spark-sql-perf
about the master version ,when I run :val tpcds = new TPCDS (sqlContext =
sqlContext) I got error:
scala> val tpcds = new TPCDS (sqlContext = sqlContext)
error: missing or invalid dependency detected while loading class file
'Benchmarkable.class'.
Could not access term typesafe in package com,
because it (or its dependencies) are missing. Check your build definition for
missing or conflicting dependencies. (Re-run with -Ylog-classpath to see the problematic classpath.)
A full rebuild may help if 'Benchmarkable.class' was compiled against an
incompatible version of com.
error: missing or invalid dependency detected while loading class file
'Benchmarkable.class'.
Could not access term scalalogging in value com.typesafe,
because it (or its dependencies) are missing. Check your build definition for
missing or conflicting dependencies. (Re-run with -Ylog-classpath to see the problematic classpath.)
A full rebuild may help if 'Benchmarkable.class' was compiled against an
incompatible version of com.typesafe.

about spark-sql-perf-0.4.3 when I run :tables.genData(""hdfs://master1:9000/tpctest"",
""parquet"", true, false, false, false, false) I got error:
Generating table catalog_sales in database to
hdfs://master1:9000/tpctest/catalog_sales with save mode Overwrite. 16/07/27 18:59:59 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0,
slave1): java.lang.ClassCastException: cannot assign instance of
scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org $apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance
of org.apache.spark.rdd.MapPartitionsRDD


Olivier Girardot | Associé
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 29 Jul 2016 07:52:12 -0700","Re: renaming ""minor release"" to ""feature release""",vaquar khan <vaquar.khan@gmail.com>,"not include only new features, but also many bug-fixes -- at least some of
which often do not get backported into the next patch-level release.
 ""Feature release"" does not convey that information.


 ,feature
 do
y
“feature release”
antic
th
 “1.3.0” would be minor releases.
or” is
s
t introduces
w
th
ases from an API
 a
 actually call
tion from semantic
would be a smaller
 and am open to
minor”.
"
Martin Le <martin.lequoc@gmail.com>,"Fri, 29 Jul 2016 16:57:25 +0200",sampling operation for DStream,user@spark.apache.org,"Hi all,

I have to handle high-speed rate data stream. To reduce the heavy load, I
want to use sampling techniques for each stream window. It means that I
want to process a subset of data instead of whole window data. I saw Spark
support sampling operations for RDD, but for DStream, Spark supports
sampling operation as well? If not,  could you please give me a suggestion
how to implement it?

Thanks,
Martin
"
Cody Koeninger <cody@koeninger.org>,"Fri, 29 Jul 2016 11:28:09 -0500",Re: sampling operation for DStream,Martin Le <martin.lequoc@gmail.com>,"Most stream systems you're still going to incur the cost of reading
each message... I suppose you could rotate among reading just the
latest messages from a single partition of a Kafka topic if they were
evenly balanced.

But once you've read the messages, nothing's stopping you from
filtering most of them out before doing further processing.  The
dstream .transform method will let you do any filtering / sampling you
could have done on an rdd.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Jul 2016 10:53:44 -0700","Re: [build system] jenkins downtime friday afternoon, july 29th 2016","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","reminder -- this is happening TODAY.  jenkins is currently in quiet mode.

i will post updates over the course of the afternoon, and we should be
back up and building before COB.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 29 Jul 2016 19:56:10 +0000",Clarifying that spark-x.x.x-bin-hadoopx.x.tgz doesn't include Hadoop itself,Spark dev list <dev@spark.apache.org>,"I had an interaction on my project
<https://github.com/nchammas/flintrock/issues/139#issuecomment-236268723>
today that suggested some people may be confused about what the packages
available on the downloads page are actually for.

Specifically, the various -hadoopx.x.tgz packages suggest that Hadoop
itself is actually included in the package. I’m not 100% sure myself
honestly, but as I explained in my comment linked above, I believe the
-hadoopx.x.tgz just indicates the version of Hadoop that Spark was built
against.

Does it make sense to add a brief note to the downloads page
<http://spark.apache.org/downloads.html> explaining this?

I am assuming it would be too disruptive to change the package names to
something more descriptive like -built-against-hadoopx.x.tgz.

Nick
​
"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Jul 2016 13:03:16 -0700","Re: [build system] jenkins downtime friday afternoon, july 29th 2016","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","machines are going down NOW


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 29 Jul 2016 13:06:13 -0700","Re: Clarifying that spark-x.x.x-bin-hadoopx.x.tgz doesn't include
 Hadoop itself",Nicholas Chammas <nicholas.chammas@gmail.com>,"Why do you say Hadoop is not included?

The Hadoop jars are there in the tarball, and match the advertised
version. There is (or at least there was in 1.x) a version called
""without-hadoop"" which did not include any Hadoop jars.

e
elf
estly, but
ust



-- 
Marcelo

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 29 Jul 2016 15:09:52 -0500","Re: Clarifying that spark-x.x.x-bin-hadoopx.x.tgz doesn't include
 Hadoop itself",Marcelo Vanzin <vanzin@cloudera.com>,"Yeah, and the without hadoop was even more confusing... because if you
weren't using hdfs at all, you still needed to download one of the
hadoop-x packages in order to get hadoop io classes used by almost
everything.  :)

:
be
self
nestly, but
just

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 29 Jul 2016 20:13:08 +0000","Re: Clarifying that spark-x.x.x-bin-hadoopx.x.tgz doesn't include
 Hadoop itself",Marcelo Vanzin <vanzin@cloudera.com>,"Hmm, perhaps I'm the one who's confused. 🤔

I thought the person in the linked discussion expected Hadoop itself (i.e.
the full application, not just the jars) to somehow be included, but
rereading the discussion I may have just misinterpreted them.

The Hadoop jars packaged with Spark just allow Spark to interact with
Hadoop, or allow it to use the Hadoop API for interacting with systems like
S3, right? If you want HDFS, MapReduce, etc. you're obviously not getting
that in those Spark packages.

Maybe this was already clear to those users and I just injected some
confusion into the discussion.

Nick


onestly,
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 29 Jul 2016 13:16:16 -0700","Re: Clarifying that spark-x.x.x-bin-hadoopx.x.tgz doesn't include
 Hadoop itself",Nicholas Chammas <nicholas.chammas@gmail.com>,"
Correct. They're the Hadoop client jars needed to talk to Hadoop
services (and also because Spark exposes some Hadoop APIs in its own
public API).

-- 
Marcelo

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Jul 2016 13:45:36 -0700","Re: [build system] jenkins downtime friday afternoon, july 29th 2016","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","the move is complete and the machines powered back up right away, with
no problems.  we're doing a quick update on the firewall, and then
we'll be done!


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 29 Jul 2016 13:52:56 -0700","Re: [build system] jenkins downtime friday afternoon, july 29th 2016",shane knapp <sknapp@berkeley.edu>,"Nice! Thanks!



"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Jul 2016 14:03:29 -0700","Re: [build system] jenkins downtime friday afternoon, july 29th 2016","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","we're done and building!

a bunch of builds failed w/git auth issues, due to me cancelling the
quiet period early (as i thought the firewall update was done).  this
is no longer the case as i was more patient this time.  :)

happy friday!

shane


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 30 Jul 2016 10:57:25 +0200",[YARN] Question about ApplicationMaster's shutdown hook (priority),dev <dev@spark.apache.org>,"Hi,

When ApplicationMaster runs it registers a shutdown hook [1] that
(quoting the comment [2] from the code):


And so it gets priority lower than SparkContext [3], i.e.

val priority = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY - 1

But, reading ShutdownHookManager.addShutdownHook says [4]:


My understanding is that one comment is no longer true (if it has ever been).

Please help me understand that part of the code. Thanks.

[1] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L206
[2] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L204
[3] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L205
[4] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala#L146-L147

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Tony Lane <tonylane.nyc@gmail.com>,"Sat, 30 Jul 2016 14:57:02 +0530",Spark 2.0 blocker on windows - spark-warehouse path issue,"user@spark.apache.org, dev@spark.apache.org","Caused by: java.net.URISyntaxException: Relative path in absolute URI:
file:C:/ibm/spark-warehouse

Anybody knows a solution to this?

cheers
tony
"
Sean Owen <sowen@cloudera.com>,"Sat, 30 Jul 2016 04:40:46 -0700",Re: Spark 2.0 blocker on windows - spark-warehouse path issue,Tony Lane <tonylane.nyc@gmail.com>,"This is https://issues.apache.org/jira/browse/SPARK-15899


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 30 Jul 2016 05:10:27 -0700",Re: [YARN] Question about ApplicationMaster's shutdown hook (priority),Jacek Laskowski <jacek@japila.pl>,"Good catch, yes, it's the scaladoc of addShutdownHook that is wrong.
It says lower priority executes firs.t

The implementation seems to do the opposite. It uses a min queue of
shutdown hooks, but inverts the notion of ordering to execute higher
priority values first.

The constants and comments in ShutdownHookManager are consistent with
executing higher priority values first.

So I think you can fix the scaladoc. Other usages of priority seem
consistent with current behavior.




---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 31 Jul 2016 10:44:35 +0200","[YARN] YarnAllocator.updateResourceRequests -- could be simpler and
 faster, too?",dev <dev@spark.apache.org>,"Hi,

I've been reviewing YarnAllocator.updateResourceRequests and think the
other branch is...too verbose (and may be deceiving that it's more
complex than it really is). I hope to get corrected.

The source code of the method is
https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L294.

The method is about requesting new or cancelling outstanding YARN
container requests for executors. Right?

It starts by checking the executor container requests using
getPendingAllocate [1] and branches by missing (the other branch uses
numPendingAllocate also which is going to be crucial in my
understanding of the method).

So, in the other branch, when the number of outstanding YARN
containers is too much [2], the method calls [3]

amClient.getMatchingRequests(RM_REQUEST_PRIORITY, ANY_HOST, resource)

which is exactly getPendingAllocate [4] (!) Is that correct?

If my understanding is correct, the code does not need to call
getPendingAllocate in the branch since it had already requested it in
[1] (at the very beginning) and since we're inside the branch the code
does not have to check ""matchingRequests.isEmpty"" either.

My understanding is that the code should be as simple as the following:

    } else if (numPendingAllocate > 0 && missing < 0) {
      val numToCancel = math.min(numPendingAllocate, -missing)
      logInfo(s""Canceling requests for $numToCancel executor
container(s) to have a new desired "" +
        s""total $targetNumExecutors executors."")
      pendingAllocate.take(numToCancel).foreach(amClient.removeContainerRequest)
    }

i.e. just a single pendingAllocate.take...

Is that correct?

THANKS a lot for reading thus far!!! I greatly appreciate your time
and effort to help me understand Spark.

p.s. I'm ready with a PR.

[1] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L295
[2] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L355
[3] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L360
[4] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L200

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
